<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.10884] Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives</title><meta property="og:description" content="Federated learning is fast becoming a popular paradigm for applications involving mobile devices, banking systems, healthcare, and IoT systems. Hence, over the past five years, researchers have undertaken extensive stu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.10884">

<!--Generated on Sat Jul  6 00:01:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning,  data security,  data privacy,  model fairness">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linlin Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:linlinwang.cityu@gmail.com">linlinwang.cityu@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">City University of Macau</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">Avenida Padre Tomás Pereira Taipa</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Macau</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianqing Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tianqing.zhu@uts.edu.au">tianqing.zhu@uts.edu.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of Technology Sydney</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_streetaddress">PO Box 123 Broadway</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_city">Sydney</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_state">NSW</span><span id="id9.5.id5" class="ltx_text ltx_affiliation_country">Australia</span><span id="id10.6.id6" class="ltx_text ltx_affiliation_postcode">2007</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wanlei Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wlzhou@cityu.edu.mo">wlzhou@cityu.edu.mo</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">City University of Macau</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_streetaddress">Avenida Padre Tomás Pereira Taipa</span><span id="id13.3.id3" class="ltx_text ltx_affiliation_city">Macau</span><span id="id14.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philip S. Yu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:psyu@uic.edu">psyu@uic.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_affiliation_institution">University of Illinois at Chicago</span><span id="id16.2.id2" class="ltx_text ltx_affiliation_streetaddress">851 S. Morgan St., Rm 1138 SEO, Chicago, IL 60607</span><span id="id17.3.id3" class="ltx_text ltx_affiliation_city">Chicago</span><span id="id18.4.id4" class="ltx_text ltx_affiliation_country">US</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id19.id1" class="ltx_p">Federated learning is fast becoming a popular paradigm for applications involving mobile devices, banking systems, healthcare, and IoT systems. Hence, over the past five years, researchers have undertaken extensive studies on the privacy leaks, security threats, and fairness associated with these emerging models. For the most part, these three critical concepts have been studied in isolation; however, recent research has revealed that there may be an intricate interplay between them. For instance, some researchers have discovered that pursuing fairness may compromise privacy, or that efforts to enhance security can impact fairness. These emerging insights shed light on the fundamental connections between privacy, security, and fairness within federated learning, and, by delving deeper into these interconnections, we may be able to significantly augment research and development across the field. Consequently, the aim of this survey is to offer comprehensive descriptions of the privacy, security, and fairness issues in federated learning. Moreover, we analyze the complex relationships between these three dimensions of cyber safety and pinpoint the fundamental elements that influence each of them. We contend that there exists a trade-off between privacy and fairness and between security and gradient sharing. On this basis, fairness can function as a bridge between privacy and security to build models that are either more secure or more private. Building upon our observations, we identify the trade-offs between privacy and fairness and between security and fairness within the context of federated learning. The survey then concludes with promising directions for future research in this vanguard field.</p>
</div>
<div class="ltx_keywords">Federated learning, data security, data privacy, model fairness
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning has garnered considerable attention in recent years due to its potential to effectively address data privacy concerns among multiple parties <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2019</a>)</cite>. Federated learning is a distributed learning approach that comprises local clients and a central server <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2017a</a>)</cite>. Within this framework, the local clients each train their own machine learning model using their own local data, while the central server coordinates their activities. Each client then sends the parameters of their model to the central server, who aggregates the updates and returns a global model to each client <cite class="ltx_cite ltx_citemacro_citep">(Issa et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2023</a>)</cite>. Crucially, the central server never directly accesses the local data, which drastically reduces the risk of information leaks and privacy violations. Since its inception, federated learning has found numerous applications across a diverse range of fields, including mobile devices <cite class="ltx_cite ltx_citemacro_citep">(Lim et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020</a>)</cite>, banking systems <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2019</a>)</cite>, healthcare institutions  <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2022a</a>)</cite>, the Internet of Things (IoT) <cite class="ltx_cite ltx_citemacro_citep">(Rahman et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2021</a>)</cite>, and other domains  <cite class="ltx_cite ltx_citemacro_citep">(Hu and Vasilakos, <a href="#bib.bib63" title="" class="ltx_ref">2016</a>)</cite>. Furthermore, federated learning has been explored as a potential solution to address the issue of data fragmentation, commonly referred to as the ”data island problem” <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although federated learning was originally designed to safeguard machine learning privacy, previous research has revealed a myriad of privacy and security vulnerabilities inherent to federated learning. Concerns over fairness have also been raised <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>)</cite>. For instance, several studies <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2021b</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib161" title="" class="ltx_ref">2022</a>)</cite> have explored how adversaries and attacks, such as gradient leaks and inference attacks, can introduce privacy risks to a federated learning framework. Manifest security issues include poisoning attacks and backdoor attacks. In addition to privacy and security concerns, certain investigations <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2022c</a>)</cite> have identified fairness issues that permeate a number of facets of federated learning, including client selection, model optimization, and contribution allocation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To date, privacy, security, and fairness have primarily been explored in isolation. However, recent studies have revealed that there are likely to be intricate interdependencies among these constructs  <cite class="ltx_cite ltx_citemacro_citep">(Cummings et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Padala et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2021</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Ozdayi and Kantarcioglu, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>; Furth, <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>. For instance, researchers have found instances where the pursuing fairness can potentially compromise privacy <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>; Cummings et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Pentyala et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2022</a>; Padala et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2021</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, and where increasing security has impacted fairness <cite class="ltx_cite ltx_citemacro_citep">(Ozdayi and Kantarcioglu, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>; Furth, <a href="#bib.bib48" title="" class="ltx_ref">2022</a>; Xu and Lyu, <a href="#bib.bib141" title="" class="ltx_ref">2020</a>)</cite>. As another example, differential privacy is one of the most widely used techniques for preserving privacy  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>)</cite>, but scholars are beginning to discover that, beyond affecting model accuracy, differential privacy may also exacerbate issues with fairness  <cite class="ltx_cite ltx_citemacro_citep">(Cummings et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. Moreover, when a federated learning framework becomes vulnerable to attacks, such as when a benign global model is substituted with a poisoned counterpart, it not only compromises security but also impacts fairness<cite class="ltx_cite ltx_citemacro_citep">(Ozdayi and Kantarcioglu, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>. These nascent findings have the potential to reveal fundamental interconnections between privacy, security, and fairness within the realm of federated learning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We posit that the root cause of security and privacy concerns in federated learning lies in the practice of gradient sharing. Building upon this premise, we contend that fairness might function as a pivotal conduit between privacy and security, facilitating the attainment of heightened security and enhanced privacy within the model. Furthermore, we assert that trade-offs exist between privacy and fairness, as well as between security and fairness in the context of federated learning. Embarking on a more extensive exploration of the intricate relationships binding these dimensions should bolster our research and development efforts in federated learning to advance the field in diverse and meaningful ways.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Despite the abundance of prior surveys addressing privacy, security, and fairness <cite class="ltx_cite ltx_citemacro_citep">(AbdulRahman et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Blanco-Justicia et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>; Gosselin et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>; Truong et al<span class="ltx_text">.</span>, <a href="#bib.bib127" title="" class="ltx_ref">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2022b</a>; Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2022</a>)</cite>, it is notable that these surveys typically treat privacy and security as distinct entities. We posit that, for a comprehensive understanding of privacy, security, and fairness in federated learning, it is imperative to identify the underlying nexus connecting these dimensions, rather than simply addressing each concept in isolation. To this end, we have undertaken an exhaustive examination of the pertinent research conducted in recent years. Our aim is to crystallize the fundamental interconnections between security, privacy, and fairness as it pertains to federated learning. We also intend to address existing challenges, offering fresh perspectives on achieving a nuanced equilibrium between our three constructs. The hope is that this survey will yield novel insights to help resolve privacy, security, and fairness concerns in federated learning. For example, Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates that the considering fairness is pivotal when contemplating privacy preservation within the federated learning paradigm. However, it is noteworthy that the prevailing privacy technologies may inadvertently attenuate fairness, and that security issues can also create fairness concerns. Consequently, fairness emerges as a bridge that can traverse the divide between privacy and security, ultimately establishing a trade-off relationship among these three fundamental dimensions. Moreover, at the core of the security and privacy concerns in federated learning lies the challenge of gradient sharing.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.10884/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="125" height="90" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">New Balances of Privacy, Security, and Fairness</span></figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we present a thorough and inclusive exploration of the intricate interplay between privacy and security within the realm of federated learning. Our examination also extends to evaluating any methods designed to promote fairness. The methods are categorized by type and accompanied by a comprehensive analysis of the potential attacks and defense methods currently known. The survey concludes with promising avenues for future research.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The principal contributions of this survey can be summarized as follows:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">This survey explores the equilibrium between privacy, security, and fairness within federated learning frameworks. The intricacies associated with gradient sharing are also discussed. We find that trade-offs exist between privacy and fairness and between fairness and security, with fairness serving as a bridge in this complex interplay.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">We introduce the concept of fairness as it pertains to federated learning, describing and analyzing numerous methodologies to achieve and increase fairness. In addition, we have undertaken a comprehensive examination of privacy and security concerns, exploring them through various lenses and offering insights to shape the direction of future research. These perspectives span considerations relating to adversaries, the trained models, gradient information, and data partitioning – each of which contributes to our understanding of potential privacy breaches. In terms of security threats, we consider the integrity of the trained model, gradient data, the integrity of the training dataset, and potential adversarial actions.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">We discuss potential research directions poised to advance our understanding of federated learning and the capabilities of the field.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The structure of the remaining article is as follows: In Section  <a href="#S2" title="2. Preliminary ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we offer an overview of federated learning and introduce generic privacy and security techniques relevant to federated learning. Section  <a href="#S3" title="3. The links between PRIVACY, SECURITY AND FAIRNESS ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a comprehensive summary of the entanglement between privacy, security, and fairness within the federated learning paradigm. Section  <a href="#S4" title="4. Fairness ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> furnishes an essential background on fairness in federated learning. Section  <a href="#S5" title="5. Privacy ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> offers a taxonomy of privacy considerations, encompassing privacy leakage attacks, privacy-preserving methods, and their applications. In Section  <a href="#S6" title="6. Security ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we provide a taxonomy of security threats, defense mechanisms, and their practical applications. Furthermore, Section  <a href="#S7" title="7. Challenges and Open Research Directions in Federated Learning ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> spotlights the current challenges and delineates promising avenues for future research. Finally, Section  <a href="#S8" title="8. Conclusion ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> concludes this survey by summarizing the key findings and offering a conclusive perspective on the subject matter.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Preliminary</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Notations</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The notations used in this article are listed in Table <a href="#S2.T1" title="Table 1 ‣ 2.1. Notations ‣ 2. Preliminary ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.46.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S2.T1.47.2" class="ltx_text" style="font-size:90%;">Summary of Notations Used in the Article</span></figcaption>
<div id="S2.T1.44" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:225.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-129.4pt,67.3pt) scale(0.626315668719782,0.626315668719782) ;">
<table id="S2.T1.44.44" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.44.44.45.1" class="ltx_tr">
<th id="S2.T1.44.44.45.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Notations</th>
<th id="S2.T1.44.44.45.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Description</th>
<th id="S2.T1.44.44.45.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Notations</th>
<th id="S2.T1.44.44.45.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Description</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mi id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">M</annotation></semantics></math></td>
<td id="S2.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">A randomized algorithm</td>
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S2.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.T1.2.2.2.2.m1.1a"><mi id="S2.T1.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">A</annotation></semantics></math></td>
<td id="S2.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">The sensitive attributes of the data</td>
</tr>
<tr id="S2.T1.4.4.4" class="ltx_tr">
<td id="S2.T1.3.3.3.1" class="ltx_td ltx_align_center"><math id="S2.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.T1.3.3.3.1.m1.1a"><mi id="S2.T1.3.3.3.1.m1.1.1" xref="S2.T1.3.3.3.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">D</annotation></semantics></math></td>
<td id="S2.T1.4.4.4.3" class="ltx_td ltx_align_center">The datasets</td>
<td id="S2.T1.4.4.4.2" class="ltx_td ltx_align_center"><math id="S2.T1.4.4.4.2.m1.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.T1.4.4.4.2.m1.1a"><msub id="S2.T1.4.4.4.2.m1.1.1" xref="S2.T1.4.4.4.2.m1.1.1.cmml"><mi id="S2.T1.4.4.4.2.m1.1.1.2" xref="S2.T1.4.4.4.2.m1.1.1.2.cmml">Y</mi><mi id="S2.T1.4.4.4.2.m1.1.1.3" xref="S2.T1.4.4.4.2.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.2.m1.1b"><apply id="S2.T1.4.4.4.2.m1.1.1.cmml" xref="S2.T1.4.4.4.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.4.4.4.2.m1.1.1.1.cmml" xref="S2.T1.4.4.4.2.m1.1.1">subscript</csymbol><ci id="S2.T1.4.4.4.2.m1.1.1.2.cmml" xref="S2.T1.4.4.4.2.m1.1.1.2">𝑌</ci><ci id="S2.T1.4.4.4.2.m1.1.1.3.cmml" xref="S2.T1.4.4.4.2.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.2.m1.1c">Y_{p}</annotation></semantics></math></td>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_center">The predictor outputs predicted outcome</td>
</tr>
<tr id="S2.T1.6.6.6" class="ltx_tr">
<td id="S2.T1.5.5.5.1" class="ltx_td ltx_align_center"><math id="S2.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S2.T1.5.5.5.1.m1.1a"><msup id="S2.T1.5.5.5.1.m1.1.1" xref="S2.T1.5.5.5.1.m1.1.1.cmml"><mi id="S2.T1.5.5.5.1.m1.1.1.2" xref="S2.T1.5.5.5.1.m1.1.1.2.cmml">D</mi><mo id="S2.T1.5.5.5.1.m1.1.1.3" xref="S2.T1.5.5.5.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.1.m1.1b"><apply id="S2.T1.5.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.5.5.5.1.m1.1.1.1.cmml" xref="S2.T1.5.5.5.1.m1.1.1">superscript</csymbol><ci id="S2.T1.5.5.5.1.m1.1.1.2.cmml" xref="S2.T1.5.5.5.1.m1.1.1.2">𝐷</ci><ci id="S2.T1.5.5.5.1.m1.1.1.3.cmml" xref="S2.T1.5.5.5.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.1.m1.1c">D^{\prime}</annotation></semantics></math></td>
<td id="S2.T1.6.6.6.3" class="ltx_td ltx_align_center">The neighboring datasets</td>
<td id="S2.T1.6.6.6.2" class="ltx_td ltx_align_center"><math id="S2.T1.6.6.6.2.m1.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.T1.6.6.6.2.m1.1a"><mi id="S2.T1.6.6.6.2.m1.1.1" xref="S2.T1.6.6.6.2.m1.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.2.m1.1b"><ci id="S2.T1.6.6.6.2.m1.1.1.cmml" xref="S2.T1.6.6.6.2.m1.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.2.m1.1c">Y</annotation></semantics></math></td>
<td id="S2.T1.6.6.6.4" class="ltx_td ltx_align_center">The labels of the data</td>
</tr>
<tr id="S2.T1.8.8.8" class="ltx_tr">
<td id="S2.T1.7.7.7.1" class="ltx_td ltx_align_center"><math id="S2.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.T1.7.7.7.1.m1.1a"><mi id="S2.T1.7.7.7.1.m1.1.1" xref="S2.T1.7.7.7.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.1.m1.1b"><ci id="S2.T1.7.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.7.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.1.m1.1c">\epsilon</annotation></semantics></math></td>
<td id="S2.T1.8.8.8.3" class="ltx_td ltx_align_center">The privacy budget</td>
<td id="S2.T1.8.8.8.2" class="ltx_td ltx_align_center"><math id="S2.T1.8.8.8.2.m1.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S2.T1.8.8.8.2.m1.1a"><msup id="S2.T1.8.8.8.2.m1.1.1" xref="S2.T1.8.8.8.2.m1.1.1.cmml"><mi id="S2.T1.8.8.8.2.m1.1.1.2" xref="S2.T1.8.8.8.2.m1.1.1.2.cmml">i</mi><mrow id="S2.T1.8.8.8.2.m1.1.1.3" xref="S2.T1.8.8.8.2.m1.1.1.3.cmml"><mi id="S2.T1.8.8.8.2.m1.1.1.3.2" xref="S2.T1.8.8.8.2.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T1.8.8.8.2.m1.1.1.3.1" xref="S2.T1.8.8.8.2.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.8.8.8.2.m1.1.1.3.3" xref="S2.T1.8.8.8.2.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.2.m1.1b"><apply id="S2.T1.8.8.8.2.m1.1.1.cmml" xref="S2.T1.8.8.8.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.8.8.8.2.m1.1.1.1.cmml" xref="S2.T1.8.8.8.2.m1.1.1">superscript</csymbol><ci id="S2.T1.8.8.8.2.m1.1.1.2.cmml" xref="S2.T1.8.8.8.2.m1.1.1.2">𝑖</ci><apply id="S2.T1.8.8.8.2.m1.1.1.3.cmml" xref="S2.T1.8.8.8.2.m1.1.1.3"><times id="S2.T1.8.8.8.2.m1.1.1.3.1.cmml" xref="S2.T1.8.8.8.2.m1.1.1.3.1"></times><ci id="S2.T1.8.8.8.2.m1.1.1.3.2.cmml" xref="S2.T1.8.8.8.2.m1.1.1.3.2">𝑡</ci><ci id="S2.T1.8.8.8.2.m1.1.1.3.3.cmml" xref="S2.T1.8.8.8.2.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.2.m1.1c">i^{th}</annotation></semantics></math></td>
<td id="S2.T1.8.8.8.4" class="ltx_td ltx_align_center">Each round of training</td>
</tr>
<tr id="S2.T1.12.12.12" class="ltx_tr">
<td id="S2.T1.9.9.9.1" class="ltx_td ltx_align_center"><math id="S2.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="\Omega" display="inline"><semantics id="S2.T1.9.9.9.1.m1.1a"><mi mathvariant="normal" id="S2.T1.9.9.9.1.m1.1.1" xref="S2.T1.9.9.9.1.m1.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.1.m1.1b"><ci id="S2.T1.9.9.9.1.m1.1.1.cmml" xref="S2.T1.9.9.9.1.m1.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.1.m1.1c">\Omega</annotation></semantics></math></td>
<td id="S2.T1.12.12.12.5" class="ltx_td ltx_align_center">Every set of outcomes</td>
<td id="S2.T1.10.10.10.2" class="ltx_td ltx_align_center"><math id="S2.T1.10.10.10.2.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.T1.10.10.10.2.m1.2a"><mrow id="S2.T1.10.10.10.2.m1.2.3.2" xref="S2.T1.10.10.10.2.m1.2.3.1.cmml"><mo stretchy="false" id="S2.T1.10.10.10.2.m1.2.3.2.1" xref="S2.T1.10.10.10.2.m1.2.3.1.cmml">(</mo><mi id="S2.T1.10.10.10.2.m1.1.1" xref="S2.T1.10.10.10.2.m1.1.1.cmml">x</mi><mo id="S2.T1.10.10.10.2.m1.2.3.2.2" xref="S2.T1.10.10.10.2.m1.2.3.1.cmml">,</mo><mi id="S2.T1.10.10.10.2.m1.2.2" xref="S2.T1.10.10.10.2.m1.2.2.cmml">y</mi><mo stretchy="false" id="S2.T1.10.10.10.2.m1.2.3.2.3" xref="S2.T1.10.10.10.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.2.m1.2b"><interval closure="open" id="S2.T1.10.10.10.2.m1.2.3.1.cmml" xref="S2.T1.10.10.10.2.m1.2.3.2"><ci id="S2.T1.10.10.10.2.m1.1.1.cmml" xref="S2.T1.10.10.10.2.m1.1.1">𝑥</ci><ci id="S2.T1.10.10.10.2.m1.2.2.cmml" xref="S2.T1.10.10.10.2.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.2.m1.2c">(x,y)</annotation></semantics></math></td>
<td id="S2.T1.12.12.12.4" class="ltx_td ltx_align_center">
<math id="S2.T1.11.11.11.3.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.T1.11.11.11.3.m1.1a"><mi id="S2.T1.11.11.11.3.m1.1.1" xref="S2.T1.11.11.11.3.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.3.m1.1b"><ci id="S2.T1.11.11.11.3.m1.1.1.cmml" xref="S2.T1.11.11.11.3.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.3.m1.1c">x</annotation></semantics></math> and <math id="S2.T1.12.12.12.4.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.T1.12.12.12.4.m2.1a"><mi id="S2.T1.12.12.12.4.m2.1.1" xref="S2.T1.12.12.12.4.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.4.m2.1b"><ci id="S2.T1.12.12.12.4.m2.1.1.cmml" xref="S2.T1.12.12.12.4.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.4.m2.1c">y</annotation></semantics></math> are the input and label of the data</td>
</tr>
<tr id="S2.T1.14.14.14" class="ltx_tr">
<td id="S2.T1.13.13.13.1" class="ltx_td ltx_align_center"><math id="S2.T1.13.13.13.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.T1.13.13.13.1.m1.1a"><mi id="S2.T1.13.13.13.1.m1.1.1" xref="S2.T1.13.13.13.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.13.1.m1.1b"><ci id="S2.T1.13.13.13.1.m1.1.1.cmml" xref="S2.T1.13.13.13.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.13.1.m1.1c">H</annotation></semantics></math></td>
<td id="S2.T1.14.14.14.3" class="ltx_td ltx_align_center">Homomorphic encryption method</td>
<td id="S2.T1.14.14.14.2" class="ltx_td ltx_align_center"><math id="S2.T1.14.14.14.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{W_{i}}" display="inline"><semantics id="S2.T1.14.14.14.2.m1.1a"><mrow id="S2.T1.14.14.14.2.m1.1.1" xref="S2.T1.14.14.14.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.14.14.14.2.m1.1.1a" xref="S2.T1.14.14.14.2.m1.1.1.cmml">▽</mo><msub id="S2.T1.14.14.14.2.m1.1.1.2" xref="S2.T1.14.14.14.2.m1.1.1.2.cmml"><mi id="S2.T1.14.14.14.2.m1.1.1.2.2" xref="S2.T1.14.14.14.2.m1.1.1.2.2.cmml">W</mi><mi id="S2.T1.14.14.14.2.m1.1.1.2.3" xref="S2.T1.14.14.14.2.m1.1.1.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.14.2.m1.1b"><apply id="S2.T1.14.14.14.2.m1.1.1.cmml" xref="S2.T1.14.14.14.2.m1.1.1"><ci id="S2.T1.14.14.14.2.m1.1.1.1.cmml" xref="S2.T1.14.14.14.2.m1.1.1">▽</ci><apply id="S2.T1.14.14.14.2.m1.1.1.2.cmml" xref="S2.T1.14.14.14.2.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T1.14.14.14.2.m1.1.1.2.1.cmml" xref="S2.T1.14.14.14.2.m1.1.1.2">subscript</csymbol><ci id="S2.T1.14.14.14.2.m1.1.1.2.2.cmml" xref="S2.T1.14.14.14.2.m1.1.1.2.2">𝑊</ci><ci id="S2.T1.14.14.14.2.m1.1.1.2.3.cmml" xref="S2.T1.14.14.14.2.m1.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.14.2.m1.1c">\bigtriangledown{W_{i}}</annotation></semantics></math></td>
<td id="S2.T1.14.14.14.4" class="ltx_td ltx_align_center">The gradient of the data</td>
</tr>
<tr id="S2.T1.18.18.18" class="ltx_tr">
<td id="S2.T1.15.15.15.1" class="ltx_td ltx_align_center"><math id="S2.T1.15.15.15.1.m1.1" class="ltx_Math" alttext="KeyGen" display="inline"><semantics id="S2.T1.15.15.15.1.m1.1a"><mrow id="S2.T1.15.15.15.1.m1.1.1" xref="S2.T1.15.15.15.1.m1.1.1.cmml"><mi id="S2.T1.15.15.15.1.m1.1.1.2" xref="S2.T1.15.15.15.1.m1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S2.T1.15.15.15.1.m1.1.1.1" xref="S2.T1.15.15.15.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.15.15.15.1.m1.1.1.3" xref="S2.T1.15.15.15.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T1.15.15.15.1.m1.1.1.1a" xref="S2.T1.15.15.15.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.15.15.15.1.m1.1.1.4" xref="S2.T1.15.15.15.1.m1.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.T1.15.15.15.1.m1.1.1.1b" xref="S2.T1.15.15.15.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.15.15.15.1.m1.1.1.5" xref="S2.T1.15.15.15.1.m1.1.1.5.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.T1.15.15.15.1.m1.1.1.1c" xref="S2.T1.15.15.15.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.15.15.15.1.m1.1.1.6" xref="S2.T1.15.15.15.1.m1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T1.15.15.15.1.m1.1.1.1d" xref="S2.T1.15.15.15.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.15.15.15.1.m1.1.1.7" xref="S2.T1.15.15.15.1.m1.1.1.7.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.15.1.m1.1b"><apply id="S2.T1.15.15.15.1.m1.1.1.cmml" xref="S2.T1.15.15.15.1.m1.1.1"><times id="S2.T1.15.15.15.1.m1.1.1.1.cmml" xref="S2.T1.15.15.15.1.m1.1.1.1"></times><ci id="S2.T1.15.15.15.1.m1.1.1.2.cmml" xref="S2.T1.15.15.15.1.m1.1.1.2">𝐾</ci><ci id="S2.T1.15.15.15.1.m1.1.1.3.cmml" xref="S2.T1.15.15.15.1.m1.1.1.3">𝑒</ci><ci id="S2.T1.15.15.15.1.m1.1.1.4.cmml" xref="S2.T1.15.15.15.1.m1.1.1.4">𝑦</ci><ci id="S2.T1.15.15.15.1.m1.1.1.5.cmml" xref="S2.T1.15.15.15.1.m1.1.1.5">𝐺</ci><ci id="S2.T1.15.15.15.1.m1.1.1.6.cmml" xref="S2.T1.15.15.15.1.m1.1.1.6">𝑒</ci><ci id="S2.T1.15.15.15.1.m1.1.1.7.cmml" xref="S2.T1.15.15.15.1.m1.1.1.7">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.15.1.m1.1c">KeyGen</annotation></semantics></math></td>
<td id="S2.T1.18.18.18.5" class="ltx_td ltx_align_center">The key generation function</td>
<td id="S2.T1.16.16.16.2" class="ltx_td ltx_align_center"><math id="S2.T1.16.16.16.2.m1.2" class="ltx_Math" alttext="(x^{\prime},y^{\prime})" display="inline"><semantics id="S2.T1.16.16.16.2.m1.2a"><mrow id="S2.T1.16.16.16.2.m1.2.2.2" xref="S2.T1.16.16.16.2.m1.2.2.3.cmml"><mo stretchy="false" id="S2.T1.16.16.16.2.m1.2.2.2.3" xref="S2.T1.16.16.16.2.m1.2.2.3.cmml">(</mo><msup id="S2.T1.16.16.16.2.m1.1.1.1.1" xref="S2.T1.16.16.16.2.m1.1.1.1.1.cmml"><mi id="S2.T1.16.16.16.2.m1.1.1.1.1.2" xref="S2.T1.16.16.16.2.m1.1.1.1.1.2.cmml">x</mi><mo id="S2.T1.16.16.16.2.m1.1.1.1.1.3" xref="S2.T1.16.16.16.2.m1.1.1.1.1.3.cmml">′</mo></msup><mo id="S2.T1.16.16.16.2.m1.2.2.2.4" xref="S2.T1.16.16.16.2.m1.2.2.3.cmml">,</mo><msup id="S2.T1.16.16.16.2.m1.2.2.2.2" xref="S2.T1.16.16.16.2.m1.2.2.2.2.cmml"><mi id="S2.T1.16.16.16.2.m1.2.2.2.2.2" xref="S2.T1.16.16.16.2.m1.2.2.2.2.2.cmml">y</mi><mo id="S2.T1.16.16.16.2.m1.2.2.2.2.3" xref="S2.T1.16.16.16.2.m1.2.2.2.2.3.cmml">′</mo></msup><mo stretchy="false" id="S2.T1.16.16.16.2.m1.2.2.2.5" xref="S2.T1.16.16.16.2.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.16.2.m1.2b"><interval closure="open" id="S2.T1.16.16.16.2.m1.2.2.3.cmml" xref="S2.T1.16.16.16.2.m1.2.2.2"><apply id="S2.T1.16.16.16.2.m1.1.1.1.1.cmml" xref="S2.T1.16.16.16.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T1.16.16.16.2.m1.1.1.1.1.1.cmml" xref="S2.T1.16.16.16.2.m1.1.1.1.1">superscript</csymbol><ci id="S2.T1.16.16.16.2.m1.1.1.1.1.2.cmml" xref="S2.T1.16.16.16.2.m1.1.1.1.1.2">𝑥</ci><ci id="S2.T1.16.16.16.2.m1.1.1.1.1.3.cmml" xref="S2.T1.16.16.16.2.m1.1.1.1.1.3">′</ci></apply><apply id="S2.T1.16.16.16.2.m1.2.2.2.2.cmml" xref="S2.T1.16.16.16.2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.T1.16.16.16.2.m1.2.2.2.2.1.cmml" xref="S2.T1.16.16.16.2.m1.2.2.2.2">superscript</csymbol><ci id="S2.T1.16.16.16.2.m1.2.2.2.2.2.cmml" xref="S2.T1.16.16.16.2.m1.2.2.2.2.2">𝑦</ci><ci id="S2.T1.16.16.16.2.m1.2.2.2.2.3.cmml" xref="S2.T1.16.16.16.2.m1.2.2.2.2.3">′</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.16.2.m1.2c">(x^{\prime},y^{\prime})</annotation></semantics></math></td>
<td id="S2.T1.18.18.18.4" class="ltx_td ltx_align_center">
<math id="S2.T1.17.17.17.3.m1.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S2.T1.17.17.17.3.m1.1a"><msup id="S2.T1.17.17.17.3.m1.1.1" xref="S2.T1.17.17.17.3.m1.1.1.cmml"><mi id="S2.T1.17.17.17.3.m1.1.1.2" xref="S2.T1.17.17.17.3.m1.1.1.2.cmml">x</mi><mo id="S2.T1.17.17.17.3.m1.1.1.3" xref="S2.T1.17.17.17.3.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.17.3.m1.1b"><apply id="S2.T1.17.17.17.3.m1.1.1.cmml" xref="S2.T1.17.17.17.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.17.17.17.3.m1.1.1.1.cmml" xref="S2.T1.17.17.17.3.m1.1.1">superscript</csymbol><ci id="S2.T1.17.17.17.3.m1.1.1.2.cmml" xref="S2.T1.17.17.17.3.m1.1.1.2">𝑥</ci><ci id="S2.T1.17.17.17.3.m1.1.1.3.cmml" xref="S2.T1.17.17.17.3.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.17.3.m1.1c">x^{\prime}</annotation></semantics></math> and <math id="S2.T1.18.18.18.4.m2.1" class="ltx_Math" alttext="y^{\prime}" display="inline"><semantics id="S2.T1.18.18.18.4.m2.1a"><msup id="S2.T1.18.18.18.4.m2.1.1" xref="S2.T1.18.18.18.4.m2.1.1.cmml"><mi id="S2.T1.18.18.18.4.m2.1.1.2" xref="S2.T1.18.18.18.4.m2.1.1.2.cmml">y</mi><mo id="S2.T1.18.18.18.4.m2.1.1.3" xref="S2.T1.18.18.18.4.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.18.4.m2.1b"><apply id="S2.T1.18.18.18.4.m2.1.1.cmml" xref="S2.T1.18.18.18.4.m2.1.1"><csymbol cd="ambiguous" id="S2.T1.18.18.18.4.m2.1.1.1.cmml" xref="S2.T1.18.18.18.4.m2.1.1">superscript</csymbol><ci id="S2.T1.18.18.18.4.m2.1.1.2.cmml" xref="S2.T1.18.18.18.4.m2.1.1.2">𝑦</ci><ci id="S2.T1.18.18.18.4.m2.1.1.3.cmml" xref="S2.T1.18.18.18.4.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.18.4.m2.1c">y^{\prime}</annotation></semantics></math> are the input and label of the dummy data</td>
</tr>
<tr id="S2.T1.20.20.20" class="ltx_tr">
<td id="S2.T1.19.19.19.1" class="ltx_td ltx_align_center"><math id="S2.T1.19.19.19.1.m1.1" class="ltx_Math" alttext="Enc" display="inline"><semantics id="S2.T1.19.19.19.1.m1.1a"><mrow id="S2.T1.19.19.19.1.m1.1.1" xref="S2.T1.19.19.19.1.m1.1.1.cmml"><mi id="S2.T1.19.19.19.1.m1.1.1.2" xref="S2.T1.19.19.19.1.m1.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.T1.19.19.19.1.m1.1.1.1" xref="S2.T1.19.19.19.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.19.19.19.1.m1.1.1.3" xref="S2.T1.19.19.19.1.m1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T1.19.19.19.1.m1.1.1.1a" xref="S2.T1.19.19.19.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.19.19.19.1.m1.1.1.4" xref="S2.T1.19.19.19.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.19.19.19.1.m1.1b"><apply id="S2.T1.19.19.19.1.m1.1.1.cmml" xref="S2.T1.19.19.19.1.m1.1.1"><times id="S2.T1.19.19.19.1.m1.1.1.1.cmml" xref="S2.T1.19.19.19.1.m1.1.1.1"></times><ci id="S2.T1.19.19.19.1.m1.1.1.2.cmml" xref="S2.T1.19.19.19.1.m1.1.1.2">𝐸</ci><ci id="S2.T1.19.19.19.1.m1.1.1.3.cmml" xref="S2.T1.19.19.19.1.m1.1.1.3">𝑛</ci><ci id="S2.T1.19.19.19.1.m1.1.1.4.cmml" xref="S2.T1.19.19.19.1.m1.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.19.19.1.m1.1c">Enc</annotation></semantics></math></td>
<td id="S2.T1.20.20.20.3" class="ltx_td ltx_align_center">The encryption function for asymmetric encryption</td>
<td id="S2.T1.20.20.20.2" class="ltx_td ltx_align_center"><math id="S2.T1.20.20.20.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{W^{i}}" display="inline"><semantics id="S2.T1.20.20.20.2.m1.1a"><mrow id="S2.T1.20.20.20.2.m1.1.1" xref="S2.T1.20.20.20.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.20.20.20.2.m1.1.1a" xref="S2.T1.20.20.20.2.m1.1.1.cmml">▽</mo><msup id="S2.T1.20.20.20.2.m1.1.1.2" xref="S2.T1.20.20.20.2.m1.1.1.2.cmml"><mi id="S2.T1.20.20.20.2.m1.1.1.2.2" xref="S2.T1.20.20.20.2.m1.1.1.2.2.cmml">W</mi><mi id="S2.T1.20.20.20.2.m1.1.1.2.3" xref="S2.T1.20.20.20.2.m1.1.1.2.3.cmml">i</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.20.20.20.2.m1.1b"><apply id="S2.T1.20.20.20.2.m1.1.1.cmml" xref="S2.T1.20.20.20.2.m1.1.1"><ci id="S2.T1.20.20.20.2.m1.1.1.1.cmml" xref="S2.T1.20.20.20.2.m1.1.1">▽</ci><apply id="S2.T1.20.20.20.2.m1.1.1.2.cmml" xref="S2.T1.20.20.20.2.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T1.20.20.20.2.m1.1.1.2.1.cmml" xref="S2.T1.20.20.20.2.m1.1.1.2">superscript</csymbol><ci id="S2.T1.20.20.20.2.m1.1.1.2.2.cmml" xref="S2.T1.20.20.20.2.m1.1.1.2.2">𝑊</ci><ci id="S2.T1.20.20.20.2.m1.1.1.2.3.cmml" xref="S2.T1.20.20.20.2.m1.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.20.20.2.m1.1c">\bigtriangledown{W^{i}}</annotation></semantics></math></td>
<td id="S2.T1.20.20.20.4" class="ltx_td ltx_align_center">The gradient of the dummy data</td>
</tr>
<tr id="S2.T1.22.22.22" class="ltx_tr">
<td id="S2.T1.21.21.21.1" class="ltx_td ltx_align_center"><math id="S2.T1.21.21.21.1.m1.1" class="ltx_Math" alttext="Dec" display="inline"><semantics id="S2.T1.21.21.21.1.m1.1a"><mrow id="S2.T1.21.21.21.1.m1.1.1" xref="S2.T1.21.21.21.1.m1.1.1.cmml"><mi id="S2.T1.21.21.21.1.m1.1.1.2" xref="S2.T1.21.21.21.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.T1.21.21.21.1.m1.1.1.1" xref="S2.T1.21.21.21.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.21.21.21.1.m1.1.1.3" xref="S2.T1.21.21.21.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T1.21.21.21.1.m1.1.1.1a" xref="S2.T1.21.21.21.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.21.21.21.1.m1.1.1.4" xref="S2.T1.21.21.21.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.21.21.21.1.m1.1b"><apply id="S2.T1.21.21.21.1.m1.1.1.cmml" xref="S2.T1.21.21.21.1.m1.1.1"><times id="S2.T1.21.21.21.1.m1.1.1.1.cmml" xref="S2.T1.21.21.21.1.m1.1.1.1"></times><ci id="S2.T1.21.21.21.1.m1.1.1.2.cmml" xref="S2.T1.21.21.21.1.m1.1.1.2">𝐷</ci><ci id="S2.T1.21.21.21.1.m1.1.1.3.cmml" xref="S2.T1.21.21.21.1.m1.1.1.3">𝑒</ci><ci id="S2.T1.21.21.21.1.m1.1.1.4.cmml" xref="S2.T1.21.21.21.1.m1.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.21.21.21.1.m1.1c">Dec</annotation></semantics></math></td>
<td id="S2.T1.22.22.22.3" class="ltx_td ltx_align_center">The encryption function for symmetric encryption</td>
<td id="S2.T1.22.22.22.2" class="ltx_td ltx_align_center"><math id="S2.T1.22.22.22.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{W^{G}}" display="inline"><semantics id="S2.T1.22.22.22.2.m1.1a"><mrow id="S2.T1.22.22.22.2.m1.1.1" xref="S2.T1.22.22.22.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.22.22.22.2.m1.1.1a" xref="S2.T1.22.22.22.2.m1.1.1.cmml">▽</mo><msup id="S2.T1.22.22.22.2.m1.1.1.2" xref="S2.T1.22.22.22.2.m1.1.1.2.cmml"><mi id="S2.T1.22.22.22.2.m1.1.1.2.2" xref="S2.T1.22.22.22.2.m1.1.1.2.2.cmml">W</mi><mi id="S2.T1.22.22.22.2.m1.1.1.2.3" xref="S2.T1.22.22.22.2.m1.1.1.2.3.cmml">G</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.22.22.22.2.m1.1b"><apply id="S2.T1.22.22.22.2.m1.1.1.cmml" xref="S2.T1.22.22.22.2.m1.1.1"><ci id="S2.T1.22.22.22.2.m1.1.1.1.cmml" xref="S2.T1.22.22.22.2.m1.1.1">▽</ci><apply id="S2.T1.22.22.22.2.m1.1.1.2.cmml" xref="S2.T1.22.22.22.2.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T1.22.22.22.2.m1.1.1.2.1.cmml" xref="S2.T1.22.22.22.2.m1.1.1.2">superscript</csymbol><ci id="S2.T1.22.22.22.2.m1.1.1.2.2.cmml" xref="S2.T1.22.22.22.2.m1.1.1.2.2">𝑊</ci><ci id="S2.T1.22.22.22.2.m1.1.1.2.3.cmml" xref="S2.T1.22.22.22.2.m1.1.1.2.3">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.22.22.22.2.m1.1c">\bigtriangledown{W^{G}}</annotation></semantics></math></td>
<td id="S2.T1.22.22.22.4" class="ltx_td ltx_align_center">The difference between the dummy gradient and the shared gradient</td>
</tr>
<tr id="S2.T1.24.24.24" class="ltx_tr">
<td id="S2.T1.23.23.23.1" class="ltx_td ltx_align_center"><math id="S2.T1.23.23.23.1.m1.1" class="ltx_Math" alttext="Eval" display="inline"><semantics id="S2.T1.23.23.23.1.m1.1a"><mrow id="S2.T1.23.23.23.1.m1.1.1" xref="S2.T1.23.23.23.1.m1.1.1.cmml"><mi id="S2.T1.23.23.23.1.m1.1.1.2" xref="S2.T1.23.23.23.1.m1.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.T1.23.23.23.1.m1.1.1.1" xref="S2.T1.23.23.23.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.23.23.23.1.m1.1.1.3" xref="S2.T1.23.23.23.1.m1.1.1.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.T1.23.23.23.1.m1.1.1.1a" xref="S2.T1.23.23.23.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.23.23.23.1.m1.1.1.4" xref="S2.T1.23.23.23.1.m1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T1.23.23.23.1.m1.1.1.1b" xref="S2.T1.23.23.23.1.m1.1.1.1.cmml">​</mo><mi id="S2.T1.23.23.23.1.m1.1.1.5" xref="S2.T1.23.23.23.1.m1.1.1.5.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.23.23.23.1.m1.1b"><apply id="S2.T1.23.23.23.1.m1.1.1.cmml" xref="S2.T1.23.23.23.1.m1.1.1"><times id="S2.T1.23.23.23.1.m1.1.1.1.cmml" xref="S2.T1.23.23.23.1.m1.1.1.1"></times><ci id="S2.T1.23.23.23.1.m1.1.1.2.cmml" xref="S2.T1.23.23.23.1.m1.1.1.2">𝐸</ci><ci id="S2.T1.23.23.23.1.m1.1.1.3.cmml" xref="S2.T1.23.23.23.1.m1.1.1.3">𝑣</ci><ci id="S2.T1.23.23.23.1.m1.1.1.4.cmml" xref="S2.T1.23.23.23.1.m1.1.1.4">𝑎</ci><ci id="S2.T1.23.23.23.1.m1.1.1.5.cmml" xref="S2.T1.23.23.23.1.m1.1.1.5">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.23.23.23.1.m1.1c">Eval</annotation></semantics></math></td>
<td id="S2.T1.24.24.24.3" class="ltx_td ltx_align_center">The evaluation function</td>
<td id="S2.T1.24.24.24.2" class="ltx_td ltx_align_center"><math id="S2.T1.24.24.24.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{m}" display="inline"><semantics id="S2.T1.24.24.24.2.m1.1a"><mrow id="S2.T1.24.24.24.2.m1.1.1" xref="S2.T1.24.24.24.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.24.24.24.2.m1.1.1a" xref="S2.T1.24.24.24.2.m1.1.1.cmml">▽</mo><mi id="S2.T1.24.24.24.2.m1.1.1.2" xref="S2.T1.24.24.24.2.m1.1.1.2.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.24.24.24.2.m1.1b"><apply id="S2.T1.24.24.24.2.m1.1.1.cmml" xref="S2.T1.24.24.24.2.m1.1.1"><ci id="S2.T1.24.24.24.2.m1.1.1.1.cmml" xref="S2.T1.24.24.24.2.m1.1.1">▽</ci><ci id="S2.T1.24.24.24.2.m1.1.1.2.cmml" xref="S2.T1.24.24.24.2.m1.1.1.2">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.24.24.24.2.m1.1c">\bigtriangledown{m}</annotation></semantics></math></td>
<td id="S2.T1.24.24.24.4" class="ltx_td ltx_align_center">The malicious global model</td>
</tr>
<tr id="S2.T1.26.26.26" class="ltx_tr">
<td id="S2.T1.25.25.25.1" class="ltx_td ltx_align_center"><math id="S2.T1.25.25.25.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.T1.25.25.25.1.m1.1a"><mi id="S2.T1.25.25.25.1.m1.1.1" xref="S2.T1.25.25.25.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.T1.25.25.25.1.m1.1b"><ci id="S2.T1.25.25.25.1.m1.1.1.cmml" xref="S2.T1.25.25.25.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.25.25.25.1.m1.1c">f</annotation></semantics></math></td>
<td id="S2.T1.26.26.26.3" class="ltx_td ltx_align_center">The query function</td>
<td id="S2.T1.26.26.26.2" class="ltx_td ltx_align_center"><math id="S2.T1.26.26.26.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{b}" display="inline"><semantics id="S2.T1.26.26.26.2.m1.1a"><mrow id="S2.T1.26.26.26.2.m1.1.1" xref="S2.T1.26.26.26.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.26.26.26.2.m1.1.1a" xref="S2.T1.26.26.26.2.m1.1.1.cmml">▽</mo><mi id="S2.T1.26.26.26.2.m1.1.1.2" xref="S2.T1.26.26.26.2.m1.1.1.2.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.26.26.26.2.m1.1b"><apply id="S2.T1.26.26.26.2.m1.1.1.cmml" xref="S2.T1.26.26.26.2.m1.1.1"><ci id="S2.T1.26.26.26.2.m1.1.1.1.cmml" xref="S2.T1.26.26.26.2.m1.1.1">▽</ci><ci id="S2.T1.26.26.26.2.m1.1.1.2.cmml" xref="S2.T1.26.26.26.2.m1.1.1.2">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.26.26.26.2.m1.1c">\bigtriangledown{b}</annotation></semantics></math></td>
<td id="S2.T1.26.26.26.4" class="ltx_td ltx_align_center">The benign global model</td>
</tr>
<tr id="S2.T1.28.28.28" class="ltx_tr">
<td id="S2.T1.27.27.27.1" class="ltx_td ltx_align_center"><math id="S2.T1.27.27.27.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.T1.27.27.27.1.m1.1a"><mi id="S2.T1.27.27.27.1.m1.1.1" xref="S2.T1.27.27.27.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.T1.27.27.27.1.m1.1b"><ci id="S2.T1.27.27.27.1.m1.1.1.cmml" xref="S2.T1.27.27.27.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.27.27.27.1.m1.1c">x</annotation></semantics></math></td>
<td id="S2.T1.28.28.28.3" class="ltx_td ltx_align_center">The private value</td>
<td id="S2.T1.28.28.28.2" class="ltx_td ltx_align_center"><math id="S2.T1.28.28.28.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{p}" display="inline"><semantics id="S2.T1.28.28.28.2.m1.1a"><mrow id="S2.T1.28.28.28.2.m1.1.1" xref="S2.T1.28.28.28.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.28.28.28.2.m1.1.1a" xref="S2.T1.28.28.28.2.m1.1.1.cmml">▽</mo><mi id="S2.T1.28.28.28.2.m1.1.1.2" xref="S2.T1.28.28.28.2.m1.1.1.2.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.28.28.28.2.m1.1b"><apply id="S2.T1.28.28.28.2.m1.1.1.cmml" xref="S2.T1.28.28.28.2.m1.1.1"><ci id="S2.T1.28.28.28.2.m1.1.1.1.cmml" xref="S2.T1.28.28.28.2.m1.1.1">▽</ci><ci id="S2.T1.28.28.28.2.m1.1.1.2.cmml" xref="S2.T1.28.28.28.2.m1.1.1.2">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.28.28.28.2.m1.1c">\bigtriangledown{p}</annotation></semantics></math></td>
<td id="S2.T1.28.28.28.4" class="ltx_td ltx_align_center">The model is updated after the final poisoning</td>
</tr>
<tr id="S2.T1.30.30.30" class="ltx_tr">
<td id="S2.T1.29.29.29.1" class="ltx_td ltx_align_center"><math id="S2.T1.29.29.29.1.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S2.T1.29.29.29.1.m1.1a"><msub id="S2.T1.29.29.29.1.m1.1.1" xref="S2.T1.29.29.29.1.m1.1.1.cmml"><mi id="S2.T1.29.29.29.1.m1.1.1.2" xref="S2.T1.29.29.29.1.m1.1.1.2.cmml">P</mi><mi id="S2.T1.29.29.29.1.m1.1.1.3" xref="S2.T1.29.29.29.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.29.29.29.1.m1.1b"><apply id="S2.T1.29.29.29.1.m1.1.1.cmml" xref="S2.T1.29.29.29.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.29.29.29.1.m1.1.1.1.cmml" xref="S2.T1.29.29.29.1.m1.1.1">subscript</csymbol><ci id="S2.T1.29.29.29.1.m1.1.1.2.cmml" xref="S2.T1.29.29.29.1.m1.1.1.2">𝑃</ci><ci id="S2.T1.29.29.29.1.m1.1.1.3.cmml" xref="S2.T1.29.29.29.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.29.29.29.1.m1.1c">P_{i}</annotation></semantics></math></td>
<td id="S2.T1.30.30.30.3" class="ltx_td ltx_align_center">The party</td>
<td id="S2.T1.30.30.30.2" class="ltx_td ltx_align_center"><math id="S2.T1.30.30.30.2.m1.1" class="ltx_Math" alttext="c_{s}" display="inline"><semantics id="S2.T1.30.30.30.2.m1.1a"><msub id="S2.T1.30.30.30.2.m1.1.1" xref="S2.T1.30.30.30.2.m1.1.1.cmml"><mi id="S2.T1.30.30.30.2.m1.1.1.2" xref="S2.T1.30.30.30.2.m1.1.1.2.cmml">c</mi><mi id="S2.T1.30.30.30.2.m1.1.1.3" xref="S2.T1.30.30.30.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.30.30.30.2.m1.1b"><apply id="S2.T1.30.30.30.2.m1.1.1.cmml" xref="S2.T1.30.30.30.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.30.30.30.2.m1.1.1.1.cmml" xref="S2.T1.30.30.30.2.m1.1.1">subscript</csymbol><ci id="S2.T1.30.30.30.2.m1.1.1.2.cmml" xref="S2.T1.30.30.30.2.m1.1.1.2">𝑐</ci><ci id="S2.T1.30.30.30.2.m1.1.1.3.cmml" xref="S2.T1.30.30.30.2.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.30.30.30.2.m1.1c">c_{s}</annotation></semantics></math></td>
<td id="S2.T1.30.30.30.4" class="ltx_td ltx_align_center">The source class</td>
</tr>
<tr id="S2.T1.32.32.32" class="ltx_tr">
<td id="S2.T1.31.31.31.1" class="ltx_td ltx_align_center"><math id="S2.T1.31.31.31.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.T1.31.31.31.1.m1.1a"><mi id="S2.T1.31.31.31.1.m1.1.1" xref="S2.T1.31.31.31.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.T1.31.31.31.1.m1.1b"><ci id="S2.T1.31.31.31.1.m1.1.1.cmml" xref="S2.T1.31.31.31.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.31.31.31.1.m1.1c">n</annotation></semantics></math></td>
<td id="S2.T1.32.32.32.3" class="ltx_td ltx_align_center">The number of parties</td>
<td id="S2.T1.32.32.32.2" class="ltx_td ltx_align_center"><math id="S2.T1.32.32.32.2.m1.1" class="ltx_Math" alttext="c_{m}" display="inline"><semantics id="S2.T1.32.32.32.2.m1.1a"><msub id="S2.T1.32.32.32.2.m1.1.1" xref="S2.T1.32.32.32.2.m1.1.1.cmml"><mi id="S2.T1.32.32.32.2.m1.1.1.2" xref="S2.T1.32.32.32.2.m1.1.1.2.cmml">c</mi><mi id="S2.T1.32.32.32.2.m1.1.1.3" xref="S2.T1.32.32.32.2.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.32.32.32.2.m1.1b"><apply id="S2.T1.32.32.32.2.m1.1.1.cmml" xref="S2.T1.32.32.32.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.32.32.32.2.m1.1.1.1.cmml" xref="S2.T1.32.32.32.2.m1.1.1">subscript</csymbol><ci id="S2.T1.32.32.32.2.m1.1.1.2.cmml" xref="S2.T1.32.32.32.2.m1.1.1.2">𝑐</ci><ci id="S2.T1.32.32.32.2.m1.1.1.3.cmml" xref="S2.T1.32.32.32.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.32.32.32.2.m1.1c">c_{m}</annotation></semantics></math></td>
<td id="S2.T1.32.32.32.4" class="ltx_td ltx_align_center">The modified class</td>
</tr>
<tr id="S2.T1.34.34.34" class="ltx_tr">
<td id="S2.T1.33.33.33.1" class="ltx_td ltx_align_center"><math id="S2.T1.33.33.33.1.m1.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.T1.33.33.33.1.m1.1a"><msub id="S2.T1.33.33.33.1.m1.1.1" xref="S2.T1.33.33.33.1.m1.1.1.cmml"><mi id="S2.T1.33.33.33.1.m1.1.1.2" xref="S2.T1.33.33.33.1.m1.1.1.2.cmml">y</mi><mi id="S2.T1.33.33.33.1.m1.1.1.3" xref="S2.T1.33.33.33.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.33.33.33.1.m1.1b"><apply id="S2.T1.33.33.33.1.m1.1.1.cmml" xref="S2.T1.33.33.33.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.33.33.33.1.m1.1.1.1.cmml" xref="S2.T1.33.33.33.1.m1.1.1">subscript</csymbol><ci id="S2.T1.33.33.33.1.m1.1.1.2.cmml" xref="S2.T1.33.33.33.1.m1.1.1.2">𝑦</ci><ci id="S2.T1.33.33.33.1.m1.1.1.3.cmml" xref="S2.T1.33.33.33.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.33.33.33.1.m1.1c">y_{i}</annotation></semantics></math></td>
<td id="S2.T1.34.34.34.3" class="ltx_td ltx_align_center">The output value</td>
<td id="S2.T1.34.34.34.2" class="ltx_td ltx_align_center"><math id="S2.T1.34.34.34.2.m1.1" class="ltx_Math" alttext="\bigtriangledown{l}" display="inline"><semantics id="S2.T1.34.34.34.2.m1.1a"><mrow id="S2.T1.34.34.34.2.m1.1.1" xref="S2.T1.34.34.34.2.m1.1.1.cmml"><mo rspace="0em" id="S2.T1.34.34.34.2.m1.1.1a" xref="S2.T1.34.34.34.2.m1.1.1.cmml">▽</mo><mi id="S2.T1.34.34.34.2.m1.1.1.2" xref="S2.T1.34.34.34.2.m1.1.1.2.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.34.34.34.2.m1.1b"><apply id="S2.T1.34.34.34.2.m1.1.1.cmml" xref="S2.T1.34.34.34.2.m1.1.1"><ci id="S2.T1.34.34.34.2.m1.1.1.1.cmml" xref="S2.T1.34.34.34.2.m1.1.1">▽</ci><ci id="S2.T1.34.34.34.2.m1.1.1.2.cmml" xref="S2.T1.34.34.34.2.m1.1.1.2">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.34.34.34.2.m1.1c">\bigtriangledown{l}</annotation></semantics></math></td>
<td id="S2.T1.34.34.34.4" class="ltx_td ltx_align_center">The locally trained model</td>
</tr>
<tr id="S2.T1.36.36.36" class="ltx_tr">
<td id="S2.T1.35.35.35.1" class="ltx_td ltx_align_center"><math id="S2.T1.35.35.35.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.T1.35.35.35.1.m1.1a"><mi id="S2.T1.35.35.35.1.m1.1.1" xref="S2.T1.35.35.35.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.T1.35.35.35.1.m1.1b"><ci id="S2.T1.35.35.35.1.m1.1.1.cmml" xref="S2.T1.35.35.35.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.35.35.35.1.m1.1c">t</annotation></semantics></math></td>
<td id="S2.T1.36.36.36.3" class="ltx_td ltx_align_center">The number of interaction sequences</td>
<td id="S2.T1.36.36.36.2" class="ltx_td ltx_align_center"><math id="S2.T1.36.36.36.2.m1.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S2.T1.36.36.36.2.m1.1a"><msub id="S2.T1.36.36.36.2.m1.1.1" xref="S2.T1.36.36.36.2.m1.1.1.cmml"><mi id="S2.T1.36.36.36.2.m1.1.1.2" xref="S2.T1.36.36.36.2.m1.1.1.2.cmml">G</mi><mi id="S2.T1.36.36.36.2.m1.1.1.3" xref="S2.T1.36.36.36.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.36.36.36.2.m1.1b"><apply id="S2.T1.36.36.36.2.m1.1.1.cmml" xref="S2.T1.36.36.36.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.36.36.36.2.m1.1.1.1.cmml" xref="S2.T1.36.36.36.2.m1.1.1">subscript</csymbol><ci id="S2.T1.36.36.36.2.m1.1.1.2.cmml" xref="S2.T1.36.36.36.2.m1.1.1.2">𝐺</ci><ci id="S2.T1.36.36.36.2.m1.1.1.3.cmml" xref="S2.T1.36.36.36.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.36.36.36.2.m1.1c">G_{i}</annotation></semantics></math></td>
<td id="S2.T1.36.36.36.4" class="ltx_td ltx_align_center">The global model</td>
</tr>
<tr id="S2.T1.38.38.38" class="ltx_tr">
<td id="S2.T1.37.37.37.1" class="ltx_td ltx_align_center"><math id="S2.T1.37.37.37.1.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.T1.37.37.37.1.m1.1a"><mi id="S2.T1.37.37.37.1.m1.1.1" xref="S2.T1.37.37.37.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.T1.37.37.37.1.m1.1b"><ci id="S2.T1.37.37.37.1.m1.1.1.cmml" xref="S2.T1.37.37.37.1.m1.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.37.37.37.1.m1.1c">W</annotation></semantics></math></td>
<td id="S2.T1.38.38.38.3" class="ltx_td ltx_align_center">A model</td>
<td id="S2.T1.38.38.38.2" class="ltx_td ltx_align_center"><math id="S2.T1.38.38.38.2.m1.1" class="ltx_Math" alttext="d_{c}" display="inline"><semantics id="S2.T1.38.38.38.2.m1.1a"><msub id="S2.T1.38.38.38.2.m1.1.1" xref="S2.T1.38.38.38.2.m1.1.1.cmml"><mi id="S2.T1.38.38.38.2.m1.1.1.2" xref="S2.T1.38.38.38.2.m1.1.1.2.cmml">d</mi><mi id="S2.T1.38.38.38.2.m1.1.1.3" xref="S2.T1.38.38.38.2.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.38.38.38.2.m1.1b"><apply id="S2.T1.38.38.38.2.m1.1.1.cmml" xref="S2.T1.38.38.38.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.38.38.38.2.m1.1.1.1.cmml" xref="S2.T1.38.38.38.2.m1.1.1">subscript</csymbol><ci id="S2.T1.38.38.38.2.m1.1.1.2.cmml" xref="S2.T1.38.38.38.2.m1.1.1.2">𝑑</ci><ci id="S2.T1.38.38.38.2.m1.1.1.3.cmml" xref="S2.T1.38.38.38.2.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.38.38.38.2.m1.1c">d_{c}</annotation></semantics></math></td>
<td id="S2.T1.38.38.38.4" class="ltx_td ltx_align_center">The small subgroup</td>
</tr>
<tr id="S2.T1.40.40.40" class="ltx_tr">
<td id="S2.T1.39.39.39.1" class="ltx_td ltx_align_center"><math id="S2.T1.39.39.39.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.T1.39.39.39.1.m1.1a"><mi id="S2.T1.39.39.39.1.m1.1.1" xref="S2.T1.39.39.39.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.T1.39.39.39.1.m1.1b"><ci id="S2.T1.39.39.39.1.m1.1.1.cmml" xref="S2.T1.39.39.39.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.39.39.39.1.m1.1c">C</annotation></semantics></math></td>
<td id="S2.T1.40.40.40.3" class="ltx_td ltx_align_center">A client</td>
<td id="S2.T1.40.40.40.2" class="ltx_td ltx_align_center"><math id="S2.T1.40.40.40.2.m1.1" class="ltx_Math" alttext="L^{t}_{i+1}" display="inline"><semantics id="S2.T1.40.40.40.2.m1.1a"><msubsup id="S2.T1.40.40.40.2.m1.1.1" xref="S2.T1.40.40.40.2.m1.1.1.cmml"><mi id="S2.T1.40.40.40.2.m1.1.1.2.2" xref="S2.T1.40.40.40.2.m1.1.1.2.2.cmml">L</mi><mrow id="S2.T1.40.40.40.2.m1.1.1.3" xref="S2.T1.40.40.40.2.m1.1.1.3.cmml"><mi id="S2.T1.40.40.40.2.m1.1.1.3.2" xref="S2.T1.40.40.40.2.m1.1.1.3.2.cmml">i</mi><mo id="S2.T1.40.40.40.2.m1.1.1.3.1" xref="S2.T1.40.40.40.2.m1.1.1.3.1.cmml">+</mo><mn id="S2.T1.40.40.40.2.m1.1.1.3.3" xref="S2.T1.40.40.40.2.m1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.T1.40.40.40.2.m1.1.1.2.3" xref="S2.T1.40.40.40.2.m1.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.T1.40.40.40.2.m1.1b"><apply id="S2.T1.40.40.40.2.m1.1.1.cmml" xref="S2.T1.40.40.40.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.40.40.40.2.m1.1.1.1.cmml" xref="S2.T1.40.40.40.2.m1.1.1">subscript</csymbol><apply id="S2.T1.40.40.40.2.m1.1.1.2.cmml" xref="S2.T1.40.40.40.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.40.40.40.2.m1.1.1.2.1.cmml" xref="S2.T1.40.40.40.2.m1.1.1">superscript</csymbol><ci id="S2.T1.40.40.40.2.m1.1.1.2.2.cmml" xref="S2.T1.40.40.40.2.m1.1.1.2.2">𝐿</ci><ci id="S2.T1.40.40.40.2.m1.1.1.2.3.cmml" xref="S2.T1.40.40.40.2.m1.1.1.2.3">𝑡</ci></apply><apply id="S2.T1.40.40.40.2.m1.1.1.3.cmml" xref="S2.T1.40.40.40.2.m1.1.1.3"><plus id="S2.T1.40.40.40.2.m1.1.1.3.1.cmml" xref="S2.T1.40.40.40.2.m1.1.1.3.1"></plus><ci id="S2.T1.40.40.40.2.m1.1.1.3.2.cmml" xref="S2.T1.40.40.40.2.m1.1.1.3.2">𝑖</ci><cn type="integer" id="S2.T1.40.40.40.2.m1.1.1.3.3.cmml" xref="S2.T1.40.40.40.2.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.40.40.40.2.m1.1c">L^{t}_{i+1}</annotation></semantics></math></td>
<td id="S2.T1.40.40.40.4" class="ltx_td ltx_align_center">The local model</td>
</tr>
<tr id="S2.T1.44.44.44" class="ltx_tr">
<td id="S2.T1.43.43.43.3" class="ltx_td ltx_align_center ltx_border_b">
<math id="S2.T1.41.41.41.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.T1.41.41.41.1.m1.1a"><mi id="S2.T1.41.41.41.1.m1.1.1" xref="S2.T1.41.41.41.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.T1.41.41.41.1.m1.1b"><ci id="S2.T1.41.41.41.1.m1.1.1.cmml" xref="S2.T1.41.41.41.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.41.41.41.1.m1.1c">X</annotation></semantics></math><math id="S2.T1.42.42.42.2.m2.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.T1.42.42.42.2.m2.1a"><mo id="S2.T1.42.42.42.2.m2.1.1" xref="S2.T1.42.42.42.2.m2.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S2.T1.42.42.42.2.m2.1b"><in id="S2.T1.42.42.42.2.m2.1.1.cmml" xref="S2.T1.42.42.42.2.m2.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.42.42.42.2.m2.1c">\in</annotation></semantics></math><math id="S2.T1.43.43.43.3.m3.1" class="ltx_Math" alttext="R^{n}" display="inline"><semantics id="S2.T1.43.43.43.3.m3.1a"><msup id="S2.T1.43.43.43.3.m3.1.1" xref="S2.T1.43.43.43.3.m3.1.1.cmml"><mi id="S2.T1.43.43.43.3.m3.1.1.2" xref="S2.T1.43.43.43.3.m3.1.1.2.cmml">R</mi><mi id="S2.T1.43.43.43.3.m3.1.1.3" xref="S2.T1.43.43.43.3.m3.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S2.T1.43.43.43.3.m3.1b"><apply id="S2.T1.43.43.43.3.m3.1.1.cmml" xref="S2.T1.43.43.43.3.m3.1.1"><csymbol cd="ambiguous" id="S2.T1.43.43.43.3.m3.1.1.1.cmml" xref="S2.T1.43.43.43.3.m3.1.1">superscript</csymbol><ci id="S2.T1.43.43.43.3.m3.1.1.2.cmml" xref="S2.T1.43.43.43.3.m3.1.1.2">𝑅</ci><ci id="S2.T1.43.43.43.3.m3.1.1.3.cmml" xref="S2.T1.43.43.43.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.43.43.43.3.m3.1c">R^{n}</annotation></semantics></math>
</td>
<td id="S2.T1.44.44.44.5" class="ltx_td ltx_align_center ltx_border_b">The data in classification tasks</td>
<td id="S2.T1.44.44.44.4" class="ltx_td ltx_align_center ltx_border_b"><math id="S2.T1.44.44.44.4.m1.2" class="ltx_Math" alttext="N(0,\sigma^{2})" display="inline"><semantics id="S2.T1.44.44.44.4.m1.2a"><mrow id="S2.T1.44.44.44.4.m1.2.2" xref="S2.T1.44.44.44.4.m1.2.2.cmml"><mi id="S2.T1.44.44.44.4.m1.2.2.3" xref="S2.T1.44.44.44.4.m1.2.2.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.T1.44.44.44.4.m1.2.2.2" xref="S2.T1.44.44.44.4.m1.2.2.2.cmml">​</mo><mrow id="S2.T1.44.44.44.4.m1.2.2.1.1" xref="S2.T1.44.44.44.4.m1.2.2.1.2.cmml"><mo stretchy="false" id="S2.T1.44.44.44.4.m1.2.2.1.1.2" xref="S2.T1.44.44.44.4.m1.2.2.1.2.cmml">(</mo><mn id="S2.T1.44.44.44.4.m1.1.1" xref="S2.T1.44.44.44.4.m1.1.1.cmml">0</mn><mo id="S2.T1.44.44.44.4.m1.2.2.1.1.3" xref="S2.T1.44.44.44.4.m1.2.2.1.2.cmml">,</mo><msup id="S2.T1.44.44.44.4.m1.2.2.1.1.1" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1.cmml"><mi id="S2.T1.44.44.44.4.m1.2.2.1.1.1.2" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1.2.cmml">σ</mi><mn id="S2.T1.44.44.44.4.m1.2.2.1.1.1.3" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S2.T1.44.44.44.4.m1.2.2.1.1.4" xref="S2.T1.44.44.44.4.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.44.44.44.4.m1.2b"><apply id="S2.T1.44.44.44.4.m1.2.2.cmml" xref="S2.T1.44.44.44.4.m1.2.2"><times id="S2.T1.44.44.44.4.m1.2.2.2.cmml" xref="S2.T1.44.44.44.4.m1.2.2.2"></times><ci id="S2.T1.44.44.44.4.m1.2.2.3.cmml" xref="S2.T1.44.44.44.4.m1.2.2.3">𝑁</ci><interval closure="open" id="S2.T1.44.44.44.4.m1.2.2.1.2.cmml" xref="S2.T1.44.44.44.4.m1.2.2.1.1"><cn type="integer" id="S2.T1.44.44.44.4.m1.1.1.cmml" xref="S2.T1.44.44.44.4.m1.1.1">0</cn><apply id="S2.T1.44.44.44.4.m1.2.2.1.1.1.cmml" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.T1.44.44.44.4.m1.2.2.1.1.1.1.cmml" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1">superscript</csymbol><ci id="S2.T1.44.44.44.4.m1.2.2.1.1.1.2.cmml" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1.2">𝜎</ci><cn type="integer" id="S2.T1.44.44.44.4.m1.2.2.1.1.1.3.cmml" xref="S2.T1.44.44.44.4.m1.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.44.44.44.4.m1.2c">N(0,\sigma^{2})</annotation></semantics></math></td>
<td id="S2.T1.44.44.44.6" class="ltx_td ltx_align_center ltx_border_b">The Gaussian noise</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Federated learning was conceived by Google  <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2017a</a>)</cite>, who introduced it as an innovative departure from conventional machine learning paradigms. The fundamental objective in federated learning revolves around decentralizing training in a way that preserves privacy by training with the data distributed across local devices on those devices without ever needing to transmit those data to another site or device. Thus, the confidentiality of the underlying data sources is preserved. This stands in stark contrast to the conventional approach to machine learning, where user data is centrally stored on a server for the purposes of training a model. Federated learning strategically disseminates the model’s training across multiple clients – a paradigm shift that not only enhances efficiency but also improves the scalability of the learning process  <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>)</cite>. In fact, as the volume of data continues to grow, federated learning is witnessing constant surges in popularity  <cite class="ltx_cite ltx_citemacro_citep">(Hu and Vasilakos, <a href="#bib.bib63" title="" class="ltx_ref">2016</a>)</cite>. For a visual representation of the basic federated learning process, see Figure  <a href="#S2.F2" title="Figure 2 ‣ 2.2. Federated Learning ‣ 2. Preliminary ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.10884/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">FL training process</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The participants in the training process are categorically divided into two distinct roles: the central server and the clients. A fundamental tenet of this framework is that the central server is deliberately precluded from accessing the local datasets of individual clients. Instead, its role is confined to receiving the client parameters only, such as gradients and weights. These parameters are subsequently aggregated to create a global model. Hence, each client retains sole possession of their own local dataset, deploying it exclusively for local training purposes  <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>The privacy and security challenges in federated learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Federated learning offers a framework wherein multiple clients collaborate to train a global model. While federated learning mitigates the need for clients to share their raw data, sensitive information can still be revealed simply by exchanging the model parameters. In fact, recent research has demonstrated that federated learning continues to pose various both privacy and security concerns <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite>. Thus, several challenges to safeguarding privacy and ensuring security persist. A comprehensive outline of all these challenges follows.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p"><span id="S2.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Model Training.</span> Like all machine learning models, federated learning models are susceptible to attacks, which can lead to privacy breaches and security vulnerabilities. In instances where a trained model becomes the target of an attack, there is a risk that the model updates might be manipulated before being sent to the server. For example, an adversary may seek to poison a local model with the objective of diminishing the overall accuracy of the global model <cite class="ltx_cite ltx_citemacro_citep">(Shejwalkar et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>. In terms of privacy, an adversary could infer whether a particular data record was used to train the global model, yet still be unable to ascertain whether a data record was used to train a specific local model <cite class="ltx_cite ltx_citemacro_citep">(Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>)</cite>. The revelation of these vulnerabilities has introduced additional and formidable security and privacy challenges to the field.</p>
</div>
</li>
<li id="S2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.ix2.p1.1" class="ltx_p"><span id="S2.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Gradient.</span> Before a client shares their local gradient updates with the server, adversaries can exploit those gradients to pose privacy and security threats. For example, adversaries can surreptitiously obtain local gradients and leverage them to reconstruct a client’s private training data <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite>. Such unauthorized access would allow an adversary to completely recover a client’s data samples. In terms of gradient security, an adversary could poison the training data or the trained models by generating detrimental gradients that compromise the integrity of the training process. These threats represent significant challenges for federated learning.</p>
</div>
</li>
<li id="S2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S2.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.ix3.p1.1" class="ltx_p"><span id="S2.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Training Data.</span> A salient feature of the federated learning training process is the complete lack of local data on the server. As such, an adversary might attempt to deduce representative classes so as to identify data samples of specific classes or even reveal the identity of a client. When an adversary poisons the training data, it not only undermines the integrity of the global model, it will also impact the performance of local models. Data poisoning attacks can be categorized into clean-label and dirty-label attacks. In a clean-label attack, the adversary cannot modify the labels of any training data, whereas in a dirty-label attack, the adversary will deliberately misclassify a substantial number of data samples <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite>. Additionally, an adversary might infer information about the properties of the training datasets <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite>, employing both passive and active techniques to deduce those properties or those of other participants involved in the training.</p>
</div>
</li>
</ol>
<p id="S2.SS3.p2.1" class="ltx_p">The trained models, the gradients, and the training data collectively constitute crucial components of the federated learning paradigm. Each of these elements presents a significant point of vulnerability with regard to privacy and security. The specific targets of such attacks and the defensive strategies that can be deployed to protect against them are discussed next.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Defense methodologies</h3>

<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1. </span>Differential privacy</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Differential privacy, initially introduced by Dwork <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib37" title="" class="ltx_ref">2006</a>)</cite>, is a stringent privacy guarantee which ensures that the manipulating one database item will have no discernible impact on the outcomes of a query. Thus, the overarching objective of differential privacy is to safeguard the privacy of a dataset’s contribution by obfuscating their collective information. This privacy-preserving framework has extensive utility in the domain of federated learning, with applications spanning real-world scenarios such as electronic health data <cite class="ltx_cite ltx_citemacro_citep">(Choudhury et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> or artificial intelligence <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2. </span>Homomorphic encryption</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">Homomorphic encryption method <math id="S2.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.SS4.SSS2.p1.1.m1.1a"><mi id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.1b"><ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.1c">H</annotation></semantics></math>
<cite class="ltx_cite ltx_citemacro_citep">(Paillier, <a href="#bib.bib100" title="" class="ltx_ref">1999</a>; Goldwasser and Micali, <a href="#bib.bib52" title="" class="ltx_ref">1982</a>)</cite>is a technique that allows operations to be performed on a piece of ciphertexts without needing to know the decryption key. It particularly allows specific algebraic operations to be performed on encrypted content. Homomorphic encryption methods are classified into three categories <cite class="ltx_cite ltx_citemacro_citep">(Acar et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>: partial homomorphic encryption, somewhat homomorphic encryption, and fully homomorphic encryption.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3. </span>Secure multiparty computation </h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.2" class="ltx_p">Secure multi-party computation allows a party to compute a function of input values, where each party can only access its corresponding output value and cannot obtain the input and output values of other parties. For instance, if a private value <math id="S2.SS4.SSS3.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS4.SSS3.p1.1.m1.1a"><mi id="S2.SS4.SSS3.p1.1.m1.1.1" xref="S2.SS4.SSS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.1.m1.1b"><ci id="S2.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.1.m1.1c">x</annotation></semantics></math> is allocated to sharing among <math id="S2.SS4.SSS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS4.SSS3.p1.2.m2.1a"><mi id="S2.SS4.SSS3.p1.2.m2.1.1" xref="S2.SS4.SSS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.2.m2.1b"><ci id="S2.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.2.m2.1c">n</annotation></semantics></math> parties, all parties could cooperate in computing:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_math_unparsed" alttext="y_{1},...y_{n}=f(x_{1},..x_{n})" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1b"><msub id="S2.E1.m1.1.1"><mi id="S2.E1.m1.1.1.2">y</mi><mn id="S2.E1.m1.1.1.3">1</mn></msub><mo id="S2.E1.m1.1.2">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.3">…</mi><msub id="S2.E1.m1.1.4"><mi id="S2.E1.m1.1.4.2">y</mi><mi id="S2.E1.m1.1.4.3">n</mi></msub><mo id="S2.E1.m1.1.5">=</mo><mi id="S2.E1.m1.1.6">f</mi><mrow id="S2.E1.m1.1.7"><mo stretchy="false" id="S2.E1.m1.1.7.1">(</mo><msub id="S2.E1.m1.1.7.2"><mi id="S2.E1.m1.1.7.2.2">x</mi><mn id="S2.E1.m1.1.7.2.3">1</mn></msub><mo id="S2.E1.m1.1.7.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S2.E1.m1.1.7.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S2.E1.m1.1.7.5">.</mo><msub id="S2.E1.m1.1.7.6"><mi id="S2.E1.m1.1.7.6.2">x</mi><mi id="S2.E1.m1.1.7.6.3">n</mi></msub><mo stretchy="false" id="S2.E1.m1.1.7.7">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.E1.m1.1c">y_{1},...y_{n}=f(x_{1},..x_{n})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS3.p1.7" class="ltx_p">but each party <math id="S2.SS4.SSS3.p1.3.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S2.SS4.SSS3.p1.3.m1.1a"><msub id="S2.SS4.SSS3.p1.3.m1.1.1" xref="S2.SS4.SSS3.p1.3.m1.1.1.cmml"><mi id="S2.SS4.SSS3.p1.3.m1.1.1.2" xref="S2.SS4.SSS3.p1.3.m1.1.1.2.cmml">P</mi><mi id="S2.SS4.SSS3.p1.3.m1.1.1.3" xref="S2.SS4.SSS3.p1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.3.m1.1b"><apply id="S2.SS4.SSS3.p1.3.m1.1.1.cmml" xref="S2.SS4.SSS3.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.3.m1.1.1.1.cmml" xref="S2.SS4.SSS3.p1.3.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.3.m1.1.1.2.cmml" xref="S2.SS4.SSS3.p1.3.m1.1.1.2">𝑃</ci><ci id="S2.SS4.SSS3.p1.3.m1.1.1.3.cmml" xref="S2.SS4.SSS3.p1.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.3.m1.1c">P_{i}</annotation></semantics></math> could only access the content of <math id="S2.SS4.SSS3.p1.4.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS4.SSS3.p1.4.m2.1a"><msub id="S2.SS4.SSS3.p1.4.m2.1.1" xref="S2.SS4.SSS3.p1.4.m2.1.1.cmml"><mi id="S2.SS4.SSS3.p1.4.m2.1.1.2" xref="S2.SS4.SSS3.p1.4.m2.1.1.2.cmml">x</mi><mi id="S2.SS4.SSS3.p1.4.m2.1.1.3" xref="S2.SS4.SSS3.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.4.m2.1b"><apply id="S2.SS4.SSS3.p1.4.m2.1.1.cmml" xref="S2.SS4.SSS3.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.4.m2.1.1.1.cmml" xref="S2.SS4.SSS3.p1.4.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.4.m2.1.1.2.cmml" xref="S2.SS4.SSS3.p1.4.m2.1.1.2">𝑥</ci><ci id="S2.SS4.SSS3.p1.4.m2.1.1.3.cmml" xref="S2.SS4.SSS3.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.4.m2.1c">x_{i}</annotation></semantics></math>. Additionally, <math id="S2.SS4.SSS3.p1.5.m3.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S2.SS4.SSS3.p1.5.m3.1a"><msub id="S2.SS4.SSS3.p1.5.m3.1.1" xref="S2.SS4.SSS3.p1.5.m3.1.1.cmml"><mi id="S2.SS4.SSS3.p1.5.m3.1.1.2" xref="S2.SS4.SSS3.p1.5.m3.1.1.2.cmml">P</mi><mi id="S2.SS4.SSS3.p1.5.m3.1.1.3" xref="S2.SS4.SSS3.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.5.m3.1b"><apply id="S2.SS4.SSS3.p1.5.m3.1.1.cmml" xref="S2.SS4.SSS3.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.5.m3.1.1.1.cmml" xref="S2.SS4.SSS3.p1.5.m3.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.5.m3.1.1.2.cmml" xref="S2.SS4.SSS3.p1.5.m3.1.1.2">𝑃</ci><ci id="S2.SS4.SSS3.p1.5.m3.1.1.3.cmml" xref="S2.SS4.SSS3.p1.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.5.m3.1c">P_{i}</annotation></semantics></math> can only know the output value <math id="S2.SS4.SSS3.p1.6.m4.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.SS4.SSS3.p1.6.m4.1a"><msub id="S2.SS4.SSS3.p1.6.m4.1.1" xref="S2.SS4.SSS3.p1.6.m4.1.1.cmml"><mi id="S2.SS4.SSS3.p1.6.m4.1.1.2" xref="S2.SS4.SSS3.p1.6.m4.1.1.2.cmml">y</mi><mi id="S2.SS4.SSS3.p1.6.m4.1.1.3" xref="S2.SS4.SSS3.p1.6.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.6.m4.1b"><apply id="S2.SS4.SSS3.p1.6.m4.1.1.cmml" xref="S2.SS4.SSS3.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.6.m4.1.1.1.cmml" xref="S2.SS4.SSS3.p1.6.m4.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.6.m4.1.1.2.cmml" xref="S2.SS4.SSS3.p1.6.m4.1.1.2">𝑦</ci><ci id="S2.SS4.SSS3.p1.6.m4.1.1.3.cmml" xref="S2.SS4.SSS3.p1.6.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.6.m4.1c">y_{i}</annotation></semantics></math> based on its input <math id="S2.SS4.SSS3.p1.7.m5.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS4.SSS3.p1.7.m5.1a"><msub id="S2.SS4.SSS3.p1.7.m5.1.1" xref="S2.SS4.SSS3.p1.7.m5.1.1.cmml"><mi id="S2.SS4.SSS3.p1.7.m5.1.1.2" xref="S2.SS4.SSS3.p1.7.m5.1.1.2.cmml">x</mi><mi id="S2.SS4.SSS3.p1.7.m5.1.1.3" xref="S2.SS4.SSS3.p1.7.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.7.m5.1b"><apply id="S2.SS4.SSS3.p1.7.m5.1.1.cmml" xref="S2.SS4.SSS3.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.7.m5.1.1.1.cmml" xref="S2.SS4.SSS3.p1.7.m5.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p1.7.m5.1.1.2.cmml" xref="S2.SS4.SSS3.p1.7.m5.1.1.2">𝑥</ci><ci id="S2.SS4.SSS3.p1.7.m5.1.1.3.cmml" xref="S2.SS4.SSS3.p1.7.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.7.m5.1c">x_{i}</annotation></semantics></math>, and could not obtain any additional information.</p>
</div>
<div id="S2.SS4.SSS3.p2" class="ltx_para">
<p id="S2.SS4.SSS3.p2.1" class="ltx_p">Secret sharing is another framework for secure multi-party computing. It involves splitting secret values into random portions, and distributing them to different parties such that each party can only receives one value <cite class="ltx_cite ltx_citemacro_citep">(Shamir, <a href="#bib.bib106" title="" class="ltx_ref">1979</a>; Beimel, <a href="#bib.bib11" title="" class="ltx_ref">2011</a>)</cite>. Depending on the context, all or a certain number of shared values are required to reconstruct the original secret value. The main types of secret sharing  <cite class="ltx_cite ltx_citemacro_citep">(Damgård et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2012</a>)</cite>, Shamir secret sharing <cite class="ltx_cite ltx_citemacro_citep">(Shamir, <a href="#bib.bib106" title="" class="ltx_ref">1979</a>)</cite>, and binary secret sharing <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2007</a>)</cite>. For instance, Shamir secret sharing is based on polynomial equations. It uses matrix operation acceleration methods and satisfies theoretical information security.</p>
</div>
</section>
<section id="S2.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4. </span>Knowledge distillation</h4>

<div id="S2.SS4.SSS4.p1" class="ltx_para">
<p id="S2.SS4.SSS4.p1.1" class="ltx_p">Knowledge distillation is a methodology aimed at addressing the challenge of compressing either a large model or multiple models into a more compact representation, all while preserving the performance levels of the original models. The approach hinges on a teacher-student training framework, where a trained teacher model imparts its knowledge, and a student model assimilates this knowledge through a process known as distillation training. This transfer of knowledge from a complex teacher model to a simplified student model typically incurs only a marginal performance loss <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2015</a>)</cite>. Various distillation techniques for strengthening privacy have been devised specifically for federated learning frameworks.</p>
</div>
</section>
<section id="S2.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.5. </span>Anomaly detection</h4>

<div id="S2.SS4.SSS5.p1" class="ltx_para">
<p id="S2.SS4.SSS5.p1.1" class="ltx_p">Anomaly detection means to identify unexpected behaviors or patterns in data <cite class="ltx_cite ltx_citemacro_citep">(Chandola et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2009</a>)</cite> and is a critical aspect of numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Chalapathy and Chawla, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>. Also known as abnormalities, deviants, or outliers, the core premise of anomaly detection primarily revolves around applying statistical analysis techniques to discern events that deviate from anticipated patterns or normal activity.</p>
</div>
<div id="S2.SS4.SSS5.p2" class="ltx_para">
<p id="S2.SS4.SSS5.p2.1" class="ltx_p">In the context of machine learning security defense mechanisms, approaches based on anomaly detection primarily focus on identifying anomalous model updates and deploying outlier detection methods. These methodologies serve the overarching objective of eliminating models that deviate significantly from the norm, all the while preserving the utility of the model itself. Moreover, deploying in a range of anomaly detection methods increases the chances that different threat scenarios will be identified in a timely fashion.</p>
</div>
</section>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5. </span>Fairness in federated learning</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In the context of machine learning, fairness in the context of machine learning is frequently defined as the safeguarding of specific attributes. However, within the realm of federated learning, fairness introduces distinctive challenges, primarily attributable to the number of datasets held by individual clients and the number of classes within them. This diversity can create notable performance disparities between clients, ultimately culminating in inequitable outcomes.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">Conventional fairness paradigms in machine learning typically rely on centralized datasets for both training and model evaluation. Yet directly applying these methods to federated learning does not necessarily ensure fairness across the entire population <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>. This is because, in federated learning, fairness needs to be achieved at the individual client level, which involves minimizing any performance discrepancies between any two clients. Further, at a group level, it is imperative to ensure that the model does not discriminate against specific demographics <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2023</a>)</cite>. So, while most fairness methodologies rely on centralized datasets, the unique characteristics of federated learning mandate a more nuanced approach to this delicate issue <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.Thmtheorem1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_smallcaps ltx_title_theorem">Definition 0 (Fairness <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite>).</h6>
<div id="S2.Thmtheorem1.p1" class="ltx_para">
<p id="S2.Thmtheorem1.p1.6" class="ltx_p"><span id="S2.Thmtheorem1.p1.6.6" class="ltx_text ltx_font_italic">If a client <math id="S2.Thmtheorem1.p1.1.1.m1.1" class="ltx_Math" alttext="C_{1}" display="inline"><semantics id="S2.Thmtheorem1.p1.1.1.m1.1a"><msub id="S2.Thmtheorem1.p1.1.1.m1.1.1" xref="S2.Thmtheorem1.p1.1.1.m1.1.1.cmml"><mi id="S2.Thmtheorem1.p1.1.1.m1.1.1.2" xref="S2.Thmtheorem1.p1.1.1.m1.1.1.2.cmml">C</mi><mn id="S2.Thmtheorem1.p1.1.1.m1.1.1.3" xref="S2.Thmtheorem1.p1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.1.1.m1.1b"><apply id="S2.Thmtheorem1.p1.1.1.m1.1.1.cmml" xref="S2.Thmtheorem1.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.1.1.m1.1.1.1.cmml" xref="S2.Thmtheorem1.p1.1.1.m1.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.1.1.m1.1.1.2.cmml" xref="S2.Thmtheorem1.p1.1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S2.Thmtheorem1.p1.1.1.m1.1.1.3.cmml" xref="S2.Thmtheorem1.p1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.1.1.m1.1c">C_{1}</annotation></semantics></math>’s model <math id="S2.Thmtheorem1.p1.2.2.m2.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S2.Thmtheorem1.p1.2.2.m2.1a"><msub id="S2.Thmtheorem1.p1.2.2.m2.1.1" xref="S2.Thmtheorem1.p1.2.2.m2.1.1.cmml"><mi id="S2.Thmtheorem1.p1.2.2.m2.1.1.2" xref="S2.Thmtheorem1.p1.2.2.m2.1.1.2.cmml">W</mi><mn id="S2.Thmtheorem1.p1.2.2.m2.1.1.3" xref="S2.Thmtheorem1.p1.2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.2.2.m2.1b"><apply id="S2.Thmtheorem1.p1.2.2.m2.1.1.cmml" xref="S2.Thmtheorem1.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.2.2.m2.1.1.1.cmml" xref="S2.Thmtheorem1.p1.2.2.m2.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.2.2.m2.1.1.2.cmml" xref="S2.Thmtheorem1.p1.2.2.m2.1.1.2">𝑊</ci><cn type="integer" id="S2.Thmtheorem1.p1.2.2.m2.1.1.3.cmml" xref="S2.Thmtheorem1.p1.2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.2.2.m2.1c">W_{1}</annotation></semantics></math> has a more uniform distribution of test performance across the network than another client <math id="S2.Thmtheorem1.p1.3.3.m3.1" class="ltx_Math" alttext="C_{2}" display="inline"><semantics id="S2.Thmtheorem1.p1.3.3.m3.1a"><msub id="S2.Thmtheorem1.p1.3.3.m3.1.1" xref="S2.Thmtheorem1.p1.3.3.m3.1.1.cmml"><mi id="S2.Thmtheorem1.p1.3.3.m3.1.1.2" xref="S2.Thmtheorem1.p1.3.3.m3.1.1.2.cmml">C</mi><mn id="S2.Thmtheorem1.p1.3.3.m3.1.1.3" xref="S2.Thmtheorem1.p1.3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.3.3.m3.1b"><apply id="S2.Thmtheorem1.p1.3.3.m3.1.1.cmml" xref="S2.Thmtheorem1.p1.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.3.3.m3.1.1.1.cmml" xref="S2.Thmtheorem1.p1.3.3.m3.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.3.3.m3.1.1.2.cmml" xref="S2.Thmtheorem1.p1.3.3.m3.1.1.2">𝐶</ci><cn type="integer" id="S2.Thmtheorem1.p1.3.3.m3.1.1.3.cmml" xref="S2.Thmtheorem1.p1.3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.3.3.m3.1c">C_{2}</annotation></semantics></math>’s model <math id="S2.Thmtheorem1.p1.4.4.m4.1" class="ltx_Math" alttext="W_{2}" display="inline"><semantics id="S2.Thmtheorem1.p1.4.4.m4.1a"><msub id="S2.Thmtheorem1.p1.4.4.m4.1.1" xref="S2.Thmtheorem1.p1.4.4.m4.1.1.cmml"><mi id="S2.Thmtheorem1.p1.4.4.m4.1.1.2" xref="S2.Thmtheorem1.p1.4.4.m4.1.1.2.cmml">W</mi><mn id="S2.Thmtheorem1.p1.4.4.m4.1.1.3" xref="S2.Thmtheorem1.p1.4.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.4.4.m4.1b"><apply id="S2.Thmtheorem1.p1.4.4.m4.1.1.cmml" xref="S2.Thmtheorem1.p1.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.4.4.m4.1.1.1.cmml" xref="S2.Thmtheorem1.p1.4.4.m4.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.4.4.m4.1.1.2.cmml" xref="S2.Thmtheorem1.p1.4.4.m4.1.1.2">𝑊</ci><cn type="integer" id="S2.Thmtheorem1.p1.4.4.m4.1.1.3.cmml" xref="S2.Thmtheorem1.p1.4.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.4.4.m4.1c">W_{2}</annotation></semantics></math> , then <math id="S2.Thmtheorem1.p1.5.5.m5.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S2.Thmtheorem1.p1.5.5.m5.1a"><msub id="S2.Thmtheorem1.p1.5.5.m5.1.1" xref="S2.Thmtheorem1.p1.5.5.m5.1.1.cmml"><mi id="S2.Thmtheorem1.p1.5.5.m5.1.1.2" xref="S2.Thmtheorem1.p1.5.5.m5.1.1.2.cmml">W</mi><mn id="S2.Thmtheorem1.p1.5.5.m5.1.1.3" xref="S2.Thmtheorem1.p1.5.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.5.5.m5.1b"><apply id="S2.Thmtheorem1.p1.5.5.m5.1.1.cmml" xref="S2.Thmtheorem1.p1.5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.5.5.m5.1.1.1.cmml" xref="S2.Thmtheorem1.p1.5.5.m5.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.5.5.m5.1.1.2.cmml" xref="S2.Thmtheorem1.p1.5.5.m5.1.1.2">𝑊</ci><cn type="integer" id="S2.Thmtheorem1.p1.5.5.m5.1.1.3.cmml" xref="S2.Thmtheorem1.p1.5.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.5.5.m5.1c">W_{1}</annotation></semantics></math> is considered more fair than <math id="S2.Thmtheorem1.p1.6.6.m6.1" class="ltx_Math" alttext="W_{2}" display="inline"><semantics id="S2.Thmtheorem1.p1.6.6.m6.1a"><msub id="S2.Thmtheorem1.p1.6.6.m6.1.1" xref="S2.Thmtheorem1.p1.6.6.m6.1.1.cmml"><mi id="S2.Thmtheorem1.p1.6.6.m6.1.1.2" xref="S2.Thmtheorem1.p1.6.6.m6.1.1.2.cmml">W</mi><mn id="S2.Thmtheorem1.p1.6.6.m6.1.1.3" xref="S2.Thmtheorem1.p1.6.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem1.p1.6.6.m6.1b"><apply id="S2.Thmtheorem1.p1.6.6.m6.1.1.cmml" xref="S2.Thmtheorem1.p1.6.6.m6.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem1.p1.6.6.m6.1.1.1.cmml" xref="S2.Thmtheorem1.p1.6.6.m6.1.1">subscript</csymbol><ci id="S2.Thmtheorem1.p1.6.6.m6.1.1.2.cmml" xref="S2.Thmtheorem1.p1.6.6.m6.1.1.2">𝑊</ci><cn type="integer" id="S2.Thmtheorem1.p1.6.6.m6.1.1.3.cmml" xref="S2.Thmtheorem1.p1.6.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem1.p1.6.6.m6.1c">W_{2}</annotation></semantics></math>.</span></p>
</div>
</div>
<div id="S2.SS5.p3" class="ltx_para">
<p id="S2.SS5.p3.1" class="ltx_p">Notably, not every fairness constraints can be satisfied in a federated learning framework, except in very rare and special cases. Thus, researchers instead strive to achieve some type of fairness. Types include: selective participant fairness  <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>, which reflects the interests of both the clients and the federation. It protects system performance overall based on some probability that a client will be selected. Good-intent fairness <cite class="ltx_cite ltx_citemacro_citep">(Mohri et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2019</a>)</cite> reflects the interests of the federated model via the minimum deviation of the protected attribute. Other types of fairness include contribution fairness  <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2020a</a>)</cite>, regret distribution fairness <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2020</a>)</cite>, and expectation fairness <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS5.p4" class="ltx_para">
<p id="S2.SS5.p4.14" class="ltx_p">Equalized odds and equal opportunity can be used to measure the impact of this trade-off in classification tasks. In this task, assume the data <math id="S2.SS5.p4.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS5.p4.1.m1.1a"><mi id="S2.SS5.p4.1.m1.1.1" xref="S2.SS5.p4.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.1.m1.1b"><ci id="S2.SS5.p4.1.m1.1.1.cmml" xref="S2.SS5.p4.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.1.m1.1c">X</annotation></semantics></math> <math id="S2.SS5.p4.2.m2.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.SS5.p4.2.m2.1a"><mo id="S2.SS5.p4.2.m2.1.1" xref="S2.SS5.p4.2.m2.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.2.m2.1b"><in id="S2.SS5.p4.2.m2.1.1.cmml" xref="S2.SS5.p4.2.m2.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.2.m2.1c">\in</annotation></semantics></math> <math id="S2.SS5.p4.3.m3.1" class="ltx_Math" alttext="R^{n}" display="inline"><semantics id="S2.SS5.p4.3.m3.1a"><msup id="S2.SS5.p4.3.m3.1.1" xref="S2.SS5.p4.3.m3.1.1.cmml"><mi id="S2.SS5.p4.3.m3.1.1.2" xref="S2.SS5.p4.3.m3.1.1.2.cmml">R</mi><mi id="S2.SS5.p4.3.m3.1.1.3" xref="S2.SS5.p4.3.m3.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.3.m3.1b"><apply id="S2.SS5.p4.3.m3.1.1.cmml" xref="S2.SS5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS5.p4.3.m3.1.1.1.cmml" xref="S2.SS5.p4.3.m3.1.1">superscript</csymbol><ci id="S2.SS5.p4.3.m3.1.1.2.cmml" xref="S2.SS5.p4.3.m3.1.1.2">𝑅</ci><ci id="S2.SS5.p4.3.m3.1.1.3.cmml" xref="S2.SS5.p4.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.3.m3.1c">R^{n}</annotation></semantics></math> with a sensitive attribute <math id="S2.SS5.p4.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS5.p4.4.m4.1a"><mi id="S2.SS5.p4.4.m4.1.1" xref="S2.SS5.p4.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.4.m4.1b"><ci id="S2.SS5.p4.4.m4.1.1.cmml" xref="S2.SS5.p4.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.4.m4.1c">A</annotation></semantics></math> <math id="S2.SS5.p4.5.m5.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.SS5.p4.5.m5.1a"><mo id="S2.SS5.p4.5.m5.1.1" xref="S2.SS5.p4.5.m5.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.5.m5.1b"><in id="S2.SS5.p4.5.m5.1.1.cmml" xref="S2.SS5.p4.5.m5.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.5.m5.1c">\in</annotation></semantics></math> <math id="S2.SS5.p4.6.m6.2" class="ltx_Math" alttext="\{0,1\}" display="inline"><semantics id="S2.SS5.p4.6.m6.2a"><mrow id="S2.SS5.p4.6.m6.2.3.2" xref="S2.SS5.p4.6.m6.2.3.1.cmml"><mo stretchy="false" id="S2.SS5.p4.6.m6.2.3.2.1" xref="S2.SS5.p4.6.m6.2.3.1.cmml">{</mo><mn id="S2.SS5.p4.6.m6.1.1" xref="S2.SS5.p4.6.m6.1.1.cmml">0</mn><mo id="S2.SS5.p4.6.m6.2.3.2.2" xref="S2.SS5.p4.6.m6.2.3.1.cmml">,</mo><mn id="S2.SS5.p4.6.m6.2.2" xref="S2.SS5.p4.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS5.p4.6.m6.2.3.2.3" xref="S2.SS5.p4.6.m6.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.6.m6.2b"><set id="S2.SS5.p4.6.m6.2.3.1.cmml" xref="S2.SS5.p4.6.m6.2.3.2"><cn type="integer" id="S2.SS5.p4.6.m6.1.1.cmml" xref="S2.SS5.p4.6.m6.1.1">0</cn><cn type="integer" id="S2.SS5.p4.6.m6.2.2.cmml" xref="S2.SS5.p4.6.m6.2.2">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.6.m6.2c">\{0,1\}</annotation></semantics></math> and the labels <math id="S2.SS5.p4.7.m7.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS5.p4.7.m7.1a"><mi id="S2.SS5.p4.7.m7.1.1" xref="S2.SS5.p4.7.m7.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.7.m7.1b"><ci id="S2.SS5.p4.7.m7.1.1.cmml" xref="S2.SS5.p4.7.m7.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.7.m7.1c">Y</annotation></semantics></math> <math id="S2.SS5.p4.8.m8.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.SS5.p4.8.m8.1a"><mo id="S2.SS5.p4.8.m8.1.1" xref="S2.SS5.p4.8.m8.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.8.m8.1b"><in id="S2.SS5.p4.8.m8.1.1.cmml" xref="S2.SS5.p4.8.m8.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.8.m8.1c">\in</annotation></semantics></math> <math id="S2.SS5.p4.9.m9.2" class="ltx_Math" alttext="\{0,1\}" display="inline"><semantics id="S2.SS5.p4.9.m9.2a"><mrow id="S2.SS5.p4.9.m9.2.3.2" xref="S2.SS5.p4.9.m9.2.3.1.cmml"><mo stretchy="false" id="S2.SS5.p4.9.m9.2.3.2.1" xref="S2.SS5.p4.9.m9.2.3.1.cmml">{</mo><mn id="S2.SS5.p4.9.m9.1.1" xref="S2.SS5.p4.9.m9.1.1.cmml">0</mn><mo id="S2.SS5.p4.9.m9.2.3.2.2" xref="S2.SS5.p4.9.m9.2.3.1.cmml">,</mo><mn id="S2.SS5.p4.9.m9.2.2" xref="S2.SS5.p4.9.m9.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS5.p4.9.m9.2.3.2.3" xref="S2.SS5.p4.9.m9.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.9.m9.2b"><set id="S2.SS5.p4.9.m9.2.3.1.cmml" xref="S2.SS5.p4.9.m9.2.3.2"><cn type="integer" id="S2.SS5.p4.9.m9.1.1.cmml" xref="S2.SS5.p4.9.m9.1.1">0</cn><cn type="integer" id="S2.SS5.p4.9.m9.2.2.cmml" xref="S2.SS5.p4.9.m9.2.2">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.9.m9.2c">\{0,1\}</annotation></semantics></math>. The predictor outputs the predicted outcome <math id="S2.SS5.p4.10.m10.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.SS5.p4.10.m10.1a"><msub id="S2.SS5.p4.10.m10.1.1" xref="S2.SS5.p4.10.m10.1.1.cmml"><mi id="S2.SS5.p4.10.m10.1.1.2" xref="S2.SS5.p4.10.m10.1.1.2.cmml">Y</mi><mi id="S2.SS5.p4.10.m10.1.1.3" xref="S2.SS5.p4.10.m10.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.10.m10.1b"><apply id="S2.SS5.p4.10.m10.1.1.cmml" xref="S2.SS5.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS5.p4.10.m10.1.1.1.cmml" xref="S2.SS5.p4.10.m10.1.1">subscript</csymbol><ci id="S2.SS5.p4.10.m10.1.1.2.cmml" xref="S2.SS5.p4.10.m10.1.1.2">𝑌</ci><ci id="S2.SS5.p4.10.m10.1.1.3.cmml" xref="S2.SS5.p4.10.m10.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.10.m10.1c">Y_{p}</annotation></semantics></math> <math id="S2.SS5.p4.11.m11.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.SS5.p4.11.m11.1a"><mo id="S2.SS5.p4.11.m11.1.1" xref="S2.SS5.p4.11.m11.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.11.m11.1b"><in id="S2.SS5.p4.11.m11.1.1.cmml" xref="S2.SS5.p4.11.m11.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.11.m11.1c">\in</annotation></semantics></math> <math id="S2.SS5.p4.12.m12.2" class="ltx_Math" alttext="\{0,1\}" display="inline"><semantics id="S2.SS5.p4.12.m12.2a"><mrow id="S2.SS5.p4.12.m12.2.3.2" xref="S2.SS5.p4.12.m12.2.3.1.cmml"><mo stretchy="false" id="S2.SS5.p4.12.m12.2.3.2.1" xref="S2.SS5.p4.12.m12.2.3.1.cmml">{</mo><mn id="S2.SS5.p4.12.m12.1.1" xref="S2.SS5.p4.12.m12.1.1.cmml">0</mn><mo id="S2.SS5.p4.12.m12.2.3.2.2" xref="S2.SS5.p4.12.m12.2.3.1.cmml">,</mo><mn id="S2.SS5.p4.12.m12.2.2" xref="S2.SS5.p4.12.m12.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS5.p4.12.m12.2.3.2.3" xref="S2.SS5.p4.12.m12.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.12.m12.2b"><set id="S2.SS5.p4.12.m12.2.3.1.cmml" xref="S2.SS5.p4.12.m12.2.3.2"><cn type="integer" id="S2.SS5.p4.12.m12.1.1.cmml" xref="S2.SS5.p4.12.m12.1.1">0</cn><cn type="integer" id="S2.SS5.p4.12.m12.2.2.cmml" xref="S2.SS5.p4.12.m12.2.2">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.12.m12.2c">\{0,1\}</annotation></semantics></math>. The model predicts independent results for <math id="S2.SS5.p4.13.m13.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS5.p4.13.m13.1a"><mi id="S2.SS5.p4.13.m13.1.1" xref="S2.SS5.p4.13.m13.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.13.m13.1b"><ci id="S2.SS5.p4.13.m13.1.1.cmml" xref="S2.SS5.p4.13.m13.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.13.m13.1c">A</annotation></semantics></math> but is accurate for <math id="S2.SS5.p4.14.m14.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS5.p4.14.m14.1a"><mi id="S2.SS5.p4.14.m14.1.1" xref="S2.SS5.p4.14.m14.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.14.m14.1b"><ci id="S2.SS5.p4.14.m14.1.1.cmml" xref="S2.SS5.p4.14.m14.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.14.m14.1c">Y</annotation></semantics></math>.</p>
</div>
<div id="S2.Thmtheorem2" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_smallcaps ltx_title_theorem">Definition 0 (Equalized odds <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2016</a>)</cite>).</h6>
<div id="S2.Thmtheorem2.p1" class="ltx_para">
<p id="S2.Thmtheorem2.p1.6" class="ltx_p"><span id="S2.Thmtheorem2.p1.6.6" class="ltx_text ltx_font_italic">If a predictor <math id="S2.Thmtheorem2.p1.1.1.m1.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.Thmtheorem2.p1.1.1.m1.1a"><msub id="S2.Thmtheorem2.p1.1.1.m1.1.1" xref="S2.Thmtheorem2.p1.1.1.m1.1.1.cmml"><mi id="S2.Thmtheorem2.p1.1.1.m1.1.1.2" xref="S2.Thmtheorem2.p1.1.1.m1.1.1.2.cmml">Y</mi><mi id="S2.Thmtheorem2.p1.1.1.m1.1.1.3" xref="S2.Thmtheorem2.p1.1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.1.1.m1.1b"><apply id="S2.Thmtheorem2.p1.1.1.m1.1.1.cmml" xref="S2.Thmtheorem2.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem2.p1.1.1.m1.1.1.1.cmml" xref="S2.Thmtheorem2.p1.1.1.m1.1.1">subscript</csymbol><ci id="S2.Thmtheorem2.p1.1.1.m1.1.1.2.cmml" xref="S2.Thmtheorem2.p1.1.1.m1.1.1.2">𝑌</ci><ci id="S2.Thmtheorem2.p1.1.1.m1.1.1.3.cmml" xref="S2.Thmtheorem2.p1.1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.1.1.m1.1c">Y_{p}</annotation></semantics></math> satisfies the equalized odds criterion with respect to the protected attribute <math id="S2.Thmtheorem2.p1.2.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.Thmtheorem2.p1.2.2.m2.1a"><mi id="S2.Thmtheorem2.p1.2.2.m2.1.1" xref="S2.Thmtheorem2.p1.2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.2.2.m2.1b"><ci id="S2.Thmtheorem2.p1.2.2.m2.1.1.cmml" xref="S2.Thmtheorem2.p1.2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.2.2.m2.1c">A</annotation></semantics></math> and label <math id="S2.Thmtheorem2.p1.3.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.Thmtheorem2.p1.3.3.m3.1a"><mi id="S2.Thmtheorem2.p1.3.3.m3.1.1" xref="S2.Thmtheorem2.p1.3.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.3.3.m3.1b"><ci id="S2.Thmtheorem2.p1.3.3.m3.1.1.cmml" xref="S2.Thmtheorem2.p1.3.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.3.3.m3.1c">Y</annotation></semantics></math>, and <math id="S2.Thmtheorem2.p1.4.4.m4.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.Thmtheorem2.p1.4.4.m4.1a"><msub id="S2.Thmtheorem2.p1.4.4.m4.1.1" xref="S2.Thmtheorem2.p1.4.4.m4.1.1.cmml"><mi id="S2.Thmtheorem2.p1.4.4.m4.1.1.2" xref="S2.Thmtheorem2.p1.4.4.m4.1.1.2.cmml">Y</mi><mi id="S2.Thmtheorem2.p1.4.4.m4.1.1.3" xref="S2.Thmtheorem2.p1.4.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.4.4.m4.1b"><apply id="S2.Thmtheorem2.p1.4.4.m4.1.1.cmml" xref="S2.Thmtheorem2.p1.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem2.p1.4.4.m4.1.1.1.cmml" xref="S2.Thmtheorem2.p1.4.4.m4.1.1">subscript</csymbol><ci id="S2.Thmtheorem2.p1.4.4.m4.1.1.2.cmml" xref="S2.Thmtheorem2.p1.4.4.m4.1.1.2">𝑌</ci><ci id="S2.Thmtheorem2.p1.4.4.m4.1.1.3.cmml" xref="S2.Thmtheorem2.p1.4.4.m4.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.4.4.m4.1c">Y_{p}</annotation></semantics></math> and <math id="S2.Thmtheorem2.p1.5.5.m5.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.Thmtheorem2.p1.5.5.m5.1a"><mi id="S2.Thmtheorem2.p1.5.5.m5.1.1" xref="S2.Thmtheorem2.p1.5.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.5.5.m5.1b"><ci id="S2.Thmtheorem2.p1.5.5.m5.1.1.cmml" xref="S2.Thmtheorem2.p1.5.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.5.5.m5.1c">A</annotation></semantics></math> are conditionally independent given <math id="S2.Thmtheorem2.p1.6.6.m6.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.Thmtheorem2.p1.6.6.m6.1a"><mi id="S2.Thmtheorem2.p1.6.6.m6.1.1" xref="S2.Thmtheorem2.p1.6.6.m6.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem2.p1.6.6.m6.1b"><ci id="S2.Thmtheorem2.p1.6.6.m6.1.1.cmml" xref="S2.Thmtheorem2.p1.6.6.m6.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem2.p1.6.6.m6.1c">Y</annotation></semantics></math>, then the predictor is said to satisfy equalized odds.</span></p>
</div>
</div>
<div id="S2.SS5.p5" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="Pr\{Y_{p}=1|A=0,Y=y\}-Pr\{Y_{p}=1|A=1,Y=y\}=0,y\in\{0,1\}" display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.3.cmml"><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.4" xref="S2.E2.m1.3.3.1.1.4.cmml"><mrow id="S2.E2.m1.3.3.1.1.2.2" xref="S2.E2.m1.3.3.1.1.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.4" xref="S2.E2.m1.3.3.1.1.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">​</mo><mi id="S2.E2.m1.3.3.1.1.2.2.5" xref="S2.E2.m1.3.3.1.1.2.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2.2.3a" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.2.3.1.cmml">{</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">Y</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2.2.2.2.4" xref="S2.E2.m1.3.3.1.1.2.2.2.3.1.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.3.cmml"><mrow id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.2.cmml">A</mi><mo id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.1" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.3" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.3.cmml">0</mn></mrow><mo id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.3a.cmml">,</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.2.cmml">Y</mi><mo id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.1" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.1.cmml">=</mo><mi id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.3.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.2.2.2.5" xref="S2.E2.m1.3.3.1.1.2.2.2.3.1.cmml">}</mo></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.4.5" xref="S2.E2.m1.3.3.1.1.4.5.cmml">−</mo><mrow id="S2.E2.m1.3.3.1.1.4.4" xref="S2.E2.m1.3.3.1.1.4.4.cmml"><mi id="S2.E2.m1.3.3.1.1.4.4.4" xref="S2.E2.m1.3.3.1.1.4.4.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.4.4.3" xref="S2.E2.m1.3.3.1.1.4.4.3.cmml">​</mo><mi id="S2.E2.m1.3.3.1.1.4.4.5" xref="S2.E2.m1.3.3.1.1.4.4.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.4.4.3a" xref="S2.E2.m1.3.3.1.1.4.4.3.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.4.4.2.2" xref="S2.E2.m1.3.3.1.1.4.4.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.4.4.2.2.3" xref="S2.E2.m1.3.3.1.1.4.4.2.3.1.cmml">{</mo><mrow id="S2.E2.m1.3.3.1.1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.cmml"><mi id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.2" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.2.cmml">Y</mi><mi id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E2.m1.3.3.1.1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.3.3.1.1.1.3" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.4.4.2.2.4" xref="S2.E2.m1.3.3.1.1.4.4.2.3.1.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.3.cmml"><mrow id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.2" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.2.cmml">A</mi><mo id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.1" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.3" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.3.cmml">1</mn></mrow><mo id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.3a.cmml">,</mo><mrow id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.2.cmml">Y</mi><mo id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.1" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.1.cmml">=</mo><mi id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.3.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.4.4.2.2.5" xref="S2.E2.m1.3.3.1.1.4.4.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.5" xref="S2.E2.m1.3.3.1.1.5.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.6" xref="S2.E2.m1.3.3.1.1.6.cmml">0</mn></mrow><mo id="S2.E2.m1.4.4.2.3" xref="S2.E2.m1.4.4.3a.cmml">,</mo><mrow id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml"><mi id="S2.E2.m1.4.4.2.2.2" xref="S2.E2.m1.4.4.2.2.2.cmml">y</mi><mo id="S2.E2.m1.4.4.2.2.1" xref="S2.E2.m1.4.4.2.2.1.cmml">∈</mo><mrow id="S2.E2.m1.4.4.2.2.3.2" xref="S2.E2.m1.4.4.2.2.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.2.2.3.2.1" xref="S2.E2.m1.4.4.2.2.3.1.cmml">{</mo><mn id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">0</mn><mo id="S2.E2.m1.4.4.2.2.3.2.2" xref="S2.E2.m1.4.4.2.2.3.1.cmml">,</mo><mn id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">1</mn><mo stretchy="false" id="S2.E2.m1.4.4.2.2.3.2.3" xref="S2.E2.m1.4.4.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3a.cmml" xref="S2.E2.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1"><eq id="S2.E2.m1.3.3.1.1.5.cmml" xref="S2.E2.m1.3.3.1.1.5"></eq><apply id="S2.E2.m1.3.3.1.1.4.cmml" xref="S2.E2.m1.3.3.1.1.4"><minus id="S2.E2.m1.3.3.1.1.4.5.cmml" xref="S2.E2.m1.3.3.1.1.4.5"></minus><apply id="S2.E2.m1.3.3.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2"><times id="S2.E2.m1.3.3.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3"></times><ci id="S2.E2.m1.3.3.1.1.2.2.4.cmml" xref="S2.E2.m1.3.3.1.1.2.2.4">𝑃</ci><ci id="S2.E2.m1.3.3.1.1.2.2.5.cmml" xref="S2.E2.m1.3.3.1.1.2.2.5">𝑟</ci><apply id="S2.E2.m1.3.3.1.1.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.2.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.3">conditional-set</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1"><eq id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1"></eq><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.2">𝑌</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E2.m1.3.3.1.1.2.2.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.2.2.2.2.3a.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1"><eq id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.1"></eq><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.2">𝐴</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.1.1.3">0</cn></apply><apply id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2"><eq id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.1"></eq><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.2">𝑌</ci><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.2.2.3">𝑦</ci></apply></apply></apply></apply><apply id="S2.E2.m1.3.3.1.1.4.4.cmml" xref="S2.E2.m1.3.3.1.1.4.4"><times id="S2.E2.m1.3.3.1.1.4.4.3.cmml" xref="S2.E2.m1.3.3.1.1.4.4.3"></times><ci id="S2.E2.m1.3.3.1.1.4.4.4.cmml" xref="S2.E2.m1.3.3.1.1.4.4.4">𝑃</ci><ci id="S2.E2.m1.3.3.1.1.4.4.5.cmml" xref="S2.E2.m1.3.3.1.1.4.4.5">𝑟</ci><apply id="S2.E2.m1.3.3.1.1.4.4.2.3.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.4.4.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.3">conditional-set</csymbol><apply id="S2.E2.m1.3.3.1.1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1"><eq id="S2.E2.m1.3.3.1.1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.1"></eq><apply id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.2">𝑌</ci><ci id="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E2.m1.3.3.1.1.3.3.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3.1.1.1.3">1</cn></apply><apply id="S2.E2.m1.3.3.1.1.4.4.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.4.4.2.2.2.3a.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1"><eq id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.1"></eq><ci id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.2">𝐴</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.1.1.3">1</cn></apply><apply id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2"><eq id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.1"></eq><ci id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.2">𝑌</ci><ci id="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.4.4.2.2.2.2.2.3">𝑦</ci></apply></apply></apply></apply></apply><cn type="integer" id="S2.E2.m1.3.3.1.1.6.cmml" xref="S2.E2.m1.3.3.1.1.6">0</cn></apply><apply id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2"><in id="S2.E2.m1.4.4.2.2.1.cmml" xref="S2.E2.m1.4.4.2.2.1"></in><ci id="S2.E2.m1.4.4.2.2.2.cmml" xref="S2.E2.m1.4.4.2.2.2">𝑦</ci><set id="S2.E2.m1.4.4.2.2.3.1.cmml" xref="S2.E2.m1.4.4.2.2.3.2"><cn type="integer" id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">0</cn><cn type="integer" id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">Pr\{Y_{p}=1|A=0,Y=y\}-Pr\{Y_{p}=1|A=1,Y=y\}=0,y\in\{0,1\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS5.p5.1" class="ltx_p">The equalized odds criterion requires constraints to ensure that the true positive rates are equal to the false positive rates across all protected attribute groups, Models that perform well only on the majority group are penalized.</p>
</div>
<div id="S2.Thmtheorem3" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_smallcaps ltx_title_theorem">Definition 0 (Equalized opportunity <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2016</a>)</cite>).</h6>
<div id="S2.Thmtheorem3.p1" class="ltx_para">
<p id="S2.Thmtheorem3.p1.3" class="ltx_p"><span id="S2.Thmtheorem3.p1.3.3" class="ltx_text ltx_font_italic">If a predictor <math id="S2.Thmtheorem3.p1.1.1.m1.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.Thmtheorem3.p1.1.1.m1.1a"><msub id="S2.Thmtheorem3.p1.1.1.m1.1.1" xref="S2.Thmtheorem3.p1.1.1.m1.1.1.cmml"><mi id="S2.Thmtheorem3.p1.1.1.m1.1.1.2" xref="S2.Thmtheorem3.p1.1.1.m1.1.1.2.cmml">Y</mi><mi id="S2.Thmtheorem3.p1.1.1.m1.1.1.3" xref="S2.Thmtheorem3.p1.1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem3.p1.1.1.m1.1b"><apply id="S2.Thmtheorem3.p1.1.1.m1.1.1.cmml" xref="S2.Thmtheorem3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem3.p1.1.1.m1.1.1.1.cmml" xref="S2.Thmtheorem3.p1.1.1.m1.1.1">subscript</csymbol><ci id="S2.Thmtheorem3.p1.1.1.m1.1.1.2.cmml" xref="S2.Thmtheorem3.p1.1.1.m1.1.1.2">𝑌</ci><ci id="S2.Thmtheorem3.p1.1.1.m1.1.1.3.cmml" xref="S2.Thmtheorem3.p1.1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem3.p1.1.1.m1.1c">Y_{p}</annotation></semantics></math> satisfies the equalized opportunity criterion with respect to the protected attribute <math id="S2.Thmtheorem3.p1.2.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.Thmtheorem3.p1.2.2.m2.1a"><mi id="S2.Thmtheorem3.p1.2.2.m2.1.1" xref="S2.Thmtheorem3.p1.2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem3.p1.2.2.m2.1b"><ci id="S2.Thmtheorem3.p1.2.2.m2.1.1.cmml" xref="S2.Thmtheorem3.p1.2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem3.p1.2.2.m2.1c">A</annotation></semantics></math> and label <math id="S2.Thmtheorem3.p1.3.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.Thmtheorem3.p1.3.3.m3.1a"><mi id="S2.Thmtheorem3.p1.3.3.m3.1.1" xref="S2.Thmtheorem3.p1.3.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem3.p1.3.3.m3.1b"><ci id="S2.Thmtheorem3.p1.3.3.m3.1.1.cmml" xref="S2.Thmtheorem3.p1.3.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem3.p1.3.3.m3.1c">Y</annotation></semantics></math>, we have the following:</span></p>
</div>
</div>
<div id="S2.SS5.p6" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.4" class="ltx_Math" alttext="Pr\{Y_{p}=1|A=0,Y=1\}-Pr\{Y_{p}=1|A=1,Y=1\}=0" display="block"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml"><mrow id="S2.E3.m1.4.4.4" xref="S2.E3.m1.4.4.4.cmml"><mrow id="S2.E3.m1.2.2.2.2" xref="S2.E3.m1.2.2.2.2.cmml"><mi id="S2.E3.m1.2.2.2.2.4" xref="S2.E3.m1.2.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E3.m1.2.2.2.2.5" xref="S2.E3.m1.2.2.2.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.3a" xref="S2.E3.m1.2.2.2.2.3.cmml">​</mo><mrow id="S2.E3.m1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.2.3.1.cmml">{</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.2.2.cmml">Y</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.2.2.4" xref="S2.E3.m1.2.2.2.2.2.3.1.cmml">|</mo><mrow id="S2.E3.m1.2.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.2.3.cmml"><mrow id="S2.E3.m1.2.2.2.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.cmml"><mi id="S2.E3.m1.2.2.2.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.2.cmml">A</mi><mo id="S2.E3.m1.2.2.2.2.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.1.cmml">=</mo><mn id="S2.E3.m1.2.2.2.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.3.cmml">0</mn></mrow><mo id="S2.E3.m1.2.2.2.2.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.2.2.2.3a.cmml">,</mo><mrow id="S2.E3.m1.2.2.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S2.E3.m1.2.2.2.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.2.cmml">Y</mi><mo id="S2.E3.m1.2.2.2.2.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.1.cmml">=</mo><mn id="S2.E3.m1.2.2.2.2.2.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.3.cmml">1</mn></mrow></mrow><mo stretchy="false" id="S2.E3.m1.2.2.2.2.2.2.5" xref="S2.E3.m1.2.2.2.2.2.3.1.cmml">}</mo></mrow></mrow><mo id="S2.E3.m1.4.4.4.5" xref="S2.E3.m1.4.4.4.5.cmml">−</mo><mrow id="S2.E3.m1.4.4.4.4" xref="S2.E3.m1.4.4.4.4.cmml"><mi id="S2.E3.m1.4.4.4.4.4" xref="S2.E3.m1.4.4.4.4.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.4.4.3" xref="S2.E3.m1.4.4.4.4.3.cmml">​</mo><mi id="S2.E3.m1.4.4.4.4.5" xref="S2.E3.m1.4.4.4.4.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.4.4.3a" xref="S2.E3.m1.4.4.4.4.3.cmml">​</mo><mrow id="S2.E3.m1.4.4.4.4.2.2" xref="S2.E3.m1.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S2.E3.m1.4.4.4.4.2.2.3" xref="S2.E3.m1.4.4.4.4.2.3.1.cmml">{</mo><mrow id="S2.E3.m1.3.3.3.3.1.1.1" xref="S2.E3.m1.3.3.3.3.1.1.1.cmml"><msub id="S2.E3.m1.3.3.3.3.1.1.1.2" xref="S2.E3.m1.3.3.3.3.1.1.1.2.cmml"><mi id="S2.E3.m1.3.3.3.3.1.1.1.2.2" xref="S2.E3.m1.3.3.3.3.1.1.1.2.2.cmml">Y</mi><mi id="S2.E3.m1.3.3.3.3.1.1.1.2.3" xref="S2.E3.m1.3.3.3.3.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E3.m1.3.3.3.3.1.1.1.1" xref="S2.E3.m1.3.3.3.3.1.1.1.1.cmml">=</mo><mn id="S2.E3.m1.3.3.3.3.1.1.1.3" xref="S2.E3.m1.3.3.3.3.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.4.4.2.2.4" xref="S2.E3.m1.4.4.4.4.2.3.1.cmml">|</mo><mrow id="S2.E3.m1.4.4.4.4.2.2.2.2" xref="S2.E3.m1.4.4.4.4.2.2.2.3.cmml"><mrow id="S2.E3.m1.4.4.4.4.2.2.2.1.1" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.cmml"><mi id="S2.E3.m1.4.4.4.4.2.2.2.1.1.2" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.2.cmml">A</mi><mo id="S2.E3.m1.4.4.4.4.2.2.2.1.1.1" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.1.cmml">=</mo><mn id="S2.E3.m1.4.4.4.4.2.2.2.1.1.3" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.3.cmml">1</mn></mrow><mo id="S2.E3.m1.4.4.4.4.2.2.2.2.3" xref="S2.E3.m1.4.4.4.4.2.2.2.3a.cmml">,</mo><mrow id="S2.E3.m1.4.4.4.4.2.2.2.2.2" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.cmml"><mi id="S2.E3.m1.4.4.4.4.2.2.2.2.2.2" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.2.cmml">Y</mi><mo id="S2.E3.m1.4.4.4.4.2.2.2.2.2.1" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.1.cmml">=</mo><mn id="S2.E3.m1.4.4.4.4.2.2.2.2.2.3" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.3.cmml">1</mn></mrow></mrow><mo stretchy="false" id="S2.E3.m1.4.4.4.4.2.2.5" xref="S2.E3.m1.4.4.4.4.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S2.E3.m1.4.4.5" xref="S2.E3.m1.4.4.5.cmml">=</mo><mn id="S2.E3.m1.4.4.6" xref="S2.E3.m1.4.4.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4"><eq id="S2.E3.m1.4.4.5.cmml" xref="S2.E3.m1.4.4.5"></eq><apply id="S2.E3.m1.4.4.4.cmml" xref="S2.E3.m1.4.4.4"><minus id="S2.E3.m1.4.4.4.5.cmml" xref="S2.E3.m1.4.4.4.5"></minus><apply id="S2.E3.m1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2"><times id="S2.E3.m1.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.3"></times><ci id="S2.E3.m1.2.2.2.2.4.cmml" xref="S2.E3.m1.2.2.2.2.4">𝑃</ci><ci id="S2.E3.m1.2.2.2.2.5.cmml" xref="S2.E3.m1.2.2.2.2.5">𝑟</ci><apply id="S2.E3.m1.2.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2.2"><csymbol cd="latexml" id="S2.E3.m1.2.2.2.2.2.3.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.3">conditional-set</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1"></eq><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.2">𝑌</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.2.2.2.3a.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E3.m1.2.2.2.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1"><eq id="S2.E3.m1.2.2.2.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.1"></eq><ci id="S2.E3.m1.2.2.2.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.2">𝐴</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.2.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.1.1.3">0</cn></apply><apply id="S2.E3.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2"><eq id="S2.E3.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.1"></eq><ci id="S2.E3.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.2">𝑌</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2.2.2.3">1</cn></apply></apply></apply></apply><apply id="S2.E3.m1.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4"><times id="S2.E3.m1.4.4.4.4.3.cmml" xref="S2.E3.m1.4.4.4.4.3"></times><ci id="S2.E3.m1.4.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4.4">𝑃</ci><ci id="S2.E3.m1.4.4.4.4.5.cmml" xref="S2.E3.m1.4.4.4.4.5">𝑟</ci><apply id="S2.E3.m1.4.4.4.4.2.3.cmml" xref="S2.E3.m1.4.4.4.4.2.2"><csymbol cd="latexml" id="S2.E3.m1.4.4.4.4.2.3.1.cmml" xref="S2.E3.m1.4.4.4.4.2.2.3">conditional-set</csymbol><apply id="S2.E3.m1.3.3.3.3.1.1.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1"><eq id="S2.E3.m1.3.3.3.3.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.1"></eq><apply id="S2.E3.m1.3.3.3.3.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.2.2">𝑌</ci><ci id="S2.E3.m1.3.3.3.3.1.1.1.2.3.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E3.m1.3.3.3.3.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.3.1.1.1.3">1</cn></apply><apply id="S2.E3.m1.4.4.4.4.2.2.2.3.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.4.4.2.2.2.3a.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E3.m1.4.4.4.4.2.2.2.1.1.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1"><eq id="S2.E3.m1.4.4.4.4.2.2.2.1.1.1.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.1"></eq><ci id="S2.E3.m1.4.4.4.4.2.2.2.1.1.2.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.2">𝐴</ci><cn type="integer" id="S2.E3.m1.4.4.4.4.2.2.2.1.1.3.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.1.1.3">1</cn></apply><apply id="S2.E3.m1.4.4.4.4.2.2.2.2.2.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2"><eq id="S2.E3.m1.4.4.4.4.2.2.2.2.2.1.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.1"></eq><ci id="S2.E3.m1.4.4.4.4.2.2.2.2.2.2.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.2">𝑌</ci><cn type="integer" id="S2.E3.m1.4.4.4.4.2.2.2.2.2.3.cmml" xref="S2.E3.m1.4.4.4.4.2.2.2.2.2.3">1</cn></apply></apply></apply></apply></apply><cn type="integer" id="S2.E3.m1.4.4.6.cmml" xref="S2.E3.m1.4.4.6">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">Pr\{Y_{p}=1|A=0,Y=1\}-Pr\{Y_{p}=1|A=1,Y=1\}=0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS5.p6.2" class="ltx_p">In a supervised learning task, the goal of equal opportunity is to have access to the training data on the label and obtain the true label <math id="S2.SS5.p6.1.m1.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS5.p6.1.m1.1a"><mi id="S2.SS5.p6.1.m1.1.1" xref="S2.SS5.p6.1.m1.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p6.1.m1.1b"><ci id="S2.SS5.p6.1.m1.1.1.cmml" xref="S2.SS5.p6.1.m1.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p6.1.m1.1c">Y</annotation></semantics></math> from any equivalent prediction data. Additionally, limiting the accuracy of the model with the prediction samples ensures the fairness of sensitive attribute <math id="S2.SS5.p6.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS5.p6.2.m2.1a"><mi id="S2.SS5.p6.2.m2.1.1" xref="S2.SS5.p6.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p6.2.m2.1b"><ci id="S2.SS5.p6.2.m2.1.1.cmml" xref="S2.SS5.p6.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p6.2.m2.1c">A</annotation></semantics></math>.</p>
</div>
<div id="S2.SS5.p7" class="ltx_para">
<p id="S2.SS5.p7.1" class="ltx_p">Demographic parity is another form of non-discrimination. This mode of fairness requires that a decision such as to be independent of protected attributes, such as such as accepting or rejecting a loan application will be made independent of protected attributes. The definition is as follows:</p>
</div>
<div id="S2.Thmtheorem4" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_smallcaps ltx_title_theorem">Definition 0 (Demographic parity <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>).</h6>
<div id="S2.Thmtheorem4.p1" class="ltx_para">
<p id="S2.Thmtheorem4.p1.4" class="ltx_p"><span id="S2.Thmtheorem4.p1.4.4" class="ltx_text ltx_font_italic">When a predictor <math id="S2.Thmtheorem4.p1.1.1.m1.1" class="ltx_Math" alttext="Y_{p}" display="inline"><semantics id="S2.Thmtheorem4.p1.1.1.m1.1a"><msub id="S2.Thmtheorem4.p1.1.1.m1.1.1" xref="S2.Thmtheorem4.p1.1.1.m1.1.1.cmml"><mi id="S2.Thmtheorem4.p1.1.1.m1.1.1.2" xref="S2.Thmtheorem4.p1.1.1.m1.1.1.2.cmml">Y</mi><mi id="S2.Thmtheorem4.p1.1.1.m1.1.1.3" xref="S2.Thmtheorem4.p1.1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem4.p1.1.1.m1.1b"><apply id="S2.Thmtheorem4.p1.1.1.m1.1.1.cmml" xref="S2.Thmtheorem4.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.Thmtheorem4.p1.1.1.m1.1.1.1.cmml" xref="S2.Thmtheorem4.p1.1.1.m1.1.1">subscript</csymbol><ci id="S2.Thmtheorem4.p1.1.1.m1.1.1.2.cmml" xref="S2.Thmtheorem4.p1.1.1.m1.1.1.2">𝑌</ci><ci id="S2.Thmtheorem4.p1.1.1.m1.1.1.3.cmml" xref="S2.Thmtheorem4.p1.1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem4.p1.1.1.m1.1c">Y_{p}</annotation></semantics></math> predicts the outcomes <math id="S2.Thmtheorem4.p1.2.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.Thmtheorem4.p1.2.2.m2.1a"><mi id="S2.Thmtheorem4.p1.2.2.m2.1.1" xref="S2.Thmtheorem4.p1.2.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem4.p1.2.2.m2.1b"><ci id="S2.Thmtheorem4.p1.2.2.m2.1.1.cmml" xref="S2.Thmtheorem4.p1.2.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem4.p1.2.2.m2.1c">Y</annotation></semantics></math> independently of the sensitive attribute <math id="S2.Thmtheorem4.p1.3.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.Thmtheorem4.p1.3.3.m3.1a"><mi id="S2.Thmtheorem4.p1.3.3.m3.1.1" xref="S2.Thmtheorem4.p1.3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem4.p1.3.3.m3.1b"><ci id="S2.Thmtheorem4.p1.3.3.m3.1.1.cmml" xref="S2.Thmtheorem4.p1.3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem4.p1.3.3.m3.1c">A</annotation></semantics></math>, the decision is only considered to be independent of the protected attribute if the positive prediction rate of the sensitive attribute <math id="S2.Thmtheorem4.p1.4.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.Thmtheorem4.p1.4.4.m4.1a"><mi id="S2.Thmtheorem4.p1.4.4.m4.1.1" xref="S2.Thmtheorem4.p1.4.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.Thmtheorem4.p1.4.4.m4.1b"><ci id="S2.Thmtheorem4.p1.4.4.m4.1.1.cmml" xref="S2.Thmtheorem4.p1.4.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Thmtheorem4.p1.4.4.m4.1c">A</annotation></semantics></math> is the same for all groups.</span></p>
</div>
</div>
<div id="S2.SS5.p8" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.4" class="ltx_Math" alttext="Pr\{Y_{p}=1|A=0\}-Pr\{Y_{p}=1|A=1\}=0" display="block"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.4" xref="S2.E4.m1.4.4.4.cmml"><mrow id="S2.E4.m1.2.2.2.2" xref="S2.E4.m1.2.2.2.2.cmml"><mi id="S2.E4.m1.2.2.2.2.4" xref="S2.E4.m1.2.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.2.2.3" xref="S2.E4.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E4.m1.2.2.2.2.5" xref="S2.E4.m1.2.2.2.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.2.2.3a" xref="S2.E4.m1.2.2.2.2.3.cmml">​</mo><mrow id="S2.E4.m1.2.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E4.m1.2.2.2.2.2.2.3" xref="S2.E4.m1.2.2.2.2.2.3.1.cmml">{</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.1.1.1.2.2.cmml">Y</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="S2.E4.m1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.2.2.2.2.4" xref="S2.E4.m1.2.2.2.2.2.3.1.cmml">|</mo><mrow id="S2.E4.m1.2.2.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.2.2.cmml"><mi id="S2.E4.m1.2.2.2.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.2.2.2.cmml">A</mi><mo id="S2.E4.m1.2.2.2.2.2.2.2.1" xref="S2.E4.m1.2.2.2.2.2.2.2.1.cmml">=</mo><mn id="S2.E4.m1.2.2.2.2.2.2.2.3" xref="S2.E4.m1.2.2.2.2.2.2.2.3.cmml">0</mn></mrow><mo stretchy="false" id="S2.E4.m1.2.2.2.2.2.2.5" xref="S2.E4.m1.2.2.2.2.2.3.1.cmml">}</mo></mrow></mrow><mo id="S2.E4.m1.4.4.4.5" xref="S2.E4.m1.4.4.4.5.cmml">−</mo><mrow id="S2.E4.m1.4.4.4.4" xref="S2.E4.m1.4.4.4.4.cmml"><mi id="S2.E4.m1.4.4.4.4.4" xref="S2.E4.m1.4.4.4.4.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.4.3" xref="S2.E4.m1.4.4.4.4.3.cmml">​</mo><mi id="S2.E4.m1.4.4.4.4.5" xref="S2.E4.m1.4.4.4.4.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.4.3a" xref="S2.E4.m1.4.4.4.4.3.cmml">​</mo><mrow id="S2.E4.m1.4.4.4.4.2.2" xref="S2.E4.m1.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.4.4.2.2.3" xref="S2.E4.m1.4.4.4.4.2.3.1.cmml">{</mo><mrow id="S2.E4.m1.3.3.3.3.1.1.1" xref="S2.E4.m1.3.3.3.3.1.1.1.cmml"><msub id="S2.E4.m1.3.3.3.3.1.1.1.2" xref="S2.E4.m1.3.3.3.3.1.1.1.2.cmml"><mi id="S2.E4.m1.3.3.3.3.1.1.1.2.2" xref="S2.E4.m1.3.3.3.3.1.1.1.2.2.cmml">Y</mi><mi id="S2.E4.m1.3.3.3.3.1.1.1.2.3" xref="S2.E4.m1.3.3.3.3.1.1.1.2.3.cmml">p</mi></msub><mo id="S2.E4.m1.3.3.3.3.1.1.1.1" xref="S2.E4.m1.3.3.3.3.1.1.1.1.cmml">=</mo><mn id="S2.E4.m1.3.3.3.3.1.1.1.3" xref="S2.E4.m1.3.3.3.3.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.4.2.2.4" xref="S2.E4.m1.4.4.4.4.2.3.1.cmml">|</mo><mrow id="S2.E4.m1.4.4.4.4.2.2.2" xref="S2.E4.m1.4.4.4.4.2.2.2.cmml"><mi id="S2.E4.m1.4.4.4.4.2.2.2.2" xref="S2.E4.m1.4.4.4.4.2.2.2.2.cmml">A</mi><mo id="S2.E4.m1.4.4.4.4.2.2.2.1" xref="S2.E4.m1.4.4.4.4.2.2.2.1.cmml">=</mo><mn id="S2.E4.m1.4.4.4.4.2.2.2.3" xref="S2.E4.m1.4.4.4.4.2.2.2.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.E4.m1.4.4.4.4.2.2.5" xref="S2.E4.m1.4.4.4.4.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S2.E4.m1.4.4.5" xref="S2.E4.m1.4.4.5.cmml">=</mo><mn id="S2.E4.m1.4.4.6" xref="S2.E4.m1.4.4.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.5.cmml" xref="S2.E4.m1.4.4.5"></eq><apply id="S2.E4.m1.4.4.4.cmml" xref="S2.E4.m1.4.4.4"><minus id="S2.E4.m1.4.4.4.5.cmml" xref="S2.E4.m1.4.4.4.5"></minus><apply id="S2.E4.m1.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2"><times id="S2.E4.m1.2.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.2.3"></times><ci id="S2.E4.m1.2.2.2.2.4.cmml" xref="S2.E4.m1.2.2.2.2.4">𝑃</ci><ci id="S2.E4.m1.2.2.2.2.5.cmml" xref="S2.E4.m1.2.2.2.2.5">𝑟</ci><apply id="S2.E4.m1.2.2.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.2.2.2"><csymbol cd="latexml" id="S2.E4.m1.2.2.2.2.2.3.1.cmml" xref="S2.E4.m1.2.2.2.2.2.2.3">conditional-set</csymbol><apply id="S2.E4.m1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1"></eq><apply id="S2.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.2.2">𝑌</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E4.m1.2.2.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2.2.2"><eq id="S2.E4.m1.2.2.2.2.2.2.2.1.cmml" xref="S2.E4.m1.2.2.2.2.2.2.2.1"></eq><ci id="S2.E4.m1.2.2.2.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2.2.2.2">𝐴</ci><cn type="integer" id="S2.E4.m1.2.2.2.2.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.2.2.2.2.3">0</cn></apply></apply></apply><apply id="S2.E4.m1.4.4.4.4.cmml" xref="S2.E4.m1.4.4.4.4"><times id="S2.E4.m1.4.4.4.4.3.cmml" xref="S2.E4.m1.4.4.4.4.3"></times><ci id="S2.E4.m1.4.4.4.4.4.cmml" xref="S2.E4.m1.4.4.4.4.4">𝑃</ci><ci id="S2.E4.m1.4.4.4.4.5.cmml" xref="S2.E4.m1.4.4.4.4.5">𝑟</ci><apply id="S2.E4.m1.4.4.4.4.2.3.cmml" xref="S2.E4.m1.4.4.4.4.2.2"><csymbol cd="latexml" id="S2.E4.m1.4.4.4.4.2.3.1.cmml" xref="S2.E4.m1.4.4.4.4.2.2.3">conditional-set</csymbol><apply id="S2.E4.m1.3.3.3.3.1.1.1.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1"><eq id="S2.E4.m1.3.3.3.3.1.1.1.1.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.1"></eq><apply id="S2.E4.m1.3.3.3.3.1.1.1.2.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.2.2">𝑌</ci><ci id="S2.E4.m1.3.3.3.3.1.1.1.2.3.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.2.3">𝑝</ci></apply><cn type="integer" id="S2.E4.m1.3.3.3.3.1.1.1.3.cmml" xref="S2.E4.m1.3.3.3.3.1.1.1.3">1</cn></apply><apply id="S2.E4.m1.4.4.4.4.2.2.2.cmml" xref="S2.E4.m1.4.4.4.4.2.2.2"><eq id="S2.E4.m1.4.4.4.4.2.2.2.1.cmml" xref="S2.E4.m1.4.4.4.4.2.2.2.1"></eq><ci id="S2.E4.m1.4.4.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.4.4.2.2.2.2">𝐴</ci><cn type="integer" id="S2.E4.m1.4.4.4.4.2.2.2.3.cmml" xref="S2.E4.m1.4.4.4.4.2.2.2.3">1</cn></apply></apply></apply></apply><cn type="integer" id="S2.E4.m1.4.4.6.cmml" xref="S2.E4.m1.4.4.6">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">Pr\{Y_{p}=1|A=0\}-Pr\{Y_{p}=1|A=1\}=0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p9" class="ltx_para">
<p id="S2.SS5.p9.1" class="ltx_p">Demographic parity ensures that all groups receive positive outcomes at the same rate. However, demographic parity can also be used to limit the disparity between the basic ratios of two groups.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The links between PRIVACY, SECURITY AND FAIRNESS</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>The fundamental basis of security and privacy in federated learning</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Gradient sharing</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Sharing of gradients plays a key role in determining the extent of privacy leaks and security threats within the context of federated learning. Unlike traditional distributed learning paradigms that share data, in federated learning, the gradients are shared, but this practice still leaves open vulnerabilities to privacy and security risks.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">In terms of privacy, adversaries can directly manipulate gradients to launch reconstruction attacks, and, in so doing, potentially recover raw data from victim clients. Alternatively, they can indirectly manipulate a gradient to infer sensitive information about a client. In terms of security, adversaries have the ability to directly modify the gradient to poison the model, or they can indirectly manipulate the gradient by tampering with the data, leading to data poisoning. Such data poisoning attacks will compromise the global model’s integrity, creating a significant security threats.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Shared gradients also present a formidable challenge to privacy of federated learning frameworks, especially at the client level. Numerous studies have demonstrated that gradients can be directly used to reconstruct training data  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2019b</a>)</cite>, and that various types of information can be indirectly inferred from analyzing shared gradients  <cite class="ltx_cite ltx_citemacro_citep">(Hitaj et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>; Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>; Fredrikson et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2015</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2019b</a>; Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>; Shokri et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite>. These reconstruction and inference attacks pose a significant threat to client-level privacy.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">Clients are particularly susceptible to such attacks. This is because clients transmit their local parameter updates to the server, and, while an adversary cannot directly access the training data from the server, they can exploit the local parameters and launch a white-box gradient attack aimed at extracting the client’s private training data. Moreover, the server, while ostensibly acting in good faith, may hold a degree of curiosity. It performs parameter aggregation and shares updated global parameters with the
clients as part of the training process. However, an inquisitive server might analyze periodic updates from clients to glean information about them. The server might even deliberately isolate certain clients and engage in an active privacy attack to acquire additional training information. Further, even if the communication between the client and the server is secure, a client under attack may still fall victim to a privacy breach before successfully uploading its local update to the server.</p>
</div>
<div id="S3.SS1.SSS1.p5" class="ltx_para">
<p id="S3.SS1.SSS1.p5.1" class="ltx_p">One notable approach to privacy attacks is the gradient-based reconstruction attack, where the goal is to recover the victim’s training data through gradient matching <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite>. In short, the attack works by generating dummy inputs and labels and then computing a dummy gradient based on this synthetic data. The goal is to minimize the discrepancy between the dummy gradient and the real gradient, effectively bringing the dummy data closer to the real data to facilitate the attack. Often, the attack terminates once the dummy gradient converges to the gradient of the training data <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite>. Alternative methods of this attack have also been explored, such as leveraging the input of a fully connected layer <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2021a</a>)</cite>. Conversely, malicious clients might access the model during each training iteration <cite class="ltx_cite ltx_citemacro_citep">(Hitaj et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> and use generative adversarial networks (GANs) to reconstruct the private data of other clients. In essence, gradient sharing in federated learning presents a complex interplay of privacy and security challenges, necessitating vigilant defenses against a range of attack vectors.</p>
</div>
<div id="S3.SS1.SSS1.p6" class="ltx_para">
<p id="S3.SS1.SSS1.p6.6" class="ltx_p">The recovery framework proposed in many studies <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite> can be viewed as an optimization process. In each <math id="S3.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS1.SSS1.p6.1.m1.1a"><msup id="S3.SS1.SSS1.p6.1.m1.1.1" xref="S3.SS1.SSS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p6.1.m1.1.1.2" xref="S3.SS1.SSS1.p6.1.m1.1.1.2.cmml">i</mi><mrow id="S3.SS1.SSS1.p6.1.m1.1.1.3" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p6.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p6.1.m1.1.1.3.1" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p6.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.1.m1.1b"><apply id="S3.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.2">𝑖</ci><apply id="S3.SS1.SSS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.3"><times id="S3.SS1.SSS1.p6.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS1.p6.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS1.SSS1.p6.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.1.m1.1c">i^{th}</annotation></semantics></math> round of training, the gradient of the data is computed where <math id="S3.SS1.SSS1.p6.2.m2.3" class="ltx_Math" alttext="(x,y,W_{i})" display="inline"><semantics id="S3.SS1.SSS1.p6.2.m2.3a"><mrow id="S3.SS1.SSS1.p6.2.m2.3.3.1" xref="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p6.2.m2.3.3.1.2" xref="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml">(</mo><mi id="S3.SS1.SSS1.p6.2.m2.1.1" xref="S3.SS1.SSS1.p6.2.m2.1.1.cmml">x</mi><mo id="S3.SS1.SSS1.p6.2.m2.3.3.1.3" xref="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS1.SSS1.p6.2.m2.2.2" xref="S3.SS1.SSS1.p6.2.m2.2.2.cmml">y</mi><mo id="S3.SS1.SSS1.p6.2.m2.3.3.1.4" xref="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml">,</mo><msub id="S3.SS1.SSS1.p6.2.m2.3.3.1.1" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1.cmml"><mi id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.2" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.3" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS1.p6.2.m2.3.3.1.5" xref="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.2.m2.3b"><vector id="S3.SS1.SSS1.p6.2.m2.3.3.2.cmml" xref="S3.SS1.SSS1.p6.2.m2.3.3.1"><ci id="S3.SS1.SSS1.p6.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p6.2.m2.1.1">𝑥</ci><ci id="S3.SS1.SSS1.p6.2.m2.2.2.cmml" xref="S3.SS1.SSS1.p6.2.m2.2.2">𝑦</ci><apply id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.cmml" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1.2">𝑊</ci><ci id="S3.SS1.SSS1.p6.2.m2.3.3.1.1.3.cmml" xref="S3.SS1.SSS1.p6.2.m2.3.3.1.1.3">𝑖</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.2.m2.3c">(x,y,W_{i})</annotation></semantics></math>, <math id="S3.SS1.SSS1.p6.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.SSS1.p6.3.m3.1a"><mi id="S3.SS1.SSS1.p6.3.m3.1.1" xref="S3.SS1.SSS1.p6.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.3.m3.1b"><ci id="S3.SS1.SSS1.p6.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p6.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.3.m3.1c">y</annotation></semantics></math> represents the data label, and <math id="S3.SS1.SSS1.p6.4.m4.2" class="ltx_Math" alttext="F(x,W_{i})" display="inline"><semantics id="S3.SS1.SSS1.p6.4.m4.2a"><mrow id="S3.SS1.SSS1.p6.4.m4.2.2" xref="S3.SS1.SSS1.p6.4.m4.2.2.cmml"><mi id="S3.SS1.SSS1.p6.4.m4.2.2.3" xref="S3.SS1.SSS1.p6.4.m4.2.2.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p6.4.m4.2.2.2" xref="S3.SS1.SSS1.p6.4.m4.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS1.p6.4.m4.2.2.1.1" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.2" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.2.cmml">(</mo><mi id="S3.SS1.SSS1.p6.4.m4.1.1" xref="S3.SS1.SSS1.p6.4.m4.1.1.cmml">x</mi><mo id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.3" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.2.cmml">,</mo><msub id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.2" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.3" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.4" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.4.m4.2b"><apply id="S3.SS1.SSS1.p6.4.m4.2.2.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2"><times id="S3.SS1.SSS1.p6.4.m4.2.2.2.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.2"></times><ci id="S3.SS1.SSS1.p6.4.m4.2.2.3.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.3">𝐹</ci><interval closure="open" id="S3.SS1.SSS1.p6.4.m4.2.2.1.2.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1"><ci id="S3.SS1.SSS1.p6.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p6.4.m4.1.1">𝑥</ci><apply id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.2">𝑊</ci><ci id="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS1.p6.4.m4.2.2.1.1.1.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.4.m4.2c">F(x,W_{i})</annotation></semantics></math> represents the differentiable learning model with the parameters <math id="S3.SS1.SSS1.p6.5.m5.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="S3.SS1.SSS1.p6.5.m5.1a"><msub id="S3.SS1.SSS1.p6.5.m5.1.1" xref="S3.SS1.SSS1.p6.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p6.5.m5.1.1.2" xref="S3.SS1.SSS1.p6.5.m5.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS1.p6.5.m5.1.1.3" xref="S3.SS1.SSS1.p6.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.5.m5.1b"><apply id="S3.SS1.SSS1.p6.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p6.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p6.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p6.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p6.5.m5.1.1.2">𝑊</ci><ci id="S3.SS1.SSS1.p6.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p6.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.5.m5.1c">W_{i}</annotation></semantics></math>, <math id="S3.SS1.SSS1.p6.6.m6.1" class="ltx_Math" alttext="x\in R^{n}" display="inline"><semantics id="S3.SS1.SSS1.p6.6.m6.1a"><mrow id="S3.SS1.SSS1.p6.6.m6.1.1" xref="S3.SS1.SSS1.p6.6.m6.1.1.cmml"><mi id="S3.SS1.SSS1.p6.6.m6.1.1.2" xref="S3.SS1.SSS1.p6.6.m6.1.1.2.cmml">x</mi><mo id="S3.SS1.SSS1.p6.6.m6.1.1.1" xref="S3.SS1.SSS1.p6.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p6.6.m6.1.1.3" xref="S3.SS1.SSS1.p6.6.m6.1.1.3.cmml"><mi id="S3.SS1.SSS1.p6.6.m6.1.1.3.2" xref="S3.SS1.SSS1.p6.6.m6.1.1.3.2.cmml">R</mi><mi id="S3.SS1.SSS1.p6.6.m6.1.1.3.3" xref="S3.SS1.SSS1.p6.6.m6.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.6.m6.1b"><apply id="S3.SS1.SSS1.p6.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1"><in id="S3.SS1.SSS1.p6.6.m6.1.1.1.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.1"></in><ci id="S3.SS1.SSS1.p6.6.m6.1.1.2.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.2">𝑥</ci><apply id="S3.SS1.SSS1.p6.6.m6.1.1.3.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p6.6.m6.1.1.3.1.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p6.6.m6.1.1.3.2.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.3.2">𝑅</ci><ci id="S3.SS1.SSS1.p6.6.m6.1.1.3.3.cmml" xref="S3.SS1.SSS1.p6.6.m6.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.6.m6.1c">x\in R^{n}</annotation></semantics></math> is computed:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="\bigtriangledown{W_{i}}=\frac{\partial(F(x,W_{i}),y)}{\partial{W_{i}}}" display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.4" xref="S3.E5.m1.3.4.cmml"><mrow id="S3.E5.m1.3.4.2" xref="S3.E5.m1.3.4.2.cmml"><mo rspace="0em" id="S3.E5.m1.3.4.2a" xref="S3.E5.m1.3.4.2.cmml">▽</mo><msub id="S3.E5.m1.3.4.2.2" xref="S3.E5.m1.3.4.2.2.cmml"><mi id="S3.E5.m1.3.4.2.2.2" xref="S3.E5.m1.3.4.2.2.2.cmml">W</mi><mi id="S3.E5.m1.3.4.2.2.3" xref="S3.E5.m1.3.4.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.E5.m1.3.4.1" xref="S3.E5.m1.3.4.1.cmml">=</mo><mfrac id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml"><mrow id="S3.E5.m1.3.3.3" xref="S3.E5.m1.3.3.3.cmml"><mo rspace="0em" id="S3.E5.m1.3.3.3.4" xref="S3.E5.m1.3.3.3.4.cmml">∂</mo><mrow id="S3.E5.m1.3.3.3.3.1" xref="S3.E5.m1.3.3.3.3.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.3.3.1.2" xref="S3.E5.m1.3.3.3.3.2.cmml">(</mo><mrow id="S3.E5.m1.3.3.3.3.1.1" xref="S3.E5.m1.3.3.3.3.1.1.cmml"><mi id="S3.E5.m1.3.3.3.3.1.1.3" xref="S3.E5.m1.3.3.3.3.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.3.3.1.1.2" xref="S3.E5.m1.3.3.3.3.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.3.3.3.3.1.1.1.1" xref="S3.E5.m1.3.3.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.3.3.1.1.1.1.2" xref="S3.E5.m1.3.3.3.3.1.1.1.2.cmml">(</mo><mi id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">x</mi><mo id="S3.E5.m1.3.3.3.3.1.1.1.1.3" xref="S3.E5.m1.3.3.3.3.1.1.1.2.cmml">,</mo><msub id="S3.E5.m1.3.3.3.3.1.1.1.1.1" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1.cmml"><mi id="S3.E5.m1.3.3.3.3.1.1.1.1.1.2" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1.2.cmml">W</mi><mi id="S3.E5.m1.3.3.3.3.1.1.1.1.1.3" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E5.m1.3.3.3.3.1.1.1.1.4" xref="S3.E5.m1.3.3.3.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.3.3.3.3.1.3" xref="S3.E5.m1.3.3.3.3.2.cmml">,</mo><mi id="S3.E5.m1.2.2.2.2" xref="S3.E5.m1.2.2.2.2.cmml">y</mi><mo stretchy="false" id="S3.E5.m1.3.3.3.3.1.4" xref="S3.E5.m1.3.3.3.3.2.cmml">)</mo></mrow></mrow><mrow id="S3.E5.m1.3.3.5" xref="S3.E5.m1.3.3.5.cmml"><mo rspace="0em" id="S3.E5.m1.3.3.5.1" xref="S3.E5.m1.3.3.5.1.cmml">∂</mo><msub id="S3.E5.m1.3.3.5.2" xref="S3.E5.m1.3.3.5.2.cmml"><mi id="S3.E5.m1.3.3.5.2.2" xref="S3.E5.m1.3.3.5.2.2.cmml">W</mi><mi id="S3.E5.m1.3.3.5.2.3" xref="S3.E5.m1.3.3.5.2.3.cmml">i</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.4.cmml" xref="S3.E5.m1.3.4"><eq id="S3.E5.m1.3.4.1.cmml" xref="S3.E5.m1.3.4.1"></eq><apply id="S3.E5.m1.3.4.2.cmml" xref="S3.E5.m1.3.4.2"><ci id="S3.E5.m1.3.4.2.1.cmml" xref="S3.E5.m1.3.4.2">▽</ci><apply id="S3.E5.m1.3.4.2.2.cmml" xref="S3.E5.m1.3.4.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.4.2.2.1.cmml" xref="S3.E5.m1.3.4.2.2">subscript</csymbol><ci id="S3.E5.m1.3.4.2.2.2.cmml" xref="S3.E5.m1.3.4.2.2.2">𝑊</ci><ci id="S3.E5.m1.3.4.2.2.3.cmml" xref="S3.E5.m1.3.4.2.2.3">𝑖</ci></apply></apply><apply id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3"><divide id="S3.E5.m1.3.3.4.cmml" xref="S3.E5.m1.3.3"></divide><apply id="S3.E5.m1.3.3.3.cmml" xref="S3.E5.m1.3.3.3"><partialdiff id="S3.E5.m1.3.3.3.4.cmml" xref="S3.E5.m1.3.3.3.4"></partialdiff><interval closure="open" id="S3.E5.m1.3.3.3.3.2.cmml" xref="S3.E5.m1.3.3.3.3.1"><apply id="S3.E5.m1.3.3.3.3.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1"><times id="S3.E5.m1.3.3.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.2"></times><ci id="S3.E5.m1.3.3.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.3.3.1.1.3">𝐹</ci><interval closure="open" id="S3.E5.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.1"><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">𝑥</ci><apply id="S3.E5.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.3.3.3.3.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1.2">𝑊</ci><ci id="S3.E5.m1.3.3.3.3.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.1.1.3">𝑖</ci></apply></interval></apply><ci id="S3.E5.m1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2">𝑦</ci></interval></apply><apply id="S3.E5.m1.3.3.5.cmml" xref="S3.E5.m1.3.3.5"><partialdiff id="S3.E5.m1.3.3.5.1.cmml" xref="S3.E5.m1.3.3.5.1"></partialdiff><apply id="S3.E5.m1.3.3.5.2.cmml" xref="S3.E5.m1.3.3.5.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.5.2.1.cmml" xref="S3.E5.m1.3.3.5.2">subscript</csymbol><ci id="S3.E5.m1.3.3.5.2.2.cmml" xref="S3.E5.m1.3.3.5.2.2">𝑊</ci><ci id="S3.E5.m1.3.3.5.2.3.cmml" xref="S3.E5.m1.3.3.5.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\bigtriangledown{W_{i}}=\frac{\partial(F(x,W_{i}),y)}{\partial{W_{i}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS1.p7" class="ltx_para">
<p id="S3.SS1.SSS1.p7.3" class="ltx_p">The dummy data<math id="S3.SS1.SSS1.p7.1.m1.2" class="ltx_Math" alttext="(x^{{}^{\prime}},y^{{}^{\prime}})" display="inline"><semantics id="S3.SS1.SSS1.p7.1.m1.2a"><mrow id="S3.SS1.SSS1.p7.1.m1.2.2.2" xref="S3.SS1.SSS1.p7.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p7.1.m1.2.2.2.3" xref="S3.SS1.SSS1.p7.1.m1.2.2.3.cmml">(</mo><msup id="S3.SS1.SSS1.p7.1.m1.1.1.1.1" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.2.cmml">x</mi><msup id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3a" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.cmml"></mi><mo id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.1" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.1.cmml">′</mo></msup></msup><mo id="S3.SS1.SSS1.p7.1.m1.2.2.2.4" xref="S3.SS1.SSS1.p7.1.m1.2.2.3.cmml">,</mo><msup id="S3.SS1.SSS1.p7.1.m1.2.2.2.2" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.2" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.2.cmml">y</mi><msup id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.cmml"><mi id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3a" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.cmml"></mi><mo id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.1" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.1.cmml">′</mo></msup></msup><mo stretchy="false" id="S3.SS1.SSS1.p7.1.m1.2.2.2.5" xref="S3.SS1.SSS1.p7.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p7.1.m1.2b"><interval closure="open" id="S3.SS1.SSS1.p7.1.m1.2.2.3.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2"><apply id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.2">𝑥</ci><apply id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3"><ci id="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p7.1.m1.1.1.1.1.3.1">′</ci></apply></apply><apply id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2">superscript</csymbol><ci id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.2">𝑦</ci><apply id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3"><ci id="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.1.cmml" xref="S3.SS1.SSS1.p7.1.m1.2.2.2.2.3.1">′</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p7.1.m1.2c">(x^{{}^{\prime}},y^{{}^{\prime}})</annotation></semantics></math> can be set randomly, where <math id="S3.SS1.SSS1.p7.2.m2.1" class="ltx_Math" alttext="x^{{}^{\prime}}" display="inline"><semantics id="S3.SS1.SSS1.p7.2.m2.1a"><msup id="S3.SS1.SSS1.p7.2.m2.1.1" xref="S3.SS1.SSS1.p7.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p7.2.m2.1.1.2" xref="S3.SS1.SSS1.p7.2.m2.1.1.2.cmml">x</mi><msup id="S3.SS1.SSS1.p7.2.m2.1.1.3" xref="S3.SS1.SSS1.p7.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p7.2.m2.1.1.3a" xref="S3.SS1.SSS1.p7.2.m2.1.1.3.cmml"></mi><mo id="S3.SS1.SSS1.p7.2.m2.1.1.3.1" xref="S3.SS1.SSS1.p7.2.m2.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p7.2.m2.1b"><apply id="S3.SS1.SSS1.p7.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p7.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p7.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p7.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p7.2.m2.1.1.2">𝑥</ci><apply id="S3.SS1.SSS1.p7.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p7.2.m2.1.1.3"><ci id="S3.SS1.SSS1.p7.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p7.2.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p7.2.m2.1c">x^{{}^{\prime}}</annotation></semantics></math> and <math id="S3.SS1.SSS1.p7.3.m3.1" class="ltx_Math" alttext="y^{{}^{\prime}}" display="inline"><semantics id="S3.SS1.SSS1.p7.3.m3.1a"><msup id="S3.SS1.SSS1.p7.3.m3.1.1" xref="S3.SS1.SSS1.p7.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p7.3.m3.1.1.2" xref="S3.SS1.SSS1.p7.3.m3.1.1.2.cmml">y</mi><msup id="S3.SS1.SSS1.p7.3.m3.1.1.3" xref="S3.SS1.SSS1.p7.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS1.p7.3.m3.1.1.3a" xref="S3.SS1.SSS1.p7.3.m3.1.1.3.cmml"></mi><mo id="S3.SS1.SSS1.p7.3.m3.1.1.3.1" xref="S3.SS1.SSS1.p7.3.m3.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p7.3.m3.1b"><apply id="S3.SS1.SSS1.p7.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p7.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p7.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p7.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p7.3.m3.1.1.2">𝑦</ci><apply id="S3.SS1.SSS1.p7.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p7.3.m3.1.1.3"><ci id="S3.SS1.SSS1.p7.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p7.3.m3.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p7.3.m3.1c">y^{{}^{\prime}}</annotation></semantics></math> are the input and label input of the dummy data, respectively. These dummy data are fed into the model to compute the gradient:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\bigtriangledown{W^{{}^{\prime}}_{i}}=\frac{\partial(F(x^{{}^{\prime}},W_{i}),y^{{}^{\prime}})}{\partial{W_{i}}}" display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.3" xref="S3.E6.m1.2.3.cmml"><mrow id="S3.E6.m1.2.3.2" xref="S3.E6.m1.2.3.2.cmml"><mo rspace="0em" id="S3.E6.m1.2.3.2a" xref="S3.E6.m1.2.3.2.cmml">▽</mo><msubsup id="S3.E6.m1.2.3.2.2" xref="S3.E6.m1.2.3.2.2.cmml"><mi id="S3.E6.m1.2.3.2.2.2.2" xref="S3.E6.m1.2.3.2.2.2.2.cmml">W</mi><mi id="S3.E6.m1.2.3.2.2.3" xref="S3.E6.m1.2.3.2.2.3.cmml">i</mi><msup id="S3.E6.m1.2.3.2.2.2.3" xref="S3.E6.m1.2.3.2.2.2.3.cmml"><mi id="S3.E6.m1.2.3.2.2.2.3a" xref="S3.E6.m1.2.3.2.2.2.3.cmml"></mi><mo id="S3.E6.m1.2.3.2.2.2.3.1" xref="S3.E6.m1.2.3.2.2.2.3.1.cmml">′</mo></msup></msubsup></mrow><mo id="S3.E6.m1.2.3.1" xref="S3.E6.m1.2.3.1.cmml">=</mo><mfrac id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml"><mrow id="S3.E6.m1.2.2.2" xref="S3.E6.m1.2.2.2.cmml"><mo rspace="0em" id="S3.E6.m1.2.2.2.3" xref="S3.E6.m1.2.2.2.3.cmml">∂</mo><mrow id="S3.E6.m1.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.3.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.1.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">(</mo><msup id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><msup id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml"></mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">′</mo></msup></msup><mo id="S3.E6.m1.1.1.1.1.1.1.2.2.4" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E6.m1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml">W</mi><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.2.2.5" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.2.2.2.2.2.4" xref="S3.E6.m1.2.2.2.2.3.cmml">,</mo><msup id="S3.E6.m1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.2.cmml">y</mi><msup id="S3.E6.m1.2.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.3a" xref="S3.E6.m1.2.2.2.2.2.2.3.cmml"></mi><mo id="S3.E6.m1.2.2.2.2.2.2.3.1" xref="S3.E6.m1.2.2.2.2.2.2.3.1.cmml">′</mo></msup></msup><mo stretchy="false" id="S3.E6.m1.2.2.2.2.2.5" xref="S3.E6.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E6.m1.2.2.4" xref="S3.E6.m1.2.2.4.cmml"><mo rspace="0em" id="S3.E6.m1.2.2.4.1" xref="S3.E6.m1.2.2.4.1.cmml">∂</mo><msub id="S3.E6.m1.2.2.4.2" xref="S3.E6.m1.2.2.4.2.cmml"><mi id="S3.E6.m1.2.2.4.2.2" xref="S3.E6.m1.2.2.4.2.2.cmml">W</mi><mi id="S3.E6.m1.2.2.4.2.3" xref="S3.E6.m1.2.2.4.2.3.cmml">i</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.3.cmml" xref="S3.E6.m1.2.3"><eq id="S3.E6.m1.2.3.1.cmml" xref="S3.E6.m1.2.3.1"></eq><apply id="S3.E6.m1.2.3.2.cmml" xref="S3.E6.m1.2.3.2"><ci id="S3.E6.m1.2.3.2.1.cmml" xref="S3.E6.m1.2.3.2">▽</ci><apply id="S3.E6.m1.2.3.2.2.cmml" xref="S3.E6.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.3.2.2.1.cmml" xref="S3.E6.m1.2.3.2.2">subscript</csymbol><apply id="S3.E6.m1.2.3.2.2.2.cmml" xref="S3.E6.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.3.2.2.2.1.cmml" xref="S3.E6.m1.2.3.2.2">superscript</csymbol><ci id="S3.E6.m1.2.3.2.2.2.2.cmml" xref="S3.E6.m1.2.3.2.2.2.2">𝑊</ci><apply id="S3.E6.m1.2.3.2.2.2.3.cmml" xref="S3.E6.m1.2.3.2.2.2.3"><ci id="S3.E6.m1.2.3.2.2.2.3.1.cmml" xref="S3.E6.m1.2.3.2.2.2.3.1">′</ci></apply></apply><ci id="S3.E6.m1.2.3.2.2.3.cmml" xref="S3.E6.m1.2.3.2.2.3">𝑖</ci></apply></apply><apply id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2"><divide id="S3.E6.m1.2.2.3.cmml" xref="S3.E6.m1.2.2"></divide><apply id="S3.E6.m1.2.2.2.cmml" xref="S3.E6.m1.2.2.2"><partialdiff id="S3.E6.m1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.3"></partialdiff><interval closure="open" id="S3.E6.m1.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2"><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"></times><ci id="S3.E6.m1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.4">𝐹</ci><interval closure="open" id="S3.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2"><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1">′</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.2">𝑊</ci><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E6.m1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E6.m1.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2">𝑦</ci><apply id="S3.E6.m1.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3"><ci id="S3.E6.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.1">′</ci></apply></apply></interval></apply><apply id="S3.E6.m1.2.2.4.cmml" xref="S3.E6.m1.2.2.4"><partialdiff id="S3.E6.m1.2.2.4.1.cmml" xref="S3.E6.m1.2.2.4.1"></partialdiff><apply id="S3.E6.m1.2.2.4.2.cmml" xref="S3.E6.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.4.2.1.cmml" xref="S3.E6.m1.2.2.4.2">subscript</csymbol><ci id="S3.E6.m1.2.2.4.2.2.cmml" xref="S3.E6.m1.2.2.4.2.2">𝑊</ci><ci id="S3.E6.m1.2.2.4.2.3.cmml" xref="S3.E6.m1.2.2.4.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\bigtriangledown{W^{{}^{\prime}}_{i}}=\frac{\partial(F(x^{{}^{\prime}},W_{i}),y^{{}^{\prime}})}{\partial{W_{i}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS1.p8" class="ltx_para">
<p id="S3.SS1.SSS1.p8.1" class="ltx_p">The goal of training is to minimize the difference between the dummy gradient and the shared gradient so that the dummy data will be close to the actual training data. This process continues until the reconstructed data converges to the actual training data.</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_math_unparsed" alttext="\bigtriangledown W_{G}=arg\min{||}\bigtriangledown{W}-\bigtriangledown W^{{}^{\prime}}{||}^{2}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1b"><mo rspace="0.222em" id="S3.E7.m1.1.1">▽</mo><msub id="S3.E7.m1.1.2"><mi id="S3.E7.m1.1.2.2">W</mi><mi id="S3.E7.m1.1.2.3">G</mi></msub><mo id="S3.E7.m1.1.3">=</mo><mi id="S3.E7.m1.1.4">a</mi><mi id="S3.E7.m1.1.5">r</mi><mi id="S3.E7.m1.1.6">g</mi><mi id="S3.E7.m1.1.7">min</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E7.m1.1.8">|</mo><mo fence="false" stretchy="false" id="S3.E7.m1.1.9">|</mo><mo lspace="0em" rspace="0.222em" id="S3.E7.m1.1.10">▽</mo><mi id="S3.E7.m1.1.11">W</mi><mo rspace="0em" id="S3.E7.m1.1.12">−</mo><mo lspace="0em" rspace="0.222em" id="S3.E7.m1.1.13">▽</mo><msup id="S3.E7.m1.1.14"><mi id="S3.E7.m1.1.14.2">W</mi><msup id="S3.E7.m1.1.14.3"><mi id="S3.E7.m1.1.14.3a"></mi><mo id="S3.E7.m1.1.14.3.1">′</mo></msup></msup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E7.m1.1.15">|</mo><msup id="S3.E7.m1.1.16"><mo fence="false" stretchy="false" id="S3.E7.m1.1.16.2">|</mo><mn id="S3.E7.m1.1.16.3">2</mn></msup></mrow><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\bigtriangledown W_{G}=arg\min{||}\bigtriangledown{W}-\bigtriangledown W^{{}^{\prime}}{||}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Making inferences from the gradient</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In the realm of machine learning, there are many types of inference attacks, with membership inference attacks and property inference attacks being among the most common. These attacks can manifest in both black-box and white-box settings and can be further categorized into active and passive forms. In the case of membership inference attacks <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021</a>)</cite>, machine learning models can exhibit distinct behaviors and parameters based on the training data of different clients. These discrepancies can inadvertently encode information about the training data, allowing the adversary to construct a threat model. This threat model will let the adversary differentiate between members and non-members, making membership inference attacks a central concern for systems that rely on a FedAvg algorithm.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Moreover, membership inference attacks can directly affect the gradient, potentially triggering the target model. Such attacks may involve active tampering with the training model’s gradient  <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite> or observing non-zero gradients to infer the presence of specific words in the training dataset <cite class="ltx_cite ltx_citemacro_citep">(Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>)</cite>. Model poisoning attacks <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2021</a>; Shejwalkar and Houmansadr, <a href="#bib.bib109" title="" class="ltx_ref">2021</a>; Shejwalkar et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite> involve malicious clients directly tampering with model updates during training, such as maliciously manipulating a gradient directly to degrade the accuracy of the global model, or doing so indirectly via a data poisoning attack <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite>. Here, a malicious client will contaminate the training data to contaminate the resulting gradient. However, direct manipulations of the gradient are the more potent form of attack.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.3" class="ltx_p">Notably, poisoning attacks can occur with or without knowledge of the server’s aggregation rules. Following an attack, the global model parameters shift in the direction influenced by the attack. because the server will aggregate both the benign and contaminated gradients to produce the global model. Here, the manipulated malicious gradient is denoted as <math id="S3.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="{\bigtriangledown}^{m}_{{i\in[m]}}" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><msubsup id="S3.SS1.SSS2.p3.1.m1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.2.cmml"><mo id="S3.SS1.SSS2.p3.1.m1.1.2.2.2" xref="S3.SS1.SSS2.p3.1.m1.1.2.2.2.cmml">▽</mo><mrow id="S3.SS1.SSS2.p3.1.m1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.3.cmml">i</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.2.cmml">∈</mo><mrow id="S3.SS1.SSS2.p3.1.m1.1.1.1.4.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p3.1.m1.1.1.1.4.2.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S3.SS1.SSS2.p3.1.m1.1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.cmml">m</mi><mo stretchy="false" id="S3.SS1.SSS2.p3.1.m1.1.1.1.4.2.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow><mi id="S3.SS1.SSS2.p3.1.m1.1.2.2.3" xref="S3.SS1.SSS2.p3.1.m1.1.2.2.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.2.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2">subscript</csymbol><apply id="S3.SS1.SSS2.p3.1.m1.1.2.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.2.2.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2">superscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.2.2.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2.2.2">▽</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.2.2.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.2.2.3">𝑚</ci></apply><apply id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1"><in id="S3.SS1.SSS2.p3.1.m1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.2"></in><ci id="S3.SS1.SSS2.p3.1.m1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.3">𝑖</ci><apply id="S3.SS1.SSS2.p3.1.m1.1.1.1.4.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.4.2"><csymbol cd="latexml" id="S3.SS1.SSS2.p3.1.m1.1.1.1.4.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">{\bigtriangledown}^{m}_{{i\in[m]}}</annotation></semantics></math>, while <math id="S3.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="{\bigtriangledown}^{b}" display="inline"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><msup id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mo id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">▽</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml">b</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">▽</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">{\bigtriangledown}^{b}</annotation></semantics></math> represents the aggregation of benign gradients from clients. After the final poisoning, the model is updated to <math id="S3.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="{\bigtriangledown}^{p}" display="inline"><semantics id="S3.SS1.SSS2.p3.3.m3.1a"><msup id="S3.SS1.SSS2.p3.3.m3.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.cmml"><mo id="S3.SS1.SSS2.p3.3.m3.1.1.2" xref="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml">▽</mo><mi id="S3.SS1.SSS2.p3.3.m3.1.1.3" xref="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml">p</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.3.m3.1b"><apply id="S3.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.2">▽</ci><ci id="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.3.m3.1c">{\bigtriangledown}^{p}</annotation></semantics></math>. Importantly, once the global model is contaminated, its impact persists in subsequent training rounds, even without further attacks <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.8" class="ltx_p">Additionally, clients can demonstrate malicious behavior or can fall victim to an adversary’s attacks, which might potentially involve including counterfeit or harmful samples in their local training data. Adversaries can also poison the global model by manipulating or influencing benign clients under the assumption that the server is honest and uncompromised. Data poisoning attacks mainly follow one of two key methods: label flipping <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite> and backdoor attacks  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2020a</a>)</cite>. In label flipping attacks, adversaries tamper with the labels of training data but do not change the data’s features. Conversely, backdoor attacks involve modifying a single feature or a small subset of the raw training data and labeling it as a specific target class. Another approach to contaminating data is by maintaining the labels but introducing malicious data. Label flipping is often used as part of a poisoning attack. Within the training data, certain data initially belonging to source class <math id="S3.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="c_{s}" display="inline"><semantics id="S3.SS1.SSS2.p4.1.m1.1a"><msub id="S3.SS1.SSS2.p4.1.m1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p4.1.m1.1.1.2" xref="S3.SS1.SSS2.p4.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS2.p4.1.m1.1.1.3" xref="S3.SS1.SSS2.p4.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.1.m1.1b"><apply id="S3.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.1.m1.1c">c_{s}</annotation></semantics></math> is altered to be classified as target class <math id="S3.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="c_{m}" display="inline"><semantics id="S3.SS1.SSS2.p4.2.m2.1a"><msub id="S3.SS1.SSS2.p4.2.m2.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p4.2.m2.1.1.2" xref="S3.SS1.SSS2.p4.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS2.p4.2.m2.1.1.3" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.2.m2.1b"><apply id="S3.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.2.m2.1c">c_{m}</annotation></semantics></math>, represented as <math id="S3.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="c_{s}\to c_{m}" display="inline"><semantics id="S3.SS1.SSS2.p4.3.m3.1a"><mrow id="S3.SS1.SSS2.p4.3.m3.1.1" xref="S3.SS1.SSS2.p4.3.m3.1.1.cmml"><msub id="S3.SS1.SSS2.p4.3.m3.1.1.2" xref="S3.SS1.SSS2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS1.SSS2.p4.3.m3.1.1.2.2" xref="S3.SS1.SSS2.p4.3.m3.1.1.2.2.cmml">c</mi><mi id="S3.SS1.SSS2.p4.3.m3.1.1.2.3" xref="S3.SS1.SSS2.p4.3.m3.1.1.2.3.cmml">s</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p4.3.m3.1.1.1" xref="S3.SS1.SSS2.p4.3.m3.1.1.1.cmml">→</mo><msub id="S3.SS1.SSS2.p4.3.m3.1.1.3" xref="S3.SS1.SSS2.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS2.p4.3.m3.1.1.3.2" xref="S3.SS1.SSS2.p4.3.m3.1.1.3.2.cmml">c</mi><mi id="S3.SS1.SSS2.p4.3.m3.1.1.3.3" xref="S3.SS1.SSS2.p4.3.m3.1.1.3.3.cmml">m</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.3.m3.1b"><apply id="S3.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1"><ci id="S3.SS1.SSS2.p4.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.1">→</ci><apply id="S3.SS1.SSS2.p4.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.2.2">𝑐</ci><ci id="S3.SS1.SSS2.p4.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.2.3">𝑠</ci></apply><apply id="S3.SS1.SSS2.p4.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS2.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.3.2">𝑐</ci><ci id="S3.SS1.SSS2.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.3.m3.1c">c_{s}\to c_{m}</annotation></semantics></math>. Despite modifying the data <math id="S3.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS2.p4.4.m4.1a"><msub id="S3.SS1.SSS2.p4.4.m4.1.1" xref="S3.SS1.SSS2.p4.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p4.4.m4.1.1.2" xref="S3.SS1.SSS2.p4.4.m4.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS2.p4.4.m4.1.1.3" xref="S3.SS1.SSS2.p4.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.4.m4.1b"><apply id="S3.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1.2">𝐷</ci><ci id="S3.SS1.SSS2.p4.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.4.m4.1c">D_{i}</annotation></semantics></math>, this also impacts the locally trained model <math id="S3.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="{\bigtriangledown}^{l}" display="inline"><semantics id="S3.SS1.SSS2.p4.5.m5.1a"><msup id="S3.SS1.SSS2.p4.5.m5.1.1" xref="S3.SS1.SSS2.p4.5.m5.1.1.cmml"><mo id="S3.SS1.SSS2.p4.5.m5.1.1.2" xref="S3.SS1.SSS2.p4.5.m5.1.1.2.cmml">▽</mo><mi id="S3.SS1.SSS2.p4.5.m5.1.1.3" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.5.m5.1b"><apply id="S3.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.2">▽</ci><ci id="S3.SS1.SSS2.p4.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.5.m5.1c">{\bigtriangledown}^{l}</annotation></semantics></math>. A malicious gradient <math id="S3.SS1.SSS2.p4.6.m6.1" class="ltx_Math" alttext="g_{m}" display="inline"><semantics id="S3.SS1.SSS2.p4.6.m6.1a"><msub id="S3.SS1.SSS2.p4.6.m6.1.1" xref="S3.SS1.SSS2.p4.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p4.6.m6.1.1.2" xref="S3.SS1.SSS2.p4.6.m6.1.1.2.cmml">g</mi><mi id="S3.SS1.SSS2.p4.6.m6.1.1.3" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.6.m6.1b"><apply id="S3.SS1.SSS2.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p4.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.2">𝑔</ci><ci id="S3.SS1.SSS2.p4.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.6.m6.1c">g_{m}</annotation></semantics></math> and is sent to the server, which is then aggregated, creating a malicious global model <math id="S3.SS1.SSS2.p4.7.m7.1" class="ltx_Math" alttext="{\bigtriangledown}^{m}" display="inline"><semantics id="S3.SS1.SSS2.p4.7.m7.1a"><msup id="S3.SS1.SSS2.p4.7.m7.1.1" xref="S3.SS1.SSS2.p4.7.m7.1.1.cmml"><mo id="S3.SS1.SSS2.p4.7.m7.1.1.2" xref="S3.SS1.SSS2.p4.7.m7.1.1.2.cmml">▽</mo><mi id="S3.SS1.SSS2.p4.7.m7.1.1.3" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.cmml">m</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.7.m7.1b"><apply id="S3.SS1.SSS2.p4.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.2">▽</ci><ci id="S3.SS1.SSS2.p4.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.7.m7.1c">{\bigtriangledown}^{m}</annotation></semantics></math> instead of a benign global model <math id="S3.SS1.SSS2.p4.8.m8.1" class="ltx_Math" alttext="{\bigtriangledown}^{b}" display="inline"><semantics id="S3.SS1.SSS2.p4.8.m8.1a"><msup id="S3.SS1.SSS2.p4.8.m8.1.1" xref="S3.SS1.SSS2.p4.8.m8.1.1.cmml"><mo id="S3.SS1.SSS2.p4.8.m8.1.1.2" xref="S3.SS1.SSS2.p4.8.m8.1.1.2.cmml">▽</mo><mi id="S3.SS1.SSS2.p4.8.m8.1.1.3" xref="S3.SS1.SSS2.p4.8.m8.1.1.3.cmml">b</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.8.m8.1b"><apply id="S3.SS1.SSS2.p4.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.8.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p4.8.m8.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p4.8.m8.1.1.2">▽</ci><ci id="S3.SS1.SSS2.p4.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p4.8.m8.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.8.m8.1c">{\bigtriangledown}^{b}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">The issues associated with sharing gradients span privacy and security attacks, all stemming from the act of sharing gradients that subsequently affects the global training scheme. Further, these attacks can involve manipulating the gradient either directly or indirectly to achieve their objectives. Addressing these gradient-sharing issues represents a significant challenge in thwarting adversary-initiated attacks within federated learning frameworks.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Fairness as a bridge</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In federated learning, the relationship between privacy and security serves as a pivotal bridge that now also spans fairness considerations thanks to recent research. Within this intricate web of interdependencies, there exists several discernible trade-offs: one between privacy and fairness and another between security and fairness. Striking a balance between safeguarding the privacy of individual clients and upholding the principles of fairness has become a particularly paramount concern. Here, it is crucial to acknowledge that privacy-preserving technologies, while indispensable, can exert a negative influence on fairness. Consequently, we must meticulously scrutinize the intricate relationship between privacy and fairness.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Simultaneously, the security challenges that federated learning grapples with can also potentially give rise to issues of unfairness. This necessitates another thorough examination this time of the trade-off between security and fairness.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The origin of fairness and bias in federated learning is multifaceted. We have known for some time that machine learning models often exhibit unexpected behaviors that have inadvertently led to some models reflecting negatively and unfairly on specific user groups. This discrimination encompasses traditional bias, disparities arising from heterogeneous data sources, selection bias in the choice of participating parties, as well as bias introduced during the aggregation of algorithms. Each of these factors contributes to the complex landscape of fairness concerns within federated learning.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Traditional biases,</span> akin to that observed in centralized machine learning, can also be persist in federated learning. Such biases includes prejudice, underestimation, and the influence of historical negative patterns <cite class="ltx_cite ltx_citemacro_citep">(Kamishima et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2012</a>)</cite>. In federated learning, where multiple clients participate, each entity contributes to the global model, potentially introducing or reinforcing biases during the process of updating their individual models. Additionally, the dynamics of interaction between clients and the central server throughout the training phase can further impact the fairness of the final model outcomes.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Data heterogeneity.</span> Bias can also stem from the inherent heterogeneity of data in federated learning. Distinct clients possess unique data characteristics, encompassing variations in data distribution and dataset sizes. For example, if all clients used the same local batch size, those with larger datasets would require more training steps for their local models. Consequently, this might result in a substantial disparity in model outcomes. Hence, the selection of clients for each communication round can significantly influence the fairness of the final model.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Party selection and drop-outs.</span> The process of selecting which clients will participate can also introduce fairness biases. During each communication round, the server randomly chooses a subset of clients to collaborate in training. However, there is no inherent guarantee that this selection process will faithfully mirror the true distribution of the population. Additionally, the criteria for client participation may in itself be biased. For instance, if the server repeatedly favors particular types of clients, it can lead to an over-representation of that client group in the final aggregation model, i.e., bias.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Algorithm biases</span> can also emerge due to the influence of the ”majority rule”. As an example, an algorithm might assign a greater weight to clients with larger datasets, potentially magnifying the impact of over or under-representing specific clients in the dataset. Consequently, when the global model undergoes training across multiple clients, those with more extensive training data tend to exhibit lower error rates compared to clients with more limited datasets, exacerbating potential fairness concerns.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>The trade-off between privacy and fairness</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>The impact of privacy on fairness</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Federated learning ostensibly prioritizes the protection of privacy, but this focus may come at the cost of some sacrifice to fairness. Homomorphic encryption and differential privacy are common techniques for addressing privacy concerns. However, the cost associated with implementing these methods is a decline in model accuracy, which, in turn, contributes to a “poor get poorer” effect, potentially exacerbating inequality.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">For instance, consider a facial image model that uses differential privacy to protect gender and age classifications. An unfair model might see individuals with darker skin tones suffer greater decreases in accuracy than their lighter-skinned counterparts. Likewise, within deep models, applying differential privacy can result in varying levels of accuracy decline, with more pronounced decreases in models that were originally less accurate.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.12" class="ltx_p">The standard definition of differential privacy, as introduced by Dwork  <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib37" title="" class="ltx_ref">2006</a>)</cite>, stipulates that a random mechanism denoted as <math id="S3.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="M:D\to R" display="inline"><semantics id="S3.SS3.SSS1.p3.1.m1.1a"><mrow id="S3.SS3.SSS1.p3.1.m1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p3.1.m1.1.1.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.2.cmml">M</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS3.SSS1.p3.1.m1.1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.cmml">:</mo><mrow id="S3.SS3.SSS1.p3.1.m1.1.1.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p3.1.m1.1.1.3.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.2.cmml">D</mi><mo stretchy="false" id="S3.SS3.SSS1.p3.1.m1.1.1.3.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.1.cmml">→</mo><mi id="S3.SS3.SSS1.p3.1.m1.1.1.3.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.3.cmml">R</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.1.m1.1b"><apply id="S3.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1"><ci id="S3.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1">:</ci><ci id="S3.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.2">𝑀</ci><apply id="S3.SS3.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.3"><ci id="S3.SS3.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.1">→</ci><ci id="S3.SS3.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.2">𝐷</ci><ci id="S3.SS3.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.3">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.1.m1.1c">M:D\to R</annotation></semantics></math>, with a domain <math id="S3.SS3.SSS1.p3.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS3.SSS1.p3.2.m2.1a"><mi id="S3.SS3.SSS1.p3.2.m2.1.1" xref="S3.SS3.SSS1.p3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.2.m2.1b"><ci id="S3.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.2.m2.1c">D</annotation></semantics></math> and range <math id="S3.SS3.SSS1.p3.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS3.SSS1.p3.3.m3.1a"><mi id="S3.SS3.SSS1.p3.3.m3.1.1" xref="S3.SS3.SSS1.p3.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.3.m3.1b"><ci id="S3.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p3.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.3.m3.1c">R</annotation></semantics></math>, satisfies <math id="S3.SS3.SSS1.p3.4.m4.2" class="ltx_Math" alttext="(\epsilon,\delta)" display="inline"><semantics id="S3.SS3.SSS1.p3.4.m4.2a"><mrow id="S3.SS3.SSS1.p3.4.m4.2.3.2" xref="S3.SS3.SSS1.p3.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.4.m4.2.3.2.1" xref="S3.SS3.SSS1.p3.4.m4.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS1.p3.4.m4.1.1" xref="S3.SS3.SSS1.p3.4.m4.1.1.cmml">ϵ</mi><mo id="S3.SS3.SSS1.p3.4.m4.2.3.2.2" xref="S3.SS3.SSS1.p3.4.m4.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p3.4.m4.2.2" xref="S3.SS3.SSS1.p3.4.m4.2.2.cmml">δ</mi><mo stretchy="false" id="S3.SS3.SSS1.p3.4.m4.2.3.2.3" xref="S3.SS3.SSS1.p3.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.4.m4.2b"><interval closure="open" id="S3.SS3.SSS1.p3.4.m4.2.3.1.cmml" xref="S3.SS3.SSS1.p3.4.m4.2.3.2"><ci id="S3.SS3.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1">italic-ϵ</ci><ci id="S3.SS3.SSS1.p3.4.m4.2.2.cmml" xref="S3.SS3.SSS1.p3.4.m4.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.4.m4.2c">(\epsilon,\delta)</annotation></semantics></math>-differential privacy if, for any two neighboring datasets <math id="S3.SS3.SSS1.p3.5.m5.2" class="ltx_Math" alttext="d,d^{\prime}\in D" display="inline"><semantics id="S3.SS3.SSS1.p3.5.m5.2a"><mrow id="S3.SS3.SSS1.p3.5.m5.2.2" xref="S3.SS3.SSS1.p3.5.m5.2.2.cmml"><mrow id="S3.SS3.SSS1.p3.5.m5.2.2.1.1" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.2.cmml"><mi id="S3.SS3.SSS1.p3.5.m5.1.1" xref="S3.SS3.SSS1.p3.5.m5.1.1.cmml">d</mi><mo id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.2" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.2.cmml">,</mo><msup id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.2" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.2.cmml">d</mi><mo id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.3" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.3.cmml">′</mo></msup></mrow><mo id="S3.SS3.SSS1.p3.5.m5.2.2.2" xref="S3.SS3.SSS1.p3.5.m5.2.2.2.cmml">∈</mo><mi id="S3.SS3.SSS1.p3.5.m5.2.2.3" xref="S3.SS3.SSS1.p3.5.m5.2.2.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.5.m5.2b"><apply id="S3.SS3.SSS1.p3.5.m5.2.2.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2"><in id="S3.SS3.SSS1.p3.5.m5.2.2.2.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.2"></in><list id="S3.SS3.SSS1.p3.5.m5.2.2.1.2.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1"><ci id="S3.SS3.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p3.5.m5.1.1">𝑑</ci><apply id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.2">𝑑</ci><ci id="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.1.1.1.3">′</ci></apply></list><ci id="S3.SS3.SSS1.p3.5.m5.2.2.3.cmml" xref="S3.SS3.SSS1.p3.5.m5.2.2.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.5.m5.2c">d,d^{\prime}\in D</annotation></semantics></math>, and for any subset of outputs <math id="S3.SS3.SSS1.p3.6.m6.1" class="ltx_Math" alttext="S\subseteq R" display="inline"><semantics id="S3.SS3.SSS1.p3.6.m6.1a"><mrow id="S3.SS3.SSS1.p3.6.m6.1.1" xref="S3.SS3.SSS1.p3.6.m6.1.1.cmml"><mi id="S3.SS3.SSS1.p3.6.m6.1.1.2" xref="S3.SS3.SSS1.p3.6.m6.1.1.2.cmml">S</mi><mo id="S3.SS3.SSS1.p3.6.m6.1.1.1" xref="S3.SS3.SSS1.p3.6.m6.1.1.1.cmml">⊆</mo><mi id="S3.SS3.SSS1.p3.6.m6.1.1.3" xref="S3.SS3.SSS1.p3.6.m6.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.6.m6.1b"><apply id="S3.SS3.SSS1.p3.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p3.6.m6.1.1"><subset id="S3.SS3.SSS1.p3.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p3.6.m6.1.1.1"></subset><ci id="S3.SS3.SSS1.p3.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p3.6.m6.1.1.2">𝑆</ci><ci id="S3.SS3.SSS1.p3.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p3.6.m6.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.6.m6.1c">S\subseteq R</annotation></semantics></math>, the following condition holds: <math id="S3.SS3.SSS1.p3.7.m7.3" class="ltx_Math" alttext="Pr[M(d)\in S]\leq e^{\epsilon}\cdot Pr[M(d^{\prime})\in S]+\delta" display="inline"><semantics id="S3.SS3.SSS1.p3.7.m7.3a"><mrow id="S3.SS3.SSS1.p3.7.m7.3.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.cmml"><mrow id="S3.SS3.SSS1.p3.7.m7.2.2.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.cmml"><mi id="S3.SS3.SSS1.p3.7.m7.2.2.1.3" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.2.2.1.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.2.cmml">​</mo><mi id="S3.SS3.SSS1.p3.7.m7.2.2.1.4" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.2.2.1.2a" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.2.1.cmml">[</mo><mrow id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.cmml"><mrow id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.3.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.3.2.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.SS3.SSS1.p3.7.m7.1.1" xref="S3.SS3.SSS1.p3.7.m7.1.1.cmml">d</mi><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.3.2.2" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.1.cmml">∈</mo><mi id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.7.m7.3.3.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.3.cmml">≤</mo><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.cmml"><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.cmml"><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.cmml"><msup id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.cmml"><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.2.cmml">e</mi><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.3.cmml">ϵ</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.1.cmml">⋅</mo><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2.cmml">​</mo><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.4" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2a" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.2.1.cmml">[</mo><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.cmml"><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.2.cmml">d</mi><mo id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.2.cmml">∈</mo><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.7.m7.3.3.2.2" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.2.cmml">+</mo><mi id="S3.SS3.SSS1.p3.7.m7.3.3.2.3" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.3.cmml">δ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.7.m7.3b"><apply id="S3.SS3.SSS1.p3.7.m7.3.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3"><leq id="S3.SS3.SSS1.p3.7.m7.3.3.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.3"></leq><apply id="S3.SS3.SSS1.p3.7.m7.2.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1"><times id="S3.SS3.SSS1.p3.7.m7.2.2.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.2"></times><ci id="S3.SS3.SSS1.p3.7.m7.2.2.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.3">𝑃</ci><ci id="S3.SS3.SSS1.p3.7.m7.2.2.1.4.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.4">𝑟</ci><apply id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1"><in id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.1"></in><apply id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2"><times id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.1"></times><ci id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.2.2">𝑀</ci><ci id="S3.SS3.SSS1.p3.7.m7.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.1.1">𝑑</ci></apply><ci id="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.2.2.1.1.1.1.3">𝑆</ci></apply></apply></apply><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2"><plus id="S3.SS3.SSS1.p3.7.m7.3.3.2.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.2"></plus><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1"><times id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.2"></times><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3"><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.1">⋅</ci><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2">superscript</csymbol><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.2">𝑒</ci><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.2.3">italic-ϵ</ci></apply><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.3.3">𝑃</ci></apply><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.4.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.4">𝑟</ci><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1"><csymbol cd="latexml" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.2.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1"><in id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.2"></in><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1"><times id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.2"></times><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.3">𝑀</ci><apply id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.2">𝑑</ci><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.1.1.1.1.3">𝑆</ci></apply></apply></apply><ci id="S3.SS3.SSS1.p3.7.m7.3.3.2.3.cmml" xref="S3.SS3.SSS1.p3.7.m7.3.3.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.7.m7.3c">Pr[M(d)\in S]\leq e^{\epsilon}\cdot Pr[M(d^{\prime})\in S]+\delta</annotation></semantics></math>. Prior to applying of differential privacy to a specific dataset, a privacy budget must be established, where <math id="S3.SS3.SSS1.p3.8.m8.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.SSS1.p3.8.m8.1a"><mi id="S3.SS3.SSS1.p3.8.m8.1.1" xref="S3.SS3.SSS1.p3.8.m8.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.8.m8.1b"><ci id="S3.SS3.SSS1.p3.8.m8.1.1.cmml" xref="S3.SS3.SSS1.p3.8.m8.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.8.m8.1c">\epsilon</annotation></semantics></math> represents the privacy budget and <math id="S3.SS3.SSS1.p3.9.m9.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS3.SSS1.p3.9.m9.1a"><mi id="S3.SS3.SSS1.p3.9.m9.1.1" xref="S3.SS3.SSS1.p3.9.m9.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.9.m9.1b"><ci id="S3.SS3.SSS1.p3.9.m9.1.1.cmml" xref="S3.SS3.SSS1.p3.9.m9.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.9.m9.1c">\delta</annotation></semantics></math> signifies the probability of privacy breach in differential privacy. Each instance of differential privacy incurs a cost to <math id="S3.SS3.SSS1.p3.10.m10.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.SSS1.p3.10.m10.1a"><mi id="S3.SS3.SSS1.p3.10.m10.1.1" xref="S3.SS3.SSS1.p3.10.m10.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.10.m10.1b"><ci id="S3.SS3.SSS1.p3.10.m10.1.1.cmml" xref="S3.SS3.SSS1.p3.10.m10.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.10.m10.1c">\epsilon</annotation></semantics></math> depleting the privacy budget. Once spent, no further computations on the dataset can be made until the budget is replenished. Lower values of <math id="S3.SS3.SSS1.p3.11.m11.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.SSS1.p3.11.m11.1a"><mi id="S3.SS3.SSS1.p3.11.m11.1.1" xref="S3.SS3.SSS1.p3.11.m11.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.11.m11.1b"><ci id="S3.SS3.SSS1.p3.11.m11.1.1.cmml" xref="S3.SS3.SSS1.p3.11.m11.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.11.m11.1c">\epsilon</annotation></semantics></math> and <math id="S3.SS3.SSS1.p3.12.m12.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS3.SSS1.p3.12.m12.1a"><mi id="S3.SS3.SSS1.p3.12.m12.1.1" xref="S3.SS3.SSS1.p3.12.m12.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.12.m12.1b"><ci id="S3.SS3.SSS1.p3.12.m12.1.1.cmml" xref="S3.SS3.SSS1.p3.12.m12.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.12.m12.1c">\delta</annotation></semantics></math> correspond to higher levels of privacy.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.9" class="ltx_p">In a federated learning framework, where <math id="S3.SS3.SSS1.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS1.p4.1.m1.1a"><mi id="S3.SS3.SSS1.p4.1.m1.1.1" xref="S3.SS3.SSS1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.1.m1.1b"><ci id="S3.SS3.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.1.m1.1c">k</annotation></semantics></math> participants are collaboratively training a model, each training round <math id="S3.SS3.SSS1.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.SSS1.p4.2.m2.1a"><mi id="S3.SS3.SSS1.p4.2.m2.1.1" xref="S3.SS3.SSS1.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.2.m2.1b"><ci id="S3.SS3.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.2.m2.1c">i</annotation></semantics></math> sees the global server allocate the current model <math id="S3.SS3.SSS1.p4.3.m3.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S3.SS3.SSS1.p4.3.m3.1a"><msub id="S3.SS3.SSS1.p4.3.m3.1.1" xref="S3.SS3.SSS1.p4.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p4.3.m3.1.1.2" xref="S3.SS3.SSS1.p4.3.m3.1.1.2.cmml">G</mi><mi id="S3.SS3.SSS1.p4.3.m3.1.1.3" xref="S3.SS3.SSS1.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.3.m3.1b"><apply id="S3.SS3.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p4.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p4.3.m3.1.1.2">𝐺</ci><ci id="S3.SS3.SSS1.p4.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.3.m3.1c">G_{i}</annotation></semantics></math> to a small subgroup <math id="S3.SS3.SSS1.p4.4.m4.1" class="ltx_Math" alttext="d_{c}" display="inline"><semantics id="S3.SS3.SSS1.p4.4.m4.1a"><msub id="S3.SS3.SSS1.p4.4.m4.1.1" xref="S3.SS3.SSS1.p4.4.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p4.4.m4.1.1.2" xref="S3.SS3.SSS1.p4.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS3.SSS1.p4.4.m4.1.1.3" xref="S3.SS3.SSS1.p4.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.4.m4.1b"><apply id="S3.SS3.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.4.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p4.4.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p4.4.m4.1.1.2">𝑑</ci><ci id="S3.SS3.SSS1.p4.4.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p4.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.4.m4.1c">d_{c}</annotation></semantics></math>. Each participant <math id="S3.SS3.SSS1.p4.5.m5.1" class="ltx_Math" alttext="t\in d_{c}" display="inline"><semantics id="S3.SS3.SSS1.p4.5.m5.1a"><mrow id="S3.SS3.SSS1.p4.5.m5.1.1" xref="S3.SS3.SSS1.p4.5.m5.1.1.cmml"><mi id="S3.SS3.SSS1.p4.5.m5.1.1.2" xref="S3.SS3.SSS1.p4.5.m5.1.1.2.cmml">t</mi><mo id="S3.SS3.SSS1.p4.5.m5.1.1.1" xref="S3.SS3.SSS1.p4.5.m5.1.1.1.cmml">∈</mo><msub id="S3.SS3.SSS1.p4.5.m5.1.1.3" xref="S3.SS3.SSS1.p4.5.m5.1.1.3.cmml"><mi id="S3.SS3.SSS1.p4.5.m5.1.1.3.2" xref="S3.SS3.SSS1.p4.5.m5.1.1.3.2.cmml">d</mi><mi id="S3.SS3.SSS1.p4.5.m5.1.1.3.3" xref="S3.SS3.SSS1.p4.5.m5.1.1.3.3.cmml">c</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.5.m5.1b"><apply id="S3.SS3.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1"><in id="S3.SS3.SSS1.p4.5.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.1"></in><ci id="S3.SS3.SSS1.p4.5.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.2">𝑡</ci><apply id="S3.SS3.SSS1.p4.5.m5.1.1.3.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.3.2">𝑑</ci><ci id="S3.SS3.SSS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS3.SSS1.p4.5.m5.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.5.m5.1c">t\in d_{c}</annotation></semantics></math> trains the model locally using their respective data, generating new local models denoted as <math id="S3.SS3.SSS1.p4.6.m6.1" class="ltx_Math" alttext="L_{i+1}^{t}" display="inline"><semantics id="S3.SS3.SSS1.p4.6.m6.1a"><msubsup id="S3.SS3.SSS1.p4.6.m6.1.1" xref="S3.SS3.SSS1.p4.6.m6.1.1.cmml"><mi id="S3.SS3.SSS1.p4.6.m6.1.1.2.2" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.2.cmml">L</mi><mrow id="S3.SS3.SSS1.p4.6.m6.1.1.2.3" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.cmml"><mi id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.2" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.2.cmml">i</mi><mo id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.1" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.1.cmml">+</mo><mn id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.3" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS3.SSS1.p4.6.m6.1.1.3" xref="S3.SS3.SSS1.p4.6.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.6.m6.1b"><apply id="S3.SS3.SSS1.p4.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1">superscript</csymbol><apply id="S3.SS3.SSS1.p4.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.6.m6.1.1.2.1.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p4.6.m6.1.1.2.2.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.2">𝐿</ci><apply id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3"><plus id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.1.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.1"></plus><ci id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.2.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS3.SSS1.p4.6.m6.1.1.2.3.3.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.2.3.3">1</cn></apply></apply><ci id="S3.SS3.SSS1.p4.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p4.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.6.m6.1c">L_{i+1}^{t}</annotation></semantics></math>. The global server then aggregates these participant-generated models, updating the global model according to a global learning rate <math id="S3.SS3.SSS1.p4.7.m7.1" class="ltx_Math" alttext="\eta_{g}" display="inline"><semantics id="S3.SS3.SSS1.p4.7.m7.1a"><msub id="S3.SS3.SSS1.p4.7.m7.1.1" xref="S3.SS3.SSS1.p4.7.m7.1.1.cmml"><mi id="S3.SS3.SSS1.p4.7.m7.1.1.2" xref="S3.SS3.SSS1.p4.7.m7.1.1.2.cmml">η</mi><mi id="S3.SS3.SSS1.p4.7.m7.1.1.3" xref="S3.SS3.SSS1.p4.7.m7.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.7.m7.1b"><apply id="S3.SS3.SSS1.p4.7.m7.1.1.cmml" xref="S3.SS3.SSS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.7.m7.1.1.1.cmml" xref="S3.SS3.SSS1.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p4.7.m7.1.1.2.cmml" xref="S3.SS3.SSS1.p4.7.m7.1.1.2">𝜂</ci><ci id="S3.SS3.SSS1.p4.7.m7.1.1.3.cmml" xref="S3.SS3.SSS1.p4.7.m7.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.7.m7.1c">\eta_{g}</annotation></semantics></math>. This update process follows the formula: <math id="S3.SS3.SSS1.p4.8.m8.3" class="ltx_Math" alttext="G_{t+1}=G_{t}+\frac{\eta_{g}}{k}\sum_{i\in d_{c}}(L_{i+1}^{t}-G_{t})+N(0,\sigma^{2}I)" display="inline"><semantics id="S3.SS3.SSS1.p4.8.m8.3a"><mrow id="S3.SS3.SSS1.p4.8.m8.3.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.cmml"><msub id="S3.SS3.SSS1.p4.8.m8.3.3.4" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.3.3.4.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.2.cmml">G</mi><mrow id="S3.SS3.SSS1.p4.8.m8.3.3.4.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.2.cmml">t</mi><mo id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.1" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.1.cmml">+</mo><mn id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS3.SSS1.p4.8.m8.3.3.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.3.cmml">=</mo><mrow id="S3.SS3.SSS1.p4.8.m8.3.3.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.cmml"><msub id="S3.SS3.SSS1.p4.8.m8.3.3.2.4" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4.2.cmml">G</mi><mi id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4.3.cmml">t</mi></msub><mo id="S3.SS3.SSS1.p4.8.m8.3.3.2.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.3.cmml">+</mo><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.cmml"><mfrac id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.cmml"><msub id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.2.cmml">η</mi><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.3.cmml">g</mi></msub><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.3.cmml">k</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.cmml"><msub id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.cmml"><mo rspace="0em" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.2.cmml">∑</mo><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.2.cmml">i</mi><mo id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.1.cmml">∈</mo><msub id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.2.cmml">d</mi><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.3.cmml">c</mi></msub></mrow></msub><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.cmml"><msubsup id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.2.cmml">L</mi><mrow id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.2" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.2.cmml">G</mi><mi id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.3" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS3.SSS1.p4.8.m8.3.3.2.3a" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.3.cmml">+</mo><mrow id="S3.SS3.SSS1.p4.8.m8.3.3.2.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.2.cmml">(</mo><mn id="S3.SS3.SSS1.p4.8.m8.1.1" xref="S3.SS3.SSS1.p4.8.m8.1.1.cmml">0</mn><mo id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.2.cmml">,</mo><mrow id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.cmml"><msup id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.2" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.2.cmml">σ</mi><mn id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.1" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.3" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.3.cmml">I</mi></mrow><mo stretchy="false" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.4" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.8.m8.3b"><apply id="S3.SS3.SSS1.p4.8.m8.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3"><eq id="S3.SS3.SSS1.p4.8.m8.3.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.3"></eq><apply id="S3.SS3.SSS1.p4.8.m8.3.3.4.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.3.3.4.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.3.3.4.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.2">𝐺</ci><apply id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3"><plus id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.1"></plus><ci id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.2">𝑡</ci><cn type="integer" id="S3.SS3.SSS1.p4.8.m8.3.3.4.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.4.3.3">1</cn></apply></apply><apply id="S3.SS3.SSS1.p4.8.m8.3.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2"><plus id="S3.SS3.SSS1.p4.8.m8.3.3.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.3"></plus><apply id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4.2">𝐺</ci><ci id="S3.SS3.SSS1.p4.8.m8.3.3.2.4.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.4.3">𝑡</ci></apply><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1"><times id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.2"></times><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3"><divide id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3"></divide><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.2">𝜂</ci><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.2.3">𝑔</ci></apply><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.3.3">𝑘</ci></apply><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1"><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2">subscript</csymbol><sum id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.2"></sum><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3"><in id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.1"></in><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.2">𝑖</ci><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.2">𝑑</ci><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.2.3.3.3">𝑐</ci></apply></apply></apply><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1"><minus id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.1"></minus><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.2">𝐿</ci><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3"><plus id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.1"></plus><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.2">𝐺</ci><ci id="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.2.2.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2"><times id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.2"></times><ci id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.3">𝑁</ci><interval closure="open" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1"><cn type="integer" id="S3.SS3.SSS1.p4.8.m8.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.1.1">0</cn><apply id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1"><times id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.1"></times><apply id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2">superscript</csymbol><ci id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.2">𝜎</ci><cn type="integer" id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.2.3">2</cn></apply><ci id="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p4.8.m8.3.3.2.2.1.1.1.3">𝐼</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.8.m8.3c">G_{t+1}=G_{t}+\frac{\eta_{g}}{k}\sum_{i\in d_{c}}(L_{i+1}^{t}-G_{t})+N(0,\sigma^{2}I)</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p4.9.m9.2" class="ltx_Math" alttext="N(0,\sigma^{2})" display="inline"><semantics id="S3.SS3.SSS1.p4.9.m9.2a"><mrow id="S3.SS3.SSS1.p4.9.m9.2.2" xref="S3.SS3.SSS1.p4.9.m9.2.2.cmml"><mi id="S3.SS3.SSS1.p4.9.m9.2.2.3" xref="S3.SS3.SSS1.p4.9.m9.2.2.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p4.9.m9.2.2.2" xref="S3.SS3.SSS1.p4.9.m9.2.2.2.cmml">​</mo><mrow id="S3.SS3.SSS1.p4.9.m9.2.2.1.1" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.2" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.2.cmml">(</mo><mn id="S3.SS3.SSS1.p4.9.m9.1.1" xref="S3.SS3.SSS1.p4.9.m9.1.1.cmml">0</mn><mo id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.3" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.2.cmml">,</mo><msup id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.2" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.2.cmml">σ</mi><mn id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.3" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.4" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.9.m9.2b"><apply id="S3.SS3.SSS1.p4.9.m9.2.2.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2"><times id="S3.SS3.SSS1.p4.9.m9.2.2.2.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.2"></times><ci id="S3.SS3.SSS1.p4.9.m9.2.2.3.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.3">𝑁</ci><interval closure="open" id="S3.SS3.SSS1.p4.9.m9.2.2.1.2.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1"><cn type="integer" id="S3.SS3.SSS1.p4.9.m9.1.1.cmml" xref="S3.SS3.SSS1.p4.9.m9.1.1">0</cn><apply id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.2">𝜎</ci><cn type="integer" id="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p4.9.m9.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.9.m9.2c">N(0,\sigma^{2})</annotation></semantics></math> represents Gaussian noise added to each update vector by a FedAvg algorithm. Here, introducing differential privacy restricts the influence of any one participant on the model.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Balancing privacy and fariness</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">To ensure both fairness and privacy in a federated learning network, one must consider a trade-off between the two. While privacy-preserving technologies such as differential privacy can maintain the accuracy of models, there is a cost to this approach: the stricter the privacy-preservation, the lower the fairness at a group level.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.4.1.1" class="ltx_text" style="font-size:63%;">Table 2</span>. </span><span id="S3.T2.5.2" class="ltx_text" style="font-size:63%;">Comparing Existing Literature with privacy and fairness</span></figcaption>
<div id="S3.T2.6" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:65.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-434.8pt,66.1pt) scale(0.332751517890781,0.332751517890781) ;">
<table id="S3.T2.6.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.6.1.1.1" class="ltx_tr">
<td id="S3.T2.6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" rowspan="2"><span id="S3.T2.6.1.1.1.1.1" class="ltx_text" style="font-size:173%;">Reference</span></td>
<td id="S3.T2.6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" colspan="2"><span id="S3.T2.6.1.1.1.2.1" class="ltx_text" style="font-size:173%;">Privacy-Preserving</span></td>
<td id="S3.T2.6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" colspan="3"><span id="S3.T2.6.1.1.1.3.1" class="ltx_text" style="font-size:173%;">Fairness</span></td>
<td id="S3.T2.6.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" colspan="2"><span id="S3.T2.6.1.1.1.4.1" class="ltx_text" style="font-size:173%;">Privacy Guarantees</span></td>
</tr>
<tr id="S3.T2.6.1.2.2" class="ltx_tr">
<td id="S3.T2.6.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.1.1" class="ltx_text" style="font-size:173%;">Training Data</span></td>
<td id="S3.T2.6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.2.1" class="ltx_text" style="font-size:173%;">Sensitive Attribute</span></td>
<td id="S3.T2.6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.3.1" class="ltx_text" style="font-size:173%;">Equalized Odds</span></td>
<td id="S3.T2.6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.4.1" class="ltx_text" style="font-size:173%;">Equalized Opportunity</span></td>
<td id="S3.T2.6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.5.1" class="ltx_text" style="font-size:173%;">Demographic Parity</span></td>
<td id="S3.T2.6.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.6.1" class="ltx_text" style="font-size:173%;">DP</span></td>
<td id="S3.T2.6.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.2.2.7.1" class="ltx_text" style="font-size:173%;">MPC</span></td>
</tr>
<tr id="S3.T2.6.1.3.3" class="ltx_tr">
<td id="S3.T2.6.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.3.3.1.1" class="ltx_text" style="font-size:144%;">Cummings </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.3.3.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Cummings et al<span class="ltx_text">.</span><span id="S3.T2.6.1.3.3.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib31" title="" class="ltx_ref">2019</a><span id="S3.T2.6.1.3.3.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.3.3.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.4.4" class="ltx_tr">
<td id="S3.T2.6.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.4.4.1.1" class="ltx_text" style="font-size:144%;">Abay </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.4.4.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Abay et al<span class="ltx_text">.</span><span id="S3.T2.6.1.4.4.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib2" title="" class="ltx_ref">2020</a><span id="S3.T2.6.1.4.4.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.3.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.4.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.4.4.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.4.4.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.5.5" class="ltx_tr">
<td id="S3.T2.6.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.5.5.1.1" class="ltx_text" style="font-size:144%;">Du </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.5.5.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Du et al<span class="ltx_text">.</span><span id="S3.T2.6.1.5.5.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib35" title="" class="ltx_ref">2021</a><span id="S3.T2.6.1.5.5.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.2.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.4.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.6.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.5.5.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.5.5.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.6.6" class="ltx_tr">
<td id="S3.T2.6.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.6.6.1.1" class="ltx_text" style="font-size:144%;">Triastcyn </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.6.6.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Triastcyn and Faltings<span id="S3.T2.6.1.6.6.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib123" title="" class="ltx_ref">2019</a><span id="S3.T2.6.1.6.6.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.6.6.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.6.6.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.7.7" class="ltx_tr">
<td id="S3.T2.6.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.7.7.1.1" class="ltx_text" style="font-size:144%;">Padala </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.7.7.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Padala et al<span class="ltx_text">.</span><span id="S3.T2.6.1.7.7.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib99" title="" class="ltx_ref">2021</a><span id="S3.T2.6.1.7.7.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.3.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.6.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.7.7.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.7.7.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.8.8" class="ltx_tr">
<td id="S3.T2.6.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.8.8.1.1" class="ltx_text" style="font-size:144%;">Rodríguez-Gálvez </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.8.8.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Gálvez et al<span class="ltx_text">.</span><span id="S3.T2.6.1.8.8.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib49" title="" class="ltx_ref">2021</a><span id="S3.T2.6.1.8.8.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.4.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.8.8.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.8.8.8.1" class="ltx_text" style="font-size:144%;">✔</span></td>
</tr>
<tr id="S3.T2.6.1.9.9" class="ltx_tr">
<td id="S3.T2.6.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.9.9.1.1" class="ltx_text" style="font-size:144%;">Pentyala </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.9.9.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Pentyala et al<span class="ltx_text">.</span><span id="S3.T2.6.1.9.9.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib101" title="" class="ltx_ref">2022</a><span id="S3.T2.6.1.9.9.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.2.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.3.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.4.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.9.9.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.9.9.8.1" class="ltx_text" style="font-size:144%;">✔</span></td>
</tr>
<tr id="S3.T2.6.1.10.10" class="ltx_tr">
<td id="S3.T2.6.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.10.10.1.1" class="ltx_text" style="font-size:144%;">Mozannar </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.10.10.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Mozannar et al<span class="ltx_text">.</span><span id="S3.T2.6.1.10.10.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib92" title="" class="ltx_ref">2020</a><span id="S3.T2.6.1.10.10.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.5.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.6.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.10.10.8" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.10.10.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
<tr id="S3.T2.6.1.11.11" class="ltx_tr">
<td id="S3.T2.6.1.11.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T2.6.1.11.11.1.1" class="ltx_text" style="font-size:144%;">Gu </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T2.6.1.11.11.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Gu et al<span class="ltx_text">.</span><span id="S3.T2.6.1.11.11.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib57" title="" class="ltx_ref">2022</a><span id="S3.T2.6.1.11.11.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
<td id="S3.T2.6.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T2.6.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.5.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.6.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.11.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.7.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S3.T2.6.1.11.11.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T2.6.1.11.11.8.1" class="ltx_text" style="font-size:144%;">✘</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.6" class="ltx_p">Cummings et al. <cite class="ltx_cite ltx_citemacro_citep">(Cummings et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> argue that achieving complete fairness within the privacy model is unattainable when transitioning from a dataset <math id="S3.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><mi id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><ci id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">x</annotation></semantics></math> to an adjacent dataset <math id="S3.SS3.SSS2.p2.2.m2.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS3.SSS2.p2.2.m2.1a"><msup id="S3.SS3.SSS2.p2.2.m2.1.1" xref="S3.SS3.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p2.2.m2.1.1.2" xref="S3.SS3.SSS2.p2.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS3.SSS2.p2.2.m2.1.1.3" xref="S3.SS3.SSS2.p2.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.2.m2.1b"><apply id="S3.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS3.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.2.m2.1c">x^{\prime}</annotation></semantics></math>, as there is no classifier <math id="S3.SS3.SSS2.p2.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS2.p2.3.m3.1a"><mi id="S3.SS3.SSS2.p2.3.m3.1.1" xref="S3.SS3.SSS2.p2.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.3.m3.1b"><ci id="S3.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p2.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.3.m3.1c">M</annotation></semantics></math> that can be applied to dataset <math id="S3.SS3.SSS2.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.SSS2.p2.4.m4.1a"><mi id="S3.SS3.SSS2.p2.4.m4.1.1" xref="S3.SS3.SSS2.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.4.m4.1b"><ci id="S3.SS3.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.4.m4.1c">x</annotation></semantics></math> to maintain complete fairness. This challenge persists even if <math id="S3.SS3.SSS2.p2.5.m5.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS3.SSS2.p2.5.m5.1a"><msup id="S3.SS3.SSS2.p2.5.m5.1.1" xref="S3.SS3.SSS2.p2.5.m5.1.1.cmml"><mi id="S3.SS3.SSS2.p2.5.m5.1.1.2" xref="S3.SS3.SSS2.p2.5.m5.1.1.2.cmml">x</mi><mo id="S3.SS3.SSS2.p2.5.m5.1.1.3" xref="S3.SS3.SSS2.p2.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.5.m5.1b"><apply id="S3.SS3.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.5.m5.1.1.1.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1.2">𝑥</ci><ci id="S3.SS3.SSS2.p2.5.m5.1.1.3.cmml" xref="S3.SS3.SSS2.p2.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.5.m5.1c">x^{\prime}</annotation></semantics></math> is derived by adding or removing a sample from <math id="S3.SS3.SSS2.p2.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.SSS2.p2.6.m6.1a"><mi id="S3.SS3.SSS2.p2.6.m6.1.1" xref="S3.SS3.SSS2.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.6.m6.1b"><ci id="S3.SS3.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS2.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.6.m6.1c">x</annotation></semantics></math>. Consequently, these researchers proposes an approximate method of measuring fairness that incorporates discrimination coupled with an an efficient classification algorithm designed to maximize the probability of achieving privacy and approximate fairness while preserving utility.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p">Pentyala et al. <cite class="ltx_cite ltx_citemacro_citep">(Pentyala et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2022</a>)</cite> also devised a privacy-preserving technique that mitigates bias as well. Their approach leverages secure multi-party computing protocols to collect and aggregate information on the label distribution and the values of sensitive attributes. Then, to rectify bias within the training set, weights are assigned to each sample based on those labels and values prior to training.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p">Another strategy to balance fairness and privacy is to extend the guarantee of differential privacy to both the training data and the sensitive attributes. Padala et al. <cite class="ltx_cite ltx_citemacro_citep">(Padala et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2021</a>)</cite>, for example, developed a two-stage framework to ensure fairness while still protecting privacy. The first stage ensures fairness while maintaining accuracy. Demographic parity is considered along with equalized odds to ensure that the model’s predictions remain independent of the sensitive attributes within the dataset. A concurrent goal is to equalize the false positive and false negative rates of the model across different groups, regardless of their sensitive attributes. The second stage yields the privacy protection through local differential privacy, safeguarding both the training data and sensitive attributes. Here, Gaussian noise is introduced to protect the training data for stochastic gradient descent. This measure ensures that privacy breaches are not induced by an aggregator attack targeting the sensitive attributes. What this framework does is effectively decouple the training process into distinct stages, offering empirical insights into the tradeoffs between the fairness, privacy, and accuracy of federated learning models.</p>
</div>
<div id="S3.SS3.SSS2.p5" class="ltx_para">
<p id="S3.SS3.SSS2.p5.1" class="ltx_p">Gu et al. <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite> explore strategies for striking a beneficial balance between privacy and fairness while also analyzing the impact of local differential privacy and global differential privacy on equity. In one of their experiments, local differential privacy is applied at the client level to observe its effects on fairness, while the server uses the DPfedAvg algorithm to establish global differential privacy. Splicing is standardized across the different groups to mitigate gradient-induced deviations. The experiment assesses group fairness through three distinct definitions of fairness as a measure of discrimination. The experiment assesses group fairness through three distinct definitions of fairness as a measure of discrimination. The results demonstrate that, given an appropriate level of noise and a fixed truncation boundary, both local and global differential privacy can deliver fairness. Notably, the stringency of the privacy-preserving mechanisms inversely affects the degree of fairness within the groups. Additionally, these researchers identify two sources of bias: the client model and imbalances in the training data. They also closely link the range of noise values to the magnitude of the noise, finding that a judicious application of local differential privacy allows for an acceptable trade-off between accuracy and privacy while still promoting fairness.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>The trade-off between security and fairness</h3>

<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>The impact of security on fairness</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.2" class="ltx_p">Arguably, the most common forms of security attack are poisoning or backdoor attacks, where a malicious client sends an incorrect model update to the server. These attacks can disrupt the normal convergence of the model or lead to results that do not accurately represent the true data distribution, introducing bias. Attacks on model fairness can be executed in two primary ways. The first method involves model poisoning, as seen in backdoor attacks, where the global model is altered to undermine model fairness. n the global model update process, the central server receives updates from all <math id="S3.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS4.SSS1.p1.1.m1.1a"><mi id="S3.SS4.SSS1.p1.1.m1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.1b"><ci id="S3.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.1c">n</annotation></semantics></math> clients and uses a global learning rate <math id="S3.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.SS4.SSS1.p1.2.m2.1a"><mi id="S3.SS4.SSS1.p1.2.m2.1.1" xref="S3.SS4.SSS1.p1.2.m2.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.2.m2.1b"><ci id="S3.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.2.m2.1c">\eta</annotation></semantics></math> to control the proportion of the joint model updated in each round. A typical expression of a joint federated learning model is <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>:</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.1" class="ltx_Math" alttext="G^{t+1}=G^{t}+\frac{\eta}{n}\sum\limits_{i=1}^{m}(L^{t+1}_{i}-G^{t})." display="block"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><mrow id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><msup id="S3.E8.m1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.3.2.cmml">G</mi><mrow id="S3.E8.m1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.3.3.cmml"><mi id="S3.E8.m1.1.1.1.1.3.3.2" xref="S3.E8.m1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S3.E8.m1.1.1.1.1.3.3.1" xref="S3.E8.m1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E8.m1.1.1.1.1.3.3.3" xref="S3.E8.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msup><mo id="S3.E8.m1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.cmml"><msup id="S3.E8.m1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.3.2.cmml">G</mi><mi id="S3.E8.m1.1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.1.3.3.cmml">t</mi></msup><mo id="S3.E8.m1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mfrac id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.3.2.cmml">η</mi><mi id="S3.E8.m1.1.1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml"><munderover id="S3.E8.m1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E8.m1.1.1.1.1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">L</mi><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">G</mi><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msup></mrow><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E8.m1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><eq id="S3.E8.m1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.2"></eq><apply id="S3.E8.m1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E8.m1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.3.2">𝐺</ci><apply id="S3.E8.m1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.3.3"><plus id="S3.E8.m1.1.1.1.1.3.3.1.cmml" xref="S3.E8.m1.1.1.1.1.3.3.1"></plus><ci id="S3.E8.m1.1.1.1.1.3.3.2.cmml" xref="S3.E8.m1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S3.E8.m1.1.1.1.1.3.3.3.cmml" xref="S3.E8.m1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E8.m1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1"><plus id="S3.E8.m1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.2"></plus><apply id="S3.E8.m1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.3.2">𝐺</ci><ci id="S3.E8.m1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E8.m1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1"><times id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2"></times><apply id="S3.E8.m1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><divide id="S3.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"></divide><ci id="S3.E8.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.2">𝜂</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.3">𝑛</ci></apply><apply id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1"><apply id="S3.E8.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E8.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E8.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3"><eq id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E8.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2.3">𝑚</ci></apply><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐿</ci><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3"><plus id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></plus><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝐺</ci><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">G^{t+1}=G^{t}+\frac{\eta}{n}\sum\limits_{i=1}^{m}(L^{t+1}_{i}-G^{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para">
<p id="S3.SS4.SSS1.p3.6" class="ltx_p">In each round, denoted as <math id="S3.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS4.SSS1.p3.1.m1.1a"><mi id="S3.SS4.SSS1.p3.1.m1.1.1" xref="S3.SS4.SSS1.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.1.m1.1b"><ci id="S3.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.1.m1.1c">t</annotation></semantics></math>, the server randomly selects a subset of <math id="S3.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS4.SSS1.p3.2.m2.1a"><mi id="S3.SS4.SSS1.p3.2.m2.1.1" xref="S3.SS4.SSS1.p3.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.2.m2.1b"><ci id="S3.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.2.m2.1c">m</annotation></semantics></math> clients, referred to as <math id="S3.SS4.SSS1.p3.3.m3.1" class="ltx_Math" alttext="C_{m}" display="inline"><semantics id="S3.SS4.SSS1.p3.3.m3.1a"><msub id="S3.SS4.SSS1.p3.3.m3.1.1" xref="S3.SS4.SSS1.p3.3.m3.1.1.cmml"><mi id="S3.SS4.SSS1.p3.3.m3.1.1.2" xref="S3.SS4.SSS1.p3.3.m3.1.1.2.cmml">C</mi><mi id="S3.SS4.SSS1.p3.3.m3.1.1.3" xref="S3.SS4.SSS1.p3.3.m3.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.3.m3.1b"><apply id="S3.SS4.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p3.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.1.2">𝐶</ci><ci id="S3.SS4.SSS1.p3.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.3.m3.1c">C_{m}</annotation></semantics></math>. It then distributes the current global model, represented as <math id="S3.SS4.SSS1.p3.4.m4.1" class="ltx_Math" alttext="G^{t}" display="inline"><semantics id="S3.SS4.SSS1.p3.4.m4.1a"><msup id="S3.SS4.SSS1.p3.4.m4.1.1" xref="S3.SS4.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS4.SSS1.p3.4.m4.1.1.2" xref="S3.SS4.SSS1.p3.4.m4.1.1.2.cmml">G</mi><mi id="S3.SS4.SSS1.p3.4.m4.1.1.3" xref="S3.SS4.SSS1.p3.4.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.4.m4.1b"><apply id="S3.SS4.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.1.2">𝐺</ci><ci id="S3.SS4.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.4.m4.1c">G^{t}</annotation></semantics></math>, to these selected clients. Each client independently updates its local model by training it on its local dataset to result in a new model denoted as <math id="S3.SS4.SSS1.p3.5.m5.1" class="ltx_Math" alttext="L^{t+1}" display="inline"><semantics id="S3.SS4.SSS1.p3.5.m5.1a"><msup id="S3.SS4.SSS1.p3.5.m5.1.1" xref="S3.SS4.SSS1.p3.5.m5.1.1.cmml"><mi id="S3.SS4.SSS1.p3.5.m5.1.1.2" xref="S3.SS4.SSS1.p3.5.m5.1.1.2.cmml">L</mi><mrow id="S3.SS4.SSS1.p3.5.m5.1.1.3" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.cmml"><mi id="S3.SS4.SSS1.p3.5.m5.1.1.3.2" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.2.cmml">t</mi><mo id="S3.SS4.SSS1.p3.5.m5.1.1.3.1" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.1.cmml">+</mo><mn id="S3.SS4.SSS1.p3.5.m5.1.1.3.3" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.5.m5.1b"><apply id="S3.SS4.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.5.m5.1.1.1.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p3.5.m5.1.1.2.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1.2">𝐿</ci><apply id="S3.SS4.SSS1.p3.5.m5.1.1.3.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1.3"><plus id="S3.SS4.SSS1.p3.5.m5.1.1.3.1.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.1"></plus><ci id="S3.SS4.SSS1.p3.5.m5.1.1.3.2.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS4.SSS1.p3.5.m5.1.1.3.3.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.5.m5.1c">L^{t+1}</annotation></semantics></math>. Subsequently, these clients send the updated model parameters, specifically <math id="S3.SS4.SSS1.p3.6.m6.1" class="ltx_Math" alttext="L^{t+1}_{i}-G^{t}" display="inline"><semantics id="S3.SS4.SSS1.p3.6.m6.1a"><mrow id="S3.SS4.SSS1.p3.6.m6.1.1" xref="S3.SS4.SSS1.p3.6.m6.1.1.cmml"><msubsup id="S3.SS4.SSS1.p3.6.m6.1.1.2" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.cmml"><mi id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.2" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.2.cmml">L</mi><mi id="S3.SS4.SSS1.p3.6.m6.1.1.2.3" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.3.cmml">i</mi><mrow id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.cmml"><mi id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.2" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.2.cmml">t</mi><mo id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.1" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.1.cmml">+</mo><mn id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.3" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S3.SS4.SSS1.p3.6.m6.1.1.1" xref="S3.SS4.SSS1.p3.6.m6.1.1.1.cmml">−</mo><msup id="S3.SS4.SSS1.p3.6.m6.1.1.3" xref="S3.SS4.SSS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS4.SSS1.p3.6.m6.1.1.3.2" xref="S3.SS4.SSS1.p3.6.m6.1.1.3.2.cmml">G</mi><mi id="S3.SS4.SSS1.p3.6.m6.1.1.3.3" xref="S3.SS4.SSS1.p3.6.m6.1.1.3.3.cmml">t</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.6.m6.1b"><apply id="S3.SS4.SSS1.p3.6.m6.1.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1"><minus id="S3.SS4.SSS1.p3.6.m6.1.1.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.1"></minus><apply id="S3.SS4.SSS1.p3.6.m6.1.1.2.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.6.m6.1.1.2.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2">subscript</csymbol><apply id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.2.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.2">𝐿</ci><apply id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3"><plus id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.1"></plus><ci id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.2.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.3.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS4.SSS1.p3.6.m6.1.1.2.3.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S3.SS4.SSS1.p3.6.m6.1.1.3.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.3.2">𝐺</ci><ci id="S3.SS4.SSS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS4.SSS1.p3.6.m6.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.6.m6.1c">L^{t+1}_{i}-G^{t}</annotation></semantics></math>, back to the server.</p>
</div>
<div id="S3.SS4.SSS1.p4" class="ltx_para">
<p id="S3.SS4.SSS1.p4.1" class="ltx_p">To execute a malicious backdoor attack, the adversary will replace the legitimate global model update with a surreptitious backdoor model, referred to as <math id="S3.SS4.SSS1.p4.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS4.SSS1.p4.1.m1.1a"><mi id="S3.SS4.SSS1.p4.1.m1.1.1" xref="S3.SS4.SSS1.p4.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p4.1.m1.1b"><ci id="S3.SS4.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p4.1.m1.1c">X</annotation></semantics></math>. They will also introduce a subset of trojan training data and a trigger into the model, altering its behavior. Importantly, this attack is covert, activating only when the trigger conditions are met, all while maintaining high accuracy to evade detection. A common implementation for the attack follows <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>:</p>
</div>
<div id="S3.SS4.SSS1.p5" class="ltx_para">
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.1" class="ltx_Math" alttext="X=G^{t}+\frac{\eta}{n}\sum\limits_{i=1}^{m}(L^{t+1}_{i}-G^{t})." display="block"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml"><mrow id="S3.E9.m1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.3.cmml">X</mi><mo id="S3.E9.m1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E9.m1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.cmml"><msup id="S3.E9.m1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.1.1.3.2.cmml">G</mi><mi id="S3.E9.m1.1.1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.1.1.3.3.cmml">t</mi></msup><mo id="S3.E9.m1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><mfrac id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.3.2.cmml">η</mi><mi id="S3.E9.m1.1.1.1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml"><munderover id="S3.E9.m1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E9.m1.1.1.1.1.1.1.1.2.3" xref="S3.E9.m1.1.1.1.1.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S3.E9.m1.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">L</mi><mi id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mrow id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">G</mi><mi id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msup></mrow><mo stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E9.m1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"><eq id="S3.E9.m1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.2"></eq><ci id="S3.E9.m1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.3">𝑋</ci><apply id="S3.E9.m1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1"><plus id="S3.E9.m1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.2"></plus><apply id="S3.E9.m1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.3.2">𝐺</ci><ci id="S3.E9.m1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E9.m1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1"><times id="S3.E9.m1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2"></times><apply id="S3.E9.m1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3"><divide id="S3.E9.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3"></divide><ci id="S3.E9.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.2">𝜂</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3">𝑛</ci></apply><apply id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1"><apply id="S3.E9.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E9.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E9.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3"><eq id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E9.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2.3">𝑚</ci></apply><apply id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐿</ci><apply id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3"><plus id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></plus><ci id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝐺</ci><ci id="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">X=G^{t}+\frac{\eta}{n}\sum\limits_{i=1}^{m}(L^{t+1}_{i}-G^{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS1.p6" class="ltx_para">
<p id="S3.SS4.SSS1.p6.6" class="ltx_p">The second method is data poisoning which is often executed via a label flipping attack. This method allows for controlled manipulation of a dataset <math id="S3.SS4.SSS1.p6.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.SSS1.p6.1.m1.1a"><mi id="S3.SS4.SSS1.p6.1.m1.1.1" xref="S3.SS4.SSS1.p6.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.1.m1.1b"><ci id="S3.SS4.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.1.m1.1c">D</annotation></semantics></math> while maintaining high accuracy, albeit at the expense of model fairness. In this approach, each malicious client selectively alters some of the labels in the dataset <math id="S3.SS4.SSS1.p6.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.SSS1.p6.2.m2.1a"><mi id="S3.SS4.SSS1.p6.2.m2.1.1" xref="S3.SS4.SSS1.p6.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.2.m2.1b"><ci id="S3.SS4.SSS1.p6.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p6.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.2.m2.1c">D</annotation></semantics></math> with a probability denoted as <math id="S3.SS4.SSS1.p6.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS4.SSS1.p6.3.m3.1a"><mi id="S3.SS4.SSS1.p6.3.m3.1.1" xref="S3.SS4.SSS1.p6.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.3.m3.1b"><ci id="S3.SS4.SSS1.p6.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p6.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.3.m3.1c">P</annotation></semantics></math>. These labels are modified from their original value <math id="S3.SS4.SSS1.p6.4.m4.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS4.SSS1.p6.4.m4.1a"><mi id="S3.SS4.SSS1.p6.4.m4.1.1" xref="S3.SS4.SSS1.p6.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.4.m4.1b"><ci id="S3.SS4.SSS1.p6.4.m4.1.1.cmml" xref="S3.SS4.SSS1.p6.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.4.m4.1c">l</annotation></semantics></math> to a target label <math id="S3.SS4.SSS1.p6.5.m5.1" class="ltx_Math" alttext="l_{tar}" display="inline"><semantics id="S3.SS4.SSS1.p6.5.m5.1a"><msub id="S3.SS4.SSS1.p6.5.m5.1.1" xref="S3.SS4.SSS1.p6.5.m5.1.1.cmml"><mi id="S3.SS4.SSS1.p6.5.m5.1.1.2" xref="S3.SS4.SSS1.p6.5.m5.1.1.2.cmml">l</mi><mrow id="S3.SS4.SSS1.p6.5.m5.1.1.3" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.cmml"><mi id="S3.SS4.SSS1.p6.5.m5.1.1.3.2" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.p6.5.m5.1.1.3.1" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS1.p6.5.m5.1.1.3.3" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.p6.5.m5.1.1.3.1a" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS1.p6.5.m5.1.1.3.4" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.5.m5.1b"><apply id="S3.SS4.SSS1.p6.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p6.5.m5.1.1.1.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p6.5.m5.1.1.2.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.2">𝑙</ci><apply id="S3.SS4.SSS1.p6.5.m5.1.1.3.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.3"><times id="S3.SS4.SSS1.p6.5.m5.1.1.3.1.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.1"></times><ci id="S3.SS4.SSS1.p6.5.m5.1.1.3.2.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.2">𝑡</ci><ci id="S3.SS4.SSS1.p6.5.m5.1.1.3.3.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.3">𝑎</ci><ci id="S3.SS4.SSS1.p6.5.m5.1.1.3.4.cmml" xref="S3.SS4.SSS1.p6.5.m5.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.5.m5.1c">l_{tar}</annotation></semantics></math>, noting that any sensitive features <math id="S3.SS4.SSS1.p6.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS4.SSS1.p6.6.m6.1a"><mi id="S3.SS4.SSS1.p6.6.m6.1.1" xref="S3.SS4.SSS1.p6.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.6.m6.1b"><ci id="S3.SS4.SSS1.p6.6.m6.1.1.cmml" xref="S3.SS4.SSS1.p6.6.m6.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.6.m6.1c">f</annotation></semantics></math> can also be modified. For instance, in the context of image classification, a malicious client might change the original label of an image from ’cat’ to ’dog’, thereby poisoning the data. The objective of this attack is to generate a global model that fails to properly classify samples during testing. If sensitive features have been modified or if the model’s predictions are influenced by sensitive features, the model will ultimately be rendered less fair.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Balancing security and fairness</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">The biases observed in federated learning can often be attributed to significant variations in the distribution of local data, which, in turn, results in disparities in the model’s performance. Ozdayi and Kantarcioglu <cite class="ltx_cite ltx_citemacro_citep">(Ozdayi and Kantarcioglu, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite> demonstrated that data heterogeneity between different clients is a primary source of bias. It is also worthy noting that bias will usually grow at a much faster rate than accuracy will decline. Even when employing client update methods to adjust the aggregated learning rates in response to potential attacks, these defensive measures may themselves introduce unfairness into the model. Consequently, when pursuing model security, one must also consider fairness, striking a delicate trade-off.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">Security vulnerabilities in a model can mean it fails to converge as expected or it converges on a biased model that does not represent the real data. Defensive strategies often involve filtering out models statistically outlying updates that deviate from the global aggregated average as anomalous or malicious behavior. Moreover, the global model itself may be considered malicious. However, while this approach might effectively address security concerns, it can also introduce bias. This is because, normally, updated data only represents a portion of the clients; hence, unfairness in the training model is only exacerbated.</p>
</div>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.1" class="ltx_p">In fact, compromising the fairness of the global model, as indicated by Furth <cite class="ltx_cite ltx_citemacro_citep">(Furth, <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>, can impact numerous local models, as any unfairness can increase demographic inequality and reduce accuracy. Vice versa, even a slight reduction in a model’s accuracy can heighten the risk of demographic disparities, rendering the model unfair.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.4.1.1" class="ltx_text" style="font-size:63%;">Table 3</span>. </span><span id="S3.T3.5.2" class="ltx_text" style="font-size:63%;">The literature on security and fairness. ✘ indicates that the paper did not include this information. </span></figcaption>
<div id="S3.T3.6" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:43.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-635.8pt,62.4pt) scale(0.254301159737646,0.254301159737646) ;">
<table id="S3.T3.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.6.1.1.1" class="ltx_tr">
<th id="S3.T3.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" rowspan="2"><span id="S3.T3.6.1.1.1.1.1" class="ltx_text" style="font-size:173%;">Reference</span></th>
<th id="S3.T3.6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" colspan="2"><span id="S3.T3.6.1.1.1.2.1" class="ltx_text" style="font-size:173%;">security</span></th>
<th id="S3.T3.6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;" colspan="2"><span id="S3.T3.6.1.1.1.3.1" class="ltx_text" style="font-size:173%;">fairness</span></th>
</tr>
<tr id="S3.T3.6.1.2.2" class="ltx_tr">
<th id="S3.T3.6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.2.2.1.1" class="ltx_text" style="font-size:173%;">Attack</span></th>
<th id="S3.T3.6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.2.2.2.1" class="ltx_text" style="font-size:173%;">Defense</span></th>
<th id="S3.T3.6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.2.2.3.1" class="ltx_text" style="font-size:173%;">Guarantees</span></th>
<th id="S3.T3.6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.2.2.4.1" class="ltx_text" style="font-size:173%;">Metrics</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.6.1.3.1" class="ltx_tr">
<th id="S3.T3.6.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.3.1.1.1" class="ltx_text" style="font-size:144%;">Xu </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.3.1.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Xu and Lyu<span id="S3.T3.6.1.3.1.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib141" title="" class="ltx_ref">2020</a><span id="S3.T3.6.1.3.1.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.3.1.2.1" class="ltx_text" style="font-size:144%;">Poisoning, Free-riders</span></td>
<td id="S3.T3.6.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.3.1.3.1" class="ltx_text" style="font-size:144%;">Remove malicious adversaries</span></td>
<td id="S3.T3.6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.3.1.4.1" class="ltx_text" style="font-size:144%;">Reputation mechanism</span></td>
<td id="S3.T3.6.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.3.1.5.1" class="ltx_text" style="font-size:144%;">Clients’ real-valued contributions</span></td>
</tr>
<tr id="S3.T3.6.1.4.2" class="ltx_tr">
<th id="S3.T3.6.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.4.2.1.1" class="ltx_text" style="font-size:144%;">Furth </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.4.2.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Furth<span id="S3.T3.6.1.4.2.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib48" title="" class="ltx_ref">2022</a><span id="S3.T3.6.1.4.2.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.4.2.2" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.4.2.2.1" class="ltx_text" style="font-size:144%;">Targeted backdoor attacks</span></td>
<td id="S3.T3.6.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.4.2.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T3.6.1.4.2.4" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.4.2.4.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T3.6.1.4.2.5" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.4.2.5.1" class="ltx_text" style="font-size:144%;">Demographic parity</span></td>
</tr>
<tr id="S3.T3.6.1.5.3" class="ltx_tr">
<th id="S3.T3.6.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.5.3.1.1" class="ltx_text" style="font-size:144%;">Fraboni </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.5.3.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Fraboni et al<span class="ltx_text">.</span><span id="S3.T3.6.1.5.3.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib43" title="" class="ltx_ref">2021</a><span id="S3.T3.6.1.5.3.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.5.3.2" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.5.3.2.1" class="ltx_text" style="font-size:144%;">Free-rider attacks</span></td>
<td id="S3.T3.6.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.5.3.3.1" class="ltx_text" style="font-size:144%;">✘</span></td>
<td id="S3.T3.6.1.5.3.4" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.5.3.4.1" class="ltx_text" style="font-size:144%;">Aggregation algorithm</span></td>
<td id="S3.T3.6.1.5.3.5" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.5.3.5.1" class="ltx_text" style="font-size:144%;">FedProx</span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.5.3.5.2.1" class="ltx_text" style="font-size:144%;">(</span>Li et al<span class="ltx_text">.</span><span id="S3.T3.6.1.5.3.5.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib75" title="" class="ltx_ref">2020</a><span id="S3.T3.6.1.5.3.5.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</td>
</tr>
<tr id="S3.T3.6.1.6.4" class="ltx_tr">
<th id="S3.T3.6.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.6.4.1.1" class="ltx_text" style="font-size:144%;">Li </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.6.4.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Li et al<span class="ltx_text">.</span><span id="S3.T3.6.1.6.4.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib74" title="" class="ltx_ref">2021a</a><span id="S3.T3.6.1.6.4.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.6.4.2" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.6.4.2.1" class="ltx_text" style="font-size:144%;">Poisoning attacks</span></td>
<td id="S3.T3.6.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.6.4.3.1" class="ltx_text" style="font-size:144%;">Clipping, Krum</span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.6.4.3.2.1" class="ltx_text" style="font-size:144%;">(</span>Blanchard et al<span class="ltx_text">.</span><span id="S3.T3.6.1.6.4.3.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2017</a><span id="S3.T3.6.1.6.4.3.4.3" class="ltx_text" style="font-size:144%;">)</span></cite><span id="S3.T3.6.1.6.4.3.5" class="ltx_text" style="font-size:144%;">, K-norm</span>
</td>
<td id="S3.T3.6.1.6.4.4" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.6.4.4.1" class="ltx_text" style="font-size:144%;">Personalized method</span></td>
<td id="S3.T3.6.1.6.4.5" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.6.4.5.1" class="ltx_text" style="font-size:144%;">Test accuracy variance</span></td>
</tr>
<tr id="S3.T3.6.1.7.5" class="ltx_tr">
<th id="S3.T3.6.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.7.5.1.1" class="ltx_text" style="font-size:144%;">Singh </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.7.5.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Singh et al<span class="ltx_text">.</span><span id="S3.T3.6.1.7.5.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib115" title="" class="ltx_ref">2020</a><span id="S3.T3.6.1.7.5.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.7.5.2" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.7.5.2.1" class="ltx_text" style="font-size:144%;">Poisoning attacks</span></td>
<td id="S3.T3.6.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.7.5.3.1" class="ltx_text" style="font-size:144%;">Anomaly detection</span></td>
<td id="S3.T3.6.1.7.5.4" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.7.5.4.1" class="ltx_text" style="font-size:144%;">Microaggregation, Gaussian mixtureModels</span></td>
<td id="S3.T3.6.1.7.5.5" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.7.5.5.1" class="ltx_text" style="font-size:144%;">False negative rate</span></td>
</tr>
<tr id="S3.T3.6.1.8.6" class="ltx_tr">
<th id="S3.T3.6.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.8.6.1.1" class="ltx_text" style="font-size:144%;">Song </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.8.6.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Song et al<span class="ltx_text">.</span><span id="S3.T3.6.1.8.6.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib117" title="" class="ltx_ref">2021</a><span id="S3.T3.6.1.8.6.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.8.6.2" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.8.6.2.1" class="ltx_text" style="font-size:144%;">Data Poisoning attacks</span></td>
<td id="S3.T3.6.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.8.6.3.1" class="ltx_text" style="font-size:144%;">Reputation-based scheduling policy</span></td>
<td id="S3.T3.6.1.8.6.4" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.8.6.4.1" class="ltx_text" style="font-size:144%;">Reputation model</span></td>
<td id="S3.T3.6.1.8.6.5" class="ltx_td ltx_align_center" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.8.6.5.1" class="ltx_text" style="font-size:144%;">Reputation value</span></td>
</tr>
<tr id="S3.T3.6.1.9.7" class="ltx_tr">
<th id="S3.T3.6.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;">
<span id="S3.T3.6.1.9.7.1.1" class="ltx_text" style="font-size:144%;">Chen </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.6.1.9.7.1.2.1" class="ltx_text" style="font-size:144%;">(</span>Chen et al<span class="ltx_text">.</span><span id="S3.T3.6.1.9.7.1.3.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib27" title="" class="ltx_ref">2022</a><span id="S3.T3.6.1.9.7.1.4.3" class="ltx_text" style="font-size:144%;">)</span></cite>
</th>
<td id="S3.T3.6.1.9.7.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.9.7.2.1" class="ltx_text" style="font-size:144%;">Poisoning attacks</span></td>
<td id="S3.T3.6.1.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.9.7.3.1" class="ltx_text" style="font-size:144%;">Mutual evaluation mechanism</span></td>
<td id="S3.T3.6.1.9.7.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.9.7.4.1" class="ltx_text" style="font-size:144%;">Mutual evaluation mechanism</span></td>
<td id="S3.T3.6.1.9.7.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.6pt;padding-bottom:3.6pt;"><span id="S3.T3.6.1.9.7.5.1" class="ltx_text" style="font-size:144%;">Correlation coefficient</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS4.SSS2.p4" class="ltx_para">
<p id="S3.SS4.SSS2.p4.1" class="ltx_p">Several methods have been proposed to strike a balance between security and fairness in federated learning. First consider that unfairness in federated learning has been attributed to two main factors: data heterogeneity and security attacks. Both have been explored in the literature.</p>
</div>
<div id="S3.SS4.SSS2.p5" class="ltx_para">
<p id="S3.SS4.SSS2.p5.1" class="ltx_p">For example, Xu and Lyu <cite class="ltx_cite ltx_citemacro_citep">(Xu and Lyu, <a href="#bib.bib141" title="" class="ltx_ref">2020</a>)</cite> introduced a federated learning framework that addresses both fairness and security concerns that specifically focuses on targeted poisoning attacks (e.g., label-flipping), untargeted poisoning attacks, and free-riders attacks. Their approach incorporates a reputation mechanism, where a client’s reputation is determined based on a contribution value that is assessed using the test accuracy from locally trained models and gradient uploads. This helps to identify and reward contributing clients while identifying and removing non-contributors or malicious actors. However, while effective against security attacks like backdoor attacks, the security and robustness mechanisms in a federated learning framework may inadvertently filter out users with simpler data, potentially undermining fairness.</p>
</div>
<div id="S3.SS4.SSS2.p6" class="ltx_para">
<p id="S3.SS4.SSS2.p6.4" class="ltx_p">Data heterogeneity can further complicate fairness concerns, as benign clients might exhibit the characteristics of overfitting and so resemble a compromised client. This resemblance may allow backdoor attackers to disguise themselves and evade feature checks  <cite class="ltx_cite ltx_citemacro_citep">(Zawad et al<span class="ltx_text">.</span>, <a href="#bib.bib146" title="" class="ltx_ref">2021</a>)</cite>. Notably, data heterogeneity can amplify the weight differences between benign clients. Consequently, benign clients may become less distinguishable from malicious ones. For this reason, cosine similarity is often used to detect weight exceptions <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>. To tackle this challenge, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite> introduced Ditto, a personalized federated learning framework primarily designed to address data heterogeneity. When malicious attackers disrupt the process of training the global model, sending that global model to different clients will result in suboptimal outcomes. However, local benign clients may struggle to train an effective model using only their local data. Hence, Ditto offers a trade-off between personalized and global models through a hyperparameter <math id="S3.SS4.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.SSS2.p6.1.m1.1a"><mi id="S3.SS4.SSS2.p6.1.m1.1.1" xref="S3.SS4.SSS2.p6.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p6.1.m1.1b"><ci id="S3.SS4.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p6.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p6.1.m1.1c">\lambda</annotation></semantics></math>. A higher <math id="S3.SS4.SSS2.p6.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.SSS2.p6.2.m2.1a"><mi id="S3.SS4.SSS2.p6.2.m2.1.1" xref="S3.SS4.SSS2.p6.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p6.2.m2.1b"><ci id="S3.SS4.SSS2.p6.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p6.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p6.2.m2.1c">\lambda</annotation></semantics></math> aligns the personalized model closer to the global model, while a lower <math id="S3.SS4.SSS2.p6.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.SSS2.p6.3.m3.1a"><mi id="S3.SS4.SSS2.p6.3.m3.1.1" xref="S3.SS4.SSS2.p6.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p6.3.m3.1b"><ci id="S3.SS4.SSS2.p6.3.m3.1.1.cmml" xref="S3.SS4.SSS2.p6.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p6.3.m3.1c">\lambda</annotation></semantics></math> allows greater deviation from the poisoned global model. By adjusting <math id="S3.SS4.SSS2.p6.4.m4.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.SSS2.p6.4.m4.1a"><mi id="S3.SS4.SSS2.p6.4.m4.1.1" xref="S3.SS4.SSS2.p6.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p6.4.m4.1b"><ci id="S3.SS4.SSS2.p6.4.m4.1.1.cmml" xref="S3.SS4.SSS2.p6.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p6.4.m4.1c">\lambda</annotation></semantics></math>, each client can find a personalized model that strikes a good balance between the global model and their own local model. Moreover, Ditto significantly improves both model accuracy and fairness under various attacks. It also minimizes the variance in test error rates such that benign clients are treated fairly. Lastly, experiments show that Ditto outperforms three other defense mechanisms in terms of global model test accuracy.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Fairness </h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In the previous section <a href="#S3" title="3. The links between PRIVACY, SECURITY AND FAIRNESS ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we explored the intricate relationship between privacy, security, and fairness. While these concepts may initially seem distinct, they are closely intertwined in a range of ways. A significant connection exists between privacy and security that primarily revolves around the sharing of gradients, and it is this connection that can potentially give rise to privacy and security vulnerabilities. Furthermore, privacy and security both share a connection with fairness, as a trade-off relationship exists between all three factors. Hence, if the objective of a training task is to safeguard client privacy and security, a singular focus on either one will likely compromise fairness. For this reason, these objectives should not be viewed in isolation, but rather all need to be considered together.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Having previously introduced fairness in Section <a href="#S2.SS5" title="2.5. Fairness in federated learning ‣ 2. Preliminary ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a> and fairness bias in Section <a href="#S3.SS2" title="3.2. Fairness as a bridge ‣ 3. The links between PRIVACY, SECURITY AND FAIRNESS ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, this section will focus on several contemporary approaches to bolstering fairness in federated learning, as well as how these approaches can be integrated into privacy and security considerations.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Fairness enhancing methods</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The kind of biases and unfairness commonly encountered in centralized machine learning is also prevalent in federated learning. Extensive research has been undertaken to tackle this issue, typically falling into three primary categories: (1) enhancing the aggregation algorithms; (2) establishing incentive mechanisms; and (3) tailoring the methodology, as illustrated in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1. Fairness enhancing methods ‣ 4. Fairness ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2406.10884/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="277" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Fairness enhance in federated learning from different perspectives</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Type 1, Aggregation Algorithm</span>.
To combat bias and unfairness in federated learning, numerous studies have proposed enhanced aggregation algorithms <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib132" title="" class="ltx_ref">2020b</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2020</a>, <a href="#bib.bib76" title="" class="ltx_ref">2019b</a>; Mohri et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2019</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite>. For example, both Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2019b</a>)</cite> and Mohri et al. <cite class="ltx_cite ltx_citemacro_citep">(Mohri et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2019</a>)</cite> have published novel optimization objectives to address fairness concerns. Mohri et al. <cite class="ltx_cite ltx_citemacro_citep">(Mohri et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2019</a>)</cite> proposed the concept of benevolent fairness to safeguard the worst performance of a random client, while Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2019b</a>)</cite> uses the uniformity of performance as a measure of fairness. Both studies leverage the idea of fair resource allocation to maintain overall performance and fairness.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Drawing inspiration from equitable resource allocation in wireless networks, Hu et al. <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite> introduced the parameter <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">q</annotation></semantics></math> to represent an aggregated weighted loss, assigning a higher relative weight to clients with greater losses. Additionally, they devised methods to preserve accuracy and defend against malicious clients without compromising either accuracy or fairness. Similarly, Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite> also employed a weighting strategy to promote fairness coupled with a double momentum gradient method. In the momentum gradient descent method, the exponential weighted average of the gradients is computed and then used to update the weights. The server then aggregates the weights of all clients based on the training accuracy and the number of times a client has participated in training. Thus, a change factor underpins the methodology behind this metric to align more closely with real-world scenarios.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Another solution, FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2020</a>)</cite>, addresses issues with heterogeneity, showing promise in combatting bias in heterogeneous networks. Given that the client devices in most federated networks are many and varied, expecting uniform workloads from each device is neither practical nor ideal. FedProx, which is a straightforward modification of the original FedAvg algorithm, achieves superior performance while also accommodating disparities in performance. It considers factors like differences in computing power and other variables related to the equipment involved in training rounds. FedProx also introduces a regularization term to address non-uniformity in local updates.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2021</a>)</cite> takes a a different approach, framing fairness as a conflict between clients. Their framework uses cosine similarity to detect gradient conflicts, which are resolved by adjusting the direction and magnitude of the gradient prior to gradient averaging.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">However, it is worth noting that the majority of studies taking the aggregation algorithm approach assume naive federated learning without and do not considering attacks. Yet, in practical situations, federated learning structures will be susceptible to these nefarus activities. Hence, fairness, privacy, and security all need to be considered comprehensively.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold">Type 2, Incentive Mechanism</span>. In addition to algorithmic approaches, incentive mechanisms have been proposed as a means to enhance fairness and accuracy in federated learning  <cite class="ltx_cite ltx_citemacro_citep">(Michieli and Ozay, <a href="#bib.bib88" title="" class="ltx_ref">2021</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib129" title="" class="ltx_ref">2019a</a>; Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2020</a>; Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2020b</a>)</cite>. Yu et al. <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2020</a>)</cite> introduces three equity criteria: contribution equity, expected loss distribution equity, and expected equity. The scheme is interesting but estimating the cost incurred by clients and quantifying their contributions is challenging.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">Lyu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2020b</a>)</cite> devised a local credit rating system to ensure fairness, assessing a client’s contribution to the learning process. This iterative system allows clients to converge towards a native model that aligns with their contributions while preserving client privacy through a three-tier onion encryption mechanism.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">Michieli and Ozay <cite class="ltx_cite ltx_citemacro_citep">(Michieli and Ozay, <a href="#bib.bib88" title="" class="ltx_ref">2021</a>)</cite> introduced a novel aggregation approach, wherein each client’s contribution to the aggregation model serves as a guarantee for equitable resource allocation. This method yields improved convergence speed and accuracy.</p>
</div>
<div id="S4.SS1.p10" class="ltx_para">
<p id="S4.SS1.p10.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib129" title="" class="ltx_ref">2019a</a>)</cite> proposed a technique for fairly assessing each client’s contributions. In a vertical federated learning framework, they use a Shapley value to gauge the importance of each client and determine their contribution. In horizontal federated learning, the contribution is calculated by removing an instance and retraining the model. The difference between the results of the new model and the original serves as a measure of the contribution.</p>
</div>
<div id="S4.SS1.p11" class="ltx_para">
<p id="S4.SS1.p11.1" class="ltx_p">In this approach to addressing fairness concerns, incentive mechanisms to motivate clients to deliver high-quality contributions to the learning program. The central challenge, though, is how to accurately compute the cost of participating in federated learning and how to estimate the client’s contribution.</p>
</div>
<div id="S4.SS1.p12" class="ltx_para">
<p id="S4.SS1.p12.1" class="ltx_p"><span id="S4.SS1.p12.1.1" class="ltx_text ltx_font_bold">Type 3, Personalize</span>. Another avenue for enhancing fairness in federated learning is personalized federated learning, where individual clients learn from distinct models tailored to accommodate the inherent heterogeneity in this learning paradigm  <cite class="ltx_cite ltx_citemacro_citep">(Divi et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2021a</a>; Balakrishnan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. In practical terms, heterogeneity represents a critical source of unfairness, stemming from factors such as computational disparities, variable client arrival times at the server, communication discrepancies, and varying data transmission rates, all of which can impact the overall durations of training on a client. Furthermore, statistical heterogeneity can manifest due to imbalanced data and differences in data distribution across clients.</p>
</div>
<div id="S4.SS1.p13" class="ltx_para">
<p id="S4.SS1.p13.1" class="ltx_p">Both Divi et al. <cite class="ltx_cite ltx_citemacro_citep">(Divi et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> and Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2021a</a>)</cite> have proposed personalized federated learning approaches to address such fairness concerns. Divi <cite class="ltx_cite ltx_citemacro_citep">(Divi et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>, for example, established a fairness index that uantifies the extent to which a personalized model offers equitable opportunities. Balakrishnan <cite class="ltx_cite ltx_citemacro_citep">(Balakrishnan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> also put forward a personalized method for ensuring fairness that focuses on the local models. Their method leverages importance sampling to tackle communication heterogeneity by harnessing client computing and communication resources. Furthermore, to overcome any issues arising from distinct data distributions, the framework personalizes the resources allocated to modelling so as to ensure improved performance for each client.</p>
</div>
<div id="S4.SS1.p14" class="ltx_para">
<p id="S4.SS1.p14.1" class="ltx_p">This approach to fairness empowers personalized methods to adapt to the diversity inherent in different models and federated settings for each client. Nevertheless, striking a balance between privacy preservation, model accuracy, and equitable resource allocation through a personalized approach remains a formidable challenge.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Enhancing fairness while balancing privacy and security </h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The above-mentioned methods for enhancing fairness primarily assume a fairly simplistic federated learning environment. As such, these approaches often overlook the privacy and security concerns inherent to federated learning. Thus, when attempting to ensure fairness, privacy, and security all at the same time, relying solely on a fairness algorithm will not be sufficient. This is because federated environments are susceptible to privacy breaches and security attacks by malicious participants, soo extra defenses will be required. That said, in addressing the challenge of simultaneously bolstering fairness, privacy, and security in federated learning, numerous studies have made notable progress
 <cite class="ltx_cite ltx_citemacro_citep">(Pentyala et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2020c</a>; Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Some have posed the aim of balancing privacy, security and fairness as additional constraints in a training task, effectively turning federated learning into multiple-objective optimization problem.
Pentyala et al.  <cite class="ltx_cite ltx_citemacro_citep">(Pentyala et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2022</a>)</cite>, for instance, proposed a privacy-preserving training approach that comprises pre-processing and post-processing phases. The pre-processing phase tackles issues related to heterogeneous data distributions and assigns weights to samples, effectively mitigating bias in a unified training dataset. Additionally, the client weights are encrypted to safeguard sensitive information. This method successfully aligns the optimization objectives of privacy and fairness, offering both model privacy and group fairness. Likewise, Zhang et al.  <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2020c</a>)</cite> also explored the optimization objectives of privacy and fairness constraints. They devised a Markov game framework to select to participate in each communication round, with client decisions contingent on the state of the global model. This approach maximizes fairness and accuracy while strictly preserving privacy through a security aggregation protocol that restricts each client’s access to local data. Singh et al.  <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a href="#bib.bib114" title="" class="ltx_ref">2023</a>)</cite> introduced security and fairness as optimization objectives. The goal with the security optimization is to detect any model or data poisoning activity, while the aim of the fairness optimization is to reduce client discrimination. The proposed framework considers scenarios where the mechanisms to defend against model poisoning might mistakenly filter out benign clients due to data heterogeneity. Three methods are outlined: micro-aggregation to distinguish minority members from attackers; a Gaussian mixture model to describe the distribution of client updates; and density-based clustering to characterize non-iid client update distributions.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Alternatively, several studies have integrated incentive mechanisms to bolster client fairness – the idea being that the final model distributed to clients is based on their contribution to building the model, with greater rewards allocated to clients who have made more substantial contributions. Lyu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2020b</a>)</cite> devised a mechanism for evaluating local credibility. The mechanism improves fairness while still ensuring privacy through an encryption scheme. Participants earn points for downloading gradients from other participants, with additional points attainable through sample or gradient uploads. These transactions are immutable and recorded on a blockchain, and homomorphic encryption safeguards all information during gradient uploads and downloads. Ruckel et al. <cite class="ltx_cite ltx_citemacro_citep">(Rückel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2022</a>)</cite> proposed an approach where fairness hinges on calculating the global performance of each client according to their actual parameters.
Rewards are then assigned accordingly. To mitigate information leaks, each client’s model update is perturbed via local differential privacy. Xu and Lyu <cite class="ltx_cite ltx_citemacro_citep">(Xu and Lyu, <a href="#bib.bib141" title="" class="ltx_ref">2020</a>)</cite> introduced a reputation mechanism to address security issues and enhance collaborative fairness. The mechanism scrutinizes the gradients uploaded by clients to identify and remove any malicious actors, thereby upholding security. The reputation system additionally quantifies the participants’ contributions, enabling tailored rewards based on distinct patterns of performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Privacy</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As mentioned, the main point of privacy vulnerabilities in a federated learning system is when gradients and model parameters ae exchanged between the clients and the central server. In this section, we aim to categorize privacy attacks and the methods used to defend against them. The section concludes with a discussion on some practical applications for safeguarding user privacy.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Privacy attacks</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">By their very nature, federated learning frameworks are designed to safeguard the privacy of their participants by circumventing the need to share raw data and sharing only model updates instead. Nevertheless, recent research has revealed that vulnerabilities still exist within these frameworks that can lead to significant privacy breaches  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite>. In this section, we present a comprehensive classification of privacy attacks and potential privacy leaks in federated learning landscapes. The four perspectives from which we view this subject include: attacks and leaks that can occur while training a model; attacks on gradients, which are particularly susceptible to breaches; privacy from the adversary’s perspective and their access to information; and data partitioning, both horizontal and vertical. Figure <a href="#S5.F4" title="Figure 4 ‣ 5.1. Privacy attacks ‣ 5. Privacy ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates these four perspectives.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2406.10884/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Privacy attacks from different multiple perspectives</span></figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S5.T4.3.2" class="ltx_text" style="font-size:90%;">The link between privacy target and fairness</span></figcaption>
<table id="S5.T4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.4.1.1" class="ltx_tr">
<th id="S5.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Privacy Target</th>
<th id="S5.T4.4.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T4.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Fairness</th>
<th id="S5.T4.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Remark</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.4.2.1" class="ltx_tr">
<td id="S5.T4.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Model</td>
<td id="S5.T4.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Attack</td>
<td id="S5.T4.4.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Yes</td>
<td id="S5.T4.4.2.1.4" class="ltx_td ltx_align_left ltx_border_t">Aggravates unfairness for the clients</td>
</tr>
<tr id="S5.T4.4.3.2" class="ltx_tr">
<td id="S5.T4.4.3.2.1" class="ltx_td"></td>
<td id="S5.T4.4.3.2.2" class="ltx_td ltx_align_left">Defense</td>
<td id="S5.T4.4.3.2.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S5.T4.4.3.2.4" class="ltx_td ltx_align_left">The cost of protecting the model privacy means sacrificing fairness</td>
</tr>
<tr id="S5.T4.4.4.3" class="ltx_tr">
<td id="S5.T4.4.4.3.1" class="ltx_td ltx_align_left">Gradient</td>
<td id="S5.T4.4.4.3.2" class="ltx_td ltx_align_left">Attack</td>
<td id="S5.T4.4.4.3.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S5.T4.4.4.3.4" class="ltx_td ltx_align_left">Aggravates unfairness for the clients</td>
</tr>
<tr id="S5.T4.4.5.4" class="ltx_tr">
<td id="S5.T4.4.5.4.1" class="ltx_td"></td>
<td id="S5.T4.4.5.4.2" class="ltx_td ltx_align_left">Defense</td>
<td id="S5.T4.4.5.4.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S5.T4.4.5.4.4" class="ltx_td ltx_align_left">Adding differential privacy to the gradients increases unfairness</td>
</tr>
<tr id="S5.T4.4.6.5" class="ltx_tr">
<td id="S5.T4.4.6.5.1" class="ltx_td ltx_align_left">Training Data</td>
<td id="S5.T4.4.6.5.2" class="ltx_td ltx_align_left">Attack</td>
<td id="S5.T4.4.6.5.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S5.T4.4.6.5.4" class="ltx_td ltx_align_left">Aggravates unfairness for the clients</td>
</tr>
<tr id="S5.T4.4.7.6" class="ltx_tr">
<td id="S5.T4.4.7.6.1" class="ltx_td ltx_border_b"></td>
<td id="S5.T4.4.7.6.2" class="ltx_td ltx_align_left ltx_border_b">Defense</td>
<td id="S5.T4.4.7.6.3" class="ltx_td ltx_align_left ltx_border_b">Yes</td>
<td id="S5.T4.4.7.6.4" class="ltx_td ltx_align_left ltx_border_b">Improving fairness for clients decreases the model’s fairness</td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Attacks on trained models.</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">In a privacy leak, an unauthorized observer is able to see or infer all or some of the features of a target training sample simply by analyzing the output of the trained model.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">Thus, trained models sit at the nexus of privacy leaks in federated learning. In fact, there are four main types of privacy attack that target trained models: (1) model inversion attacks; (2) model extraction attacks; (3) membership inference attacks; and (4) property inference attacks.</p>
</div>
<div id="S5.SS1.SSS1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.p3.1" class="ltx_p"><span id="S5.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Model inversion attacks</span>.
Model inversion attacks exploit access to a model to deduce information about the training data. More specifically, the intention is to extract sensitive features about the model’s input based on some corresponding output from the model.</p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.p4.1" class="ltx_p">The pioneering model inversion attack was introduced in the context of genomic privacy, with Fredrikson et al. <cite class="ltx_cite ltx_citemacro_citep">(Fredrikson et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2014</a>)</cite>demonstrating that information could be recovered from a training set even with only black-box access to the prediction model. This attack was subsequently extended to novel settings where sensitive features could be inferred from the inputs to a decision tree model  <cite class="ltx_cite ltx_citemacro_citep">(Fredrikson et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2015</a>)</cite>. These attacks are most effective in scenarios where the inferred representative class originates from a small training set.</p>
</div>
<div id="S5.SS1.SSS1.p5" class="ltx_para">
<p id="S5.SS1.SSS1.p5.1" class="ltx_p">In cases involving malicious clients, Wang et al.  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2019b</a>)</cite> presented a multi-task framework based on a GAN designed to construct user-level private information from input samples, such as the true category of an instance or the identity of the client. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2020b</a>)</cite> also turned to GANs to guide an inversion process, establishing a fundamental link between a model’s predictive capabilities and its susceptibility to inversion attacks.</p>
</div>
<div id="S5.SS1.SSS1.p6" class="ltx_para">
<p id="S5.SS1.SSS1.p6.1" class="ltx_p">Notably, the abovementioned methods are passive attacks. But model owners should take caution because adversaries can also actively interfere with a model’s training process. For example, Hitaj et al.
 <cite class="ltx_cite ltx_citemacro_citep">(Hitaj et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> outline a novel class of active attacks that involve launching a model inversion attack on some GANs to reconstruct facial images of the users through white-box mechanisms. Here, the adversary uses deception to extract accurate yet sensitive information about the victim.</p>
</div>
<div id="S5.SS1.SSS1.p7" class="ltx_para">
<p id="S5.SS1.SSS1.p7.1" class="ltx_p"><span id="S5.SS1.SSS1.p7.1.1" class="ltx_text ltx_font_bold">Model extract attacks</span>. In a model extraction attacks, the adversary has no prior knowledge of the target model’s parameters or training data and so attempts to obtain the model’s parameters by extracting the target model. As a result, the adversary actually increases the success rate of subsequent attacks on the model’s training data. Ateniese and colleagues <cite class="ltx_cite ltx_citemacro_citep">(Ateniese et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> were the first to address the notion of extracting unexpected but useful information from a trained model. They designed a meta-classifier and trained it to hack other classifiers so as to infer sensitive information about the training set. In <cite class="ltx_cite ltx_citemacro_citep">(Tramèr et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2016</a>)</cite>, Tramèr et al. demonstrated an equation-solving method of model extraction, extending Ateniese et al.’s work. They also demonstrated successful model extraction attacks against a variety of model types that output only class labels.</p>
</div>
<div id="S5.SS1.SSS1.p8" class="ltx_para">
<p id="S5.SS1.SSS1.p8.1" class="ltx_p"><span id="S5.SS1.SSS1.p8.1.1" class="ltx_text ltx_font_bold">Membership Inference Attacks</span>.
In a membership inference attack, the goal is to determine whether a sample record has been used to train a model, i.e., to infer whether that record is a member of the training set. The consequences of such attacks can be very serious for the individuals involved. For instance, if a data record is known to have been used in a model trained to classify types of cancer, a membership inference attack could potentially leak information about the health of that individual <cite class="ltx_cite ltx_citemacro_citep">(Shokri et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite>. Several recent studies have demonstrated that machine learning models, no matter the learning schema, are vulnerable to membership inference attacks  <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021</a>; Shokri et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2017</a>; Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2019b</a>)</cite>, even in black-box settings  <cite class="ltx_cite ltx_citemacro_citep">(Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2019b</a>)</cite>. For example, an adversary might discern whether a data record forms part of the model’s training set through an API. In this category of attack, Nasr  <cite class="ltx_cite ltx_citemacro_citep">(Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>)</cite> devised a white-box membership inference strike that can retrieve private data from the model but fails to obtain data from a model but cannot obtain data from other models with the same distribution.</p>
</div>
<div id="S5.SS1.SSS1.p9" class="ltx_para">
<p id="S5.SS1.SSS1.p9.1" class="ltx_p"><span id="S5.SS1.SSS1.p9.1.1" class="ltx_text ltx_font_bold">Property Inference Attacks</span>.
Property inference involves identifying properties that hold for specific subsets of the training data but are not universally applicable to all class members. In these attacks, adversaries access trained models to extract global statistics about the training data  <cite class="ltx_cite ltx_citemacro_citep">(Chase et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. In Melis et al.’s <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite> attack, for example, the aim is to infer all the properties of a subset of the training data, focusing on the properties independent of class-specific features. For instance, they inferred whether certain images depict individuals wearing glasses using a gender classifier. These attacks can be either passive or active. In a passive attack, the adversary aims to infer two sets of data-one with and one without attributes. These data points must belong to the same class as the target participant but may not be dissimilar in other respects. In an active attack, the adversary actively seeks to infer attributes via multi-task learning.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Attacks on gradients</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">While gradient exchange was once considered a secure method for uploading data to a server, recent research has exposed this practice as being vulnerable to attack  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2021a</a>)</cite>. Even though the central server avoids direct access to user data, gradient attacks have demonstrated the potential to recover a user’s local training data from exchanged parameter gradients. When clients share their models with a central server, malicious attackers can intercept those shared gradients and reconstruct sensitive information about the participant, resulting in privacy breaches.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">Consider, for example, a case where the adversary is actually a participant in the training scheme. Under these circumstances, the adversary could optimize dummy gradients to approximate the shared gradients and, in the process, infer both inputs and labels  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite>. Notably, this method tends to be less effective in deep networks. But a simpler, yet effective, approach involves extracting the ground-truth label based on the sign of the gradient vector <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite>. This approach is applicable to any differentiable model trained with cross-entropy loss over one-hot labels, which is the typical scenario for classification tasks.</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.p3.1" class="ltx_p">Geiping et al. <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>)</cite> go so far as to demonstrated that it is possible to reconstruct the input to a fully-connected layer from parameter gradients, regardless of the layer’s position in a neural network. Their study shows that gradient inversion can faithfully reconstruct samples, even for trained and untrained parameters in deep and non-smooth architectures. Another method can recover a single image from a batch by inverting the averaged gradients and using a fully-connected layer’s gradients to recover ground-truth labels for the labels within the batch  <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2021a</a>)</cite>. Importantly, all these methods are capable of recovering training data without the need to transform any gradients.</p>
</div>
<div id="S5.SS1.SSS2.p4" class="ltx_para">
<p id="S5.SS1.SSS2.p4.1" class="ltx_p">In another vein, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite> outlines a gradient attack method called generative gradient leakage (GGL). GGLs rely on a generative model trained on public datasets as prior information to ensure a quality reconstruction given noise and defensive transformations. To reduce the search space and enhance the quality of the generated image, the hidden expression closest to the gradient of the real image is identified within the potential space of a GAN.</p>
</div>
<div id="S5.SS1.SSS2.p5" class="ltx_para">
<p id="S5.SS1.SSS2.p5.1" class="ltx_p">In these research endeavors, adversaries approximate and reconstruct the local models of clients using gradients, which leads to privacy breaches. Consequently, safeguarding gradients has become a significant challenge in federated learning, as to at least some extent, preventing gradient leaks can mitigate privacy risks.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3. </span>The adversary’s perspective</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">Training in federated learning involves rounds of communication that adversaries can exploit to reveal data. Within this process, the adversary can either be an eavesdropper or a participant. Eavesdroppers observe the process without impacting performance – their objective being to expose and gather sensitive information, such as the training data or the model parameters. Participants, on the other hand, are those directly involved in the process, i.e., a client or even the central server. Hence, participants have the ability to both observe and modify the training data or model parameters. Figure <a href="#S5.F5" title="Figure 5 ‣ 5.1.3. The adversary’s perspective ‣ 5.1. Privacy attacks ‣ 5. Privacy ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates these three types of adversary information.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.10884/assets/x5.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="102" height="76" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">((a))</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.10884/assets/x6.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="90" height="78" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">((b))</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.10884/assets/x7.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="98" height="76" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">((c))</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Adversary information in federated learning privacy attacks</span></figcaption>
</figure>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">When the central server acts as an adversary, it can either be semi-honest or malicious. In a semi-honest setting, the objective is to reveal user data  <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>)</cite>. Here, the adversary does not typically alter the model architecture to enhance their attack, nor do they transmit malicious global parameters that do not genuinely represent the global model. However, they can access the current global model and the shared gradients <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>. From this information, they should be able to infer sensitive information about the clients’ data <cite class="ltx_cite ltx_citemacro_citep">(Fereidooni et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S5.SS1.SSS3.p3" class="ltx_para">
<p id="S5.SS1.SSS3.p3.1" class="ltx_p">In a malicious setting, the adversary’s goal is to reconstruct the private data of a target client, enabling client-level privacy attacks <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2019b</a>)</cite>. Attacks like gradient disaggregation fall into this category <cite class="ltx_cite ltx_citemacro_citep">(Lam et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2021</a>)</cite>, where a malicious central server performs the attack, potentially conducting inference attacks on the model updates of individual participants.</p>
</div>
<div id="S5.SS1.SSS3.p4" class="ltx_para">
<p id="S5.SS1.SSS3.p4.1" class="ltx_p">However, when an adversary impersonates a client, the attack’s potency increases significantly. For example, Melis et al. <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite> analyzed periodic training updates and was able to infer the properties of the participants’ training data from them. However, notably, this analysis was independent of the class characteristics. By contrast, Fu et al.’s <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite> approach was somewhat more novel. After completing the training process, each client receives a trained bottom model. The adversary can then train a model to infer information based on the trained bottom model and can infer labels for any sample of interest, not limited to those in the training dataset. The adversary might even take the form of a curious client <cite class="ltx_cite ltx_citemacro_citep">(Nasr et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2019</a>)</cite>, actively influencing the target model to extract more information about its training set during the training process.</p>
</div>
<div id="S5.SS1.SSS3.p5" class="ltx_para">
<p id="S5.SS1.SSS3.p5.1" class="ltx_p">When the adversary is an eavesdropper, the aim is usually to pilfer sensitive information exchanged between the clients and server, such as a training update or the final global model <cite class="ltx_cite ltx_citemacro_citep">(Elgabli et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>. Recent studies  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite> reveal that eavesdroppers first create dummy data to generate a gradient via local training and subsequently use this gradient to approximate the real gradient, leading to profound privacy breaches as the original training samples can be reconstructed from this shared gradient.</p>
</div>
</section>
<section id="S5.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.4. </span>Data partitioning</h4>

<div id="S5.SS1.SSS4.p1" class="ltx_para">
<p id="S5.SS1.SSS4.p1.1" class="ltx_p">There are two data partitioning schemes in federated learning: horizontal and vertical. Each is contingent on how the data is distributed across the sample and feature spaces. In horizontal data partitioning, the client datasets share the same feature spaces but have different sample spaces. Conversely, in vertical data partitioning, the client datasets have different feature spaces but the same sample spaces.</p>
</div>
<div id="S5.SS1.SSS4.p2" class="ltx_para">
<p id="S5.SS1.SSS4.p2.1" class="ltx_p">In the realm of horizontal data partitioning, Google introduced an application for waking an Android phone using wake-word recognition <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2016</a>)</cite>. However, Zhu et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite> revealed the potential for malicious attackers to reveal the raw features and labels in some client data simply by having knowledge of the model’s architecture, parameters, and communicated gradient loss. Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite> further demonstrated that the ground truth label of an example could be extracted by exploiting the index of the output associated with a negative gradient. Hitaj et al. <cite class="ltx_cite ltx_citemacro_citep">(Hitaj et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> explored how an honest client could inadvertently expose their private data to an inference attack – that is, by using GANs to generate samples intended to mimic the same distribution as the training data. Generally, the aim of these membership inference attacks is to acquire information by determining whether a sample distribution exists within a training set <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S5.SS1.SSS4.p3" class="ltx_para">
<p id="S5.SS1.SSS4.p3.1" class="ltx_p">In the context of vertical data partitioning, Hardy et al. <cite class="ltx_cite ltx_citemacro_citep">(Hardy et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2017</a>)</cite> introduced a protocol tailored for linear models. However, privacy leaks within vertical data partitioning scenarios remain relatively underexplored.</p>
</div>
<div id="S5.SS1.SSS4.p4" class="ltx_para">
<p id="S5.SS1.SSS4.p4.1" class="ltx_p">Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2021b</a>)</cite> argued that stringent measures are necessary to safeguard the privacy of the labels in participant data. This caution arises from a recognition that raw labels may contain highly sensitive information. On this note, Fu et al. <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite> presented a novel set of label inference attacks specifically designed for vertical federated learning. In these attacks, a participant without access to the labels assumes the role of the adversary, seeking to infer the labels. Conversely, Cheng <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> proposed an end-to-end privacy-preserving tree-boosting framework, which ensures that all participants are shielded from leaking data information to one another.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Methods of privacy preservation </h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In light of the diverse methods for perpetrating a privacy attack within a federated learning setting, it is abundantly clear that safeguarding against such breaches is of the utmost significance. Fortunately, there are also numerous methods of bolstering privacy protection.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">An intricate relationship exists between privacy breaches and defense mechanisms. For instance, the incorporating differential privacy into local data can be a very effective measure. This strategic addition of noise ensures that, even if an adversary does manage to recover the data through a reconstruction attack, that data will be too noisy to decipher. Similarly, when contending with a model inversion attack, introducing differential privacy to the results after the local/global model has been trained should be an effective countermeasure to stop the adversary from inferring any attributes.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Further, a substantial connection exists between confidential computation technologies and defensive measures. For example, techniques like model updates and data encryption act as formidable barriers that can stop adversaries from accessing sensitive information. Consequently, these measures effectively stymie an array of attacks, including those centered around reconstructing or inferring data from gradients.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Another defense mechanism is secure aggregation. This is a communications measure that streamlines model aggregation. It deftly strikes a balance between privacy preservation and communication efficiency, thereby ensuring data protection while enabling seamless collaboration.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Last on the list is knowledge distillation. This process circumvents the exchange of model parameters, preemptively thwarting attacks that seek to exploit these parameters so as to infer member information or recover the original data.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Differential privacy</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">In federated learning, collaboratively training a shared model is an iterative process involving multiple devices. Beyond not sharing private data, two of the main vulnerabilities that remain in this privacy-preserving learning paradigm are inference <cite class="ltx_cite ltx_citemacro_citep">(Shokri et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2017</a>; Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite> and reconstruction  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">Differential privacy and its two modalities – local and global differential privacy – can be a powerful safeguard against both these issues.</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para">
<p id="S5.SS2.SSS1.p3.1" class="ltx_p">Global differential privacy depends on a trusted server that is responsible for injecting noise into the output, typically done during the global model updates. In Geyer et al.’s <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite> mechanism, the server randomly selects which participants will train a local model. This is followed by global model updates during each communication round as usual, but random Gaussian noise is injected into the global model during aggregation updates, as outlined by McMahan et al. <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2017b</a>)</cite>. In fact,
McMahan et al. pioneered the application of user-level privacy protection for language models, underpinned by global differential privacy. Notably, this marked the initial application of differential privacy to averaging algorithms in federated learning for safeguarding user-level sensitive information.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para">
<p id="S5.SS2.SSS1.p4.1" class="ltx_p">Another notable contribution was by Wei et al. <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib135" title="" class="ltx_ref">2020</a>)</cite>, who introduced the NbAFL framework. In NbAFL, artificial noise is thoughtfully infused into the participants’ parameters before any updates transpire. This proactive integration of global differential privacy by introducing only a limited amount of noise can extend the protective purview of the privacy guarantee to entire datasets, concurrently guarding the participants’ privacy while maintaining commendable levels of accuracy. Nonetheless, it is imperative to acknowledge that the Achilles’ heel of global differential privacy resides in the challenge of determining an appropriate sensitivity level, as this parameter has a substantial influence on both the level of privacy assured and the model’s performance.</p>
</div>
<div id="S5.SS2.SSS1.p5" class="ltx_para">
<p id="S5.SS2.SSS1.p5.1" class="ltx_p">Local differential privacy stands apart from global differential privacy in that it directly safeguards sensitive data. Here, noise is added locally and so there is no need for a trusted server to intervene. This noise injection can happen in one of two ways:</p>
</div>
<div id="S5.SS2.SSS1.p6" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p">noise is added to the user’s local gradients before the parameter updates is transmitted; or</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p">noise is added to the local model.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS2.SSS1.p7" class="ltx_para">
<p id="S5.SS2.SSS1.p7.1" class="ltx_p">To fortify a system’s defenses against inference attacks, Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib158" title="" class="ltx_ref">2020c</a>)</cite> introduced an algorithm known as LDP-FedSGD, tailor-made for IoT. This algorithm prevents an adversary from inferring any original data by perturbing the gradients before any updates are sent. In this setup, the server orchestrates updates to the global model by averaging the outcomes of these perturbed gradients.</p>
</div>
<div id="S5.SS2.SSS1.p8" class="ltx_para">
<p id="S5.SS2.SSS1.p8.1" class="ltx_p">Truex et al. <cite class="ltx_cite ltx_citemacro_citep">(Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2020</a>)</cite> introduced a scheme, called LDP-Fed, that introduces perturbations to each participant’s gradient based on a localized instance of local differential privacy module. This approach diligently shields the participants’ data against inference attacks.</p>
</div>
<div id="S5.SS2.SSS1.p9" class="ltx_para">
<p id="S5.SS2.SSS1.p9.1" class="ltx_p">Also tackling the realm of IoT, Cao et al. <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020b</a>)</cite> explored an innovative framework called IFed. IFed introduces standard normal noise to users’ local models to create new models that subsequently update the global model. This strategic noise injection serves as a deterrent against anyone trying to reconstruct the raw data.</p>
</div>
<div id="S5.SS2.SSS1.p10" class="ltx_para">
<p id="S5.SS2.SSS1.p10.1" class="ltx_p">Wu et al. <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2021</a>)</cite> contrived an incentive-based framework that helps to protect sensitive information by infusing artificial Gaussian noise into the local model. This measure strategically thwarts any attempts at data reconstruction. Bhowmick et al. <cite class="ltx_cite ltx_citemacro_citep">(Bhowmick et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> also leverage local differential privacy to fortify defenses against data reconstruction during client-side communications while upholding the privacy of any model during global updates. In this way, local differential privacy emerges as a robust solution for averting privacy breaches from a participant’s vantage point, affording a more potent privacy guarantee.</p>
</div>
<div id="S5.SS2.SSS1.p11" class="ltx_para">
<p id="S5.SS2.SSS1.p11.1" class="ltx_p">In summary, within the landscape of federated learning, both global differential privacy and local differential privacy emerge as potent tools for privacy preservation. While global differential privacy finds applicability across entire datasets, it depends on a trusted server and carries the challenge of determining the correct sensitivity levels. By contrast, local differential privacy offers participants a more robust privacy guarantee.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Confidential computation technology</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Confidential computation technology serves as a pivotal asset in preventing privacy breaches and enhancing accuracy within the sphere of federated learning. Federated learning faces an array of privacy threats, notably inference attacks and gradient attacks, which can be effectively mitigated through the use of three distinct categories of technology: (1) homomorphic encryption, (2) secret sharing, and (3) secure multi-party computation.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p"><span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Homomorphic encryption:</span> With homomorphic encryption, one can directly encrypt data without the need for a secret key. However, even with homomorphic encryption, an adversary might recover some training samples or glean membership information by inferring data from shared gradients. To counteract this concern, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite> introduced an enhanced privacy framework couplied with a public key that involves a homomorphic encryption mechanism to encrypt gradients. Similarly, Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite> devised a privacy-preserving strategy for federated training processes that integrates additive homomorphic cryptosystems to guarantee the confidentiality of sensitive user information. This multifaceted approach involves initializing encrypted aggregated values, user reliability updates, aggregated value updates, and weighted federated parameter updates. Thus, homomorphic encryption not only significantly bolsters accuracy <cite class="ltx_cite ltx_citemacro_citep">(Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2019a</a>)</cite> but can also extends formal privacy assurances by encrypting both the participants’ data and the aggregated updates.</p>
</div>
<div id="S5.SS2.SSS2.p3" class="ltx_para">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p"><span id="S5.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Secure multi-party computation:</span> Secure multi-party computation (SMC) <cite class="ltx_cite ltx_citemacro_citep">(Canetti et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">1996</a>)</cite> plays a pivotal role in addressing the inherent challenge of determining whether to trust a scheme’s participants, each of whom possesses confidential data. This is because secure multi-party computation guarantees that each participant can access accurate computation results based on their own data but not any information beyond the results themselves. Bonawitz <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> devised a protocol based on secure multi-party computation to compute the sum of the aggregated updates. The aim was to not only advance techniques for preserving privacy but also enhance communication efficiency. Recent developments have witnessed the design of secure protocols intended to achieve secure predictions for deep neural networks (DNNs). In this context, Agrawal et al. <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> introduced a novel approach to secure DNN computation that boasts semi-honest security. This method leverages an efficient implementation of backpropagated gradients to replace quantization and normalization in secure computation without compromising accuracy.</p>
</div>
<div id="S5.SS2.SSS2.p4" class="ltx_para">
<p id="S5.SS2.SSS2.p4.1" class="ltx_p"><span id="S5.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Secret sharing:</span> Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2019</a>)</cite> adopted a variant of secret sharing technology.
The approach both safeguards the privacy of local gradients and addresses the issue of participants dropping out during training. Dong et al. <cite class="ltx_cite ltx_citemacro_citep">(Duan et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> also incorporated secret sharing into the local shared gradients of participants in a method that allows the servers to both receive secret shares and compute the secret shares of aggregated gradients, striking a balance between efficiency and security. Applications of secret sharing extends to a federated transfer learning frameworks <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2019</a>)</cite>. Here, the goal is to preserve data privacy against adversaries while also improving efficiency.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3. </span>Secure aggregation</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">Secure aggregation protocols, which reside on the server of a federated learning system, combine the many local models sent by clients into a global model <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>. Within this protocol, each user conceals their local updates via a random mask before transmitting the masked updates to the server. The introduced randomness offsets both the private key and a paired random key, enabling the server to aggregate all the client models. Once the protocol has run its course, the server only has knowledge of the aggregated model, and not any knowledge of the individual models, as they remain obscured by unknown random keys.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p">So et al. <cite class="ltx_cite ltx_citemacro_citep">(So et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2020</a>)</cite> were the first to introduced the idea of a secure aggregation framework, and, notably, their implementation not only ensures privacy but also guarantees convergence. However, this does highlight that there is a fundamental trade-off between the scale of the network, user loss, and privacy protection. Elkordy and Avestimehr <cite class="ltx_cite ltx_citemacro_citep">(Elkordy and Avestimehr, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> safeguard each user’s model updates by nullifying the mutual information between the masked model and the unshielded model. In this way, they secure the local model update in the strong information-theoretic sense. Heterogeneous quantization has also been deployed to strike a better balance between training accuracy and communication efficiency.</p>
</div>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4. </span>Knowledge distillation</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Federated learning generally involves a multitude of decentralized computing nodes collaborating to train a centralized machine learning model, all while keeping local data samples local. This decentralized approach naturally results in uneven or disparate data distributions among the local models, compounded by multiple rounds of communication between nodes. This not only consumes substantial bandwidth but also amplifies the risk of leaks and introduces privacy challenges. To address these concerns, some privacy-preserving techniques employ distillation to further compress the size of the resultant global model while affording additional privacy assurances.</p>
</div>
<div id="S5.SS2.SSS4.p2" class="ltx_para">
<p id="S5.SS2.SSS4.p2.1" class="ltx_p">Sui et al. <cite class="ltx_cite ltx_citemacro_citep">(Sui et al<span class="ltx_text">.</span>, <a href="#bib.bib118" title="" class="ltx_ref">2020</a>)</cite> introduced a privacy-preserving medical extraction model tailored to the field of medicine that employs a knowledge distillation strategy. This strategy leverages the uploaded integrated local model for predictions, obviating the need to transmit the parameters of the entire local model to the central server. Another distillation approach is one-shot distillation, developed in response to the growing difficulty of servers and clients sharing large datasets. Gong et al. <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2021</a>)</cite> also proposed a federated learning framework tailored for one-shot distillation. This method safeguards the privacy of local data by exclusively utilizing model outputs devoid of any labels linked to common data during distillation-all without the need to exchange local model gradients. Furthermore, building upon prior research efforts, Gong et al. <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite> also explored an offline strategy to bolster privacy that effectively disconnects the local training data from the local server.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Privacy-preserving applications</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Privacy preservation in federated learning finds diverse applications in fields such as IoT, mobile edge computing, and blockchain. In this section, we offer an overview of the privacy-preserving methods for federated learning in each of these domains.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Privacy-preserving in IoT</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">Typically, IoT systems generate copious amounts of data, and traditional machine learning methods expose these data to privacy risks. However, endeavors to safeguard the privacy of this generated data within federated learning systems also need to mitigate the cost of data transmission. Federated learning finds utility across a spectrum of IoT domains, including smart homes <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2020b</a>)</cite>, industrial settings <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>, power management <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020b</a>)</cite>, and marine applications <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2021</a>)</cite>, and these applications generally involve critically sensitive information that needs to be protected. Hence, privacy-preserving technology not only plays a vital role in safeguarding these systems but also in enhancing the efficiency and accuracy of these applications. Notably, in industrial IoT applications, federated learning can incorporate blind technology to shield participants’ gradient information
 <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>. This ensures that the encrypted gradients of fellow participants remain secure and cannot be reversed.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p"><span id="S5.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Power IoT:</span> Power IoT is a rapidly growing segment within IoT systems that is facing escalating privacy risks. Unauthorized access could potentially lead to the inference of household behavior patterns by reverse-engineering device statuses from energy consumption data <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020b</a>)</cite>. In this context, local differential privacy protection is deemed more dependable than centralized protection. Federated learning, when applied to IoT scenarios, tackles the challenges posed when data is communicated to the cloud and provides robust support for privacy preservation while curbing communication costs.</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p"><span id="S5.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Mobile edge computing:</span> Traditional machine learning methods that rely on cloud-based infrastructure often centralize those data in cloud servers or data centers, raising critical privacy concerns. In response, edge computing has emerged as a promising solution, enabling the deployment of intelligent processes closer to the network’s edge while maintaining control over personal data. Federated learning offers an effective means to collaboratively train machine learning models without the need to transmit raw data to the cloud. However, as more participants willingly engage in cooperative model training, the potential to jointly infer a model rises <cite class="ltx_cite ltx_citemacro_citep">(Lim et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020</a>)</cite>. Yet, when integrated with differential privacy, federated learning systems based on mobile edge computing can effectively thwart adversaries attempting to exploit network interactions to gain access to private edge data <cite class="ltx_cite ltx_citemacro_citep">(Gottipati et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2020d</a>)</cite>. Still several challenges remain – significantly, resource allocation, computation costs, and handling unbalanced data in mobile edge computing.</p>
</div>
<div id="S5.SS3.SSS1.p4" class="ltx_para">
<p id="S5.SS3.SSS1.p4.1" class="ltx_p"><span id="S5.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Blockchain:</span> Blockchain, characterized by its decentralized structure, encompasses both licensed and unlicensed blockchains. It is a unique data structure that aggregates chronological data blocks, with nodes to collectively manage and share data. Recently, there has been a surge of interest in harnessing blockchain technology for deep learning <cite class="ltx_cite ltx_citemacro_citep">(Weng et al<span class="ltx_text">.</span>, <a href="#bib.bib136" title="" class="ltx_ref">2019</a>)</cite>. Moreover, several novel applications have integrated federated learning into blockchain frameworks to protect privacy and as a way of offering incentives <cite class="ltx_cite ltx_citemacro_citep">(Shayan et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2020</a>; Awan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>; Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>. For instance, Shayan et al. <cite class="ltx_cite ltx_citemacro_citep">(Shayan et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2020</a>)</cite> introduced a decentralized framework called Biscotti that ingeniously melds blockchain and cryptography to safeguard client updates while maintaining global model performance, scalability, and fault tolerance. Concurrently, Awan et al. <cite class="ltx_cite ltx_citemacro_citep">(Awan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> devised a blockchain-based privacy-preserving framework for federated learning settings, premised on the assumption that all the clients are semi-honest. Their method leverages blockchain’s immutable and decentralized trust properties to serve as a reliable source for model updates. Overall, blockchain’s intrinsic attributes, i.e., that it is tamper-proof, cannot be falsified, and carries no repudiation, are instrumental in documenting the flow of information in federated learning scenarios.</p>
</div>
<div id="S5.SS3.SSS1.p5" class="ltx_para">
<p id="S5.SS3.SSS1.p5.1" class="ltx_p"><span id="S5.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Smart healthcare:</span> Smart healthcare systems have traditionally relied on cloud sharing methods, which can make health information vulnerable to privacy attacks. However, health-related information is usually highly sensitive <cite class="ltx_cite ltx_citemacro_citep">(Act, <a href="#bib.bib5" title="" class="ltx_ref">1996</a>)</cite>. In this context, federated learning has emerged as a promising solution for safeguarding privacy. For example, Choudhury et al. <cite class="ltx_cite ltx_citemacro_citep">(Choudhury et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> proposed a collaborative mobile phone and server training model designed for health monitoring that recognizes human activity. The application also includes an awareness of privacy protection measures. Transfer learning techniques are used to personalize the training model, taking the substantial variances observed among different individuals into consideration <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2019a</a>)</cite> introduced a federated learning model designed for federated brain imaging that segments brain tumors using a DNN. In this model, each client shares weight updates with the server for aggregation via a model averaging technique.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Security</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Security attacks</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In this section, we undertake a comprehensive classification of the security issues intrinsic to federated learning. These issues are discussed from four distinct perspectives: the models, the gradients, the training data, and the adversary’s perspective. Each section describes the various methods used to attack these components. This is followed by a discussion of the available methods to defend against these attacks.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2406.10884/assets/x8.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="223" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S6.F6.3.2" class="ltx_text" style="font-size:90%;">Security in federated learning different perspectives of attacks</span></figcaption>
</figure>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S6.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="S6.T5.3.2" class="ltx_text" style="font-size:90%;">The link between security issues and fairness</span></figcaption>
<table id="S6.T5.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.4.1.1" class="ltx_tr">
<th id="S6.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Security issues</th>
<th id="S6.T5.4.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S6.T5.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Fairness</th>
<th id="S6.T5.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Remark</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.4.2.1" class="ltx_tr">
<td id="S6.T5.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Training Model</td>
<td id="S6.T5.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Attack</td>
<td id="S6.T5.4.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Yes</td>
<td id="S6.T5.4.2.1.4" class="ltx_td ltx_align_left ltx_border_t">Aggravates unfairness for clients</td>
</tr>
<tr id="S6.T5.4.3.2" class="ltx_tr">
<td id="S6.T5.4.3.2.1" class="ltx_td"></td>
<td id="S6.T5.4.3.2.2" class="ltx_td ltx_align_left">Defense</td>
<td id="S6.T5.4.3.2.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S6.T5.4.3.2.4" class="ltx_td ltx_align_left">Increases unfairness in the trained model</td>
</tr>
<tr id="S6.T5.4.4.3" class="ltx_tr">
<td id="S6.T5.4.4.3.1" class="ltx_td ltx_align_left">Training Data</td>
<td id="S6.T5.4.4.3.2" class="ltx_td ltx_align_left">Attack</td>
<td id="S6.T5.4.4.3.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S6.T5.4.4.3.4" class="ltx_td ltx_align_left">Increases data heterogeneity, aggravates unfairness</td>
</tr>
<tr id="S6.T5.4.5.4" class="ltx_tr">
<td id="S6.T5.4.5.4.1" class="ltx_td"></td>
<td id="S6.T5.4.5.4.2" class="ltx_td ltx_align_left">Defense</td>
<td id="S6.T5.4.5.4.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S6.T5.4.5.4.4" class="ltx_td ltx_align_left">Increases client fairness</td>
</tr>
<tr id="S6.T5.4.6.5" class="ltx_tr">
<td id="S6.T5.4.6.5.1" class="ltx_td ltx_align_left">Gradient</td>
<td id="S6.T5.4.6.5.2" class="ltx_td ltx_align_left">Attack</td>
<td id="S6.T5.4.6.5.3" class="ltx_td ltx_align_left">Yes</td>
<td id="S6.T5.4.6.5.4" class="ltx_td ltx_align_left">Reduces fairness between clients</td>
</tr>
<tr id="S6.T5.4.7.6" class="ltx_tr">
<td id="S6.T5.4.7.6.1" class="ltx_td ltx_border_b"></td>
<td id="S6.T5.4.7.6.2" class="ltx_td ltx_align_left ltx_border_b">Defense</td>
<td id="S6.T5.4.7.6.3" class="ltx_td ltx_align_left ltx_border_b">Yes</td>
<td id="S6.T5.4.7.6.4" class="ltx_td ltx_align_left ltx_border_b">Damages fairness for clients</td>
</tr>
</tbody>
</table>
</figure>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Attacks on trained models.</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">There are two main types of attacks on trained models, these being poisoning attacks and backdoor attacks. In either case, when a trained model is manipulated, either directly or indirectly, it can jeopardize the security of the entire training process.</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p">In a poisoning attack, the adversary can either attack the model or the data, with the aim of manipulating the target to sabotage its integrity. Further, in the case of a model poisoning attack, the attack can either be targeted or untargeted. Targeted attacks seek to minimize accuracy on specific test inputs, while untargeted attacks indiscriminately reduce accuracy on any test input within the global model.</p>
</div>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p id="S6.SS1.SSS1.p3.1" class="ltx_p">Backdoor attacks, on the other hand, involve inserting hidden triggers into the training model that force the model to output some fixed response when tested. Our primary focus here is on instances where the trained model is directly affected, as is the case with both backdoor and poisoning attacks.</p>
</div>
<div id="S6.SS1.SSS1.p4" class="ltx_para">
<p id="S6.SS1.SSS1.p4.1" class="ltx_p">Shejwalkar and Houmansadr <cite class="ltx_cite ltx_citemacro_citep">(Shejwalkar and Houmansadr, <a href="#bib.bib109" title="" class="ltx_ref">2021</a>)</cite> introduced a comprehensive framework for launching model poisoning attacks in a federated learning setting. The framework yields a multitude of poisoning attacks that surpass the capabilities of today’s state-of-the-art model poison attacks. Adversaries can compute a malicious model update by perturbing a benign model aggregation with a malicious vector. Here, the goal is to avoid being detected by an aggregation algorithm. Fang et al. <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite> proposed an attack that poisons local models, which involves manipulating the local parameters on a compromised device during the learning process. The objective is to construct local models on the compromised device in a way that maximally biases the global model in a specific direction, altering it before the attack occurs. Bhagoji et al. <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite> devised a targeted model poisoning attack that introduces stealth. This covert attack blends malicious updates into an adversarial target. Bagdasaryan  <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> designed a method of poisoning models that is based on replacing the model. The attacker use backdoor input data to train their model, with each training batch containing both correctly labeled data and backdoor data to facilitate the model distinguish between the two.</p>
</div>
<div id="S6.SS1.SSS1.p5" class="ltx_para">
<p id="S6.SS1.SSS1.p5.1" class="ltx_p">In all the above studies, both poisoning attacks and backdoor attacks have the potential to disrupt the availability of the federated learning system simply by compromising the trained model. Consequently, safeguarding the model is of paramount importance when attempting to build a robust security system.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Attacks on data.</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">When the integrity of a client’s training data is compromised, it has a cascading effect on the test error rate, ultimately putting a dent the global model’s accuracy. Adversaries typically undermine the veracity of training samples through tactics like data poisoning or backdoor attacks. Data poisoning involves directly and maliciously manipulating the training data, which has the potential to disrupt the entire system. Backdoor attacks on data are a form of data poisoning, with various schemes being devised for both image classification and text prediction <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.1" class="ltx_p">One prevalent method of data poisoning is label flipping. For example, Shejwalkar et al. <cite class="ltx_cite ltx_citemacro_citep">(Shejwalkar et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite> introduced a classic label flipping data poisoning attack based on label flipping and formulated the first comprehensive strategy to systematically address the data poisoning threat in federated learning. Tolpegin et al. <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite> similarly explored a label flipping attack as a way of executing a targeted data poisoning in federated learning. The effectiveness of such attacks hinges on the proportion of malicious participants in later rounds. Here, achieving maximum poisoning impact requires a high level of availability.</p>
</div>
<div id="S6.SS1.SSS2.p3" class="ltx_para">
<p id="S6.SS1.SSS2.p3.1" class="ltx_p">Backdoor attacks usually lead to the global model misclassifying specific test samples. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2020a</a>)</cite> outline how edge-case backdoors can lead to critical failures with substantial implications for fairness. Nguyen et al. <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>)</cite> devised a novel data poisoning attack that enables attackers to embed backdoors into an aggregation detection model, As a result, malicious traffic is misclassified as benign. Notably, attackers can leverage compromised IoT devices to implant backdoors without directly attacking the client. However, although these backdoors lack durability, they can persist in the model even after the attacker ceases to upload toxic updates. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2022a</a>)</cite> introduced Neurotoxin, a mechanism designed to embed more enduring backdoors into the training data within the federated learning systems. Notably, Neurotoxin can intelligently select the update direction of the update to circumvent clashes with benign clients.</p>
</div>
<div id="S6.SS1.SSS2.p4" class="ltx_para">
<p id="S6.SS1.SSS2.p4.1" class="ltx_p">From training data perspective, when some data is subject to malicious tampering or becomes tainted due to a poisoning or backdoor attack, it has the power to manipulate the global model. This not only endangers the security of the FL training process but also impinges upon the fairness for clients.</p>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Attacks on gradients.</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.1" class="ltx_p">Gradients serve as a critical intermediary element between clients and servers within federated learning frameworks. From a gradient perspective, both direct and indirect attacks have the potential to impact all the participants of a federated learning system. Additionally, it is important to note that attacks targeting both a trained model and the data can indirectly reverberate influencing the gradient. For instance, when a model itself is compromised or poisoned, the resulting gradient will carry this tainted information forward to the client or server, depending on the direction of the update. Similarly, data poisoning, which contaminates the training data, can also have an indirect effect on the gradient as this poisoned data is used for training, and will give rise to problematic gradients.</p>
</div>
<div id="S6.SS1.SSS3.p2" class="ltx_para">
<p id="S6.SS1.SSS3.p2.1" class="ltx_p">Both poisoning and backdoor implantation mechanisms fundamentally disrupt the updates generated by clients, with the overarching objective of altering the global model and compromising the security of the entire system. In the wake of model poisoning <cite class="ltx_cite ltx_citemacro_citep">(Shejwalkar et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>, malicious gradients will propagate an increased amount of inverted label data with a deleterious effect on the overall accuracy of the global model. Further, a malicious subset of mislabeled data will result in gradients that pollute the global model <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite>. Along these lines, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2022a</a>)</cite> introduced a more enduring form of backdoor attack relies on the attacker’s ability to access the gradient in the preceding round, This means the adversary can approximate a benign gradient in the subsequent round by computing the first <math id="S6.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS1.SSS3.p2.1.m1.1a"><mi id="S6.SS1.SSS3.p2.1.m1.1.1" xref="S6.SS1.SSS3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p2.1.m1.1b"><ci id="S6.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS3.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p2.1.m1.1c">k</annotation></semantics></math>% coordinates of the benign gradient and strategically adjusting it according to a predefined constraint set.</p>
</div>
<div id="S6.SS1.SSS3.p3" class="ltx_para">
<p id="S6.SS1.SSS3.p3.1" class="ltx_p">In light of the above research, it is evident that adversaries do have the ability to compromise gradients through poisoning or backdoor attacks. The repercussions of such actions then cascade, impacting the global model and ultimately resulting in compromised security. Consequently, devising robust methodologies to mitigate the risks posed by backdoor and poisoning attacks has emerged as a formidable challenge in the domain of federated learning security.</p>
</div>
</section>
<section id="S6.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4. </span>The adversary’s perspective</h4>

<div id="S6.SS1.SSS4.p1" class="ltx_para">
<p id="S6.SS1.SSS4.p1.1" class="ltx_p">Within the context of federated learning, adversaries can manifest as malicious clients that either infiltrate the system or work to compromise existing clients. These malevolent actors are driven by the overarching objective of sabotaging the performance of the global model. Hence, they pose a significant threat to the integrity of the entire federated learning framework.</p>
</div>
<div id="S6.SS1.SSS4.p2" class="ltx_para">
<p id="S6.SS1.SSS4.p2.1" class="ltx_p">Inherently, federated learning is a collaborative learning scheme, where clients collectively acquire shared global models. However, malicious clients can wield their influence to subvert the model selection process, causing the global model to predict spurious labels <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>. As an example, consider an image classification scenario where a malicious client clandestinely relabels the training images of cars as birds.The client’s model update is then sent to the server. As a result, the learned global model is duped into classifying cars as birds <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S6.SS1.SSS4.p3" class="ltx_para">
<p id="S6.SS1.SSS4.p3.1" class="ltx_p">Malicious clients also have the ability to manipulate the direction and magnitude of local model updates <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020a</a>)</cite>. In this disconcerting scenario, each client represents a potential locus of both malice and susceptibility to poisoning attacks, as viewed from the server’s vantage point. Notably, various studies <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2019</a>; Fang et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Xie et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2019</a>)</cite> have underscored that tampering with the model updates transmitted to the server can profoundly undermine the global model’s Byzantine robustness during the learning process.</p>
</div>
<div id="S6.SS1.SSS4.p4" class="ltx_para">
<p id="S6.SS1.SSS4.p4.1" class="ltx_p">In a distinct vein, Sun et al.’s research <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2021</a>)</cite> introduces the concept of malicious clients sharing a dataset replete with corrupted data labels. Consequently, when this dataset is employed as input, the global model consistently produces these same corrupted labels as outputs. Furthermore, the malicious client propagates training data infused with these spurious labels <cite class="ltx_cite ltx_citemacro_citep">(Kim and Lim, <a href="#bib.bib71" title="" class="ltx_ref">2022</a>)</cite>, giving rise to pernicious learning models. This, in turn, precipitates a substantial deterioration in global learning outcomes.</p>
</div>
<div id="S6.SS1.SSS4.p5" class="ltx_para">
<p id="S6.SS1.SSS4.p5.1" class="ltx_p">Such declines in a global model’s performance are largely due to an adversary manipulating the local client’s model or their local data. Effectively defending against these malevolent clients to uphold the security of federated learning constitutes a formidable challenge.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Security defenses</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Table <a href="#S6.T6" title="Table 6 ‣ 6.2. Security defenses ‣ 6. Security ‣ Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the correlation between security attacks and the corresponding defense methodologies, noting that the best choice of defense method is almost always contingent upon a number of factors, such as the nature of the attack and any known information about the attacker.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">In a model poisoning attack, the adversary either tampers with the model or its parameters. Hence, the primary defense strategy revolves around identifying and removing these malevolent models. Conversely, when dealing with a data poisoning attack, the adversary’s goal is to manipulate or inject malicious content into the training data. Mitigating this attack involves eliminating the compromised or malicious party.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">In scenarios involving backdoor attacks, the adversary clandestinely embeds a trigger in the model or training data. Defense mechanisms here encompass strategies like data perturbation or implementing a detection mechanism to identify and neutralize these surreptitious backdoors.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T6.4.1.1" class="ltx_text" style="font-size:63%;">Table 6</span>. </span><span id="S6.T6.5.2" class="ltx_text" style="font-size:63%;">Link defense methods and security attacks</span></figcaption>
<div id="S6.T6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:73.1pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-480.9pt,80.8pt) scale(0.310748998853045,0.310748998853045) ;">
<table id="S6.T6.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T6.6.1.1.1" class="ltx_tr">
<th id="S6.T6.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.1.1" class="ltx_text" style="font-size:144%;">References</span></th>
<td id="S6.T6.6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.2.1" class="ltx_text" style="font-size:144%;">MPA</span></td>
<td id="S6.T6.6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.3.1" class="ltx_text" style="font-size:144%;">DPA</span></td>
<td id="S6.T6.6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.4.1" class="ltx_text" style="font-size:144%;">BA</span></td>
<td id="S6.T6.6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.5.1" class="ltx_text" style="font-size:144%;">Attacker Information</span></td>
<td id="S6.T6.6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.1.1.6.1" class="ltx_text" style="font-size:144%;">Defense Method</span></td>
</tr>
<tr id="S6.T6.6.1.2.2" class="ltx_tr">
<th id="S6.T6.6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.2.2.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Shejwalkar and Houmansadr<span id="S6.T6.6.1.2.2.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib109" title="" class="ltx_ref">2021</a><span id="S6.T6.6.1.2.2.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.2.2.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.2.2.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.2.2.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.2.2.5.1" class="ltx_text" style="font-size:144%;">Malicious clients is craft malicious gradients</span></td>
<td id="S6.T6.6.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.2.2.6.1" class="ltx_text" style="font-size:144%;">Detect outliers detection and removal-based spectral methods</span></td>
</tr>
<tr id="S6.T6.6.1.3.3" class="ltx_tr">
<th id="S6.T6.6.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.3.3.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Fang et al<span class="ltx_text">.</span><span id="S6.T6.6.1.3.3.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib40" title="" class="ltx_ref">2020</a><span id="S6.T6.6.1.3.3.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.3.3.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.3.3.3" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.3.3.4" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.3.3.5.1" class="ltx_text" style="font-size:144%;">The adversary arbitrarily manipulates the local
models</span></td>
<td id="S6.T6.6.1.3.3.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.3.3.6.1" class="ltx_text" style="font-size:144%;">Detect and remove the malicious local models</span></td>
</tr>
<tr id="S6.T6.6.1.4.4" class="ltx_tr">
<th id="S6.T6.6.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.4.4.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Zhao et al<span class="ltx_text">.</span><span id="S6.T6.6.1.4.4.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib156" title="" class="ltx_ref">2022</a><span id="S6.T6.6.1.4.4.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.4.4.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.4.4.3" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.4.4.4" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.4.4.5.1" class="ltx_text" style="font-size:144%;">The adversary crafts a poisoned model that is similar to the benign model</span></td>
<td id="S6.T6.6.1.4.4.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.4.4.6.1" class="ltx_text" style="font-size:144%;">Detect anomalous model updates and remove them</span></td>
</tr>
<tr id="S6.T6.6.1.5.5" class="ltx_tr">
<th id="S6.T6.6.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.5.5.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Sun et al<span class="ltx_text">.</span><span id="S6.T6.6.1.5.5.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib119" title="" class="ltx_ref">2021</a><span id="S6.T6.6.1.5.5.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.5.5.2.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.5.5.3" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.5.5.4" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.5.5.5.1" class="ltx_text" style="font-size:144%;">Malicious devices perform the local training in different manners</span></td>
<td id="S6.T6.6.1.5.5.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.5.5.6.1" class="ltx_text" style="font-size:144%;">Perturb the parameters of the model</span></td>
</tr>
<tr id="S6.T6.6.1.6.6" class="ltx_tr">
<th id="S6.T6.6.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.6.6.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Jagielski et al<span class="ltx_text">.</span><span id="S6.T6.6.1.6.6.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib68" title="" class="ltx_ref">2018</a><span id="S6.T6.6.1.6.6.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.6.6.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.6.6.3.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.6.6.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.6.6.5.1" class="ltx_text" style="font-size:144%;">The adversary injects poisoned data into the training set</span></td>
<td id="S6.T6.6.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.6.6.6.1" class="ltx_text" style="font-size:144%;">The TRIM defense algorithm</span></td>
</tr>
<tr id="S6.T6.6.1.7.7" class="ltx_tr">
<th id="S6.T6.6.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.7.7.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Tolpegin et al<span class="ltx_text">.</span><span id="S6.T6.6.1.7.7.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib121" title="" class="ltx_ref">2020</a><span id="S6.T6.6.1.7.7.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.7.7.2" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.7.7.3.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.7.7.4" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.7.7.5.1" class="ltx_text" style="font-size:144%;">Each malicious participant manipulates the training data</span></td>
<td id="S6.T6.6.1.7.7.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.7.7.6.1" class="ltx_text" style="font-size:144%;">Detect anomalous updates and identify malicious participants</span></td>
</tr>
<tr id="S6.T6.6.1.8.8" class="ltx_tr">
<th id="S6.T6.6.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.8.8.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Bagdasaryan et al<span class="ltx_text">.</span><span id="S6.T6.6.1.8.8.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib9" title="" class="ltx_ref">2020</a><span id="S6.T6.6.1.8.8.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.8.8.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.8.8.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.8.8.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.8.8.5.1" class="ltx_text" style="font-size:144%;">The adversary attempts to substitute the new global model with a malicious model</span></td>
<td id="S6.T6.6.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.8.8.6.1" class="ltx_text" style="font-size:144%;">Limit the updates by adding noise</span></td>
</tr>
<tr id="S6.T6.6.1.9.9" class="ltx_tr">
<th id="S6.T6.6.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.9.9.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Sun et al<span class="ltx_text">.</span><span id="S6.T6.6.1.9.9.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib120" title="" class="ltx_ref">2019</a><span id="S6.T6.6.1.9.9.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.9.9.2" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.9.9.3" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.9.9.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.9.9.5.1" class="ltx_text" style="font-size:144%;">The adversary attempts to replace the whole model with a model that has a backdoor</span></td>
<td id="S6.T6.6.1.9.9.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.9.9.6.1" class="ltx_text" style="font-size:144%;">Apply norm thresholding to the updates or differential privacy</span></td>
</tr>
<tr id="S6.T6.6.1.10.10" class="ltx_tr">
<th id="S6.T6.6.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.10.10.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Rieger et al<span class="ltx_text">.</span><span id="S6.T6.6.1.10.10.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib104" title="" class="ltx_ref">2022</a><span id="S6.T6.6.1.10.10.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.10.10.2" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.10.10.3" class="ltx_td ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.10.10.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.10.10.5.1" class="ltx_text" style="font-size:144%;">The adversary injects a backdoor into the aggregated model</span></td>
<td id="S6.T6.6.1.10.10.6" class="ltx_td ltx_align_center" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.10.10.6.1" class="ltx_text" style="font-size:144%;">Detect the deep model with a filtering framework</span></td>
</tr>
<tr id="S6.T6.6.1.11.11" class="ltx_tr">
<th id="S6.T6.6.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S6.T6.6.1.11.11.1.1.1" class="ltx_text" style="font-size:144%;">(</span>Nguyen et al<span class="ltx_text">.</span><span id="S6.T6.6.1.11.11.1.2.2.1.1" class="ltx_text" style="font-size:144%;">, </span><a href="#bib.bib96" title="" class="ltx_ref">2022b</a><span id="S6.T6.6.1.11.11.1.3.3" class="ltx_text" style="font-size:144%;">)</span></cite></th>
<td id="S6.T6.6.1.11.11.2" class="ltx_td ltx_border_b ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.11.11.3" class="ltx_td ltx_border_b ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"></td>
<td id="S6.T6.6.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.11.11.4.1" class="ltx_text" style="font-size:144%;">✔</span></td>
<td id="S6.T6.6.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.11.11.5.1" class="ltx_text" style="font-size:144%;">The adversary manipulates the global model</span></td>
<td id="S6.T6.6.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:7.2pt;padding-bottom:7.2pt;"><span id="S6.T6.6.1.11.11.6.1" class="ltx_text" style="font-size:144%;">Adding noise to the model updates. Identify and eliminate poisoned model updates. Clip the model weights before aggregation</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Anomaly detection</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">The goal with anomaly detection is to identify any aberrant model updates or outliers training data, as these are the most likely candidates to be contaminated in a poisoning attack. Once detected, the aberrant update or outlying data are removed.</p>
</div>
<div id="S6.SS2.SSS1.p2" class="ltx_para">
<p id="S6.SS2.SSS1.p2.1" class="ltx_p">Muñoz et al.  <cite class="ltx_cite ltx_citemacro_citep">(Muñoz-González et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>)</cite> devised a method of detecting anomalies, attacks, and spurious updates within collaborative models based on adaptive federated averaging. The method leverages a hidden Markov model that assesses the quality of the client update and then discards any flawed or malevolent updates in each iteration of training. By contrast, Rieger et al. <cite class="ltx_cite ltx_citemacro_citep">(Rieger et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2022</a>)</cite> introduced DeepSight, a filtering framework designed to counter backdoor attacks that filters out models with malicious intent. DeepSight combines a classifier with a clustering-based similarity estimation and probabilistic voting as its core mechanism. This approach mitigates the risk of benign client models with deviations in data distribution being erroneously filtered, which helps to maintain the integrity of the model aggregation process.</p>
</div>
<div id="S6.SS2.SSS1.p3" class="ltx_para">
<p id="S6.SS2.SSS1.p3.1" class="ltx_p">Shejwalkar and Houmansadr <cite class="ltx_cite ltx_citemacro_citep">(Shejwalkar and Houmansadr, <a href="#bib.bib109" title="" class="ltx_ref">2021</a>)</cite> introduced a defense mechanism against model poisoning known as Dive-and-Conquer (DnC). DnC first computes the principal component of the input update set, then calculates the scalar product of each submitted model update with respect to this principal component, ultimately eliminating the constant component from the maximum scalar product within the submitted model update. Thus, DnC performs a spectral analysis of the input updates through dimension reduction to ensure that malicious updates are detected.</p>
</div>
<div id="S6.SS2.SSS1.p4" class="ltx_para">
<p id="S6.SS2.SSS1.p4.1" class="ltx_p">Detecting anomalous client updates can also help to pinpoint their adverse impacts. For example, Jagielski et al. <cite class="ltx_cite ltx_citemacro_citep">(Jagielski et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2018</a>)</cite> proposed the TRIM defense algorithm, which is designed to combat data poisoning attacks. TRIM endeavors to identify the model parameters and a subset of the training set (of a predefined size) that will work to minimize the loss function. To this end, it iteratively estimates the regression
parameters while employing a constructed loss function to eliminate data points with substantial residuals. Fang et al. <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite> devised two Byzantine-robust federated learning defense strategies against poisoning attacks. The first strategy is to evaluate the effect that the local models will have on the error rate of the validation dataset and then discard any local models that have a significant adverse impact. The second shifts the local model based on its impact on loss rather than the validation set error rate. These solutions appear to work well against specific adversary models as they follow assumptions about the adversary’s attack strategy and the underlying distributions of the dataset, whether benign or adversarial. However, if these assumptions do not hold, these defense strategies may prove ineffective.</p>
</div>
<div id="S6.SS2.SSS1.p5" class="ltx_para">
<p id="S6.SS2.SSS1.p5.1" class="ltx_p">In addition to purging local model updates from clients, Cao et al. <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020a</a>)</cite> introduced the FLTrust framework, in which the service provider assigns trust scores to each local model update during each iteration. If the direction of local model updates significantly deviates from the update server model, the trust scores for these updates increase. Subsequently, the size of the local model updates is normalized to position them within the same hypersphere as the server model updates in the vector space. This limits the influence of malicious local model updates.</p>
</div>
<div id="S6.SS2.SSS1.p6" class="ltx_para">
<p id="S6.SS2.SSS1.p6.1" class="ltx_p">Zhao et al.  <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite> proposed an alternative approach
to defending against poisoning attacks. Their solution is a new Byzantine-robust federated learning framework that inverts the local model updates. Before sending the global model update in each round, the server performs a model inversion on each client’s local update and synthesizes a corresponding virtual dataset. The Wasserstein distance <cite class="ltx_cite ltx_citemacro_citep">(Deshpande et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> between the generated virtual cube and the others is then computed, and any local update with a very large distance is not aggregated with the other local models.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Differential privacy</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">In the realm of safeguarding against model poisoning attacks, perturbations serve as a crucial defense mechanism. These perturbations can be introduced during local training or applied to the aggregated global model to shield clients from such attacks. The core objective of perturbation is to mitigate the impact of these malevolent attacks by introducing controlled noise, often through constraints on the updated <math id="S6.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S6.SS2.SSS2.p1.1.m1.1a"><msub id="S6.SS2.SSS2.p1.1.m1.1.1" xref="S6.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S6.SS2.SSS2.p1.1.m1.1.1.2" xref="S6.SS2.SSS2.p1.1.m1.1.1.2.cmml">l</mi><mn id="S6.SS2.SSS2.p1.1.m1.1.1.3" xref="S6.SS2.SSS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.1.m1.1b"><apply id="S6.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S6.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.1.m1.1c">l_{2}</annotation></semantics></math>-norm <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S6.SS2.SSS2.p2" class="ltx_para">
<p id="S6.SS2.SSS2.p2.2" class="ltx_p">In pursuit of a training model with differential privacy to thwart model poisoning attacks <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2019</a>)</cite>, Sun et al.  <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2021</a>)</cite> proposed an innovative methodology. In this approach, the server identifies updates exceeding a predefined threshold of potential toxicity <math id="S6.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S6.SS2.SSS2.p2.1.m1.1a"><mi id="S6.SS2.SSS2.p2.1.m1.1.1" xref="S6.SS2.SSS2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p2.1.m1.1b"><ci id="S6.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p2.1.m1.1c">M</annotation></semantics></math>, the assumption being that adversaries will be aware of the threshold <math id="S6.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S6.SS2.SSS2.p2.2.m2.1a"><mi id="S6.SS2.SSS2.p2.2.m2.1.1" xref="S6.SS2.SSS2.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p2.2.m2.1b"><ci id="S6.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S6.SS2.SSS2.p2.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p2.2.m2.1c">M</annotation></semantics></math>. This strategy effectively aligns the boundary defense norm with local clipping during updates. Following this step, differential privacy measures are applied to the aggregated global model, strengthening resilience against model poisoning attacks. Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2021</a>)</cite> further introduced the concept of Federated Learning-White Blood Cell (FL-WBC), a client-centric approach to combatting model poisoning attacks originating from a tainted global model. The method incorporates a quantitative parameter estimation technique to identify a parameter space with enduring effects on the local training parametersn that subsequently introduces perturbations to mitigate these effects.</p>
</div>
<div id="S6.SS2.SSS2.p3" class="ltx_para">
<p id="S6.SS2.SSS2.p3.1" class="ltx_p">Nguyen et al. <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2022b</a>)</cite> devised the Flexible Learning Against Model Poisoning Attacks framework (FLAME), specifically designed to counter backdoor attacks. FLAME injects controlled noise into a model to neutralize any backdoors while employing model clustering and weight clipping techniques to minimize the noise necessary, all the while preserving benign performance. FLAME excels in negating the impact of backdoors. It accomplishes this by identifying and eliminating potentially toxic model updates through automated model clustering and implementing model weight pruning before aggregation to curb the influence of malicious model updates on the aggregation results. However, it is important to note that this defense mechanism has limitations, particularly in cases of poisoned models with high-impact attacks. When training samples exhibiting backdoor behavior are introduced into the original benign training data, the poisoned model will often yield higher accuracy on the backdoor task, rendering this defense less effective.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Security applications</h3>

<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Anomaly detection in IoT</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.1" class="ltx_p">Federated learning encompasses a wide array of participants, including IoT devices, where only the model parameters are exchanged between the cloud server and the clients. However, this FL framework is not immune to poisoning attacks initiated by internal programs <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2020a</a>)</cite>. IoT networks, in particular, face the daunting challenge of accommodating a substantial number of potentially malicious clients, some of which may be harboring malicious users. Given that the user’s local data and training process remain concealed from the server, validating the authenticity of a user’s update can become a challenging endeavor. Additionally, IoT devices place a premium on energy efficiency, which renders the deployment of computationally intensive security firewalls impractical and, consequently, leaves them susceptible to various forms of attack.</p>
</div>
<div id="S6.SS3.SSS1.p2" class="ltx_para">
<p id="S6.SS3.SSS1.p2.1" class="ltx_p">To mitigate these security concerns, Mothukuri et al. <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021a</a>)</cite> introduced an FL anomaly detection method designed to detect attacks and proactively identify intrusions within IoT networks. This method leverages data from distributed devices to facilitate on-device training that teaches machine learning models to detect anomalies within IoT networks – all without the need to transfer network data to centralized servers. To tackle the issue of tag noise, Chatterjee and Hanawal <cite class="ltx_cite ltx_citemacro_citep">(Chatterjee and Hanawal, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> proposed a tag noise intrusion detection system tailored to the security needs of IoT networks, capable of adapting to federated settings. Another noteworthy approach is the deployment of a decentralized asynchronous federated learning framework underpinned by blockchain technology within the IoT domain <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>. This framework achieves global aggregation in federated learning through blockchain consensus mechanisms rather than relying on a central server, thus providing a secure and accurate means of detecting anomalies within IoT networks.</p>
</div>
<div id="S6.SS3.SSS1.p3" class="ltx_para">
<p id="S6.SS3.SSS1.p3.1" class="ltx_p">Both attack detection and security protection are integral components of fortifying federated learning within IoT. However, it is important to recognize that the detection mechanisms may themselves be vulnerable to various attacks, including backdoor attacks that misclassify malicious traffic as benign. Addressing this challenge is paramount for enhancing the security of federated learning in IoT systems.</p>
</div>
</section>
<section id="S6.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Visual detection protection</h4>

<div id="S6.SS3.SSS2.p1" class="ltx_para">
<p id="S6.SS3.SSS2.p1.1" class="ltx_p">In the realm of traditional security, data acquisition often relies on camera systems, with monitoring rooms serving as the central hubs for observation. Nevertheless, this conventional approach does not consistently guarantee security <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2022</a>)</cite>. It is susceptible to shortcomings – for example, where anomalies in the behavior of personnel may not be noticed in a timely manner, resulting in missed warnings and erroneous assessments.</p>
</div>
<div id="S6.SS3.SSS2.p2" class="ltx_para">
<p id="S6.SS3.SSS2.p2.1" class="ltx_p">In response to these challenges, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite> proposed an innovative solution aimed at bolstering security monitoring in smart cities. Rooted in federated learning and visual detection, this novel approach effectively harnesses data from multiple communities to construct a resilient security model. It interconnects different communities and exchanges information between them. As such, it gives rise to a myriad of potential applications, including but not limited to fire monitoring.</p>
</div>
<div id="S6.SS3.SSS2.p3" class="ltx_para">
<p id="S6.SS3.SSS2.p3.1" class="ltx_p">On the whole, this approach emerges as a promising remedy for the limitations inherent to traditional security paradigms that rely on centralized monitoring. That said, further research is needed to evaluate its effectiveness and potential limitations in a diverse range of contexts.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Challenges and Open Research Directions in Federated Learning</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This section is dedicated to providing insights into some of the fundamental challenges encountered in federated learning. While federated learning holds immense promise for safeguarding the privacy of clients’ local data, there are still a multitude of privacy, security, and fairness concerns to grapple with. These include issues such as privacy breaches, security vulnerabilities, and disparities in fairness. Resolving these challenges and exploring future trajectories in the field is therefore highly worthy of a comprehensive examination.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>The Impact of Privacy and Security on Fairness</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Bias in machine learning models is a persistent concern, capable of significantly shaping decision-making processes and potentially culminating in unfair outcomes. In the realm of federated learning, the challenge of bias takes on heightened complexity due to the inherent variability of client data. As such, it is imperative to consider fairness alongside privacy and security in the context of federated learning <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2021</a>)</cite>. Part of this requires acknowledging that safeguarding privacy and ensuring security can mean unintended bias might surface.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">Take, for example, the application of differential privacy measures, which, while fortifying privacy, may inadvertently amplify unfairness. Similarly, in the process of filtering out malicious adversaries as part of security measures, there is risk of erroneously identifying benign clients as potential attackers. Such misclassifications often arise because the nature and distribution of that client’s data departs from the majority. This is a form of discrimination. Consequently, it is paramount to explore the repercussions of privacy and security on fairness and implement measures to effectively curtail bias.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">In future research on federated learning, two promising topics for investigation include how to cultivate trust and how to tailor personalized methodologies that might ameliorate the impact of privacy and security on fairness. In-depth explorations of the intricate interplays between privacy, security, and fairness, might give rise to targeted approaches along these lines that both mitigate bias and advance the cause of equity within federated learning.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>The balances between privacy, security and fairness</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Examining the intricate equilibrium between privacy, security, and fairness within the realm of federated learning stands as a subject of paramount significance. Historically, research endeavors have predominantly gravitated toward investigating these three pillars in isolation, with some dedicating their efforts solely to the preservation of privacy and security within federated learning, while others have delved into rectifying fairness concerns among clients or cohorts. However, recent scholarship has underscored the intrinsic interconnections between privacy, security, and fairness  <cite class="ltx_cite ltx_citemacro_citep">(Cummings et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Ozdayi and Kantarcioglu, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>. Recognizing and harnessing these intricate connections could catalyze the development of methodologies that concurrently elevate fairness, safeguard privacy, and fortify security within federated learning.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">As an example, empirical evidence suggests that ensuring fairness in federated learning might be achievable if one could identify and exclude malicious contributors and adversaries. In turn, this would help to bolster privacy and security <cite class="ltx_cite ltx_citemacro_citep">(Xu and Lyu, <a href="#bib.bib141" title="" class="ltx_ref">2020</a>)</cite>. Further, exploring novel approaches to ensuring fairness, privacy, and security in concert, such as identifying and commending clients based on the fidelity of their submitted gradients, offers a multifaceted avenue for addressing concerns related to cyber safety. Consequently, researchers in the field must begin to pursue a balanced and holistic approach to safeguarding data – one that takes into account the inherent interconnectedness of privacy, security, and fairness. This is the grail that holds the most significant promise in surmounting the complexities posed by these challenges.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">By embracing the intricate tapestry of relationships and diligently striving for equilibrium among privacy, security, and fairness, researchers are poised to make substantial strides in conquering these formidable obstacles. The judicious consideration and harmonization of these dimensions herald a future characterized by comprehensive solutions that not only champion fairness but also steadfastly uphold the imperatives of privacy and security.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Prior research studies  <cite class="ltx_cite ltx_citemacro_citep">(AbdulRahman et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Blanco-Justicia et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>; Gosselin et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>; Truong et al<span class="ltx_text">.</span>, <a href="#bib.bib127" title="" class="ltx_ref">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2022b</a>; Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2022</a>)</cite> have exhaustively investigated the discrete realms of privacy, security, and fairness within the paradigm of federated learning, with numerous researchers proposing various mechanisms of defense to safeguard these modalities. However, these undertakings have predominantly fixated on redressing privacy, security, or fairness in isolation. To navigate these multifaceted challenges adeptly, it is paramount to explore the foundational interrelations that underpin these three constructs.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">In this study, we offer pioneering insights into the intricate tapestry of connections binding privacy, security, and fairness, ushering forth a fresh perspective predicated on achieving equilibrium between the three. We find that privacy and security concerns can intertwine, most notably through the conduit of gradient sharing, and that a trade-off dynamic exists between most combinations of these dimensions. These revelations substantially broaden our comprehension of the forthcoming challenges awaiting federated learning within the domains of privacy, security, and fairness, proffering innovative avenues for resolving these intricate quandaries. Significantly, our inquiry introduces the novel concept of fairness as a unifying bridge that links the realms of privacy and security, offering a transformative outlook on this dynamic.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Consequently, our study underscores the imperative of perceiving privacy, security, and fairness as interconnected facets rather than solitary predicaments when grappling with the intricacies of privacy or security within federated learning. This is a perspective that also underscores the need to holistically contemplate privacy, security, and fairness in any federated learning system. To the best of our knowledge, this is the first study to deeply explore the nexus binding privacy, security, and fairness. For this reason, we recommend that subsequent research endeavors be tailored toward applying these discoveries to real-world federated learning scenarios, particularly those that involve numerous clients. In addition to addressing the security and privacy requisites of these multiple parties, treating clients equitably is an indispensable consideration for the path forward.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research is supported by the NSFC-FDCT as part of the Joint Scientific Research Project Fund (Grant No. 0051/2022/AFJ), China &amp; Macau.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Annie Abay, Yi Zhou,
Nathalie Baracaldo, Shashank Rajamoni,
Ebube Chuba, and Heiko Ludwig.
2020.

</span>
<span class="ltx_bibblock">Mitigating bias in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.02447</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AbdulRahman et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sawsan AbdulRahman, Hanine
Tout, Hakima Ould-Slimane, Azzam Mourad,
Chamseddine Talhi, and Mohsen Guizani.
2020.

</span>
<span class="ltx_bibblock">A survey on federated learning: The journey from
centralized to distributed on-site learning and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 7 (2020),
5476–5497.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acar et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Abbas Acar, Hidayet Aksu,
A Selcuk Uluagac, and Mauro Conti.
2018.

</span>
<span class="ltx_bibblock">A survey on homomorphic encryption schemes: Theory
and implementation.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (Csur)</em>
51, 4 (2018),
1–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Act (1996)</span>
<span class="ltx_bibblock">
Accountability Act.
1996.

</span>
<span class="ltx_bibblock">Health insurance portability and accountability act
of 1996.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Public law</em> 104
(1996), 191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nitin Agrawal, Ali
Shahin Shamsabadi, Matt J Kusner, and
Adrià Gascón. 2019.

</span>
<span class="ltx_bibblock">QUOTIENT: two-party secure neural network training
and prediction. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1231–1247.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ateniese et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Giuseppe Ateniese, Luigi V
Mancini, Angelo Spognardi, Antonio
Villani, Domenico Vitali, and Giovanni
Felici. 2015.

</span>
<span class="ltx_bibblock">Hacking smart machines with smarter ones: How to
extract meaningful data from machine learning classifiers.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">International Journal of Security and
Networks</em> 10, 3 (2015),
137–150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awan et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sana Awan, Fengjun Li,
Bo Luo, and Mei Liu.
2019.

</span>
<span class="ltx_bibblock">Poster: A reliable and accountable
privacy-preserving federated learning framework using the blockchain. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security</em>. 2561–2563.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan,
Andreas Veit, Yiqing Hua,
Deborah Estrin, and Vitaly Shmatikov.
2020.

</span>
<span class="ltx_bibblock">How to backdoor federated learning. In
<em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence
and Statistics</em>. PMLR, 2938–2948.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balakrishnan et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ravikumar Balakrishnan,
Mustafa Akdeniz, Sagar Dhakal,
Arjun Anand, Ariela Zeira, and
Nageen Himayat. 2021.

</span>
<span class="ltx_bibblock">Resource management and model personalization for
federated learning over wireless edge networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Journal of Sensor and Actuator Networks</em>
10, 1 (2021),
17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beimel (2011)</span>
<span class="ltx_bibblock">
Amos Beimel.
2011.

</span>
<span class="ltx_bibblock">Secret-sharing schemes: A survey. In
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference on coding and
cryptology</em>. Springer, 11–46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Arjun Nitin Bhagoji,
Supriyo Chakraborty, Prateek Mittal,
and Seraphin Calo. 2019.

</span>
<span class="ltx_bibblock">Analyzing federated learning through an adversarial
lens. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 634–643.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhowmick et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Abhishek Bhowmick, John
Duchi, Julien Freudiger, Gaurav Kapoor,
and Ryan Rogers. 2018.

</span>
<span class="ltx_bibblock">Protection against reconstruction and its
applications in private federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00984</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi
El Mhamdi, Rachid Guerraoui, and Julien
Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine
tolerant gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanco-Justicia et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alberto Blanco-Justicia,
Josep Domingo-Ferrer, Sergio
Martínez, David Sánchez, Adrian
Flanagan, and Kuan Eeik Tan.
2021.

</span>
<span class="ltx_bibblock">Achieving security and privacy in federated
learning systems: Survey, research challenges and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Engineering Applications of Artificial
Intelligence</em> 106 (2021),
104468.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canetti et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (1996)</span>
<span class="ltx_bibblock">
Ran Canetti, Uri Feige,
Oded Goldreich, and Moni Naor.
1996.

</span>
<span class="ltx_bibblock">Adaptively secure multi-party computation. In
<em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the twenty-eighth annual ACM
symposium on Theory of computing</em>. 639–648.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Hui Cao, Shubo Liu,
Renfang Zhao, and Xingxing Xiong.
2020b.

</span>
<span class="ltx_bibblock">IFed: A novel federated learning framework for
local differential privacy in Power Internet of Things.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">International Journal of Distributed Sensor
Networks</em> 16, 5 (2020),
1550147720919698.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Xiaoyu Cao, Minghong
Fang, Jia Liu, and Neil Zhenqiang
Gong. 2020a.

</span>
<span class="ltx_bibblock">Fltrust: Byzantine-robust federated learning via
trust bootstrapping.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.13995</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaoyu Cao, Jinyuan Jia,
and Neil Zhenqiang Gong.
2021.

</span>
<span class="ltx_bibblock">Provably secure federated learning against
malicious clients. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, Vol. 35.
6885–6893.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chalapathy and Chawla (2019)</span>
<span class="ltx_bibblock">
Raghavendra Chalapathy and
Sanjay Chawla. 2019.

</span>
<span class="ltx_bibblock">Deep learning for anomaly detection: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.03407</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandola et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Varun Chandola, Arindam
Banerjee, and Vipin Kumar.
2009.

</span>
<span class="ltx_bibblock">Anomaly detection: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">ACM computing surveys (CSUR)</em>
41, 3 (2009),
1–58.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Melissa Chase, Esha
Ghosh, and Saeed Mahloujifar.
2021.

</span>
<span class="ltx_bibblock">Property Inference from Poisoning.

</span>
<span class="ltx_bibblock">Cryptology ePrint Archive, Paper 2021/099.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://eprint.iacr.org/2021/099" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eprint.iacr.org/2021/099</a>

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://eprint.iacr.org/2021/099" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eprint.iacr.org/2021/099</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chatterjee and Hanawal (2022)</span>
<span class="ltx_bibblock">
Sayan Chatterjee and
Manjesh Kumar Hanawal. 2022.

</span>
<span class="ltx_bibblock">Federated learning for intrusion detection in IoT
security: a hybrid ensemble approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Journal of Internet of Things
and Cyber-Assurance</em> 2, 1
(2022), 62–86.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Huiqiang Chen, Tianqing
Zhu, Tao Zhang, Wanlei Zhou, and
Philip S Yu. 2023.

</span>
<span class="ltx_bibblock">Privacy and Fairness in Federated Learning: on the
Perspective of Trade-off.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Comput. Surveys</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiqiang Chen, Xin Qin,
Jindong Wang, Chaohui Yu, and
Wen Gao. 2020.

</span>
<span class="ltx_bibblock">Fedhealth: A federated transfer learning framework
for wearable healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>
35, 4 (2020),
83–93.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zunming Chen, Hongyan
Cui, Ensen Wu, and Xi Yu.
2022.

</span>
<span class="ltx_bibblock">Dynamic asynchronous anti poisoning federated deep
learning with blockchain-based reputation-aware solutions.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 22,
2 (2022), 684.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kewei Cheng, Tao Fan,
Yilun Jin, Yang Liu,
Tianjian Chen, Dimitrios Papadopoulos,
and Qiang Yang. 2021.

</span>
<span class="ltx_bibblock">Secureboost: A lossless federated learning
framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>
36, 6 (2021),
87–98.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhury et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Olivia Choudhury, Aris
Gkoulalas-Divanis, Theodoros Salonidis,
Issa Sylla, Yoonyoung Park,
Grace Hsu, and Amar Das.
2019.

</span>
<span class="ltx_bibblock">Differential privacy-enabled federated learning for
sensitive health data.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.02578</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Lei Cui, Youyang Qu,
Gang Xie, Deze Zeng,
Ruidong Li, Shigen Shen, and
Shui Yu. 2021.

</span>
<span class="ltx_bibblock">Security and privacy-enhanced federated learning
for anomaly detection in iot infrastructures.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>
18, 5 (2021),
3492–3500.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cummings et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Rachel Cummings, Varun
Gupta, Dhamma Kimpara, and Jamie
Morgenstern. 2019.

</span>
<span class="ltx_bibblock">On the compatibility of privacy and fairness. In
<em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Adjunct Publication of the 27th Conference on User
Modeling, Adaptation and Personalization</em>. 309–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damgård et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Ivan Damgård, Valerio
Pastro, Nigel Smart, and Sarah
Zakarias. 2012.

</span>
<span class="ltx_bibblock">Multiparty computation from somewhat homomorphic
encryption. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Annual Cryptology Conference</em>.
Springer, 643–662.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ishan Deshpande, Yuan-Ting
Hu, Ruoyu Sun, Ayis Pyrros,
Nasir Siddiqui, Sanmi Koyejo,
Zhizhen Zhao, David Forsyth, and
Alexander G Schwing. 2019.

</span>
<span class="ltx_bibblock">Max-sliced wasserstein distance and its use for
gans. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>. 10648–10656.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Divi et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Siddharth Divi, Yi-Shan
Lin, Habiba Farrukh, and Z Berkay
Celik. 2021.

</span>
<span class="ltx_bibblock">New metrics to evaluate the performance and
fairness of personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.13173</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei Du, Depeng Xu,
Xintao Wu, and Hanghang Tong.
2021.

</span>
<span class="ltx_bibblock">Fairness-aware agnostic federated learning. In
<em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 SIAM International
Conference on Data Mining (SDM)</em>. SIAM, 181–189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jia Duan, Jiantao Zhou,
and Yuanman Li. 2020.

</span>
<span class="ltx_bibblock">Privacy-Preserving distributed deep learning based
on secret sharing.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Information Sciences</em> 527
(2020), 108–127.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2006)</span>
<span class="ltx_bibblock">
Cynthia Dwork.
2006.

</span>
<span class="ltx_bibblock">Differential Privacy. In
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Automata, Languages and Programming, 33rd
International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006,
Proceedings, Part II</em> <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">(Lecture Notes in Computer
Science, Vol. 4052)</em>,
Michele Bugliesi, Bart
Preneel, Vladimiro Sassone, and Ingo
Wegener (Eds.). Springer, 1–12.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/11787006_1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/11787006_1</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elgabli et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Anis Elgabli, Jihong
Park, Chaouki Ben Issaid, and Mehdi
Bennis. 2021.

</span>
<span class="ltx_bibblock">Harnessing wireless channels for scalable and
privacy-preserving federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Communications</em>
69, 8 (2021),
5194–5208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elkordy and Avestimehr (2022)</span>
<span class="ltx_bibblock">
Ahmed Roushdy Elkordy and
A Salman Avestimehr. 2022.

</span>
<span class="ltx_bibblock">Heterosag: Secure aggregation with heterogeneous
quantization in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Communications</em>
70, 4 (2022),
2372–2386.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span id="bib.bib40.4.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu
Cao, Jinyuan Jia, and Neil Gong.
2020.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to
<math id="bib.bib40.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib40.1.m1.1a"><mo stretchy="false" id="bib.bib40.1.m1.1.1" xref="bib.bib40.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib40.1.m1.1b"><ci id="bib.bib40.1.m1.1.1.cmml" xref="bib.bib40.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib40.1.m1.1c">\{</annotation></semantics></math>Byzantine-Robust<math id="bib.bib40.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib40.2.m2.1a"><mo stretchy="false" id="bib.bib40.2.m2.1.1" xref="bib.bib40.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib40.2.m2.1b"><ci id="bib.bib40.2.m2.1.1.cmml" xref="bib.bib40.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib40.2.m2.1c">\}</annotation></semantics></math> federated learning. In
<em id="bib.bib40.5.1" class="ltx_emph ltx_font_italic">29th USENIX Security Symposium (USENIX Security
20)</em>. 1605–1622.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Lei Feng, Yiqi Zhao,
Shaoyong Guo, Xuesong Qiu,
Wenjing Li, and Peng Yu.
2021.

</span>
<span class="ltx_bibblock">BAFL: A Blockchain-Based Asynchronous Federated
Learning Framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">IEEE Trans. Comput.</em> 71,
5 (2021), 1092–1103.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fereidooni et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hossein Fereidooni, Samuel
Marchal, Markus Miettinen, Azalia
Mirhoseini, Helen Möllering,
Thien Duc Nguyen, Phillip Rieger,
Ahmad-Reza Sadeghi, Thomas Schneider,
Hossein Yalame, et al<span id="bib.bib42.3.1" class="ltx_text">.</span>
2021.

</span>
<span class="ltx_bibblock">SAFELearn: secure aggregation for private federated
learning. In <em id="bib.bib42.4.1" class="ltx_emph ltx_font_italic">2021 IEEE Security and Privacy
Workshops (SPW)</em>. IEEE, 56–62.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraboni et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yann Fraboni, Richard
Vidal, and Marco Lorenzi.
2021.

</span>
<span class="ltx_bibblock">Free-rider attacks on model aggregation in
federated learning. In <em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">International Conference on
Artificial Intelligence and Statistics</em>. PMLR, 1846–1854.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fredrikson et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Matt Fredrikson, Somesh
Jha, and Thomas Ristenpart.
2015.

</span>
<span class="ltx_bibblock">Model inversion attacks that exploit confidence
information and basic countermeasures. In
<em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on
computer and communications security</em>. 1322–1333.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fredrikson et al<span id="bib.bib45.4.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Matthew Fredrikson, Eric
Lantz, Somesh Jha, Simon Lin,
David Page, and Thomas Ristenpart.
2014.

</span>
<span class="ltx_bibblock">Privacy in pharmacogenetics: An <math id="bib.bib45.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib45.1.m1.1a"><mo stretchy="false" id="bib.bib45.1.m1.1.1" xref="bib.bib45.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib45.1.m1.1b"><ci id="bib.bib45.1.m1.1.1.cmml" xref="bib.bib45.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib45.1.m1.1c">\{</annotation></semantics></math>End-to-End<math id="bib.bib45.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib45.2.m2.1a"><mo stretchy="false" id="bib.bib45.2.m2.1.1" xref="bib.bib45.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib45.2.m2.1b"><ci id="bib.bib45.2.m2.1.1.cmml" xref="bib.bib45.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib45.2.m2.1c">\}</annotation></semantics></math>
case study of personalized warfarin dosing. In
<em id="bib.bib45.5.1" class="ltx_emph ltx_font_italic">23rd USENIX Security Symposium (USENIX Security
14)</em>. 17–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Anmin Fu, Xianglong
Zhang, Naixue Xiong, Yansong Gao,
Huaqun Wang, and Jing Zhang.
2020.

</span>
<span class="ltx_bibblock">VFL: a verifiable federated learning with
privacy-preserving for big data in industrial IoT.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chong Fu, Xuhong Zhang,
Shouling Ji, Jinyin Chen,
Jingzheng Wu, Shanqing Guo,
Jun Zhou, Alex X Liu, and
Ting Wang. 2022.

</span>
<span class="ltx_bibblock">Label inference attacks against vertical federated
learning. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">31st USENIX Security Symposium
(USENIX Security 22), Boston, MA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Furth (2022)</span>
<span class="ltx_bibblock">
Nicholas Furth.
2022.

</span>
<span class="ltx_bibblock">Un-fair trojan: Targeted backdoor attacks against
model fairness.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gálvez et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Borja Rodríguez Gálvez,
Filip Granqvist, Rogier van Dalen, and
Matt Seigel. 2021.

</span>
<span class="ltx_bibblock">Enforcing fairness in private federated learning
via the modified method of differential multipliers. In
<em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">NeurIPS 2021 Workshop Privacy in Machine
Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut
Bauermeister, Hannah Dröge, and
Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting gradients-how easy is it to break privacy
in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
16937–16947.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo
Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client
level perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldwasser and Micali (1982)</span>
<span class="ltx_bibblock">
Shafi Goldwasser and
Silvio Micali. 1982.

</span>
<span class="ltx_bibblock">Probabilistic encryption &amp; how to play mental
poker keeping secret all partial information. In
<em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the fourteenth annual ACM symposium
on Theory of computing</em>. 365–377.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xuan Gong, Abhishek
Sharma, Srikrishna Karanam, Ziyan Wu,
Terrence Chen, David Doermann, and
Arun Innanje. 2021.

</span>
<span class="ltx_bibblock">Ensemble attention distillation for
privacy-preserving federated learning. In
<em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>. 15076–15086.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xuan Gong, Abhishek
Sharma, Srikrishna Karanam, Ziyan Wu,
Terrence Chen, David Doermann, and
Arun Innanje. 2022.

</span>
<span class="ltx_bibblock">Preserving privacy in federated learning with
ensemble cross-domain knowledge distillation. In
<em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 36. 11891–11899.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gosselin et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Rémi Gosselin,
Loïc Vieu, Faiza Loukil, and
Alexandre Benoit. 2022.

</span>
<span class="ltx_bibblock">Privacy and Security in Federated Learning: A
Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 12,
19 (2022), 9901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gottipati et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Aashish Gottipati, Alex
Stewart, Jiawen Song, and Qianlang
Chen. 2021.

</span>
<span class="ltx_bibblock">FedRAN: Federated Mobile Edge Computing with
Differential Privacy. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th
FlexNets Workshop on Flexible Networks Artificial Intelligence Supported
Network Flexibility and Agility</em>. 14–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiuting Gu, Zhu Tianqing,
Jie Li, Tao Zhang, Wei
Ren, and Kim-Kwang Raymond Choo.
2022.

</span>
<span class="ltx_bibblock">Privacy, accuracy, and model fairness trade-offs in
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em>
122 (2022), 102907.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardt et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Moritz Hardt, Eric Price,
and Nati Srebro. 2016.

</span>
<span class="ltx_bibblock">Equality of opportunity in supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 29 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko
Henecka, Hamish Ivey-Law, Richard Nock,
Giorgio Patrini, Guillaume Smith, and
Brian Thorne. 2017.

</span>
<span class="ltx_bibblock">Private federated learning on vertically
partitioned data via entity resolution and additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.10677</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol
Vinyals, Jeff Dean, et al<span id="bib.bib60.3.1" class="ltx_text">.</span>
2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em>
2, 7 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hitaj et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe
Ateniese, and Fernando Perez-Cruz.
2017.

</span>
<span class="ltx_bibblock">Deep models under the GAN: information leakage from
collaborative deep learning. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
2017 ACM SIGSAC conference on computer and communications security</em>.
603–618.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongsheng Hu, Zoran
Salcic, Lichao Sun, Gillian Dobbie,
Philip S Yu, and Xuyun Zhang.
2021.

</span>
<span class="ltx_bibblock">Membership inference attacks on machine learning: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and Vasilakos (2016)</span>
<span class="ltx_bibblock">
Jiankun Hu and
Athanasios V Vasilakos. 2016.

</span>
<span class="ltx_bibblock">Energy big data analytics and security: challenges
and opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Smart Grid</em>
7, 5 (2016),
2423–2436.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shengyuan Hu, Steven Wu,
and Virginia Smith. 2023.

</span>
<span class="ltx_bibblock">Fair Federated Learning via Bounded Group Loss.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zeou Hu, Kiarash
Shaloudegi, Guojun Zhang, and Yaoliang
Yu. 2020.

</span>
<span class="ltx_bibblock">Fedmgda+: Federated learning meets multi-objective
optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.11489</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wei Huang, Tianrui Li,
Dexian Wang, Shengdong Du, and
Junbo Zhang. 2020.

</span>
<span class="ltx_bibblock">Fairness and accuracy in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.10069</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Issa et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wael Issa, Nour Moustafa,
Benjamin Turnbull, Nasrin Sohrabi, and
Zahir Tari. 2023.

</span>
<span class="ltx_bibblock">Blockchain-based federated learning for securing
internet of things: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Comput. Surveys</em> 55,
9 (2023), 1–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jagielski et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Matthew Jagielski, Alina
Oprea, Battista Biggio, Chang Liu,
Cristina Nita-Rotaru, and Bo Li.
2018.

</span>
<span class="ltx_bibblock">Manipulating machine learning: Poisoning attacks
and countermeasures for regression learning. In
<em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">2018 IEEE Symposium on Security and Privacy (SP)</em>.
IEEE, 19–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib69.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.4.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in
Machine Learning</em> 14, 1–2
(2021), 1–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamishima et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Toshihiro Kamishima,
Shotaro Akaho, Hideki Asoh, and
Jun Sakuma. 2012.

</span>
<span class="ltx_bibblock">Fairness-aware classifier with prejudice remover
regularizer. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Joint European conference on
machine learning and knowledge discovery in databases</em>. Springer,
35–50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Lim (2022)</span>
<span class="ltx_bibblock">
Woocheol Kim and Hyuk
Lim. 2022.

</span>
<span class="ltx_bibblock">FedCC: Federated Learning with Consensus
Confirmation for Byzantine Attack Resistance (Student Abstract).

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Maximilian Lam, Gu-Yeon
Wei, David Brooks, Vijay Janapa Reddi,
and Michael Mitzenmacher.
2021.

</span>
<span class="ltx_bibblock">Gradient disaggregation: Breaking privacy in
federated learning by reconstructing the user participant matrix. In
<em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 5959–5968.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Oscar Li, Jiankai Sun,
Xin Yang, Weihao Gao,
Hongyi Zhang, Junyuan Xie,
Virginia Smith, and Chong Wang.
2021b.

</span>
<span class="ltx_bibblock">Label Leakage and Protection in Two-party Split
Learning. In <em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu,
Ahmad Beirami, and Virginia Smith.
2021a.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through
personalization. In <em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 6357–6368.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith.
2020.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine learning and systems</em>
2 (2020), 429–450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi,
Ahmad Beirami, and Virginia Smith.
2019b.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10497</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib77.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Wenqi Li, Fausto
Milletarì, Daguang Xu, Nicola
Rieke, Jonny Hancox, Wentao Zhu,
Maximilian Baust, Yan Cheng,
Sébastien Ourselin, M Jorge Cardoso,
et al<span id="bib.bib77.3.1" class="ltx_text">.</span> 2019a.

</span>
<span class="ltx_bibblock">Privacy-preserving federated brain tumour
segmentation. In <em id="bib.bib77.4.1" class="ltx_emph ltx_font_italic">Machine Learning in Medical
Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with
MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10</em>. Springer,
133–141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhuohang Li, Jiaxin
Zhang, Luyang Liu, and Jian Liu.
2022.

</span>
<span class="ltx_bibblock">Auditing Privacy Defenses in Federated Learning via
Generative Gradient Leakage. In <em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
10132–10142.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim,
Nguyen Cong Luong, Dinh Thai Hoang,
Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and
Chunyan Miao. 2020.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A
comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>
22, 3 (2020),
2031–2063.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaoyuan Liu, Hongwei Li,
Guowen Xu, Zongqi Chen,
Xiaoming Huang, and Rongxing Lu.
2021.

</span>
<span class="ltx_bibblock">Privacy-enhanced federated learning against
poisoning adversaries.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 16 (2021),
4574–4588.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yang Liu, Anbu Huang,
Yun Luo, He Huang,
Youzhi Liu, Yuanyuan Chen,
Lican Feng, Tianjian Chen,
Han Yu, and Qiang Yang.
2020.

</span>
<span class="ltx_bibblock">Fedvision: An online visual object detection
platform powered by federated learning. In
<em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 34. 13172–13179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Xinyi Xu,
Qian Wang, and Han Yu.
2020a.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Federated Learning: Privacy and Incentive</em>
(2020), 189–204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Jiangshan
Yu, Karthik Nandakumar, Yitong Li,
Xingjun Ma, Jiong Jin,
Han Yu, and Kee Siong Ng.
2020b.

</span>
<span class="ltx_bibblock">Towards fair and privacy-preserving federated deep
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed
Systems</em> 31, 11 (2020),
2524–2541.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017a.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">Artificial
intelligence and statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider
Moore, Daniel Ramage, and
Blaise Agüera y Arcas.
2016.

</span>
<span class="ltx_bibblock">Federated learning of deep networks using model
averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>
2 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel
Ramage, Kunal Talwar, and Li Zhang.
2017b.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng
Song, Emiliano De Cristofaro, and
Vitaly Shmatikov. 2019.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in
collaborative learning. In <em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">2019 IEEE symposium on
security and privacy (SP)</em>. IEEE, 691–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michieli and Ozay (2021)</span>
<span class="ltx_bibblock">
Umberto Michieli and
Mete Ozay. 2021.

</span>
<span class="ltx_bibblock">Are all users treated fairly in federated learning
systems?. In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
2318–2322.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary
Sivek, and Ananda Theertha Suresh.
2019.

</span>
<span class="ltx_bibblock">Agnostic federated learning. In
<em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 4615–4625.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mothukuri et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Prachi
Khare, Reza M Parizi, Seyedamin
Pouriyeh, Ali Dehghantanha, and Gautam
Srivastava. 2021a.

</span>
<span class="ltx_bibblock">Federated-Learning-Based Anomaly Detection for IoT
Security Attacks.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
9, 4 (2021),
2545–2554.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mothukuri et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M
Parizi, Seyedamin Pouriyeh, Yan Huang,
Ali Dehghantanha, and Gautam
Srivastava. 2021b.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>
115 (2021), 619–640.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mozannar et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hussein Mozannar, Mesrob
Ohannessian, and Nathan Srebro.
2020.

</span>
<span class="ltx_bibblock">Fair learning with private demographic data. In
<em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 7066–7075.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muñoz-González et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luis Muñoz-González,
Kenneth T Co, and Emil C Lupu.
2019.

</span>
<span class="ltx_bibblock">Byzantine-robust federated machine learning through
adaptive model averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05125</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasr et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri,
and Amir Houmansadr. 2019.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning:
Passive and active white-box inference attacks against centralized and
federated learning. In <em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">2019 IEEE symposium on
security and privacy (SP)</em>. IEEE, 739–753.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Dinh C Nguyen, Quoc-Viet
Pham, Pubudu N Pathirana, Ming Ding,
Aruna Seneviratne, Zihuai Lin,
Octavia Dobre, and Won-Joo Hwang.
2022a.

</span>
<span class="ltx_bibblock">Federated learning for smart healthcare: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>
55, 3 (2022),
1–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib96.4.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Thien Duc Nguyen, Phillip
Rieger, Huili Chen, Hossein Yalame,
Helen Möllering, Hossein Fereidooni,
Samuel Marchal, Markus Miettinen,
Azalia Mirhoseini, Shaza Zeitouni,
et al<span id="bib.bib96.5.1" class="ltx_text">.</span> 2022b.

</span>
<span class="ltx_bibblock"><math id="bib.bib96.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib96.1.m1.1a"><mo stretchy="false" id="bib.bib96.1.m1.1.1" xref="bib.bib96.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib96.1.m1.1b"><ci id="bib.bib96.1.m1.1.1.cmml" xref="bib.bib96.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib96.1.m1.1c">\{</annotation></semantics></math>FLAME<math id="bib.bib96.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib96.2.m2.1a"><mo stretchy="false" id="bib.bib96.2.m2.1.1" xref="bib.bib96.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib96.2.m2.1b"><ci id="bib.bib96.2.m2.1.1.cmml" xref="bib.bib96.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib96.2.m2.1c">\}</annotation></semantics></math>: Taming Backdoors in Federated
Learning. In <em id="bib.bib96.6.1" class="ltx_emph ltx_font_italic">31st USENIX Security Symposium
(USENIX Security 22)</em>. 1415–1432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Thien Duc Nguyen, Phillip
Rieger, Markus Miettinen, and
Ahmad-Reza Sadeghi. 2020.

</span>
<span class="ltx_bibblock">Poisoning attacks on federated learning-based IoT
intrusion detection system. In <em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">Proc. Workshop
Decentralized IoT Syst. Secur.(DISS)</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozdayi and Kantarcioglu (2021)</span>
<span class="ltx_bibblock">
Mustafa Safa Ozdayi and
Murat Kantarcioglu. 2021.

</span>
<span class="ltx_bibblock">The Impact of Data Distribution on Fairness and
Robustness in Federated Learning. In <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">2021 Third
IEEE International Conference on Trust, Privacy and Security in Intelligent
Systems and Applications (TPS-ISA)</em>. IEEE, 191–196.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Padala et al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Manisha Padala, Sankarshan
Damle, and Sujit Gujar.
2021.

</span>
<span class="ltx_bibblock">Federated Learning Meets Fairness and Differential
Privacy. In <em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">International Conference on Neural
Information Processing</em>. Springer, 692–699.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paillier (1999)</span>
<span class="ltx_bibblock">
Pascal Paillier.
1999.

</span>
<span class="ltx_bibblock">Public-key cryptosystems based on composite degree
residuosity classes. In <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">International conference
on the theory and applications of cryptographic techniques</em>. Springer,
223–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pentyala et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sikha Pentyala, Nicola
Neophytou, Anderson Nascimento, Martine
De Cock, and Golnoosh Farnadi.
2022.

</span>
<span class="ltx_bibblock">PrivFairFL: Privacy-Preserving Group Fairness in
Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11584</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhenquan Qin, Jin Ye,
Jie Meng, Bingxian Lu, and
Lei Wang. 2021.

</span>
<span class="ltx_bibblock">Privacy-Preserving Blockchain-Based Federated
Learning for Marine Internet of Things.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Computational Social
Systems</em> 9, 1 (2021),
159–173.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahman et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
KM Jawadur Rahman, Faisal
Ahmed, Nazma Akhter, Mohammad Hasan,
Ruhul Amin, Kazi Ehsan Aziz,
AKM Muzahidul Islam, Md Saddam Hossain
Mukta, and AKM Najmul Islam.
2021.

</span>
<span class="ltx_bibblock">Challenges, applications and design aspects of
federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 9
(2021), 124682–124700.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieger et al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Phillip Rieger, Thien Duc
Nguyen, Markus Miettinen, and
Ahmad-Reza Sadeghi. 2022.

</span>
<span class="ltx_bibblock">Deepsight: Mitigating backdoor attacks in federated
learning through deep model inspection.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.00763</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rückel et al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Timon Rückel, Johannes
Sedlmeir, and Peter Hofmann.
2022.

</span>
<span class="ltx_bibblock">Fairness, integrity, and privacy in a scalable
blockchain-based federated learning system.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.3.1" class="ltx_emph ltx_font_italic">Computer Networks</em> 202
(2022), 108621.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shamir (1979)</span>
<span class="ltx_bibblock">
Adi Shamir.
1979.

</span>
<span class="ltx_bibblock">How to share a secret.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 22,
11 (1979), 612–613.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Shreya Sharma, Chaoping
Xing, Yang Liu, and Yan Kang.
2019.

</span>
<span class="ltx_bibblock">Secure and efficient federated transfer learning.
In <em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data
(Big Data)</em>. IEEE, 2569–2576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shayan et al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Muhammad Shayan, Clement
Fung, Chris JM Yoon, and Ivan
Beschastnikh. 2020.

</span>
<span class="ltx_bibblock">Biscotti: A blockchain system for private and
secure federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed
Systems</em> 32, 7 (2020),
1513–1525.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shejwalkar and Houmansadr (2021)</span>
<span class="ltx_bibblock">
Virat Shejwalkar and
Amir Houmansadr. 2021.

</span>
<span class="ltx_bibblock">Manipulating the byzantine: Optimizing model
poisoning attacks and defenses for federated learning. In
<em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">NDSS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shejwalkar et al<span id="bib.bib110.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Virat Shejwalkar, Amir
Houmansadr, Peter Kairouz, and Daniel
Ramage. 2022.

</span>
<span class="ltx_bibblock">Back to the drawing board: A critical evaluation of
poisoning attacks on production federated learning. In
<em id="bib.bib110.3.1" class="ltx_emph ltx_font_italic">2022 IEEE Symposium on Security and Privacy (SP)</em>.
IEEE, 1354–1371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sheng Shen, Tianqing Zhu,
Di Wu, Wei Wang, and
Wanlei Zhou. 2022.

</span>
<span class="ltx_bibblock">From distributed machine learning to federated
learning: In the view of data privacy and security.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.3.1" class="ltx_emph ltx_font_italic">Concurrency and Computation: Practice and
Experience</em> 34, 16
(2022), e6002.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span id="bib.bib112.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuxin Shi, Han Yu, and
Cyril Leung. 2023.

</span>
<span class="ltx_bibblock">Towards fairness-aware federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and
Learning Systems</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri et al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Reza Shokri, Marco
Stronati, Congzheng Song, and Vitaly
Shmatikov. 2017.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine
learning models. In <em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">2017 IEEE symposium on
security and privacy (SP)</em>. IEEE, 3–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span id="bib.bib114.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ashneet Khandpur Singh,
Alberto Blanco-Justicia, and Josep
Domingo-Ferrer. 2023.

</span>
<span class="ltx_bibblock">Fair detection of poisoning attacks in federated
learning on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.3.1" class="ltx_emph ltx_font_italic">Data Mining and Knowledge Discovery</em>
(2023), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ashneet Khandpur Singh,
Alberto Blanco-Justicia, Josep
Domingo-Ferrer, David Sánchez, and
David Rebollo-Monedero. 2020.

</span>
<span class="ltx_bibblock">Fair detection of poisoning attacks in federated
learning. In <em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">2020 IEEE 32nd International
Conference on Tools with Artificial Intelligence (ICTAI)</em>. IEEE,
224–229.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jinhyun So, Başak
Güler, and A Salman Avestimehr.
2020.

</span>
<span class="ltx_bibblock">Byzantine-resilient secure federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em> 39, 7
(2020), 2168–2181.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span id="bib.bib117.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhendong Song, Hongguang
Sun, Howard H Yang, Xijun Wang,
Yan Zhang, and Tony QS Quek.
2021.

</span>
<span class="ltx_bibblock">Reputation-based federated learning for secure
wireless networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
9, 2 (2021),
1212–1226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui et al<span id="bib.bib118.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Dianbo Sui, Yubo Chen,
Jun Zhao, Yantao Jia,
Yuantao Xie, and Weijian Sun.
2020.

</span>
<span class="ltx_bibblock">Feded: Federated learning via ensemble distillation
for medical relation extraction. In <em id="bib.bib118.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2020 conference on empirical methods in natural language processing
(EMNLP)</em>. 2118–2128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib119.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jingwei Sun, Ang Li,
Louis DiValentin, Amin Hassanzadeh,
Yiran Chen, and Hai Li.
2021.

</span>
<span class="ltx_bibblock">Fl-wbc: Enhancing robustness against model
poisoning attacks in federated learning from a client perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
12613–12624.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib120.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ziteng Sun, Peter
Kairouz, Ananda Theertha Suresh, and
H Brendan McMahan. 2019.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib120.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tolpegin et al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey
Truex, Mehmet Emre Gursoy, and Ling
Liu. 2020.

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning
systems. In <em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">European Symposium on Research in
Computer Security</em>. Springer, 480–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tramèr et al<span id="bib.bib122.4.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Florian Tramèr, Fan
Zhang, Ari Juels, Michael K Reiter,
and Thomas Ristenpart. 2016.

</span>
<span class="ltx_bibblock">Stealing machine learning models via prediction
<math id="bib.bib122.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib122.1.m1.1a"><mo stretchy="false" id="bib.bib122.1.m1.1.1" xref="bib.bib122.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib122.1.m1.1b"><ci id="bib.bib122.1.m1.1.1.cmml" xref="bib.bib122.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib122.1.m1.1c">\{</annotation></semantics></math>APIs<math id="bib.bib122.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib122.2.m2.1a"><mo stretchy="false" id="bib.bib122.2.m2.1.1" xref="bib.bib122.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib122.2.m2.1b"><ci id="bib.bib122.2.m2.1.1.cmml" xref="bib.bib122.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib122.2.m2.1c">\}</annotation></semantics></math>. In <em id="bib.bib122.5.1" class="ltx_emph ltx_font_italic">25th USENIX security symposium
(USENIX Security 16)</em>. 601–618.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Triastcyn and Faltings (2019)</span>
<span class="ltx_bibblock">
Aleksei Triastcyn and
Boi Faltings. 2019.

</span>
<span class="ltx_bibblock">Federated learning with bayesian differential
privacy. In <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Big Data (Big Data)</em>. IEEE, 2587–2596.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie
Baracaldo, Ali Anwar, Thomas Steinke,
Heiko Ludwig, Rui Zhang, and
Yi Zhou. 2019a.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated
learning. In <em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM workshop
on artificial intelligence and security</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib125.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Stacey Truex, Ling Liu,
Ka-Ho Chow, Mehmet Emre Gursoy, and
Wenqi Wei. 2020.

</span>
<span class="ltx_bibblock">LDP-Fed: Federated learning with local differential
privacy. In <em id="bib.bib125.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Third ACM
International Workshop on Edge Systems, Analytics and Networking</em>.
61–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib126.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Stacey Truex, Ling Liu,
Mehmet Emre Gursoy, Lei Yu, and
Wenqi Wei. 2019b.

</span>
<span class="ltx_bibblock">Demystifying membership inference attacks in
machine learning as a service.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Services Computing</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truong et al<span id="bib.bib127.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nguyen Truong, Kai Sun,
Siyao Wang, Florian Guitton, and
YiKe Guo. 2021.

</span>
<span class="ltx_bibblock">Privacy preservation in federated learning: An
insightful survey from the GDPR perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em>
110 (2021), 102402.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Daoshun Wang, Lei Zhang,
Ning Ma, and Xiaobo Li.
2007.

</span>
<span class="ltx_bibblock">Two secret sharing schemes based on Boolean
operations.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em> 40,
10 (2007), 2776–2785.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib129.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Guan Wang,
Charlie Xiaoqian Dang, and Ziye Zhou.
2019a.

</span>
<span class="ltx_bibblock">Measure contribution of participants in federated
learning. In <em id="bib.bib129.3.1" class="ltx_emph ltx_font_italic">2019 IEEE international conference on
big data (Big Data)</em>. IEEE, 2597–2604.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib130.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ganghua Wang, Ali Payani,
Myungjin Lee, and Ramana Kompella.
2023.

</span>
<span class="ltx_bibblock">Mitigating Group Bias in Federated Learning: Beyond
Local Fairness.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09931</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik
Sreenivasan, Shashank Rajput, Harit
Vishwakarma, Saurabh Agarwal, Jy-yong
Sohn, Kangwook Lee, and Dimitris
Papailiopoulos. 2020a.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
16070–16084.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib132.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Yansheng Wang, Yongxin
Tong, and Dingyuan Shi.
2020b.

</span>
<span class="ltx_bibblock">Federated latent dirichlet allocation: A local
differential privacy based framework. In
<em id="bib.bib132.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 34. 6283–6290.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zheng Wang, Xiaoliang
Fan, Jianzhong Qi, Chenglu Wen,
Cheng Wang, and Rongshan Yu.
2021.

</span>
<span class="ltx_bibblock">Federated learning with fair averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14937</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song,
Zhifei Zhang, Yang Song,
Qian Wang, and Hairong Qi.
2019b.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level
privacy leakage from federated learning. In <em id="bib.bib134.3.1" class="ltx_emph ltx_font_italic">IEEE
INFOCOM 2019-IEEE Conference on Computer Communications</em>. IEEE,
2512–2520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li,
Ming Ding, Chuan Ma,
Howard H Yang, Farhad Farokhi,
Shi Jin, Tony QS Quek, and
H Vincent Poor. 2020.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy:
Algorithms and performance analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 15 (2020),
3454–3469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al<span id="bib.bib136.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiasi Weng, Jian Weng,
Jilian Zhang, Ming Li,
Yue Zhang, and Weiqi Luo.
2019.

</span>
<span class="ltx_bibblock">Deepchain: Auditable and privacy-preserving deep
learning with blockchain-based incentive.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Dependable and Secure
Computing</em> 18, 5 (2019),
2438–2455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib137.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Maoqiang Wu, Dongdong Ye,
Jiahao Ding, Yuanxiong Guo,
Rong Yu, and Miao Pan.
2021.

</span>
<span class="ltx_bibblock">Incentivizing differentially private federated
learning: A multidimensional contract approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 13 (2021),
10639–10651.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span id="bib.bib138.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Chulin Xie, Keli Huang,
Pin-Yu Chen, and Bo Li.
2019.

</span>
<span class="ltx_bibblock">Dba: Distributed backdoor attacks against federated
learning. In <em id="bib.bib138.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib139.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Guowen Xu, Hongwei Li,
Sen Liu, Kan Yang, and
Xiaodong Lin. 2019.

</span>
<span class="ltx_bibblock">Verifynet: Secure and verifiable federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 15 (2019),
911–926.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib140.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Guowen Xu, Hongwei Li,
Yun Zhang, Shengmin Xu,
Jianting Ning, and Robert Deng.
2020.

</span>
<span class="ltx_bibblock">Privacy-preserving federated deep learning with
irregular users.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Dependable and Secure
Computing</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Lyu (2020)</span>
<span class="ltx_bibblock">
Xinyi Xu and Lingjuan
Lyu. 2020.

</span>
<span class="ltx_bibblock">A reputation mechanism is all you need:
Collaborative fairness and adversarial robustness in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.10464</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib142.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em> 10, 2
(2019), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib143.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Hongxu Yin, Arun Mallya,
Arash Vahdat, Jose M Alvarez,
Jan Kautz, and Pavlo Molchanov.
2021a.

</span>
<span class="ltx_bibblock">See through gradients: Image batch recovery via
gradinversion. In <em id="bib.bib143.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
16337–16346.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib144.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Xuefei Yin, Yanming Zhu,
and Jiankun Hu. 2021b.

</span>
<span class="ltx_bibblock">A comprehensive survey of privacy-preserving
federated learning: A taxonomy, review, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>
54, 6 (2021),
1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib145.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Han Yu, Zelei Liu,
Yang Liu, Tianjian Chen,
Mingshu Cong, Xi Weng,
Dusit Niyato, and Qiang Yang.
2020.

</span>
<span class="ltx_bibblock">A fairness-aware incentive scheme for federated
learning. In <em id="bib.bib145.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society</em>. 393–399.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zawad et al<span id="bib.bib146.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Syed Zawad, Ahsan Ali,
Pin-Yu Chen, Ali Anwar,
Yi Zhou, Nathalie Baracaldo,
Yuan Tian, and Feng Yan.
2021.

</span>
<span class="ltx_bibblock">Curse or redemption? how data heterogeneity affects
the robustness of federated learning. In
<em id="bib.bib146.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 35. 10807–10814.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib147.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Daniel Yue Zhang, Ziyi
Kou, and Dong Wang. 2020c.

</span>
<span class="ltx_bibblock">Fairfl: A fair federated learning approach to
reducing demographic bias in privacy-sensitive classification models. In
<em id="bib.bib147.3.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Big Data (Big
Data)</em>. IEEE, 1051–1060.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib148.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Jiale Zhang, Bing Chen,
Xiang Cheng, Huynh Thi Thanh Binh, and
Shui Yu. 2020a.

</span>
<span class="ltx_bibblock">Poisongan: Generative poisoning attacks against
federated learning in edge computing systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib148.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 5 (2020),
3310–3322.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib149.2.2.1" class="ltx_text">.</span> (2020d)</span>
<span class="ltx_bibblock">
Jiale Zhang, Yanchao
Zhao, Junyu Wang, and Bing Chen.
2020d.

</span>
<span class="ltx_bibblock">FedMEC: improving efficiency of differentially
private federated learning via mobile edge computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.3.1" class="ltx_emph ltx_font_italic">Mobile Networks and Applications</em>
25, 6 (2020),
2421–2433.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib150.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Kaiyue Zhang, Xuan Song,
Chenhan Zhang, and Shui Yu.
2022b.

</span>
<span class="ltx_bibblock">Challenges and future directions of secure
federated learning: a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.3.1" class="ltx_emph ltx_font_italic">Frontiers of computer science</em>
16, 5 (2022),
1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib151.2.2.1" class="ltx_text">.</span> (2022c)</span>
<span class="ltx_bibblock">
Lefeng Zhang, Tianqing
Zhu, Ping Xiong, Wanlei Zhou, and
S Yu Philip. 2022c.

</span>
<span class="ltx_bibblock">A Game-theoretic Federated Learning Framework for
Data Quality Improvement.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data
Engineering</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib152.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Lefeng Zhang, Tianqing
Zhu, Haibin Zhang, Ping Xiong, and
Wanlei Zhou. 2023.

</span>
<span class="ltx_bibblock">FedRecovery: Differentially Private Machine
Unlearning for Federated Learning Frameworks.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib153.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Yuheng Zhang, Ruoxi Jia,
Hengzhi Pei, Wenxiao Wang,
Bo Li, and Dawn Song.
2020b.

</span>
<span class="ltx_bibblock">The secret revealer: Generative model-inversion
attacks against deep neural networks. In
<em id="bib.bib153.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>. 253–261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib154.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Zhengming Zhang, Ashwinee
Panda, Linyue Song, Yaoqing Yang,
Michael Mahoney, Prateek Mittal,
Ramchandran Kannan, and Joseph
Gonzalez. 2022a.

</span>
<span class="ltx_bibblock">Neurotoxin: durable backdoors in federated
learning. In <em id="bib.bib154.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 26429–26446.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib155.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy
Mopuri, and Hakan Bilen.
2020a.

</span>
<span class="ltx_bibblock">idlg: Improved deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.02610</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib156.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Bo Zhao, Peng Sun,
Tao Wang, and Keyu Jiang.
2022.

</span>
<span class="ltx_bibblock">FedInv: Byzantine-robust Federated Learning by
Inversing Local Model Updates.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib157.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao,
Linshan Jiang, Rui Tan,
Dusit Niyato, Zengxiang Li,
Lingjuan Lyu, and Yingbo Liu.
2020b.

</span>
<span class="ltx_bibblock">Privacy-preserving blockchain-based federated
learning for IoT devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib157.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 3 (2020),
1817–1829.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib158.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao,
Mengmeng Yang, Teng Wang,
Ning Wang, Lingjuan Lyu,
Dusit Niyato, and Kwok-Yan Lam.
2020c.

</span>
<span class="ltx_bibblock">Local differential privacy-based federated learning
for internet of things.

</span>
<span class="ltx_bibblock"><em id="bib.bib158.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 11 (2020),
8836–8853.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span id="bib.bib159.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhaohua Zheng, Yize Zhou,
Yilong Sun, Zhang Wang,
Boyi Liu, and Keqiu Li.
2022.

</span>
<span class="ltx_bibblock">Applications of federated learning in smart cities:
recent advances, taxonomy, and open challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib159.3.1" class="ltx_emph ltx_font_italic">Connection Science</em> 34,
1 (2022), 1–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib160.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Pengyuan Zhou, Pei Fang,
and Pan Hui. 2021.

</span>
<span class="ltx_bibblock">Loss tolerant federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib160.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.03591</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib161.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Shuai Zhou, Chi Liu,
Dayong Ye, Tianqing Zhu,
Wanlei Zhou, and Philip S Yu.
2022.

</span>
<span class="ltx_bibblock">Adversarial attacks and defenses in deep learning:
From a perspective of cybersecurity.

</span>
<span class="ltx_bibblock"><em id="bib.bib161.3.1" class="ltx_emph ltx_font_italic">Comput. Surveys</em> 55,
8 (2022), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib162.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu,
and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib162.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib163.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tianqing Zhu, Dayong Ye,
Wei Wang, Wanlei Zhou, and
Philip Yu. 2020.

</span>
<span class="ltx_bibblock">More than privacy: Applying differential privacy in
key areas of artificial intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib163.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data
Engineering</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.10883" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.10884" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.10884">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.10884" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.10885" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:01:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
