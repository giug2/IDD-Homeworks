<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.03114] Testing the Effect of Code Documentation on Large Language Model Code Understanding</title><meta property="og:description" content="Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Testing the Effect of Code Documentation on Large Language Model Code Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Testing the Effect of Code Documentation on Large Language Model Code Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.03114">

<!--Generated on Sun May  5 15:40:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Testing the Effect of Code Documentation on Large Language Model Code Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Macke
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Doyle 
<br class="ltx_break">The MITRE Corporation

<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{wmacke,mdoyle}@mitre.org
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">Approved for Public Release; Distribution Unlimited. Public Release Case Number 23-4132. Copyright ©2024 The MITRE Corporation. ALL RIGHTS RESERVED.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM’s ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM’s capabilities. We show that providing an LLM with "incorrect" documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM’s ability to understand code.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Testing the Effect of Code Documentation on Large Language Model Code Understanding</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<span class="ltx_thead">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">William Macke  </span>and<span id="p1.1.2.1.1.1.1.1.2" class="ltx_text ltx_font_bold"> Michael Doyle</span></span></span>
</span>
<span class="ltx_tbody">
<span id="p1.1.2.1.1.2.1" class="ltx_tr">
<span id="p1.1.2.1.1.2.1.1" class="ltx_td ltx_align_center">The MITRE Corporation<span id="p1.1.2.1.1.2.1.1.1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Approved for Public Release; Distribution Unlimited. Public Release Case Number 23-4132. Copyright ©2024 The MITRE Corporation. ALL RIGHTS RESERVED.</span></span></span></span></span>
<span id="p1.1.2.1.1.3.2" class="ltx_tr">
<span id="p1.1.2.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">{wmacke,mdoyle}@mitre.org</span></span></span>
</span>
</span>
</span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, Large Language Models (LLMs) have approached or pushed the state of the art for multiple natural language processing (NLP) tasks and benchmarks such as machine translation (MT) <cite class="ltx_cite ltx_citemacro_cite">Moslem et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>); Jiao et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, human evaluation of MT <cite class="ltx_cite ltx_citemacro_cite">Kocmi and Federmann (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>, sentence completion, and question answering <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. The same can be said for some programming language processing (PLP) tasks, such as code generation <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> and code translation <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>, but other tasks, such as code summarization, have proven to be difficult for LLMs <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. These tasks requiring code understanding pose challenges distinct from those faced in natural language. As an example, a unique challenge in PLP is the rigidity of syntax and semantic precision required in generation or translation that is not required to the same degree in NLP. Solving these problems, or at least improving their solutions, has the potential to greatly increase productivity and satisfaction in software development, as has already been shown with the use of GitHub Copilot <cite class="ltx_cite ltx_citemacro_cite">Kalliamvakou (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While several works have shown the importance of applying effective prompting strategies for PLP with LLMs <cite class="ltx_cite ltx_citemacro_cite">Le et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>); Shinn et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, very little work has investigated how the reliability of the documentation at input can affect an LLM’s performance. In this work, we provide a preliminary empirical analysis of how documentation quality can influence this ability. We hypothesize that correct code documentation will improve an LLM’s code understanding, and that its understanding will decrease as the prevalence and accuracy of the documentation is decreased. To the authors’ knowledge this is the first work to consider code documentation reliability for this problem.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The rest of the paper is organized as follows: First we discuss other works and how they relate to our study. Then we present experimental analysis and results. Our experiments provide a basic analysis of how an LLM’s code understanding performance degrades along with documentation quality and quantity, and we end with a discussion and analysis of the results.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Code analysis is a very well-studied problem. Traditional compilers and syntax-tree parsers are commonly used to perform code analysis <cite class="ltx_cite ltx_citemacro_cite">Moor et al. (<a href="#bib.bib16" title="" class="ltx_ref">2007</a>); Lenarduzzi et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> or extract code metrics <cite class="ltx_cite ltx_citemacro_cite">McCabe (<a href="#bib.bib15" title="" class="ltx_ref">1976</a>); Timóteo et al. (<a href="#bib.bib27" title="" class="ltx_ref">2008</a>); <a href="#bib.bib1" title="" class="ltx_ref">Agnihotri and Chug </a></cite>. However, these tools lack an LLM’s ability to process natural language, and therefore are incapable of extracting semantic understanding or considering code documentation. Since LLMs have become widely available, many works have also attempted to leverage language models to analyze code. Several works have sought to create LLMs designed for code understanding tasks such as code generation, code completion, program repair, and code translation <cite class="ltx_cite ltx_citemacro_cite">Xia et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>); Bui et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. Unlike these works, instead of developing new tools we provide a rigorous analysis of where and how existing tools can best be leveraged. Our work also introduces other means of determining code understanding than the benchmarks developed in <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Other works provided empirical analyses of existing models’ abilities to analyze code. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> present an analysis of the performance of various LLMs on different code benchmarks, and introduce a new LLM. <cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> present an empirical analysis of using GPT to generate syntax trees, call graph and other syntactic and semantic representations of code. They conclude that GPT has approximately the same abilities as an abstract syntax tree (AST) parser. Similarly <cite class="ltx_cite ltx_citemacro_citet">Palacio et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> introduce a framework called ASTxplainer that uses questions about an AST as an evaluation criteria for LLMs. <cite class="ltx_cite ltx_citemacro_citet">Leinonen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> provide an empirical study comparing human generated explanations with those generated by GPT. All of these works are focused on evaluating an LLMs performance in general and, unlike this work, none investigate how code content or documentation can effect an LLM’s understanding.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>LLM Code Understanding</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">It has been argued that <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">understanding</span> is merely the “knowledge of causes” <cite class="ltx_cite ltx_citemacro_cite">Pritchard (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite>, but one can see that this is an incomplete definition of what it means to truly understand something and that “a proper explanatory grip on how cause and effect are related” <cite class="ltx_cite ltx_citemacro_cite">Pritchard (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>)</cite> is required for true understanding. Nevertheless, the knowledge of causes is at least a crucial early step in developing a complete understanding of a topic.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">If this is the basic definition we are using for our basis of determining understanding, then to truly assess whether a piece of code is <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">understood</span> by a person or a language model, then the subject must display knowledge of the cause and effect relationship of the code. Or, to put more simply, the subject should be able to answer the question, “What does this piece of code do?”</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Past work has attempted to test an LLM’s ability to answer this question through code translation by measuring errors in the target language <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>; through code summarization by measuring differences in BLEU, METEOR, or ROUGE-L scores between an LLM-generated summarization and the reference summarization <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>; and through the generation of unit tests by measuring statement and branch coverage across various JavaScript libraries <cite class="ltx_cite ltx_citemacro_cite">Schäfer et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>. Our approach for testing an LLM’s understanding of source code is most closely related to the latter-most method, but differs by not improving upon the initial attempt at unit test generation iteratively if the tests fail.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.3" class="ltx_p">In many cases, a program can be represented as a mapping from inputs into outputs <cite class="ltx_cite ltx_citemacro_cite">Solar-Lezama (<a href="#bib.bib25" title="" class="ltx_ref">2009</a>)</cite>. For example, the program <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="square(n)" display="inline"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.2" xref="S3.p4.1.m1.1.2.cmml"><mi id="S3.p4.1.m1.1.2.2" xref="S3.p4.1.m1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mi id="S3.p4.1.m1.1.2.3" xref="S3.p4.1.m1.1.2.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1a" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mi id="S3.p4.1.m1.1.2.4" xref="S3.p4.1.m1.1.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1b" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mi id="S3.p4.1.m1.1.2.5" xref="S3.p4.1.m1.1.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1c" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mi id="S3.p4.1.m1.1.2.6" xref="S3.p4.1.m1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1d" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mi id="S3.p4.1.m1.1.2.7" xref="S3.p4.1.m1.1.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.2.1e" xref="S3.p4.1.m1.1.2.1.cmml">​</mo><mrow id="S3.p4.1.m1.1.2.8.2" xref="S3.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S3.p4.1.m1.1.2.8.2.1" xref="S3.p4.1.m1.1.2.cmml">(</mo><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">n</mi><mo stretchy="false" id="S3.p4.1.m1.1.2.8.2.2" xref="S3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.2.cmml" xref="S3.p4.1.m1.1.2"><times id="S3.p4.1.m1.1.2.1.cmml" xref="S3.p4.1.m1.1.2.1"></times><ci id="S3.p4.1.m1.1.2.2.cmml" xref="S3.p4.1.m1.1.2.2">𝑠</ci><ci id="S3.p4.1.m1.1.2.3.cmml" xref="S3.p4.1.m1.1.2.3">𝑞</ci><ci id="S3.p4.1.m1.1.2.4.cmml" xref="S3.p4.1.m1.1.2.4">𝑢</ci><ci id="S3.p4.1.m1.1.2.5.cmml" xref="S3.p4.1.m1.1.2.5">𝑎</ci><ci id="S3.p4.1.m1.1.2.6.cmml" xref="S3.p4.1.m1.1.2.6">𝑟</ci><ci id="S3.p4.1.m1.1.2.7.cmml" xref="S3.p4.1.m1.1.2.7">𝑒</ci><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">square(n)</annotation></semantics></math> can be described with <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="square(1)=1" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.2" xref="S3.p4.2.m2.1.2.cmml"><mrow id="S3.p4.2.m2.1.2.2" xref="S3.p4.2.m2.1.2.2.cmml"><mi id="S3.p4.2.m2.1.2.2.2" xref="S3.p4.2.m2.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mi id="S3.p4.2.m2.1.2.2.3" xref="S3.p4.2.m2.1.2.2.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1a" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mi id="S3.p4.2.m2.1.2.2.4" xref="S3.p4.2.m2.1.2.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1b" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mi id="S3.p4.2.m2.1.2.2.5" xref="S3.p4.2.m2.1.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1c" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mi id="S3.p4.2.m2.1.2.2.6" xref="S3.p4.2.m2.1.2.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1d" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mi id="S3.p4.2.m2.1.2.2.7" xref="S3.p4.2.m2.1.2.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.2.1e" xref="S3.p4.2.m2.1.2.2.1.cmml">​</mo><mrow id="S3.p4.2.m2.1.2.2.8.2" xref="S3.p4.2.m2.1.2.2.cmml"><mo stretchy="false" id="S3.p4.2.m2.1.2.2.8.2.1" xref="S3.p4.2.m2.1.2.2.cmml">(</mo><mn id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">1</mn><mo stretchy="false" id="S3.p4.2.m2.1.2.2.8.2.2" xref="S3.p4.2.m2.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.p4.2.m2.1.2.1" xref="S3.p4.2.m2.1.2.1.cmml">=</mo><mn id="S3.p4.2.m2.1.2.3" xref="S3.p4.2.m2.1.2.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.2.cmml" xref="S3.p4.2.m2.1.2"><eq id="S3.p4.2.m2.1.2.1.cmml" xref="S3.p4.2.m2.1.2.1"></eq><apply id="S3.p4.2.m2.1.2.2.cmml" xref="S3.p4.2.m2.1.2.2"><times id="S3.p4.2.m2.1.2.2.1.cmml" xref="S3.p4.2.m2.1.2.2.1"></times><ci id="S3.p4.2.m2.1.2.2.2.cmml" xref="S3.p4.2.m2.1.2.2.2">𝑠</ci><ci id="S3.p4.2.m2.1.2.2.3.cmml" xref="S3.p4.2.m2.1.2.2.3">𝑞</ci><ci id="S3.p4.2.m2.1.2.2.4.cmml" xref="S3.p4.2.m2.1.2.2.4">𝑢</ci><ci id="S3.p4.2.m2.1.2.2.5.cmml" xref="S3.p4.2.m2.1.2.2.5">𝑎</ci><ci id="S3.p4.2.m2.1.2.2.6.cmml" xref="S3.p4.2.m2.1.2.2.6">𝑟</ci><ci id="S3.p4.2.m2.1.2.2.7.cmml" xref="S3.p4.2.m2.1.2.2.7">𝑒</ci><cn type="integer" id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">1</cn></apply><cn type="integer" id="S3.p4.2.m2.1.2.3.cmml" xref="S3.p4.2.m2.1.2.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">square(1)=1</annotation></semantics></math>, <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="square(2)=4" display="inline"><semantics id="S3.p4.3.m3.1a"><mrow id="S3.p4.3.m3.1.2" xref="S3.p4.3.m3.1.2.cmml"><mrow id="S3.p4.3.m3.1.2.2" xref="S3.p4.3.m3.1.2.2.cmml"><mi id="S3.p4.3.m3.1.2.2.2" xref="S3.p4.3.m3.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mi id="S3.p4.3.m3.1.2.2.3" xref="S3.p4.3.m3.1.2.2.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1a" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mi id="S3.p4.3.m3.1.2.2.4" xref="S3.p4.3.m3.1.2.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1b" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mi id="S3.p4.3.m3.1.2.2.5" xref="S3.p4.3.m3.1.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1c" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mi id="S3.p4.3.m3.1.2.2.6" xref="S3.p4.3.m3.1.2.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1d" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mi id="S3.p4.3.m3.1.2.2.7" xref="S3.p4.3.m3.1.2.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.1.2.2.1e" xref="S3.p4.3.m3.1.2.2.1.cmml">​</mo><mrow id="S3.p4.3.m3.1.2.2.8.2" xref="S3.p4.3.m3.1.2.2.cmml"><mo stretchy="false" id="S3.p4.3.m3.1.2.2.8.2.1" xref="S3.p4.3.m3.1.2.2.cmml">(</mo><mn id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">2</mn><mo stretchy="false" id="S3.p4.3.m3.1.2.2.8.2.2" xref="S3.p4.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.p4.3.m3.1.2.1" xref="S3.p4.3.m3.1.2.1.cmml">=</mo><mn id="S3.p4.3.m3.1.2.3" xref="S3.p4.3.m3.1.2.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.2.cmml" xref="S3.p4.3.m3.1.2"><eq id="S3.p4.3.m3.1.2.1.cmml" xref="S3.p4.3.m3.1.2.1"></eq><apply id="S3.p4.3.m3.1.2.2.cmml" xref="S3.p4.3.m3.1.2.2"><times id="S3.p4.3.m3.1.2.2.1.cmml" xref="S3.p4.3.m3.1.2.2.1"></times><ci id="S3.p4.3.m3.1.2.2.2.cmml" xref="S3.p4.3.m3.1.2.2.2">𝑠</ci><ci id="S3.p4.3.m3.1.2.2.3.cmml" xref="S3.p4.3.m3.1.2.2.3">𝑞</ci><ci id="S3.p4.3.m3.1.2.2.4.cmml" xref="S3.p4.3.m3.1.2.2.4">𝑢</ci><ci id="S3.p4.3.m3.1.2.2.5.cmml" xref="S3.p4.3.m3.1.2.2.5">𝑎</ci><ci id="S3.p4.3.m3.1.2.2.6.cmml" xref="S3.p4.3.m3.1.2.2.6">𝑟</ci><ci id="S3.p4.3.m3.1.2.2.7.cmml" xref="S3.p4.3.m3.1.2.2.7">𝑒</ci><cn type="integer" id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">2</cn></apply><cn type="integer" id="S3.p4.3.m3.1.2.3.cmml" xref="S3.p4.3.m3.1.2.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">square(2)=4</annotation></semantics></math>,… etc. Unsurprisingly then, how well an individual can predict input-output pairs can serve as a very useful surrogate metric for how well an individual understands a piece of code. After all, someone cannot truly understand software without being able to predict the software’s behavior. Module documentation, function docstrings, inline code comments, and variable names provide hints or detailed descriptions of what the user can expect of these input-output pairs, and often allow human users to better predict that behavior.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Modern unit test frameworks provide both a formal language for describing input-output pairs and an easily automated method of checking an LLM’s output. In order to measure an LLM’s ability to generate these input-output pairs, we therefore task the LLM with generating unit tests for a piece of software while varying the quality or quantity of the documentation at input, as well as other code properties. We then measure the percent of unit tests that pass vs the number that fail or produce an error as the percent of correctly predicted input-output pairs.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In the following section we describe our experimental process for testing an LLM’s ability to understand code well enough to produce successful unit tests. In particular, we present an experimental setup for testing the generation of unit tests under varying initial documentation conditions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To test an LLM’s ability to understand a piece of software, we task the language model to generate unit tests. We then run the unit tests and record each test as 1 of 3 results: <span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Runtime Error</span>, where a unit test crashed before it could finish running; <span id="S4.p2.1.2" class="ltx_text ltx_font_bold">Failure</span>, where a unit test ran but failed to pass; and <span id="S4.p2.1.3" class="ltx_text ltx_font_bold">Success</span>, where a unit test ran and successfully passed. All LLMs used their default parameters during experiments.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We use all 164 HumanEval <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> ground truth solutions as our basis for generating unit tests. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Available for use under the MIT license <a target="_blank" href="https://github.com/openai/human-eval" title="" class="ltx_ref ltx_href">here</a>.</span></span></span> We chose HumanEval both for its common use as an LLM benchmark for code understanding <cite class="ltx_cite ltx_citemacro_cite">Hong et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>); Zelikman et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023a</a>)</cite> (although intended for text-to-code generation), and for its wide variety of self encapsulated functions that can be easily run within unit test frameworks. We perform several post processing steps on the LLM generated code to ensure it can run automatically. First, we remove all import statements that result in an error (the LLM will commonly import hallucinated modules as it was not provided the original module names at input). Second, we append the LLM’s code to a file with the HumanEval function to ensure the unit test has access to all code pieces required to run.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We generate several automatic variations of each function in the HumanEval dataset. <span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Base File</span>, which includes only the ground truth solution for a HumanEval function with no docstring comments. <span id="S4.p4.1.2" class="ltx_text ltx_font_bold">Comments</span>, which is the <span id="S4.p4.1.3" class="ltx_text ltx_font_italic">base file</span> but includes any comments and the docstring provided for the HumanEval function. The docstrings for HumanEval functions typically contain a description of the function along with a few execution examples. <span id="S4.p4.1.4" class="ltx_text ltx_font_bold">Random Comments</span> which has the <span id="S4.p4.1.5" class="ltx_text ltx_font_italic">base file</span> but with a random comment or docstring from a different HumanEval function, <span id="S4.p4.1.6" class="ltx_text ltx_font_bold">Animal Variable Names</span>, which is the <span id="S4.p4.1.7" class="ltx_text ltx_font_italic">base file</span> but with all variable and (non-external) function names replaced with animals (e.g. Bird, Cat, Dog etc.), and <span id="S4.p4.1.8" class="ltx_text ltx_font_bold">Random Variable Names</span> which is the <span id="S4.p4.1.9" class="ltx_text ltx_font_italic">base file</span> but with all variable and function names replaced with random strings. For instance, a variable <span id="S4.p4.1.10" class="ltx_text ltx_lst_identifier ltx_lstlisting ltx_font_typewriter ltx_framed ltx_framed_rectangle" style="background-color:#FDF6E3;">format_str</span> might be changed to <span id="S4.p4.1.11" class="ltx_text ltx_lst_identifier ltx_lstlisting ltx_font_typewriter ltx_framed ltx_framed_rectangle" style="background-color:#FDF6E3;">eMbafsd</span>. In addition, we also investigate how removing random portions of the docstrings, line-by-line, can effect performance. Each line was kept with uniform random probability. We evaluate unit tests generated from 10%, 25%, 50% and 75% of the original docstrings remaining. To avoid any infinite loops generated by the LLM, all tests would automatically fail after taking more than 10 seconds to run. Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Experimental Setup ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of a function from the <span id="S4.p4.1.12" class="ltx_text ltx_font_bold">Comments</span> category.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">In order to determine that the LLMs are not generating trivial tests, we also conduct an analysis of how much of the source code is executed by the generated unit tests. We use line coverage, or the percent of lines in the source code that are executed by the unit tests, as our metric for this analysis.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<div id="S4.F1.1" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_align_center ltx_framed ltx_framed_rectangle ltx_listing" style="background-color:#FDF6E3;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGVmIG1vbm90b25pYyhsOiBsaXN0KToKICAgICIiIlJldHVybiBUcnVlIGlzIGxpc3QgZWxlbWVudHMKICAgIGFyZSBtb25vdG9uaWNhbGx5IGluY3JlYXNpbmcKICAgIG9yIGRlY3JlYXNpbmcuCiAgICA+Pj4gbW9ub3RvbmljKFsxLCAyLCA0LCAyMF0pCiAgICBUcnVlCiAgICA+Pj4gbW9ub3RvbmljKFsxLCAyMCwgNCwgMTBdKQogICAgRmFsc2UKICAgID4+PiBtb25vdG9uaWMoWzQsIDEsIDAsIC0xMF0pCiAgICBUcnVlCiAgICAiIiIKICAgIGlmIGwgPT0gc29ydGVkKGwpIG9yXAogICAgICAgIGwgPT0gc29ydGVkKGwsIHJldmVyc2U9VHJ1ZSk6CiAgICAgICAgcmV0dXJuIFRydWUKICAgIHJldHVybiBGYWxzZQ==" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="color:#CB4B15;">def</span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">monotonic</span><span id="lstnumberx1.4" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx1.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span id="lstnumberx1.6" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx1.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" style="color:#CB4B15;">list</span><span id="lstnumberx1.9" class="ltx_text ltx_font_typewriter">):</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_space ltx_font_typewriter">    </span><span id="lstnumberx2.2" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#D33682;">""</span><span id="lstnumberx2.3" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#D33682;">"Return<span id="lstnumberx2.3.1" class="ltx_text ltx_lst_space"> </span>True<span id="lstnumberx2.3.2" class="ltx_text ltx_lst_space"> </span>is<span id="lstnumberx2.3.3" class="ltx_text ltx_lst_space"> </span>list<span id="lstnumberx2.3.4" class="ltx_text ltx_lst_space"> </span>elements</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx3.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">are</span><span id="lstnumberx3.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx3.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">monotonically</span><span id="lstnumberx3.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx3.6" class="ltx_text ltx_font_typewriter" style="color:#D33682;">increasing</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx4.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">or</span><span id="lstnumberx4.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx4.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">decreasing.</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx5.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">&gt;&gt;&gt;</span><span id="lstnumberx5.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx5.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">monotonic([1,</span><span id="lstnumberx5.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx5.6" class="ltx_text ltx_font_typewriter" style="color:#D33682;">2,</span><span id="lstnumberx5.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx5.8" class="ltx_text ltx_font_typewriter" style="color:#D33682;">4,</span><span id="lstnumberx5.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx5.10" class="ltx_text ltx_font_typewriter" style="color:#D33682;">20])</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx6.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">True</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx7.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">&gt;&gt;&gt;</span><span id="lstnumberx7.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx7.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">monotonic([1,</span><span id="lstnumberx7.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx7.6" class="ltx_text ltx_font_typewriter" style="color:#D33682;">20,</span><span id="lstnumberx7.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx7.8" class="ltx_text ltx_font_typewriter" style="color:#D33682;">4,</span><span id="lstnumberx7.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx7.10" class="ltx_text ltx_font_typewriter" style="color:#D33682;">10])</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx8.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">False</span>
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx9.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">&gt;&gt;&gt;</span><span id="lstnumberx9.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx9.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">monotonic([4,</span><span id="lstnumberx9.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx9.6" class="ltx_text ltx_font_typewriter" style="color:#D33682;">1,</span><span id="lstnumberx9.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx9.8" class="ltx_text ltx_font_typewriter" style="color:#D33682;">0,</span><span id="lstnumberx9.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;"> </span><span id="lstnumberx9.10" class="ltx_text ltx_font_typewriter" style="color:#D33682;">-10])</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx10.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">True</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="color:#D33682;">    </span><span id="lstnumberx11.2" class="ltx_text ltx_font_typewriter" style="color:#D33682;">"</span><span id="lstnumberx11.3" class="ltx_text ltx_lst_string"></span><span id="lstnumberx11.4" class="ltx_text ltx_font_typewriter" style="color:#D33682;">""</span>
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_space ltx_font_typewriter">    </span><span id="lstnumberx12.2" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="color:#CB4B15;">if</span><span id="lstnumberx12.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span id="lstnumberx12.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.6" class="ltx_text ltx_font_typewriter">==</span><span id="lstnumberx12.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" style="color:#CB4B15;">sorted</span><span id="lstnumberx12.9" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx12.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span id="lstnumberx12.11" class="ltx_text ltx_font_typewriter">)</span><span id="lstnumberx12.12" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.13" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="color:#CB4B15;">or</span><span id="lstnumberx12.14" class="ltx_text ltx_font_typewriter">\</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx13.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span id="lstnumberx13.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx13.4" class="ltx_text ltx_font_typewriter">==</span><span id="lstnumberx13.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx13.6" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" style="color:#CB4B15;">sorted</span><span id="lstnumberx13.7" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx13.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span id="lstnumberx13.9" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx13.10" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx13.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter">reverse</span><span id="lstnumberx13.12" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx13.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter">True</span><span id="lstnumberx13.14" class="ltx_text ltx_font_typewriter">):</span>
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx14.2" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="color:#CB4B15;">return</span><span id="lstnumberx14.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx14.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">True</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span id="lstnumberx15.1" class="ltx_text ltx_lst_space ltx_font_typewriter">    </span><span id="lstnumberx15.2" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="color:#CB4B15;">return</span><span id="lstnumberx15.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx15.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">False</span>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example HumanEval reference implementation with docstring.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Results ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of GPT-3.5 (<span id="S5.p1.1.1" class="ltx_text ltx_font_italic">gpt-3.5-turbo</span>) and GPT-4 generating unit tests on the variations of the HumanEval dataset discussed above. <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>These results were generated using OpenAI’s <a target="_blank" href="https://platform.openai.com/docs/guides/text-generation/chat-completions-api" title="" class="ltx_ref ltx_href">Chat Completions API</a>. OpenAI’s sharing and publication policy regarding the use of their API can be seen <a target="_blank" href="https://openai.com/policies/sharing-publication-policy#research" title="" class="ltx_ref ltx_href">here</a>.</span></span></span> There are several key things to note from these results. First the number of runtime errors produced by GPT-3.5 is much greater than those produced by GPT-4, as is consistent with many results for similar code understanding tasks <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. A manual examination of the generated test cases shows that GPT-3.5 often generated simple assert statements rather than using pytest, and many of the runtime errors were caused by assert statements failing. Second, we note that the <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">random comments</span> scenario performs worse than any other scenario on both GPT-3.5 and 4 (only 22.1% and 68.1% successes respectively) as shown in Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Results ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, confirming our hypothesis that incorrect documentation can hurt an LLM’s understanding of a piece of code. A statistical bootstrap shows that <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">random comments</span> have a higher proportion of runtime errors and failed tests than any other version of the code by a statistically significant margin with <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\alpha=0.05" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">α</mi><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><eq id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></eq><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝛼</ci><cn type="float" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\alpha=0.05</annotation></semantics></math>. Thirdly, we note that changing the code content (via variable names), had a relatively minor effect on the LLM’s code understanding when compared with the <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">base file</span>. A bootstrap shows that changing variables to <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">random names</span> did not impact the proportion of errors and failed tests by a statistically significant margin, but changing the variables to <span id="S5.p1.1.6" class="ltx_text ltx_font_italic">animals</span> did cause a statistically significant change (albeit a small one of 44.7% to 40.6% successes for GPT-3.5 and 78.5% to 76.6% for GPT-4). Finally, having comments did not <span id="S5.p1.1.7" class="ltx_text ltx_font_bold">significantly</span> increase the LLM’s ability to understand the code. Again a bootstrap confirms the proportion of errors and failed tests did not change between the <span id="S5.p1.1.8" class="ltx_text ltx_font_italic">base file</span> with/without comments by a statistically significant margin.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Model</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Basefile</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Comments</th>
<th id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Random Comments</th>
<th id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Random Names</th>
<th id="S5.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Animal Names</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<th id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">GPT-3.5</th>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1323</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">883</td>
<td id="S5.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">892</td>
<td id="S5.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1464</td>
<td id="S5.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">1482</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<th id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">GPT-4</th>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3832</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3295</td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2324</td>
<td id="S5.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3725</td>
<td id="S5.T1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">3589</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of tests generated by models under variations of HumanEval files.</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Model</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">p = 0</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">p = 0.1</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">p = 0.25</th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">p = 0.50</th>
<th id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">p = 0.75</th>
<th id="S5.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">p = 1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">GPT-3.5</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1323</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">958</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">929</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">893</td>
<td id="S5.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">908</td>
<td id="S5.T2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">883</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">GPT-4</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3832</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3596</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3446</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3198</td>
<td id="S5.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3335</td>
<td id="S5.T2.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">3295</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Number of tests generated by models with proportions of dropped comment lines within HumanEval files.</figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">As mentioned above, we also conducted experiments to determine how partial comments might affect an LLM’s ability to understand code. Figure <a href="#S5.F3" title="Figure 3 ‣ 5 Results ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows these results. GPT-3.5 shows that providing partial comments causes an initial spike in the number of errors, before declining as the percentage of comments kept increases. GPT-4 does not show the same pattern, however a bootstrap shows that having the full amount of comments gave fewer errors over when the proportion of kept comments was 10%, 25% and 50% by statistically significant margin. As shown in the figure, these changes were relatively minor. No other comparisons were statistically significant for GPT-4. We therefore cannot currently conclude whether or not partial comments can significantly hurt an LLM’s code understanding based on the current evidence.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Finally, we conducted an analysis of line coverage by unit tests to prevent the possibility of the LLM producing trivial tests to gain a higher success rate. This is shown in Figures <a href="#S6.F4" title="Figure 4 ‣ 6 Conclusions and Discussion ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S6.F5" title="Figure 5 ‣ 6 Conclusions and Discussion ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. First, it is worth noting that the results with <span id="S5.p3.1.1" class="ltx_text ltx_font_italic">random comments</span> had lower average code coverage than all other methods by a statistically significant margin. Second, we note that for both GPT-3.5 and GPT-4, code with <span id="S5.p3.1.2" class="ltx_text ltx_font_italic">comments</span> generated a higher average line coverage than all other methods by a statistically significant margin. Thirdly, we note that modifying the variables names of the code largely did not have a statistically significant effect. The one exception was the <span id="S5.p3.1.3" class="ltx_text ltx_font_italic">animal names</span> category generated by GPT-3.5, which had an average amount of line coverage that was significantly lower than the line coverage of unit tests generated from the baseline, as seen in Figure <a href="#S6.F4" title="Figure 4 ‣ 6 Conclusions and Discussion ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, partial docstrings mostly did not have a statistically significant effect on the average amount of line coverage. There was one exception when 10% of the docstring was kept for GPT-3.5, which did worse than the baseline by a statistically significant margin.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">We also provide the number of unit tests generated by models for various conditions. Table <a href="#S5.T1" title="Table 1 ‣ 5 Results ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of test cases created by GPT-3.5 and GPT-4 for each of the file categories mentioned above. Table <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ Testing the Effect of Code Documentation on Large Language Model Code Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the number of tests generated by GPT-3.5 and GPT-4 with various portions of the HumanEval docstring lines dropped.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2404.03114/assets/x1.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Proportion of runtime errors or failed tests that happen with GPT-3.5 (left) and GPT-4 (right) generating unit tests on modified versions of HumanEval code.</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2404.03114/assets/x2.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Proportion of runtime errors or failed tests that happen with GPT-3.5 (left) and GPT-4 (right) generating unit tests on different proportions of docstring lines kept on HumanEval code.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduce the effect of code documentation on LLM code understanding and show in our initial experiments that the relative prevalence of documentation has little to no significant effect on an LLM’s understanding as we have defined it. This is a little complicated by the fact that we found no significant difference in the amount of successful unit tests but did see a significant difference in the code coverage of those unit tests. Even when comparing commented code to code without comments and all of the variable names changed to random characters, we find little to no significant difference in the LLM’s ability to generate successful unit tests. So, although more of the code with <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">comments</span> is being covered by the unit tests than the code without, this does not improve the overall unit test success rate. This suggests that the LLM is better at understanding the different execution paths of the code, but this may make the creation of a successful unit test more difficult. Alternatively, we do show that incorrect comments do significantly affect an LLM’s ability to understand a piece of code.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">It is possible that in much of the training data used to train the OpenAI models tested in this study, the code did not contain many comments or documentation. As such, it may be possible that a model’s ability to utilize comments is dependent on how much of this documentation is in the training data, and other non-OpenAI LLM’s may make better or worse use of the information provided. It is also possible that correct comments do not add much information as it would be relevant to the creation of unit tests, but incorrect comments still <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">confuse</span> the LLMs.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2404.03114/assets/x3.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Average percent of line coverage with GPT-3.5 (left) and GPT-4 (right) generating unit tests on modified versions of HumanEval code.</figcaption>
</figure>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2404.03114/assets/x4.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Average percent of line coverage with GPT-3.5 (left) and GPT-4 (right) generating unit tests on different proportions of docstring lines kept on HumanEval code.</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While we do introduce a new research question to the space of LLM code understanding, we recognize that this is a limited test that is missing a broader comparison study between programming languages, models, prompting techniques, hyperparameters, and input documentation modification, and that this paper represents merely an introduction to the idea of documentation’s effects on LLM code understanding. In addition, since OpenAI’s training data is not publicly available, there is a small but real possibility that the training data may contain the HumanEval dataset, in which case these results would be invalid.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In future work, we believe that more comprehensive tests should be done by including examples of more complex code and documentation (such as that found in ClassEval <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>). These tests could be evaluated while simultaneously seeing the affect of and controlling for code complexity using metrics such as cyclomatic complexity or maintainability index <cite class="ltx_cite ltx_citemacro_cite">McCabe (<a href="#bib.bib15" title="" class="ltx_ref">1976</a>); Coleman et al. (<a href="#bib.bib4" title="" class="ltx_ref">1994</a>)</cite>. We also recommend evaluating unit test success rate while controlling for code coverage to properly isolate and determine good success metrics.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Finally, this same research question could be evaluated using code summarization as the code understanding task, because it still measures how well the LLM attempts to answer “What does this piece of code do?”</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors thank Tim Welsh and Guido Zarrella for advice on experimental design and editing. This work was funded under MITRE’s 2023 Independent Research and Development Program.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Mansi Agnihotri and Anuradha Chug.

</span>
<span class="ltx_bibblock">A systematic literature survey of software metrics, code smells and refactoring techniques.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Information Processing Systems</em>, 16(4):915–934.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bui et al. (2023)</span>
<span class="ltx_bibblock">
Nghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.00029" title="" class="ltx_ref ltx_href">Codetf: One-stop transformer library for state-of-the-art code llm</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2107.03374" title="" class="ltx_ref ltx_href">Evaluating large language models trained on code</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coleman et al. (1994)</span>
<span class="ltx_bibblock">
D. Coleman, D. Ash, B. Lowther, and P. Oman. 1994.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/2.303623" title="" class="ltx_ref ltx_href">Using metrics to evaluate software system maintainability</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer</em>, 27(8):44–49.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2023)</span>
<span class="ltx_bibblock">
Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.01861" title="" class="ltx_ref ltx_href">Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. (2023)</span>
<span class="ltx_bibblock">
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.00352" title="" class="ltx_ref ltx_href">Metagpt: Meta programming for a multi-agent collaborative framework</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. (2023)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2301.08745" title="" class="ltx_ref ltx_href">Is ChatGPT a good translator? yes with GPT-4 as the engine</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalliamvakou (2022)</span>
<span class="ltx_bibblock">
Eirini Kalliamvakou. 2022.

</span>
<span class="ltx_bibblock">Research: quantifying GitHub Copilot’s impact on developer productivity and happiness — github.blog.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/</a>.

</span>
<span class="ltx_bibblock">[Accessed 27-11-2023].

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi and Federmann (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi and Christian Federmann. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.eamt-1.19" title="" class="ltx_ref ltx_href">Large language models are state-of-the-art evaluators of translation quality</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 193–203, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2023)</span>
<span class="ltx_bibblock">
Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, and Shafiq Joty. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.08992" title="" class="ltx_ref ltx_href">Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leinonen et al. (2023)</span>
<span class="ltx_bibblock">
Juho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne Kim, Andrew Tran, and Arto Hellas. 2023.

</span>
<span class="ltx_bibblock">Comparing code explanations created by students and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03938</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lenarduzzi et al. (2020)</span>
<span class="ltx_bibblock">
Valentina Lenarduzzi, Alberto Sillitti, and Davide Taibi. 2020.

</span>
<span class="ltx_bibblock">A survey on code analysis tools for software maintenance prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of 6th International Conference in Software Engineering for Defence Applications: SEDA 2018 6</em>, pages 165–175. Springer.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2021)</span>
<span class="ltx_bibblock">
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2102.04664" title="" class="ltx_ref ltx_href">Codexglue: A machine learning benchmark dataset for code understanding and generation</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.12138" title="" class="ltx_ref ltx_href">Chatgpt: Understanding code syntax and semantics</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCabe (1976)</span>
<span class="ltx_bibblock">
T.J. McCabe. 1976.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TSE.1976.233837" title="" class="ltx_ref ltx_href">A complexity measure</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, SE-2(4):308–320.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moor et al. (2007)</span>
<span class="ltx_bibblock">
Oege de Moor, Mathieu Verbaere, Elnar Hajiyev, Pavel Avgustinov, Torbjorn Ekman, Neil Ongkingco, Damien Sereni, and Julian Tibble. 2007.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SCAM.2007.31" title="" class="ltx_ref ltx_href">Keynote address: .ql for source code analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Seventh IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM 2007)</em>, pages 3–16.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem et al. (2023)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.eamt-1.22" title="" class="ltx_ref ltx_href">Adaptive machine translation with large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 227–237, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">GPT-4 technical report</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palacio et al. (2023)</span>
<span class="ltx_bibblock">
David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03873" title="" class="ltx_ref ltx_href">Evaluating and explaining large language models for code using syntactic structures</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023)</span>
<span class="ltx_bibblock">
Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03109" title="" class="ltx_ref ltx_href">Understanding the effectiveness of large language models in code translation</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pritchard (2014)</span>
<span class="ltx_bibblock">
Duncan Pritchard. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-04672-3_18" title="" class="ltx_ref ltx_href"><em id="bib.bib21.1.1.1" class="ltx_emph ltx_font_italic">Knowledge and Understanding</em></a>, page 315–327. Springer International Publishing.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pritchard (2016)</span>
<span class="ltx_bibblock">
Duncan Pritchard. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1017/epi.2015.59" title="" class="ltx_ref ltx_href">Seeing it for oneself: Perceptual knowledge, understanding, and intellectual autonomy</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Episteme</em>, 13(1):29–42.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schäfer et al. (2023)</span>
<span class="ltx_bibblock">
Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023.

</span>
<span class="ltx_bibblock">An empirical evaluation of using large language models for automated unit test generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.06527</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.11366" title="" class="ltx_ref ltx_href">Reflexion: Language agents with verbal reinforcement learning</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solar-Lezama (2009)</span>
<span class="ltx_bibblock">
Armando Solar-Lezama. 2009.

</span>
<span class="ltx_bibblock">The sketching approach to program synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Asian symposium on programming languages and systems</em>, pages 4–13. Springer.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.12865" title="" class="ltx_ref ltx_href">Automatic code summarization via ChatGPT: How far are we?</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Timóteo et al. (2008)</span>
<span class="ltx_bibblock">
Aline Lopes Timóteo, Alexandre Álvaro, Eduardo Santana De Almeida, and Silvio Romero de Lemos Meira. 2008.

</span>
<span class="ltx_bibblock">Software metrics: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Sl: sn</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R. Lyu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3540250.3549113" title="" class="ltx_ref ltx_href">No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>, ESEC/FSE 2022, page 382–394, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, and Ge Yu. 2023a.

</span>
<span class="ltx_bibblock">Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09868</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023b.

</span>
<span class="ltx_bibblock">Codet5+: Open code large language models for code understanding and generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07922</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2022)</span>
<span class="ltx_bibblock">
Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2210.14179" title="" class="ltx_ref ltx_href">Practical program repair in the era of large pre-trained language models</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2022)</span>
<span class="ltx_bibblock">
Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022.

</span>
<span class="ltx_bibblock">A systematic evaluation of large language models of code.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</em>, pages 1–10.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al. (2023)</span>
<span class="ltx_bibblock">
Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. 2023.

</span>
<span class="ltx_bibblock">Parsel: Algorithmic reasoning with language models by composing decompositions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.03113" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.03114" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.03114">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.03114" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.03115" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 15:40:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
