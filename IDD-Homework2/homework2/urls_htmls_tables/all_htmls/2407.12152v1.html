<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction</title>
<!--Generated on Tue Jul 16 20:01:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.12152v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S1" title="In A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2" title="In A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS1" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Molecular String Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS2" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Token Encoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS3" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Deep Learning Architectures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS4" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Bioactivity Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS5" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Performance Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS6" title="In 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS6.SSS1" title="In 2.6 Experimental Setup ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.1 </span>Data Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS6.SSS2" title="In 2.6 Experimental Setup ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.2 </span>Model Training and Optimization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3" title="In A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.SS1" title="In 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Choosing a Neural Network Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.SS2" title="In 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Representing and Encoding Molecular Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.SS3" title="In 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Other Tricks of the Trade</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S4" title="In A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>So Long, and Thanks for All the Data</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rıza Özçelik
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesca Grisoni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Deep learning has significantly accelerated drug discovery, with ‘chemical language’ processing (CLP) emerging as a prominent approach. CLP learns from molecular string representations (<span class="ltx_text ltx_font_italic" id="id1.id1.1">e.g., </span>Simplified Molecular Input Line Entry Systems [SMILES] and Self-Referencing Embedded Strings [SELFIES]) with methods akin to natural language processing. Despite their growing importance, training predictive CLP models is far from trivial, as it involves many ‘bells and whistles’. Here, we analyze the key elements of CLP training, to provide guidelines for newcomers and experts alike. Our study spans three neural network architectures, two string representations, three embedding strategies, across ten bioactivity datasets, for both classification and regression purposes. This ‘hitchhiker’s guide’ not only underscores the importance of certain methodological choices, but it also equips researchers with practical recommendations on ideal choices, <span class="ltx_text ltx_font_italic" id="id1.id1.2">e.g., </span>in terms of neural network architectures, molecular representations, and hyperparameter optimization.</p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine learning has accelerated drug discovery <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib1" title="">1</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib2" title="">2</a></sup></cite>. The prediction of biological properties, such as the interaction with macromolecular targets, has been pivotal in this context, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">e.g., </span>for hit finding and lead optimization <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib3" title="">3</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib4" title="">4</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib2" title="">2</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib5" title="">5</a></sup></cite>. Deep learning models that use string representations of molecules, like Simplified Molecular Input Line Entry System (SMILES) <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib6" title="">6</a></sup></cite> and Self-Referencing Embedded Strings (SELFIES) <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib7" title="">7</a></sup></cite>, have drawn particular interest <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib8" title="">8</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib9" title="">9</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib10" title="">10</a></sup></cite>. Such deep ‘chemical language’ processing approaches apply methods akin to natural language processing to learn from molecular string representations <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib11" title="">11</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib12" title="">12</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Molecular string representations (<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">e.g., </span>SMILES <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib6" title="">6</a></sup></cite> and SELFIES <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib7" title="">7</a></sup></cite>, among others<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib13" title="">13</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib14" title="">14</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib15" title="">15</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib16" title="">16</a></sup></cite>) have found widespread application in cheminformatics and related fields <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib17" title="">17</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib11" title="">11</a></sup></cite>. They convert two-dimensional molecular information into strings, by traversing the molecular graph and annotating atom and bond information with dedicated letters (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>a). Deep ‘chemical language processing’ (CLP) models are then trained to map the chemical information in such strings to a property to be predicted, <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">e.g., </span>a ligand interaction with a target or toxicological properties. Once trained, CLP models can be applied prospectively, for instance, to screen large molecular libraries in search of molecules with desirable properties <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib18" title="">18</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib19" title="">19</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Developing predictive CLP models is far from trivial <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib20" title="">20</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib21" title="">21</a></sup></cite> and it requires many choices to be made <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib22" title="">22</a></sup></cite>, <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">e.g., </span>in terms of molecular string representations and their encoding, and of neural network architectures and their hyperparameters. Each such choice might affect the model performance. Stemming from these observations, this ‘hitchhiker’s guide’ aims to discover best practices in the field, and provide a guideline for what choices to make when training CLP models for bioactivity prediction. Here, we derive our insights from a systematic analysis of three deep learning architectures, two molecular string representations, and three encoding approaches on ten datasets spanning regression and classification tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Ultimately, this ‘hitchhiker’s guide’ provides some ‘tricks of the trade’ and practical recommendations – for beginners and experts alike – on what choices to prioritize when training deep chemical language processing models from scratch. We hope that this paper will accelerate the adoption of deep chemical language processing approaches, and spark novel research to further their potential.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="431" id="S1.F1.g1" src="x1.png" width="623"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_italic" id="S1.F1.9.1">Deep Chemical Language Processing for Bioactivity Prediction.</span> <span class="ltx_text ltx_font_bold" id="S1.F1.10.2">(a)</span> String notations such as SMILES and SELFIES represent a molecular graph as a sequence of characters (‘tokens’). The atoms are represented with periodic table symbols, while branches, rings, and bonds are assigned special characters. <span class="ltx_text ltx_font_bold" id="S1.F1.11.3">(b)</span> Token encoding, where the chosen molecular string is converted into a matrix to train deep learning models. One-hot encoding represents each token with a unique binary vector. Random encoding maps tokens to fixed, unique, and continuous vectors. Learnable encoding starts with a random vector per token and updates the vectors during training to improve the model performance. <span class="ltx_text ltx_font_bold" id="S1.F1.12.4">(c)</span> Architectures used in this study. Convolutional neural networks slide windows over the input sequences, and learn to weight and aggregate the input elements. Recurrent neural networks iterate over the input tokens in a step-wise manner, and update the ‘hidden’ information learned from the sequence (<math alttext="h_{i}" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><msub id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml"><mi id="S1.F1.3.m1.1.1.2" xref="S1.F1.3.m1.1.1.2.cmml">h</mi><mi id="S1.F1.3.m1.1.1.3" xref="S1.F1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><apply id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.3.m1.1.1.1.cmml" xref="S1.F1.3.m1.1.1">subscript</csymbol><ci id="S1.F1.3.m1.1.1.2.cmml" xref="S1.F1.3.m1.1.1.2">ℎ</ci><ci id="S1.F1.3.m1.1.1.3.cmml" xref="S1.F1.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">h_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>). Transformers learn all-pair relationships between the input tokens and learn to weight each input representation to create the representations in the next layers (<math alttext="a_{i}" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><msub id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml"><mi id="S1.F1.4.m2.1.1.2" xref="S1.F1.4.m2.1.1.2.cmml">a</mi><mi id="S1.F1.4.m2.1.1.3" xref="S1.F1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><apply id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.4.m2.1.1.1.cmml" xref="S1.F1.4.m2.1.1">subscript</csymbol><ci id="S1.F1.4.m2.1.1.2.cmml" xref="S1.F1.4.m2.1.1.2">𝑎</ci><ci id="S1.F1.4.m2.1.1.3.cmml" xref="S1.F1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Molecular String Representations</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">String representations capture two-dimensional molecular information as a sequence of characters (‘tokens’). Here, we focus on the two most popular string representations (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>a,b):</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.1">Simplified Molecule Input Line Entry Systems (SMILES)<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib6" title="">6</a></sup></cite></span> strings, which start from any non-hydrogen atom in the molecule and traverse the molecular graph. Atoms are annotated as their element symbols, bonds (except for single bonds) are annotated with special tokens (<span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.2">e.g., </span>, ‘=’: double, ‘<math alttext="\#" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><mi id="S2.I1.i1.p1.1.m1.1.1" mathvariant="normal" xref="S2.I1.i1.p1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">\#</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">#</annotation></semantics></math>’: triple), and branching is indicated by bracket opening and closure. Stereochemical information can also be indicated by dedicated tokens, although this will be not considered in this study. Initially proposed for chemical information storage, SMILES strings constitute, to date, the <span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.3">de facto</span> notation in chemical language processing <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib23" title="">23</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib24" title="">24</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib25" title="">25</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib18" title="">18</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib17" title="">17</a></sup></cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.1">Self-Referencing Embedded Strings (SELFIES)<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib7" title="">7</a></sup></cite></span>, which were recently proposed as SMILES alternatives. SELFIES encode the atoms with their symbols, and annotate their connectivity via branch length, ring size, and referencing previous elements. SELFIES strings have been developed for <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.2">de novo</span> design <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib7" title="">7</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib26" title="">26</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib27" title="">27</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib28" title="">28</a></sup></cite> and are finding increasing applications for bioactivity prediction <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib29" title="">29</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib30" title="">30</a></sup></cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Token Encoding</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">For deep learning purposes, molecular strings are converted into sequences of vectors, by ‘vectorizing’ each token in the string. Here, we experimented with three encoding approaches (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>b), namely:</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.2"><span class="ltx_text ltx_font_italic" id="S2.I2.i1.p1.2.1">One-hot encoding</span>, which represents tokens with <math alttext="V" class="ltx_Math" display="inline" id="S2.I2.i1.p1.1.m1.1"><semantics id="S2.I2.i1.p1.1.m1.1a"><mi id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><ci id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.1.m1.1d">italic_V</annotation></semantics></math>-dimensional binary vectors, <math alttext="V" class="ltx_Math" display="inline" id="S2.I2.i1.p1.2.m2.1"><semantics id="S2.I2.i1.p1.2.m2.1a"><mi id="S2.I2.i1.p1.2.m2.1.1" xref="S2.I2.i1.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.2.m2.1b"><ci id="S2.I2.i1.p1.2.m2.1.1.cmml" xref="S2.I2.i1.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.2.m2.1d">italic_V</annotation></semantics></math> being the number of unique tokens (‘vocabulary’ size). Each token is allocated a different dimension in this space and has a vector on which only that dimension is set to 1, and the rest is set to 0. One-hot encoding ensures that all token vectors are orthogonal to each other, <span class="ltx_text ltx_font_italic" id="S2.I2.i1.p1.2.2">i.e.</span>, the similarity between all tokens is zero.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i2.p1.1.1">Learnable embeddings</span>, whereby a random continuous vector is assigned to each token. These vectors are updated (‘learned’) during training to optimize the predictions. The updates might enable models to learn relationships between parts of the molecules (and the corresponding tokens) that can be useful for bioactivity prediction.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.1">Random Encoding</span>, which assigns a continuous vector to each token and uses the same vector throughout the model training. This approach is intermediate between learnable embeddings and one-hot encoding. Like learnable embeddings, the vectors have continuous values, and they are fixed during training like one-hot encoding.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Deep Learning Architectures</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We experimented with three well-established deep learning architectures (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>c). They differ in how they process and combine information on the (encoded) input molecular strings to predict bioactivity.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> <span class="ltx_text ltx_font_italic" id="S2.T1.7.2">Datasets used in this study.</span> We curated ten bioactivity datasets, for classification (<span class="ltx_text ltx_font_italic" id="S2.T1.8.3">i.e., </span>binding vs non-binding<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib31" title="">31</a></sup></cite>) and regression (<span class="ltx_text ltx_font_italic" id="S2.T1.2.1">i.e., <math alttext="pK_{i}" class="ltx_Math" display="inline" id="S2.T1.2.1.m1.1"><semantics id="S2.T1.2.1.m1.1b"><mrow id="S2.T1.2.1.m1.1.1" xref="S2.T1.2.1.m1.1.1.cmml"><mi id="S2.T1.2.1.m1.1.1.2" xref="S2.T1.2.1.m1.1.1.2.cmml">p</mi><mo id="S2.T1.2.1.m1.1.1.1" xref="S2.T1.2.1.m1.1.1.1.cmml">⁢</mo><msub id="S2.T1.2.1.m1.1.1.3" xref="S2.T1.2.1.m1.1.1.3.cmml"><mi id="S2.T1.2.1.m1.1.1.3.2" xref="S2.T1.2.1.m1.1.1.3.2.cmml">K</mi><mi id="S2.T1.2.1.m1.1.1.3.3" xref="S2.T1.2.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.1.m1.1c"><apply id="S2.T1.2.1.m1.1.1.cmml" xref="S2.T1.2.1.m1.1.1"><times id="S2.T1.2.1.m1.1.1.1.cmml" xref="S2.T1.2.1.m1.1.1.1"></times><ci id="S2.T1.2.1.m1.1.1.2.cmml" xref="S2.T1.2.1.m1.1.1.2">𝑝</ci><apply id="S2.T1.2.1.m1.1.1.3.cmml" xref="S2.T1.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.2.1.m1.1.1.3.1.cmml" xref="S2.T1.2.1.m1.1.1.3">subscript</csymbol><ci id="S2.T1.2.1.m1.1.1.3.2.cmml" xref="S2.T1.2.1.m1.1.1.3.2">𝐾</ci><ci id="S2.T1.2.1.m1.1.1.3.3.cmml" xref="S2.T1.2.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.1.m1.1d">pK_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.1.m1.1e">italic_p italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></span> prediction<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib20" title="">20</a></sup></cite>) purposes. For each dataset, we report ID, target name, and total number of molecules (<span class="ltx_text ltx_font_italic" id="S2.T1.9.4">n</span>).</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.3" style="width:433.6pt;height:315.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.7pt,-56.5pt) scale(1.55890472067858,1.55890472067858) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.2.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.3.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.2.1.2.1">ID</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S2.T1.3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.3.1.2.1.3.1.1" style="width:136.6pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.2.1.3.1.1.1">Target name</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.3.1.2.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.T1.3.1.2.1.4.1">n</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.3.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.1.3.1.1">Class.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.1.3.1.2">DRD3(c)</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.1.3.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.3.1.3.1">
<span class="ltx_p" id="S2.T1.3.1.3.1.3.1.1" style="width:136.6pt;">Dopamine Receptor D3</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.3.1.4">5500</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.4.2">
<td class="ltx_td" id="S2.T1.3.1.4.2.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.4.2.2">FEN1</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.4.2.3.1">
<span class="ltx_p" id="S2.T1.3.1.4.2.3.1.1" style="width:136.6pt;">Flap Structure-specific Endonuclease 1</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.2.4">5500</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.5.3">
<td class="ltx_td" id="S2.T1.3.1.5.3.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.5.3.2">MAP4K2</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.5.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.5.3.3.1">
<span class="ltx_p" id="S2.T1.3.1.5.3.3.1.1" style="width:136.6pt;">Mitogen-activated protein 4x Kinase 2</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.3.4">5500</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.6.4">
<td class="ltx_td" id="S2.T1.3.1.6.4.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.6.4.2">PIN1</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.6.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.6.4.3.1">
<span class="ltx_p" id="S2.T1.3.1.6.4.3.1.1" style="width:136.6pt;">Peptidyl-prolyl cis/trans Isomerase</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.4.4">5500</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.7.5">
<td class="ltx_td" id="S2.T1.3.1.7.5.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.7.5.2">VDR</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.7.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.7.5.3.1">
<span class="ltx_p" id="S2.T1.3.1.7.5.3.1.1" style="width:136.6pt;">Vitamin D Receptor</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.5.4">5500</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.1.1.2">Reg.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.1.1.3">MOR</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.1.1.1">
<span class="ltx_p" id="S2.T1.3.1.1.1.1.1" style="width:136.6pt;"><math alttext="\mu" class="ltx_Math" display="inline" id="S2.T1.3.1.1.1.1.1.m1.1"><semantics id="S2.T1.3.1.1.1.1.1.m1.1a"><mi id="S2.T1.3.1.1.1.1.1.m1.1.1" xref="S2.T1.3.1.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.3.1.1.1.1.1.m1.1b"><ci id="S2.T1.3.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.3.1.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.1.1.1.1.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.1.1.1.1.1.m1.1d">italic_μ</annotation></semantics></math>-opioid Receptor</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.1.4">2838</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.8.6">
<td class="ltx_td" id="S2.T1.3.1.8.6.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.8.6.2">DRD3(r)</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.8.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.8.6.3.1">
<span class="ltx_p" id="S2.T1.3.1.8.6.3.1.1" style="width:136.6pt;">Dopamine Receptor D3</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.8.6.4">3596</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.9.7">
<td class="ltx_td" id="S2.T1.3.1.9.7.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.3.1.9.7.2">SOR</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.3.1.9.7.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.9.7.3.1">
<span class="ltx_p" id="S2.T1.3.1.9.7.3.1.1" style="width:136.6pt;">Sigma Opioid Receptor</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.9.7.4">1325</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.10.8">
<td class="ltx_td ltx_border_b" id="S2.T1.3.1.10.8.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T1.3.1.10.8.2">PIM1</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S2.T1.3.1.10.8.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.1.10.8.3.1">
<span class="ltx_p" id="S2.T1.3.1.10.8.3.1.1" style="width:136.6pt;">Serine/threonine-protein Kinase PIM1</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.3.1.10.8.4">1453</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i1.p1.1.1">Convolutional neural networks</span> (CNNs) <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib32" title="">32</a></sup></cite>. CNNs slide windows (called kernels) over an input sequence, and learn to weight input elements at each window. Such window sliding enables CNNs to capture local patterns in sequences, which are then stacked to predict the global properties of a string (<span class="ltx_text ltx_font_italic" id="S2.I3.i1.p1.1.2">e.g., </span>bioactivity).</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.1">Recurrent neural networks (RNNs) <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib33" title="">33</a></sup></cite></span>. RNNs are recurrent models, <span class="ltx_text ltx_font_italic" id="S2.I3.i2.p1.1.2">i.e., </span>they iterate over the input token and, at each step, compress the information into a ‘hidden state’. Here, we used bidirectional RNNs – which iterate over the sequence in both directions and concatenate the final hidden states to encode the sequence – in combination with gated recurrent units <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib34" title="">34</a></sup></cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I3.i3.p1.1.1">Transformers</span> <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib35" title="">35</a></sup></cite>, which learns patterns between pairs of input tokens, using a mechanism called ‘self-attention’. Self-attention learns to represent input sequences by learning to weight the link between every token pair. Since self-attention makes transformers invariant to the token position in the sequence, here we adopted learnable positional embeddings to capture the sequence structure.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Bioactivity Datasets</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">We curated ten bioactivity datasets containing 1453 to 5500 molecules (Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.T1" title="Table 1 ‣ 2.3 Deep Learning Architectures ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>), and spanning two tasks, namely (a) classification (5 datasets), <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">i.e., </span>predicting whether a molecule is active or inactive on a given target (in the form of a label), and (b) regression (5 datasets), where the coefficient of inhibition (<math alttext="K_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><msub id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">K</mi><mi id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">𝐾</ci><ci id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) is to be predicted.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I4.i1.p1.1.1">Classification datasets.</span> Five datasets were curated from ExCAPE-DB <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib31" title="">31</a></sup></cite>, which collects ligand-target bioactivity information (in the form of ‘active’/‘inactive’) on 1677 proteins. In this work, we selected five targets: dopamine receptor D3 (DRD3), Flap structure-specific endonuclease 1 (FEN1), Mitogen-activated protein kinase kinase kinase kinase 2 (MAP4K2), peptidyl-prolyl cis/trans isomerase (PIN1), and vitamin D receptor (VDR). For each macromolecular target, a set of 5500 molecules (with 10<math alttext="\%" class="ltx_Math" display="inline" id="S2.I4.i1.p1.1.m1.1"><semantics id="S2.I4.i1.p1.1.m1.1a"><mo id="S2.I4.i1.p1.1.m1.1.1" xref="S2.I4.i1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.I4.i1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.I4.i1.p1.1.m1.1.1.cmml" xref="S2.I4.i1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i1.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S2.I4.i1.p1.1.m1.1d">%</annotation></semantics></math> of actives) were selected (<span class="ltx_text ltx_font_italic" id="S2.I4.i1.p1.1.2">see</span> Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS6" title="2.6 Experimental Setup ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2.6</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.2"><span class="ltx_text ltx_font_italic" id="S2.I4.i2.p1.2.1">Regression.</span> We selected five bioactivity datasets from MoleculeACE <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib20" title="">20</a></sup></cite>, which is based on ChEMBL <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib36" title="">36</a></sup></cite>. The following datasets were used for <math alttext="pK_{i}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.1.m1.1"><semantics id="S2.I4.i2.p1.1.m1.1a"><mrow id="S2.I4.i2.p1.1.m1.1.1" xref="S2.I4.i2.p1.1.m1.1.1.cmml"><mi id="S2.I4.i2.p1.1.m1.1.1.2" xref="S2.I4.i2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S2.I4.i2.p1.1.m1.1.1.1" xref="S2.I4.i2.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="S2.I4.i2.p1.1.m1.1.1.3" xref="S2.I4.i2.p1.1.m1.1.1.3.cmml"><mi id="S2.I4.i2.p1.1.m1.1.1.3.2" xref="S2.I4.i2.p1.1.m1.1.1.3.2.cmml">K</mi><mi id="S2.I4.i2.p1.1.m1.1.1.3.3" xref="S2.I4.i2.p1.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.I4.i2.p1.1.m1.1b"><apply id="S2.I4.i2.p1.1.m1.1.1.cmml" xref="S2.I4.i2.p1.1.m1.1.1"><times id="S2.I4.i2.p1.1.m1.1.1.1.cmml" xref="S2.I4.i2.p1.1.m1.1.1.1"></times><ci id="S2.I4.i2.p1.1.m1.1.1.2.cmml" xref="S2.I4.i2.p1.1.m1.1.1.2">𝑝</ci><apply id="S2.I4.i2.p1.1.m1.1.1.3.cmml" xref="S2.I4.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I4.i2.p1.1.m1.1.1.3.1.cmml" xref="S2.I4.i2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.I4.i2.p1.1.m1.1.1.3.2.cmml" xref="S2.I4.i2.p1.1.m1.1.1.3.2">𝐾</ci><ci id="S2.I4.i2.p1.1.m1.1.1.3.3.cmml" xref="S2.I4.i2.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i2.p1.1.m1.1c">pK_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.I4.i2.p1.1.m1.1d">italic_p italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> prediction: Serotonin 1a receptor (5-HT1A), <math alttext="\mu" class="ltx_Math" display="inline" id="S2.I4.i2.p1.2.m2.1"><semantics id="S2.I4.i2.p1.2.m2.1a"><mi id="S2.I4.i2.p1.2.m2.1.1" xref="S2.I4.i2.p1.2.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.I4.i2.p1.2.m2.1b"><ci id="S2.I4.i2.p1.2.m2.1.1.cmml" xref="S2.I4.i2.p1.2.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i2.p1.2.m2.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.I4.i2.p1.2.m2.1d">italic_μ</annotation></semantics></math>-opioid Receptor 1 (MOR), dopamine receptor D3 (DRD3), sigma Opioid Receptor 1 (SOR), and Serine/threonine-protein Kinase PIM1 (PIM1). These datasets were selected to span several target families and to ensure a sufficient number of molecules available for training and testing (from 1453 to 2596).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">The classification datasets have more molecules than the regression datasets and were built to contain structurally diverse molecules (<span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.1">see</span> Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.SS6" title="2.6 Experimental Setup ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2.6</span></a>). Hence, they can be seen as a proxy for hit discovery campaigns, where structurally novel, and bioactive molecules are searched for. Conversely, the regression datasets, which originate from ChEMBL, mostly contain series of highly similar molecules, hence resembling a lead optimization campaign.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Performance Evaluation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.5">The performance of <span class="ltx_text ltx_font_italic" id="S2.SS5.p1.5.1">classification</span> models was evaluated via the balanced accuracy (BA), expressed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{BA}=\frac{1}{2}\left(\frac{TP}{nP}+\frac{TN}{nN}\right)," class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mtext id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3a.cmml">BA</mtext><mo id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.3.2.cmml">1</mn><mn id="S2.E1.m1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.3.3.cmml">2</mn></mfrac><mo id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">T</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">P</mi></mrow><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">n</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">P</mi></mrow></mfrac><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">N</mi></mrow><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">N</mi></mrow></mfrac></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"></eq><ci id="S2.E1.m1.1.1.1.1.3a.cmml" xref="S2.E1.m1.1.1.1.1.3"><mtext id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">BA</mtext></ci><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.3"></divide><cn id="S2.E1.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.3.2">1</cn><cn id="S2.E1.m1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.3.3">2</cn></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><plus id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"><divide id="S2.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"></divide><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.2">𝑇</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.3">𝑃</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.2">𝑛</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.3">𝑃</ci></apply></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3"></divide><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.2.3">𝑁</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.2">𝑛</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.3.3">𝑁</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{BA}=\frac{1}{2}\left(\frac{TP}{nP}+\frac{TN}{nN}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">BA = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( divide start_ARG italic_T italic_P end_ARG start_ARG italic_n italic_P end_ARG + divide start_ARG italic_T italic_N end_ARG start_ARG italic_n italic_N end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS5.p1.4">where <math alttext="TP" class="ltx_Math" display="inline" id="S2.SS5.p1.1.m1.1"><semantics id="S2.SS5.p1.1.m1.1a"><mrow id="S2.SS5.p1.1.m1.1.1" xref="S2.SS5.p1.1.m1.1.1.cmml"><mi id="S2.SS5.p1.1.m1.1.1.2" xref="S2.SS5.p1.1.m1.1.1.2.cmml">T</mi><mo id="S2.SS5.p1.1.m1.1.1.1" xref="S2.SS5.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS5.p1.1.m1.1.1.3" xref="S2.SS5.p1.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.1.m1.1b"><apply id="S2.SS5.p1.1.m1.1.1.cmml" xref="S2.SS5.p1.1.m1.1.1"><times id="S2.SS5.p1.1.m1.1.1.1.cmml" xref="S2.SS5.p1.1.m1.1.1.1"></times><ci id="S2.SS5.p1.1.m1.1.1.2.cmml" xref="S2.SS5.p1.1.m1.1.1.2">𝑇</ci><ci id="S2.SS5.p1.1.m1.1.1.3.cmml" xref="S2.SS5.p1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.1.m1.1c">TP</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p1.1.m1.1d">italic_T italic_P</annotation></semantics></math> and <math alttext="TN" class="ltx_Math" display="inline" id="S2.SS5.p1.2.m2.1"><semantics id="S2.SS5.p1.2.m2.1a"><mrow id="S2.SS5.p1.2.m2.1.1" xref="S2.SS5.p1.2.m2.1.1.cmml"><mi id="S2.SS5.p1.2.m2.1.1.2" xref="S2.SS5.p1.2.m2.1.1.2.cmml">T</mi><mo id="S2.SS5.p1.2.m2.1.1.1" xref="S2.SS5.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S2.SS5.p1.2.m2.1.1.3" xref="S2.SS5.p1.2.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.2.m2.1b"><apply id="S2.SS5.p1.2.m2.1.1.cmml" xref="S2.SS5.p1.2.m2.1.1"><times id="S2.SS5.p1.2.m2.1.1.1.cmml" xref="S2.SS5.p1.2.m2.1.1.1"></times><ci id="S2.SS5.p1.2.m2.1.1.2.cmml" xref="S2.SS5.p1.2.m2.1.1.2">𝑇</ci><ci id="S2.SS5.p1.2.m2.1.1.3.cmml" xref="S2.SS5.p1.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.2.m2.1c">TN</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p1.2.m2.1d">italic_T italic_N</annotation></semantics></math> are the numbers of correctly classified positives and negatives, while <math alttext="nP" class="ltx_Math" display="inline" id="S2.SS5.p1.3.m3.1"><semantics id="S2.SS5.p1.3.m3.1a"><mrow id="S2.SS5.p1.3.m3.1.1" xref="S2.SS5.p1.3.m3.1.1.cmml"><mi id="S2.SS5.p1.3.m3.1.1.2" xref="S2.SS5.p1.3.m3.1.1.2.cmml">n</mi><mo id="S2.SS5.p1.3.m3.1.1.1" xref="S2.SS5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S2.SS5.p1.3.m3.1.1.3" xref="S2.SS5.p1.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.3.m3.1b"><apply id="S2.SS5.p1.3.m3.1.1.cmml" xref="S2.SS5.p1.3.m3.1.1"><times id="S2.SS5.p1.3.m3.1.1.1.cmml" xref="S2.SS5.p1.3.m3.1.1.1"></times><ci id="S2.SS5.p1.3.m3.1.1.2.cmml" xref="S2.SS5.p1.3.m3.1.1.2">𝑛</ci><ci id="S2.SS5.p1.3.m3.1.1.3.cmml" xref="S2.SS5.p1.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.3.m3.1c">nP</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p1.3.m3.1d">italic_n italic_P</annotation></semantics></math> and <math alttext="nN" class="ltx_Math" display="inline" id="S2.SS5.p1.4.m4.1"><semantics id="S2.SS5.p1.4.m4.1a"><mrow id="S2.SS5.p1.4.m4.1.1" xref="S2.SS5.p1.4.m4.1.1.cmml"><mi id="S2.SS5.p1.4.m4.1.1.2" xref="S2.SS5.p1.4.m4.1.1.2.cmml">n</mi><mo id="S2.SS5.p1.4.m4.1.1.1" xref="S2.SS5.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S2.SS5.p1.4.m4.1.1.3" xref="S2.SS5.p1.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.4.m4.1b"><apply id="S2.SS5.p1.4.m4.1.1.cmml" xref="S2.SS5.p1.4.m4.1.1"><times id="S2.SS5.p1.4.m4.1.1.1.cmml" xref="S2.SS5.p1.4.m4.1.1.1"></times><ci id="S2.SS5.p1.4.m4.1.1.2.cmml" xref="S2.SS5.p1.4.m4.1.1.2">𝑛</ci><ci id="S2.SS5.p1.4.m4.1.1.3.cmml" xref="S2.SS5.p1.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.4.m4.1c">nN</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p1.4.m4.1d">italic_n italic_N</annotation></semantics></math> are the total number of positive and negative molecules, respectively.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">The performance of <span class="ltx_text ltx_font_italic" id="S2.SS5.p2.1.1">regression</span> models was evaluated via concordance index <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib37" title="">37</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib38" title="">38</a></sup></cite>, which quantifies the model’s ability to rank molecules by their experimental potency based on the predicted potency. Both metrics are bound between 0 and 1 – the closer to 1, the better the performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Experimental Setup</h3>
<section class="ltx_subsubsection" id="S2.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.1 </span>Data Preparation</h4>
<div class="ltx_para" id="S2.SS6.SSS1.p1">
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i1.p1.1.1">Classification.</span> For each selected target, we constructed two sets: (i) Set 1 – built by randomly sampling 350 actives and 3500 inactives, and (ii) Set 2 – built by randomly selecting 150 actives and 1500 inactives that were sufficiently distant from Set 1 (<span class="ltx_text ltx_font_italic" id="S2.I5.i1.p1.1.2">i.e.</span>, having a minimum edit distance on canonical SMILES strings larger than 10, and a maximum Tanimoto similarity on extended connectivity fingerprints<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib39" title="">39</a></sup></cite> smaller than 60%). Set 1 was used as a training set, while the molecules of Set 2 were equally divided into a validation and a test set.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I5.i2.p1.1.1">Regression.</span><span class="ltx_text" id="S2.I5.i2.p1.1.2"> For each target, we created five folds of training validation and test sets (70%, 15%, 15%, respectively). We heuristically minimized train-test similarity by first grouping molecules based on substructure similarity, and then dividing them into training and test set (via <span class="ltx_text ltx_font_typewriter" id="S2.I5.i2.p1.1.2.1">deepchem</span>, <span class="ltx_text ltx_font_typewriter" id="S2.I5.i2.p1.1.2.2">FingerprintSplitter</span> <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib40" title="">40</a></sup></cite>).</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS6.SSS1.p2">
<p class="ltx_p" id="S2.SS6.SSS1.p2.1">For all collected molecules, we removed stereochemistry, sanitized the molecules, and canonicalized the SMILES strings (<span class="ltx_text ltx_font_typewriter" id="S2.SS6.SSS1.p2.1.1">rdkit v2020.09.01</span>). We filtered out the molecules with canonical SMILES strings longer than 75 tokens and created the SELFIES strings for all retained molecules. Our data curation pipeline led to different distributions of molecular similarities between training and test set molecules for classification (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F2" title="Figure 2 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>a) and regression datasets (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F2" title="Figure 2 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>b).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.2 </span>Model Training and Optimization</h4>
<div class="ltx_para" id="S2.SS6.SSS2.p1">
<p class="ltx_p" id="S2.SS6.SSS2.p1.1">We tested all combinations of (a) model architectures (CNN, RNN, and Transformers), (b) molecular strings (SMILES and SELFIES), and encoding approaches (one-hot, random, and learnable) for all datasets. We optimized hyperparameters for each combination and each dataset separately (Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S2.T2" title="Table 2 ‣ 2.6.2 Model Training and Optimization ‣ 2.6 Experimental Setup ‣ 2 Methods ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>). A three-layer perceptron was used as a prediction module for consistency. Finally, XGBoost models<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib41" title="">41</a></sup></cite> were trained on extended connectivity fingerprints<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib39" title="">39</a></sup></cite> as baselines across all datasets. Early stopping with a patience of five epochs (or trees for XGBoost) and a tolerance of 10<sup class="ltx_sup" id="S2.SS6.SSS2.p1.1.1"><span class="ltx_text ltx_font_italic" id="S2.SS6.SSS2.p1.1.1.1">-5</span></sup> on validation loss were used. For classification models, we used loss re-weighting to tackle the data imbalance, which assigns the inverted frequency of classes as weights to molecules during loss computation. Finally, the best models were selected based on validation loss, <span class="ltx_text ltx_font_italic" id="S2.SS6.SSS2.p1.1.2">i.e., </span>cross-entropy and mean squared error for classification and regression, respectively.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span class="ltx_text ltx_font_italic" id="S2.T2.20.1">Model hyperparameters</span>. Grid search is used to optimize model hyperparameters. Learning rate of 10<sup class="ltx_sup" id="S2.T2.21.2"><span class="ltx_text ltx_font_italic" id="S2.T2.21.2.1">-2</span></sup> is used only for RNN to balance the number of experiments per architecture.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.17" style="width:433.6pt;height:583.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(89.7pt,-120.6pt) scale(1.70548373818016,1.70548373818016) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.17.15">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.17.15.16.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.16.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.17.15.16.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.16.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.17.15.16.1.2.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.16.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.17.15.16.1.3.1">Search Space</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.17.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.17.2.1">All</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.17.2.2">No. layers</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.17.2.3">1, 2, 3</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.18.3">
<td class="ltx_td" id="S2.T2.17.15.18.3.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.18.3.2">Dropout</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.18.3.3">0.25</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.19.4">
<td class="ltx_td" id="S2.T2.17.15.19.4.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.19.4.2">Batch size</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.19.4.3">32</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.20.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.20.5.1">CNN</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.20.5.2">No. filters</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.20.5.3">32, 64, 128</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.21.6">
<td class="ltx_td" id="S2.T2.17.15.21.6.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.21.6.2">Kernel length</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.21.6.3">3, 5, 7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.6.4.4">
<td class="ltx_td" id="S2.T2.6.4.4.5"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.6.4.4.6">Learning rate</td>
<td class="ltx_td ltx_align_left" id="S2.T2.6.4.4.4">10<sup class="ltx_sup" id="S2.T2.6.4.4.4.1"><span class="ltx_text ltx_font_italic" id="S2.T2.6.4.4.4.1.1">-2</span></sup>, 10<sup class="ltx_sup" id="S2.T2.6.4.4.4.2"><span class="ltx_text ltx_font_italic" id="S2.T2.6.4.4.4.2.1">-3</span></sup>, 5<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.5.3.3.3.m3.1"><semantics id="S2.T2.5.3.3.3.m3.1a"><mo id="S2.T2.5.3.3.3.m3.1.1" xref="S2.T2.5.3.3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.5.3.3.3.m3.1b"><times id="S2.T2.5.3.3.3.m3.1.1.cmml" xref="S2.T2.5.3.3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.3.3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.3.3.3.m3.1d">×</annotation></semantics></math>10<sup class="ltx_sup" id="S2.T2.6.4.4.4.3"><span class="ltx_text ltx_font_italic" id="S2.T2.6.4.4.4.3.1">-3</span></sup>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.9.7.7">
<td class="ltx_td" id="S2.T2.9.7.7.4"></td>
<td class="ltx_td" id="S2.T2.9.7.7.5"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.9.7.7.3">10<sup class="ltx_sup" id="S2.T2.9.7.7.3.1"><span class="ltx_text ltx_font_italic" id="S2.T2.9.7.7.3.1.1">-4</span></sup>, 5<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.8.6.6.2.m2.1"><semantics id="S2.T2.8.6.6.2.m2.1a"><mo id="S2.T2.8.6.6.2.m2.1.1" xref="S2.T2.8.6.6.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.6.6.2.m2.1b"><times id="S2.T2.8.6.6.2.m2.1.1.cmml" xref="S2.T2.8.6.6.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.6.6.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.6.6.2.m2.1d">×</annotation></semantics></math>10<sup class="ltx_sup" id="S2.T2.9.7.7.3.2"><span class="ltx_text ltx_font_italic" id="S2.T2.9.7.7.3.2.1">-5</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.22.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.22.7.1">RNN</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.22.7.2">Hidden state dim.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.22.7.3">16, 32, 64, 128</td>
</tr>
<tr class="ltx_tr" id="S2.T2.10.8.8">
<td class="ltx_td" id="S2.T2.10.8.8.2"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.10.8.8.3">Learning rate</td>
<td class="ltx_td ltx_align_left" id="S2.T2.10.8.8.1">10<sup class="ltx_sup" id="S2.T2.10.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.10.8.8.1.1.1">-2</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.23.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.23.8.1">Transformer</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.23.8.2">No. heads</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.23.8.3">1, 2, 4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.24.9">
<td class="ltx_td" id="S2.T2.17.15.24.9.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.24.9.2">MLP dim.</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.24.9.3">32, 64, 128</td>
</tr>
<tr class="ltx_tr" id="S2.T2.14.12.12">
<td class="ltx_td" id="S2.T2.14.12.12.5"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.14.12.12.6">Learning rate</td>
<td class="ltx_td ltx_align_left" id="S2.T2.14.12.12.4">10<sup class="ltx_sup" id="S2.T2.14.12.12.4.1"><span class="ltx_text ltx_font_italic" id="S2.T2.14.12.12.4.1.1">-2</span></sup>, 10<sup class="ltx_sup" id="S2.T2.14.12.12.4.2"><span class="ltx_text ltx_font_italic" id="S2.T2.14.12.12.4.2.1">-3</span></sup>, 5<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.13.11.11.3.m3.1"><semantics id="S2.T2.13.11.11.3.m3.1a"><mo id="S2.T2.13.11.11.3.m3.1.1" xref="S2.T2.13.11.11.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.13.11.11.3.m3.1b"><times id="S2.T2.13.11.11.3.m3.1.1.cmml" xref="S2.T2.13.11.11.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.11.11.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.13.11.11.3.m3.1d">×</annotation></semantics></math>10<sup class="ltx_sup" id="S2.T2.14.12.12.4.3"><span class="ltx_text ltx_font_italic" id="S2.T2.14.12.12.4.3.1">-3</span></sup>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.15">
<td class="ltx_td" id="S2.T2.17.15.15.4"></td>
<td class="ltx_td" id="S2.T2.17.15.15.5"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.15.3">10<sup class="ltx_sup" id="S2.T2.17.15.15.3.1"><span class="ltx_text ltx_font_italic" id="S2.T2.17.15.15.3.1.1">-4</span></sup>, 5<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.16.14.14.2.m2.1"><semantics id="S2.T2.16.14.14.2.m2.1a"><mo id="S2.T2.16.14.14.2.m2.1.1" xref="S2.T2.16.14.14.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.16.14.14.2.m2.1b"><times id="S2.T2.16.14.14.2.m2.1.1.cmml" xref="S2.T2.16.14.14.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.14.14.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.16.14.14.2.m2.1d">×</annotation></semantics></math>10<sup class="ltx_sup" id="S2.T2.17.15.15.3.2"><span class="ltx_text ltx_font_italic" id="S2.T2.17.15.15.3.2.1">-5</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.25.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.25.10.1">XGBoost</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.25.10.2">No. trees</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.17.15.25.10.3">2000</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.26.11">
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.26.11.1">(baseline)</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.26.11.2">Max. depth</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.26.11.3">3, 4, 5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.27.12">
<td class="ltx_td" id="S2.T2.17.15.27.12.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.27.12.2">Eta</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.27.12.3">0.01, 0.05, 0.1, 0.2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.28.13">
<td class="ltx_td" id="S2.T2.17.15.28.13.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.28.13.2">Column fraction</td>
<td class="ltx_td ltx_align_left" id="S2.T2.17.15.28.13.3">0.5, 0.75, 1.0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.17.15.29.14">
<td class="ltx_td ltx_border_b" id="S2.T2.17.15.29.14.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.17.15.29.14.2">Sample fraction</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.17.15.29.14.3">0.5, 0.75, 1.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S3.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_italic" id="S3.F2.4.1">Overview of dataset similarity and of model performance.</span> <span class="ltx_text ltx_font_bold" id="S3.F2.5.2">(a,b)</span> Distribution of test set similarities in comparison with training set molecules. The similarity was quantified as the Tanimoto coefficient on extended connectivity fingerprints<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib39" title="">39</a></sup></cite>, and the maximum similarity was reported. Different distributions can be observed in the classification (a) and regression (b) datasets, with the former containing more dissimilar molecules on average. <span class="ltx_text ltx_font_bold" id="S3.F2.6.3">(c,d)</span> Performance of neural network architectures across datasets. Bar plots indicate the mean test set performance (with error bars denoting the standard deviation), in comparison with the XGBoost baseline (dashed line: average performance, shaded area: standard deviation). Performance was quantified as balanced accuracy in classification (c), and as concordance index in regression (d).</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="598" id="S3.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_italic" id="S3.F3.2.1">Effect of input molecular strings and of token encoding strategies</span>. (a,b) Performance of SMILES and SELFIES representations on the model performance. Classification (a) and regression dataset (b) are analyzed separately. (c,d) Performance of token encoding strategies on classification (c) and regression (d). For all plots, bars indicate the mean performance on the test set of each notation, and error bars indicate the standard deviation. The performance of the XGBoost baseline is also indicated (dashed line: average; shaded area: standard deviation).</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Choosing a Neural Network Architecture</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Here, we aim to gather insights into the effect of the model architecture (CNN <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">vs</span> RNN <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">vs</span> Transformers) on the performance. To this end, we analyzed the best models per architecture (chosen on the validation set, and analyzed on the test set), regardless of the molecule representation and encoding strategies (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F2" title="Figure 2 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>c,d).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">CNNs were consistently the best-performing approach in classification. In regression, CNNs outperform the other approaches on two out of five targets. Transformers yielded the top-performing models in regression (three out of five datasets), while RNNs never yielded the best performance. A Wilcoxon signed-ranked test (<math alttext="\alpha=0.05" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><eq id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></eq><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝛼</ci><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.p2.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\alpha=0.05</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_α = 0.05</annotation></semantics></math>) on pooled scores across targets per task indicated that CNNs outperform both transformers and RNNs in classification, and RNNs in regression. No statistical differences were observed between CNNs and Transformers in regression.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Interestingly, CNN outperformed the XGBoost baseline in four of five classification datasets, where the test set molecules are structurally dissimilar to the training set (Tanimoto similarity on extended connectivity fingerprints lower than 0.5, Fig, <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F2" title="Figure 2 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>a). In regression, where the test set molecules are more similar to the training set (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F2" title="Figure 2 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>b), neither deep models nor XGBoost are statistically superior across the datasets. These results suggest that CLP approaches, and in particular, CNNs might have a higher potential than ‘traditional’ machine learning models when applied to molecules that are structurally diverse from the training set.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Hence, when considering their performance, architectural simplicity (compared to transformers) and training speed (compared to RNNs), convolutional neural networks constitute the ideal starting choice for chemical language processing and bioactivity prediction.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Representing and Encoding Molecular Structures</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_italic" id="S3.F4.3.1">Effect of loss re-weighting.</span> Comparison of the classification performance obtained with and without loss re-weighting (<span class="ltx_text ltx_font_italic" id="S3.F4.4.2">i.e., </span>assigning different weights to the molecules, as the inverse of their class frequency).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Here, we aimed to unveil the effect of the chosen molecular string representation (SMILES <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">vs</span> SELFIES) and token embedding (one-hot, random, and learnable) strategies. To this end, we compared the best models for each molecule representation and token encoding (minimum average error on the validation set). When investigating for practical guidelines, the differences are less evident than when choosing a neural network architecture (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F3" title="Figure 3 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">SMILES strings yield higher performance than SELFIES across classification tasks (p ¡ 0.05, Wilcoxon signed-ranked test). In regression, SELFIES outperform SMILES strings on two datasets (DRD3 and PIM1), and show similar performance otherwise, without statistically significant differences (Wilcoxon signed-ranked test, <math alttext="\alpha=0.05" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝛼</ci><cn id="S3.SS2.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p2.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\alpha=0.05</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_α = 0.05</annotation></semantics></math>). In general, the performance differences due to the chosen string notation are lower than those caused by the model architecture, with few exceptions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">When analyzing the encoding strategies, no approach consistently outperformed the others (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F3" title="Figure 3 ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">3</span></a>c,d), suggesting that all encoding approaches impact bioactivity prediction comparably. This underscores that, in the space of our design of experiments, choosing model architecture first, and then molecular string notations, should have higher priority than the encoding strategy.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">When considering these results, we recommend CLP hitchhikers <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib42" title="">42</a></sup></cite> to use SMILES strings combined with learnable encoding. SMILES strings are, in fact, ubiquitous in available databases, and numerous tools exist to process them (<span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.1">e.g., </span><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.2">rdkit</span>). This aspect makes SMILES strings easier to work with, with no loss in performance. Learnable representations are also simple to use, and are implemented in most major deep learning libraries (<span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.3">e.g., </span>Pytorch,<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib43" title="">43</a></sup></cite>, Tensorflow<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib44" title="">44</a></sup></cite>, and Keras<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib45" title="">45</a></sup></cite>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Other Tricks of the Trade</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">While the previous sections have tackled the most important algorithm-design choices in CLP, there are still many ‘bells and whistles’<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib22" title="">22</a></sup></cite> involved in obtaining predictive models. In what follows, we will focus on the loss function and hyperparameter optimization – both aspects impacting the effectiveness of the training process, and, ultimately, the model predictivity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.SS3.p2.1.1">Loss functions for imbalanced classes</span>. Class imbalance is common in bioactivity datasets <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib46" title="">46</a></sup></cite>, since desirable outcomes (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">e.g., </span>bioactive or non-toxic molecules) occur less frequently. Surprisingly, public bioactivity databases might be <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.3">unrealistically</span> imbalanced, with a lack of ‘negative’ data (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.4">e.g., </span>inactive molecules) due to reporting bias. Hence, mitigating the negative effects of class imbalance on the model performance is key for CLP hitchhikers <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib42" title="">42</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To mitigate class imbalance, in all the classification results shown so far, we applied loss re-weighting. We assigned a weight of 10 to the active molecules and of 1 to the inactive molecules (corresponding to the inverse of their respective class frequency). Loss re-weighting substantially increased balanced accuracy of 6% on average (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F4" title="Figure 4 ‣ 3.2 Representing and Encoding Molecular Structures ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>). In some extreme cases (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.1">i.e., </span>PIM1 and VDR) the lack of loss re-weighting led to a balanced accuracy of 0.5 (baseline-level performance). Class re-weighting is hence a simple and effective strategy that we recommend, among other options<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib47" title="">47</a></sup></cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="673" id="S3.F5.g1" src="x5.png" width="490"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_italic" id="S3.F5.2.1">Hyperparameter tuning.</span> (a-d) Most frequently occurring hyperparameter values among the top-ten models per dataset (CNN architecture, with SMILES strings and learnable embeddings). The following parameters were investigated: number of convolution layers (a), kernel length (b), number of filters (c), and token embedding dimension (d). (e,f) Model performance vs. explored hyperparameter space size. Performance of progressively subsampled models from 1 to 432 hyperparameter configurations (total) for both classification (e) and regression (f). The dashed line indicates 50% of models being explored.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.SS3.p4.1.1">Optimal hyperparameters</span>. Hyperparameter optimization can be a demanding task due to the high number of hyperparameters to explore and required domain expertise. To equip CLP practitioners with guidelines, we focused on our recommended setup (CNNs trained on SMILES strings with learnable embedding), and inspected the top-10 performing models (the test set average) for the following hyperparameters (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F5" title="Figure 5 ‣ 3.3 Other Tricks of the Trade ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>): (a) <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2">number of convolution layers</span>, impacting the network depth and complexity, (b) <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.3">kernel length</span>, controlling the size of learned patterns, (c) <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.4">number of convolution filters</span>, controlling information compression across layers, and (d) <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.5">token embedding dimension</span>, controlling the size of the latent representations learned.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">The best-performing models tend to have a low number of layers, with one being the most prevalent (seven out of ten datasets, and 67% occurrence, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F5" title="Figure 5 ‣ 3.3 Other Tricks of the Trade ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>a). Optimal kernel size and number of filters (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F5" title="Figure 5 ‣ 3.3 Other Tricks of the Trade ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>b,c) results are dataset dependent. Finally, embeddings of 32 or higher dimensions are preferred (84% of cases, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F5" title="Figure 5 ‣ 3.3 Other Tricks of the Trade ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>d). These results offer indications for hyperparameter prioritization ‘on a budget’, although we recommend conducting extensive searches whenever feasible.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.SS3.p6.1.1">Exploring the hyperparameter galaxy</span>. To provide guidelines for parsimonious hyperparameter optimization, we randomly sampled an increasing number of models from the hyperparameter space (from 1 to the total, 432), and analyzed the performance of the top-ten models (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#S3.F5" title="Figure 5 ‣ 3.3 Other Tricks of the Trade ‣ 3 Results ‣ A Hitchhiker’s Guide to Deep Chemical Language Processing for Bioactivity Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>e,f). Performance often plateaued before reaching 100 models, with a shrink in its variability when half of the space was explored. These findings indicate that defining a high-dimensional hyperparameter space can be better than relying on a narrow one, and that randomly exploring half of the grid can be sufficient to reach the maximum performance level possible in that space.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>So Long, and Thanks for All the Data</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Casting molecular tasks as chemical language processing has achieved enormous success in the molecular sciences<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib17" title="">17</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib11" title="">11</a></sup></cite>, owed to a unique combination of simplicity (<span class="ltx_text ltx_font_italic" id="S4.p1.1.1">e.g., </span>in representing and processing molecules as strings) and performance <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib48" title="">48</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib49" title="">49</a></sup></cite>. The importance of chemical language processing is hence only expected to increase. To accelerate the adoption of CLP approaches by novices and experts alike, these are our guidelines for hitchhikers<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib42" title="">42</a></sup></cite>, based on the data we have collected:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">‘KISS: Keep It Simple, Silly!’</span> Convolutional neural networks – an architecture that is simpler than the Transformer and faster than Recurrent Neural Networks – yielded the best performance overall, and are recommended as the first choice. Since representation and encoding strategies minimally affected performance, we recommend using SMILES strings for their ubiquity in databases and software, and learnable embeddings for existing implementations in most deep learning packages.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">‘Cut your losses’</span>. Molecular bioactivity datasets are inherently imbalanced <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib46" title="">46</a></sup></cite>, and the ‘losses’ due to such imbalance should be minimized to ensure predictivity <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib50" title="">50</a></sup></cite>. We recommend loss re-weighting as a simple and yet effective strategy to increase model performance.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">‘<span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Cast a wide fishing net</span>’. Hyperparameter optimization can be computationally demanding. Here, we show that, in general, networks with a low (one to two) number of layers tend to perform well enough, while other hyperparameter choices depend on the dataset. In general, once a hyperparameter space is defined, optimal hyperparameters are likely to be found by exploring half of the possible combinations. Hence, we recommend casting a broad (rather than a narrow) hyperparameter grid for exploration, and refine the hyperparameter values at a later stage.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Several other fascinating properties of the ‘chemical language’ can further the potential of CLP approaches. One of them is molecular string augmentation<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib10" title="">10</a></sup></cite>, where multiple molecular strings can be used to represent the same molecule, <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">e.g., </span>to increase the number of data available for training <cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib51" title="">51</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib52" title="">52</a></sup></cite>, or for uncertainty estimation<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib18" title="">18</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib53" title="">53</a></sup></cite>. Moreover, transfer learning<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib54" title="">54</a></sup></cite> can be particularly effective on molecular strings<cite class="ltx_cite ltx_citemacro_citep"><sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib55" title="">55</a></sup>, <sup class="ltx_sup"><a class="ltx_ref" href="https://arxiv.org/html/2407.12152v1#bib.bib19" title="">19</a></sup></cite>, <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">e.g., </span>to mitigate the limited data availability on a specific target. We encourage ‘CLP hitchhikers’ to venture forth into such elements and assess their effectiveness on a case-by-case basis.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Author contributions</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text ltx_font_italic" id="Sx1.p1.1.1">Conceptualization</span>: both authors. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.2">Data curation</span>: RÖ. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.3">Formal Analysis</span>: both authors. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.4">Investigation</span>: both authors. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.5">Methodology</span>: both authors. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.6">Software</span>: RÖ. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.7">Visualization</span>: RÖ. <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.8">Writing – original draft</span>: RÖ.
<span class="ltx_text ltx_font_italic" id="Sx1.p1.1.9">Writing – review and editing</span>: both authors.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This research was co-funded by the European Union (ERC, ReMINDER, 101077879). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. The authors also acknowledge support from the Irene Curie Fellowship and the Centre for Living Technologies.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran, G. Lee, B. Li, A. Madabhushi, P. Shah, M. Spitzer, <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">et al.</span>, “Applications of machine learning in drug discovery and development,” <span class="ltx_text ltx_font_italic" id="bib.bib1.2.2">Nature reviews Drug discovery</span>, vol. 18, no. 6, pp. 463–477, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. Özçelik, D. van Tilborg, J. Jiménez-Luna, and F. Grisoni, “Structure-based drug discovery with deep learning,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">ChemBioChem</span>, vol. 24, no. 13, p. e202200776, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Chakraborty and Y. Hasija, “Utilizing deep learning to explore chemical space for drug lead optimization,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Expert Systems with Applications</span>, vol. 229, p. 120592, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia, C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackermann, <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">et al.</span>, “A deep learning approach to antibiotic discovery,” <span class="ltx_text ltx_font_italic" id="bib.bib4.2.2">Cell</span>, vol. 180, no. 4, pp. 688–702, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. van Tilborg, H. Brinkmann, E. Criscuolo, L. Rossen, R. Özçelik, and F. Grisoni, “Deep learning for low-data drug discovery: hurdles and opportunities,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Current Opinion in Structural Biology</span>, vol. 86, p. 102818, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Weininger, “Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Journal of chemical information and computer sciences</span>, vol. 28, no. 1, pp. 31–36, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Krenn, F. Häse, A. Nigam, P. Friederich, and A. Aspuru-Guzik, “Self-referencing embedded strings (selfies): A 100% robust molecular string representation,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Machine Learning: Science and Technology</span>, vol. 1, no. 4, p. 045024, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Öztürk, A. Özgür, and E. Ozkirimli, “Deepdta: deep drug–target binding affinity prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Bioinformatics</span>, vol. 34, no. 17, pp. i821–i829, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Q. Zhao, G. Duan, M. Yang, Z. Cheng, Y. Li, and J. Wang, “Attentiondta: Drug–target binding affinity prediction by sequence-based deep learning with attention mechanism,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE/ACM transactions on computational biology and bioinformatics</span>, vol. 20, no. 2, pp. 852–863, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
E. J. Bjerrum, “Smiles enumeration as data augmentation for neural network modeling of molecules,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1703.07076</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Öztürk, A. Özgür, P. Schwaller, T. Laino, and E. Ozkirimli, “Exploring chemical space using natural language processing methodologies for drug discovery,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Drug Discovery Today</span>, vol. 25, no. 4, pp. 689–705, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Ross, B. Belgodere, V. Chenthamarakshan, I. Padhi, Y. Mroueh, and P. Das, “Large-scale chemical language representations capture molecular structure and properties,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Nature Machine Intelligence</span>, vol. 4, no. 12, pp. 1256–1264, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N. O’Boyle and A. Dalke, “Deepsmiles: an adaptation of smiles for use in machine-learning of chemical structures,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">ChemRxiv</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.-N. Wu, T. Wang, Y. Chen, L.-J. Tang, H.-L. Wu, and R.-Q. Yu, “t-smiles: a fragment-based molecular representation framework for de novo ligand design,” <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Nature Communications</span>, vol. 15, no. 1, p. 4993, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. R. Heller, A. McNaught, I. Pletnev, S. Stein, and D. Tchekhovskoi, “Inchi, the iupac international chemical identifier,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Journal of cheminformatics</span>, vol. 7, pp. 1–34, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Noutahi, C. Gabellini, M. Craig, J. S. Lim, and P. Tossou, “Gotta be safe: a new framework for molecular design,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Digital Discovery</span>, vol. 3, no. 4, pp. 796–804, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F. Grisoni, “Chemical language models for de novo drug design: Challenges and opportunities,” <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Current Opinion in Structural Biology</span>, vol. 79, p. 102527, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. B. Kimber, M. Gagnebin, and A. Volkamer, “Maxsmi: maximizing molecular property prediction performance with confidence estimation using smiles augmentation and deep learning,” <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Artificial Intelligence in the Life Sciences</span>, vol. 1, p. 100014, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Moret, I. Pachon Angona, L. Cotos, S. Yan, K. Atz, C. Brunner, M. Baumgartner, F. Grisoni, and G. Schneider, “Leveraging molecular structure and bioactivity with chemical language models for de novo drug design,” <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Nature Communications</span>, vol. 14, no. 1, p. 114, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. van Tilborg, A. Alenicheva, and F. Grisoni, “Exposing the limitations of molecular machine learning with activity cliffs,” <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Journal of Chemical Information and Modeling</span>, vol. 62, no. 23, pp. 5938–5951, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Zhou, S. Cahya, S. A. Combs, C. A. Nicolaou, J. Wang, P. V. Desai, and J. Shen, “Exploring tunable hyperparameters for deep neural networks with industrial adme data sets,” <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Journal of chemical information and modeling</span>, vol. 59, no. 3, pp. 1005–1016, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Bengio, “Practical recommendations for gradient-based training of deep architectures,” in <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Neural networks: Tricks of the trade: Second edition</span>, pp. 437–478, Springer, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Özçelik, H. Öztürk, A. Özgür, and E. Ozkirimli, “Chemboost: A chemical language based approach for protein–ligand binding affinity prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Molecular Informatics</span>, vol. 40, no. 5, p. 2000212, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Sharma, R. Kumar, S. Ranjta, and P. K. Varadwaj, “Smiles to smell: decoding the structure–odor relationship of chemical compounds using the deep neural network approach,” <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Journal of Chemical Information and Modeling</span>, vol. 61, no. 2, pp. 676–688, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C.-K. Wu, X.-C. Zhang, Z.-J. Yang, A.-P. Lu, T.-J. Hou, and D.-S. Cao, “Learning to smiles: Ban-based strategies to improve latent representation learning from molecules,” <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Briefings in bioinformatics</span>, vol. 22, no. 6, p. bbab327, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Nigam, R. Pollice, M. Krenn, G. dos Passos Gomes, and A. Aspuru-Guzik, “Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (stoned) algorithm for molecules using selfies,” <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Chemical science</span>, vol. 12, no. 20, pp. 7079–7090, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Choi, S. Seo, S. Choi, S. Piao, C. Park, S. J. Ryu, B. J. Kim, and S. Park, “Rebadd-se: Multi-objective molecular optimisation using selfies fragment and off-policy self-critical sequence training,” <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Computers in Biology and Medicine</span>, vol. 157, p. 106721, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Krenn, Q. Ai, S. Barthel, N. Carson, A. Frei, N. C. Frey, P. Friederich, T. Gaudin, A. A. Gayle, K. M. Jablonka, <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">et al.</span>, “Selfies and the future of molecular string representations,” <span class="ltx_text ltx_font_italic" id="bib.bib28.2.2">Patterns</span>, vol. 3, no. 10, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Yüksel, E. Ulusoy, A. Ünlü, and T. Doğan, “Selformer: molecular representation learning via selfies language models,” <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Machine Learning: Science and Technology</span>, vol. 4, no. 2, p. 025035, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Feng, Y. Zhang, Z. Deng, and M. Xiong, “Gcardti: Drug–target interaction prediction based on a hybrid mechanism in drug selfies,” <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Quantitative Biology</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Sun, N. Jeliazkova, V. Chupakhin, J.-F. Golib-Dzib, O. Engkvist, L. Carlsson, J. Wegner, H. Ceulemans, I. Georgiev, V. Jeliazkov, <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">et al.</span>, “Excape-db: an integrated large scale dataset facilitating big data analysis in chemogenomics,” <span class="ltx_text ltx_font_italic" id="bib.bib31.2.2">Journal of cheminformatics</span>, vol. 9, pp. 1–9, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the IEEE</span>, vol. 86, no. 11, pp. 2278–2324, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. J. Hopfield, “Neural networks and physical systems with emergent collective computational abilities.,” <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the national academy of sciences</span>, vol. 79, no. 8, pp. 2554–2558, 1982.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN encoder–decoder for statistical machine translation,” in <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span> (A. Moschitti, B. Pang, and W. Daelemans, eds.), (Doha, Qatar), pp. 1724–1734, Association for Computational Linguistics, Oct. 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</span>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Gaulton, A. Hersey, M. Nowotka, A. P. Bento, J. Chambers, D. Mendez, P. Mutowo, F. Atkinson, L. J. Bellis, E. Cibrián-Uhalte, <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">et al.</span>, “The chembl database in 2017,” <span class="ltx_text ltx_font_italic" id="bib.bib36.2.2">Nucleic acids research</span>, vol. 45, no. D1, pp. D945–D954, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Gönen and G. Heller, “Concordance probability and discriminatory power in proportional hazards regression,” <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Biometrika</span>, vol. 92, no. 4, pp. 965–970, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. Pahikkala, A. Airola, S. Pietilä, S. Shakyawar, A. Szwajda, J. Tang, and T. Aittokallio, “Toward more realistic drug–target interaction predictions,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Briefings in bioinformatics</span>, p. bbu010, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
D. Rogers and M. Hahn, “Extended-connectivity fingerprints,” <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Journal of chemical information and modeling</span>, vol. 50, no. 5, pp. 742–754, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
B. Ramsundar, P. Eastman, P. Walters, V. Pande, K. Leswing, and Z. Wu, <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Deep Learning for the Life Sciences</span>.

</span>
<span class="ltx_bibblock">O’Reilly Media, 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837" title="">https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,” in <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</span>, pp. 785–794, ACM, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
The Answer to the Ultimate Question of Life, the Universe, and Everything.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">et al.</span>, “Pytorch: An imperative style, high-performance deep learning library,” <span class="ltx_text ltx_font_italic" id="bib.bib43.2.2">Advances in neural information processing systems</span>, vol. 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015.

</span>
<span class="ltx_bibblock">Software available from tensorflow.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
F. Chollet, “keras.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/fchollet/keras" title="">https://github.com/fchollet/keras</a>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Volkamer, S. Riniker, E. Nittinger, J. Lanini, F. Grisoni, E. Evertsson, R. Rodríguez-Pérez, and N. Schneider, “Machine learning for small molecule drug discovery in academia and industry,” <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Artificial Intelligence in the Life Sciences</span>, vol. 3, p. 100056, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Q. Wang, Y. Ma, K. Zhao, and Y. Tian, “A comprehensive survey of loss functions in machine learning,” <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Annals of Data Science</span>, pp. 1–26, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. Flam-Shepherd, K. Zhu, and A. Aspuru-Guzik, “Language models can learn complex molecular distributions,” <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Nature Communications</span>, vol. 13, no. 1, p. 3293, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
H. Öztürk, E. Ozkirimli, and A. Özgür, “A comparative study of smiles-based compound similarity functions for drug-target interaction prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">BMC bioinformatics</span>, vol. 17, pp. 1–11, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A. Fernández, S. García, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera, <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Learning from imbalanced data sets</span>, vol. 10.

</span>
<span class="ltx_bibblock">Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
C. Li, J. Feng, S. Liu, and J. Yao, “A novel molecular representation learning for molecular property prediction with a multiple smiles-based augmentation,” <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Computational Intelligence and Neuroscience</span>, vol. 2022, no. 1, p. 8464452, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
T. B. Kimber, S. Engelke, I. V. Tetko, E. Bruno, and G. Godin, “Synergy effect between convolutional neural networks and the multiplicity of smiles for improvement of molecular prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:1812.04439</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R. Birolo, R. Özçelik, A. Aramini, R. Gobetto, M. R. Chierotti, and F. Grisoni, “Deep supramolecular language processing for co-crystal prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">ChemRxiv</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
C. Cai, S. Wang, Y. Xu, W. Zhang, K. Tang, Q. Ouyang, L. Lai, and J. Pei, “Transfer learning for drug discovery,” <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Journal of Medicinal Chemistry</span>, vol. 63, no. 16, pp. 8683–8694, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
G. Uludoğan, E. Ozkirimli, K. O. Ulgen, N. Karalı, and A. Özgür, “Exploiting pretrained biochemical language models for targeted drug design,” <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Bioinformatics</span>, vol. 38, no. Supplement_2, pp. ii155–ii161, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 16 20:01:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
