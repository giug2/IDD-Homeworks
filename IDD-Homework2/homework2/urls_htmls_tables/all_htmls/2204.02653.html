<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.02653] Using Synthetic Data for Conversational Response Generation in Low-resource Settings</title><meta property="og:description" content="Response generation is a task in natural language processing (NLP) where a model is trained to respond to human statements. Conversational response generators take this one step further with the ability to respond with…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Using Synthetic Data for Conversational Response Generation in Low-resource Settings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Using Synthetic Data for Conversational Response Generation in Low-resource Settings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.02653">

<!--Generated on Mon Mar 11 12:04:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Using Synthetic Data for Conversational Response Generation in Low-resource Settings</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Response generation is a task in natural language processing (NLP) where a model is trained to respond to human statements. Conversational response generators take this one step further with the ability to respond within the context of previous responses. While there are existing techniques for training such models, they all require an abundance of conversational data which are not always available for low-resource languages. In this research, we make three contributions. First, we released the first Filipino conversational dataset collected from a popular Philippine online forum, which we named the “PEx Conversations Dataset”. Second, we introduce a data augmentation (DA) methodology for Filipino data by employing a Tagalog RoBERTa model to increase the size of the existing corpora. Lastly, we published the first Filipino conversational response generator capable of generating responses related to the previous 3 responses. With the supplementary synthetic data, we were able to improve the performance of the response generator by up to 12.2% in BERTScore, 10.7% in perplexity, and 11.7% in content word usage as compared to training with zero synthetic data.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>Response Generation, Transformers, Conversational Models</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\newcites</span>
<p id="p1.2" class="ltx_p">languageresourceLanguage Resources



</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Using Synthetic Data for Conversational Response Generation in Low-resource Settings</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Gabriel Louis Tan, Adrian Paule Ty, Schuyler Ng, Denzel Adrian Co</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Jan Christian Blaise Cruz <span id="id1.p1.2.2.2.1.1.1" class="ltx_text ltx_font_medium">and</span> Charibeth Cheng</span></td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Center for Language Technologies (CeLT), De La Salle University</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">2401 Taft Ave., Malate, Manila, Philippines</td>
</tr>
<tr id="id1.p1.2.5.5" class="ltx_tr">
<td id="id1.p1.2.5.5.1" class="ltx_td ltx_align_center">{gabriel_louis_tan, adrian_ty, schuyler_ng, denzel_adrian_co, jan_christian_cruz, charibeth.cheng}@dlsu.edu.ph</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Existing language models with real life applications require huge amounts of data to produce favorable results or an improvement in performance. A customer service chatbot trained a Seq2Seq model with long short-term memory networks on over 1M Twitter brand accounts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Xu et al., 2017</a>]</cite>; a commercial artificial intelligence by Amazon, Alexa, was pretrained on 4M utterances with each domain fine-tuned with 50K grammar utterances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Goyal et al., 2018</a>]</cite>. The assumption of these extensive resources poses challenges for low-resource languages with little to no existing dialogue corpora.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">As opposed to attempting to construct extensive conversational datasets, data augmentation (DA) techniques can be used to alleviate the deficiency by generating synthetic data using pre-existing data. In NLP, one prevailing technique is word replacement where a selected word can be replaced with synonymous words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Wei and Zou, 2019</a>]</cite>, morphologically similar words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Vania et al., 2019</a>]</cite>, or entities of the same type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Dai and Adel, 2020</a>]</cite>. Another technique is modifying instances by using correct sentences and applying a set of transformations that introduce errors for grammar correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Grundkiewicz et al., 2019</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In response generation, the work of <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">?</span>) shows that question answering systems can be successfully trained entirely by using synthetic data using an approach that involves generating questions and answers from a GPT-2 model. <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">?</span>) explored fluent response generation to conversational questions which utilized Syntactic Transformations on the SQuAD 2.0 QA dataset to extract supervised conversational training data for response generation models based on Pointer-Generator Networks (PGN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">See et al., 2017</a>]</cite> and DialoGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Zhang et al., 2019</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">These previous works show that models perform better with large datasets, and even when they lack the resources, synthetic data allows them to perform just as well. However, these assume the existence of these resources in the targeted language, whether it be proven methods or datasets. All works mentioned using data augmentation primarily used English sources. This makes it difficult to adapt these techniques into low-resource languages like Filipino.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In this work, we constructed the first Filipino conversational dataset from an online Filipino forum website called Pinoy Exchange (PEx). With this dataset, we generated synthetic data through RoBERTa-based data augmentation, experimenting with the quantity of token replacements in an utterance. Then with varying experimental setups involving the token replacement percentage and simulated low data sizes, we fine-tune a DialoGPT model for a conversational response generator. We benchmark our models using perplexity, content word usage, and BERTScore, an automatic evaluation metric that favors contextual embedding matching over string matching.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Filipino Conversational Dataset</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">In order to alleviate the absence of conversational data in Filipino, we produce the PinoyExchange Conversations Dataset or ”PEx Conversations”.
The dataset consists of 2,424,748 comments across 45,621 threads, producing a total of 2,171,579 conversation-like exchanges.
</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Conversational data was extracted from PinoyExchange <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.pinoyexchange.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.pinoyexchange.com/</a></span></span></span> (PEx), an online forum mainly used by Filipinos with topics concerning the Philippines or Filipinos in general. The dataset is primarily in casual Filipino, with the addition of some English words commonly used in daily Filipino conversations.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">We construct the dataset by scraping the Small Talk subforum and the different subforums within the lifestyle category namely Food and Drinks, Home and Garden, Style and Fashion, Travel and Leisure, Visas and Immigration, Health and Wellness, and Body and Fitness. All unicode errors are resolved. Media-related information, links, account tags, emojis, repetitive punctuations, and thread-related nuances (i.e. referencing) were removed. Following <span id="S2.p3.1.1" class="ltx_text ltx_font_bold">?</span>), Further cleaning methods such as decapitalization, removal of stopwords, stemming and lemmatization, and spelling and grammar correction were not performed to preserve the conversational nuances.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.1.   RoBERTa-based Data Augmentation</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Given that other existing techniques require external resources such as WordNet, <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">?</span>) introduces contextual augmentation, a technique which uses bi-directional language models to consider context in order to better predict words for word replacement tasks. Due to its memory capabilities for long term dependencies, BERT has been explored as a viable option for data augmentation as it is frankly more powerful than ordinary bidirectional language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Wu et al., 2018</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Bidirectional Encoder Representations from Transformers (BERT)
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/google-research/bert" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google-research/bert</a></span></span></span> is a multi-layer bidirectional Transformer encoder that is designed to pretrain deep bidirectional representations that can be fine-tuned to a wide range of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Devlin et al., 2019</a>]</cite>. BERT is pretrained on the masked language modelling (MLM) task and the next sentence prediction (NSP) task. The model’s capability to contextually-fill masked tokens opens an opportunity for the creation of synthetic data.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">RoBERTa.</span> <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_bold">?</span>) found that the original BERT pretraining procedure was significantly undertrained and proposed a tweaked approach of training BERT models that improve end-task performance, which they referred to as Robustly optimized BERT approach (RoBERTa). RoBERTa was trained longer with dynamic masking, sentences without NSP loss, larger batches, and a larger byte-level Byte-Pair Encoding (BPE) and achieved state-of-the-art results on the GLUE, RACE, and SQuAD datasets.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.2.   DialoGPT</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">DialoGPT is a response generation model based on the GPT-2 transformer architecture which uses masked self-attention and feedforward neural networks to predict the probability of the next output token. DialoGPT is trained using a 147M multi-turn dialogue dataset scraped from Reddit discussion threads, leveraging a stack of masked multi-head self-attention layers in generating realistic responses. Specifically, DialoGPT inherits the transformer with layer normalization, the same initialization scheme but modified, and the same BPE for the tokenizer. Although the model is initially trained on an English corpora, DialoGPT is an open-domain pretrained model that can be further fine-tuned on a language-specific dataset to obtain a custom conversational model in any language such as Spanish <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://huggingface.co/ncoop57/DiGPTame-medium" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/ncoop57/DiGPTame-medium</a></span></span></span>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.3.   Evaluation Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Perplexity is a simple but powerful metric, a standard in most NLP tasks. It is the inverse probability of the test set, normalized by the number of words. It analyzes the probability of n-grams (product across all consecutive n-grams) appearing as the model learned in the dataset. Perplexity is normalized so that it does not rely on the size of the dataset, to give a more accurate per-word measure. Perplexity has been shown to correlate very well with how humans perceive coherent and natural conversations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Adiwardana et al., 2020</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">BERTScore is a text generation evaluation metric introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Zhang et al., 2020</a>]</cite>. The automatic evaluation metric computes for the similarity score for each token matched in the reference set, similar to common n-gram matching metrics such as BLEU, METEOR, and NIST. However, it has been proven that these common metrics are restrictive with regards to matching words with multiple meanings due to the popular approach of string-level matching. BERTScore addresses the issue by computing the similarity score based on BERT’s contextual embeddings. Contextual embeddings are able to preserve the meanings of similarly-spelled words in varied contexts, and have been shown to work for tasks such as paraphrase detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Devlin et al., 2019</a>]</cite>. The sum of cosine similarities between token embeddings is computed for the similarity score between two sequences. Additionally, BERTScore has also been found to correlate better with human evaluators as compared to the standard metrics.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Generally, data augmentation in NLP is not entirely limited with producing very similar, and even context-relevant, responses as DA also has the potential to produce semantically-correct synthetic sequences that are contextually-distant to that of the original sequence. Hence, we introduce a third metric involving identifying the quantity of function words versus content words present in the response. Particularly, an increase in content words are theorized to be indicative of additions of detail in the generation. Function words traditionally refer to the articles, prepositions, pronouns, conjunction and other parts of speech with specific syntactic usage in constructing sentences, whereas content words are the nouns, verbs, adjectives, and some adverbs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Miller et al., 1958</a>]</cite>. Analyzing large amounts of text in the same study, the average character length of a function word was identified to be at 3.13 characters, while content words had an average of 6.47 characters. With this in mind, tokens with 1-3 characters were categorized as function words and tokens with 4-15 characters were categorized as content words. Tokens that are solely made up of punctuation were not included during processing. The slight adjustment of character counts was done to account for the lengthier Tagalog words observed in the dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Data Preparation</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">PEx data was used in both finetuning a Tagalog RoBERTa model for DA and the DialoGPT model. We used a 97%-3% split for DialoGPT and RoBERTa respectively. To produce the splits, we load the full dataset into Pandas and set the random seed to 42.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">Conversation-like exchanges were derived by extracting depth-first chains of the structure topic-reply-reply in the recursive structure, creating conversations. 5% from the DialoGPT split was taken to serve as the RoBERTa evalaution set. The remaining was used in the data augmentation to produce the synthetic data, which was then also used in conjunction with the original set when training the DialoGPT model. This set was split into 80%-20% for the training set and test set. The training set goes through the data augmentation process for generation of synthetic data while the test set remains untouched.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Data Augmentation</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We fine-tuned a RoBERTa model in order to capture the nuances of casual written Filipino present in the dataset. We used Tagalog RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Cruz and Cheng, 2021</a>]</cite> then fine-tuned it with 3% of the PEx dataset split into an 80%-20% test and validate set, with a split random seed of 42. We fine-tuned RoBERTa for 3 epochs, using a batch size of 8, and a learning rate of 5e-5.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">To increase the size of the existing corpora, the flattened conversations from preprocessing are separated into independent utterances. This will allow our Tagalog RoBERTa model to perform mask-fill on a per-response basis over a per-conversation basis.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">A percentage is indicated for augmentation in order to limit the amount of original tokens that are replaced with RoBERTa-predicted tokens. To ensure that the amount of token replacements scales according to the sequence length of each utterance, the specified percentage is multiplied with each sequence length where a ceiling function is used to return the smallest succeeding integer (i.e. 15% of 10 tokens equate to 2 tokens to be replaced).</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.2" class="ltx_p">This predetermined amount was used to randomly select <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">n</annotation></semantics></math> amount of indices corresponding to the list of tokens. In a sequential manner, the token in each selected index in the original sequence was replaced with a <span id="S5.p4.2.1" class="ltx_text ltx_font_typewriter">&lt;mask&gt;</span> token. The altered sequence was then fed to the fine-tuned RoBERTa model in order to predict a replacement in place of the mask token, choosing the highest scoring prediction on every repetition. The process of masking and filling was iteratively performed until all <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">n</annotation></semantics></math> amount of indices are consumed. The RoBERTa-predicted tokens were used to replace their corresponding tokens in the original sequence, hereby forming the synthetic utterance.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p">Each synthetic utterance was then merged along with the other generated utterances to form a synthetic conversation, effectively doubling the training corpus. Both the synthetic conversations were compiled with the original conversations to form a 50/50 dataset for finetuning DialoGPT.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Response Generation Model</h2>

<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T1.1.1.1" class="ltx_tr">
<td id="S6.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">%</span></td>
<td id="S6.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Data Size</span></td>
<td id="S6.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Avg Loss</span></td>
<td id="S6.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Training Time in Hrs</span></td>
<td id="S6.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Prplx</span></td>
</tr>
<tr id="S6.T1.1.2.2" class="ltx_tr">
<td id="S6.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">0%</td>
<td id="S6.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52,640</td>
<td id="S6.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.5436</td>
<td id="S6.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.75</td>
<td id="S6.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.1206</td>
</tr>
<tr id="S6.T1.1.3.3" class="ltx_tr">
<td id="S6.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">10%</td>
<td id="S6.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">105,280</td>
<td id="S6.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.5306</td>
<td id="S6.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.2</td>
<td id="S6.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.6771</td>
</tr>
<tr id="S6.T1.1.4.4" class="ltx_tr">
<td id="S6.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">15%</td>
<td id="S6.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">105,280</td>
<td id="S6.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6059</td>
<td id="S6.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18</td>
<td id="S6.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.7867</td>
</tr>
<tr id="S6.T1.1.5.5" class="ltx_tr">
<td id="S6.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">20%</td>
<td id="S6.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">105,280</td>
<td id="S6.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6646</td>
<td id="S6.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.2</td>
<td id="S6.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.8593</td>
</tr>
<tr id="S6.T1.1.6.6" class="ltx_tr">
<td id="S6.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">25%</td>
<td id="S6.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">105,280</td>
<td id="S6.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.7156</td>
<td id="S6.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.8</td>
<td id="S6.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.9142</td>
</tr>
<tr id="S6.T1.1.7.7" class="ltx_tr">
<td id="S6.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">30%</td>
<td id="S6.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">105,280</td>
<td id="S6.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.7610</td>
<td id="S6.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17.8</td>
<td id="S6.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3.9574</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Results of fine-tuning models with varying percentages of token replacement</figcaption>
</figure>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.1.1.1" class="ltx_tr">
<td id="S6.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S6.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Data Size</td>
<td id="S6.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Avg Loss</td>
<td id="S6.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training Time in Hrs</td>
<td id="S6.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Prplx</td>
</tr>
<tr id="S6.T2.1.2.2" class="ltx_tr">
<td id="S6.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1K Base</td>
<td id="S6.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,000</td>
<td id="S6.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.4788</td>
<td id="S6.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.17</td>
<td id="S6.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.5821</td>
</tr>
<tr id="S6.T2.1.3.3" class="ltx_tr">
<td id="S6.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1K Aug</td>
<td id="S6.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,000</td>
<td id="S6.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.3005</td>
<td id="S6.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S6.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.4791</td>
</tr>
<tr id="S6.T2.1.4.4" class="ltx_tr">
<td id="S6.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">10K Base</td>
<td id="S6.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10,000</td>
<td id="S6.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.8721</td>
<td id="S6.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.6</td>
<td id="S6.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6.1937</td>
</tr>
<tr id="S6.T2.1.5.5" class="ltx_tr">
<td id="S6.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">10K Aug</td>
<td id="S6.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20,000</td>
<td id="S6.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8311</td>
<td id="S6.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.4</td>
<td id="S6.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.755</td>
</tr>
<tr id="S6.T2.1.6.6" class="ltx_tr">
<td id="S6.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">25K Base</td>
<td id="S6.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25,000</td>
<td id="S6.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.7009</td>
<td id="S6.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.3</td>
<td id="S6.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.9905</td>
</tr>
<tr id="S6.T2.1.7.7" class="ltx_tr">
<td id="S6.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">25K Aug</td>
<td id="S6.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">50,000</td>
<td id="S6.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.6780</td>
<td id="S6.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">9.16</td>
<td id="S6.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4.5684</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Results of fine-tuning models in varying training data sizes</figcaption>
</figure>
<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Our model of choice for all our experiments is DialoGPT-medium (345M). We fine-tune DialoGPT for 5 epochs, using a batch size of 4, and a learning rate of 5e-5.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.9" class="ltx_p">Finetuning DialoGPT requires a dataset where each conversation has an equal length of exchanges. This essentially means that it needs to learn that the latest, or <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.p2.1.m1.1a"><mi id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">n</annotation></semantics></math>th turn in that exchange takes into account the previous <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="n-1" display="inline"><semantics id="S6.p2.2.m2.1a"><mrow id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mi id="S6.p2.2.m2.1.1.2" xref="S6.p2.2.m2.1.1.2.cmml">n</mi><mo id="S6.p2.2.m2.1.1.1" xref="S6.p2.2.m2.1.1.1.cmml">−</mo><mn id="S6.p2.2.m2.1.1.3" xref="S6.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><minus id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1"></minus><ci id="S6.p2.2.m2.1.1.2.cmml" xref="S6.p2.2.m2.1.1.2">𝑛</ci><cn type="integer" id="S6.p2.2.m2.1.1.3.cmml" xref="S6.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">n-1</annotation></semantics></math> turns for its <math id="S6.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.p2.3.m3.1a"><mi id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><ci id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">n</annotation></semantics></math>th reply. Due to the varying length of exchanges present in the PEx data, we designated a specific length of 4 turns for all exchanges. Conversations with less than 4 exchanges were dropped. For conversations with more than 4 exchanges, contiguous sequences of 4 were derived from the beginning of the conversation until the last response. This means that a conversation with <math id="S6.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S6.p2.4.m4.1a"><mi id="S6.p2.4.m4.1.1" xref="S6.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.1b"><ci id="S6.p2.4.m4.1.1.cmml" xref="S6.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.1c">x</annotation></semantics></math> exchanges can generate <math id="S6.p2.5.m5.1" class="ltx_Math" alttext="x-n+1" display="inline"><semantics id="S6.p2.5.m5.1a"><mrow id="S6.p2.5.m5.1.1" xref="S6.p2.5.m5.1.1.cmml"><mrow id="S6.p2.5.m5.1.1.2" xref="S6.p2.5.m5.1.1.2.cmml"><mi id="S6.p2.5.m5.1.1.2.2" xref="S6.p2.5.m5.1.1.2.2.cmml">x</mi><mo id="S6.p2.5.m5.1.1.2.1" xref="S6.p2.5.m5.1.1.2.1.cmml">−</mo><mi id="S6.p2.5.m5.1.1.2.3" xref="S6.p2.5.m5.1.1.2.3.cmml">n</mi></mrow><mo id="S6.p2.5.m5.1.1.1" xref="S6.p2.5.m5.1.1.1.cmml">+</mo><mn id="S6.p2.5.m5.1.1.3" xref="S6.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.5.m5.1b"><apply id="S6.p2.5.m5.1.1.cmml" xref="S6.p2.5.m5.1.1"><plus id="S6.p2.5.m5.1.1.1.cmml" xref="S6.p2.5.m5.1.1.1"></plus><apply id="S6.p2.5.m5.1.1.2.cmml" xref="S6.p2.5.m5.1.1.2"><minus id="S6.p2.5.m5.1.1.2.1.cmml" xref="S6.p2.5.m5.1.1.2.1"></minus><ci id="S6.p2.5.m5.1.1.2.2.cmml" xref="S6.p2.5.m5.1.1.2.2">𝑥</ci><ci id="S6.p2.5.m5.1.1.2.3.cmml" xref="S6.p2.5.m5.1.1.2.3">𝑛</ci></apply><cn type="integer" id="S6.p2.5.m5.1.1.3.cmml" xref="S6.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.5.m5.1c">x-n+1</annotation></semantics></math> contiguous sequences of <math id="S6.p2.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.p2.6.m6.1a"><mi id="S6.p2.6.m6.1.1" xref="S6.p2.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.p2.6.m6.1b"><ci id="S6.p2.6.m6.1.1.cmml" xref="S6.p2.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.6.m6.1c">n</annotation></semantics></math>, e.g. a conversation with exchanges <math id="S6.p2.7.m7.5" class="ltx_Math" alttext="e1,e2,e3,e4,e5" display="inline"><semantics id="S6.p2.7.m7.5a"><mrow id="S6.p2.7.m7.5.5.5" xref="S6.p2.7.m7.5.5.6.cmml"><mrow id="S6.p2.7.m7.1.1.1.1" xref="S6.p2.7.m7.1.1.1.1.cmml"><mi id="S6.p2.7.m7.1.1.1.1.2" xref="S6.p2.7.m7.1.1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.7.m7.1.1.1.1.1" xref="S6.p2.7.m7.1.1.1.1.1.cmml">​</mo><mn id="S6.p2.7.m7.1.1.1.1.3" xref="S6.p2.7.m7.1.1.1.1.3.cmml">1</mn></mrow><mo id="S6.p2.7.m7.5.5.5.6" xref="S6.p2.7.m7.5.5.6.cmml">,</mo><mrow id="S6.p2.7.m7.2.2.2.2" xref="S6.p2.7.m7.2.2.2.2.cmml"><mi id="S6.p2.7.m7.2.2.2.2.2" xref="S6.p2.7.m7.2.2.2.2.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.7.m7.2.2.2.2.1" xref="S6.p2.7.m7.2.2.2.2.1.cmml">​</mo><mn id="S6.p2.7.m7.2.2.2.2.3" xref="S6.p2.7.m7.2.2.2.2.3.cmml">2</mn></mrow><mo id="S6.p2.7.m7.5.5.5.7" xref="S6.p2.7.m7.5.5.6.cmml">,</mo><mrow id="S6.p2.7.m7.3.3.3.3" xref="S6.p2.7.m7.3.3.3.3.cmml"><mi id="S6.p2.7.m7.3.3.3.3.2" xref="S6.p2.7.m7.3.3.3.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.7.m7.3.3.3.3.1" xref="S6.p2.7.m7.3.3.3.3.1.cmml">​</mo><mn id="S6.p2.7.m7.3.3.3.3.3" xref="S6.p2.7.m7.3.3.3.3.3.cmml">3</mn></mrow><mo id="S6.p2.7.m7.5.5.5.8" xref="S6.p2.7.m7.5.5.6.cmml">,</mo><mrow id="S6.p2.7.m7.4.4.4.4" xref="S6.p2.7.m7.4.4.4.4.cmml"><mi id="S6.p2.7.m7.4.4.4.4.2" xref="S6.p2.7.m7.4.4.4.4.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.7.m7.4.4.4.4.1" xref="S6.p2.7.m7.4.4.4.4.1.cmml">​</mo><mn id="S6.p2.7.m7.4.4.4.4.3" xref="S6.p2.7.m7.4.4.4.4.3.cmml">4</mn></mrow><mo id="S6.p2.7.m7.5.5.5.9" xref="S6.p2.7.m7.5.5.6.cmml">,</mo><mrow id="S6.p2.7.m7.5.5.5.5" xref="S6.p2.7.m7.5.5.5.5.cmml"><mi id="S6.p2.7.m7.5.5.5.5.2" xref="S6.p2.7.m7.5.5.5.5.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.7.m7.5.5.5.5.1" xref="S6.p2.7.m7.5.5.5.5.1.cmml">​</mo><mn id="S6.p2.7.m7.5.5.5.5.3" xref="S6.p2.7.m7.5.5.5.5.3.cmml">5</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.7.m7.5b"><list id="S6.p2.7.m7.5.5.6.cmml" xref="S6.p2.7.m7.5.5.5"><apply id="S6.p2.7.m7.1.1.1.1.cmml" xref="S6.p2.7.m7.1.1.1.1"><times id="S6.p2.7.m7.1.1.1.1.1.cmml" xref="S6.p2.7.m7.1.1.1.1.1"></times><ci id="S6.p2.7.m7.1.1.1.1.2.cmml" xref="S6.p2.7.m7.1.1.1.1.2">𝑒</ci><cn type="integer" id="S6.p2.7.m7.1.1.1.1.3.cmml" xref="S6.p2.7.m7.1.1.1.1.3">1</cn></apply><apply id="S6.p2.7.m7.2.2.2.2.cmml" xref="S6.p2.7.m7.2.2.2.2"><times id="S6.p2.7.m7.2.2.2.2.1.cmml" xref="S6.p2.7.m7.2.2.2.2.1"></times><ci id="S6.p2.7.m7.2.2.2.2.2.cmml" xref="S6.p2.7.m7.2.2.2.2.2">𝑒</ci><cn type="integer" id="S6.p2.7.m7.2.2.2.2.3.cmml" xref="S6.p2.7.m7.2.2.2.2.3">2</cn></apply><apply id="S6.p2.7.m7.3.3.3.3.cmml" xref="S6.p2.7.m7.3.3.3.3"><times id="S6.p2.7.m7.3.3.3.3.1.cmml" xref="S6.p2.7.m7.3.3.3.3.1"></times><ci id="S6.p2.7.m7.3.3.3.3.2.cmml" xref="S6.p2.7.m7.3.3.3.3.2">𝑒</ci><cn type="integer" id="S6.p2.7.m7.3.3.3.3.3.cmml" xref="S6.p2.7.m7.3.3.3.3.3">3</cn></apply><apply id="S6.p2.7.m7.4.4.4.4.cmml" xref="S6.p2.7.m7.4.4.4.4"><times id="S6.p2.7.m7.4.4.4.4.1.cmml" xref="S6.p2.7.m7.4.4.4.4.1"></times><ci id="S6.p2.7.m7.4.4.4.4.2.cmml" xref="S6.p2.7.m7.4.4.4.4.2">𝑒</ci><cn type="integer" id="S6.p2.7.m7.4.4.4.4.3.cmml" xref="S6.p2.7.m7.4.4.4.4.3">4</cn></apply><apply id="S6.p2.7.m7.5.5.5.5.cmml" xref="S6.p2.7.m7.5.5.5.5"><times id="S6.p2.7.m7.5.5.5.5.1.cmml" xref="S6.p2.7.m7.5.5.5.5.1"></times><ci id="S6.p2.7.m7.5.5.5.5.2.cmml" xref="S6.p2.7.m7.5.5.5.5.2">𝑒</ci><cn type="integer" id="S6.p2.7.m7.5.5.5.5.3.cmml" xref="S6.p2.7.m7.5.5.5.5.3">5</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.7.m7.5c">e1,e2,e3,e4,e5</annotation></semantics></math>, we can generate the conversations (1) <math id="S6.p2.8.m8.4" class="ltx_Math" alttext="e1,e2,e3,e4" display="inline"><semantics id="S6.p2.8.m8.4a"><mrow id="S6.p2.8.m8.4.4.4" xref="S6.p2.8.m8.4.4.5.cmml"><mrow id="S6.p2.8.m8.1.1.1.1" xref="S6.p2.8.m8.1.1.1.1.cmml"><mi id="S6.p2.8.m8.1.1.1.1.2" xref="S6.p2.8.m8.1.1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.8.m8.1.1.1.1.1" xref="S6.p2.8.m8.1.1.1.1.1.cmml">​</mo><mn id="S6.p2.8.m8.1.1.1.1.3" xref="S6.p2.8.m8.1.1.1.1.3.cmml">1</mn></mrow><mo id="S6.p2.8.m8.4.4.4.5" xref="S6.p2.8.m8.4.4.5.cmml">,</mo><mrow id="S6.p2.8.m8.2.2.2.2" xref="S6.p2.8.m8.2.2.2.2.cmml"><mi id="S6.p2.8.m8.2.2.2.2.2" xref="S6.p2.8.m8.2.2.2.2.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.8.m8.2.2.2.2.1" xref="S6.p2.8.m8.2.2.2.2.1.cmml">​</mo><mn id="S6.p2.8.m8.2.2.2.2.3" xref="S6.p2.8.m8.2.2.2.2.3.cmml">2</mn></mrow><mo id="S6.p2.8.m8.4.4.4.6" xref="S6.p2.8.m8.4.4.5.cmml">,</mo><mrow id="S6.p2.8.m8.3.3.3.3" xref="S6.p2.8.m8.3.3.3.3.cmml"><mi id="S6.p2.8.m8.3.3.3.3.2" xref="S6.p2.8.m8.3.3.3.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.8.m8.3.3.3.3.1" xref="S6.p2.8.m8.3.3.3.3.1.cmml">​</mo><mn id="S6.p2.8.m8.3.3.3.3.3" xref="S6.p2.8.m8.3.3.3.3.3.cmml">3</mn></mrow><mo id="S6.p2.8.m8.4.4.4.7" xref="S6.p2.8.m8.4.4.5.cmml">,</mo><mrow id="S6.p2.8.m8.4.4.4.4" xref="S6.p2.8.m8.4.4.4.4.cmml"><mi id="S6.p2.8.m8.4.4.4.4.2" xref="S6.p2.8.m8.4.4.4.4.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.8.m8.4.4.4.4.1" xref="S6.p2.8.m8.4.4.4.4.1.cmml">​</mo><mn id="S6.p2.8.m8.4.4.4.4.3" xref="S6.p2.8.m8.4.4.4.4.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.8.m8.4b"><list id="S6.p2.8.m8.4.4.5.cmml" xref="S6.p2.8.m8.4.4.4"><apply id="S6.p2.8.m8.1.1.1.1.cmml" xref="S6.p2.8.m8.1.1.1.1"><times id="S6.p2.8.m8.1.1.1.1.1.cmml" xref="S6.p2.8.m8.1.1.1.1.1"></times><ci id="S6.p2.8.m8.1.1.1.1.2.cmml" xref="S6.p2.8.m8.1.1.1.1.2">𝑒</ci><cn type="integer" id="S6.p2.8.m8.1.1.1.1.3.cmml" xref="S6.p2.8.m8.1.1.1.1.3">1</cn></apply><apply id="S6.p2.8.m8.2.2.2.2.cmml" xref="S6.p2.8.m8.2.2.2.2"><times id="S6.p2.8.m8.2.2.2.2.1.cmml" xref="S6.p2.8.m8.2.2.2.2.1"></times><ci id="S6.p2.8.m8.2.2.2.2.2.cmml" xref="S6.p2.8.m8.2.2.2.2.2">𝑒</ci><cn type="integer" id="S6.p2.8.m8.2.2.2.2.3.cmml" xref="S6.p2.8.m8.2.2.2.2.3">2</cn></apply><apply id="S6.p2.8.m8.3.3.3.3.cmml" xref="S6.p2.8.m8.3.3.3.3"><times id="S6.p2.8.m8.3.3.3.3.1.cmml" xref="S6.p2.8.m8.3.3.3.3.1"></times><ci id="S6.p2.8.m8.3.3.3.3.2.cmml" xref="S6.p2.8.m8.3.3.3.3.2">𝑒</ci><cn type="integer" id="S6.p2.8.m8.3.3.3.3.3.cmml" xref="S6.p2.8.m8.3.3.3.3.3">3</cn></apply><apply id="S6.p2.8.m8.4.4.4.4.cmml" xref="S6.p2.8.m8.4.4.4.4"><times id="S6.p2.8.m8.4.4.4.4.1.cmml" xref="S6.p2.8.m8.4.4.4.4.1"></times><ci id="S6.p2.8.m8.4.4.4.4.2.cmml" xref="S6.p2.8.m8.4.4.4.4.2">𝑒</ci><cn type="integer" id="S6.p2.8.m8.4.4.4.4.3.cmml" xref="S6.p2.8.m8.4.4.4.4.3">4</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.8.m8.4c">e1,e2,e3,e4</annotation></semantics></math>, and (2) <math id="S6.p2.9.m9.4" class="ltx_Math" alttext="e2,e3,e4,e5" display="inline"><semantics id="S6.p2.9.m9.4a"><mrow id="S6.p2.9.m9.4.4.4" xref="S6.p2.9.m9.4.4.5.cmml"><mrow id="S6.p2.9.m9.1.1.1.1" xref="S6.p2.9.m9.1.1.1.1.cmml"><mi id="S6.p2.9.m9.1.1.1.1.2" xref="S6.p2.9.m9.1.1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.9.m9.1.1.1.1.1" xref="S6.p2.9.m9.1.1.1.1.1.cmml">​</mo><mn id="S6.p2.9.m9.1.1.1.1.3" xref="S6.p2.9.m9.1.1.1.1.3.cmml">2</mn></mrow><mo id="S6.p2.9.m9.4.4.4.5" xref="S6.p2.9.m9.4.4.5.cmml">,</mo><mrow id="S6.p2.9.m9.2.2.2.2" xref="S6.p2.9.m9.2.2.2.2.cmml"><mi id="S6.p2.9.m9.2.2.2.2.2" xref="S6.p2.9.m9.2.2.2.2.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.9.m9.2.2.2.2.1" xref="S6.p2.9.m9.2.2.2.2.1.cmml">​</mo><mn id="S6.p2.9.m9.2.2.2.2.3" xref="S6.p2.9.m9.2.2.2.2.3.cmml">3</mn></mrow><mo id="S6.p2.9.m9.4.4.4.6" xref="S6.p2.9.m9.4.4.5.cmml">,</mo><mrow id="S6.p2.9.m9.3.3.3.3" xref="S6.p2.9.m9.3.3.3.3.cmml"><mi id="S6.p2.9.m9.3.3.3.3.2" xref="S6.p2.9.m9.3.3.3.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.9.m9.3.3.3.3.1" xref="S6.p2.9.m9.3.3.3.3.1.cmml">​</mo><mn id="S6.p2.9.m9.3.3.3.3.3" xref="S6.p2.9.m9.3.3.3.3.3.cmml">4</mn></mrow><mo id="S6.p2.9.m9.4.4.4.7" xref="S6.p2.9.m9.4.4.5.cmml">,</mo><mrow id="S6.p2.9.m9.4.4.4.4" xref="S6.p2.9.m9.4.4.4.4.cmml"><mi id="S6.p2.9.m9.4.4.4.4.2" xref="S6.p2.9.m9.4.4.4.4.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p2.9.m9.4.4.4.4.1" xref="S6.p2.9.m9.4.4.4.4.1.cmml">​</mo><mn id="S6.p2.9.m9.4.4.4.4.3" xref="S6.p2.9.m9.4.4.4.4.3.cmml">5</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.9.m9.4b"><list id="S6.p2.9.m9.4.4.5.cmml" xref="S6.p2.9.m9.4.4.4"><apply id="S6.p2.9.m9.1.1.1.1.cmml" xref="S6.p2.9.m9.1.1.1.1"><times id="S6.p2.9.m9.1.1.1.1.1.cmml" xref="S6.p2.9.m9.1.1.1.1.1"></times><ci id="S6.p2.9.m9.1.1.1.1.2.cmml" xref="S6.p2.9.m9.1.1.1.1.2">𝑒</ci><cn type="integer" id="S6.p2.9.m9.1.1.1.1.3.cmml" xref="S6.p2.9.m9.1.1.1.1.3">2</cn></apply><apply id="S6.p2.9.m9.2.2.2.2.cmml" xref="S6.p2.9.m9.2.2.2.2"><times id="S6.p2.9.m9.2.2.2.2.1.cmml" xref="S6.p2.9.m9.2.2.2.2.1"></times><ci id="S6.p2.9.m9.2.2.2.2.2.cmml" xref="S6.p2.9.m9.2.2.2.2.2">𝑒</ci><cn type="integer" id="S6.p2.9.m9.2.2.2.2.3.cmml" xref="S6.p2.9.m9.2.2.2.2.3">3</cn></apply><apply id="S6.p2.9.m9.3.3.3.3.cmml" xref="S6.p2.9.m9.3.3.3.3"><times id="S6.p2.9.m9.3.3.3.3.1.cmml" xref="S6.p2.9.m9.3.3.3.3.1"></times><ci id="S6.p2.9.m9.3.3.3.3.2.cmml" xref="S6.p2.9.m9.3.3.3.3.2">𝑒</ci><cn type="integer" id="S6.p2.9.m9.3.3.3.3.3.cmml" xref="S6.p2.9.m9.3.3.3.3.3">4</cn></apply><apply id="S6.p2.9.m9.4.4.4.4.cmml" xref="S6.p2.9.m9.4.4.4.4"><times id="S6.p2.9.m9.4.4.4.4.1.cmml" xref="S6.p2.9.m9.4.4.4.4.1"></times><ci id="S6.p2.9.m9.4.4.4.4.2.cmml" xref="S6.p2.9.m9.4.4.4.4.2">𝑒</ci><cn type="integer" id="S6.p2.9.m9.4.4.4.4.3.cmml" xref="S6.p2.9.m9.4.4.4.4.3">5</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.9.m9.4c">e2,e3,e4,e5</annotation></semantics></math>. If an exchange happens to get filtered out in preprocessing, the whole sequence is dropped as there will be a gap in context.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Our fine-tuned model is capable of generating responses based on the previous 3 responses. Therefore, the model is presented with a concatenated sequence of these responses delimited with an end-of-sequence token.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">The decoding method used for both manual and automatic testing is beam search of <span id="S6.p4.1.1" class="ltx_text ltx_font_italic">width</span> 5. A <span id="S6.p4.1.2" class="ltx_text ltx_font_italic">trigram</span> repetition penalty was also introduced due to some instances where beam search would produce a repetitive token generation causing the max sequence length to be fully consumed. We saw the need for applying such penalty as it directly affected hypothesis generation.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Finetuning Results</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">For data augmentation, we fine-tuned the existing Tagalog RoBERTa model with 3% of the whole dataset, equating to 178K individual utterances, for 3 epochs in approximately 40 minutes. The model resulted in a perplexity of 17.7287.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">For DialoGPT, we fine-tuned a baseline model and five augmented models with varying percentages of token replacement. The baseline model was fine-tuned with all 53K untouched conversations from the corpus and zero synthetic data for approximately 10 hours. The augmented models were fine-tuned with a merged corpus of original data (53K) and synthetic data (53K) for approximately 18 hours each. The varying percentages of RoBERTa-generated tokens present in the synthetic dataset constitutes the only independent variable for the augmented models. We maintain identical training configurations for all the experiments. The specific finetuning scores are shown in Table <a href="#S6.T1" title="Table 1 ‣ 6. Response Generation Model ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">As part our ablations below, we fine-tuned another six models with decreased training data sizes following the same training configurations. Finetuning results are displayed in Table <a href="#S6.T2" title="Table 2 ‣ 6. Response Generation Model ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Ablation Results and Discussion</h2>

<div id="S8.p1" class="ltx_para ltx_noindent">
<p id="S8.p1.1" class="ltx_p">In this section, we perform ablations to understand the effects of the data augmentation percentages with regards to model performance. We also simulated lower training data sizes to assess effectiveness of the methodology. In addition, sample conversations within and outside the domain of the corpus are also presented in this section.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">8.1.   Effect of Token Replacement Percentage</h3>

<div id="S8.SS1.p1" class="ltx_para ltx_noindent">
<p id="S8.SS1.p1.1" class="ltx_p">An ablation was performed to determine the effect of varying token replacement percentages on the final performance of the model. The evaluation scores can be seen in Table <a href="#S8.T3" title="Table 3 ‣ 8.1. Effect of Token Replacement Percentage ‣ 8. Ablation Results and Discussion ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<div id="S8.SS1.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.p2.1" class="ltx_p">As shown in Table <a href="#S8.T3" title="Table 3 ‣ 8.1. Effect of Token Replacement Percentage ‣ 8. Ablation Results and Discussion ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, replacing 10% of the original tokens exhibits the highest improvement in model performance both in Perplexity and BERTScore as compared to the baseline model. We observe a degradation in both metrics after the 10% token replacement when increasing the replacement percentage further. We theorized that the presence of more synthetic tokens introduces less contextually-similar responses regardless if the synthetic tokens are semantically valid. We also hypothesize that there is a higher possibility of semantically-invalid tokens present in higher replacement percentages as the masking and filling process occurs independently as mentioned in Section <a href="#S5" title="5. Data Augmentation ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S8.SS1.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.p3.1" class="ltx_p">We also sought to determine the effect of the token replacement percentages outside context similarity and probability. We observe a significant difference in the amount of content words and functions words for all augmented models. This provides evidence that synthetic data introduces lengthier responses in general. More importantly, the increase in content words reinforces our observation that the augmented models includes more substance with regards to detail in the generated response.</p>
</div>
<figure id="S8.T3" class="ltx_table">
<table id="S8.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T3.1.1.1" class="ltx_tr">
<th id="S8.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S8.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">%</span></th>
<th id="S8.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Prplx</span></th>
<th id="S8.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S8.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Func.</span></th>
<th id="S8.T3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S8.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Cont.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T3.1.2.1" class="ltx_tr">
<th id="S8.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">0%</th>
<td id="S8.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">4.1206</td>
<td id="S8.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">0.3379</td>
<td id="S8.T3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">82,956</td>
<td id="S8.T3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">100,940</td>
</tr>
<tr id="S8.T3.1.3.2" class="ltx_tr">
<th id="S8.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">10%</th>
<td id="S8.T3.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">3.6771</td>
<td id="S8.T3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">0.3794</td>
<td id="S8.T3.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">88,944</td>
<td id="S8.T3.1.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">112,776</td>
</tr>
<tr id="S8.T3.1.4.3" class="ltx_tr">
<th id="S8.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">15%</th>
<td id="S8.T3.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">3.7867</td>
<td id="S8.T3.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">0.3762</td>
<td id="S8.T3.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">88,175</td>
<td id="S8.T3.1.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">116,501</td>
</tr>
<tr id="S8.T3.1.5.4" class="ltx_tr">
<th id="S8.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">20%</th>
<td id="S8.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t">3.8593</td>
<td id="S8.T3.1.5.4.3" class="ltx_td ltx_align_left ltx_border_t">0.3740</td>
<td id="S8.T3.1.5.4.4" class="ltx_td ltx_align_left ltx_border_t">87,910</td>
<td id="S8.T3.1.5.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">114,018</td>
</tr>
<tr id="S8.T3.1.6.5" class="ltx_tr">
<th id="S8.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">25%</th>
<td id="S8.T3.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t">3.9142</td>
<td id="S8.T3.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">0.3719</td>
<td id="S8.T3.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">88,467</td>
<td id="S8.T3.1.6.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">113,473</td>
</tr>
<tr id="S8.T3.1.7.6" class="ltx_tr">
<th id="S8.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">30%</th>
<td id="S8.T3.1.7.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">3.9574</td>
<td id="S8.T3.1.7.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">0.3704</td>
<td id="S8.T3.1.7.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">86,856</td>
<td id="S8.T3.1.7.6.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">110,135</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Perplexity, F1, and Function Word and Content Word count for the models with different token replacement percentages</figcaption>
</figure>
<figure id="S8.T4" class="ltx_table">
<table id="S8.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T4.1.1.1" class="ltx_tr">
<th id="S8.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S8.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Setup</span></th>
<th id="S8.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Prplx</span></th>
<th id="S8.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S8.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S8.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Func.</span></th>
<th id="S8.T4.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S8.T4.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.1.1.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S8.T4.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Cont.</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T4.1.2.1" class="ltx_tr">
<td id="S8.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">1K</td>
<td id="S8.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">9.5821</td>
<td id="S8.T4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">0.3387</td>
<td id="S8.T4.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">57,327</td>
<td id="S8.T4.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S8.T4.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.2.1.5.1.1" class="ltx_p" style="width:31.3pt;">58,872</span>
</span>
</td>
</tr>
<tr id="S8.T4.1.3.2" class="ltx_tr">
<td id="S8.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">1K+Aug</td>
<td id="S8.T4.1.3.2.2" class="ltx_td ltx_align_left">8.4791</td>
<td id="S8.T4.1.3.2.3" class="ltx_td ltx_align_left">0.3441</td>
<td id="S8.T4.1.3.2.4" class="ltx_td ltx_align_left">60,774</td>
<td id="S8.T4.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S8.T4.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.3.2.5.1.1" class="ltx_p" style="width:31.3pt;">64,368</span>
</span>
</td>
</tr>
<tr id="S8.T4.1.4.3" class="ltx_tr">
<td id="S8.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">10K</td>
<td id="S8.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">6.1937</td>
<td id="S8.T4.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">0.3520</td>
<td id="S8.T4.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">66,371</td>
<td id="S8.T4.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S8.T4.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.4.3.5.1.1" class="ltx_p" style="width:31.3pt;">71,706</span>
</span>
</td>
</tr>
<tr id="S8.T4.1.5.4" class="ltx_tr">
<td id="S8.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">10K+Aug</td>
<td id="S8.T4.1.5.4.2" class="ltx_td ltx_align_left">5.755</td>
<td id="S8.T4.1.5.4.3" class="ltx_td ltx_align_left">0.3539</td>
<td id="S8.T4.1.5.4.4" class="ltx_td ltx_align_left">70,078</td>
<td id="S8.T4.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S8.T4.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.5.4.5.1.1" class="ltx_p" style="width:31.3pt;">80,662</span>
</span>
</td>
</tr>
<tr id="S8.T4.1.6.5" class="ltx_tr">
<td id="S8.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">25K</td>
<td id="S8.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t">4.1206</td>
<td id="S8.T4.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">0.3584</td>
<td id="S8.T4.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">76,665</td>
<td id="S8.T4.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S8.T4.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.6.5.5.1.1" class="ltx_p" style="width:31.3pt;">87,873</span>
</span>
</td>
</tr>
<tr id="S8.T4.1.7.6" class="ltx_tr">
<td id="S8.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">25K+Aug</td>
<td id="S8.T4.1.7.6.2" class="ltx_td ltx_align_left ltx_border_b">3.6771</td>
<td id="S8.T4.1.7.6.3" class="ltx_td ltx_align_left ltx_border_b">0.3617</td>
<td id="S8.T4.1.7.6.4" class="ltx_td ltx_align_left ltx_border_b">82,240</td>
<td id="S8.T4.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S8.T4.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S8.T4.1.7.6.5.1.1" class="ltx_p" style="width:31.3pt;">104,211</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Perpelixty, F1, and Function Word and Content Word count for varying data sizes</figcaption>
</figure>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">8.2.   Effect of Training Data Size</h3>

<div id="S8.SS2.p1" class="ltx_para ltx_noindent">
<p id="S8.SS2.p1.1" class="ltx_p">Using 10% token replacement, we also trained six models across varying data sizes to determine the effect of the methodology in other scales of low data sizes.
The model with a base training size of 1K samples improved the most in terms of perplexity achieving a decrease of 1.103. The full training size also improved by a significant margin of 10.75% change, albeit only a 0.4435 decrease in perplexity. Scores for the 53K models were taken from the experiments on token replacement percentage.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para ltx_noindent">
<p id="S8.SS2.p2.1" class="ltx_p">The models also showed relative minor improvements in BERTScore as seen in Table <a href="#S8.T4" title="Table 4 ‣ 8.1. Effect of Token Replacement Percentage ‣ 8. Ablation Results and Discussion ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The model with a base training size of 1K samples improved in its BERTScore as well by 1.59% or a 0.0054 increase in F1 after being trained with synthetic data. The full training size improved the most, with a 3.26% increase in F1, while the models trained with 10K and 25K base training sizes improved relatively insignificant.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para ltx_noindent">
<p id="S8.SS2.p3.1" class="ltx_p">We theorized that the smaller training samples tend to be more sensitive when using a model such as DialoGPT medium which requires more data on average than 1K samples. We speculate that the dip in scores for 10K and 25K base training sizes and the sudden increase in scores again for the full 53K training size could be attributed to the normal expected training sizes of the model. Consistent with the previous ablation, content word and function word usage also increased with all augmented models as seen in Table <a href="#S8.T4" title="Table 4 ‣ 8.1. Effect of Token Replacement Percentage ‣ 8. Ablation Results and Discussion ‣ Using Synthetic Data for Conversational Response Generation in Low-resource Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   Conclusion</h2>

<div id="S9.p1" class="ltx_para ltx_noindent">
<p id="S9.p1.1" class="ltx_p">We constructed the first Filipino conversational dataset called PEx Conversations. We leverage this dataset by proposing a RoBERTa-based data augmentation methodology via a fine-tuned Tagalog RoBERTa model to perform context-aided token replacement. The collected data and the synthetic data were both used to fine-tune DialoGPT to produce the first Filipino conversational response generator capable of generating a response related to the previous 3 responses.</p>
</div>
<div id="S9.p2" class="ltx_para ltx_noindent">
<p id="S9.p2.1" class="ltx_p">We were able to show that introducing synthetic data does improve model performance by a comparable margin. The synthetic data was able to enhance our best model in BERTScore F1 Accuracy by 12.2% and perplexity by 10.7%. We also found that introducing synthetic data also increases the usage of content words in a response, with our highest scoring augmented model achieving a 11.7% increase.</p>
</div>
<div id="S9.p3" class="ltx_para ltx_noindent">
<p id="S9.p3.1" class="ltx_p">While the ablations show that replacing 10% tokens in an utterance improved the performance quantitatively, the qualitative differences between having more tokens replaced were negligible as observed. Additionally, we also experimented with applying our technique with simulated lower training data sizes. Our findings show that the technique still relatively improves performance using our 1K, 10K, and 25K training size samples.</p>
</div>
<div id="S9.p4" class="ltx_para ltx_noindent">
<p id="S9.p4.1" class="ltx_p">To validate the effectiveness of our DA, we suggest looking into applying the method on other Filipino conversational datasets, or even recreate the technique with other low-resource languages if possible. Chat data, in contrast to transformed forum data, can also be explored when attempting to produce more conversation-like responses.</p>
</div>
<div id="S9.p5" class="ltx_para ltx_noindent">
<p id="S9.p5.1" class="ltx_p">For DA, our methodology involves independent replacement followed by a merge step when producing synthetic utterances. Hence, we recommend exploring into a cascading approach where replacement occurs in succession where the newly-produced sequence is used as the candidate for the next token replacement.</p>
</div>
<div id="S9.p6" class="ltx_para ltx_noindent">
<p id="S9.p6.1" class="ltx_p">With regards to the model, it can also be beneficial to look into finetuning the large DialoGPT model (762M) for corpora of a larger-scale. In order to reinforce the findings, we also encourage future work to perform human evaluations to understand the effect of the DA qualitatively, and potentially with the support of expert linguists should the resources allow.</p>
</div>
<div id="S9.p7" class="ltx_para ltx_noindent">
<p id="S9.p7.1" class="ltx_p">Lastly, we recommend exploring real-world applications of these conversational agents. This includes agents that may have different personas or conversing personalities, to suit specific purposes such as being a companion, an assistant or an expert.</p>
</div>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">10.   Bibliographical References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adiwardana et al., 2020</span>
<span class="ltx_bibblock">
Adiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R.,
Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le, Q. V.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Towards a human-like open-domain chatbot.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baheti et al., 2020</span>
<span class="ltx_bibblock">
Baheti, A., Ritter, A., and Small, K.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Fluent response generation for conversational question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 191–207, Online, July. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cruz and Cheng, 2021</span>
<span class="ltx_bibblock">
Cruz, J. C. B. and Cheng, C.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Improving large-scale language models and resources for filipino.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.06053</span>.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cruz et al., 2021</span>
<span class="ltx_bibblock">
Cruz, J. C. B., Resabal, J. K., Lin, J., Velasco, D. J., and Cheng, C.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Exploiting news article structure for automatic corpus generation of
entailment datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">Pacific Rim International Conference on Artificial
Intelligence</span>, pages 86–99. Springer.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai and Adel, 2020</span>
<span class="ltx_bibblock">
Dai, X. and Adel, H.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">An analysis of simple data augmentation for named entity recognition.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al., 2019</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186,
Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al., 2018</span>
<span class="ltx_bibblock">
Goyal, A. K., Metallinou, A., and Matsoukas, S.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Fast and scalable expansion of natural language understanding
functionality for intelligent agents.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 3 (Industry Papers)</span>, pages 145–152, New Orleans -
Louisiana, June. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grundkiewicz et al., 2019</span>
<span class="ltx_bibblock">
Grundkiewicz, R., Junczys-Dowmunt, M., and Heafield, K.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Neural grammatical error correction systems with unsupervised
pre-training on synthetic data.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">Proceedings of the Fourteenth Workshop on Innovative Use of
NLP for Building Educational Applications</span>, pages 252–263, Florence, Italy,
August. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi, 2018</span>
<span class="ltx_bibblock">
Kobayashi, S.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Contextual augmentation: Data augmentation by words with paradigmatic
relations.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</span>, pages 452–457, New Orleans,
Louisiana, June. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al., 2019</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., and Stoyanov, V.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al., 1958</span>
<span class="ltx_bibblock">
Miller, G., Newman, E., and Friedman, E.

</span>
<span class="ltx_bibblock">(1958).

</span>
<span class="ltx_bibblock">Length-frequency statistics for written english.

</span>
<span class="ltx_bibblock"><span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">Information and Control</span>, 1(4):370–389.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puri et al., 2020</span>
<span class="ltx_bibblock">
Puri, R., Spring, R., Patwary, M. A., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Training question answering models from synthetic data.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2002.09599.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et al., 2017</span>
<span class="ltx_bibblock">
See, A., Liu, P. J., and Manning, C. D.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Get to the point: Summarization with pointer-generator networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 1073–1083,
Vancouver, Canada, July. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vania et al., 2019</span>
<span class="ltx_bibblock">
Vania, C., Kementchedjhieva, Y., Søgaard, A., and Lopez, A.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">A systematic comparison of methods for low-resource dependency
parsing on genuinely low-resource languages.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</span>, pages 1105–1116, Hong Kong,
China, November. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei and Zou, 2019</span>
<span class="ltx_bibblock">
Wei, J. W. and Zou, K.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">EDA: easy data augmentation techniques for boosting performance on
text classification tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1901.11196.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al., 2018</span>
<span class="ltx_bibblock">
Wu, X., Lv, S., Zang, L., Han, J., and Hu, S.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Conditional BERT contextual augmentation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1812.06705.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al., 2017</span>
<span class="ltx_bibblock">
Xu, A., Liu, Z., Guo, Y., Sinha, V., and Akkiraju, R.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">A new chatbot for customer service on social media.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 CHI Conference on Human Factors in
Computing Systems</span>, pages 3506–3510.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al., 2019</span>
<span class="ltx_bibblock">
Zhang, Y., Sun, S., Galley, M., Chen, Y.-C., Brockett, C., Gao, X., Gao, J.,
Liu, J., and Dolan, B.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Dialogpt: Large-scale generative pre-training for conversational
response generation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.00536</span>.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al., 2020</span>
<span class="ltx_bibblock">
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.02652" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.02653" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.02653">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.02653" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.02654" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:04:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
