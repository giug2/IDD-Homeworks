<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.01015] Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction</title><meta property="og:description" content="Ever-increasing smartphone-generated video content demands intelligent techniques to edit and enhance videos on power-constrained devices. Most of the best performing algorithms for video understanding tasks like actio…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.01015">

<!--Generated on Tue Mar 12 10:20:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Spatio-Temporal Video Representation Learning for 
<br class="ltx_break">AI Based Video Playback Style Prediction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Rishubh Parihar 
<br class="ltx_break">Indian Institute of Science,
<br class="ltx_break">Bangalore, India.
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">parihar.rishubh@gmail.com</span>
</span><span class="ltx_author_notes">Equal Contribution</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gaurav Ramola <span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">Samsung India Research Institute,
<br class="ltx_break">Bangalore, India.
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">g.ramola@samsung.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ranajit Saha
<br class="ltx_break">Microsoft Corporation,
<br class="ltx_break">Hyderabad, India.
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">rs.ranajitsaha@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ravi Kini
<br class="ltx_break">Samsung India Research Institute,
<br class="ltx_break">Bangalore, India.
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">abcdravi@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aniket Rege
<br class="ltx_break">Univ. of Washinton,
<br class="ltx_break">Seattle, USA.
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">aniketrg7@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sudha Velusamy
<br class="ltx_break">Samsung India Research Institute,
<br class="ltx_break">Bangalore, India.
<br class="ltx_break"><span id="id6.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sudha.v@samsung.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Ever-increasing smartphone-generated video content demands intelligent techniques to edit and enhance videos on power-constrained devices. Most of the best performing algorithms for video understanding tasks like action recognition, localization, etc., rely heavily on rich spatio-temporal representations to make accurate predictions. For effective learning of the spatio-temporal representation, it is crucial to understand the underlying object motion patterns present in the video. In this paper, we propose a novel approach for understanding object motions via motion type classification. The proposed motion type classifier predicts a motion type for the video based on the trajectories of the objects present.
Our classifier assigns a motion type for the given video from the following five primitive motion classes: linear, projectile, oscillatory, local and random.
We demonstrate that the representations learned from the motion type classification generalizes well for the challenging downstream task of video retrieval. Further, we proposed a recommendation system for video playback style based on the motion type classifier predictions.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">An increasing volume of smart-phones with high-quality cameras in recent years has led to a meteoric rise in the amount of video content captured and shared on social media platforms such as Tiktok, YouTube, Facebook, Instagram, SnapChat, ShareChat etc. This trend has fostered the need for automated video analysis tools that can aid the user to edit videos with ease on mobile devices, on-the-fly.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.01015/assets/Fig1_Football.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="306" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Visualizing an example motion trajectory of a ball </figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Videos contain rich information embedded in both spatial and temporal dimensions, which together capture the overall dynamics of the scene. Learning meaningful spatio-temporal representation is at the core of most video analysis tasks like video retrieval, action recognition, temporal and spatial action localization, object motion analysis, video captioning, and modelling of human-object interactions.
There is a fundamental need for methods to learn generalized spatio-temporal representations that can work effectively for multiple downstream tasks.
One of the popular approaches is to train a model for video action recognition and obtain the implicitly learned video representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Recently, many self-supervised methods have been proposed, where a deep network is trained for an auxiliary pre-text task to learn rich spatio-temporal representations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Object motion understanding is crucial to learn rich spatio-temporal representations as it provides insights about the natural motion pattern of objects in the world and how they interact with other objects in the scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. For instance, consider the example of a video where a person is shooting a ball towards the goalpost as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Analysing the motion of the ball during this action will provide insight about the most likely motion of the soccer ball: just after kicking, the ball will follow a projectile motion in the air, and after dropping on floor the ball will bounce a few times. This motion pattern of a relatively common occurrences in everyday life is extremely complex to model in a mathematical or mechanical sense as it comprises, for instance in the above example, movement of the player’s body and real world forces (friction, air drag) at play.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we present a method of analysing the underlying directional information of object motions that occur in real-world human actions like kicking, walking, jumping, clapping, etc., by estimating the object motion type in a video. As it is difficult to jointly model motions of all the objects in the scene, we focus only on the dominant motion in the video. To this end, we have formulated a classification problem, to classify the directional motion pattern into one of the defined classes. Based on our internal study on action classes present in popular video dataset HMDB<math id="S1.p4.1.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S1.p4.1.m1.1a"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><cn type="integer" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">51</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, we have defined five primitive motion type classes: <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">linear, projectile, oscillatory, local and random.</em> According to us, most of the real world human actions can be assigned to one of the above defined motion types. For instance: <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">walking, running, bike-riding</em> have a linear motion type as the dominant motion, <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">kicking, cartwheel</em> makes projectile motion, and <em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">talking, chewing, smoking</em> have a local motion type. All the motion patterns having periodic motion are considered under oscillatory class, for example, <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">pushup and exercise.</em> The actions which do not lie into any of these categories were assigned the class random. To our knowledge, there is no open-source video dataset currently available with motion type labels for videos. To this end, we have added motion type annotations to the HMD51 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> dataset for training the motion classifier. The motivation of this work is to address the following:<em id="S1.p4.1.6" class="ltx_emph ltx_font_italic">1) Is it possible for a neural network model to perform well on the task of motion type classification? 2) What internal feature representations does the model learn in this process? 3) Are these learned features generalize well on other downstream video understanding tasks?</em> We have tried to answer these questions throughout this paper by training a CNN model for motion type classification and analyzing its learned features through general video analysis tasks like video retrieval.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We also demonstrate an exciting use-case of the above-presented motion type classification method: video playback style recommendation, which boosts the overall aesthetics of the videos. A few common playback styles include: Reverse (temporally reversing the video), Loop (repeating the video in a loop), Boomerang (playing a concatenated video of normal and reverse). Finding a suitable playback style is often a time-consuming process where a user manually applies each available playback style. This created a space to engineer automated tools for this problem. Our proposed solution tries to automate this process of playback style selection. More details for the design of this recommendation algorithm are presented in Sec. 3.2.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.3" class="ltx_p">Lastly, we show that through the proposed motion type classification, we are able to learn rich spatio-temporal representations that generalize well for other video analysis tasks such as video retrieval. In a subjective evaluation of the learned representations for video retrieval, we achieved promising results on the HMDB<math id="S1.p6.1.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S1.p6.1.m1.1a"><mn id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><cn type="integer" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">51</annotation></semantics></math> dataset. Furthermore, we made specific design choices to make the network efficient for mobile deployment. Our model for motion classification has inference time of <math id="S1.p6.2.m2.1" class="ltx_Math" alttext="200ms" display="inline"><semantics id="S1.p6.2.m2.1a"><mrow id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml"><mn id="S1.p6.2.m2.1.1.2" xref="S1.p6.2.m2.1.1.2.cmml">200</mn><mo lspace="0em" rspace="0em" id="S1.p6.2.m2.1.1.1" xref="S1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S1.p6.2.m2.1.1.3" xref="S1.p6.2.m2.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S1.p6.2.m2.1.1.1a" xref="S1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S1.p6.2.m2.1.1.4" xref="S1.p6.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><apply id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1"><times id="S1.p6.2.m2.1.1.1.cmml" xref="S1.p6.2.m2.1.1.1"></times><cn type="integer" id="S1.p6.2.m2.1.1.2.cmml" xref="S1.p6.2.m2.1.1.2">200</cn><ci id="S1.p6.2.m2.1.1.3.cmml" xref="S1.p6.2.m2.1.1.3">𝑚</ci><ci id="S1.p6.2.m2.1.1.4.cmml" xref="S1.p6.2.m2.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">200ms</annotation></semantics></math> for a <math id="S1.p6.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S1.p6.3.m3.1a"><mn id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><cn type="integer" id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">10</annotation></semantics></math> second video clip on a Samsung S20 phone.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We summarize our major contributions as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A neural network for understanding object motion in videos by classifying object motion type into one of the five primitive motion classes: <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">linear, projectile, oscillatory, local and random.</em></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A light-weight network for video representation learning that is suitable for real-time execution on mobile devices.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A recommender system to predict suitable video playback style for videos by analysing predicted object motion patterns</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2110.01015/assets/fig2_e.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall network architecture for motion type classification. Given an input video, we divide it temporally into three segments and extract one central frame from each segment. These three frames are fed to the Feature Extractor Network, and the extracted features are then averaged to obtain a <math id="S2.F2.2.m1.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S2.F2.2.m1.1b"><mn id="S2.F2.2.m1.1.1" xref="S2.F2.2.m1.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S2.F2.2.m1.1c"><cn type="integer" id="S2.F2.2.m1.1.1.cmml" xref="S2.F2.2.m1.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.m1.1d">1280</annotation></semantics></math>-dimensional (1280D) feature vector, which is used for motion type classification</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Video action recognition has been studied extensively by computer vision community. The success of video action recognition majorly depends on crafting the spatio-temporal features in the video representations. Traditionally, video features are extracted from optical-flow based motion information in the videos, e.g. Motion Boundary Histograms (MBH) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and trajectories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Histograms Of Flow (HOF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> or spatiotemporal oriented filtering such as HOG3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Cuboids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Spatiotemporal Oriented Energies (SOEs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
The resounding success of Convolutional Neural Networks (CNNs) for image processing applications has caused its extension to video processing problems as well. Just like the spatial features, deep CNNs are also capable to extract accurate temporal information as well e.g. FlowNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Both the temporal and spatial information are important in various video recognition tasks. Simonyan and Zisserman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> has proposed a two-stream CNN architecture to incorporate both spatial and temporal features of the videos. The spatial features are captured by passing the RGB frames of the videos and the temporal features are captured by extracting the flow frames. Several other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> have explored the different effective fusion options of two streams - flow and RGB streams.
The major bottleneck in two-stream networks as well as optical flow based methods is the optical flow extraction step as it consumes a lot of time and hence the inference time increases.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.13" class="ltx_p">DMC-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> approximates the flow using a reconstruction loss and an adversarial loss jointly for the task of action classification. This model is two folds faster than the state-of-the-art methods and achieves accuracy close to the methods using optical flow information. The study of Tran <em id="S2.p2.13.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.13.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> shows the effectiveness of using <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mn id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">3</cn><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">3D</annotation></semantics></math>-CNNs instead of <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S2.p2.2.m2.1a"><mrow id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mn id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.p2.2.m2.1.1.1" xref="S2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><times id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1.1"></times><cn type="integer" id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">2</cn><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">2D</annotation></semantics></math>-CNNs to model both spatial and temporal features together in a single branch. Although <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.3.m3.1a"><mrow id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mn id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.3.m3.1.1.1" xref="S2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><times id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1"></times><cn type="integer" id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">3</cn><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">3D</annotation></semantics></math>-CNNs produce promising results, it is much more expensive than <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S2.p2.4.m4.1a"><mrow id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mn id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.p2.4.m4.1.1.1" xref="S2.p2.4.m4.1.1.1.cmml">​</mo><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><times id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1.1"></times><cn type="integer" id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">2</cn><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">2D</annotation></semantics></math>-CNNs. Experiments by Xie <em id="S2.p2.13.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.13.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> showed that we can trade-off accuracy and speed by replacing some <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mn id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.5.m5.1.1.1" xref="S2.p2.5.m5.1.1.1.cmml">​</mo><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><times id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1.1"></times><cn type="integer" id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">3</cn><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">3D</annotation></semantics></math> conv layers by <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mn id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml">​</mo><mi id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><times id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1"></times><cn type="integer" id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">2</cn><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">2D</annotation></semantics></math> convolutions. Having <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.7.m7.1a"><mrow id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mn id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.7.m7.1.1.1" xref="S2.p2.7.m7.1.1.1.cmml">​</mo><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><times id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1.1"></times><cn type="integer" id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">3</cn><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">3D</annotation></semantics></math> conv layers at the higher layers and <math id="S2.p2.8.m8.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S2.p2.8.m8.1a"><mrow id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><mn id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.p2.8.m8.1.1.1" xref="S2.p2.8.m8.1.1.1.cmml">​</mo><mi id="S2.p2.8.m8.1.1.3" xref="S2.p2.8.m8.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><times id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1.1"></times><cn type="integer" id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2">2</cn><ci id="S2.p2.8.m8.1.1.3.cmml" xref="S2.p2.8.m8.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">2D</annotation></semantics></math> conv layers at the lower part of the network is faster and this configuration surprisingly has higher accuracy. They also propose separable <math id="S2.p2.9.m9.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.9.m9.1a"><mrow id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml"><mn id="S2.p2.9.m9.1.1.2" xref="S2.p2.9.m9.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.9.m9.1.1.1" xref="S2.p2.9.m9.1.1.1.cmml">​</mo><mi id="S2.p2.9.m9.1.1.3" xref="S2.p2.9.m9.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.1b"><apply id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1"><times id="S2.p2.9.m9.1.1.1.cmml" xref="S2.p2.9.m9.1.1.1"></times><cn type="integer" id="S2.p2.9.m9.1.1.2.cmml" xref="S2.p2.9.m9.1.1.2">3</cn><ci id="S2.p2.9.m9.1.1.3.cmml" xref="S2.p2.9.m9.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.1c">3D</annotation></semantics></math>-CNN (S3D) configuration which separates spatial and temporal <math id="S2.p2.10.m10.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.10.m10.1a"><mrow id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml"><mn id="S2.p2.10.m10.1.1.2" xref="S2.p2.10.m10.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.10.m10.1.1.1" xref="S2.p2.10.m10.1.1.1.cmml">​</mo><mi id="S2.p2.10.m10.1.1.3" xref="S2.p2.10.m10.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.1b"><apply id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1"><times id="S2.p2.10.m10.1.1.1.cmml" xref="S2.p2.10.m10.1.1.1"></times><cn type="integer" id="S2.p2.10.m10.1.1.2.cmml" xref="S2.p2.10.m10.1.1.2">3</cn><ci id="S2.p2.10.m10.1.1.3.cmml" xref="S2.p2.10.m10.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.1c">3D</annotation></semantics></math> convolutions. MARS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> introduces the learning approaches to train <math id="S2.p2.11.m11.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.11.m11.1a"><mrow id="S2.p2.11.m11.1.1" xref="S2.p2.11.m11.1.1.cmml"><mn id="S2.p2.11.m11.1.1.2" xref="S2.p2.11.m11.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.1.1.1" xref="S2.p2.11.m11.1.1.1.cmml">​</mo><mi id="S2.p2.11.m11.1.1.3" xref="S2.p2.11.m11.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.1b"><apply id="S2.p2.11.m11.1.1.cmml" xref="S2.p2.11.m11.1.1"><times id="S2.p2.11.m11.1.1.1.cmml" xref="S2.p2.11.m11.1.1.1"></times><cn type="integer" id="S2.p2.11.m11.1.1.2.cmml" xref="S2.p2.11.m11.1.1.2">3</cn><ci id="S2.p2.11.m11.1.1.3.cmml" xref="S2.p2.11.m11.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.1c">3D</annotation></semantics></math>-CNN operating on RGB frames which mimics the motion stream. It eradicates the need of flow extraction during the inference time. Frame sampling from videos is also an important part in video processing. Temporal Segment Network (TSN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> works on sparse temporal snippets. The videos are split into k chunks and a small snippet in chosen from each of the chunk. The chunks are processed individually and at the end the decisions are aggregated as per the consensus function to come to the final conclusion. TSN gives promising result for action recognition task. Lin <em id="S2.p2.13.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.13.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> proposes a generic module called Temporal Shift module (TSM). It is a ”plug and play” module in a network designed for video understanding task. TSM has high efficiency and high performance. It maintains the complexity of <math id="S2.p2.12.m12.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S2.p2.12.m12.1a"><mrow id="S2.p2.12.m12.1.1" xref="S2.p2.12.m12.1.1.cmml"><mn id="S2.p2.12.m12.1.1.2" xref="S2.p2.12.m12.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.p2.12.m12.1.1.1" xref="S2.p2.12.m12.1.1.1.cmml">​</mo><mi id="S2.p2.12.m12.1.1.3" xref="S2.p2.12.m12.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.12.m12.1b"><apply id="S2.p2.12.m12.1.1.cmml" xref="S2.p2.12.m12.1.1"><times id="S2.p2.12.m12.1.1.1.cmml" xref="S2.p2.12.m12.1.1.1"></times><cn type="integer" id="S2.p2.12.m12.1.1.2.cmml" xref="S2.p2.12.m12.1.1.2">2</cn><ci id="S2.p2.12.m12.1.1.3.cmml" xref="S2.p2.12.m12.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.12.m12.1c">2D</annotation></semantics></math>-CNN and performance of the <math id="S2.p2.13.m13.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S2.p2.13.m13.1a"><mrow id="S2.p2.13.m13.1.1" xref="S2.p2.13.m13.1.1.cmml"><mn id="S2.p2.13.m13.1.1.2" xref="S2.p2.13.m13.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.p2.13.m13.1.1.1" xref="S2.p2.13.m13.1.1.1.cmml">​</mo><mi id="S2.p2.13.m13.1.1.3" xref="S2.p2.13.m13.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.13.m13.1b"><apply id="S2.p2.13.m13.1.1.cmml" xref="S2.p2.13.m13.1.1"><times id="S2.p2.13.m13.1.1.1.cmml" xref="S2.p2.13.m13.1.1.1"></times><cn type="integer" id="S2.p2.13.m13.1.1.2.cmml" xref="S2.p2.13.m13.1.1.2">3</cn><ci id="S2.p2.13.m13.1.1.3.cmml" xref="S2.p2.13.m13.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.13.m13.1c">3D</annotation></semantics></math>-CNN. TSM facilitates the information exchange by shifting a part of the channels along temporal dimension.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Object motion pattern understanding is crucial for learning strong spatio-temporal features for downstream video analysis tasks  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. There are approaches which try to capture the object motions in the videos via learning flow features from the videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. These methods predict pixel-level feature maps for every time frame in the video, which essentially captures only local motion patterns.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Most of the methods discussed above are based on the supervised learning technique. But due to the scarcity of publicly available labeled dataset, it is difficult to train deep networks with supervised learning. Several Self-supervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for video tasks have been studied by the computer vision community. Qian proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> self-supervised Contrastive Video Representation Learning (<span id="S2.p4.1.1" class="ltx_text ltx_font_italic">CVRL</span>) method which uses the contrastive loss to map the video clips in the embedding space. It is desired that in the embedding space the distance between two clips from the same video is lesser than the clips from different videos. Jenni <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> introduced a novel self-supervised framework to learn video representations which are sensitive to the changes in the motion dynamics. They have observed that the motion of objects is essential for action recognition tasks. In the proposed work, we build on the above intuition to show that a deep network can learn rich representations by training for motion classification.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">Humans largely use primary motion cues like underlying object motion patterns to understand video semantics like actions or events in a scene. To perform well on video analysis tasks like action recognition and localization, the motion pattern representations require a semantic understanding of both the appearance and dynamics features of the video. We aim to learn rich spatio-temporal video representations through classification of the motion type based on the directional motion information present in the video. To this end, we trained a motion type classification model that classifies a video into one of the following five primitive classes we define: <em id="S3.p1.2.1" class="ltx_emph ltx_font_italic">linear, oscillatory, local, projectile, and random</em>. We observed that the trajectories of most natural object motions that we encounter in the real-world can be categorized into the first four motion classes. As it is difficult to jointly model motions of all the objects in the scene, we focus only on the dominant motion in the video. For instance, actions such as <em id="S3.p1.2.2" class="ltx_emph ltx_font_italic">walk</em> and <em id="S3.p1.2.3" class="ltx_emph ltx_font_italic">run</em> usually follow a linear trajectory and have a dominant linear motion. Many activities that we perform indoors have motion in only small local regions like <em id="S3.p1.2.4" class="ltx_emph ltx_font_italic">eat, drink, chew, talk</em>. Some of the examples of actions having dominant oscillatory motion type are <em id="S3.p1.2.5" class="ltx_emph ltx_font_italic">dribble, cartwheel and sit-up</em>. <em id="S3.p1.2.6" class="ltx_emph ltx_font_italic">Catch, throw, golf</em> are examples for dominant projectile motion type. Actions which do not follow any of these directional patterns, are considered random, for instance <em id="S3.p1.2.7" class="ltx_emph ltx_font_italic">dance and fight</em>. Some of the common real-world actions and their corresponding motion types are shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. To validate the quality of our learned representations, we used these representations for video retrieval task as explained in Sec. <a href="#S4.SS4" title="4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. As there is no publicly available video dataset with motion type labels, we have annotated the HMDB<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">51</annotation></semantics></math> dataset with motion type labels to obtain mHMDB51 dataset as seen in Sec. <a href="#S4.SS1" title="4.1 Dataset ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. The core of our method is a Deep Convolutional Neural Network (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), which is trained in a supervised fashion on mHMDB<math id="S3.p1.2.m2.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S3.p1.2.m2.1a"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">51</annotation></semantics></math> dataset for a five class motion-type classification problem.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mapping of real world actions to motion type and Video Playback Style in the mHMDB<math id="S3.T1.2.2.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S3.T1.2.2.m1.1b"><mn id="S3.T1.2.2.m1.1.1" xref="S3.T1.2.2.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.m1.1c"><cn type="integer" id="S3.T1.2.2.m1.1.1.cmml" xref="S3.T1.2.2.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.m1.1d">51</annotation></semantics></math> dataset.</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<th id="S3.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Example Action</th>
<th id="S3.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Motion Type</th>
<th id="S3.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Playback Style</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.2.1" class="ltx_tr">
<td id="S3.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Walk, Run</td>
<td id="S3.T1.3.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Linear</td>
<td id="S3.T1.3.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Reverse</td>
</tr>
<tr id="S3.T1.3.3.2" class="ltx_tr">
<td id="S3.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Dive, Throw</td>
<td id="S3.T1.3.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
<td id="S3.T1.3.3.2.3" class="ltx_td ltx_align_left ltx_border_r">Boomerang</td>
</tr>
<tr id="S3.T1.3.4.3" class="ltx_tr">
<td id="S3.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Eat, Clap</td>
<td id="S3.T1.3.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
<td id="S3.T1.3.4.3.3" class="ltx_td ltx_align_left ltx_border_r">Loop</td>
</tr>
<tr id="S3.T1.3.5.4" class="ltx_tr">
<td id="S3.T1.3.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">PullUp</td>
<td id="S3.T1.3.5.4.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
<td id="S3.T1.3.5.4.3" class="ltx_td ltx_align_left ltx_border_r">Loop</td>
</tr>
<tr id="S3.T1.3.6.5" class="ltx_tr">
<td id="S3.T1.3.6.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Dance, Fight</td>
<td id="S3.T1.3.6.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Random</td>
<td id="S3.T1.3.6.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Random</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Network Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.29" class="ltx_p">Most state-of-the-art networks for video representation learning and action recognition methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> rely on <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">3</cn><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">3D</annotation></semantics></math> convolutions due to their ability to jointly learn both spatial and temporal features. However, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">3</cn><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">3D</annotation></semantics></math> convolutions have significantly higher computational cost than <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">2</cn><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">2D</annotation></semantics></math> convolutions, which make them unsuitable for mobile applications that have strict power and latency constraints. Our network uses a backbone of only <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mn id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></times><cn type="integer" id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">2</cn><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">2D</annotation></semantics></math> convolutions with added Temporal Shift Modules (TSM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to facilitate an information exchange between temporally adjacent frames. This results in a light-weight network architecture that needs very limited memory and compute requirement. The proposed network architecture is shown in Fig.<a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our network is inspired by TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> architecture, where a set of frames is sampled from a video and processed independently. Finally, a consensus is taken to obtain a global feature representation. We first divide the input video temporally into <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">T</annotation></semantics></math> segments of equal durations and one representative central frame is sampled from each segment. The input of our model is thus a <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="T*N*N" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">∗</mo><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.6.m6.1.1.1a" xref="S3.SS1.p1.6.m6.1.1.1.cmml">∗</mo><mi id="S3.SS1.p1.6.m6.1.1.4" xref="S3.SS1.p1.6.m6.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><times id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></times><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑇</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑁</ci><ci id="S3.SS1.p1.6.m6.1.1.4.cmml" xref="S3.SS1.p1.6.m6.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">T*N*N</annotation></semantics></math> volume, where <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">T</annotation></semantics></math> is the number of segments from the video and <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">N</annotation></semantics></math> is both the height and the width of the video. The input volume is passed through a TSN-style backbone network to obtain a <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="T*1280" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">∗</mo><mn id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">1280</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><times id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1"></times><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝑇</ci><cn type="integer" id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">1280</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">T*1280</annotation></semantics></math> shape feature representation. The obtained feature vector is then averaged over the temporal dimension to obtain a combined 1280-dimension feature vector for the entire video. This global video feature vector is then fed into a classifier head having two fully connected layers with <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mn id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><cn type="integer" id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">128</annotation></semantics></math> and <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mn id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><cn type="integer" id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">64</annotation></semantics></math> neurons respectively, followed by a softmax layer for classification.
The working of the original TSN architecture is explained by the equation <a href="#S3.E1" title="In 3.1 Network Architecture ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The video <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">V</annotation></semantics></math> is divided into <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><mi id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><ci id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">K</annotation></semantics></math> segments {<math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="S_{1}" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><msub id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml"><mi id="S3.SS1.p1.14.m14.1.1.2" xref="S3.SS1.p1.14.m14.1.1.2.cmml">S</mi><mn id="S3.SS1.p1.14.m14.1.1.3" xref="S3.SS1.p1.14.m14.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m14.1.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p1.14.m14.1.1.2.cmml" xref="S3.SS1.p1.14.m14.1.1.2">𝑆</ci><cn type="integer" id="S3.SS1.p1.14.m14.1.1.3.cmml" xref="S3.SS1.p1.14.m14.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">S_{1}</annotation></semantics></math>,<math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="S_{2}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><msub id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">S</mi><mn id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">𝑆</ci><cn type="integer" id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">S_{2}</annotation></semantics></math>, …, <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="S_{K}" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><msub id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml"><mi id="S3.SS1.p1.16.m16.1.1.2" xref="S3.SS1.p1.16.m16.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.16.m16.1.1.3" xref="S3.SS1.p1.16.m16.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><apply id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2">𝑆</ci><ci id="S3.SS1.p1.16.m16.1.1.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">S_{K}</annotation></semantics></math>} of equal duration and (<math id="S3.SS1.p1.17.m17.1" class="ltx_Math" alttext="T_{1}" display="inline"><semantics id="S3.SS1.p1.17.m17.1a"><msub id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml"><mi id="S3.SS1.p1.17.m17.1.1.2" xref="S3.SS1.p1.17.m17.1.1.2.cmml">T</mi><mn id="S3.SS1.p1.17.m17.1.1.3" xref="S3.SS1.p1.17.m17.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><apply id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.17.m17.1.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p1.17.m17.1.1.2.cmml" xref="S3.SS1.p1.17.m17.1.1.2">𝑇</ci><cn type="integer" id="S3.SS1.p1.17.m17.1.1.3.cmml" xref="S3.SS1.p1.17.m17.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">T_{1}</annotation></semantics></math>,<math id="S3.SS1.p1.18.m18.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S3.SS1.p1.18.m18.1a"><msub id="S3.SS1.p1.18.m18.1.1" xref="S3.SS1.p1.18.m18.1.1.cmml"><mi id="S3.SS1.p1.18.m18.1.1.2" xref="S3.SS1.p1.18.m18.1.1.2.cmml">T</mi><mn id="S3.SS1.p1.18.m18.1.1.3" xref="S3.SS1.p1.18.m18.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m18.1b"><apply id="S3.SS1.p1.18.m18.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m18.1.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1">subscript</csymbol><ci id="S3.SS1.p1.18.m18.1.1.2.cmml" xref="S3.SS1.p1.18.m18.1.1.2">𝑇</ci><cn type="integer" id="S3.SS1.p1.18.m18.1.1.3.cmml" xref="S3.SS1.p1.18.m18.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m18.1c">T_{2}</annotation></semantics></math>, …, <math id="S3.SS1.p1.19.m19.1" class="ltx_Math" alttext="T_{K}" display="inline"><semantics id="S3.SS1.p1.19.m19.1a"><msub id="S3.SS1.p1.19.m19.1.1" xref="S3.SS1.p1.19.m19.1.1.cmml"><mi id="S3.SS1.p1.19.m19.1.1.2" xref="S3.SS1.p1.19.m19.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.19.m19.1.1.3" xref="S3.SS1.p1.19.m19.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m19.1b"><apply id="S3.SS1.p1.19.m19.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1">subscript</csymbol><ci id="S3.SS1.p1.19.m19.1.1.2.cmml" xref="S3.SS1.p1.19.m19.1.1.2">𝑇</ci><ci id="S3.SS1.p1.19.m19.1.1.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m19.1c">T_{K}</annotation></semantics></math>) are the sequence of snippets where each <math id="S3.SS1.p1.20.m20.1" class="ltx_Math" alttext="T_{K}" display="inline"><semantics id="S3.SS1.p1.20.m20.1a"><msub id="S3.SS1.p1.20.m20.1.1" xref="S3.SS1.p1.20.m20.1.1.cmml"><mi id="S3.SS1.p1.20.m20.1.1.2" xref="S3.SS1.p1.20.m20.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.20.m20.1.1.3" xref="S3.SS1.p1.20.m20.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.20.m20.1b"><apply id="S3.SS1.p1.20.m20.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.20.m20.1.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1">subscript</csymbol><ci id="S3.SS1.p1.20.m20.1.1.2.cmml" xref="S3.SS1.p1.20.m20.1.1.2">𝑇</ci><ci id="S3.SS1.p1.20.m20.1.1.3.cmml" xref="S3.SS1.p1.20.m20.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.20.m20.1c">T_{K}</annotation></semantics></math> is sampled from its corresponding segment <math id="S3.SS1.p1.21.m21.1" class="ltx_Math" alttext="S_{K}" display="inline"><semantics id="S3.SS1.p1.21.m21.1a"><msub id="S3.SS1.p1.21.m21.1.1" xref="S3.SS1.p1.21.m21.1.1.cmml"><mi id="S3.SS1.p1.21.m21.1.1.2" xref="S3.SS1.p1.21.m21.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.21.m21.1.1.3" xref="S3.SS1.p1.21.m21.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.21.m21.1b"><apply id="S3.SS1.p1.21.m21.1.1.cmml" xref="S3.SS1.p1.21.m21.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.21.m21.1.1.1.cmml" xref="S3.SS1.p1.21.m21.1.1">subscript</csymbol><ci id="S3.SS1.p1.21.m21.1.1.2.cmml" xref="S3.SS1.p1.21.m21.1.1.2">𝑆</ci><ci id="S3.SS1.p1.21.m21.1.1.3.cmml" xref="S3.SS1.p1.21.m21.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.21.m21.1c">S_{K}</annotation></semantics></math>. <math id="S3.SS1.p1.22.m22.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS1.p1.22.m22.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.22.m22.1.1" xref="S3.SS1.p1.22.m22.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.22.m22.1b"><ci id="S3.SS1.p1.22.m22.1.1.cmml" xref="S3.SS1.p1.22.m22.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.22.m22.1c">\mathcal{F}</annotation></semantics></math>(<math id="S3.SS1.p1.23.m23.1" class="ltx_Math" alttext="T_{K}" display="inline"><semantics id="S3.SS1.p1.23.m23.1a"><msub id="S3.SS1.p1.23.m23.1.1" xref="S3.SS1.p1.23.m23.1.1.cmml"><mi id="S3.SS1.p1.23.m23.1.1.2" xref="S3.SS1.p1.23.m23.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.23.m23.1.1.3" xref="S3.SS1.p1.23.m23.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.23.m23.1b"><apply id="S3.SS1.p1.23.m23.1.1.cmml" xref="S3.SS1.p1.23.m23.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.23.m23.1.1.1.cmml" xref="S3.SS1.p1.23.m23.1.1">subscript</csymbol><ci id="S3.SS1.p1.23.m23.1.1.2.cmml" xref="S3.SS1.p1.23.m23.1.1.2">𝑇</ci><ci id="S3.SS1.p1.23.m23.1.1.3.cmml" xref="S3.SS1.p1.23.m23.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.23.m23.1c">T_{K}</annotation></semantics></math>; <math id="S3.SS1.p1.24.m24.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.p1.24.m24.1a"><mi id="S3.SS1.p1.24.m24.1.1" xref="S3.SS1.p1.24.m24.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.24.m24.1b"><ci id="S3.SS1.p1.24.m24.1.1.cmml" xref="S3.SS1.p1.24.m24.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.24.m24.1c">W</annotation></semantics></math>) defines the output after passing the snippet <math id="S3.SS1.p1.25.m25.1" class="ltx_Math" alttext="T_{K}" display="inline"><semantics id="S3.SS1.p1.25.m25.1a"><msub id="S3.SS1.p1.25.m25.1.1" xref="S3.SS1.p1.25.m25.1.1.cmml"><mi id="S3.SS1.p1.25.m25.1.1.2" xref="S3.SS1.p1.25.m25.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.25.m25.1.1.3" xref="S3.SS1.p1.25.m25.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.25.m25.1b"><apply id="S3.SS1.p1.25.m25.1.1.cmml" xref="S3.SS1.p1.25.m25.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.25.m25.1.1.1.cmml" xref="S3.SS1.p1.25.m25.1.1">subscript</csymbol><ci id="S3.SS1.p1.25.m25.1.1.2.cmml" xref="S3.SS1.p1.25.m25.1.1.2">𝑇</ci><ci id="S3.SS1.p1.25.m25.1.1.3.cmml" xref="S3.SS1.p1.25.m25.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.25.m25.1c">T_{K}</annotation></semantics></math> through the ConvNet with parameters <math id="S3.SS1.p1.26.m26.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.p1.26.m26.1a"><mi id="S3.SS1.p1.26.m26.1.1" xref="S3.SS1.p1.26.m26.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.26.m26.1b"><ci id="S3.SS1.p1.26.m26.1.1.cmml" xref="S3.SS1.p1.26.m26.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.26.m26.1c">W</annotation></semantics></math>. The consensus module <math id="S3.SS1.p1.27.m27.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.SS1.p1.27.m27.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.27.m27.1.1" xref="S3.SS1.p1.27.m27.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.27.m27.1b"><ci id="S3.SS1.p1.27.m27.1.1.cmml" xref="S3.SS1.p1.27.m27.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.27.m27.1c">\mathcal{G}</annotation></semantics></math> combines the extracted features of all the snippets through <math id="S3.SS1.p1.28.m28.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS1.p1.28.m28.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.28.m28.1.1" xref="S3.SS1.p1.28.m28.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.28.m28.1b"><ci id="S3.SS1.p1.28.m28.1.1.cmml" xref="S3.SS1.p1.28.m28.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.28.m28.1c">\mathcal{F}</annotation></semantics></math> operation. The consensus module for our architecture takes the average of the features. The average output of consensus module is passed through the fully-connected layer with a softmax at the end to get the final class label. This operation is defined by <math id="S3.SS1.p1.29.m29.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS1.p1.29.m29.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.29.m29.1.1" xref="S3.SS1.p1.29.m29.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.29.m29.1b"><ci id="S3.SS1.p1.29.m29.1.1.cmml" xref="S3.SS1.p1.29.m29.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.29.m29.1c">\mathcal{H}</annotation></semantics></math> in the equation <a href="#S3.E1" title="In 3.1 Network Architecture ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.51" class="ltx_Math" alttext="\begin{split}TSN(T_{1},T_{2},\dots,T_{K})=\mathcal{H}(\mathcal{G}(\mathcal{F}(T_{1};W),\\
\mathcal{F}(T_{2};W),\dots,\mathcal{F}(T_{K};W)))\end{split}" display="block"><semantics id="S3.E1.m1.51a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.47.47" xref="S3.E1.m1.51.51.4.cmml"><mtr id="S3.E1.m1.47.47a" xref="S3.E1.m1.51.51.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.47.47b" xref="S3.E1.m1.51.51.4.cmml"><mrow id="S3.E1.m1.28.28.28.28.28" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">T</mi><mi id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">S</mi><mi id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">N</mi><mrow id="S3.E1.m1.28.28.28.28.28.29" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.51.51.4.cmml">(</mo><msub id="S3.E1.m1.28.28.28.28.28.29.1" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.cmml">T</mi><mn id="S3.E1.m1.6.6.6.6.6.6.1" xref="S3.E1.m1.6.6.6.6.6.6.1.cmml">1</mn></msub><mo id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.51.51.4.cmml">,</mo><msub id="S3.E1.m1.28.28.28.28.28.29.2" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">T</mi><mn id="S3.E1.m1.9.9.9.9.9.9.1" xref="S3.E1.m1.9.9.9.9.9.9.1.cmml">2</mn></msub><mo id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.51.51.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">…</mi><mo id="S3.E1.m1.12.12.12.12.12.12" xref="S3.E1.m1.51.51.4.cmml">,</mo><msub id="S3.E1.m1.28.28.28.28.28.29.3" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">T</mi><mi id="S3.E1.m1.14.14.14.14.14.14.1" xref="S3.E1.m1.14.14.14.14.14.14.1.cmml">K</mi></msub><mo stretchy="false" id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow><mo id="S3.E1.m1.16.16.16.16.16.16" xref="S3.E1.m1.16.16.16.16.16.16.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.17.17.17.17.17.17" xref="S3.E1.m1.17.17.17.17.17.17.cmml">ℋ</mi><mrow id="S3.E1.m1.28.28.28.28.28.30" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.51.51.4.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.19.19.19.19.19.19.cmml">𝒢</mi><mrow id="S3.E1.m1.28.28.28.28.28.30.1" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.51.51.4.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.21.21.21.21.21.21" xref="S3.E1.m1.21.21.21.21.21.21.cmml">ℱ</mi><mrow id="S3.E1.m1.28.28.28.28.28.30.1.1" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.51.51.4.cmml">(</mo><msub id="S3.E1.m1.28.28.28.28.28.30.1.1.1" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.23.23.23.23.23.23" xref="S3.E1.m1.23.23.23.23.23.23.cmml">T</mi><mn id="S3.E1.m1.24.24.24.24.24.24.1" xref="S3.E1.m1.24.24.24.24.24.24.1.cmml">1</mn></msub><mo id="S3.E1.m1.25.25.25.25.25.25" xref="S3.E1.m1.51.51.4.cmml">;</mo><mi id="S3.E1.m1.26.26.26.26.26.26" xref="S3.E1.m1.26.26.26.26.26.26.cmml">W</mi><mo stretchy="false" id="S3.E1.m1.27.27.27.27.27.27" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow><mo id="S3.E1.m1.28.28.28.28.28.28" xref="S3.E1.m1.51.51.4.cmml">,</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.47.47c" xref="S3.E1.m1.51.51.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.47.47d" xref="S3.E1.m1.51.51.4.cmml"><mrow id="S3.E1.m1.47.47.47.19.19" xref="S3.E1.m1.51.51.4.cmml"><mrow id="S3.E1.m1.47.47.47.19.19.20" xref="S3.E1.m1.51.51.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.29.29.29.1.1.1" xref="S3.E1.m1.29.29.29.1.1.1.cmml">ℱ</mi><mrow id="S3.E1.m1.47.47.47.19.19.20.1" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.30.30.30.2.2.2" xref="S3.E1.m1.51.51.4.cmml">(</mo><msub id="S3.E1.m1.47.47.47.19.19.20.1.1" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.31.31.31.3.3.3" xref="S3.E1.m1.31.31.31.3.3.3.cmml">T</mi><mn id="S3.E1.m1.32.32.32.4.4.4.1" xref="S3.E1.m1.32.32.32.4.4.4.1.cmml">2</mn></msub><mo id="S3.E1.m1.33.33.33.5.5.5" xref="S3.E1.m1.51.51.4.cmml">;</mo><mi id="S3.E1.m1.34.34.34.6.6.6" xref="S3.E1.m1.34.34.34.6.6.6.cmml">W</mi><mo stretchy="false" id="S3.E1.m1.35.35.35.7.7.7" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow><mo id="S3.E1.m1.36.36.36.8.8.8" xref="S3.E1.m1.51.51.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.37.37.37.9.9.9" xref="S3.E1.m1.37.37.37.9.9.9.cmml">…</mi><mo id="S3.E1.m1.38.38.38.10.10.10" xref="S3.E1.m1.51.51.4.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.39.39.39.11.11.11" xref="S3.E1.m1.39.39.39.11.11.11.cmml">ℱ</mi><mrow id="S3.E1.m1.47.47.47.19.19.20.2" xref="S3.E1.m1.51.51.4.cmml"><mo stretchy="false" id="S3.E1.m1.40.40.40.12.12.12" xref="S3.E1.m1.51.51.4.cmml">(</mo><msub id="S3.E1.m1.47.47.47.19.19.20.2.1" xref="S3.E1.m1.51.51.4.cmml"><mi id="S3.E1.m1.41.41.41.13.13.13" xref="S3.E1.m1.41.41.41.13.13.13.cmml">T</mi><mi id="S3.E1.m1.42.42.42.14.14.14.1" xref="S3.E1.m1.42.42.42.14.14.14.1.cmml">K</mi></msub><mo id="S3.E1.m1.43.43.43.15.15.15" xref="S3.E1.m1.51.51.4.cmml">;</mo><mi id="S3.E1.m1.44.44.44.16.16.16" xref="S3.E1.m1.44.44.44.16.16.16.cmml">W</mi><mo stretchy="false" id="S3.E1.m1.45.45.45.17.17.17" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.46.46.46.18.18.18" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.47.47.47.19.19.19" xref="S3.E1.m1.51.51.4.cmml">)</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.51b"><apply id="S3.E1.m1.51.51.4.cmml" xref="S3.E1.m1.47.47"><eq id="S3.E1.m1.16.16.16.16.16.16.cmml" xref="S3.E1.m1.16.16.16.16.16.16"></eq><apply id="S3.E1.m1.50.50.3.3.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.50.50.3.3.4.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝑇</ci><ci id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">𝑆</ci><ci id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3">𝑁</ci><vector id="S3.E1.m1.50.50.3.3.3.4.cmml" xref="S3.E1.m1.47.47"><apply id="S3.E1.m1.48.48.1.1.1.1.1.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.1.1.1.1.1.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5">𝑇</ci><cn type="integer" id="S3.E1.m1.6.6.6.6.6.6.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1">1</cn></apply><apply id="S3.E1.m1.49.49.2.2.2.2.2.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.49.49.2.2.2.2.2.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">𝑇</ci><cn type="integer" id="S3.E1.m1.9.9.9.9.9.9.1.cmml" xref="S3.E1.m1.9.9.9.9.9.9.1">2</cn></apply><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">…</ci><apply id="S3.E1.m1.50.50.3.3.3.3.3.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.50.50.3.3.3.3.3.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13">𝑇</ci><ci id="S3.E1.m1.14.14.14.14.14.14.1.cmml" xref="S3.E1.m1.14.14.14.14.14.14.1">𝐾</ci></apply></vector></apply><apply id="S3.E1.m1.51.51.4.4.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.51.51.4.4.2.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.17.17.17.17.17.17.cmml" xref="S3.E1.m1.17.17.17.17.17.17">ℋ</ci><apply id="S3.E1.m1.51.51.4.4.1.1.1.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.51.51.4.4.1.1.1.4.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.19.19.19.19.19.19.cmml" xref="S3.E1.m1.19.19.19.19.19.19">𝒢</ci><vector id="S3.E1.m1.51.51.4.4.1.1.1.3.4.cmml" xref="S3.E1.m1.47.47"><apply id="S3.E1.m1.51.51.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.51.51.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.21.21.21.21.21.21.cmml" xref="S3.E1.m1.21.21.21.21.21.21">ℱ</ci><list id="S3.E1.m1.51.51.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.47.47"><apply id="S3.E1.m1.51.51.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.51.51.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.23.23.23.23.23.23.cmml" xref="S3.E1.m1.23.23.23.23.23.23">𝑇</ci><cn type="integer" id="S3.E1.m1.24.24.24.24.24.24.1.cmml" xref="S3.E1.m1.24.24.24.24.24.24.1">1</cn></apply><ci id="S3.E1.m1.26.26.26.26.26.26.cmml" xref="S3.E1.m1.26.26.26.26.26.26">𝑊</ci></list></apply><apply id="S3.E1.m1.51.51.4.4.1.1.1.2.2.2.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.51.51.4.4.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.29.29.29.1.1.1.cmml" xref="S3.E1.m1.29.29.29.1.1.1">ℱ</ci><list id="S3.E1.m1.51.51.4.4.1.1.1.2.2.2.1.2.cmml" xref="S3.E1.m1.47.47"><apply id="S3.E1.m1.51.51.4.4.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.51.51.4.4.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.31.31.31.3.3.3.cmml" xref="S3.E1.m1.31.31.31.3.3.3">𝑇</ci><cn type="integer" id="S3.E1.m1.32.32.32.4.4.4.1.cmml" xref="S3.E1.m1.32.32.32.4.4.4.1">2</cn></apply><ci id="S3.E1.m1.34.34.34.6.6.6.cmml" xref="S3.E1.m1.34.34.34.6.6.6">𝑊</ci></list></apply><ci id="S3.E1.m1.37.37.37.9.9.9.cmml" xref="S3.E1.m1.37.37.37.9.9.9">…</ci><apply id="S3.E1.m1.51.51.4.4.1.1.1.3.3.3.cmml" xref="S3.E1.m1.47.47"><times id="S3.E1.m1.51.51.4.4.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.47.47"></times><ci id="S3.E1.m1.39.39.39.11.11.11.cmml" xref="S3.E1.m1.39.39.39.11.11.11">ℱ</ci><list id="S3.E1.m1.51.51.4.4.1.1.1.3.3.3.1.2.cmml" xref="S3.E1.m1.47.47"><apply id="S3.E1.m1.51.51.4.4.1.1.1.3.3.3.1.1.1.cmml" xref="S3.E1.m1.47.47"><csymbol cd="ambiguous" id="S3.E1.m1.51.51.4.4.1.1.1.3.3.3.1.1.1.1.cmml" xref="S3.E1.m1.47.47">subscript</csymbol><ci id="S3.E1.m1.41.41.41.13.13.13.cmml" xref="S3.E1.m1.41.41.41.13.13.13">𝑇</ci><ci id="S3.E1.m1.42.42.42.14.14.14.1.cmml" xref="S3.E1.m1.42.42.42.14.14.14.1">𝐾</ci></apply><ci id="S3.E1.m1.44.44.44.16.16.16.cmml" xref="S3.E1.m1.44.44.44.16.16.16">𝑊</ci></list></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.51c">\begin{split}TSN(T_{1},T_{2},\dots,T_{K})=\mathcal{H}(\mathcal{G}(\mathcal{F}(T_{1};W),\\
\mathcal{F}(T_{2};W),\dots,\mathcal{F}(T_{K};W)))\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We added TSM modules in the backbone network to help the network learn strong temporal relations across segments via shifting of the intermediate feature channels of one segment to neighboring segments. To further reduce the computational complexity of our network, we have used MobileNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as the backbone due to its low computational cost. Our specific design choices for the network architecture makes it suitable for video processing on mobile devices having low compute budget.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Video Playback Style Recommendation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Applying a suitable playback style to a video can enhance a video and make it more likely to be shared.
Motion patterns present in the videos play an important role in selecting the most suited playback style for the video. For instance, for a video having linear motion like running, applying Boomerang type will make the video counter intuitive and hence interesting. To this end, we have designed a system for video playback style recommendation based on predictions from motion type classifier. We have considered three most widely used playback styles for recommendation namely Boomerang, Loop and Reverse. Specifically, we have introduced a mapping from motion type to a suitable playback style for an input video based on a user survey of 14 volunteers. In this study, we showed a few example actions for each motion type to each volunteer and asked them to select the best-suited playback style for that corresponding action. We aggregated the results from each volunteer and selected the most voted playback style for each motion type for the mapping. From the results of the study as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that the Reverse effect suits linear actions, and projectile motion looks good with a Boomerang effect. For both oscillatory and local motion, loop is the best-suited playback style. For random motion type, we randomly apply Boomerang, Reverse or Loop. We have performed a subjective study for evaluation of our video playback style recommendation system which is detailed in Sec. <a href="#S4.SS3" title="4.3 Video Playback Style Recommendation ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.14" class="ltx_p">We have done multiple experiments for comprehensive evaluation of our proposed motion type classifier model. In Section <a href="#S4.SS2" title="4.2 Motion Classifier ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> we perform an ablation with various pre-trained weights to examine the impact of weight initialization. To evaluate the quality of the learnt representations through motion classification, we have performed video retrieval as detailed in Sec. <a href="#S4.SS4" title="4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. We have also performed a subjective study for evaluation of our video playback recommendation system. To prepare our training data, each video was first resized: the smaller dimension was set to <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">256</annotation></semantics></math> pixels wide, and a random square region was cropped of side length <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">d</annotation></semantics></math> <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\epsilon</annotation></semantics></math> <math id="S4.p1.4.m4.4" class="ltx_Math" alttext="(256,224,192,169)" display="inline"><semantics id="S4.p1.4.m4.4a"><mrow id="S4.p1.4.m4.4.5.2" xref="S4.p1.4.m4.4.5.1.cmml"><mo stretchy="false" id="S4.p1.4.m4.4.5.2.1" xref="S4.p1.4.m4.4.5.1.cmml">(</mo><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">256</mn><mo id="S4.p1.4.m4.4.5.2.2" xref="S4.p1.4.m4.4.5.1.cmml">,</mo><mn id="S4.p1.4.m4.2.2" xref="S4.p1.4.m4.2.2.cmml">224</mn><mo id="S4.p1.4.m4.4.5.2.3" xref="S4.p1.4.m4.4.5.1.cmml">,</mo><mn id="S4.p1.4.m4.3.3" xref="S4.p1.4.m4.3.3.cmml">192</mn><mo id="S4.p1.4.m4.4.5.2.4" xref="S4.p1.4.m4.4.5.1.cmml">,</mo><mn id="S4.p1.4.m4.4.4" xref="S4.p1.4.m4.4.4.cmml">169</mn><mo stretchy="false" id="S4.p1.4.m4.4.5.2.5" xref="S4.p1.4.m4.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.4b"><vector id="S4.p1.4.m4.4.5.1.cmml" xref="S4.p1.4.m4.4.5.2"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">256</cn><cn type="integer" id="S4.p1.4.m4.2.2.cmml" xref="S4.p1.4.m4.2.2">224</cn><cn type="integer" id="S4.p1.4.m4.3.3.cmml" xref="S4.p1.4.m4.3.3">192</cn><cn type="integer" id="S4.p1.4.m4.4.4.cmml" xref="S4.p1.4.m4.4.4">169</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.4c">(256,224,192,169)</annotation></semantics></math> , followed by a random horizontal flip. Finally, the crop was resized to <math id="S4.p1.5.m5.2" class="ltx_Math" alttext="(256,256)" display="inline"><semantics id="S4.p1.5.m5.2a"><mrow id="S4.p1.5.m5.2.3.2" xref="S4.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.p1.5.m5.2.3.2.1" xref="S4.p1.5.m5.2.3.1.cmml">(</mo><mn id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">256</mn><mo id="S4.p1.5.m5.2.3.2.2" xref="S4.p1.5.m5.2.3.1.cmml">,</mo><mn id="S4.p1.5.m5.2.2" xref="S4.p1.5.m5.2.2.cmml">256</mn><mo stretchy="false" id="S4.p1.5.m5.2.3.2.3" xref="S4.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.2b"><interval closure="open" id="S4.p1.5.m5.2.3.1.cmml" xref="S4.p1.5.m5.2.3.2"><cn type="integer" id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">256</cn><cn type="integer" id="S4.p1.5.m5.2.2.cmml" xref="S4.p1.5.m5.2.2">256</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.2c">(256,256)</annotation></semantics></math> and the pixel values were normalized to the range <math id="S4.p1.6.m6.2" class="ltx_Math" alttext="(0,1)" display="inline"><semantics id="S4.p1.6.m6.2a"><mrow id="S4.p1.6.m6.2.3.2" xref="S4.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S4.p1.6.m6.2.3.2.1" xref="S4.p1.6.m6.2.3.1.cmml">(</mo><mn id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">0</mn><mo id="S4.p1.6.m6.2.3.2.2" xref="S4.p1.6.m6.2.3.1.cmml">,</mo><mn id="S4.p1.6.m6.2.2" xref="S4.p1.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S4.p1.6.m6.2.3.2.3" xref="S4.p1.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.2b"><interval closure="open" id="S4.p1.6.m6.2.3.1.cmml" xref="S4.p1.6.m6.2.3.2"><cn type="integer" id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">0</cn><cn type="integer" id="S4.p1.6.m6.2.2.cmml" xref="S4.p1.6.m6.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.2c">(0,1)</annotation></semantics></math>. In the testing phase, we resized the smaller dimension to <math id="S4.p1.7.m7.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.p1.7.m7.1a"><mn id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><cn type="integer" id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">256</annotation></semantics></math> and took a center crop. We used <math id="S4.p1.8.m8.1" class="ltx_Math" alttext="T=3" display="inline"><semantics id="S4.p1.8.m8.1a"><mrow id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml"><mi id="S4.p1.8.m8.1.1.2" xref="S4.p1.8.m8.1.1.2.cmml">T</mi><mo id="S4.p1.8.m8.1.1.1" xref="S4.p1.8.m8.1.1.1.cmml">=</mo><mn id="S4.p1.8.m8.1.1.3" xref="S4.p1.8.m8.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><apply id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1"><eq id="S4.p1.8.m8.1.1.1.cmml" xref="S4.p1.8.m8.1.1.1"></eq><ci id="S4.p1.8.m8.1.1.2.cmml" xref="S4.p1.8.m8.1.1.2">𝑇</ci><cn type="integer" id="S4.p1.8.m8.1.1.3.cmml" xref="S4.p1.8.m8.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">T=3</annotation></semantics></math> segments in all of our experiments unless mentioned otherwise, and sampled the temporally central frame from each segment. These three frames are the input to the network. For training, we used an initial learning rate of <math id="S4.p1.9.m9.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.p1.9.m9.1a"><mn id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><cn type="float" id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">0.001</annotation></semantics></math> and a learning rate schedule to reduce the learning rate by half after the <math id="S4.p1.10.m10.1" class="ltx_Math" alttext="20^{th}" display="inline"><semantics id="S4.p1.10.m10.1a"><msup id="S4.p1.10.m10.1.1" xref="S4.p1.10.m10.1.1.cmml"><mn id="S4.p1.10.m10.1.1.2" xref="S4.p1.10.m10.1.1.2.cmml">20</mn><mrow id="S4.p1.10.m10.1.1.3" xref="S4.p1.10.m10.1.1.3.cmml"><mi id="S4.p1.10.m10.1.1.3.2" xref="S4.p1.10.m10.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.10.m10.1.1.3.1" xref="S4.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.p1.10.m10.1.1.3.3" xref="S4.p1.10.m10.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p1.10.m10.1b"><apply id="S4.p1.10.m10.1.1.cmml" xref="S4.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.p1.10.m10.1.1.1.cmml" xref="S4.p1.10.m10.1.1">superscript</csymbol><cn type="integer" id="S4.p1.10.m10.1.1.2.cmml" xref="S4.p1.10.m10.1.1.2">20</cn><apply id="S4.p1.10.m10.1.1.3.cmml" xref="S4.p1.10.m10.1.1.3"><times id="S4.p1.10.m10.1.1.3.1.cmml" xref="S4.p1.10.m10.1.1.3.1"></times><ci id="S4.p1.10.m10.1.1.3.2.cmml" xref="S4.p1.10.m10.1.1.3.2">𝑡</ci><ci id="S4.p1.10.m10.1.1.3.3.cmml" xref="S4.p1.10.m10.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m10.1c">20^{th}</annotation></semantics></math> and <math id="S4.p1.11.m11.1" class="ltx_Math" alttext="40^{th}" display="inline"><semantics id="S4.p1.11.m11.1a"><msup id="S4.p1.11.m11.1.1" xref="S4.p1.11.m11.1.1.cmml"><mn id="S4.p1.11.m11.1.1.2" xref="S4.p1.11.m11.1.1.2.cmml">40</mn><mrow id="S4.p1.11.m11.1.1.3" xref="S4.p1.11.m11.1.1.3.cmml"><mi id="S4.p1.11.m11.1.1.3.2" xref="S4.p1.11.m11.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.11.m11.1.1.3.1" xref="S4.p1.11.m11.1.1.3.1.cmml">​</mo><mi id="S4.p1.11.m11.1.1.3.3" xref="S4.p1.11.m11.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p1.11.m11.1b"><apply id="S4.p1.11.m11.1.1.cmml" xref="S4.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.p1.11.m11.1.1.1.cmml" xref="S4.p1.11.m11.1.1">superscript</csymbol><cn type="integer" id="S4.p1.11.m11.1.1.2.cmml" xref="S4.p1.11.m11.1.1.2">40</cn><apply id="S4.p1.11.m11.1.1.3.cmml" xref="S4.p1.11.m11.1.1.3"><times id="S4.p1.11.m11.1.1.3.1.cmml" xref="S4.p1.11.m11.1.1.3.1"></times><ci id="S4.p1.11.m11.1.1.3.2.cmml" xref="S4.p1.11.m11.1.1.3.2">𝑡</ci><ci id="S4.p1.11.m11.1.1.3.3.cmml" xref="S4.p1.11.m11.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.11.m11.1c">40^{th}</annotation></semantics></math> epoch. The network was trained for a total of <math id="S4.p1.12.m12.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.p1.12.m12.1a"><mn id="S4.p1.12.m12.1.1" xref="S4.p1.12.m12.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.p1.12.m12.1b"><cn type="integer" id="S4.p1.12.m12.1.1.cmml" xref="S4.p1.12.m12.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.12.m12.1c">200</annotation></semantics></math> epochs. Stochastic Gradient Descent was used for optimization with momentum value of <math id="S4.p1.13.m13.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.p1.13.m13.1a"><mn id="S4.p1.13.m13.1.1" xref="S4.p1.13.m13.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.p1.13.m13.1b"><cn type="float" id="S4.p1.13.m13.1.1.cmml" xref="S4.p1.13.m13.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.13.m13.1c">0.9</annotation></semantics></math> and a weight decay of <math id="S4.p1.14.m14.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="S4.p1.14.m14.1a"><mrow id="S4.p1.14.m14.1.1" xref="S4.p1.14.m14.1.1.cmml"><mrow id="S4.p1.14.m14.1.1.2" xref="S4.p1.14.m14.1.1.2.cmml"><mn id="S4.p1.14.m14.1.1.2.2" xref="S4.p1.14.m14.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.p1.14.m14.1.1.2.1" xref="S4.p1.14.m14.1.1.2.1.cmml">​</mo><mi id="S4.p1.14.m14.1.1.2.3" xref="S4.p1.14.m14.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p1.14.m14.1.1.1" xref="S4.p1.14.m14.1.1.1.cmml">−</mo><mn id="S4.p1.14.m14.1.1.3" xref="S4.p1.14.m14.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.14.m14.1b"><apply id="S4.p1.14.m14.1.1.cmml" xref="S4.p1.14.m14.1.1"><minus id="S4.p1.14.m14.1.1.1.cmml" xref="S4.p1.14.m14.1.1.1"></minus><apply id="S4.p1.14.m14.1.1.2.cmml" xref="S4.p1.14.m14.1.1.2"><times id="S4.p1.14.m14.1.1.2.1.cmml" xref="S4.p1.14.m14.1.1.2.1"></times><cn type="integer" id="S4.p1.14.m14.1.1.2.2.cmml" xref="S4.p1.14.m14.1.1.2.2">5</cn><ci id="S4.p1.14.m14.1.1.2.3.cmml" xref="S4.p1.14.m14.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S4.p1.14.m14.1.1.3.cmml" xref="S4.p1.14.m14.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.14.m14.1c">5e-5</annotation></semantics></math>. We have trained all our models with a single P100 GPU and each training configuration took 4hrs to converge.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.11" class="ltx_p">For all our experiments, we use the HMDB<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">51</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> dataset. The HMDB<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">51</annotation></semantics></math> dataset contains short videos (1-15 seconds) for <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">51</annotation></semantics></math> human actions like cycling, eating, running and dancing etc. We have used the split-1 set of HMDB<math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mn id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><cn type="integer" id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">51</annotation></semantics></math> provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to create the train/test/validation set. There are <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="3570" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">3570</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">3570</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">3570</annotation></semantics></math> videos in the train set, <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="1530" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mn id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">1530</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><cn type="integer" id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">1530</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">1530</annotation></semantics></math> videos in the test set and <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="1749" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mn id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">1749</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><cn type="integer" id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">1749</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">1749</annotation></semantics></math> in the validation set. These videos are collected from YouTube and digitized movies and have large variability in camera motion, view-point and illumination. For our purpose, we annotated each of the <math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mn id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><cn type="integer" id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">51</annotation></semantics></math> action classes from the HMDB<math id="S4.SS1.p1.9.m9.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.9.m9.1a"><mn id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><cn type="integer" id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">51</annotation></semantics></math> dataset with one of our five defined motion types. We have named this annotated version of the HMDB<math id="S4.SS1.p1.10.m10.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.10.m10.1a"><mn id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><cn type="integer" id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">51</annotation></semantics></math> dataset the mHMDB<math id="S4.SS1.p1.11.m11.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS1.p1.11.m11.1a"><mn id="S4.SS1.p1.11.m11.1.1" xref="S4.SS1.p1.11.m11.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.11.m11.1b"><cn type="integer" id="S4.SS1.p1.11.m11.1.1.cmml" xref="S4.SS1.p1.11.m11.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.11.m11.1c">51</annotation></semantics></math> dataset. A subset of this mapping is shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, while the full version can be found in the appendix.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Motion Classifier</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For evaluation, we have compared our model with a optical flow based baseline model and performed an ablation study with various pre-training methods. The results are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2.3 Model Complexity Analysis ‣ 4.2 Motion Classifier ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Baseline Classifier</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.6" class="ltx_p">To benchmark our motion type classifier, we designed a baseline classifier as a two-layer fully connected neural network. The input to this classifier is based on the statistics of motion magnitudes in the video. To extract the input features, we first compute the pixel-wise average over time of the motion boundaries for the input video and divide it into <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mn id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">16</annotation></semantics></math> cells as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. We use the standard deviation of the magnitude of motion boundaries within each cell to form the 16-dimensional input feature vector to the motion type classifier. In the network design, there are <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mn id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">128</annotation></semantics></math> neurons in the first hidden layer and <math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mn id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">5</annotation></semantics></math> neurons in the second hidden layer for the network. ReLU activation was used after the first hidden layer and a softmax activation was applied after the second hidden layer for the final classification. Dropout regularization was applied with a drop probability of <math id="S4.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mn id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><cn type="float" id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">0.2</annotation></semantics></math> and the classifier was trained for <math id="S4.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.SSS1.p1.5.m5.1a"><mn id="S4.SS2.SSS1.p1.5.m5.1.1" xref="S4.SS2.SSS1.p1.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.5.m5.1b"><cn type="integer" id="S4.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.5.m5.1c">5</annotation></semantics></math> epochs with a learning rate of <math id="S4.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS2.SSS1.p1.6.m6.1a"><mn id="S4.SS2.SSS1.p1.6.m6.1.1" xref="S4.SS2.SSS1.p1.6.m6.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.6.m6.1b"><cn type="float" id="S4.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS1.p1.6.m6.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.6.m6.1c">0.001</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Model Performance Analysis</h4>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2110.01015/assets/PlayBackResults.jpg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="648" height="452" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Playback style recommendation by our system for YouTube videos</figcaption>
</figure>
<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.4" class="ltx_p">We observed that training our classifier from scratch achieved a performance boost of nearly <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="13\%" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">13</mn><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">13\%</annotation></semantics></math> over the baseline flow-based model, but still low as compared to fully supervised pre-training with ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and Kinetics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This was expected behavior, as our model was trained with only <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="3500" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mn id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">3500</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">3500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">3500</annotation></semantics></math> videos from the HMDB<math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mn id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">51</annotation></semantics></math> dataset, which is insufficient for supervised training when compared to the millions of data points used to train existing ImageNet and Kinetics classifiers. Thus our usage of transfer learning via initializing our classifier with weights learned from the ImageNet classification task increased our accuracy by a margin of around <math id="S4.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="14\%" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mrow id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.SSS2.p1.4.m4.1.1.2" xref="S4.SS2.SSS2.p1.4.m4.1.1.2.cmml">14</mn><mo id="S4.SS2.SSS2.p1.4.m4.1.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><apply id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.2">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">14\%</annotation></semantics></math>, due to the pre-trained understanding of important spatial features. Initializing with weights learned for action classification on the Kinetics dataset achieved the best accuracy, as they have a pre-trained understanding of both spatial and temporal features, which are useful to perform motion classification. Our baseline local-flow-based classifier expectedly performed the worst. These observations indicate that accurately predicting object motion type requires global semantic information contained in motion patterns. our results demonstrate that our motion type classifier learns more than just the motion magnitude, and has a deeper understanding of object motion patterns.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Model Complexity Analysis</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.2" class="ltx_p">We also performed an ablation study by varying the complexity of the backbone network. Our baseline model is TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> with shift modules which process multiple segments from a video and fuse them together at a later stage to obtain the combined feature vector. As the number of segments represent the complexity of the model, we have trained models with 1, 2, 3 and 8 segments in our ablation study. The overall accuracy of the model and the number of multiply-accumulate (MAC) operations is shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Video Playback Style Recommendation ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The three-segment model achieved the best accuracy for motion type classification. However, the two-segment model was able to achieve comparable accuracies to the three-segment model with just 0.82G MAC operations, making it the optimally suited configuration for mobile deployment. The inference time for the two-segment model on a Samsung S20 mobile device running a Qualcomm Snapdragon Adreno 650 GPU is just 200 milliseconds.
The single-segment model processes only a single frame from the complete video and therefore struggles to learn temporal dynamics of the video. However, it was still able to achieve a reasonable accuracy of <math id="S4.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="61.75\%" display="inline"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS3.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml">61.75</mn><mo id="S4.SS2.SSS3.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.2">61.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">61.75\%</annotation></semantics></math> for motion type classification, demonstrating the importance of object appearance in determining the natural motion patterns for an object. The eight-segment model did not perform well, due to the HMDB<math id="S4.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mn id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">51</annotation></semantics></math> dataset having small action videos and thus not requiring too many frames for effective motion pattern understanding. We believe that passing a large number of frames for actions with short duration captures multiple motion types present in the video at different instances and hence confuses the network training.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Motion Type Classifier Top-1 Accuracy.</figcaption>
<table id="S4.T2.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<th id="S4.T2.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
</tr>
<tr id="S4.T2.3.3.5.2" class="ltx_tr">
<th id="S4.T2.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Baseline Classifier</th>
<th id="S4.T2.3.3.5.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">25.64</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{Ours}_{\text{Scratch}}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2a.cmml">Ours</mtext><mtext id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3a.cmml">Scratch</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2"><mtext id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">Ours</mtext></ci><ci id="S4.T2.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">Scratch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\text{Ours}_{\text{Scratch}}</annotation></semantics></math></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r">38.56</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\text{Ours}_{\text{ImageNet}}" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><msub id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml"><mtext id="S4.T2.2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.2.1.m1.1.1.2a.cmml">Ours</mtext><mtext id="S4.T2.2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.2.1.m1.1.1.3a.cmml">ImageNet</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><apply id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.1.m1.1.1.2a.cmml" xref="S4.T2.2.2.2.1.m1.1.1.2"><mtext id="S4.T2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.2">Ours</mtext></ci><ci id="S4.T2.2.2.2.1.m1.1.1.3a.cmml" xref="S4.T2.2.2.2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.3">ImageNet</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\text{Ours}_{\text{ImageNet}}</annotation></semantics></math></th>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r">57.58</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\text{Ours}_{\text{Kinetics}}" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><msub id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml"><mtext id="S4.T2.3.3.3.1.m1.1.1.2" xref="S4.T2.3.3.3.1.m1.1.1.2a.cmml">Ours</mtext><mtext id="S4.T2.3.3.3.1.m1.1.1.3" xref="S4.T2.3.3.3.1.m1.1.1.3a.cmml">Kinetics</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><apply id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T2.3.3.3.1.m1.1.1.2a.cmml" xref="S4.T2.3.3.3.1.m1.1.1.2"><mtext id="S4.T2.3.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.2">Ours</mtext></ci><ci id="S4.T2.3.3.3.1.m1.1.1.3a.cmml" xref="S4.T2.3.3.3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.3.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.3">Kinetics</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\text{Ours}_{\text{Kinetics}}</annotation></semantics></math></th>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">72.68</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2110.01015/assets/fig3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Subjective study for video playback style recommendation.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Video Playback Style Recommendation</h3>

<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of motion classifier top-1 accuracy and MAC operations for a varying number of input segments for the network.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Segments</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">MACs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">61.76</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.41G</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">2</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">71.05</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">0.82G</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">3</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">72.68</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">1.23G</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">8</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">68.17</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">3.28G</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For a subjective evaluation of our video playback style recommendations, we conducted a user study with <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">10</annotation></semantics></math> volunteers. We downloaded two clips for each of the following five actions from YouTube: <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">cartwheel, diving, running, clapping, and drinking</em>. Our network predicted the motion type of each video, and we applied the matching playback style based on the mapping shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We also prepared a comparison set for the same videos with randomly applied playback styles. We evaluated our recommended playback styles against these randomly selected playback styles. The volunteers were asked to select the most aesthetic and preferred result from these two sets, the results of which are shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.2.3 Model Complexity Analysis ‣ 4.2 Motion Classifier ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For the categories that have a large global motions like cartwheel, diving, and running, our predicted playback style was ranked better than random playback style on an average. To our surprise, while the diving action was not present in our training set, our engine was able to recommend the best-suited playback style for the class. This provides evidence for the proposition that training to predict motion type captures more abstract information than actions, and generalizes well for unseen data. On the contrary, for local action categories such as drinking and clapping, our method was indistinguishable to random selection as the impact of playback style is not very evident when motion is confined to a small spatial region.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Video Retrieval</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">To further analyze the spatio-temporal features learned by our motion type classifier, we used these features to perform video retrieval. Given a query video, we aim to find the three most similar videos to the query video from a database of videos. We feed all the videos from HMDB<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="integer" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">51</annotation></semantics></math> to our motion classifier and extract the <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn type="integer" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">1280</annotation></semantics></math>-dimensional feature vector described in Sec. <a href="#S3.SS1" title="3.1 Network Architecture ‣ 3 Methodology ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for each video. In an ideal scenario, this feature vector represents the motion present in the video in a compressed form. We apply the k-nearest-neighbor algorithm in the <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mn id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><cn type="integer" id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">1280</annotation></semantics></math>-dimensional feature vector space to find videos having similar motion patterns as that of the query video. Some example retrievals from HMDB<math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="51" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mn id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">51</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><cn type="integer" id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">51</annotation></semantics></math> are shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, from which it is evident that our learned representations capture meaningful semantic information of object motion. In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a) the query video was of smoking, and all retrieved results (laugh, chew and chew) have local facial motions. In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b) and c) the first two results are from the same scene but at different points in time. In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b) the third retrieved result is of a golf swing, which has similar hand movement to that of a cartwheel. Similarly, for c) the last retrieved result is of a person diving from a cliff, which is very similar to the query video of a goalkeeper diving for football. For Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>d) all retrieved videos have linear motion and in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Video Retrieval ‣ 4 Experiments ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>e) all the retrieved actions for the query video of throw follow projectile motion.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2110.01015/assets/fig7.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="550" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Video retrieval from HMDB51 dataset using learned feature vectors</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we have examined the importance of object motion features in video analysis. We trained a model that understands the underlying object motion patterns and classifies the object motion into one of the five defined directional motion classes. We have also shown the exciting use case of playback style recommendation based on our classifier’s predicted motion type. Finally, we have evaluated the representations learned by motion type classifiers for video retrieval and have found that these representations generalize well for this task. In the future, we plan to explore other possible approaches to model object motions in the videos. We will also evaluate the generalization ability of learned representations for more challenging video tasks such as action localization and classification.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Nadine Behrmann, Jurgen Gall, and Mehdi Noroozi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Unsupervised video representation learning by bidirectional feature
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 1670–1679, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T Freeman,
Michael Rubinstein, Michal Irani, and Tali Dekel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Speednet: Learning the speediness in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 9922–9931, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Joao Carreira and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Quo vadis, action recognition? a new model and the kinetics dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 6299–6308, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Nieves Crasto, Philippe Weinzaepfel, Karteek Alahari, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Mars: Motion-augmented rgb stream for action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 7882–7891, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Navneet Dalal, Bill Triggs, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Human detection using oriented histograms of flow and appearance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 428–441. Springer, 2006.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Konstantinos G Derpanis, Mikhail Sizintsev, Kevin J Cannons, and Richard P
Wildes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Action spotting and recognition based on a spatiotemporal orientation
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">,
35(3):527–540, 2012.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Piotr Dollár, Vincent Rabaud, Garrison Cottrell, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Behavior recognition via sparse spatio-temporal features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2005 IEEE International Workshop on Visual Surveillance and
Performance Evaluation of Tracking and Surveillance</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 65–72. IEEE,
2005.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 2758–2766, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon, Boqing Gong, and Junzhou
Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">End-to-end learning of motion representation for video understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 6016–6025, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Dynamically encoded actions based on spacetime saliency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2755–2764, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Spatiotemporal multiplier networks for video action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 4768–4777, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Convolutional two-stream network fusion for video action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 1933–1941, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Self-supervised video representation learning with odd-one-out
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 3636–3645, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Tengda Han, Weidi Xie, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Video representation learning by dense predictive coding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPRW</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Can spatiotemporal 3d cnns retrace the history of 2d cnns and
imagenet?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 6546–6555, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,
and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Flownet 2.0: Evolution of optical flow estimation with deep networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 2462–2470, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Simon Jenni, Givi Meishvili, and Paolo Favaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Video representation learning by recognizing temporal
transformations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.10730</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">The kinetics human action video dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1705.06950</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alexander Klaser, Marcin Marszałek, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">A spatio-temporal descriptor based on 3d-gradients.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">2008.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and
Thomas Serre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Hmdb: a large video database for human motion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2011 ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 2556–2563. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Benjamin Rozenfeld.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Learning realistic human actions from movies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2008 CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8. IEEE, 2008.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ji Lin, Chuang Gan, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Tsm: Temporal shift module for efficient video understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICCV</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 7083–7093, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Ishan Misra, C Lawrence Zitnick, and Martial Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Shuffle and learn: unsupervised learning using temporal order
verification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 527–544. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Joe Yue-Hei Ng, Jonghyun Choi, Jan Neumann, and Larry S Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Actionflownet: Learning motion representation for action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 WACV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 1616–1624. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge
Belongie, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Spatiotemporal contrastive video representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.03800</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 4510–4520, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach,
Shih-Fu Chang, and Zhicheng Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Dmc-net: Generating discriminative motion cues for fast compressed
video action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1268–1277, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Two-stream convolutional networks for action recognition in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1406.2199</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning spatiotemporal features with 3d convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICCV</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 4489–4497, 2015.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
Paluri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">A closer look at spatiotemporal convolutions for action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 6450–6459, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Heng Wang and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Action recognition with improved trajectories.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 3551–3558, 2013.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Self-supervised spatio-temporal representation learning for videos by
predicting motion and appearance statistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 4006–4015, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc
Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Temporal segment networks: Towards good practices for deep action
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 20–36. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Xiaolong Wang and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual representations using videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 2794–2802, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
in video classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 305–321, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Self-supervised spatiotemporal learning via video clip order
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of CVPR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 10334–10343, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Dejun Zhang, Linchao He, Zhigang Tu, Shifu Zhang, Fei Han, and Boxiong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Learning motion representation for real-time spatio-temporal action
localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 103:107312, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Action to Motion Type Mapping</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We have manually annotated all the action classes present in HMDB51 with motion type classes based on the mapping shown in Table <a href="#A1.T4" title="Table 4 ‣ A.1 Action to Motion Type Mapping ‣ Appendix A Appendix ‣ Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. to obtain mHMDB51 dataset. We have used mHMDB51 dataset for training and evaluation of motion type classifier.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Motion Type mapping based on action class</figcaption>
<table id="A1.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.1.1.1" class="ltx_tr">
<th id="A1.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Action</th>
<th id="A1.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Motion Type</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.1.2.1" class="ltx_tr">
<td id="A1.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">brush_hair</td>
<td id="A1.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Linear</td>
</tr>
<tr id="A1.T4.1.3.2" class="ltx_tr">
<td id="A1.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">cartwheel</td>
<td id="A1.T4.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.4.3" class="ltx_tr">
<td id="A1.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">catch</td>
<td id="A1.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.5.4" class="ltx_tr">
<td id="A1.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">chew</td>
<td id="A1.T4.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.6.5" class="ltx_tr">
<td id="A1.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">clap</td>
<td id="A1.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
</tr>
<tr id="A1.T4.1.7.6" class="ltx_tr">
<td id="A1.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">climb</td>
<td id="A1.T4.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.8.7" class="ltx_tr">
<td id="A1.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">climb_stairs</td>
<td id="A1.T4.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.9.8" class="ltx_tr">
<td id="A1.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">dive</td>
<td id="A1.T4.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.10.9" class="ltx_tr">
<td id="A1.T4.1.10.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">draw_sword</td>
<td id="A1.T4.1.10.9.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.11.10" class="ltx_tr">
<td id="A1.T4.1.11.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">dribble</td>
<td id="A1.T4.1.11.10.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
</tr>
<tr id="A1.T4.1.12.11" class="ltx_tr">
<td id="A1.T4.1.12.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">drink</td>
<td id="A1.T4.1.12.11.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.13.12" class="ltx_tr">
<td id="A1.T4.1.13.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">eat</td>
<td id="A1.T4.1.13.12.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.14.13" class="ltx_tr">
<td id="A1.T4.1.14.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">fall_floor</td>
<td id="A1.T4.1.14.13.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.15.14" class="ltx_tr">
<td id="A1.T4.1.15.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">fencing</td>
<td id="A1.T4.1.15.14.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.16.15" class="ltx_tr">
<td id="A1.T4.1.16.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">flic_flac</td>
<td id="A1.T4.1.16.15.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.17.16" class="ltx_tr">
<td id="A1.T4.1.17.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">golf</td>
<td id="A1.T4.1.17.16.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.18.17" class="ltx_tr">
<td id="A1.T4.1.18.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">handstand</td>
<td id="A1.T4.1.18.17.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.19.18" class="ltx_tr">
<td id="A1.T4.1.19.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">hit</td>
<td id="A1.T4.1.19.18.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.20.19" class="ltx_tr">
<td id="A1.T4.1.20.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">hug</td>
<td id="A1.T4.1.20.19.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.21.20" class="ltx_tr">
<td id="A1.T4.1.21.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">jump</td>
<td id="A1.T4.1.21.20.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.22.21" class="ltx_tr">
<td id="A1.T4.1.22.21.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">kick</td>
<td id="A1.T4.1.22.21.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.23.22" class="ltx_tr">
<td id="A1.T4.1.23.22.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">kick_ball</td>
<td id="A1.T4.1.23.22.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.24.23" class="ltx_tr">
<td id="A1.T4.1.24.23.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">kiss</td>
<td id="A1.T4.1.24.23.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.25.24" class="ltx_tr">
<td id="A1.T4.1.25.24.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">laugh</td>
<td id="A1.T4.1.25.24.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.26.25" class="ltx_tr">
<td id="A1.T4.1.26.25.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">pick</td>
<td id="A1.T4.1.26.25.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.27.26" class="ltx_tr">
<td id="A1.T4.1.27.26.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">pour</td>
<td id="A1.T4.1.27.26.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.28.27" class="ltx_tr">
<td id="A1.T4.1.28.27.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">pullup</td>
<td id="A1.T4.1.28.27.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
</tr>
<tr id="A1.T4.1.29.28" class="ltx_tr">
<td id="A1.T4.1.29.28.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">punch</td>
<td id="A1.T4.1.29.28.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.30.29" class="ltx_tr">
<td id="A1.T4.1.30.29.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">push</td>
<td id="A1.T4.1.30.29.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.31.30" class="ltx_tr">
<td id="A1.T4.1.31.30.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">pushup</td>
<td id="A1.T4.1.31.30.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
</tr>
<tr id="A1.T4.1.32.31" class="ltx_tr">
<td id="A1.T4.1.32.31.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">ride_bike</td>
<td id="A1.T4.1.32.31.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.33.32" class="ltx_tr">
<td id="A1.T4.1.33.32.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">ride_horse</td>
<td id="A1.T4.1.33.32.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.34.33" class="ltx_tr">
<td id="A1.T4.1.34.33.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">run</td>
<td id="A1.T4.1.34.33.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.35.34" class="ltx_tr">
<td id="A1.T4.1.35.34.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">shake_hands</td>
<td id="A1.T4.1.35.34.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.36.35" class="ltx_tr">
<td id="A1.T4.1.36.35.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">shoot_ball</td>
<td id="A1.T4.1.36.35.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.37.36" class="ltx_tr">
<td id="A1.T4.1.37.36.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">shoot_bow</td>
<td id="A1.T4.1.37.36.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.38.37" class="ltx_tr">
<td id="A1.T4.1.38.37.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">shoot_gun</td>
<td id="A1.T4.1.38.37.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.39.38" class="ltx_tr">
<td id="A1.T4.1.39.38.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">sit</td>
<td id="A1.T4.1.39.38.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.40.39" class="ltx_tr">
<td id="A1.T4.1.40.39.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">situp</td>
<td id="A1.T4.1.40.39.2" class="ltx_td ltx_align_left ltx_border_r">Oscillatory</td>
</tr>
<tr id="A1.T4.1.41.40" class="ltx_tr">
<td id="A1.T4.1.41.40.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">smile</td>
<td id="A1.T4.1.41.40.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.42.41" class="ltx_tr">
<td id="A1.T4.1.42.41.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">smoke</td>
<td id="A1.T4.1.42.41.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.43.42" class="ltx_tr">
<td id="A1.T4.1.43.42.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">somersault</td>
<td id="A1.T4.1.43.42.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.44.43" class="ltx_tr">
<td id="A1.T4.1.44.43.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">stand</td>
<td id="A1.T4.1.44.43.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.45.44" class="ltx_tr">
<td id="A1.T4.1.45.44.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">swing_baseball</td>
<td id="A1.T4.1.45.44.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.46.45" class="ltx_tr">
<td id="A1.T4.1.46.45.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">sword</td>
<td id="A1.T4.1.46.45.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.47.46" class="ltx_tr">
<td id="A1.T4.1.47.46.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">sword_exercise</td>
<td id="A1.T4.1.47.46.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.48.47" class="ltx_tr">
<td id="A1.T4.1.48.47.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">talk</td>
<td id="A1.T4.1.48.47.2" class="ltx_td ltx_align_left ltx_border_r">Local</td>
</tr>
<tr id="A1.T4.1.49.48" class="ltx_tr">
<td id="A1.T4.1.49.48.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">throw</td>
<td id="A1.T4.1.49.48.2" class="ltx_td ltx_align_left ltx_border_r">Projectile</td>
</tr>
<tr id="A1.T4.1.50.49" class="ltx_tr">
<td id="A1.T4.1.50.49.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">turn</td>
<td id="A1.T4.1.50.49.2" class="ltx_td ltx_align_left ltx_border_r">Random</td>
</tr>
<tr id="A1.T4.1.51.50" class="ltx_tr">
<td id="A1.T4.1.51.50.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">walk</td>
<td id="A1.T4.1.51.50.2" class="ltx_td ltx_align_left ltx_border_r">Linear</td>
</tr>
<tr id="A1.T4.1.52.51" class="ltx_tr">
<td id="A1.T4.1.52.51.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">wave</td>
<td id="A1.T4.1.52.51.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Local</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.01013" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.01015" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.01015">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.01015" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.01016" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 12 10:20:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
