<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes</title>
<!--Generated on Fri Sep 20 16:23:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="attribute extraction,  beauty recommendation,  ingredient analysis,  explainability" lang="en" name="keywords"/>
<base href="/html/2409.13628v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S1" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S2" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S2.SS0.SSS0.Px1" title="In 2. Related Works ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Attribute Value Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S2.SS0.SSS0.Px2" title="In 2. Related Works ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Beauty Product Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S3" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>System Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S3.SS0.SSS0.Px1" title="In 3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Model Input</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S4" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS1" title="In 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS2" title="In 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Baseline Solutions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS2.SSS0.Px1" title="In 5.2. Baseline Solutions ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Fuzzy Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS2.SSS0.Px2" title="In 5.2. Baseline Solutions ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Explicit Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS3" title="In 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS4" title="In 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Explainability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS5" title="In 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Robustness in Low Data Regime</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S6" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S6.SS1" title="In 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Does the choice of logits transformation matter?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S6.SS2" title="In 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Finetuning on Additional Attributes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S6.SS3" title="In 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Alternating Query Attribute Tokens</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S7" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Explainable Beauty Recommendation and Customer Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S7.SS0.SSS0.Px1" title="In 7. Explainable Beauty Recommendation and Customer Understanding ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Explainable Beauty Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S7.SS0.SSS0.Px2" title="In 7. Explainable Beauty Recommendation and Customer Understanding ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title">Customer understanding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S8" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#A1" title="In Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#A1.SS1" title="In Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Labels for Skincare Products</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#A1.SS2" title="In Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Product information and Labels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#A1.SS3" title="In Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>FuzzySearch Attribute Key Words</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Celine Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:celineli@amazon.com">celineli@amazon.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Vancouver</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Canada</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rahul Suresh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:surerahu@amazon.com">surerahu@amazon.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Vancouver</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Canada</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amin Banitalebi-Dehkordi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aminbt@amazon.com">aminbt@amazon.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Vancouver</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Canada</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Accurate attribute extraction is critical for beauty product recommendations and building trust with customers. This remains an open problem, as existing solutions are often unreliable and incomplete. We present a system to extract beauty-specific attributes using end-to-end supervised learning based on beauty product ingredients. A key insight to our system is a novel energy-based implicit model architecture. We show that this implicit model architecture offers significant benefits in terms of accuracy, explainability, robustness, and flexibility. Furthermore, our implicit model can be easily fine-tuned to incorporate additional attributes as they become available, making it more useful in real-world applications. We validate our model on a major e-commerce skincare product catalog dataset and demonstrate its effectiveness. Finally, we showcase how ingredient-based attribute extraction contributes to enhancing the explainability of beauty recommendations.</p>
</div>
<div class="ltx_keywords">attribute extraction, beauty recommendation, ingredient analysis, explainability
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>ACM Conference; October 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TOG</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalvolume" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalnumber" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_article" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information retrieval Information extraction; Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S1.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of beauty product extraction workflow and the BT-BERT architecture. Our model is identical to the BERT Transformer <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib7" title="">2018</a>)</cite> except in the last layer—the initial <span class="ltx_text ltx_font_typewriter" id="S1.F1.2.1">N-1</span> layers remain unmodified. We remove the final MLP from the last layer of the Transformer encoder and directly use the self-attention values to formulate the output probability.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The value of the global beauty and personal care market is estimated to be over $646 billion in 2024 <cite class="ltx_cite ltx_citemacro_citep">(Wood, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib28" title="">2024</a>)</cite>.
Product discovery and trust are two of the biggest considerations in Beauty customers’ shopping journeys in e-commerce stores.
Many factors contribute to these problems, such as lack of personalized recommendations, inaccurate or incomplete product benefit and/or ingredient information, lack of targeted curation, etc.
Having such information accurately listed in the product catalogue is particularly important for Beauty category of products, as they are topically applied to the skin.
Manual curation and sanitization of such metadata is possible at small scales.
However, for larger e-commerce stores, with a large portfolio of products, it will be impractical to rely on manual annotation.
</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The primary objective of our work is to enhance the beauty shopping experience by automatically and accurately extracting beauty attributes at scale.
These attributes not only aid customers in comparing and refining product choices but also foster trust in the e-commerce stores.
Furthermore, the extracted attributes contribute to building more explainable beauty recommendations, which empower customers to make informed purchasing decisions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We propose a robust and scalable learning-based solution capable of predicting beauty attributes from product ingredients.
To achieve this, we integrate an energy-based implicit strategy to extract 5 skin types, 11 skin concerns, and 17 attributes commonly preferred across beauty products, as elaborated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#A1.SS1" title="A.1. Labels for Skincare Products ‣ Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Section A.1</span></a>.
In summary, the key benefits of our proposed model are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Improved <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">accuracy</span> and <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.2">precision</span> compared to the alternatives,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Explainability</span> through analysis of the attention weights (<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS4" title="5.4. Explainability ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes">§<span class="ltx_ref ltx_nolink"><span class="ltx_text ltx_ref_tag">5.4</span></span></a>),</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Robustness</span> in a low-resource regime via implicit data augmentation (<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S5.SS5" title="5.5. Robustness in Low Data Regime ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes">§<span class="ltx_ref ltx_nolink"><span class="ltx_text ltx_ref_tag">5.5</span></span></a>),</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Flexibility</span> when finetuning previously trained models on new labels (<a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#S6.SS2" title="6.2. Finetuning on Additional Attributes ‣ 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes">§<span class="ltx_ref ltx_nolink"><span class="ltx_text ltx_ref_tag">6.2</span></span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To the best of our knowledge, there has been no prior study on the extraction of beauty-specific attributes based on product ingredients. Our contributions are outlined as follows:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce a novel energy-based implicit model for extracting beauty attributes from product ingredients and the title. We define <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.1">implicit</span> vs. <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.2">explicit</span> models in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S3" title="3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Section 3</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">Our proposed approach is assessed using skincare products from a major e-commerce store. We demonstrate its superiority over traditional keyword-based solutions and an explicit classifier baseline on a test dataset annotated by beauty domain experts.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">We document and extensively discuss the key algorithmic and architectural features that contribute to explainability, robustness, and flexibility of our proposed model.
</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i4.p1">
<p class="ltx_p" id="S1.I2.i4.p1.1">As a use-case study, we illustrate how ingredient-based extracted attributes can enhance the development of explainable beauty recommendations in  <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S7" title="7. Explainable Beauty Recommendation and Customer Understanding ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Section 7</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Works</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Attribute Value Extraction</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">The problem of product attribute extraction in e-commerce is traditionally solved using named entity recognition (NER).
NER approaches typically use beginning-inside-outside (BIO) tagging <cite class="ltx_cite ltx_citemacro_citep">(Chiticariu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib6" title="">2010</a>; Putthividhya and Hu, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib22" title="">2011</a>)</cite> to segment texts.
However, NER-based approaches exhibit substantial limitations due to their reliance on predefined entity types.
This rigidity makes it difficult to scale in dynamic environments where attributes are numerous and constantly evolving, such as in beauty product recommendations.
Certain research also models the attribute extraction task as a sequential tagging problem  <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib13" title="">2015</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib34" title="">2018</a>)</cite> using CRF and BiLSTM.
 <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib31" title="">2021</a>)</cite> describes a method that extracts attributes using a parameterized decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module.
 <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib30" title="">2019</a>)</cite> also model the attribute to make the prediction task more scalable.
Our work is similar to the solution proposed in <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib30" title="">2019</a>)</cite>, which uses BERT and Bi-LSTMs to model semantic relations between attribute and product titles on a large-scale dataset.
However, the deep learning modules in <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib30" title="">2019</a>)</cite> are primarily used as components in the NER pipeline and the outputs of the model are still the BIO tags.
Our work is different in that our proposed model directly outputs the attribute values and the architectural design choices are heavily guided by explainability, robustness, and flexibility.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">In the direction of classification tasks, recent advancements utilize multitask framework and multi-modality <cite class="ltx_cite ltx_citemacro_citep">(Cardoso et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib5" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib27" title="">2022</a>; Dezaki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib8" title="">2023</a>)</cite>.
Furthermore, these models utilize parameter sharing across different attribute prediction tasks, reducing the model’s complexity and encouraging generalization.
Each attribute has its own output layer, allowing the network to predict multiple attributes simultaneously.
On the other hand, prior works have demonstrated that incorporating an implicit method <cite class="ltx_cite ltx_citemacro_citep">(Du and Mordatch, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib9" title="">2019</a>; Florence et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib11" title="">2021</a>)</cite> offers unique benefits.
In particular, when treating product attribute extraction as an implicit classification problem—where attributes themselves are also part of the input—the model can focus on specific attributes to extract from the product description.
This approach helps the model learn more meaningful and relevant embeddings from the input which leads to more accurate attribute value extraction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Beauty Product Recommendation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Extant literature provides limited research on beauty product recommendation that incorporates ingredient analysis <cite class="ltx_cite ltx_citemacro_citep">(Afshar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib2" title="">2023</a>; Alashkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib3" title="">2017</a>)</cite>.
 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib15" title="">2020</a>)</cite> directly uses an ingredient-concern mapping table to provide solutions for users of various skin conditions detected by an object detection computer vision model.
However, this mapping table is often supplied by a third party where mappings are constructed independently for each ingredient without accounting for the order and the interactions with other ingredients, leading to inflexible rule-based recommendation methods.
 <cite class="ltx_cite ltx_citemacro_citep">(Nakajima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib21" title="">2019</a>)</cite>’s approach extracts ingredient efficacy based on user reviews and recommends products containing those ingredients for customers across various age groups.
Although this method relies on user-generated content, it does not align with our fact-based approach, making it inapplicable to our use-case scenario.
 <cite class="ltx_cite ltx_citemacro_citep">(S et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib23" title="">2022</a>)</cite> employs a method based on ingredient similarity using one-hot encoding to recommend products given a user’s past purchase.
However, this work does not leverage ingredient data to predict targeted skin types and concerns directly, which is the focus of our work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>System Overview</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="S3.F2.g1" src="extracted/5866900/Figures/widget2.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Skincare recommendation with explainable ingredient for each attribute. </figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We approach the beauty attribute extraction problem as a supervised multi-label classification task.
Our proposed solution features a bidirectional Transformer encoder network similar to BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib7" title="">2018</a>)</cite>, with a slight modification applied to the last attention layer as summarized in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#alg1" title="Algorithm 1 ‣ Model Input ‣ 3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Algorithm 1</span></a>.
It is important to note that the network does not use the feed-forward layers in the last Transformer encoder block and does not have any additional classifier modules commonly used in downstream learning tasks.
Instead, the logits are directly calculated from the attention values.
We refer to our model as <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">BeautyTech-BERT</span>, or <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">BT-BERT</span> for short.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The model operates by taking as inputs a query attribute, a list of ingredients, and the product title, and producing the probability for the query attribute.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> shows an example use-case where the user is querying six attributes for a product titled “COSRX Snail Mucin Essence”.
Based on the product ingredients, the network will make an inference on whether to label the query attributes true or false.
In this case, since <span class="ltx_text ltx_font_typewriter" id="S3.p2.1.1">Betaine</span> is an ingredient known for its hydrating properties, the network is likely to predict true for <span class="ltx_text ltx_font_typewriter" id="S3.p2.1.2">Dry Skin</span>, meaning this product likely benefits those who have a dry skin type.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Conceptually, our model can be viewed as an energy-based model (EBM) <cite class="ltx_cite ltx_citemacro_citep">(Teh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib26" title="">2003</a>; Song and Kingma, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib25" title="">2021</a>; LeCun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib14" title="">2006</a>)</cite>, as it assigns a normalized scalar (or ”energy”) to each input data point, thereby representing a probability distribution over the training data.
We also denote our model as an <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">implicit model</span>, as it accepts the query attribute as input and generates a prediction solely for that attribute.
This distinguishes it from conventional multi-label classifiers, where the classifier module and the number of output classes must be <span class="ltx_text ltx_font_italic" id="S3.p3.1.2">explicitly</span> defined.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Input</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">For each product, the query attribute is concatenated with ingredients and title to pass to the model.
Maintaining the original sequence order of the ingredient list is essential, as it reflects the standard convention of listing higher potency ingredients first.
We first tokenize the query label and pad query tokens up to a length of 3.
The product ingredients and title are also tokenized.
The entire sequence is truncated or padded such that the final length is 512.
We place the query attribute at the beginning of the input sequence so that its position is consistent across all input sequences—similar to the effect of the <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.1">[CLS]</span> token in BERT when using it in downstream tasks—which is important for computing the logits.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="383" id="S3.F3.g1" src="x2.png" width="665"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Difference between implicit and explicit models. <span class="ltx_text ltx_font_bold" id="S3.F3.3.1">Left:</span> In implicit models, the model intakes query attribute together with product ingredients and title. Note that in our case, the output logits come directly from the self-attention values of the last encoder layer.
<span class="ltx_text ltx_font_bold" id="S3.F3.4.2">Right:</span> Explicit models represent the standard way of fine-tuning the BERT model, where a classifier is attached to the end of the Transformer.</figcaption>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.4.1.1">Algorithm 1</span> </span> BT-BERT Forward Pass</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.5">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_typewriter" id="alg1.l1.2" style="font-size:90%;">bert_model = AutoModel.from_pretrained(...)</span><span class="ltx_text" id="alg1.l1.3" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l3.2" style="font-size:90%;">function</span><span class="ltx_text" id="alg1.l3.3" style="font-size:90%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg1.l3.4" style="font-size:90%;">forward</span><span class="ltx_text" id="alg1.l3.5" style="font-size:90%;">(</span><span class="ltx_text ltx_font_typewriter" id="alg1.l3.6" style="font-size:90%;">input_ids, labels</span><span class="ltx_text" id="alg1.l3.7" style="font-size:90%;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span><span class="ltx_text" id="alg1.l4.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l4.3" style="font-size:90%;">outputs = bert_model(input_ids)</span><span class="ltx_text" id="alg1.l4.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span><span class="ltx_text" id="alg1.l5.2" style="font-size:90%;">    </span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span><span class="ltx_text" id="alg1.l6.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l6.3" style="font-size:90%;color:#0000FF;"># extract the last layer’s attention, e.g., -1</span><span class="ltx_text" id="alg1.l6.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span><span class="ltx_text" id="alg1.l7.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l7.3" style="font-size:90%;color:#0000FF;"># attentions are [batch, heads, seqlen, seqlen]</span><span class="ltx_text" id="alg1.l7.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span><span class="ltx_text" id="alg1.l8.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l8.3" style="font-size:90%;">attentions = outputs["attentions"][-1]</span><span class="ltx_text" id="alg1.l8.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span><span class="ltx_text" id="alg1.l9.2" style="font-size:90%;">    </span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span><span class="ltx_text" id="alg1.l10.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l10.3" style="font-size:90%;color:#0000FF;"># summing attention values over all heads</span><span class="ltx_text" id="alg1.l10.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span><span class="ltx_text" id="alg1.l11.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l11.3" style="font-size:90%;color:#0000FF;"># for the first token attending to itself</span><span class="ltx_text" id="alg1.l11.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span><span class="ltx_text" id="alg1.l12.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l12.3" style="font-size:90%;color:#0000FF;"># 16 is a hyperparameter multiplication factor</span><span class="ltx_text" id="alg1.l12.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span><span class="ltx_text" id="alg1.l13.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l13.3" style="font-size:90%;">logits = 16 * attentions[:, :, 0, 0].sum(dim=1)</span><span class="ltx_text" id="alg1.l13.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span><span class="ltx_text" id="alg1.l14.2" style="font-size:90%;">    </span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l15.1.1.1" style="font-size:80%;">15:</span></span><span class="ltx_text" id="alg1.l15.2" style="font-size:90%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg1.l15.3" style="font-size:90%;">L = binary_cross_entropy_with_logits(logits, labels)</span><span class="ltx_text" id="alg1.l15.4" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l16.1.1.1" style="font-size:80%;">16:</span></span><span class="ltx_text" id="alg1.l16.2" style="font-size:90%;">    </span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l17.1.1.1" style="font-size:80%;">17:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l17.2" style="font-size:90%;">end</span><span class="ltx_text" id="alg1.l17.3" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" id="alg1.l17.4" style="font-size:90%;">function</span>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Data Preparation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">Our proposed method is a supervised learning approach and thus requires labeled training data.
We first collect a dataset of skincare products from product data available publicly  <cite class="ltx_cite ltx_citemacro_citep">(Skillsmuggler, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib24" title="">2024</a>; Feeds, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib10" title="">2024</a>)</cite>.
For each product, attribute labels were meticulously annotated by domain experts based on years of scientific ingredient research.
An example is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#A1.F6" title="Figure 6 ‣ A.2. Product information and Labels ‣ Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.
Overall, we collected a total of 11580 data points, where 9334 (<math alttext="\approx 80\%" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mn id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">80</mn><mo id="S4.p1.1.m1.1.1.3.1" xref="S4.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><approx id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3.1">percent</csymbol><cn id="S4.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.p1.1.m1.1.1.3.2">80</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\approx 80\%</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">≈ 80 %</annotation></semantics></math>) are dedicated to training and 2246 (<math alttext="\approx 20\%" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">≈</mo><mrow id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mn id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml">20</mn><mo id="S4.p1.2.m2.1.1.3.1" xref="S4.p1.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><approx id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"></approx><csymbol cd="latexml" id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">absent</csymbol><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3.1">percent</csymbol><cn id="S4.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.p1.2.m2.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\approx 20\%</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">≈ 20 %</annotation></semantics></math>) to evaluation.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S4.F4" title="Figure 4 ‣ 4. Data Preparation ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> shows the distribution of products categorized by product types and attributes in our dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S4.F4.g1" src="x3.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Label Distribution across Product Type in our dataset. The height of each bar indicates the number of products associated with the respective attribute. For instance, there are a total of 1809 out of 11580 products for Dry Skin.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section contains the experiment results and additional analysis around the results.
All experiments are conducted on an EC2 “p3.16xlarge” instance with 8 Nvidia Tesla V100 GPUs.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Training Details</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">For all experiments, we train the network end-to-end with a batch size of 8 until convergence.
We use the AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib18" title="">2017</a>)</cite> with an initial learning rate of <math alttext="3\times 10^{-5}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo id="S5.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p1.1.m1.1.1.1.cmml">×</mo><msup id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p1.1.m1.1.1.3.3a" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.1.1.3.3.2" xref="S5.SS1.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><cn id="S5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2">3</cn><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn id="S5.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3.2">10</cn><apply id="S5.SS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3"><minus id="S5.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3"></minus><cn id="S5.SS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">3\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>.
We follow the standard setup for training Transformer models by splitting the trainable parameters into two categories: decay and non-decay parameters.
Non-decaying parameters are biases and LayerNorm <cite class="ltx_cite ltx_citemacro_citep">(Ba et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib4" title="">2016</a>)</cite> parameters; all other parameters are weight decayed.
We set <math alttext="{\tt beta2}=0.95" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">𝚋𝚎𝚝𝚊𝟸</mi><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><eq id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></eq><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝚋𝚎𝚝𝚊𝟸</ci><cn id="S5.SS1.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS1.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">{\tt beta2}=0.95</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">typewriter_beta2 = 0.95</annotation></semantics></math> to improve training stability as recommended in <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib32" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We explored a few different training recipes but found them to have negligible impact on the final model performance, including using a cosine annealing learning rate scheduler <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib17" title="">2016</a>)</cite>, linear decay scheduler, and weighted loss for addressing the class imbalance issue.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Baseline Solutions</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We evaluated our method against two simple baseline solutions: Fuzzy Search and the explicit model alternative illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S3.F3" title="Figure 3 ‣ Model Input ‣ 3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Fuzzy Search</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">This is a straightforward approach of finding keywords based on edit distance and other heuristics.
Specifically, a predefined list of target keywords is established (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#A1.SS3" title="A.3. FuzzySearch Attribute Key Words ‣ Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Section A.3</span></a>) for each of the 33 attributes.
Subsequently, a product is categorized as possessing a particular attribute if any of the keywords from the corresponding list are detected within the product information.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p2.1">We compare to this baseline as an example of highly explainable solution, but we are well aware that it is not state-of-the-art by any means.
By examining a few examples, the limitations of the fuzzy search approach is immediately apparent.
First, fuzzy search is unable to discern complex textual context.
For example, it may overlook the labeling of a product described as <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px1.p2.1.1">free of perfume, silicones, phthalates, fragrance</span> as ‘Fragrance Free’.
Second, it is sensitive to error tolerance threshold.
For instance, despite a product being described as <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px1.p2.1.2">hydra intensive treatment</span>, the method may not assign the attribute ”Hydration” if the error tolerance is set too low.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Explicit Model</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">A common approach for classification tasks often trains an explicit feed-forward network on top of a pre-trained rich embedding, similar to the approach described in <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib7" title="">2018</a>)</cite>.
As a benchmark, we experimented with this approach, where the model receives product information as input and outputs the likelihood of the 33 labels.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S3.F3" title="Figure 3 ‣ Model Input ‣ 3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> highlights the differences between the implicit and the explicit models.
In the explicit model, the classifier’s output dimension is predefined to be the same as the number of attributes.
For this approach, we use the pre-trained weights and tokenizer of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px2.p1.1.1">bge-base-en-v1.5</span> <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib29" title="">2023</a>)</cite> from HuggingFace.
We chose <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px2.p1.1.2">bge-base-en-v1.5</span> as it is considered the state-of-the-art text embedding model for retrieval, clustering, reranking tasks in the Massive Text Embedding Benchmark (MTEB) <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib20" title="">2022</a>)</cite>.
As a common practice, we freeze the backbone weights and only update the classifier parameters for four epochs to avoid catastrophic forgetting.
We find that training end-to-end after four epochs provides the optimal results compared to other configurations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Model Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluate models on the standard classification metrics.
In the following definitions, <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.1">TP/TN/FP/FN</span> refers to the number of true positive, true negative, false positive, and false negative predictions respectively.</p>
<dl class="ltx_description" id="S5.I1">
<dt class="ltx_item" id="S5.I1.i1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.1.1.1">: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Accuracy</span> is defined as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i1.p1.1.2">(TP+TN)/(TP+TN+FP+FN)</span>.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.i2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.1.1.1">: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Precision</span> is defined as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i2.p1.1.2">TP/(TP+FP)</span>.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.i3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.1.1.1">: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Recall</span> is defined as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.2">TP/(TP+FN)</span>.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.i4"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.1.1.1">: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">F1-Score</span> is defined as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i4.p1.1.2">(2*TP)/(2*TP+FP+FN)</span>.</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Although we report recall and F1-score, we prioritize accuracy and precision as the main evaluation metrics.
A higher precision aligns more closely with our acceptable risk threshold by minimizing the likelihood of potentially recommending products containing unsuitable ingredients to customers with particularly sensitive skin.
This is important as we envision attribute-based beauty recommendations as one of the direct applications on this work.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S5.T1" title="Table 1 ‣ 5.3. Model Results ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Table 1</span></a> summarizes the results of label prediction across different methods.
We observe that both learning-based methods significantly outperform the fuzzy search baseline, as expected.
The implicit model performs slightly better than the explicit alternative across all evaluation metrics.
Aside from the quantitative edge, the implicit model offers other qualitative advantages that the explicit model does not.
We discuss this extensively in the following sections.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Model Performance: Explicit vs. Implicit Approach (BT-BERT)</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.3" style="width:208.1pt;height:47.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.2pt,12.0pt) scale(0.665767092918068,0.665767092918068) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.3.1.1.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.1.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.2.1" style="font-size:90%;">Accuracy</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.1.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.3.1" style="font-size:90%;">Precision</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.1.1.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.4.1" style="font-size:90%;">Recall</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.1.1.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.5.1" style="font-size:90%;">F1-Score</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.1.1.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1.6.1" style="font-size:90%;">Parameters</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.1.2.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.2.1.1.1" style="font-size:90%;">BT-BERT</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.3.1.2.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.2.1.2.1" style="font-size:90%;">0.964</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.3.1.2.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.2.1.3.1" style="font-size:90%;">0.987</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.3.1.2.1.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.2.1.4.1" style="font-size:90%;">0.958</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.3.1.2.1.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.2.1.5.1" style="font-size:90%;">0.960</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.3.1.2.1.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.2.1.6.1" style="font-size:90%;">109,360,128</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.3.2.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.1.1" style="font-size:90%;">Explicit Model</span></th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.1.3.2.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.2.1" style="font-size:90%;">0.946</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.1.3.2.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.3.1" style="font-size:90%;">0.954</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.1.3.2.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.4.1" style="font-size:90%;">0.904</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.1.3.2.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.5.1" style="font-size:90%;">0.912</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.1.3.2.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.3.2.6.1" style="font-size:90%;">109,975,296</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.3.1.4.3.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.1.1" style="font-size:90%;">Fuzzy Search</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.1.4.3.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.2.1" style="font-size:90%;">0.301</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.1.4.3.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.3.1" style="font-size:90%;">0.287</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.1.4.3.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.4.1" style="font-size:90%;">0.356</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.1.4.3.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.5.1" style="font-size:90%;">0.327</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.1.4.3.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T1.3.1.4.3.6.1" style="font-size:90%;">–</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Explainability</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In this section, we analyze the input tokens with high attention values in the second last layer of the Transformer encoder block.
Top tokens are obtained using <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#alg2" title="Algorithm 2 ‣ 6.2. Finetuning on Additional Attributes ‣ 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Algorithm 2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S5.T2" title="Table 2 ‣ 5.4. Explainability ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, we choose three query attributes—‘Acne’, ‘Fine Lines and Wrinkles’ and ‘Hydration’—and show that tokens with high attention values are ingredients that address the target skin concerns.
This means that our model has learned the effects of different ingredients and how they are associated to different skin concerns and skin types.
We chose these labels, as they are the most popular filter criteria for beauty products.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Attention analysis for ‘Acne’, ‘Fine Lines and Wrinkles’, and ‘Hydration‘ attributes</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:195.1pt;height:130.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.2pt,42.9pt) scale(0.60308262455879,0.60308262455879) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.3.1.1.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.1.1" style="font-size:90%;">Attribute</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.3.1.1.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.2.1" style="font-size:90%;">High Attention Sub-word Tokens</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.3.1.1.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.3.1" style="font-size:90%;">Ingredient</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.2.2.1" rowspan="4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.1.1" style="font-size:90%;">Acne</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.2.2.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.2.2.2.1" style="font-size:90%;">‘sal’, ‘#ic’, ‘#yl’, ‘#ic’, ‘acid’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.2.2.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.3.1" style="font-size:90%;">Salicylic Acid</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.3.3">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.3.3.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.3.3.1.1" style="font-size:90%;">‘alcohol’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.3.3.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.2.1" style="font-size:90%;">Alcohol</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.4.4.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.4.4.1.1" style="font-size:90%;">‘benz’, ‘#oy’, ‘#l’, ‘per’, ‘#oxide’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.4.4.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.2.1" style="font-size:90%;">Benzoyl Peroxide</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.5.5.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.5.5.1.1" style="font-size:90%;">‘beta’, ‘#ine’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.5.5.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.2.1" style="font-size:90%;">Betaine</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.6.6.1" rowspan="4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.1.1" style="font-size:90%;">Lines &amp; Wrinkles</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.6.6.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.6.6.2.1" style="font-size:90%;">‘#pher’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.6.6.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.3.1" style="font-size:90%;">Tocopheryl Acetate</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.7.7">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.7.7.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.7.7.1.1" style="font-size:90%;">‘#ito’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.7.7.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.2.1" style="font-size:90%;">Palmitoyl</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.8.8">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.8.8.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.8.8.1.1" style="font-size:90%;">‘baku’, ‘#chio’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.8.8.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.8.8.2.1" style="font-size:90%;">Bakuchiol</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.9.9.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.9.9.1.1" style="font-size:90%;">‘re’, ‘#tino’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.9.9.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.9.9.2.1" style="font-size:90%;">Retinol</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.3.1.10.10.1" rowspan="3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.10.10.1.1" style="font-size:90%;">Hydration</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.10.10.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.10.10.2.1" style="font-size:90%;">‘#yal’, ‘#uron’, ‘ate’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.10.10.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.10.10.3.1" style="font-size:90%;">Sodium Hyaluronate</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.11.11">
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.11.11.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.11.11.1.1" style="font-size:90%;">‘#ly’, ‘#cer’, ‘#in’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.1.11.11.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.11.11.2.1" style="font-size:90%;">Glycerin</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.1.12.12.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.1.12.12.1.1" style="font-size:90%;">‘ni’, ‘#ac’, ‘#ina’, ‘#mide’</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.1.12.12.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T2.3.1.12.12.2.1" style="font-size:90%;">Niacinamide</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Attention analysis for product attributes</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.3" style="width:390.3pt;height:255.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(11.0pt,-7.2pt) scale(1.05953894237724,1.05953894237724) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.3.1.1.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.1.1" style="font-size:90%;">Attribute</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.3.1.1.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.2.1" style="font-size:90%;">High Attention Sub-word Tokens</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.3.1.1.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.3.1" style="font-size:90%;">Corresponding Ingredient</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" colspan="3" id="S5.T3.3.1.2.2.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.3.1.2.2.1.1">
<span class="ltx_p" id="S5.T3.3.1.2.2.1.1.1" style="width:368.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.2.1.1.1.1" style="font-size:90%;">Product</span><span class="ltx_text" id="S5.T3.3.1.2.2.1.1.1.2" style="font-size:90%;">: </span><span class="ltx_text ltx_font_italic" id="S5.T3.3.1.2.2.1.1.1.3" style="font-size:90%;">PanOxyl AM Oil Control Moisturizer, NEW Sheer Formula, Absorbs Excess Oil and Reduces Shine, with Mineral Sunscreen for Acne Prone and Oily And All Skin Tones - 1.7 oz</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.3.3.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.3.3.1.1" style="font-size:90%;">Dry Skin</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.3.3.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.3.3.2.1" style="font-size:90%;">‘#yal’, ‘#uron’, ‘ate’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.3.3.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.3.3.3.1" style="font-size:90%;">Sodium Hyaluronate</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.4.4.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.4.4.1.1" style="font-size:90%;">Sensitive Skin</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.4.4.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.4.4.2.1" style="font-size:90%;">‘#olo’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.4.4.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.4.4.3.1" style="font-size:90%;">Bisabolol</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.5.5.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.5.5.1.1" style="font-size:90%;">Dark Circles</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.5.5.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.5.5.2.1" style="font-size:90%;">‘but’, ‘#yl’, ‘#ic’, ‘#yla’, ‘#te’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.5.5.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.5.5.3.1" style="font-size:90%;">Butyloctyl Salicylate</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="S5.T3.3.1.6.6.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.3.1.6.6.1.1">
<span class="ltx_p" id="S5.T3.3.1.6.6.1.1.1" style="width:368.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.6.6.1.1.1.1" style="font-size:90%;">Product</span><span class="ltx_text" id="S5.T3.3.1.6.6.1.1.1.2" style="font-size:90%;">: </span><span class="ltx_text ltx_font_italic" id="S5.T3.3.1.6.6.1.1.1.3" style="font-size:90%;">Good Molecules BHA Clarifying Gel Cream - Facial Cream with Salicylic Acid, Green Tea, and Gotu Kola Extract Soothe and Hydrate - Skincare for Face</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.7.7.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.7.7.1.1" style="font-size:90%;">Acne</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.7.7.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.7.7.2.1" style="font-size:90%;">‘sal’, ‘#ic’, ‘#yl’, ‘#ic’, ‘acid’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.7.7.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.7.7.3.1" style="font-size:90%;">Salicylic Acid</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.8.8.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.8.8.1.1" style="font-size:90%;">Dry Skin</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.8.8.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.8.8.2.1" style="font-size:90%;">‘#ly’, ‘#cer’, ‘#in’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.8.8.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.8.8.3.1" style="font-size:90%;">Glycerin</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.9.9.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.9.9.1.1" style="font-size:90%;">Redness</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.9.9.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.9.9.2.1" style="font-size:90%;">‘allan’, ‘#to’</span></td>
<td class="ltx_td ltx_align_left" id="S5.T3.3.1.9.9.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.9.9.3.1" style="font-size:90%;">Allantoin</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="S5.T3.3.1.10.10.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.3.1.10.10.1.1">
<span class="ltx_p" id="S5.T3.3.1.10.10.1.1.1" style="width:368.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.10.10.1.1.1.1" style="font-size:90%;">Product</span><span class="ltx_text" id="S5.T3.3.1.10.10.1.1.1.2" style="font-size:90%;">: </span><span class="ltx_text ltx_font_italic" id="S5.T3.3.1.10.10.1.1.1.3" style="font-size:90%;">I DEW CARE Moisturizer Face Cream - Chill Kitten — Moringa Seed, Prickly Pear, Heartleaf Extract, 24 Hour, Aloe Vera Gel for Dry, Red Skin, Cactus Oil-free, 1.69 Fl Oz</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.11.11.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.11.11.1.1" style="font-size:90%;">Redness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.11.11.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.11.11.2.1" style="font-size:90%;">‘tea’, ‘ni’, ‘#ac’, ‘#ina’, ‘#mide’</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.3.1.11.11.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.11.11.3.1" style="font-size:90%;">Green Tea, Niacinamide</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.3.1.12.12.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.12.12.1.1" style="font-size:90%;">Fine Lines and Wrinkles</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.3.1.12.12.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T3.3.1.12.12.2.1" style="font-size:90%;">‘as’, ‘#cor’, ‘#bic’</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.3.1.12.12.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S5.T3.3.1.12.12.3.1" style="font-size:90%;">Ascorbic Acid</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">We also assess the high attention tokens for each predicted label of a single product and show that these tokens are different across attributes of a given product.
This means that our model has learned to pay attention to different tokens when it is being asked about different attributes.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S5.T3" title="Table 3 ‣ 5.4. Explainability ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Table 3</span></a> demonstrates some of the examined products.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Robustness in Low Data Regime</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="366" id="S5.F5.g1" src="x4.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Validation accuracy training on various sizes of dataset</figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">In this section, we present empirical evidence demonstrating the robust performance of BT-BERT even when the volume of training data is limited.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S5.F5" title="Figure 5 ‣ 5.5. Robustness in Low Data Regime ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 5</span></a> shows the validation accuracy across various degrees of data scarcity, namely when the model is trained using the full dataset, as well as 1/2, 1/4, and 1/8 of the full training corpus.
In each training run, we systematically down-sample the training set and keep the validation set constant, i.e., it still contains the same 2246 products.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">Note that for the 1/8 training, the model is trained with only 1167 products and yet still the validation accuracy only drops by less than 1.25%.
We hypothesize that the robust performance of BT-BERT in such a low-resource regime can be attributed to the fact that it is an energy-based implicit model, as opposed to an explicit classifier.
The same scaling pattern is observed in other energy-based models <cite class="ltx_cite ltx_citemacro_citep">(Florence et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib11" title="">2021</a>)</cite>.
Additionally, we attribute part of such robustness to the implicit data augmentation strategy employed in training—specifically, each product is paired with all 33 query attributes, exposing our model to diverse input contexts.
We have not yet fully characterize the scaling behaviors of implicit and explicit models.
It is possible that with improved training techniques, the explicit approach can close the gap in low-resource regimes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Does the choice of logits transformation matter?</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.4">Our early experiments indicate that scaling the probability linearly with 16 achieves better results than not employing it.
We explored an alternative scaling formulation using <math alttext="{f(x)=\log(x/(1-x))}" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.3"><semantics id="S6.SS1.p1.1.m1.3a"><mrow id="S6.SS1.p1.1.m1.3.3" xref="S6.SS1.p1.1.m1.3.3.cmml"><mrow id="S6.SS1.p1.1.m1.3.3.3" xref="S6.SS1.p1.1.m1.3.3.3.cmml"><mi id="S6.SS1.p1.1.m1.3.3.3.2" xref="S6.SS1.p1.1.m1.3.3.3.2.cmml">f</mi><mo id="S6.SS1.p1.1.m1.3.3.3.1" xref="S6.SS1.p1.1.m1.3.3.3.1.cmml">⁢</mo><mrow id="S6.SS1.p1.1.m1.3.3.3.3.2" xref="S6.SS1.p1.1.m1.3.3.3.cmml"><mo id="S6.SS1.p1.1.m1.3.3.3.3.2.1" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.3.cmml">(</mo><mi id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">x</mi><mo id="S6.SS1.p1.1.m1.3.3.3.3.2.2" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S6.SS1.p1.1.m1.3.3.2" xref="S6.SS1.p1.1.m1.3.3.2.cmml">=</mo><mrow id="S6.SS1.p1.1.m1.3.3.1.1" xref="S6.SS1.p1.1.m1.3.3.1.2.cmml"><mi id="S6.SS1.p1.1.m1.2.2" xref="S6.SS1.p1.1.m1.2.2.cmml">log</mi><mo id="S6.SS1.p1.1.m1.3.3.1.1a" xref="S6.SS1.p1.1.m1.3.3.1.2.cmml">⁡</mo><mrow id="S6.SS1.p1.1.m1.3.3.1.1.1" xref="S6.SS1.p1.1.m1.3.3.1.2.cmml"><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.2" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.1.2.cmml">(</mo><mrow id="S6.SS1.p1.1.m1.3.3.1.1.1.1" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.3.3.1.1.1.1.3" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.3.cmml">x</mi><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.1.2" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.2.cmml">/</mo><mrow id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml"><mn id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.2" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.3" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S6.SS1.p1.1.m1.3.3.1.1.1.3" stretchy="false" xref="S6.SS1.p1.1.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.3b"><apply id="S6.SS1.p1.1.m1.3.3.cmml" xref="S6.SS1.p1.1.m1.3.3"><eq id="S6.SS1.p1.1.m1.3.3.2.cmml" xref="S6.SS1.p1.1.m1.3.3.2"></eq><apply id="S6.SS1.p1.1.m1.3.3.3.cmml" xref="S6.SS1.p1.1.m1.3.3.3"><times id="S6.SS1.p1.1.m1.3.3.3.1.cmml" xref="S6.SS1.p1.1.m1.3.3.3.1"></times><ci id="S6.SS1.p1.1.m1.3.3.3.2.cmml" xref="S6.SS1.p1.1.m1.3.3.3.2">𝑓</ci><ci id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">𝑥</ci></apply><apply id="S6.SS1.p1.1.m1.3.3.1.2.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1"><log id="S6.SS1.p1.1.m1.2.2.cmml" xref="S6.SS1.p1.1.m1.2.2"></log><apply id="S6.SS1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1"><divide id="S6.SS1.p1.1.m1.3.3.1.1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.2"></divide><ci id="S6.SS1.p1.1.m1.3.3.1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.3">𝑥</ci><apply id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1"><minus id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.1"></minus><cn id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.2">1</cn><ci id="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.3.3.1.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.3c">{f(x)=\log(x/(1-x))}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.3d">italic_f ( italic_x ) = roman_log ( italic_x / ( 1 - italic_x ) )</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mi id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><ci id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">italic_x</annotation></semantics></math> represents the attention value of the first query token from all attention heads.
The design is inspired by probability theory, where <math alttext="x/(1-x)" class="ltx_Math" display="inline" id="S6.SS1.p1.3.m3.1"><semantics id="S6.SS1.p1.3.m3.1a"><mrow id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml"><mi id="S6.SS1.p1.3.m3.1.1.3" xref="S6.SS1.p1.3.m3.1.1.3.cmml">x</mi><mo id="S6.SS1.p1.3.m3.1.1.2" xref="S6.SS1.p1.3.m3.1.1.2.cmml">/</mo><mrow id="S6.SS1.p1.3.m3.1.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.1.1.cmml"><mo id="S6.SS1.p1.3.m3.1.1.1.1.2" stretchy="false" xref="S6.SS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S6.SS1.p1.3.m3.1.1.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.1.1.cmml"><mn id="S6.SS1.p1.3.m3.1.1.1.1.1.2" xref="S6.SS1.p1.3.m3.1.1.1.1.1.2.cmml">1</mn><mo id="S6.SS1.p1.3.m3.1.1.1.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.1.1.1.cmml">−</mo><mi id="S6.SS1.p1.3.m3.1.1.1.1.1.3" xref="S6.SS1.p1.3.m3.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S6.SS1.p1.3.m3.1.1.1.1.3" stretchy="false" xref="S6.SS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><apply id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"><divide id="S6.SS1.p1.3.m3.1.1.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2"></divide><ci id="S6.SS1.p1.3.m3.1.1.3.cmml" xref="S6.SS1.p1.3.m3.1.1.3">𝑥</ci><apply id="S6.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1.1.1"><minus id="S6.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1.1.1.1.1"></minus><cn id="S6.SS1.p1.3.m3.1.1.1.1.1.2.cmml" type="integer" xref="S6.SS1.p1.3.m3.1.1.1.1.1.2">1</cn><ci id="S6.SS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S6.SS1.p1.3.m3.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">x/(1-x)</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.3.m3.1d">italic_x / ( 1 - italic_x )</annotation></semantics></math> is commonly referred to as the odds or odds ratio when <math alttext="x" class="ltx_Math" display="inline" id="S6.SS1.p1.4.m4.1"><semantics id="S6.SS1.p1.4.m4.1a"><mi id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><ci id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.4.m4.1d">italic_x</annotation></semantics></math> is a probability.
Taking the logarithm of the odds ratio is a common transformation used in logistic regression to convert probability into logits.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Additionally, we experimented with using the summation and average of the attention values from the first three query tokens as <math alttext="x" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mi id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><ci id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">italic_x</annotation></semantics></math> before applying the log transformation.
However, these variations did not produce better results.
Ultimately, we chose the linear scaling method of multiplying by 16 due to its simplicity and slightly faster computation times.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Finetuning on Additional Attributes</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In this section, we discuss the adaptability of implicit models in incorporating new labeled attributes as they become available.
We design a scenario mirroring real-world dynamics, where an initial dataset comprises 30 out of 33 labels, with the remaining 3 labels introduced in a subsequent release.
Such scenarios are commonplace in the beauty industry, where emerging trends and evolving consumer preferences necessitate the addition of new product attributes.
For instance, the advent of clean beauty as a trend in 2023 <cite class="ltx_cite ltx_citemacro_citep">(MCGRATH, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib19" title="">2023</a>)</cite> underscores the relevance of this work.
Through comprehensive analysis and experimentation, we assess and highlight the implicit model’s efficacy in seamlessly incorporating new attributes.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.3">We removed the labels for ‘Fragrance Free’ (generally-preferred), ‘Oily Skin’ (skin type), and ‘Acne’ (skin concern) from the full dataset (<math alttext="\mathcal{D}_{\text{full}}" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><msub id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">𝒟</mi><mtext id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3a.cmml">full</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">𝒟</ci><ci id="S6.SS2.p2.1.m1.1.1.3a.cmml" xref="S6.SS2.p2.1.m1.1.1.3"><mtext id="S6.SS2.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S6.SS2.p2.1.m1.1.1.3">full</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\mathcal{D}_{\text{full}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT full end_POSTSUBSCRIPT</annotation></semantics></math>) and trained a model on the remaining 30 labels (<math alttext="\mathcal{D}_{\text{30}}" class="ltx_Math" display="inline" id="S6.SS2.p2.2.m2.1"><semantics id="S6.SS2.p2.2.m2.1a"><msub id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.2.m2.1.1.2" xref="S6.SS2.p2.2.m2.1.1.2.cmml">𝒟</mi><mtext id="S6.SS2.p2.2.m2.1.1.3" xref="S6.SS2.p2.2.m2.1.1.3a.cmml">30</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><apply id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.2.m2.1.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S6.SS2.p2.2.m2.1.1.2.cmml" xref="S6.SS2.p2.2.m2.1.1.2">𝒟</ci><ci id="S6.SS2.p2.2.m2.1.1.3a.cmml" xref="S6.SS2.p2.2.m2.1.1.3"><mtext id="S6.SS2.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S6.SS2.p2.2.m2.1.1.3">30</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">\mathcal{D}_{\text{30}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.2.m2.1d">caligraphic_D start_POSTSUBSCRIPT 30 end_POSTSUBSCRIPT</annotation></semantics></math>).
Then, we add back the removed labels and finetune the previously trained model with the complete dataset for only one epoch.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S6.T4" title="Table 4 ‣ 6.2. Finetuning on Additional Attributes ‣ 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Table 4</span></a> shows the validation accuracies before and after the finetuning step.
When finetuning on only the three additional labels (<math alttext="\mathcal{D}_{\text{3}}" class="ltx_Math" display="inline" id="S6.SS2.p2.3.m3.1"><semantics id="S6.SS2.p2.3.m3.1a"><msub id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">𝒟</mi><mtext id="S6.SS2.p2.3.m3.1.1.3" xref="S6.SS2.p2.3.m3.1.1.3a.cmml">3</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S6.SS2.p2.3.m3.1.1.2.cmml" xref="S6.SS2.p2.3.m3.1.1.2">𝒟</ci><ci id="S6.SS2.p2.3.m3.1.1.3a.cmml" xref="S6.SS2.p2.3.m3.1.1.3"><mtext id="S6.SS2.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S6.SS2.p2.3.m3.1.1.3">3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">\mathcal{D}_{\text{3}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.3.m3.1d">caligraphic_D start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>), we observe a significant drop in validation accuracy for the existing 30 labels in the validation set.
We believe this is due to the <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.3.1">catastrophic forgetting</span> problem and could potentially be alleviated by using more advanced finetuning algorithms <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib12" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib16" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13628v1#bib.bib33" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">When finetuning with <math alttext="\mathcal{D}_{\text{full}}" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><msub id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p3.1.m1.1.1.2" xref="S6.SS2.p3.1.m1.1.1.2.cmml">𝒟</mi><mtext id="S6.SS2.p3.1.m1.1.1.3" xref="S6.SS2.p3.1.m1.1.1.3a.cmml">full</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><apply id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p3.1.m1.1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p3.1.m1.1.1.2.cmml" xref="S6.SS2.p3.1.m1.1.1.2">𝒟</ci><ci id="S6.SS2.p3.1.m1.1.1.3a.cmml" xref="S6.SS2.p3.1.m1.1.1.3"><mtext id="S6.SS2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S6.SS2.p3.1.m1.1.1.3">full</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">\mathcal{D}_{\text{full}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT full end_POSTSUBSCRIPT</annotation></semantics></math>, we observe only a slight drop of performance when predicting the existing 30 labels, but the accuracy for the new labels is drastically improved.
It is important to note that this finetuning procedure is impossible when using explicit models, since the number of output classes is different and therefore the classifier must be replaced and retrained.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Model performance on partially held out data. In this experiment, we evaluate the model’s ability to incorporate additional labels when they become available.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.3" style="width:208.1pt;height:43.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.5pt,5.1pt) scale(0.809402812239559,0.809402812239559) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T4.1.1.1.1">
<span class="ltx_text" id="S6.T4.1.1.1.1.1" style="font-size:90%;">Train </span><math alttext="\mathcal{D_{\text{30}}}" class="ltx_Math" display="inline" id="S6.T4.1.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.1.m1.1a"><msub id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.T4.1.1.1.1.m1.1.1.2" mathsize="90%" xref="S6.T4.1.1.1.1.m1.1.1.2.cmml">𝒟</mi><mtext id="S6.T4.1.1.1.1.m1.1.1.3" mathsize="90%" xref="S6.T4.1.1.1.1.m1.1.1.3a.cmml">30</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T4.1.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.1.m1.1.1.2">𝒟</ci><ci id="S6.T4.1.1.1.1.m1.1.1.3a.cmml" xref="S6.T4.1.1.1.1.m1.1.1.3"><mtext id="S6.T4.1.1.1.1.m1.1.1.3.cmml" mathsize="63%" xref="S6.T4.1.1.1.1.m1.1.1.3">30</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\mathcal{D_{\text{30}}}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT 30 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2.2">
<span class="ltx_text" id="S6.T4.2.2.2.2.1" style="font-size:90%;">Finetune </span><math alttext="\mathcal{D_{\text{3}}}" class="ltx_Math" display="inline" id="S6.T4.2.2.2.2.m1.1"><semantics id="S6.T4.2.2.2.2.m1.1a"><msub id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.T4.2.2.2.2.m1.1.1.2" mathsize="90%" xref="S6.T4.2.2.2.2.m1.1.1.2.cmml">𝒟</mi><mtext id="S6.T4.2.2.2.2.m1.1.1.3" mathsize="90%" xref="S6.T4.2.2.2.2.m1.1.1.3a.cmml">3</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><apply id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.2.2.2.2.m1.1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S6.T4.2.2.2.2.m1.1.1.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.2">𝒟</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3a.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3"><mtext id="S6.T4.2.2.2.2.m1.1.1.3.cmml" mathsize="63%" xref="S6.T4.2.2.2.2.m1.1.1.3">3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">\mathcal{D_{\text{3}}}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.2.m1.1d">caligraphic_D start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.3.3.3">
<span class="ltx_text" id="S6.T4.3.3.3.3.1" style="font-size:90%;">Finetune </span><math alttext="\mathcal{D_{\text{full}}}" class="ltx_Math" display="inline" id="S6.T4.3.3.3.3.m1.1"><semantics id="S6.T4.3.3.3.3.m1.1a"><msub id="S6.T4.3.3.3.3.m1.1.1" xref="S6.T4.3.3.3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.T4.3.3.3.3.m1.1.1.2" mathsize="90%" xref="S6.T4.3.3.3.3.m1.1.1.2.cmml">𝒟</mi><mtext id="S6.T4.3.3.3.3.m1.1.1.3" mathsize="90%" xref="S6.T4.3.3.3.3.m1.1.1.3a.cmml">full</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.3.m1.1b"><apply id="S6.T4.3.3.3.3.m1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.3.3.3.3.m1.1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S6.T4.3.3.3.3.m1.1.1.2.cmml" xref="S6.T4.3.3.3.3.m1.1.1.2">𝒟</ci><ci id="S6.T4.3.3.3.3.m1.1.1.3a.cmml" xref="S6.T4.3.3.3.3.m1.1.1.3"><mtext id="S6.T4.3.3.3.3.m1.1.1.3.cmml" mathsize="63%" xref="S6.T4.3.3.3.3.m1.1.1.3">full</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.3.m1.1c">\mathcal{D_{\text{full}}}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.3.3.3.3.m1.1d">caligraphic_D start_POSTSUBSCRIPT full end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.3.3.4.1.1"><span class="ltx_text" id="S6.T4.3.3.4.1.1.1" style="font-size:90%;">Acc. on 30 labels</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.3.3.4.1.2"><span class="ltx_text" id="S6.T4.3.3.4.1.2.1" style="font-size:90%;">93.9%</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.4.1.3"><span class="ltx_text" id="S6.T4.3.3.4.1.3.1" style="font-size:90%;">82.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.4.1.4"><span class="ltx_text" id="S6.T4.3.3.4.1.4.1" style="font-size:90%;">93.4%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.3.3.5.2.1"><span class="ltx_text" id="S6.T4.3.3.5.2.1.1" style="font-size:90%;">Acc. on 3 labels</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T4.3.3.5.2.2"><span class="ltx_text" id="S6.T4.3.3.5.2.2.1" style="font-size:90%;">59.6%</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.3.5.2.3"><span class="ltx_text" id="S6.T4.3.3.5.2.3.1" style="font-size:90%;">94.7%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.3.5.2.4"><span class="ltx_text" id="S6.T4.3.3.5.2.4.1" style="font-size:90%;">93.5%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg2.4.1.1">Algorithm 2</span> </span> <span class="ltx_text" id="alg2.5.2" style="font-size:113%;">Key Token Extraction Based on Attention Values</span></figcaption>
<div class="ltx_listing ltx_listing" id="alg2.6">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_bold" id="alg2.l1.2" style="font-size:80%;">function</span><span class="ltx_text" id="alg2.l1.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg2.l1.4" style="font-size:80%;">GetTopAttentionTokens</span><span class="ltx_text" id="alg2.l1.5" style="font-size:80%;">(</span><span class="ltx_text ltx_font_typewriter" id="alg2.l1.6" style="font-size:80%;">input_ids, attentions, topk</span><span class="ltx_text" id="alg2.l1.7" style="font-size:80%;">)
</span>
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l2.1.1.1" style="font-size:80%;">2:</span></span><span class="ltx_text" id="alg2.l2.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l2.3" style="font-size:80%;color:#0000FF;"># input_ids is a tensor of shape (seqlen,)</span><span class="ltx_text" id="alg2.l2.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l3.1.1.1" style="font-size:80%;">3:</span></span><span class="ltx_text" id="alg2.l3.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l3.3" style="font-size:80%;color:#0000FF;"># attentions is a tensor of shape (heads, seqlen, seqlen)</span><span class="ltx_text" id="alg2.l3.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l4.1.1.1" style="font-size:80%;">4:</span></span><span class="ltx_text" id="alg2.l4.2" style="font-size:80%;">    </span>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l5.1.1.1" style="font-size:80%;">5:</span></span><span class="ltx_text" id="alg2.l5.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l5.3" style="font-size:80%;color:#0000FF;"># get index of top-k attention per row across all heads</span><span class="ltx_text" id="alg2.l5.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l6.1.1.1" style="font-size:80%;">6:</span></span><span class="ltx_text" id="alg2.l6.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l6.3" style="font-size:80%;">topk_indices = attentions.flatten(0, 1).topk(topk).indices</span><span class="ltx_text" id="alg2.l6.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l7.1.1.1" style="font-size:80%;">7:</span></span><span class="ltx_text" id="alg2.l7.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l7.3" style="font-size:80%;">topk_indices = topk_indices.unique()</span><span class="ltx_text" id="alg2.l7.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l8.1.1.1" style="font-size:80%;">8:</span></span><span class="ltx_text" id="alg2.l8.2" style="font-size:80%;">    </span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l9.1.1.1" style="font-size:80%;">9:</span></span><span class="ltx_text" id="alg2.l9.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l9.3" style="font-size:80%;color:#0000FF;"># convert col indices to token strings</span><span class="ltx_text" id="alg2.l9.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l10.1.1.1" style="font-size:80%;">10:</span></span><span class="ltx_text" id="alg2.l10.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l10.3" style="font-size:80%;">topk_tokens = convert_ids_to_tokens(input_ids[topk_indices])</span><span class="ltx_text" id="alg2.l10.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l11.1.1.1" style="font-size:80%;">11:</span></span><span class="ltx_text" id="alg2.l11.2" style="font-size:80%;">    </span>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l12.1.1.1" style="font-size:80%;">12:</span></span><span class="ltx_text" id="alg2.l12.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l12.3" style="font-size:80%;color:#0000FF;"># remove non-meaningful tokens</span><span class="ltx_text" id="alg2.l12.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l13.1.1.1" style="font-size:80%;">13:</span></span><span class="ltx_text" id="alg2.l13.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l13.3" style="font-size:80%;">TO_REMOVE = [‘,’, ‘[CLS]’, ‘[SEP]’, ‘(’, ‘)’, ‘[PAD]’]</span><span class="ltx_text" id="alg2.l13.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l14.1.1.1" style="font-size:80%;">14:</span></span><span class="ltx_text" id="alg2.l14.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" id="alg2.l14.3" style="font-size:80%;">topk_tokens = [k for k in topk_tokens if k not in TO_REMOVE]</span><span class="ltx_text" id="alg2.l14.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l15.1.1.1" style="font-size:80%;">15:</span></span><span class="ltx_text" id="alg2.l15.2" style="font-size:80%;">    </span>
</div>
<div class="ltx_listingline" id="alg2.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l16.1.1.1" style="font-size:80%;">16:</span></span><span class="ltx_text ltx_font_bold" id="alg2.l16.2" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l16.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l16.4" style="font-size:80%;">function</span>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Alternating Query Attribute Tokens</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In this section, we highlight the benefit of our implicit model during inference time.
First, we show that it can handle similar but not identical query attributes.
We take ‘Fine Lines and Wrinkles’ as an example and replace the query attribute with just a single word ‘Lines’ for a commonly available anti-wrinkle renewal skin cream.
We use <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#alg2" title="Algorithm 2 ‣ 6.2. Finetuning on Additional Attributes ‣ 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Algorithm 2</span></a> to extract the high attention tokens and track how they change when the attribute tokens are replaced.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">We observed a number of overlapping tokens especially those addressing lines and wrinkles—<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.1">‘#chio’</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.2">‘pu’</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.3">‘soy’</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.4">‘lines’</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.5">‘baku’</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.6">‘re’</span>, and <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.7">‘#tino’</span>.
We also identified non-overlapping tokens such as <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.8">water</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.9">after</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.10">cleansing</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.11">fine</span>, <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.12">cart</span>, and <span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.13">wr</span>.
It is important to note that the non-overlapping tokens, such as ’water’ and ’cleansing,’ are more general and not as directly relevant to the specific skin concern.
We believe that this approach can help us better understand the ingredients and their target uses.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Explainable Beauty Recommendation and Customer Understanding</h2>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Explainable Beauty Recommendation</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1">One critical application of ingredient-based attribute extraction lies in delivering explainable recommendations to beauty customers.
In the ever-evolving beauty industry, where personalization is key, transparency and clarity in product suggestions are vital.
As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S3.F2" title="Figure 2 ‣ 3. System Overview ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>, skincare recommendations are made using a point-wise approach, where each product is individually assessed based on the customer’s specific skin type and concerns.
Here, the customer has selected “oily” skin and concerns of “acne” and “dullskin”.
The recommended products not only contain ingredients intended to address these issues but are also compatible with the customer’s stated skin type, enhancing the trustworthiness and relevance of each suggestion.
Each product is annotated with its predicted target skin concerns and skin types, alongside the ingredients intended to address those concerns, using <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#alg2" title="Algorithm 2 ‣ 6.2. Finetuning on Additional Attributes ‣ 6. Discussion ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Algorithm 2</span></a> discussed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#S5.SS4" title="5.4. Explainability ‣ 5. Experiments ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Section 5.4</span></a>.
For example, Salicylic Acid is highlighted for its anti-acne properties across various product types like cleansers, pads, and serums.
Furthermore, the system strategically omits products with oil-based ingredients that could exacerbate oily skin, ensuring that recommendations are appropriate for the user’s concerns.</p>
</div>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p2.1">By providing fact-based explanations for recommended products, this approach offers clear and transparent justifications for the recommendations.
As customers purchase and use products with effective ingredients, they are more likely to achieve the desired skin results, fostering long-term trust and encouraging repeat engagement with the e-commerce store.
This method not only empowers customers to make informed purchasing decisions but also strengthens their trust in the recommendation system.
This approach is versatile and can be applied broadly across most beauty catalogs, including haircare and makeup, where ingredients stay on the skin for extended periods.
In the context of strategic and utility-aware recommendations, explainability is crucial for aligning personalized suggestions with both individual needs and broader objectives.
This alignment ultimately enhances customer confidence, satisfaction, and long-term audience growth.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Customer understanding</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px2.p1.1">Conversely, customer propensity toward specific attributes—such as preferred skin type, skin concerns, and ingredient preferences—can be inferred from their past purchases.
Our future work focuses on understanding customer skin types and concerns by building upon existing attribute extraction methodologies.
This advancement will enable further refinement of our recommendation algorithms, particularly in the ranking layer.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We present an energy-based implicit model for extracting beauty-specific attributes trained using end-to-end supervised learning.
We empirically show that the implicit approach outperforms traditional explicit classifiers in terms of accuracy, precision, and other evaluation metrics.
Aside from better performance, we show that the implicit model is explainable, robust to low-data scenarios, and easy to incorporate new attributes as they become available.
Using the explainability feature of our model, we propose novel ways to use the predictions without additional training by comparing and contrasting the high value tokens across different products and attributes.
We have not yet fully characterized the limits of the model’s capabilities.
Currently, we only qualitatively identify the high attention value tokens and discuss how they are related to the specific skin concerns and skin types in our attention analysis.
We wish to better quantify the correlations between all predicted ingredients and the attributes.
Although our work focuses on beauty attribute extraction, we believe the simplicity of our approach and comprehensiveness of our analysis provide a solid foundation for future research in designing more capable and explainable models in all domains of machine learning.
In future work, we will validate the generated attributes within downstream recommendation systems and conduct a thorough evaluation. Furthermore, we will assess the impact of explainability for end users through A/B testing.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afshar et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Parnian Afshar, Jenny
Yeon, Andriy Levitskyy, Rahul Suresh,
and Amin Banitalebi-Dehkordi.
2023.

</span>
<span class="ltx_bibblock">Improving the accuracy of beauty product
recommendations by assessing face illumination quality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">arXiv preprint arXiv:2309.04022</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alashkar et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Taleb Alashkar, Songyao
Jiang, Shuyang Wang, and Yun Fu.
2017.

</span>
<span class="ltx_bibblock">Examples-rules guided deep neural network for
makeup recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the AAAI
conference on artificial intelligence</em>, Vol. 31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Jimmy Lei Ba, Jamie Ryan
Kiros, and Geoffrey E Hinton.
2016.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv preprint arXiv:1607.06450</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cardoso et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Angelo Cardoso, Fabio
Daolio, and Saul Vargas.
2018.

</span>
<span class="ltx_bibblock">Product Characterisation towards Personalisation:
Learning Attributes from Unstructured Data to Recommend Fashion Products. In
<em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery &amp; Data Mining</em>. 80–89.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiticariu et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Laura Chiticariu,
Rajasekar Krishnamurthy, Yunyao Li,
Frederick Reiss, and Shivakumar
Vaithyanathan. 2010.

</span>
<span class="ltx_bibblock">Domain Adaptation of Rule-Based Annotators for
Named-Entity Recognition Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing</em>.
Association for Computational Linguistics,
Cambridge, MA, 1002–1012.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D10-1098" title="">https://aclanthology.org/D10-1098</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2018.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1810.04805" title="">http://arxiv.org/abs/1810.04805</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">CoRR</em> abs/1810.04805
(2018).

</span>
<span class="ltx_bibblock">arXiv:1810.04805

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dezaki et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fatemeh Taheri Dezaki,
Himanshu Arora, Rahul Suresh, and
Amin Banitalebi-Dehkordi. 2023.

</span>
<span class="ltx_bibblock">Automated material properties extraction for
enhanced beauty product discovery and makeup virtual try-on.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">arXiv preprint arXiv:2312.00766</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du and Mordatch (2019)</span>
<span class="ltx_bibblock">
Yilun Du and Igor
Mordatch. 2019.

</span>
<span class="ltx_bibblock">Implicit Generation and Modeling with Energy-Based
Models.
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf</a>.
In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing
Systems</em>, H. Wallach,
H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc,
E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feeds (2024)</span>
<span class="ltx_bibblock">
Crawl Feeds.
2024.

</span>
<span class="ltx_bibblock">Amazon USA Beauty Products Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.world/crawlfeeds/amazon-usa-beauty-products-dataset" title="">https://data.world/crawlfeeds/amazon-usa-beauty-products-dataset</a>
</span>
<span class="ltx_bibblock">Accessed: 2024-08-26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Florence et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pete Florence, Corey
Lynch, Andy Zeng, Oscar A Ramirez,
Ayzaan Wahid, Laura Downs,
Adrian Wong, Johnny Lee,
Igor Mordatch, and Jonathan Tompson.
2021.

</span>
<span class="ltx_bibblock">Implicit Behavioral Cloning.
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rif3a5NAxU6" title="">https://openreview.net/forum?id=rif3a5NAxU6</a>. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">5th Annual Conference on Robot Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen,
Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen.
2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2106.09685</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Zhiheng Huang, Wei Xu,
and Kai Yu. 2015.

</span>
<span class="ltx_bibblock">Bidirectional LSTM-CRF Models for Sequence
Tagging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CoRR</em> abs/1508.01991
(2015).

</span>
<span class="ltx_bibblock">arXiv:1508.01991

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1508.01991" title="">http://arxiv.org/abs/1508.01991</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Yann LeCun, Sumit Chopra,
Raia Hadsell, M Ranzato, and
Fujie Huang. 2006.

</span>
<span class="ltx_bibblock">A tutorial on energy-based learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Predicting structured data</em>
1, 0 (2006).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hsiao-Hui Li, Yuan-Hsun
Liao, Yen-Nun Huang, and Po-Jen
Cheng. 2020.

</span>
<span class="ltx_bibblock">Based on machine learning for personalized skin
care products recommendation engine. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">2020
International Symposium on Computer, Consumer and Control (IS3C)</em>.
460–462.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/IS3C50286.2020.00125" title="">https://doi.org/10.1109/IS3C50286.2020.00125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shih-Yang Liu, Chien-Yi
Wang, Hongxu Yin, Pavlo Molchanov,
Yu-Chiang Frank Wang, Kwang-Ting Cheng,
and Min-Hung Chen. 2024.

</span>
<span class="ltx_bibblock">DoRA: Weight-Decomposed Low-Rank Adaptation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2402.09353</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2016)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and
Frank Hutter. 2016.

</span>
<span class="ltx_bibblock">Sgdr: Stochastic gradient descent with warm
restarts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1608.03983</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and
Frank Hutter. 2017.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1711.05101</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MCGRATH (2023)</span>
<span class="ltx_bibblock">
KARA MCGRATH.
2023.

</span>
<span class="ltx_bibblock">Did Clean Beauty Go Too Far?

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.allure.com/story/is-clean-beauty-over" title="">https://www.allure.com/story/is-clean-beauty-over</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff,
Nouamane Tazi, Loïc Magne, and
Nils Reimers. 2022.

</span>
<span class="ltx_bibblock">MTEB: Massive Text Embedding Benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2210.07316</em>
(2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2210.07316" title="">https://doi.org/10.48550/ARXIV.2210.07316</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakajima et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yoko Nakajima, Hirotoshi
Honma, Haruka Aoshima, Tomoyoshi Akiba,
and Shigeru Masuyama. 2019.

</span>
<span class="ltx_bibblock">Recommender System Based on User Evaluations and
Cosmetic Ingredients. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">2019 4th International
Conference on Information Technology (InCIT)</em>. 22–27.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/INCIT.2019.8912051" title="">https://doi.org/10.1109/INCIT.2019.8912051</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Putthividhya and Hu (2011)</span>
<span class="ltx_bibblock">
Duangmanee Putthividhya and
Junling Hu. 2011.

</span>
<span class="ltx_bibblock">Bootstrapped Named Entity Recognition for Product
Attribute Extraction. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">EMNLP</em>.
1557–1567.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.aclweb.org/anthology/D11-1144" title="">http://www.aclweb.org/anthology/D11-1144</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">S et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rubasri. S, Hemavathi S,
K. Jayasakthi, Sangeerani Devi. A,
K. Latha, and N. Gopinath.
2022.

</span>
<span class="ltx_bibblock">Cosmetic Product Selection Using Machine Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">2022 International Conference on
Communication, Computing and Internet of Things (IC3IoT)</em>
(2022), 1–6.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:248753814" title="">https://api.semanticscholar.org/CorpusID:248753814</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skillsmuggler (2024)</span>
<span class="ltx_bibblock">
Skillsmuggler.
2024.

</span>
<span class="ltx_bibblock">Amazon Ratings Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/skillsmuggler/amazon-ratings" title="">https://www.kaggle.com/datasets/skillsmuggler/amazon-ratings</a>
</span>
<span class="ltx_bibblock">Accessed: 2024-08-26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song and Kingma (2021)</span>
<span class="ltx_bibblock">
Yang Song and
Diederik P. Kingma. 2021.

</span>
<span class="ltx_bibblock">How to train your energy-based models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2101.03288" title="">https://arxiv.org/abs/2101.03288</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teh et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
Yee Whye Teh, Max
Welling, Simon Osindero, and
Geoffrey E. Hinton. 2003.

</span>
<span class="ltx_bibblock">Energy-based models for sparse overcomplete
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">J. Mach. Learn. Res.</em> 4,
null (dec 2003),
1235–1260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qifan Wang, Li Yang,
Jingang Wang, Jitin Krishnan,
Bo Dai, Sinong Wang,
Zenglin Xu, Madian Khabsa, and
Hao Ma. 2022.

</span>
<span class="ltx_bibblock">SMARTAVE: Structured Multimodal Transformer for
Product Attribute Value Extraction. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Findings of
the Association for Computational Linguistics: EMNLP 2022</em>,
Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang (Eds.).
Association for Computational Linguistics,
Abu Dhabi, United Arab Emirates,
263–276.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.findings-emnlp.20" title="">https://doi.org/10.18653/v1/2022.findings-emnlp.20</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wood (2024)</span>
<span class="ltx_bibblock">
Laura Wood.
2024.

</span>
<span class="ltx_bibblock">Beauty &amp; Personal Care-Worldwide.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statista.com/outlook/cmo/beauty-personal-care/worldwide" title="">https://www.statista.com/outlook/cmo/beauty-personal-care/worldwide</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu,
Peitian Zhang, and Niklas Muennighoff.
2023.

</span>
<span class="ltx_bibblock">C-Pack: Packaged Resources To Advance General Chinese
Embedding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.07597 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Huimin Xu, Wenting Wang,
Xin Mao, Xinyu Jiang, and
Man Lan. 2019.

</span>
<span class="ltx_bibblock">Scaling Up Open Tagging from Tens to Thousands:
Comprehension Empowered Attribute Value Extraction from Product Title. In
<em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics</em> (Florence, Italy).
5214–5223.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jun Yan, Nasser Zalmout,
Yan Liang, Christan Grant,
Xiang Ren, and Xin Luna Dong.
2021.

</span>
<span class="ltx_bibblock">Adatag: Multi-attribute value extraction from
product profiles with adaptive decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:2106.02318</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil
Mustafa, Alexander Kolesnikov, and
Lucas Beyer. 2023.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training. In
<em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>. 11975–11986.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao,
and Maneesh Agrawala. 2023.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image
diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>. 3836–3847.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Guineng Zheng, Subhabrata
Mukherjee, Xin Luna Dong, and Feifei
Li. 2018.

</span>
<span class="ltx_bibblock">OpenTag: Open Attribute Value Extraction from
Product Profiles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">CoRR</em> abs/1806.01264
(2018).

</span>
<span class="ltx_bibblock">arXiv:1806.01264

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1806.01264" title="">http://arxiv.org/abs/1806.01264</a>
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Labels for Skincare Products</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We define 33 labels for skincare products that include 5 skin types, 11 skin
concerns, and 17 attributes that are generally preferred across beauty products.</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">Target skin types: Dry Skin, Normal Skin, Oily Skin, Combination Skin, Sensitive Skin</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Target skin concerns: Acne, Hydration, Pores, Fine Lines and Wrinkles, Sagging, Dark Spots, Dullness, Redness, Uneven Texture, Dark Circles, Puffiness</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">General preferred beauty attributes: 100% Vegan, Cruelty Free, Fragrance Free, Hypoallergenic, Paraben Free, Mineral Oil Free, Palm Oil Free, Oil Free, Alcohol Free, Sulphate Free, Gluten Free, Silicone Free, Phthalate Free, Talc free, Non Comedogenic, Aluminum Free, Fluoride Free.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Product information and Labels</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Each product comes with a title, list of ingredients, and a Boolean label for each attribute. An example is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13628v1#A1.F6" title="Figure 6 ‣ A.2. Product information and Labels ‣ Appendix A Appendix ‣ Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="76" id="A1.F6.g1" src="extracted/5866900/Figures/example_labels.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Sample Pandas dataframe with product ingredient list (<span class="ltx_text ltx_font_typewriter" id="A1.F6.3.1">Full Ingredients</span>) and title (<span class="ltx_text ltx_font_typewriter" id="A1.F6.4.2">item_name)</span> for each product.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>FuzzySearch Attribute Key Words</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">For FuzzySearch method, We define keywords for each of the 33 labels.</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">Dry Skin: ”dry”, ”all”, ”universal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">Normal Skin: ”normal”, ”all”, ”universal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">Oily Skin: ”oil”, ”all”, ”universal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">Combination Skin: ”combination”, ”all”, ”universal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1">Sensitive Skin: ”sensitive”, ”all”, ”universal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i6.p1">
<p class="ltx_p" id="A1.I2.i6.p1.1">Acne:
”anti acne”,
”blackheads”,
”salicylic acid”,
”Glycolic Acid”,
”Benzoyl Peroxide”,
”breakouts treatment”,
”acne preventing”,
”skin clarifying”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i7.p1">
<p class="ltx_p" id="A1.I2.i7.p1.1">Hydration: ”dehydration”,
”dryness”,
”hydrating”,
”rehydrate”,
”soothing”,
”moisturizing”,
”nourishing”,
”softening”,
”replenishing”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i8.p1">
<p class="ltx_p" id="A1.I2.i8.p1.1">Pores: ”pore”, ”oil control”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i9.p1">
<p class="ltx_p" id="A1.I2.i9.p1.1">Fine Lines and Wrinkles: ”wrinkle”,
”anti-aging”,
”anti aging”,
”anti-aging”,
”wrinkle treatment”,
”wrinkles treatment”,
”skin cell renewal”,
”skin-cell-renewal”,
”plumping”,
”refine skin texture”,
”refine-skin-texture”,
”repairing”,
”fine line”,
”anti aging”,
”plumping”,
”skin cell renewal”,
”replenishing”,
”octinoxate”,
”octisalate”,
”avobenzone”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i10.p1">
<p class="ltx_p" id="A1.I2.i10.p1.1">Sagging: ”firming”, ”wrinkle”, ”anti aging”, ”skin cell renewal”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i11.p1">
<p class="ltx_p" id="A1.I2.i11.p1.1">Dark Spots: ”hyperpigmentation”,
”melasma”,
”dyschromia”,
”brown spot”,
”age spot”,
”dark spot”,
”brightening”,
”even toning”,
”color correction”,
”lightening”,
”antioxidant”,
”oxygenating”,
”whitening”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i12.p1">
<p class="ltx_p" id="A1.I2.i12.p1.1">Dullness: ”even toning”,
”dull skin”,
”lightening”,
”brightening”,
”colour correction”,
”skin cell renewal”,
”rejuvenating”,
”exfoliating”,
”plumping”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i13.p1">
<p class="ltx_p" id="A1.I2.i13.p1.1">Redness: ”redness”, ”anti inflammatory”, ”soothening”, ”soothing”, ”redness reduction”, ”redness removal”, ”oxygenating”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i14.p1">
<p class="ltx_p" id="A1.I2.i14.p1.1">Uneven Texture: ”uneven texture”, ”uneven skin”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i15" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i15.p1">
<p class="ltx_p" id="A1.I2.i15.p1.1">Dark Circles:”puffiness”,
”dark circles”,
”color correction”,
”lightening”,
”antioxidant”,
”radiant skin”,
”brightening”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i16" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i16.p1">
<p class="ltx_p" id="A1.I2.i16.p1.1">100% Vegan: ”vegetarian”, ”plantbased”, ”vegan”, ”animalbyproductfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i17" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i17.p1">
<p class="ltx_p" id="A1.I2.i17.p1.1">Cruelty Free: ”crueltyfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i18" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i18.p1">
<p class="ltx_p" id="A1.I2.i18.p1.1">Fragrance Free: ”unscented”, ”fragrancefree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i19" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i19.p1">
<p class="ltx_p" id="A1.I2.i19.p1.1">Hypoallergenic: ”preservativefree”,
”latexfree”,
”chemicalfree”,
”formaldehydefree”,
”slesfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i20" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i20.p1">
<p class="ltx_p" id="A1.I2.i20.p1.1">Paraben Free: ”preservativefree”, ”slesfree”, ”slsfree”, ”parabenfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i21" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i21.p1">
<p class="ltx_p" id="A1.I2.i21.p1.1">Mineral Oil Free: ”palmoilfree”, ”mineraloilfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i22" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i22.p1">
<p class="ltx_p" id="A1.I2.i22.p1.1">Palm Oil Free: ”palmoilfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i23" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i23.p1">
<p class="ltx_p" id="A1.I2.i23.p1.1">Oil Free: ”oilfree”, ”palmoilfree”, ”mineraloilfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i24" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i24.p1">
<p class="ltx_p" id="A1.I2.i24.p1.1">Alcohol Free: ”alcoholfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i25" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i25.p1">
<p class="ltx_p" id="A1.I2.i25.p1.1">Sulphate Free: ”sulfatefree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i26" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i26.p1">
<p class="ltx_p" id="A1.I2.i26.p1.1">Gluten Free: ”glutenfree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i27" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i27.p1">
<p class="ltx_p" id="A1.I2.i27.p1.1">Silicone Free: ”siliconefree”.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i28" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i28.p1">
<p class="ltx_p" id="A1.I2.i28.p1.1">Phthalate Free: ”phthalatefree”.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 16:23:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
