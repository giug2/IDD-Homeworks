<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place</title>
<!--Generated on Fri Jun  7 17:37:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.05111v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S1" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S2" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Contribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S3" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S4" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Framing the Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S5" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>The Initial Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S5.SS1" title="In 5 The Initial Framework ‣ Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_bold">Category 1 - User Historical Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S5.SS2" title="In 5 The Initial Framework ‣ Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_bold">Category 2 - Salient Environment Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S5.SS3" title="In 5 The Initial Framework ‣ Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span><span class="ltx_text ltx_font_bold">Category 3 - Task Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S5.SS4" title="In 5 The Initial Framework ‣ Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span><span class="ltx_text ltx_font_bold">Category 4 - AI System Data</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S6" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Challenges and Technical Opportunities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#S7" title="In Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\setcopyright</span>
<p class="ltx_p" id="p1.2">rightsretained</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\copyrightinfo</span>
<p class="ltx_p" id="p2.2"><span class="ltx_text ltx_font_italic" id="p2.2.1">Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
Copyright held by the owner/author(s).
ACM CHI’24, May 11-16, 2024, Honolulu, HI, USA</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id1.1.id1">Niharika Mathur
<br class="ltx_break"/></span>
<br class="ltx_break"/>
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="id2.2.id2">Elizabeth Mynatt
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/></span> <span class="ltx_text ltx_font_bold" id="id3.3.id3">Tamara Zubatiy
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Georgia Institute of Technology
</span>
<span class="ltx_contact ltx_role_address">Atlanta, GA, USA
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:nmathur35@gatech.edu">nmathur35@gatech.edu</a>
</span>
<span class="ltx_contact ltx_role_address">Northeastern University
</span>
<span class="ltx_contact ltx_role_address">Boston, MA, USA
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:e.mynatt@northeastern.edu">e.mynatt@northeastern.edu</a>
</span>
<span class="ltx_contact ltx_role_address">Georgia Institute of Technology
</span>
<span class="ltx_contact ltx_role_address">Atlanta, GA, USA
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tzubatiy3@gatech.edu">tzubatiy3@gatech.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">As the permeability of AI systems in interpersonal domains like the home expands, their technical capabilities of generating explanations are required to be aligned with user expectations for transparency and reasoning. This paper presents insights from our ongoing work in understanding the effectiveness of explanations in Conversational AI systems for older adults aging in place and their family caregivers. We argue that in collaborative and multi-user environments like the home, AI systems will make recommendations based on a host of information sources to generate explanations. These sources may be more or less salient based on user mental models of the system and the specific task. We highlight the need for cross technological collaboration between AI systems and other available sources of information in the home to generate multiple explanations for a single user query. Through example scenarios in a caregiving home setting, this paper provides an initial framework for categorizing these sources and informing a potential design space for AI explanations surrounding everyday tasks in the home.</p>
</div>
<section class="ltx_section ltx_align_left" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The vision of Human-Centered Explainable AI (HCXAI) emphasizes that merely providing an explanation for an AI decision is insufficient; it must be situated sociotechnically and contextually within the user’s environment to be effective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib5" title="">5</a>]</cite>. In this paper, we align our work with the workshop’s research objectives by focusing on the following question: <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Whose “voices” are represented in AI explanations?</span>. In proceedings of the Workshop on Human-centered Explainable AI at CHI (2022), Alizadeh et al highlight limitations of commercial Conversational AI systems (CAIs) in facilitating an effective conversation with users leading to user confusion and frustration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib2" title="">2</a>]</cite>. These limitations also increase user burden in employing recovery strategies to establish a somewhat successful interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib14" title="">14</a>]</cite>. Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib2" title="">2</a>]</cite> further argue that explanations in AI systems need to consider a “holistic user experience” and not address single incident queries. Building on this recommendation, we argue that this holistic understanding of the user is crucial in generating contextual AI explanations in CAIs. Our aim is to explore this holistic understanding of the user in further detail, grounded in our work to support older adults aging in place. CAIs (smart speakers or chatbots) are a potent testing ground for explanations since they encapsulate explainability as a dialog problem between the AI agent and the user, and can be used to design Wizard of Oz experiments to evaluate different explanatory conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib8" title="">8</a>]</cite>. Such AI systems are also equipped with the capability to store and recall user interactions over time, thereby providing an opportunity to explore the potential of an enhanced holistic understanding of the user context.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, our primary argument is that CAIs operating within collaborative and multi-user settings like the home must leverage diverse information sources to build affordances that can generate explanations tailored to user motivations. Consider a scenario when an AI system reminds an older adult (John) to take his medication. John, in return, asks why the reminder was made at the time. In the case that John is motivated by monitoring physical items associated with the medication task such as his pillbox, the AI should have the ability to sense information that his pillbox has not been opened in a few hours, and hence use that to explain the timing of the reminder. Alternatively, if John is motivated by focusing on routines adjacent to the medication task, the AI should be able to explain that since John was late in getting home this evening, he likely may have missed his medication with dinnertime and provide additional data to explain and contextualize the reminder.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Given the varied sources of information available to AI systems, there’s potential to generate explanations of differing saliency levels. Our aim is to explore explanations that instill the most confidence in users, considering their mental models and tasks. In this workshop, we hope to discuss an initial framework categorizing these information sources and the challenges of designing effective explanations based on diverse mental models and user preferences, particularly focusing on older adults with MCI. Unlike subject matter experts or younger demographics, older adults are unlikely to have a detailed mental model of AI systems. Given this, possible AI explanations will need to draw from characteristics that are salient in their daily lives, such as routines, preferences and data from previous interactions and common household items.</p>
</div>
</section>
<section class="ltx_section ltx_align_left" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Contribution</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Efforts have been made to categorize information for explanations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib7" title="">7</a>]</cite>, yet they lack specificity regarding contextual understanding from home environments. Liao et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib9" title="">9</a>]</cite> focus on explaining information and designing suitable presentation formats, emphasizing iterative design and evaluation. We argue that AI systems should integrate diverse information sources to bridge the sociotechnical gulf between expectation and experience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib10" title="">10</a>]</cite>. To support this, we propose an initial framework categorizing home information sources and their importance in explanation design. We offer examples for each category to aid AI designers and engineers, suggesting ways to integrate data streams from these sources into AI explanations. These examples stem from ongoing research on interactions among older adults, caregivers, and AI systems. Our aim is to foster discussions on user-centric explanations by considering multiple external sources of information beyond AI algorithms.</p>
</div>
</section>
<section class="ltx_section ltx_align_left" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The framework in this paper is grounded in findings from 2 longitudinal studies of 20 weeks and 10 weeks involving 7 and 10 pairs respectively of older adults with MCI and their caregivers. The aim of these studies was to analyze the longitudinal usage of a CAI in the home, specifically for medication management in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib11" title="">11</a>]</cite>. Participants interacted with the CAI daily over the study duration, facilitated by a conversational medication check-in routine in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib11" title="">11</a>]</cite>. Our research team analyzed 1596 total initiated interactions from the AI system and we recorded 844 responses from older adults and analyzed them manually for patterns and themes. We also conducted semi-structured interviews at different points in the studies to delve deeper into participant experiences and challenges with the AI system. Further details about these studies can be found in our full papers that discuss the design, deployment and evaluation of the longitudinal usage of the AI and of the medication assistant in depth with the participants <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section ltx_align_left" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Framing the Context</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our framework for categorizing information sources serves two main purposes. Firstly, it acts as a foundation for navigating user data within the home environment, shedding light on the variety of accessible information for AI systems. Secondly, we aim to underscore the algorithm-centric nature of AI explanations as a checklist item rather than a human-centric feature. This perspective aligns with the emphasis on understanding the "who" of explanations, as advocated by Ehsan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.05111v1#bib.bib6" title="">6</a>]</cite>. Our position emphasizes the personalized nature of explanation, where each user may necessitate tailored explanations from different sources based on their unique context. Therefore, our framework advocates for referencing diverse knowledge sources and promoting cross-technological collaboration within the home to generate multiple explanations, with the selection depending on the specific context of the user query.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="412" id="S4.F1.g1" src="extracted/5650204/figures/framework.png" width="1196"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>This initial framework categorizes sources of information in the home into 4 larger categories with 9 subcategories.</figcaption>
</figure>
</section>
<section class="ltx_section ltx_align_left" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>The Initial Framework</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We first specify a larger information category, and then subcategories within it. Each sub-category has an example explanation in Figure 1 drawn from scenarios in assisted family caregiving for older adults. The subcategories within the framework are not mutually exclusive; rather, they illustrate how an explanation can be represented differently through various information sources, depending on what a user is motivated by.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_bold" id="S5.SS1.1.1">Category 1 - User Historical Data</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Source of information in this category is the <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">user</span>. This includes information that was conveyed to the AI in historical interactions either <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.2">by</span> a user; or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.3">for</span> a user. In both instances, the AI stores the information as an attached piece of knowledge about the user to append in an explanation. There are three ways in which information can be represented in an explanation in this category - <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.4">As a habit (“usual”)</span> or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.5">As a preference (“likeness”)</span> or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.6">A definite past interaction.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_bold" id="S5.SS2.1.1">Category 2 - Salient Environment Data</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Source of information in this category is information that the AI has gleaned either from an environmental sensor or from another task, i.e., information acquisition happens through a source <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p1.1.1">external</span> to the user’s bounding box. There are two ways in which information can be represented in an explanation in this category - <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p1.1.2">From a smart sensor in the home,</span> or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p1.1.3">From another task (task that is different from the original task that the explanation is sought for)</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span class="ltx_text ltx_font_bold" id="S5.SS3.1.1">Category 3 - Task Data</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Source of information in this category is the contextual <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">specifics</span> or <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.2">features</span> of the task that the explanation is sought for. This is more relevant in complex tasks such as cooking or medication. There are two ways in which information can be represented in an explanation - <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS3.p1.1.3">Explanation with a factual/objective information</span>, or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS3.p1.1.4">Explanation with a subjective value/moral judgment</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span class="ltx_text ltx_font_bold" id="S5.SS4.1.1">Category 4 - AI System Data</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Source of information in this category is the <span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.1">internal AI algorithm</span> that delivers a non-contextual explanation without information from a past interaction, a user preference, or a sensor data. This can be thought of as a last resort for the AI to provide an explanation when either the other sources do not provide significant data or user action is not motivated by the other sources. There are two ways in which information can be represented in an explanation in this category - <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.p1.1.2">Explanation with a numeric confidence score</span>, or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.p1.1.3">Explanation with a qualitative verbal representation</span></p>
</div>
</section>
</section>
<section class="ltx_section ltx_align_left" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Challenges and Technical Opportunities</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">While we are excited about the potential of the presented initial framework to generate user-tailored explanations, there are a few technical opportunities and challenges that we would like to address concerning the implementation of this framework. Among other potential technical systems, we currently envision this framework to be implemented in a feedback loop model that begins with delivering a default explanation to the user at the start, then takes in the user feedback and models that to generate a new explanation from the framework, thereby increasing its user understanding in the process (somewhat similar to a “learning from demonstration” model in robotics). In this model, we envision a potential challenge in being able to capture user feedback that is not explicit in nature, i.e., developing a system that captures implicit feedback in the form of open-ended questions, or a survey, and models that or learns from it to deliver a new explanation. Another potential challenge is that of user privacy when extrapolating data coming in from home sensors to build into explanations. This would require the implementation of security frameworks for sensitive user data coming in from sensors, as well as the incorporation of individual user attitudes towards privacy conscientiousness.</p>
</div>
</section>
<section class="ltx_section ltx_align_left" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduce an initial framework for categorizing home information sources to enhance AI explanations for older adults aging in place for everyday tasks. We advocate for diverse explanations drawn from various user-related information such as their routines, preferences, and activities. Future work will assess the effectiveness of these explanations for older adults with varying motivations and expectations, additionally evaluating their impact on user confidence based on the conveyed information types and their affordances.</p>
</div>
</section>
<section class="ltx_bibliography ltx_align_left" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Fatemeh Alizadeh, Dominik Pins, Aikaterini Mniestri, and Gunnar Stevens. 2022.

</span>
<span class="ltx_bibblock">User-friendly Conversational Explanations: A Research Summary. In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2022 Human-Centered Explainable AI workshop at CHI conference on human factors in computing systems 2022</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Vijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny Luss, Aleksandra Mojsilović, and others. 2019.

</span>
<span class="ltx_bibblock">One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1909.03012</span> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Erin Beneteau, Olivia K Richards, Mingrui Zhang, Julie A Kientz, Jason Yip, and Alexis Hiniker. 2019.

</span>
<span class="ltx_bibblock">Communication breakdowns between families and Alexa. In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2019 CHI conference on human factors in computing systems</span>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Upol Ehsan, Koustuv Saha, Munmun De Choudhury, and Mark O Riedl. 2023.

</span>
<span class="ltx_bibblock">Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the ACM on Human-Computer Interaction</span> 7, CSCW1 (2023), 1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Elizabeth Anne Watkins, Carina Manger, Hal Daumé III, Andreas Riener, and Mark O Riedl. 2022.

</span>
<span class="ltx_bibblock">Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI. In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">CHI conference on human factors in computing systems extended abstracts</span>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018.

</span>
<span class="ltx_bibblock">A survey of methods for explaining black box models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">ACM computing surveys (CSUR)</span> 51, 5 (2018), 1–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sophie F Jentzsch, Sviatlana Höhn, and Nico Hochgeschwender. 2019.

</span>
<span class="ltx_bibblock">Conversational interfaces for explainable AI: a human-centred approach. In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13–14, 2019, Revised Selected Papers 1</span>. Springer, 77–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Q Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, and Daby Sow. 2021.

</span>
<span class="ltx_bibblock">Question-driven design process for explainable AI user experiences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2104.03483</span> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ewa Luger and Abigail Sellen. 2016.

</span>
<span class="ltx_bibblock">" Like Having a Really Bad PA" The Gulf between User Expectation and Experience of Conversational Agents. In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2016 CHI conference on human factors in computing systems</span>. 5286–5297.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Niharika Mathur, Kunal Dhodapkar, Tamara Zubatiy, Jiachen Li, Brian Jones, and Elizabeth Mynatt. 2022.

</span>
<span class="ltx_bibblock">A Collaborative Approach to Support Medication Management in Older Adults with Mild Cognitive Impairment Using Conversational Assistants (CAs). In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility</span>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Alex Sciuto, Arnita Saini, Jodi Forlizzi, and Jason I Hong. 2018.

</span>
<span class="ltx_bibblock">" Hey Alexa, What’s Up?" A Mixed-Methods Studies of In-Home Conversational Agent Usage. In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2018 designing interactive systems conference</span>. 857–868.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019.

</span>
<span class="ltx_bibblock">Designing theory-driven user-centric explainable AI. In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2019 CHI conference on human factors in computing systems</span>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tamara Zubatiy, Kayci L Vickers, Niharika Mathur, and Elizabeth D Mynatt. 2021.

</span>
<span class="ltx_bibblock">Empowering dyads of older adults with mild cognitive impairment and their care partners using conversational agents. In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</span>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="Authors’ choice; of terms; separated; by
semicolons; include commas, within terms only; required." property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Categorizing Sources of Information for Explanations in Conversational AI Systems in the Home for Older Adults Aging in Place" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun  7 17:37:48 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
