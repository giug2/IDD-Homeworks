<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG</title>
<!--Generated on Thu Sep 12 01:41:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Text retrieval,  ranking models,  embedding models,  retrieval-augmented generation,  rag pipelines,  model deployment,  transformers." lang="en" name="keywords"/>
<base href="/html/2409.07691v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S1" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S2" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Benchmarking ranking models for Q&amp;A text retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS1" title="In 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Retrieval models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS2" title="In 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Ranking models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS3" title="In 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS4" title="In 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Benchmark results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS5" title="In 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>A small note about model licensing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Fine-tuning a state-of-the-art ranking model: <span class="ltx_text ltx_font_italic">NV-RerankQA-Mistral-4B-v3</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Ablation study on fine-tuning ranking models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.SS1" title="In 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Model size matters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.SS2" title="In 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Causal vs Bi-directional Attention mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.SS3" title="In 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>BCE vs InfoNCE Loss</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S6" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Deployment trade-off considerations for text retrieval pipelines with ranking models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S7" title="In Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gabriel de Souza P. Moreira
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">São Paulo</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Brazil</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:gmoreira@nvidia.com">gmoreira@nvidia.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ronay Ak
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Sarasota</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ronaya@nvidia.com">ronaya@nvidia.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benedikt Schifferer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Berlin</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:bschifferer@nvidia.com">bschifferer@nvidia.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mengyao Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Santa Clara</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mengyaox@nvidia.com">mengyaox@nvidia.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Radek Osmulski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Brisbane</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rosmulski@nvidia.com">rosmulski@nvidia.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Even Oldridge
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Vancouver</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:eoldridge@nvidia.com">eoldridge@nvidia.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Ranking models play a crucial role in enhancing overall accuracy of text retrieval systems. These multi-stage systems typically utilize either dense embedding models or sparse lexical indices to retrieve relevant passages based on a given query, followed by ranking models that refine the ordering of the candidate passages by its relevance to the query.</p>
<p class="ltx_p" id="id20.id2">This paper benchmarks various publicly available ranking models and examines their impact on ranking accuracy. We focus on text retrieval for question-answering tasks, a common use case for Retrieval-Augmented Generation systems. Our evaluation benchmarks include models some of which are commercially viable for industrial applications.</p>
<p class="ltx_p" id="id21.id3">We introduce a state-of-the-art ranking model, <span class="ltx_text ltx_font_italic" id="id21.id3.1">NV-RerankQA-Mistral-4B-v3</span>, which achieves a significant accuracy increase of  14% compared to pipelines with other rerankers. We also provide an ablation study comparing the fine-tuning of ranking models with different sizes, losses and self-attention mechanisms.</p>
<p class="ltx_p" id="id22.id4">Finally, we discuss challenges of text retrieval pipelines with ranking models in real-world industry applications, in particular the trade-offs among model size, ranking accuracy and system requirements like indexing and serving latency / throughput.</p>
</div>
<div class="ltx_keywords">Text retrieval, ranking models, embedding models, retrieval-augmented generation, rag pipelines, model deployment, transformers.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Text retrieval is a core component for many information retrieval applications like search, Question-Answering (Q&amp;A) and recommender systems.
More recently, text retrieval has been leveraged by Retrieval-Augmented Generation (RAG)<cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib16" title="">2020</a>; Ram et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib28" title="">2023</a>)</cite> systems, empowering Large Language Models (LLM) with external and up-to-date context.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Text embedding models represent variable-length text as a fixed dimension vector that can be used for downstream tasks. They are key for effective text retrieval, as they can semantically match pieces of textual content that can be symmetric (e.g. similar sentences or documents) or asymmetric (question and passages that might containing its answer).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Embedding models are based on the Transformer architecture. Some examples of seminal works are Sentence-BERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib29" title="">2019</a>)</cite>, DPR <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib13" title="">2020</a>)</cite>, E5 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib33" title="">2022b</a>)</cite> and E5-Mistral <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib34" title="">2023</a>)</cite>. They are typically trained with Constrastive Learning <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib4" title="">2020</a>; Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib13" title="">2020</a>)</cite> as a <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">bi-encoder</span> or <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">late combination model</span> <cite class="ltx_cite ltx_citemacro_citep">(Zamani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib39" title="">2018</a>)</cite>, i.e. query and passage are embedded separately and the model is optimized to maximize the similarity between query and relevant (positive) passages and minimize the similarity between query and non-relevant (negative) passages.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Retrieval systems that leverage text embedding models typically split the corpus into small chunks or passages (e.g. sentences or paragraphs), embed those passages and index corresponding embeddings into a vector database. This setup allows efficiently retrieving relevant passages from the embedded query by using Maximum Inner Product Search (MIPS) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib16" title="">2020</a>)</cite> or another Approximate Nearest-Neighbor (ANN) algorithm.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The MTEB <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib23" title="">2022</a>)</cite> is a popular benchmark of text embedding models for different tasks like retrieval, classification, clustering, semantic textual similarity, among others. We can notice from MTEB leaderboard<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/mteb/leaderboard" title="">https://huggingface.co/spaces/mteb/leaderboard</a></span></span></span> that in general the larger the embedding model in terms of parameters the higher the accuracy it can achieve.
However, that brings engineering challenges for companies in deploying such systems, as large embedding models can be prohibitively costly or slow to index very large textual corpus / knowledge bases.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">For that reason, multi-stage text retrieval pipelines have been proposed to increase indexing and serving throughput, as well as improving the retrieval accuracy. In those pipelines, a sparse and/or dense embedding model are first used to retrieve top-k candidate passages, followed by a ranking model that refines the final ranking of those passages, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="951" id="S1.F1.g1" src="x1.png" width="837"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Illustration of the typical indexing and querying pipelines for multi-stage text retrieval</figcaption>
</figure>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Ranking models are typically Transformer models that operate as a <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">cross-encoder</span> or <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">early combination model</span> <cite class="ltx_cite ltx_citemacro_citep">(Zamani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib39" title="">2018</a>)</cite> that takes as input both the query and passage pair concatenated and uses the self-attention mechanism to interact more deeply with the query and passage pair, and model their semantic relationship. Ranking models are used to provide relevance predictions only for the top-k candidates retrieved by the retrieval model. They can increase retrieval accuracy and make it possible using smaller embedding model, considerably reducing the indexing time and cost.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this paper, we present a benchmark of publicly available ranking models for text retrieval and discuss how they affect the ranking accuracy compared to the original ordering provided by different embedding models. To ensure the usefulness of this benchmark for companies, in our experimentation we include certain embedding and ranking models that can be used commercially, i.e. that have a proper license and were not trained on research-only public data such as the popular MS-MARCO <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib2" title="">2016</a>)</cite> dataset.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The main contributions of this paper are:</p>
</div>
<div class="ltx_para" id="S1.p10">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a comprehensive accuracy evaluation of publicly available ranking models with different commercially usable embedding models for Q&amp;A Text Retrieval;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce a state-of-the-art ranking model: <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">NV-RerankQA-Mistral-4B-v3</span>, and describe its architecture, pruning from Mistral 7B and fine-tuning techniques for leading ranking accuracy;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We present an ablation study fine-tuning ranking models on top of different base model sizes. Then for the Mistral base model, we experiment with both point-wise and pair-wise losses and different attention/pooling mechanisms and discuss how these combinations affect the accuracy;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Finally, we discuss trade-off aspects related to deploying text retrieval pipelines with or without a ranking model, like inference latency and embeddings indexing throughput.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In earlier days of information retrieval, a common approach to refine search results based on sparse retrieval models (e.g. BM25) was to use feature-based learning-to-rank models with point-wise, pair-wise or list-wise losses <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib17" title="">2009</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Neural Ranking Models (NRM) <cite class="ltx_cite ltx_citemacro_citep">(Zamani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib39" title="">2018</a>)</cite>, typically feed-forward networks, were proposed to capture interactions between query and document. DeepMatch <cite class="ltx_cite ltx_citemacro_citep">(Lu and Li, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib18" title="">2013</a>)</cite> modeled each text as a sequence of terms to generate a matching score. DRMM <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib9" title="">2016</a>)</cite> represents the input texts as histogram-based features. Duet <cite class="ltx_cite ltx_citemacro_citep">(Mitra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib21" title="">2017</a>)</cite> proposed a ranking model composed of two separate neural networks trained jointly, one that matches query and document using local representation and another using learned distributed representation. In <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib7" title="">2017</a>)</cite> input text was represented with bag-of-words and averaged bag-of-embeddings, and it was proposed three point-wise and pair-wise models, trained using weekly supervised data.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib31" title="">2017</a>)</cite> have moved natural language processing (NLP) field from manually handcrafting features to learning semantic and deeper text representations. The Deep Learning Track at TREC 2019 <cite class="ltx_cite ltx_citemacro_citep">(Craswell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib6" title="">2020</a>)</cite> hosted an extensive assessment of retrieval models after BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib8" title="">2018</a>)</cite> was introduced by Google, and demonstrated the effectiveness of leveraging pre-trained transformers as ranking models <cite class="ltx_cite ltx_citemacro_citep">(Hambarde and Proença, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In <cite class="ltx_cite ltx_citemacro_citep">(Nogueira and Cho, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib24" title="">2019</a>)</cite> BERT was adapted as a passage re-ranker. They proposed leveraging the <span class="ltx_text ltx_font_italic" id="S2.p4.1.1">next sentence prediction</span> training task to feed as input the concatenated query and passage separated by a special token and adding on top of the [CLS] output vector a single layer binary classification model. The authors later proposed in <cite class="ltx_cite ltx_citemacro_citep">(Nogueira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib25" title="">2019</a>)</cite> a multi-stage retrieval pipeline composed by BM25 and two BERT ranking models: one trained with point-wise loss (monoBERT) and the other with a pair-wise loss (duoBERT).</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">To deal with BERT limitation of 512 tokens, <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib37" title="">1903</a>)</cite> proposed splitting documents into sentences for BERT usage. In <cite class="ltx_cite ltx_citemacro_citep">(Qiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib27" title="">2019</a>)</cite> they experiment with bi-encoder and cross-encoder ranking models based on BERT, the latter being more effective due to the deeper interaction it provides. They also noticed BERT is more effective when working with semantically close query-document pairs compared to using click data for training.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Since then, a number of <span class="ltx_text ltx_font_italic" id="S2.p6.1.1">cross-encoder</span> models based on Transformers have been released from the community <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/cross-encoder" title="">https://huggingface.co/cross-encoder</a></span></span></span> <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sbert.net/docs/cross_encoder/pretrained_models.html" title="">https://www.sbert.net/docs/cross_encoder/pretrained_models.html</a></span></span></span>. We discuss and compare some of the most accurate ranking models available publicly in the next section.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Benchmarking ranking models for Q&amp;A text retrieval</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Benchmarking models in terms of accuracy is important to support decision on which models should be used in production pipelines or fine-tuned for domain adaptation.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In this section, we evaluate text retrieval pipelines composed by different embedding and ranking models. To ensure the usefulness of this benchmark for companies, we evaluate the pipelines using three commercially usable embedding models and top-performing ranking models.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We emphasize our evaluation on Question-Answering (Q&amp;A) datasets, as that is a popular application of RAG systems.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Retrieval models</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">There are many embedding models publicly available for the community, whose accuracy for multiple tasks can be found at the MTEB leaderboard<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/mteb/leaderboard" title="">https://huggingface.co/spaces/mteb/leaderboard</a></span></span></span>. Most of those models have being trained on research-only datasets like MS-MARCO and cannot be used commercially.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We evaluate embedding models that can be used for industry text retrieval applications. We select for experiments three embedding models for candidate retrieval, as the emphasis of our experiments is evaluating ranking models:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Snowflake/snowflake-arctic-embed-l</span> <cite class="ltx_cite ltx_citemacro_citep">(Merrick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib20" title="">2024</a>)</cite> <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l" title="">https://huggingface.co/Snowflake/snowflake-arctic-embed-l</a></span></span></span> <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://build.nvidia.com/snowflake/arctic-embed-l" title="">https://build.nvidia.com/snowflake/arctic-embed-l</a></span></span></span> (335M params) - Embedding model based on BERT-large, trained with contrastive learning for two training rounds, with in-batch negative samples and hard negatives.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">nvidia/nv-embedqa-e5-v5</span> <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://build.nvidia.com/nvidia/nv-embedqa-e5-v5" title="">https://build.nvidia.com/nvidia/nv-embedqa-e5-v5</a></span></span></span> (335M params) - Embedding model based on <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">e5-large-unsupervised</span> trained for multiple rounds on supervised data with contrastive learning, and both in-batch negatives and hard-negatives.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">nvidia/nv-embedqa-mistral-7b-v2</span> <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2" title="">https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2</a></span></span></span> (7.24B params)- Large embedding model based on Mistral 7B v0.1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib12" title="">2023</a>)</cite> <span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="">https://huggingface.co/mistralai/Mistral-7B-v0.1</a></span></span></span>, modified to use bi-directional attention and average pooling as done in NV-Embed <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib15" title="">2024</a>)</cite> and NV-Retriever <cite class="ltx_cite ltx_citemacro_citep">(Moreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Ranking models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We describe here the ranking models we evaluated in this investigation, including recent ranking models that perform high on retrieval benchmarks according to their reports. From those, the only reranking models that can be used for commercial purposes from their licences and train data are <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">NV-RerankQA-Mistral-4B-v3</span>, which we introduce in this paper, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">mixedbread-ai/mxbai-rerank-large-v1</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.1">ms-marco-MiniLM-L-12-v2</span> <span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2" title="">https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2</a></span></span></span> (33M params) - fine-tuned on top of the MiniLMv2 model <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib35" title="">2020a</a>)</cite> with <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.2">SentenceTransformers</span> package<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sbert.net/" title="">https://www.sbert.net/</a></span></span></span> on the MS MARCO passage ranking dataset <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib2" title="">2016</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i2.p1.1.1">jina-reranker-v2-base-multilingual</span>
<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual" title="">https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual</a></span></span></span> (278M params) - A multi-lingual ranking model finetuned from XLM-RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib5" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i3.p1.1.1">mixedbread-ai/mxbai-rerank-large-v1</span>
<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1" title="">https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1</a></span></span></span> (435M params) - Largest re-ranker model provided by Mixedbread</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i4.p1.1.1">bge-reranker-v2-m3</span> <span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BAAI/bge-reranker-v2-m3" title="">https://huggingface.co/BAAI/bge-reranker-v2-m3</a></span></span></span> (568M params) - A multi-lingual ranking model fine-tuned from <span class="ltx_text ltx_font_italic" id="S3.I2.i4.p1.1.2">BGE M3-Embedding</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib3" title="">2024</a>)</cite> with FlagEmbedding package<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/FlagOpen/FlagEmbedding" title="">https://github.com/FlagOpen/FlagEmbedding</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i5.p1.1.1">NV-RerankQA-Mistral-4B-v3</span> <span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://build.nvidia.com/explore/retrieval#nv-rerankqa-mistral-4b-v3" title="">https://build.nvidia.com/explore/retrieval#nv-rerankqa-mistral-4b-v3</a></span></span></span> (4B params) - Large and powerful re-ranker that takes as base model a pruned version of Mistral 7B and is fine-tuned with a blend of supervised data with contrastive learning. It is fully described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Evaluation setup</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The evaluation setup mimics the typical text retrieval indexing and querying pipelines, as previously illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The indexing pipeline takes place first, where a text corpus is chunked into smaller passages. For our evaluation, we use datasets from BEIR <cite class="ltx_cite ltx_citemacro_citep">(Thakur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib30" title="">2021</a>)</cite> datasets, which are already chunked and truncated to max 512 tokens. The chunked passages are embedded using an embedding model and stored in a vector index / database.
The querying pipeline then takes place for providing for each query a list with ranked passages for retrieval metrics computation (NDCG@10). In detail, the question is embedded and it is performed a vector search (e.g. using exact or Approximate Nearest Neighbour (ANN) algorithm) on the vector index, returning the top-k most relevant passages for the question. Finally, the top-k (set to 100 in our evaluation experiments) passages are re-ranked with a ranking model to generate the final ordered list.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">We perform the evaluation on the three Question-Answering datasets from BEIR <cite class="ltx_cite ltx_citemacro_citep">(Thakur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib30" title="">2021</a>)</cite> retrieval benchmark: Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib14" title="">2019</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib38" title="">2018</a>)</cite> and FiQA <cite class="ltx_cite ltx_citemacro_citep">(Maia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib19" title="">2018</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Benchmark results</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In this section, we provide the benchmark results of text retrieval pipelines with different embedding and ranking models.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.T1" title="Table 1 ‣ 3.4. Benchmark results ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare pipelines with three commercially usable embedding models (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS1" title="3.1. Retrieval models ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and their combination with a number of ranking models (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS2" title="3.2. Ranking models ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Retrieval accuracy is measured with NDCG@10 for Q&amp;A BEIR datasets.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Evaluation (NDCG@10) of multi-stage text retrieval pipelines with different embedding and ranking models on text Q&amp;A datasets from BEIR</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1" style="font-size:80%;">Reranker model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1" style="font-size:80%;">Avg.</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.1.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1.1.1" style="font-size:80%;">NQ</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.4.1">
<span class="ltx_p" id="S3.T1.1.1.1.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1.1.1" style="font-size:80%;">HotpotQA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1" style="font-size:80%;">FiQA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.2.1.1">
<span class="ltx_p" id="S3.T1.1.2.2.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.2.2.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S3.T1.1.2.2.1.1.1.1.1"> snowflake-arctic-embed-l</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.2.2.2"><span class="ltx_text" id="S3.T1.1.2.2.2.1" style="font-size:80%;">0.6100</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.2.3.1">
<span class="ltx_p" id="S3.T1.1.2.2.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.2.2.3.1.1.1" style="font-size:80%;">0.6311</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.2.4.1">
<span class="ltx_p" id="S3.T1.1.2.2.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.2.2.4.1.1.1" style="font-size:80%;">0.7518</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.5"><span class="ltx_text" id="S3.T1.1.2.2.5.1" style="font-size:80%;">0.4471</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.3.1.1">
<span class="ltx_p" id="S3.T1.1.3.3.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.3.3.1.1.1.1" style="font-size:80%;">+ ms-marco-MiniLM-L-12-v2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.3.3.2"><span class="ltx_text" id="S3.T1.1.3.3.2.1" style="font-size:80%;">0.5771</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.3.3.1">
<span class="ltx_p" id="S3.T1.1.3.3.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.3.3.3.1.1.1" style="font-size:80%;">0.5876</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.3.4.1">
<span class="ltx_p" id="S3.T1.1.3.3.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.3.3.4.1.1.1" style="font-size:80%;">0.7586</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.3.5"><span class="ltx_text" id="S3.T1.1.3.3.5.1" style="font-size:80%;">0.3850</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.4.1.1">
<span class="ltx_p" id="S3.T1.1.4.4.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.4.4.1.1.1.1" style="font-size:80%;">+ mxbai-rerank-large-v1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.4.2"><span class="ltx_text" id="S3.T1.1.4.4.2.1" style="font-size:80%;">0.6077</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.4.3.1">
<span class="ltx_p" id="S3.T1.1.4.4.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.4.4.3.1.1.1" style="font-size:80%;">0.6433</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.4.4.1">
<span class="ltx_p" id="S3.T1.1.4.4.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.4.4.4.1.1.1" style="font-size:80%;">0.7401</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.4.5"><span class="ltx_text" id="S3.T1.1.4.4.5.1" style="font-size:80%;">0.4396</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.5.1.1">
<span class="ltx_p" id="S3.T1.1.5.5.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.5.5.1.1.1.1" style="font-size:80%;">+ jina-reranker-v2-base-multilingual</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.5.2"><span class="ltx_text" id="S3.T1.1.5.5.2.1" style="font-size:80%;">0.6481</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.5.3.1">
<span class="ltx_p" id="S3.T1.1.5.5.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.5.5.3.1.1.1" style="font-size:80%;">0.6768</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.5.4.1">
<span class="ltx_p" id="S3.T1.1.5.5.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.5.5.4.1.1.1" style="font-size:80%;">0.8165</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.5.5"><span class="ltx_text" id="S3.T1.1.5.5.5.1" style="font-size:80%;">0.4511</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.6.1.1">
<span class="ltx_p" id="S3.T1.1.6.6.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.6.6.1.1.1.1" style="font-size:80%;">+ bge-reranker-v2-m3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.6.2"><span class="ltx_text" id="S3.T1.1.6.6.2.1" style="font-size:80%;">0.6585</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.6.3.1">
<span class="ltx_p" id="S3.T1.1.6.6.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.6.6.3.1.1.1" style="font-size:80%;">0.6965</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.6.4.1">
<span class="ltx_p" id="S3.T1.1.6.6.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.6.6.4.1.1.1" style="font-size:80%;">0.8458</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.6.5"><span class="ltx_text" id="S3.T1.1.6.6.5.1" style="font-size:80%;">0.4332</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.7.1.1">
<span class="ltx_p" id="S3.T1.1.7.7.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.7.7.1.1.1.1" style="font-size:80%;">+ NV-RerankQA-Mistral-4B-v3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.2.1" style="font-size:80%;">0.7529</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.7.3.1">
<span class="ltx_p" id="S3.T1.1.7.7.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.3.1.1.1" style="font-size:80%;">0.7788</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.7.4.1">
<span class="ltx_p" id="S3.T1.1.7.7.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.4.1.1.1" style="font-size:80%;">0.8726</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.5.1" style="font-size:80%;">0.6073</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.8.1.1">
<span class="ltx_p" id="S3.T1.1.8.8.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.8.8.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S3.T1.1.8.8.1.1.1.1.1"> NV-EmbedQA-e5-v5</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.8.8.2"><span class="ltx_text" id="S3.T1.1.8.8.2.1" style="font-size:80%;">0.6083</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.8.3.1">
<span class="ltx_p" id="S3.T1.1.8.8.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.8.8.3.1.1.1" style="font-size:80%;">0.6380</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.8.4.1">
<span class="ltx_p" id="S3.T1.1.8.8.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.8.8.4.1.1.1" style="font-size:80%;">0.7160</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.8.8.5"><span class="ltx_text" id="S3.T1.1.8.8.5.1" style="font-size:80%;">0.4710</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.9.1.1">
<span class="ltx_p" id="S3.T1.1.9.9.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.9.9.1.1.1.1" style="font-size:80%;">+ ms-marco-MiniLM-L-12-v2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.9.9.2"><span class="ltx_text" id="S3.T1.1.9.9.2.1" style="font-size:80%;">0.5785</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.9.3.1">
<span class="ltx_p" id="S3.T1.1.9.9.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.9.9.3.1.1.1" style="font-size:80%;">0.5909</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.9.4.1">
<span class="ltx_p" id="S3.T1.1.9.9.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.9.9.4.1.1.1" style="font-size:80%;">0.7458</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.9.9.5"><span class="ltx_text" id="S3.T1.1.9.9.5.1" style="font-size:80%;">0.3988</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.10.1.1">
<span class="ltx_p" id="S3.T1.1.10.10.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.10.10.1.1.1.1" style="font-size:80%;">+ mxbai-rerank-large-v1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.10.10.2"><span class="ltx_text" id="S3.T1.1.10.10.2.1" style="font-size:80%;">0.6077</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.10.3.1">
<span class="ltx_p" id="S3.T1.1.10.10.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.10.10.3.1.1.1" style="font-size:80%;">0.6450</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.10.4.1">
<span class="ltx_p" id="S3.T1.1.10.10.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.10.10.4.1.1.1" style="font-size:80%;">0.7279</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.10.5"><span class="ltx_text" id="S3.T1.1.10.10.5.1" style="font-size:80%;">0.4502</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.11.1.1">
<span class="ltx_p" id="S3.T1.1.11.11.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.11.11.1.1.1.1" style="font-size:80%;">+ jina-reranker-v2-base-multilingual</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.11.11.2"><span class="ltx_text" id="S3.T1.1.11.11.2.1" style="font-size:80%;">0.6454</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.11.3.1">
<span class="ltx_p" id="S3.T1.1.11.11.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.11.11.3.1.1.1" style="font-size:80%;">0.6780</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.11.4.1">
<span class="ltx_p" id="S3.T1.1.11.11.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.11.11.4.1.1.1" style="font-size:80%;">0.7996</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.11.5"><span class="ltx_text" id="S3.T1.1.11.11.5.1" style="font-size:80%;">0.4585</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.12.1.1">
<span class="ltx_p" id="S3.T1.1.12.12.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.12.12.1.1.1.1" style="font-size:80%;">+ bge-reranker-v2-m3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.12.12.2"><span class="ltx_text" id="S3.T1.1.12.12.2.1" style="font-size:80%;">0.6584</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.12.3.1">
<span class="ltx_p" id="S3.T1.1.12.12.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.12.12.3.1.1.1" style="font-size:80%;">0.6974</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.12.4.1">
<span class="ltx_p" id="S3.T1.1.12.12.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.12.12.4.1.1.1" style="font-size:80%;">0.8272</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.12.5"><span class="ltx_text" id="S3.T1.1.12.12.5.1" style="font-size:80%;">0.4506</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.13.1.1">
<span class="ltx_p" id="S3.T1.1.13.13.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.13.13.1.1.1.1" style="font-size:80%;">+ NV-RerankQA-Mistral-4B-v3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.2.1" style="font-size:80%;">0.7486</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.13.3.1">
<span class="ltx_p" id="S3.T1.1.13.13.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.3.1.1.1" style="font-size:80%;">0.7785</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.13.4.1">
<span class="ltx_p" id="S3.T1.1.13.13.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.4.1.1.1" style="font-size:80%;">0.8470</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.5.1" style="font-size:80%;">0.6203</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.14.1.1">
<span class="ltx_p" id="S3.T1.1.14.14.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.14.14.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S3.T1.1.14.14.1.1.1.1.1"> NV-EmbedQA-Mistral7B-v2</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.14.14.2"><span class="ltx_text" id="S3.T1.1.14.14.2.1" style="font-size:80%;">0.7173</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.14.3.1">
<span class="ltx_p" id="S3.T1.1.14.14.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.14.14.3.1.1.1" style="font-size:80%;">0.7216</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.14.4.1">
<span class="ltx_p" id="S3.T1.1.14.14.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.14.14.4.1.1.1" style="font-size:80%;">0.8109</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.14.14.5"><span class="ltx_text" id="S3.T1.1.14.14.5.1" style="font-size:80%;">0.6194</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.15.1.1">
<span class="ltx_p" id="S3.T1.1.15.15.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.15.15.1.1.1.1" style="font-size:80%;">+ ms-marco-MiniLM-L-12-v2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.15.15.2"><span class="ltx_text" id="S3.T1.1.15.15.2.1" style="font-size:80%;">0.5875</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.15.3.1">
<span class="ltx_p" id="S3.T1.1.15.15.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.15.15.3.1.1.1" style="font-size:80%;">0.5945</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.15.15.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.15.4.1">
<span class="ltx_p" id="S3.T1.1.15.15.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.15.15.4.1.1.1" style="font-size:80%;">0.7641</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.15.15.5"><span class="ltx_text" id="S3.T1.1.15.15.5.1" style="font-size:80%;">0.4039</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.16.1.1">
<span class="ltx_p" id="S3.T1.1.16.16.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.16.16.1.1.1.1" style="font-size:80%;">+ mxbai-rerank-large-v1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.16.16.2"><span class="ltx_text" id="S3.T1.1.16.16.2.1" style="font-size:80%;">0.6133</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.16.3.1">
<span class="ltx_p" id="S3.T1.1.16.16.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.16.16.3.1.1.1" style="font-size:80%;">0.6439</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.16.4.1">
<span class="ltx_p" id="S3.T1.1.16.16.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.16.16.4.1.1.1" style="font-size:80%;">0.7436</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.16.16.5"><span class="ltx_text" id="S3.T1.1.16.16.5.1" style="font-size:80%;">0.4523</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.17.1.1">
<span class="ltx_p" id="S3.T1.1.17.17.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.17.17.1.1.1.1" style="font-size:80%;">+ jina-reranker-v2-base-multilingual</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.17.17.2"><span class="ltx_text" id="S3.T1.1.17.17.2.1" style="font-size:80%;">0.6590</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.17.3.1">
<span class="ltx_p" id="S3.T1.1.17.17.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.17.17.3.1.1.1" style="font-size:80%;">0.6819</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.17.4.1">
<span class="ltx_p" id="S3.T1.1.17.17.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.17.17.4.1.1.1" style="font-size:80%;">0.8262</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.17.17.5"><span class="ltx_text" id="S3.T1.1.17.17.5.1" style="font-size:80%;">0.4689</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.18.1.1">
<span class="ltx_p" id="S3.T1.1.18.18.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.18.18.1.1.1.1" style="font-size:80%;">+ bge-reranker-v2-m3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.18.18.2"><span class="ltx_text" id="S3.T1.1.18.18.2.1" style="font-size:80%;">0.6734</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.18.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.18.3.1">
<span class="ltx_p" id="S3.T1.1.18.18.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T1.1.18.18.3.1.1.1" style="font-size:80%;">0.7028</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.18.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.18.4.1">
<span class="ltx_p" id="S3.T1.1.18.18.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S3.T1.1.18.18.4.1.1.1" style="font-size:80%;">0.8635</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.18.5"><span class="ltx_text" id="S3.T1.1.18.18.5.1" style="font-size:80%;">0.4539</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19.19">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.1.19.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.19.1.1">
<span class="ltx_p" id="S3.T1.1.19.19.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S3.T1.1.19.19.1.1.1.1" style="font-size:80%;">+ NV-RerankQA-Mistral-4B-v3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.1.19.19.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.19.19.2.1" style="font-size:80%;">0.7694</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.1.19.19.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.19.3.1">
<span class="ltx_p" id="S3.T1.1.19.19.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.19.19.3.1.1.1" style="font-size:80%;">0.7830</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.1.19.19.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.19.4.1">
<span class="ltx_p" id="S3.T1.1.19.19.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.19.19.4.1.1.1" style="font-size:80%;">0.8904</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.19.19.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.19.19.5.1" style="font-size:80%;">0.6350</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">We can clearly observe that for smaller embedding models like <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.1">snowflake-arctic-embed-l</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.2">NV-EmbedQA-e5-v5</span> (335M params), all cross-encoders (except for the small <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.3">ms-marco-MiniLM-L-12-v2</span>) improve considerably the ranking accuracy compared to the retriever. On the other hand, for the larger <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.4">NV-EmbedQA-Mistral7B-v2</span> embedding model, only the large <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.5">NV-RerankQA-Mistral-4B-v3</span> reranker is able to improve its accuracy.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">The <span class="ltx_text ltx_font_italic" id="S3.SS4.p4.1.1">NV-RerankQA-Mistral-4B-v3</span> reranker provides the highest ranking accuracy for all datasets by a large margin (+14% compared to the second best reranker: <span class="ltx_text ltx_font_italic" id="S3.SS4.p4.1.2">bge-reranker-v2-m3</span>). That demonstrates the effectiveness of our adaptation of Mistral 7B as a cross-encoder.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>A small note about model licensing</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">For training <span class="ltx_text ltx_font_italic" id="S3.SS5.p1.1.1">NV-RerankQA-Mistral-4B-v3</span> we have selected only public datasets whose license allows their usage for industry applications. Some other models are released with permissive licenses like Apache 2.0 or MIT, but we do not know which datasets they were trained on or whether they got a special license to use research-only datasets like MS-Marco, for example. Every company should check with its legal team on model licensing for commercial usage.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Fine-tuning a state-of-the-art ranking model: <span class="ltx_text ltx_font_italic" id="S4.1.1">NV-RerankQA-Mistral-4B-v3</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We introduce in this paper the state-of-the-art <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">NV-RerankQA-Mistral-4B-v3</span>, that performs best in our benchmark on text retrieval for Q&amp;A (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS4" title="3.4. Benchmark results ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib12" title="">2023</a>)</cite> decoder model has been successfully adopted as embedding models for retrieval when repurposed and fine-tuned with contrastive learning<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib34" title="">2023</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib15" title="">2024</a>; Moreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">In this work, we adapted Mistral 7B v0.1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib12" title="">2023</a>)</cite> as a ranking model. In order to reduce the number of parameters from the base model, thus its inference compute and memory requirements, we prune it by keeping only the bottom 16 layers out of its 32 layers<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>We also tried pruning different number of top layers from Mistral 7B model, the more layers we remove the lower the accuracy. We found out that keeping bottom 16 layers provides a good trade-off between accuracy penalty (-1%) and model size reduction (-50% # parameters) compared to the original 32-layer model.</span></span></span>. We also modify its self-attention mechanism from uni-directional (causal) to bi-directional, so that for each token it is possible to attend to other tokens in both right and left sides, as that has shown to improve accuracy for Mistral-based embedding models <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib15" title="">2024</a>; Moreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">We feed as input to the model the tokenized question and candidate passage pair, concatenated and separated by a special token. We perform average pooling on the outputs of last Transformer layer and add a feed-forward layer on top that outputs a single-unit with the likelihood of a given passage being relevant to a question.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="741" id="S4.F2.g1" src="x2.png" width="1076"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Architecture of <span class="ltx_text ltx_font_italic" id="S4.F2.2.1">NV-RerankQA-Mistral-4B-v3</span> cross-encoder, pruned and adapted from Mistral 7B</figcaption>
</figure>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.3">Cross-encoder ranking models are binary classifiers that discriminate between positive and negative passages. They typically are trained with the binary cross-entropy loss as in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4.E1" title="In 4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math alttext="p=\phi(q,d)" class="ltx_Math" display="inline" id="S4.p5.1.m1.2"><semantics id="S4.p5.1.m1.2a"><mrow id="S4.p5.1.m1.2.3" xref="S4.p5.1.m1.2.3.cmml"><mi id="S4.p5.1.m1.2.3.2" xref="S4.p5.1.m1.2.3.2.cmml">p</mi><mo id="S4.p5.1.m1.2.3.1" xref="S4.p5.1.m1.2.3.1.cmml">=</mo><mrow id="S4.p5.1.m1.2.3.3" xref="S4.p5.1.m1.2.3.3.cmml"><mi id="S4.p5.1.m1.2.3.3.2" xref="S4.p5.1.m1.2.3.3.2.cmml">ϕ</mi><mo id="S4.p5.1.m1.2.3.3.1" xref="S4.p5.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S4.p5.1.m1.2.3.3.3.2" xref="S4.p5.1.m1.2.3.3.3.1.cmml"><mo id="S4.p5.1.m1.2.3.3.3.2.1" stretchy="false" xref="S4.p5.1.m1.2.3.3.3.1.cmml">(</mo><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">q</mi><mo id="S4.p5.1.m1.2.3.3.3.2.2" xref="S4.p5.1.m1.2.3.3.3.1.cmml">,</mo><mi id="S4.p5.1.m1.2.2" xref="S4.p5.1.m1.2.2.cmml">d</mi><mo id="S4.p5.1.m1.2.3.3.3.2.3" stretchy="false" xref="S4.p5.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.2b"><apply id="S4.p5.1.m1.2.3.cmml" xref="S4.p5.1.m1.2.3"><eq id="S4.p5.1.m1.2.3.1.cmml" xref="S4.p5.1.m1.2.3.1"></eq><ci id="S4.p5.1.m1.2.3.2.cmml" xref="S4.p5.1.m1.2.3.2">𝑝</ci><apply id="S4.p5.1.m1.2.3.3.cmml" xref="S4.p5.1.m1.2.3.3"><times id="S4.p5.1.m1.2.3.3.1.cmml" xref="S4.p5.1.m1.2.3.3.1"></times><ci id="S4.p5.1.m1.2.3.3.2.cmml" xref="S4.p5.1.m1.2.3.3.2">italic-ϕ</ci><interval closure="open" id="S4.p5.1.m1.2.3.3.3.1.cmml" xref="S4.p5.1.m1.2.3.3.3.2"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">𝑞</ci><ci id="S4.p5.1.m1.2.2.cmml" xref="S4.p5.1.m1.2.2">𝑑</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.2c">p=\phi(q,d)</annotation><annotation encoding="application/x-llamapun" id="S4.p5.1.m1.2d">italic_p = italic_ϕ ( italic_q , italic_d )</annotation></semantics></math> is the model predicted likelihood of the passage <math alttext="d" class="ltx_Math" display="inline" id="S4.p5.2.m2.1"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.p5.2.m2.1d">italic_d</annotation></semantics></math> being relevant to query <math alttext="q" class="ltx_Math" display="inline" id="S4.p5.3.m3.1"><semantics id="S4.p5.3.m3.1a"><mi id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><ci id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.p5.3.m3.1d">italic_q</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.p6">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=-{(y\log(p)+(1-y)\log(1-p))}" class="ltx_Math" display="block" id="S4.E1.m1.4"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4" xref="S4.E1.m1.4.4.cmml"><mi id="S4.E1.m1.4.4.3" xref="S4.E1.m1.4.4.3.cmml">L</mi><mo id="S4.E1.m1.4.4.2" xref="S4.E1.m1.4.4.2.cmml">=</mo><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.cmml"><mo id="S4.E1.m1.4.4.1a" xref="S4.E1.m1.4.4.1.cmml">−</mo><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.cmml"><mo id="S4.E1.m1.4.4.1.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.1.1.4" xref="S4.E1.m1.4.4.1.1.1.1.4.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.4.2" xref="S4.E1.m1.4.4.1.1.1.1.4.2.cmml">y</mi><mo id="S4.E1.m1.4.4.1.1.1.1.4.1" lspace="0.167em" xref="S4.E1.m1.4.4.1.1.1.1.4.1.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.4.3.2" xref="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml"><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">log</mi><mo id="S4.E1.m1.4.4.1.1.1.1.4.3.2a" xref="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml">⁡</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.4.3.2.1" xref="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.4.3.2.1.1" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml">(</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">p</mi><mo id="S4.E1.m1.4.4.1.1.1.1.4.3.2.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.3.cmml">+</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml"><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mn id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.2.3" lspace="0.167em" xref="S4.E1.m1.4.4.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.2.2.1" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml"><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">log</mi><mo id="S4.E1.m1.4.4.1.1.1.1.2.2.1a" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml">⁡</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.cmml"><mn id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.1.cmml">−</mo><mi id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.3.cmml">p</mi></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.3" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.1.3" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.cmml" xref="S4.E1.m1.4.4"><eq id="S4.E1.m1.4.4.2.cmml" xref="S4.E1.m1.4.4.2"></eq><ci id="S4.E1.m1.4.4.3.cmml" xref="S4.E1.m1.4.4.3">𝐿</ci><apply id="S4.E1.m1.4.4.1.cmml" xref="S4.E1.m1.4.4.1"><minus id="S4.E1.m1.4.4.1.2.cmml" xref="S4.E1.m1.4.4.1"></minus><apply id="S4.E1.m1.4.4.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"><plus id="S4.E1.m1.4.4.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.3"></plus><apply id="S4.E1.m1.4.4.1.1.1.1.4.cmml" xref="S4.E1.m1.4.4.1.1.1.1.4"><times id="S4.E1.m1.4.4.1.1.1.1.4.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.4.1"></times><ci id="S4.E1.m1.4.4.1.1.1.1.4.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.4.2">𝑦</ci><apply id="S4.E1.m1.4.4.1.1.1.1.4.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.4.3.2"><log id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"></log><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑝</ci></apply></apply><apply id="S4.E1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2"><times id="S4.E1.m1.4.4.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.3"></times><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1"><minus id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.1"></minus><cn id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.2">1</cn><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.1.3">𝑦</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1"><log id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3"></log><apply id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1"><minus id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.1"></minus><cn id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.2.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.2">1</cn><ci id="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.1.1.1.3">𝑝</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">L=-{(y\log(p)+(1-y)\log(1-p))}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.4d">italic_L = - ( italic_y roman_log ( italic_p ) + ( 1 - italic_y ) roman_log ( 1 - italic_p ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.4">Instead, for <span class="ltx_text ltx_font_italic" id="S4.p7.4.1">NV-RerankQA-Mistral-4B-v3</span> we follow <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib32" title="">2022a</a>)</cite> and train the reranker with contrastive learning over the positive and its negative candidates scores using the list-wise InfoNCE loss <cite class="ltx_cite ltx_citemacro_citep">(Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib26" title="">2018</a>)</cite>, shown in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4.E2" title="In 4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">2</span></a>, where <math alttext="d^{+}" class="ltx_Math" display="inline" id="S4.p7.1.m1.1"><semantics id="S4.p7.1.m1.1a"><msup id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">d</mi><mo id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1">superscript</csymbol><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝑑</ci><plus id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">d^{+}</annotation><annotation encoding="application/x-llamapun" id="S4.p7.1.m1.1d">italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math> is a positive relevant passage, <math alttext="d^{-}" class="ltx_Math" display="inline" id="S4.p7.2.m2.1"><semantics id="S4.p7.2.m2.1a"><msup id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">d</mi><mo id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1">superscript</csymbol><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">𝑑</ci><minus id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">d^{-}</annotation><annotation encoding="application/x-llamapun" id="S4.p7.2.m2.1d">italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT</annotation></semantics></math> is one of the <math alttext="N" class="ltx_Math" display="inline" id="S4.p7.3.m3.1"><semantics id="S4.p7.3.m3.1a"><mi id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><ci id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.p7.3.m3.1d">italic_N</annotation></semantics></math> negative passages and <math alttext="\tau" class="ltx_Math" display="inline" id="S4.p7.4.m4.1"><semantics id="S4.p7.4.m4.1a"><mi id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><ci id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.p7.4.m4.1d">italic_τ</annotation></semantics></math> is the temperature parameter.</p>
</div>
<div class="ltx_para" id="S4.p8">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=-\text{log}\frac{\text{exp}(\phi(q,d^{+})/\tau))}{\text{exp}(\phi(q,d^{+})/%
\tau))+\sum_{i=1}^{N}\text{exp}(\phi(q,d^{-})/\tau))}" class="ltx_math_unparsed" display="block" id="S4.E2.m1.2"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.3"><mi id="S4.E2.m1.2.3.2">L</mi><mo id="S4.E2.m1.2.3.1">=</mo><mrow id="S4.E2.m1.2.3.3"><mo id="S4.E2.m1.2.3.3a">−</mo><mrow id="S4.E2.m1.2.3.3.2"><mtext id="S4.E2.m1.2.3.3.2.2">log</mtext><mo id="S4.E2.m1.2.3.3.2.1">⁢</mo><mfrac id="S4.E2.m1.2.2"><mrow id="S4.E2.m1.1.1.1"><mtext id="S4.E2.m1.1.1.1.2">exp</mtext><mrow id="S4.E2.m1.1.1.1.3"><mo id="S4.E2.m1.1.1.1.3.1" stretchy="false">(</mo><mi id="S4.E2.m1.1.1.1.3.2">ϕ</mi><mrow id="S4.E2.m1.1.1.1.3.3"><mo id="S4.E2.m1.1.1.1.3.3.1" stretchy="false">(</mo><mi id="S4.E2.m1.1.1.1.1">q</mi><mo id="S4.E2.m1.1.1.1.3.3.2">,</mo><msup id="S4.E2.m1.1.1.1.3.3.3"><mi id="S4.E2.m1.1.1.1.3.3.3.2">d</mi><mo id="S4.E2.m1.1.1.1.3.3.3.3">+</mo></msup><mo id="S4.E2.m1.1.1.1.3.3.4" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.1.1.1.3.4">/</mo><mi id="S4.E2.m1.1.1.1.3.5">τ</mi><mo id="S4.E2.m1.1.1.1.3.6" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.1.1.1.4" stretchy="false">)</mo></mrow><mrow id="S4.E2.m1.2.2.2"><mrow id="S4.E2.m1.2.2.2.2"><mtext id="S4.E2.m1.2.2.2.2.1">exp</mtext><mrow id="S4.E2.m1.2.2.2.2.2"><mo id="S4.E2.m1.2.2.2.2.2.1" stretchy="false">(</mo><mi id="S4.E2.m1.2.2.2.2.2.2">ϕ</mi><mrow id="S4.E2.m1.2.2.2.2.2.3"><mo id="S4.E2.m1.2.2.2.2.2.3.1" stretchy="false">(</mo><mi id="S4.E2.m1.2.2.2.1">q</mi><mo id="S4.E2.m1.2.2.2.2.2.3.2">,</mo><msup id="S4.E2.m1.2.2.2.2.2.3.3"><mi id="S4.E2.m1.2.2.2.2.2.3.3.2">d</mi><mo id="S4.E2.m1.2.2.2.2.2.3.3.3">+</mo></msup><mo id="S4.E2.m1.2.2.2.2.2.3.4" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.2.2.2.2.2.4">/</mo><mi id="S4.E2.m1.2.2.2.2.2.5">τ</mi><mo id="S4.E2.m1.2.2.2.2.2.6" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.2.2.2.2.3" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.2.2.2.3" rspace="0.055em">+</mo><msubsup id="S4.E2.m1.2.2.2.4"><mo id="S4.E2.m1.2.2.2.4.2.2">∑</mo><mrow id="S4.E2.m1.2.2.2.4.2.3"><mi id="S4.E2.m1.2.2.2.4.2.3.2">i</mi><mo id="S4.E2.m1.2.2.2.4.2.3.1">=</mo><mn id="S4.E2.m1.2.2.2.4.2.3.3">1</mn></mrow><mi id="S4.E2.m1.2.2.2.4.3">N</mi></msubsup><mtext id="S4.E2.m1.2.2.2.5">exp</mtext><mrow id="S4.E2.m1.2.2.2.6"><mo id="S4.E2.m1.2.2.2.6.1" stretchy="false">(</mo><mi id="S4.E2.m1.2.2.2.6.2">ϕ</mi><mrow id="S4.E2.m1.2.2.2.6.3"><mo id="S4.E2.m1.2.2.2.6.3.1" stretchy="false">(</mo><mi id="S4.E2.m1.2.2.2.6.3.2">q</mi><mo id="S4.E2.m1.2.2.2.6.3.3">,</mo><msup id="S4.E2.m1.2.2.2.6.3.4"><mi id="S4.E2.m1.2.2.2.6.3.4.2">d</mi><mo id="S4.E2.m1.2.2.2.6.3.4.3">−</mo></msup><mo id="S4.E2.m1.2.2.2.6.3.5" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.2.2.2.6.4">/</mo><mi id="S4.E2.m1.2.2.2.6.5">τ</mi><mo id="S4.E2.m1.2.2.2.6.6" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.2.2.2.7" stretchy="false">)</mo></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S4.E2.m1.2b">L=-\text{log}\frac{\text{exp}(\phi(q,d^{+})/\tau))}{\text{exp}(\phi(q,d^{+})/%
\tau))+\sum_{i=1}^{N}\text{exp}(\phi(q,d^{-})/\tau))}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.2c">italic_L = - log divide start_ARG exp ( italic_ϕ ( italic_q , italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) / italic_τ ) ) end_ARG start_ARG exp ( italic_ϕ ( italic_q , italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) / italic_τ ) ) + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT exp ( italic_ϕ ( italic_q , italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) / italic_τ ) ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.p9">
<p class="ltx_p" id="S4.p9.1">The negative candidates used for contrastive learning are mined from the corpus in the data pre-processing stage by using a teacher embedding model. We use the <span class="ltx_text ltx_font_italic" id="S4.p9.1.1">TopK-PercPos</span> hard-negative mining method introduced in <cite class="ltx_cite ltx_citemacro_citep">(Moreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib22" title="">2024</a>)</cite>, configured with maximum negative score threshold as 95% of the positive scores to remove potential false negatives.</p>
</div>
<div class="ltx_para" id="S4.p10">
<p class="ltx_p" id="S4.p10.1">We present in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5" title="5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">5</span></a> an ablation study on fine-tuning ranking models, with some experiments focused on our choices for the loss and self-attention mechanism for <span class="ltx_text ltx_font_italic" id="S4.p10.1.1">NV-RerankQA-Mistral-4B-v3</span>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Ablation study on fine-tuning ranking models</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present an ablation study on fine-tuning and comparing different base models as rerankers. For Mistral base model, we also compare choices of self-attention mechanism (unidirectional vs bi-directional) and training losses (binary vs categorical cross-entropy).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">For broader comparison, we evaluate the ranking models with the three different embedding models described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.SS1" title="3.1. Retrieval models ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Model size matters</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Model size is an important aspect for trading-off model accuracy and inference throughput. For this section we fine-tune and compare three base models with different sizes as ranking models: <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">MiniLM-L12-H384-uncased</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib36" title="">2020b</a>)</cite> (33M params) <span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/microsoft/MiniLM-L12-H384-uncased" title="">https://huggingface.co/microsoft/MiniLM-L12-H384-uncased</a></span></span></span>, <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">deberta-v3-large</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib11" title="">2021</a>)</cite> <span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/microsoft/deberta-v3-large" title="">https://huggingface.co/microsoft/deberta-v3-large</a></span></span></span> (435M) and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">Mistral 4B</span> (4B params), the latter pruned and modified from Mistral 7B v0.1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib12" title="">2023</a>)</cite> as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Those ranking models are all fine-tuned with the same compute budget (max 4 hours of training in a single server with 8x A100 GPUs) and same train set.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.T2" title="Table 2 ‣ 5.1. Model size matters ‣ 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the comparison of fine-tuning those different ranking models, with the same train set and compute budget. Although the pre-training of those base models is different, it is possible to observe a pattern where larger ranking models provide higher retrieval accuracy.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">The most accurate model is based on <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.1">Mistral 4B</span>, but <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">deberta-v3-large</span> is surprisingly accurate for its smaller number of parameters, and is a good candidate architecture for deploying as a cross-encoder, as we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S6" title="6. Deployment trade-off considerations for text retrieval pipelines with ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Comparing multi-stage text retrieval pipelines with different-sized ranking models, fine-tuned with the same train set and compute budget. Metric is NDCG@10.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.1.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1" style="font-size:80%;">Reranker model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1" style="font-size:80%;">Avg.</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.3.1">
<span class="ltx_p" id="S5.T2.1.1.1.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1.1.1" style="font-size:80%;">NQ</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.4.1">
<span class="ltx_p" id="S5.T2.1.1.1.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1.1.1" style="font-size:80%;">HotpotQA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.5.1" style="font-size:80%;">FiQA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.2.1.1">
<span class="ltx_p" id="S5.T2.1.2.2.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.2.2.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T2.1.2.2.1.1.1.1.1"> snowflake-arctic-embed-l</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.2.2.2"><span class="ltx_text" id="S5.T2.1.2.2.2.1" style="font-size:80%;">0.6100</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.2.3.1">
<span class="ltx_p" id="S5.T2.1.2.2.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.2.2.3.1.1.1" style="font-size:80%;">0.6311</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.2.4.1">
<span class="ltx_p" id="S5.T2.1.2.2.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.2.2.4.1.1.1" style="font-size:80%;">0.7518</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.2.5"><span class="ltx_text" id="S5.T2.1.2.2.5.1" style="font-size:80%;">0.4471</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.1.1">
<span class="ltx_p" id="S5.T2.1.3.3.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.3.3.1.1.1.1" style="font-size:80%;">+ MiniLM-L12-H384-uncased</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.3.3.2"><span class="ltx_text" id="S5.T2.1.3.3.2.1" style="font-size:80%;">0.6227</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.3.1">
<span class="ltx_p" id="S5.T2.1.3.3.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.3.3.3.1.1.1" style="font-size:80%;">0.6436</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.4.1">
<span class="ltx_p" id="S5.T2.1.3.3.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.3.3.4.1.1.1" style="font-size:80%;">0.8128</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.3.5"><span class="ltx_text" id="S5.T2.1.3.3.5.1" style="font-size:80%;">0.4118</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.1.1">
<span class="ltx_p" id="S5.T2.1.4.4.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.4.4.1.1.1.1" style="font-size:80%;">+ deberta-v3-large</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.4.4.2"><span class="ltx_text" id="S5.T2.1.4.4.2.1" style="font-size:80%;">0.7277</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.3.1">
<span class="ltx_p" id="S5.T2.1.4.4.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.4.4.3.1.1.1" style="font-size:80%;">0.7452</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.4.1">
<span class="ltx_p" id="S5.T2.1.4.4.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.4.4.4.1.1.1" style="font-size:80%;">0.8548</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.4.5"><span class="ltx_text" id="S5.T2.1.4.4.5.1" style="font-size:80%;">0.5832</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.1.1">
<span class="ltx_p" id="S5.T2.1.5.5.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.5.5.1.1.1.1" style="font-size:80%;">+ Mistral 4B</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.2.1" style="font-size:80%;">0.7414</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.3.1">
<span class="ltx_p" id="S5.T2.1.5.5.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.3.1.1.1" style="font-size:80%;">0.7690</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.4.1">
<span class="ltx_p" id="S5.T2.1.5.5.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.4.1.1.1" style="font-size:80%;">0.8681</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.5.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.5.1" style="font-size:80%;">0.5872</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.6.1.1">
<span class="ltx_p" id="S5.T2.1.6.6.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.6.6.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T2.1.6.6.1.1.1.1.1"> NV-EmbedQA-e5-v5</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.6.6.2"><span class="ltx_text" id="S5.T2.1.6.6.2.1" style="font-size:80%;">0.6083</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.6.3.1">
<span class="ltx_p" id="S5.T2.1.6.6.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.6.6.3.1.1.1" style="font-size:80%;">0.6380</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.6.4.1">
<span class="ltx_p" id="S5.T2.1.6.6.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.6.6.4.1.1.1" style="font-size:80%;">0.7160</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.6.6.5"><span class="ltx_text" id="S5.T2.1.6.6.5.1" style="font-size:80%;">0.4710</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.1.1">
<span class="ltx_p" id="S5.T2.1.7.7.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.7.7.1.1.1.1" style="font-size:80%;">+ MiniLM-L12-H384-uncased</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.7.7.2"><span class="ltx_text" id="S5.T2.1.7.7.2.1" style="font-size:80%;">0.6213</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.3.1">
<span class="ltx_p" id="S5.T2.1.7.7.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.7.7.3.1.1.1" style="font-size:80%;">0.6438</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.4.1">
<span class="ltx_p" id="S5.T2.1.7.7.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.7.7.4.1.1.1" style="font-size:80%;">0.7954</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.7.7.5"><span class="ltx_text" id="S5.T2.1.7.7.5.1" style="font-size:80%;">0.4248</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.1.1">
<span class="ltx_p" id="S5.T2.1.8.8.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.8.8.1.1.1.1" style="font-size:80%;">+ deberta-v3-large</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.8.8.2"><span class="ltx_text" id="S5.T2.1.8.8.2.1" style="font-size:80%;">0.7150</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.3.1">
<span class="ltx_p" id="S5.T2.1.8.8.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.8.8.3.1.1.1" style="font-size:80%;">0.7441</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.4.1">
<span class="ltx_p" id="S5.T2.1.8.8.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.8.8.4.1.1.1" style="font-size:80%;">0.8319</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.8.8.5"><span class="ltx_text" id="S5.T2.1.8.8.5.1" style="font-size:80%;">0.5689</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.1.1">
<span class="ltx_p" id="S5.T2.1.9.9.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.9.9.1.1.1.1" style="font-size:80%;">+ Mistral 4B</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.9.2.1" style="font-size:80%;">0.7366</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.3.1">
<span class="ltx_p" id="S5.T2.1.9.9.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.9.3.1.1.1" style="font-size:80%;">0.7689</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.4.1">
<span class="ltx_p" id="S5.T2.1.9.9.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.9.4.1.1.1" style="font-size:80%;">0.8423</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.9.5.1" style="font-size:80%;">0.5987</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.10.1.1">
<span class="ltx_p" id="S5.T2.1.10.10.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.10.10.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T2.1.10.10.1.1.1.1.1"> NV-EmbedQA-Mistral7B-v2</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.10.10.2"><span class="ltx_text" id="S5.T2.1.10.10.2.1" style="font-size:80%;">0.7173</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.10.3.1">
<span class="ltx_p" id="S5.T2.1.10.10.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.10.10.3.1.1.1" style="font-size:80%;">0.7216</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.10.4.1">
<span class="ltx_p" id="S5.T2.1.10.10.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.10.10.4.1.1.1" style="font-size:80%;">0.8109</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.10.10.5"><span class="ltx_text" id="S5.T2.1.10.10.5.1" style="font-size:80%;">0.6194</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.1.1">
<span class="ltx_p" id="S5.T2.1.11.11.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.11.11.1.1.1.1" style="font-size:80%;">+ MiniLM-L12-H384-uncased</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.11.11.2"><span class="ltx_text" id="S5.T2.1.11.11.2.1" style="font-size:80%;">0.6355</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.3.1">
<span class="ltx_p" id="S5.T2.1.11.11.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.11.11.3.1.1.1" style="font-size:80%;">0.6484</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T2.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.4.1">
<span class="ltx_p" id="S5.T2.1.11.11.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.11.11.4.1.1.1" style="font-size:80%;">0.8269</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.11.11.5"><span class="ltx_text" id="S5.T2.1.11.11.5.1" style="font-size:80%;">0.4312</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.1.1">
<span class="ltx_p" id="S5.T2.1.12.12.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.12.12.1.1.1.1" style="font-size:80%;">+ deberta-v3-large</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.12.12.2"><span class="ltx_text" id="S5.T2.1.12.12.2.1" style="font-size:80%;">0.7413</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.3.1">
<span class="ltx_p" id="S5.T2.1.12.12.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T2.1.12.12.3.1.1.1" style="font-size:80%;">0.7486</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.4.1">
<span class="ltx_p" id="S5.T2.1.12.12.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T2.1.12.12.4.1.1.1" style="font-size:80%;">0.8700</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.12.12.5"><span class="ltx_text" id="S5.T2.1.12.12.5.1" style="font-size:80%;">0.6055</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T2.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.13.1.1">
<span class="ltx_p" id="S5.T2.1.13.13.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T2.1.13.13.1.1.1.1" style="font-size:80%;">+ Mistral 4B</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.13.2.1" style="font-size:80%;">0.7575</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T2.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.13.3.1">
<span class="ltx_p" id="S5.T2.1.13.13.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.13.3.1.1.1" style="font-size:80%;">0.7717</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T2.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.13.4.1">
<span class="ltx_p" id="S5.T2.1.13.13.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.13.4.1.1.1" style="font-size:80%;">0.8857</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.13.5.1" style="font-size:80%;">0.6152</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Causal vs Bi-directional Attention mechanism</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a> we describe that for our adapted <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Mistral 4B</span> we modified the standard self-attention mechanism of Mistral from uni-directional (causal) to bi-directional attention.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We compare the accuracy with those two self-attention mechanisms in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.T3" title="Table 3 ‣ 5.2. Causal vs Bi-directional Attention mechanism ‣ 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">3</span></a>, both using average pooling, and demonstrate the effectiveness of bi-directional attention for allowing deeper interaction among input query and passage tokens.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparing NDCG@10 of Mistral 4B reranker fine-tuned with different attention mechanisms.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.1.1.1.1">
<span class="ltx_p" id="S5.T3.1.1.1.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.1" style="font-size:80%;">Reranker model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1" style="font-size:80%;">Avg.</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.1.1.3.1">
<span class="ltx_p" id="S5.T3.1.1.1.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1.1.1" style="font-size:80%;">NQ</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.1.1.4.1">
<span class="ltx_p" id="S5.T3.1.1.1.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1.1.1" style="font-size:80%;">HotpotQA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.5.1" style="font-size:80%;">FiQA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.2.2.1.1">
<span class="ltx_p" id="S5.T3.1.2.2.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.2.2.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T3.1.2.2.1.1.1.1.1"> snowflake-arctic-embed-l</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.2.2.2"><span class="ltx_text" id="S5.T3.1.2.2.2.1" style="font-size:80%;">0.6100</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.2.2.3.1">
<span class="ltx_p" id="S5.T3.1.2.2.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.2.2.3.1.1.1" style="font-size:80%;">0.6311</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.2.2.4.1">
<span class="ltx_p" id="S5.T3.1.2.2.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.2.2.4.1.1.1" style="font-size:80%;">0.7518</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.5"><span class="ltx_text" id="S5.T3.1.2.2.5.1" style="font-size:80%;">0.4471</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.3.3.1.1">
<span class="ltx_p" id="S5.T3.1.3.3.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.3.3.1.1.1.1" style="font-size:80%;">+ Mistral 4B (unidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.3.2"><span class="ltx_text" id="S5.T3.1.3.3.2.1" style="font-size:80%;">0.7312</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.3.3.3.1">
<span class="ltx_p" id="S5.T3.1.3.3.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.3.3.3.1.1.1" style="font-size:80%;">0.7663</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.3.3.4.1">
<span class="ltx_p" id="S5.T3.1.3.3.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.3.3.4.1.1.1" style="font-size:80%;">0.8612</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.3.5"><span class="ltx_text" id="S5.T3.1.3.3.5.1" style="font-size:80%;">0.5660</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.4.4.1.1">
<span class="ltx_p" id="S5.T3.1.4.4.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.4.4.1.1.1.1" style="font-size:80%;">+ Mistral 4B (bidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.2.1" style="font-size:80%;">0.7414</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.4.4.3.1">
<span class="ltx_p" id="S5.T3.1.4.4.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.3.1.1.1" style="font-size:80%;">0.7690</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.4.4.4.1">
<span class="ltx_p" id="S5.T3.1.4.4.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.4.1.1.1" style="font-size:80%;">0.8681</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.5.1" style="font-size:80%;">0.5872</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.5.5.1.1">
<span class="ltx_p" id="S5.T3.1.5.5.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.5.5.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T3.1.5.5.1.1.1.1.1"> NV-EmbedQA-e5-v5</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T3.1.5.5.2"><span class="ltx_text" id="S5.T3.1.5.5.2.1" style="font-size:80%;">0.6083</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.5.5.3.1">
<span class="ltx_p" id="S5.T3.1.5.5.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.5.5.3.1.1.1" style="font-size:80%;">0.6380</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.5.5.4.1">
<span class="ltx_p" id="S5.T3.1.5.5.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.5.5.4.1.1.1" style="font-size:80%;">0.7160</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.5.5.5"><span class="ltx_text" id="S5.T3.1.5.5.5.1" style="font-size:80%;">0.4710</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.6.6.1.1">
<span class="ltx_p" id="S5.T3.1.6.6.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.6.6.1.1.1.1" style="font-size:80%;">+ Mistral 4B (unidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.6.6.2"><span class="ltx_text" id="S5.T3.1.6.6.2.1" style="font-size:80%;">0.7264</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.6.6.3.1">
<span class="ltx_p" id="S5.T3.1.6.6.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.6.6.3.1.1.1" style="font-size:80%;">0.7655</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.6.6.4.1">
<span class="ltx_p" id="S5.T3.1.6.6.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.6.6.4.1.1.1" style="font-size:80%;">0.8372</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.6.6.5"><span class="ltx_text" id="S5.T3.1.6.6.5.1" style="font-size:80%;">0.5766</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.7.7.1.1">
<span class="ltx_p" id="S5.T3.1.7.7.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.7.7.1.1.1.1" style="font-size:80%;">+ Mistral 4B (bidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.2.1" style="font-size:80%;">0.7366</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.7.7.3.1">
<span class="ltx_p" id="S5.T3.1.7.7.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.3.1.1.1" style="font-size:80%;">0.7689</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.7.7.4.1">
<span class="ltx_p" id="S5.T3.1.7.7.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.4.1.1.1" style="font-size:80%;">0.8423</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.5.1" style="font-size:80%;">0.5987</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.8.8.1.1">
<span class="ltx_p" id="S5.T3.1.8.8.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.8.8.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T3.1.8.8.1.1.1.1.1"> NV-EmbedQA-Mistral7B-v2</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T3.1.8.8.2"><span class="ltx_text" id="S5.T3.1.8.8.2.1" style="font-size:80%;">0.7173</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.8.8.3.1">
<span class="ltx_p" id="S5.T3.1.8.8.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.8.8.3.1.1.1" style="font-size:80%;">0.7216</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.8.8.4.1">
<span class="ltx_p" id="S5.T3.1.8.8.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.8.8.4.1.1.1" style="font-size:80%;">0.8109</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.8.8.5"><span class="ltx_text" id="S5.T3.1.8.8.5.1" style="font-size:80%;">0.6194</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.9.9.1.1">
<span class="ltx_p" id="S5.T3.1.9.9.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.9.9.1.1.1.1" style="font-size:80%;">+ Mistral 4B (unidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.9.9.2"><span class="ltx_text" id="S5.T3.1.9.9.2.1" style="font-size:80%;">0.7464</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.9.9.3.1">
<span class="ltx_p" id="S5.T3.1.9.9.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T3.1.9.9.3.1.1.1" style="font-size:80%;">0.7690</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.9.9.4.1">
<span class="ltx_p" id="S5.T3.1.9.9.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T3.1.9.9.4.1.1.1" style="font-size:80%;">0.8781</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.9.9.5"><span class="ltx_text" id="S5.T3.1.9.9.5.1" style="font-size:80%;">0.5920</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T3.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.10.10.1.1">
<span class="ltx_p" id="S5.T3.1.10.10.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T3.1.10.10.1.1.1.1" style="font-size:80%;">+ Mistral 4B (bidirectional attention)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.10.10.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.2.1" style="font-size:80%;">0.7575</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T3.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.10.10.3.1">
<span class="ltx_p" id="S5.T3.1.10.10.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.3.1.1.1" style="font-size:80%;">0.7717</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T3.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.1.10.10.4.1">
<span class="ltx_p" id="S5.T3.1.10.10.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.4.1.1.1" style="font-size:80%;">0.8857</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.5.1" style="font-size:80%;">0.6152</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>BCE vs InfoNCE Loss</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Cross-encoders are typically trained with the point-wise Binary Cross-Entropy (BCE) loss (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4.E1" title="In 4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>), as we discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">On the other hand, we fine-tune <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">Mistral 4B</span> with the list-wise InfoNCE loss<cite class="ltx_cite ltx_citemacro_citep">(Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#bib.bib26" title="">2018</a>)</cite> (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4.E2" title="In 4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">2</span></a>) and contrastive learning.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">We experiment with those two losses, both using the same sets of hard-negative passages mined from the corpus, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S4" title="4. Fine-tuning a state-of-the-art ranking model: NV-RerankQA-Mistral-4B-v3 ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.T4" title="Table 4 ‣ 5.3. BCE vs InfoNCE Loss ‣ 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">4</span></a>, we can clearly observe the higher retrieval accuracy obtained when using InfoNCE, a list-wise contrastive learning loss trained to maximize the relevance score of the question and positive passage pair, while minimizing the score for question and negative passage pairs.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparing NDCG@10 of Mistral 4B reranker with bi-directional attention fine-tuned with different losses.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.1" style="font-size:80%;">Reranker model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1" style="font-size:80%;">Avg.</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.3.1">
<span class="ltx_p" id="S5.T4.1.1.1.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1.1.1" style="font-size:80%;">NQ</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.4.1">
<span class="ltx_p" id="S5.T4.1.1.1.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1.1.1" style="font-size:80%;">HotpotQA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.5.1" style="font-size:80%;">FiQA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.2.1.1">
<span class="ltx_p" id="S5.T4.1.2.2.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T4.1.2.2.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T4.1.2.2.1.1.1.1.1"> snowflake-arctic-embed-l</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.2.2.2"><span class="ltx_text" id="S5.T4.1.2.2.2.1" style="font-size:80%;">0.6100</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.2.3.1">
<span class="ltx_p" id="S5.T4.1.2.2.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.2.2.3.1.1.1" style="font-size:80%;">0.6311</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.2.4.1">
<span class="ltx_p" id="S5.T4.1.2.2.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.2.2.4.1.1.1" style="font-size:80%;">0.7518</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.2.5"><span class="ltx_text" id="S5.T4.1.2.2.5.1" style="font-size:80%;">0.4471</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.3.1.1">
<span class="ltx_p" id="S5.T4.1.3.3.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.3.3.1.1.1.1" style="font-size:80%;">+ Mistral 4B (BCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.3.3.2"><span class="ltx_text" id="S5.T4.1.3.3.2.1" style="font-size:80%;">0.7230</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.3.3.1">
<span class="ltx_p" id="S5.T4.1.3.3.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.3.3.3.1.1.1" style="font-size:80%;">0.7375</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.3.4.1">
<span class="ltx_p" id="S5.T4.1.3.3.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.3.3.4.1.1.1" style="font-size:80%;">0.8609</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.3.3.5"><span class="ltx_text" id="S5.T4.1.3.3.5.1" style="font-size:80%;">0.5706</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.4.1.1">
<span class="ltx_p" id="S5.T4.1.4.4.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.4.4.1.1.1.1" style="font-size:80%;">+ Mistral 4B (InfoNCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.4.2.1" style="font-size:80%;">0.7414</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.4.3.1">
<span class="ltx_p" id="S5.T4.1.4.4.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.4.3.1.1.1" style="font-size:80%;">0.7690</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.4.4.1">
<span class="ltx_p" id="S5.T4.1.4.4.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.4.4.1.1.1" style="font-size:80%;">0.8681</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.4.5.1" style="font-size:80%;">0.5872</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.5.1.1">
<span class="ltx_p" id="S5.T4.1.5.5.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T4.1.5.5.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T4.1.5.5.1.1.1.1.1"> NV-EmbedQA-e5-v5</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T4.1.5.5.2"><span class="ltx_text" id="S5.T4.1.5.5.2.1" style="font-size:80%;">0.6083</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.5.3.1">
<span class="ltx_p" id="S5.T4.1.5.5.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.5.5.3.1.1.1" style="font-size:80%;">0.6380</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.5.4.1">
<span class="ltx_p" id="S5.T4.1.5.5.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.5.5.4.1.1.1" style="font-size:80%;">0.7160</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.1.5.5.5"><span class="ltx_text" id="S5.T4.1.5.5.5.1" style="font-size:80%;">0.4710</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.6.1.1">
<span class="ltx_p" id="S5.T4.1.6.6.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.6.6.1.1.1.1" style="font-size:80%;">+ Mistral 4B (BCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.6.6.2"><span class="ltx_text" id="S5.T4.1.6.6.2.1" style="font-size:80%;">0.7171</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.6.3.1">
<span class="ltx_p" id="S5.T4.1.6.6.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.6.6.3.1.1.1" style="font-size:80%;">0.7368</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.6.4.1">
<span class="ltx_p" id="S5.T4.1.6.6.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.6.6.4.1.1.1" style="font-size:80%;">0.8357</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.6.6.5"><span class="ltx_text" id="S5.T4.1.6.6.5.1" style="font-size:80%;">0.5786</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.7.7.1.1">
<span class="ltx_p" id="S5.T4.1.7.7.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.7.7.1.1.1.1" style="font-size:80%;">+ Mistral 4B (InfoNCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.7.2.1" style="font-size:80%;">0.7366</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.7.7.3.1">
<span class="ltx_p" id="S5.T4.1.7.7.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.7.3.1.1.1" style="font-size:80%;">0.7689</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.7.7.4.1">
<span class="ltx_p" id="S5.T4.1.7.7.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.7.4.1.1.1" style="font-size:80%;">0.8423</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.7.5.1" style="font-size:80%;">0.5987</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.8.8.1.1">
<span class="ltx_p" id="S5.T4.1.8.8.1.1.1" style="width:125.2pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T4.1.8.8.1.1.1.1" style="font-size:80%;">Embedding:<span class="ltx_text ltx_font_upright" id="S5.T4.1.8.8.1.1.1.1.1"> NV-EmbedQA-Mistral7B-v2</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T4.1.8.8.2"><span class="ltx_text" id="S5.T4.1.8.8.2.1" style="font-size:80%;">0.7173</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.8.8.3.1">
<span class="ltx_p" id="S5.T4.1.8.8.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.8.8.3.1.1.1" style="font-size:80%;">0.7216</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T4.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.8.8.4.1">
<span class="ltx_p" id="S5.T4.1.8.8.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.8.8.4.1.1.1" style="font-size:80%;">0.8109</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.1.8.8.5"><span class="ltx_text" id="S5.T4.1.8.8.5.1" style="font-size:80%;">0.6194</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.9.9.1.1">
<span class="ltx_p" id="S5.T4.1.9.9.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.9.9.1.1.1.1" style="font-size:80%;">+ Mistral 4B (BCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.9.9.2"><span class="ltx_text" id="S5.T4.1.9.9.2.1" style="font-size:80%;">0.7373</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.9.9.3.1">
<span class="ltx_p" id="S5.T4.1.9.9.3.1.1" style="width:14.2pt;"><span class="ltx_text" id="S5.T4.1.9.9.3.1.1.1" style="font-size:80%;">0.7394</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.9.9.4.1">
<span class="ltx_p" id="S5.T4.1.9.9.4.1.1" style="width:25.6pt;"><span class="ltx_text" id="S5.T4.1.9.9.4.1.1.1" style="font-size:80%;">0.8774</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.9.9.5"><span class="ltx_text" id="S5.T4.1.9.9.5.1" style="font-size:80%;">0.5949</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T4.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.10.10.1.1">
<span class="ltx_p" id="S5.T4.1.10.10.1.1.1" style="width:125.2pt;"><span class="ltx_text" id="S5.T4.1.10.10.1.1.1.1" style="font-size:80%;">+ Mistral 4B (InfoNCE loss)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T4.1.10.10.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.10.10.2.1" style="font-size:80%;">0.7575</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T4.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.10.10.3.1">
<span class="ltx_p" id="S5.T4.1.10.10.3.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.10.10.3.1.1.1" style="font-size:80%;">0.7717</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.T4.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.10.10.4.1">
<span class="ltx_p" id="S5.T4.1.10.10.4.1.1" style="width:25.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.10.10.4.1.1.1" style="font-size:80%;">0.8857</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T4.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.10.10.5.1" style="font-size:80%;">0.6152</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">This ablation study explains our choices of using bi-directional attention and InfoNCE loss for fine-tuning <span class="ltx_text ltx_font_italic" id="S5.SS3.p5.1.1">NV-RerankQA-Mistral-4B-v3</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Deployment trade-off considerations for text retrieval pipelines with ranking models</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">As we discussed before, the model sizes and usage of ranking models have implications on the performance of the deployed text retrieval pipelines and downstream systems that use it, such as RAG applications. Deploying the indexing pipeline to production using a large embedding model would be computationally expensive, especially if the document corpus is large. Furthermore, when we deploy query pipeline to production, it is critical that it can handle a large number of queries in a timely manner and scale on demand.
In some cases, it might be possible to improve both the retrieval accuracy and indexing throughput by replacing a single-stage query pipeline of a large embedding model by a two-stage pipeline composed of a smaller embedding model and a ranking model.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We conduct performance experiments with our models optimized<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>You can find Nemo Retriever embedding and ranking models optimized for fast serving with TensorRT and Triton Inference Server in <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.nvidia.com/nim/index.html#nemo-retriever" title="">https://docs.nvidia.com/nim/index.html#nemo-retriever</a></span></span></span> with TensorRT<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/tensorrt" title="">https://developer.nvidia.com/tensorrt</a></span></span></span> and deployed on Triton Inference Server<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/triton-inference-server" title="">https://developer.nvidia.com/triton-inference-server</a></span></span></span>. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S6.T5" title="Table 5 ‣ 6. Deployment trade-off considerations for text retrieval pipelines with ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">5</span></a>, we present the average query embedding latency and passages indexing throughput for two embedding models with different sizes. Although the time difference to embed a single query with those two models will not compromise the overall online retrieval latency, the indexing time of the chunked passages of the textual corpus will take 8.2x longer with the larger embedding model, which results in higher compute/cost requirements when re-indexing is needed (e.g. when corpus or embedding model changes).</p>
</div>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Average query embedding latency (batch size=1 question with 20 tokens) and passages indexing throughput (batch size=64 passages with 512 tokens), with models converted to FP-16 and TensorRT deployed on Triton Inference Server on a single H100-HBM3-80GB GPU<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>You can find more information on Nemo Retriever performance benchmarks with other GPUs / batch sizes for embedding models in <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/performance.html" title="">https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/performance.html</a> and for ranking models in <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/performance.html" title="">https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/performance.html</a></span></span></span></figcaption>
<p class="ltx_p" id="S6.T5.1">.

<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T5.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S6.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1" style="font-size:80%;">Embedding model</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S6.T5.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.1.1.1.1.2.1">
<span class="ltx_p" id="S6.T5.1.1.1.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1.1.1" style="font-size:80%;">Query embedding latency</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S6.T5.1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.1.1.1.1.3.1">
<span class="ltx_p" id="S6.T5.1.1.1.1.3.1.1" style="width:65.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.3.1.1.1" style="font-size:80%;">Passages indexing throughput</span></span>
</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T5.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.1.2.1.1"><span class="ltx_text" id="S6.T5.1.1.2.1.1.1" style="font-size:80%;">NV-EmbedQA-E5-v5</span></span>
<span class="ltx_td ltx_align_right ltx_align_top ltx_border_t" id="S6.T5.1.1.2.1.2"><span class="ltx_text" id="S6.T5.1.1.2.1.2.1" style="font-size:80%;">5.1 ms</span></span>
<span class="ltx_td ltx_align_right ltx_align_top ltx_border_t" id="S6.T5.1.1.2.1.3"><span class="ltx_text" id="S6.T5.1.1.2.1.3.1" style="font-size:80%;">558.4 passages/sec</span></span></span>
<span class="ltx_tr" id="S6.T5.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S6.T5.1.1.3.2.1"><span class="ltx_text" id="S6.T5.1.1.3.2.1.1" style="font-size:80%;">NV-EmbedQA-Mistral7B-v2</span></span>
<span class="ltx_td ltx_align_right ltx_align_top ltx_border_b" id="S6.T5.1.1.3.2.2"><span class="ltx_text" id="S6.T5.1.1.3.2.2.1" style="font-size:80%;">19.8 ms</span></span>
<span class="ltx_td ltx_align_right ltx_align_top ltx_border_b" id="S6.T5.1.1.3.2.3"><span class="ltx_text" id="S6.T5.1.1.3.2.3.1" style="font-size:80%;">68.7 passages/sec</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T5.1.2" style="font-size:80%;"></span></p>
</figure>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Thus, by using a two-stage pipeline with <span class="ltx_text ltx_font_italic" id="S6.p3.1.1">NV-EmbedQA-E5-v5</span> embedding and <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">NV-RerankQA-Mistral-4B-v3</span> ranking models instead of a single-stage pipeline with <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">NV-EmbedQA-Mistral7B-v2</span>, in addition to achieving higher retrieval accuracy (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S3.T1" title="Table 1 ‣ 3.4. Benchmark results ‣ 3. Benchmarking ranking models for Q&amp;A text retrieval ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">1</span></a>), we will also reduce the indexing time by 8.2x.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">We also have to consider the latency the ranking model adds to the query pipeline. In our example, after the query is embedded with <span class="ltx_text ltx_font_italic" id="S6.p4.1.1">NV-EmbedQA-E5-v5</span> (in 5.1 ms), candidate relevant passages are retrieved from a vector database using ANN, and the top-40 passages will be provided to a ranking model for final ranking. The 40 candidate passages would be scored on their relevancy with respect to the query by the <span class="ltx_text ltx_font_italic" id="S6.p4.1.2">NV-RerankQA-Mistral-4B-v3</span> model in total 266 ms on average, which might add a reasonable time to the overall query pipeline latency depending on the non-functional requirements for the system. That could be improved by distributing the ranking scoring requests of those 40 candidates among multiple GPUs / Triton Inference Server instances. Another option would be deploying a model with a good trade-off between size/latency and accuracy, like <span class="ltx_text ltx_font_italic" id="S6.p4.1.3">deberta-v3-large<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote25.1.1.1">25</span></span><span class="ltx_text ltx_font_upright" id="footnote25.5">We are actually building a ranking model based on </span><span class="ltx_text" id="footnote25.6">deberta-v3-large</span><span class="ltx_text ltx_font_upright" id="footnote25.7"> and plan to release it soon.</span></span></span></span></span>, which is much smaller (435M) than <span class="ltx_text ltx_font_italic" id="S6.p4.1.4">NV-RerankQA-Mistral-4B-v3</span> (4B) and might provide a good ranking accuracy as discussed in the ablation study on Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07691v1#S5.T2" title="Table 2 ‣ 5.1. Model size matters ‣ 5. Ablation study on fine-tuning ranking models ‣ Enhancing Q&amp;A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">In summary, the decision of which models to include in a text retrieval pipeline should consider the business requirements for retrieval accuracy and system requirements on indexing throughput and serving latency. For example, if the text corpus to index is huge, probably indexing throughput will be the bottleneck and smaller embedding models should be used. On the other hand, if serving latency requirements are very strict, a fast query pipeline is more critical, and large ranking models should be avoided.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we provide a comprehensive evaluation of multi-stage text retrieval pipelines for Question-Answering, a common use case for RAG applications.
The evaluated pipelines composed of commercially viable embedding models and state-of-the-art ranking models. We introduce the <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">NV-RerankQA-Mistral-4B-v3</span>, that provides the best ranking accuracy by a large margin in our benchmark and is commercially usable for industrial applications.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We describe how we adapted the decoder-only <span class="ltx_text ltx_font_italic" id="S7.p2.1.1">Mistral 7B</span> and fine-tuned it as a cross-encoder, pruning the base model and modifying its attention and pooling mechanism to build the <span class="ltx_text ltx_font_italic" id="S7.p2.1.2">NV-RerankQA-Mistral-4B-v3</span>.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">We also provide an ablation study comparing the fine-tuning of different-sized base models as cross-encoders, and highlight the relationship between their number of parameters and ranking accuracy. We also compare the benefits of leveraging bi-directional attention and InfoNCE loss for training a Mistral cross-encoder.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Finally, we discussed important deployment considerations for real-world text retrieval systems, with respect to trading-off model size, retrieval accuracy and systems requirements like serving latency and indexing throughput.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bajaj et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2016.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">arXiv preprint arXiv:1611.09268</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">arXiv preprint arXiv:2402.03216</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">International conference on machine learning</em>. PMLR, 1597–1607.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:1911.02116</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020.

</span>
<span class="ltx_bibblock">Overview of the TREC 2019 deep learning track.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:2003.07820</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. 2017.

</span>
<span class="ltx_bibblock">Neural ranking models with weak supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval</em>. 65–74.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016.

</span>
<span class="ltx_bibblock">A deep relevance matching model for ad-hoc retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 25th ACM international on conference on information and knowledge management</em>. 55–64.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hambarde and Proença (2023)</span>
<span class="ltx_bibblock">
Kailash A. Hambarde and Hugo Proença. 2023.

</span>
<span class="ltx_bibblock">Information Retrieval: Recent Advances and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Access</em> 11 (2023), 76581–76604.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ACCESS.2023.3295776" title="">https://doi.org/10.1109/ACCESS.2023.3295776</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">arXiv preprint arXiv:2111.09543</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">arXiv preprint arXiv:2310.06825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv preprint arXiv:2004.04906</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">Transactions of the Association for Computational Linguistics</em> 7 (2019), 453–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024.

</span>
<span class="ltx_bibblock">NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2405.17428</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib16.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2009.

</span>
<span class="ltx_bibblock">Learning to rank for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">Foundations and Trends® in Information Retrieval</em> 3, 3 (2009), 225–331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu and Li (2013)</span>
<span class="ltx_bibblock">
Zhengdong Lu and Hang Li. 2013.

</span>
<span class="ltx_bibblock">A deep architecture for matching short texts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in neural information processing systems</em> 26 (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maia et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018.

</span>
<span class="ltx_bibblock">Www’18 open challenge: financial opinion mining and question answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Companion proceedings of the the web conference 2018</em>. 1941–1942.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrick et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. 2024.

</span>
<span class="ltx_bibblock">Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2405.05374</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017.

</span>
<span class="ltx_bibblock">Learning to match using local and distributed representations of text for web search. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 26th international conference on world wide web</em>. 1291–1299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moreira et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Gabriel de Souza P Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. 2024.

</span>
<span class="ltx_bibblock">NV-Retriever: Improving text embedding models with effective hard-negative mining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2407.15831</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022.

</span>
<span class="ltx_bibblock">MTEB: Massive text embedding benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2210.07316</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira and Cho (2019)</span>
<span class="ltx_bibblock">
Rodrigo Nogueira and Kyunghyun Cho. 2019.

</span>
<span class="ltx_bibblock">Passage Re-ranking with BERT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1901.04085</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock">Multi-stage document ranking with BERT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:1910.14424</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:1807.03748</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019.

</span>
<span class="ltx_bibblock">Understanding the Behaviors of BERT in Ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:1904.07531</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Transactions of the Association for Computational Linguistics</em> 11 (2023), 1316–1331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:1908.10084</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock">Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2104.08663</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022a.

</span>
<span class="ltx_bibblock">Simlm: Pre-training with representation bottleneck for dense passage retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2207.02578</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022b.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2212.03533</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Improving text embeddings with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2401.00368</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2020a.

</span>
<span class="ltx_bibblock">Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2012.15828</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020b.

</span>
<span class="ltx_bibblock">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Advances in Neural Information Processing Systems</em> 33 (2020), 5776–5788.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (1903)</span>
<span class="ltx_bibblock">
Wei Yang, Haotian Zhang, and Jimmy Lin. 1903.

</span>
<span class="ltx_bibblock">Simple applications of BERT for ad hoc document retrieval. CoRR abs/1903.10972 (2019).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">URL: http://arxiv. org/abs/1903.10972</em> (1903).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:1809.09600</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zamani et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and Jaap Kamps. 2018.

</span>
<span class="ltx_bibblock">From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 497–506.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 01:41:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
