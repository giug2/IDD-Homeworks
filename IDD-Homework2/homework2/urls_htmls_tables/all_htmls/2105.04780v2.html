<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.04780] Cross-Modal Generative Augmentation for Visual Question Answering</title><meta property="og:description" content="Data augmentation has been shown to effectively improve the performance of multimodal machine learning models.
This paper introduces a generative model for data augmentation by leveraging the correlations among multiplâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cross-Modal Generative Augmentation for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cross-Modal Generative Augmentation for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.04780">

<!--Generated on Sun Mar 17 12:59:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Zixu Wangzixu.wang@imperial.ac.uk1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Yishu Miaoy.miao20@imperial.ac.uk1
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Lucia Special.specia@imperial.ac.uk12
<span id="p1.2.3" class="ltx_ERROR undefined">\addinstitution</span>
Department of Computing
<br class="ltx_break">Imperial College London
<br class="ltx_break">London, UK

<span id="p1.2.4" class="ltx_ERROR undefined">\addinstitution</span>
Department of Computer Science
University of Sheffield
<br class="ltx_break">Sheffield, UK

Cross-Modal Generative Augmentation for VQA



</p>
</div>
<h1 class="ltx_title ltx_title_document">Cross-Modal Generative Augmentation for Visual Question Answering</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Data augmentation has been shown to effectively improve the performance of multimodal machine learning models.
This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities.
Different from conventional data augmentation approaches that apply low-level operations with deterministic heuristics,
our method learns a generator that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework.
Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly optimised with a downstream task.
Experiments on Visual Question Answering as downstream task demonstrate the effectiveness of the proposed generative model, which is able to improve strong UpDn-based models to achieve state-of-the-art performance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multimodal machine learning is a multidisciplinary field combining language, vision, and speech processing to address a multitude of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">Ngiam etÂ al. (2011)</a>, <a href="#bib.bib5" title="" class="ltx_ref">BaltruÅ¡aitis etÂ al. (2019)</a>, <a href="#bib.bib34" title="" class="ltx_ref">Mogadala etÂ al. (2019)</a>, <a href="#bib.bib49" title="" class="ltx_ref">Uppal etÂ al. (2020)</a>, <a href="#bib.bib6" title="" class="ltx_ref">Bisk etÂ al. (2020)</a>]</cite>.
However, a major bottleneck in multimodal learning is the need for multi-way parallel data, i.e. data with all modalities for all samples.
For example, Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">Lu etÂ al. (2016)</a>, <a href="#bib.bib57" title="" class="ltx_ref">Yang etÂ al. (2016)</a>, <a href="#bib.bib3" title="" class="ltx_ref">Anderson etÂ al. (2018)</a>, <a href="#bib.bib59" title="" class="ltx_ref">Yu Jiang* etÂ al. (2018)</a>, <a href="#bib.bib45" title="" class="ltx_ref">Tan and Bansal (2019)</a>, <a href="#bib.bib44" title="" class="ltx_ref">Su etÂ al. (2020)</a>, <a href="#bib.bib9" title="" class="ltx_ref">Chen etÂ al. (2020b)</a>, <a href="#bib.bib13" title="" class="ltx_ref">Gan etÂ al. (2020)</a>]</cite> requires learning from parallel data across three different sources â€“ image, question and answer, a costly resource if created
at large scale.
In this paper we propose a generative model that leverages the joint distribution of multiple modalities from existing datasets to carry out data augmentation for VQA.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.2.2" class="ltx_tr">
<td id="S1.F1.2.2.2" class="ltx_td ltx_align_center">
<img src="/html/2105.04780/assets/x1.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="186" height="66" alt="Refer to caption">
<img src="/html/2105.04780/assets/x2.png" id="S1.F1.2.2.2.g2" class="ltx_graphics ltx_img_landscape" width="186" height="66" alt="Refer to caption">
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the proposed generative process for data augmentation. The left one describes the conventional VQA pipeline consisting of an image and annotated QA pairs.
The right one is our approach to generating QA pairs for data augmentation.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There are two major challenges for data augmentation.
First, the augmented data should contain meaningful <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">variations</span> and less repetition, and second, the <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">reliability</span> of the augmented data should be guaranteed and effectively evaluated.
The repetitive, insignificant variants or unreliable augmented data could have a negative effect on downstream tasks.
Conventional data augmentation approaches apply low-level operations, e.g. adding noise or using replacements via heuristics.
For example, <cite class="ltx_cite ltx_citemacro_citet">Chen etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Gokhale etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> apply visual and semantic transformations which create additional counterfactual samples to improve the modelâ€™s sensitivity to trivial noise in VQA.
This provides enough variations but requires specific design and extensive preprocessing to fit the task domain.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our proposed model explores the generative abilities of conditional distributions for augmenting question-answer (QA) pairs given only images.
Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the difference between a conventional VQA model and our proposed generative framework. The annotated QA pairs are directly used in supervised learning to predict answers in a VQA model, while our model aims at generating QA pairs from unlabelled images.
We first learn the conditional distributions of different modalities using annotated images and QA pairs.
Then, we introduce Q and A as two discrete latent variables, and construct a generative model for unlabelled images to generate QA pairs as augmented data.
Finally, the augmented data with reliability scores are used for training to improve on strong VQA base models.
Without any specific low-level operations on the data, we utilise the dynamics of the generative distributions and the compositionality of multiple modalities to explore the novel QA pairs given an image.
Additionally, the generators are optimised by the REINFORCE algorithm <cite class="ltx_cite ltx_citemacro_citep">(Williams, <a href="#bib.bib54" title="" class="ltx_ref">1992</a>)</cite> while minimising the variational lower bound.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The generative approach has the following advantages.
First, the model has strong generalisation ability to incorporate additional unseen and unlabelled images.
Diverse QA pairs can be generated by exploring the dynamics of the generator and the compositionality of multiple modalities.
Second, the answers are sampled from images before generating the questions, hence it is less prone to exploit linguistic priors in questions and to generate trivial QA pairs that are irrelevant to the given images.
Third, the augmented data can be quantified by the generative distribution, which acts as reliability scores of QA pairs for downstream VQA training.
Our approach opens a promising new direction for multimodal learning, with strong potential for cross-modal understanding and generalisation.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Using VQA as downstream task, our approach outperforms base models (UpDn and LXMERT), and substantially improves strong UpDn models, leading to the state-of-the-art performance on the task.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2105.04780/assets/x3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Architecture of the proposed generative model for data augmentation in the VQA classification task.
For the training of the generator <math id="S1.F2.5.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S1.F2.5.m1.2b"><mrow id="S1.F2.5.m1.2.2" xref="S1.F2.5.m1.2.2.cmml"><msub id="S1.F2.5.m1.2.2.3" xref="S1.F2.5.m1.2.2.3.cmml"><mi id="S1.F2.5.m1.2.2.3.2" xref="S1.F2.5.m1.2.2.3.2.cmml">q</mi><mi id="S1.F2.5.m1.2.2.3.3" xref="S1.F2.5.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S1.F2.5.m1.2.2.2" xref="S1.F2.5.m1.2.2.2.cmml">â€‹</mo><mrow id="S1.F2.5.m1.2.2.1.1" xref="S1.F2.5.m1.2.2.1.2.cmml"><mo stretchy="false" id="S1.F2.5.m1.2.2.1.1.2" xref="S1.F2.5.m1.2.2.1.2.cmml">(</mo><mi id="S1.F2.5.m1.1.1" xref="S1.F2.5.m1.1.1.cmml">Q</mi><mo id="S1.F2.5.m1.2.2.1.1.3" xref="S1.F2.5.m1.2.2.1.2.cmml">,</mo><mrow id="S1.F2.5.m1.2.2.1.1.1" xref="S1.F2.5.m1.2.2.1.1.1.cmml"><mi id="S1.F2.5.m1.2.2.1.1.1.2" xref="S1.F2.5.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" id="S1.F2.5.m1.2.2.1.1.1.1" xref="S1.F2.5.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S1.F2.5.m1.2.2.1.1.1.3" xref="S1.F2.5.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S1.F2.5.m1.2.2.1.1.4" xref="S1.F2.5.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.5.m1.2c"><apply id="S1.F2.5.m1.2.2.cmml" xref="S1.F2.5.m1.2.2"><times id="S1.F2.5.m1.2.2.2.cmml" xref="S1.F2.5.m1.2.2.2"></times><apply id="S1.F2.5.m1.2.2.3.cmml" xref="S1.F2.5.m1.2.2.3"><csymbol cd="ambiguous" id="S1.F2.5.m1.2.2.3.1.cmml" xref="S1.F2.5.m1.2.2.3">subscript</csymbol><ci id="S1.F2.5.m1.2.2.3.2.cmml" xref="S1.F2.5.m1.2.2.3.2">ğ‘</ci><ci id="S1.F2.5.m1.2.2.3.3.cmml" xref="S1.F2.5.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S1.F2.5.m1.2.2.1.2.cmml" xref="S1.F2.5.m1.2.2.1.1"><ci id="S1.F2.5.m1.1.1.cmml" xref="S1.F2.5.m1.1.1">ğ‘„</ci><apply id="S1.F2.5.m1.2.2.1.1.1.cmml" xref="S1.F2.5.m1.2.2.1.1.1"><csymbol cd="latexml" id="S1.F2.5.m1.2.2.1.1.1.1.cmml" xref="S1.F2.5.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S1.F2.5.m1.2.2.1.1.1.2.cmml" xref="S1.F2.5.m1.2.2.1.1.1.2">ğ´</ci><ci id="S1.F2.5.m1.2.2.1.1.1.3.cmml" xref="S1.F2.5.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.5.m1.2d">q_{\phi}(Q,A|V)</annotation></semantics></math>,
(1) given an image, we first build an image classification module <math id="S1.F2.6.m2.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S1.F2.6.m2.1b"><mrow id="S1.F2.6.m2.1.1" xref="S1.F2.6.m2.1.1.cmml"><msub id="S1.F2.6.m2.1.1.3" xref="S1.F2.6.m2.1.1.3.cmml"><mi id="S1.F2.6.m2.1.1.3.2" xref="S1.F2.6.m2.1.1.3.2.cmml">q</mi><mi id="S1.F2.6.m2.1.1.3.3" xref="S1.F2.6.m2.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S1.F2.6.m2.1.1.2" xref="S1.F2.6.m2.1.1.2.cmml">â€‹</mo><mrow id="S1.F2.6.m2.1.1.1.1" xref="S1.F2.6.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S1.F2.6.m2.1.1.1.1.2" xref="S1.F2.6.m2.1.1.1.1.1.cmml">(</mo><mrow id="S1.F2.6.m2.1.1.1.1.1" xref="S1.F2.6.m2.1.1.1.1.1.cmml"><mi id="S1.F2.6.m2.1.1.1.1.1.2" xref="S1.F2.6.m2.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S1.F2.6.m2.1.1.1.1.1.1" xref="S1.F2.6.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S1.F2.6.m2.1.1.1.1.1.3" xref="S1.F2.6.m2.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S1.F2.6.m2.1.1.1.1.3" xref="S1.F2.6.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.6.m2.1c"><apply id="S1.F2.6.m2.1.1.cmml" xref="S1.F2.6.m2.1.1"><times id="S1.F2.6.m2.1.1.2.cmml" xref="S1.F2.6.m2.1.1.2"></times><apply id="S1.F2.6.m2.1.1.3.cmml" xref="S1.F2.6.m2.1.1.3"><csymbol cd="ambiguous" id="S1.F2.6.m2.1.1.3.1.cmml" xref="S1.F2.6.m2.1.1.3">subscript</csymbol><ci id="S1.F2.6.m2.1.1.3.2.cmml" xref="S1.F2.6.m2.1.1.3.2">ğ‘</ci><ci id="S1.F2.6.m2.1.1.3.3.cmml" xref="S1.F2.6.m2.1.1.3.3">italic-Ï•</ci></apply><apply id="S1.F2.6.m2.1.1.1.1.1.cmml" xref="S1.F2.6.m2.1.1.1.1"><csymbol cd="latexml" id="S1.F2.6.m2.1.1.1.1.1.1.cmml" xref="S1.F2.6.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S1.F2.6.m2.1.1.1.1.1.2.cmml" xref="S1.F2.6.m2.1.1.1.1.1.2">ğ´</ci><ci id="S1.F2.6.m2.1.1.1.1.1.3.cmml" xref="S1.F2.6.m2.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.6.m2.1d">q_{\phi}(A|V)</annotation></semantics></math> to predict possible answers;
(2) with a sampled answer (<em id="S1.F2.13.1" class="ltx_emph ltx_font_italic">e.g</em><span id="S1.F2.14.2" class="ltx_ERROR undefined">\bmvaOneDot</span>â€œsunnyâ€) from answer distribution, we build a conditional question generator <math id="S1.F2.7.m3.3" class="ltx_Math" alttext="q_{\phi}(Q|V,A)" display="inline"><semantics id="S1.F2.7.m3.3b"><mrow id="S1.F2.7.m3.3.3" xref="S1.F2.7.m3.3.3.cmml"><msub id="S1.F2.7.m3.3.3.3" xref="S1.F2.7.m3.3.3.3.cmml"><mi id="S1.F2.7.m3.3.3.3.2" xref="S1.F2.7.m3.3.3.3.2.cmml">q</mi><mi id="S1.F2.7.m3.3.3.3.3" xref="S1.F2.7.m3.3.3.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S1.F2.7.m3.3.3.2" xref="S1.F2.7.m3.3.3.2.cmml">â€‹</mo><mrow id="S1.F2.7.m3.3.3.1.1" xref="S1.F2.7.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S1.F2.7.m3.3.3.1.1.2" xref="S1.F2.7.m3.3.3.1.1.1.cmml">(</mo><mrow id="S1.F2.7.m3.3.3.1.1.1" xref="S1.F2.7.m3.3.3.1.1.1.cmml"><mi id="S1.F2.7.m3.3.3.1.1.1.2" xref="S1.F2.7.m3.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S1.F2.7.m3.3.3.1.1.1.1" xref="S1.F2.7.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="S1.F2.7.m3.3.3.1.1.1.3.2" xref="S1.F2.7.m3.3.3.1.1.1.3.1.cmml"><mi id="S1.F2.7.m3.1.1" xref="S1.F2.7.m3.1.1.cmml">V</mi><mo id="S1.F2.7.m3.3.3.1.1.1.3.2.1" xref="S1.F2.7.m3.3.3.1.1.1.3.1.cmml">,</mo><mi id="S1.F2.7.m3.2.2" xref="S1.F2.7.m3.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S1.F2.7.m3.3.3.1.1.3" xref="S1.F2.7.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.7.m3.3c"><apply id="S1.F2.7.m3.3.3.cmml" xref="S1.F2.7.m3.3.3"><times id="S1.F2.7.m3.3.3.2.cmml" xref="S1.F2.7.m3.3.3.2"></times><apply id="S1.F2.7.m3.3.3.3.cmml" xref="S1.F2.7.m3.3.3.3"><csymbol cd="ambiguous" id="S1.F2.7.m3.3.3.3.1.cmml" xref="S1.F2.7.m3.3.3.3">subscript</csymbol><ci id="S1.F2.7.m3.3.3.3.2.cmml" xref="S1.F2.7.m3.3.3.3.2">ğ‘</ci><ci id="S1.F2.7.m3.3.3.3.3.cmml" xref="S1.F2.7.m3.3.3.3.3">italic-Ï•</ci></apply><apply id="S1.F2.7.m3.3.3.1.1.1.cmml" xref="S1.F2.7.m3.3.3.1.1"><csymbol cd="latexml" id="S1.F2.7.m3.3.3.1.1.1.1.cmml" xref="S1.F2.7.m3.3.3.1.1.1.1">conditional</csymbol><ci id="S1.F2.7.m3.3.3.1.1.1.2.cmml" xref="S1.F2.7.m3.3.3.1.1.1.2">ğ‘„</ci><list id="S1.F2.7.m3.3.3.1.1.1.3.1.cmml" xref="S1.F2.7.m3.3.3.1.1.1.3.2"><ci id="S1.F2.7.m3.1.1.cmml" xref="S1.F2.7.m3.1.1">ğ‘‰</ci><ci id="S1.F2.7.m3.2.2.cmml" xref="S1.F2.7.m3.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.7.m3.3d">q_{\phi}(Q|V,A)</annotation></semantics></math> using an LSTM to generate questions (<em id="S1.F2.15.3" class="ltx_emph ltx_font_italic">e.g</em><span id="S1.F2.16.4" class="ltx_ERROR undefined">\bmvaOneDot</span>â€œhow is the weather ?â€) corresponding to the image and the sampled answer;
then (3) we apply a neural network <math id="S1.F2.8.m4.3" class="ltx_Math" alttext="p_{\theta}(V|Q,A)" display="inline"><semantics id="S1.F2.8.m4.3b"><mrow id="S1.F2.8.m4.3.3" xref="S1.F2.8.m4.3.3.cmml"><msub id="S1.F2.8.m4.3.3.3" xref="S1.F2.8.m4.3.3.3.cmml"><mi id="S1.F2.8.m4.3.3.3.2" xref="S1.F2.8.m4.3.3.3.2.cmml">p</mi><mi id="S1.F2.8.m4.3.3.3.3" xref="S1.F2.8.m4.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S1.F2.8.m4.3.3.2" xref="S1.F2.8.m4.3.3.2.cmml">â€‹</mo><mrow id="S1.F2.8.m4.3.3.1.1" xref="S1.F2.8.m4.3.3.1.1.1.cmml"><mo stretchy="false" id="S1.F2.8.m4.3.3.1.1.2" xref="S1.F2.8.m4.3.3.1.1.1.cmml">(</mo><mrow id="S1.F2.8.m4.3.3.1.1.1" xref="S1.F2.8.m4.3.3.1.1.1.cmml"><mi id="S1.F2.8.m4.3.3.1.1.1.2" xref="S1.F2.8.m4.3.3.1.1.1.2.cmml">V</mi><mo fence="false" id="S1.F2.8.m4.3.3.1.1.1.1" xref="S1.F2.8.m4.3.3.1.1.1.1.cmml">|</mo><mrow id="S1.F2.8.m4.3.3.1.1.1.3.2" xref="S1.F2.8.m4.3.3.1.1.1.3.1.cmml"><mi id="S1.F2.8.m4.1.1" xref="S1.F2.8.m4.1.1.cmml">Q</mi><mo id="S1.F2.8.m4.3.3.1.1.1.3.2.1" xref="S1.F2.8.m4.3.3.1.1.1.3.1.cmml">,</mo><mi id="S1.F2.8.m4.2.2" xref="S1.F2.8.m4.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S1.F2.8.m4.3.3.1.1.3" xref="S1.F2.8.m4.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.8.m4.3c"><apply id="S1.F2.8.m4.3.3.cmml" xref="S1.F2.8.m4.3.3"><times id="S1.F2.8.m4.3.3.2.cmml" xref="S1.F2.8.m4.3.3.2"></times><apply id="S1.F2.8.m4.3.3.3.cmml" xref="S1.F2.8.m4.3.3.3"><csymbol cd="ambiguous" id="S1.F2.8.m4.3.3.3.1.cmml" xref="S1.F2.8.m4.3.3.3">subscript</csymbol><ci id="S1.F2.8.m4.3.3.3.2.cmml" xref="S1.F2.8.m4.3.3.3.2">ğ‘</ci><ci id="S1.F2.8.m4.3.3.3.3.cmml" xref="S1.F2.8.m4.3.3.3.3">ğœƒ</ci></apply><apply id="S1.F2.8.m4.3.3.1.1.1.cmml" xref="S1.F2.8.m4.3.3.1.1"><csymbol cd="latexml" id="S1.F2.8.m4.3.3.1.1.1.1.cmml" xref="S1.F2.8.m4.3.3.1.1.1.1">conditional</csymbol><ci id="S1.F2.8.m4.3.3.1.1.1.2.cmml" xref="S1.F2.8.m4.3.3.1.1.1.2">ğ‘‰</ci><list id="S1.F2.8.m4.3.3.1.1.1.3.1.cmml" xref="S1.F2.8.m4.3.3.1.1.1.3.2"><ci id="S1.F2.8.m4.1.1.cmml" xref="S1.F2.8.m4.1.1">ğ‘„</ci><ci id="S1.F2.8.m4.2.2.cmml" xref="S1.F2.8.m4.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.8.m4.3d">p_{\theta}(V|Q,A)</annotation></semantics></math> to measure the score of relevance between generated QA pairs and the image.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data Augmentation in VQA</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">There has been extensive work on improving VQA robustness with data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a href="#bib.bib46" title="" class="ltx_ref">2020a</a>; Zhu etÂ al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>; Kafle etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Gokhale etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>; Tang etÂ al., <a href="#bib.bib47" title="" class="ltx_ref">2020b</a>; Ray etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>.
The pioneer work is introduced by <cite class="ltx_cite ltx_citemacro_citet">Kafle etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> where they generate new questions by using semantic annotations
on images.
<cite class="ltx_cite ltx_citemacro_citet">Shah etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> proposes a cycle consistency training scheme where it generates questions and trains the model with question and answer consistency.
Augmenting VQA training data with counterfactual samples has also been proven effective <cite class="ltx_cite ltx_citemacro_citep">(Agarwal etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>; Kant etÂ al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> with complementary samples by masking critical objects in images or words in questions, and assigning different ground-truth answers (<em id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em><span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span>, CSS <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite> and MUTANT <cite class="ltx_cite ltx_citemacro_citep">(Gokhale etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>).
However, most data augmentation work focuses on explicitly injecting semantic and visual perturbations on raw inputs or using adversarial samples to provide contrastive learning signals.
Our work augments the original dataset with implicitly generated QA pairs especially for unlabelled images. This helps the model learn correlations between modalities.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Question Generation</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Generating questions from images has received equal interests as VQA <cite class="ltx_cite ltx_citemacro_citep">(Mostafazadeh etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2016</a>; Jain etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>; Li etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2018</a>; Lee etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Xiong and Wu, <a href="#bib.bib56" title="" class="ltx_ref">2020</a>; Vedd etÂ al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>.
Several recent works have explored the task of visual question generation with variational auto-encoders and maximising mutual information with answer categories <cite class="ltx_cite ltx_citemacro_citep">(Jain etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>; Krishna etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>.
Our work draws inspiration from VQG by incorporating a generation module in the generative framework.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Our generative model incorporates a cross-modal retrieval module <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Wang etÂ al., <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite> to measure the correlations between generated QA pairs and the image.
Cross-modal retrieval performance relies on appropriate representations for multi-modal data.
Most of the existing studies on cross-modal retrieval mainly focus on learning a high-level common space and exploiting visual-semantic embedding to calculate the similarities between image and sentence features with ranking loss <cite class="ltx_cite ltx_citemacro_citep">(Socher etÂ al., <a href="#bib.bib43" title="" class="ltx_ref">2014</a>; Kiros etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2014</a>; Vendrov etÂ al., <a href="#bib.bib51" title="" class="ltx_ref">2016</a>; Wang etÂ al., <a href="#bib.bib52" title="" class="ltx_ref">2016</a>; Fang etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>.
We build a classifier to output the reliability scores of generated QA pairs and images.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Variational auto-encoders (VAE) are generally applied for unsupervised representation learning of language and images <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Welling, <a href="#bib.bib22" title="" class="ltx_ref">2014</a>; Rezende etÂ al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>; Mnih and Gregor, <a href="#bib.bib33" title="" class="ltx_ref">2014</a>; Kingma etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2014</a>; Miao etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2016</a>; Miao and Blunsom, <a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite>.
Inspired by the idea of VAE for data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>; Hou etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>; Yoo etÂ al., <a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite>, we propose a generative framework in order to explore cross-modal interactions in the multimodal data for the task of VQA.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.5" class="ltx_p">Here, <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">V</annotation></semantics></math>, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">Q</annotation></semantics></math>, <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">A</annotation></semantics></math>Â are used to denote the input image, question, and the answer respectively.
Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the structure of variational auto-encoder for VQA. <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">Q</annotation></semantics></math>, <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">A</annotation></semantics></math> are the two latent variables introduced to represent questions and answers, respectively.
The training objective consists of the variable lower bound and the cross-entropy (CE) loss for VQA classification:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.10" class="ltx_Math" alttext="E_{q_{\phi}(Q,A|V)}[\log p_{\theta}(V,Q,A)-\log q_{\phi}(Q,A|V)]+p_{\psi}(A|V,Q)" display="block"><semantics id="S3.E1.m1.10a"><mrow id="S3.E1.m1.10.10" xref="S3.E1.m1.10.10.cmml"><mrow id="S3.E1.m1.9.9.1" xref="S3.E1.m1.9.9.1.cmml"><msub id="S3.E1.m1.9.9.1.3" xref="S3.E1.m1.9.9.1.3.cmml"><mi id="S3.E1.m1.9.9.1.3.2" xref="S3.E1.m1.9.9.1.3.2.cmml">E</mi><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">q</mi><mi id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">Q</mi><mo id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">A</mi><mo fence="false" id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.4" xref="S3.E1.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.1.2" xref="S3.E1.m1.9.9.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.9.9.1.1.1" xref="S3.E1.m1.9.9.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.2" xref="S3.E1.m1.9.9.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.9.9.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.cmml"><mrow id="S3.E1.m1.9.9.1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.9.9.1.1.1.1.3.2" xref="S3.E1.m1.9.9.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.3.2.1" xref="S3.E1.m1.9.9.1.1.1.1.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.9.9.1.1.1.1.3.2a" xref="S3.E1.m1.9.9.1.1.1.1.3.2.cmml">â¡</mo><msub id="S3.E1.m1.9.9.1.1.1.1.3.2.2" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.3.2.2.2" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2.2.cmml">p</mi><mi id="S3.E1.m1.9.9.1.1.1.1.3.2.2.3" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2.3.cmml">Î¸</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.1.1.1.1.3.1" xref="S3.E1.m1.9.9.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E1.m1.9.9.1.1.1.1.3.3.2" xref="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.1.3.3.2.1" xref="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">V</mi><mo id="S3.E1.m1.9.9.1.1.1.1.3.3.2.2" xref="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Q</mi><mo id="S3.E1.m1.9.9.1.1.1.1.3.3.2.3" xref="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">A</mi><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.1.3.3.2.4" xref="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.9.9.1.1.1.1.2" xref="S3.E1.m1.9.9.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E1.m1.9.9.1.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.9.9.1.1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.1.3.1" xref="S3.E1.m1.9.9.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.9.9.1.1.1.1.1.3a" xref="S3.E1.m1.9.9.1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.E1.m1.9.9.1.1.1.1.1.3.2" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.1.3.2.2" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2.2.cmml">q</mi><mi id="S3.E1.m1.9.9.1.1.1.1.1.3.2.3" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2.3.cmml">Ï•</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.1.1.1.1.1.2" xref="S3.E1.m1.9.9.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.9.9.1.1.1.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.1.1.1.1.2" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">Q</mi><mo id="S3.E1.m1.9.9.1.1.1.1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.1.1.1.1.4" xref="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E1.m1.9.9.1.1.1.3" xref="S3.E1.m1.9.9.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.10.10.3" xref="S3.E1.m1.10.10.3.cmml">+</mo><mrow id="S3.E1.m1.10.10.2" xref="S3.E1.m1.10.10.2.cmml"><msub id="S3.E1.m1.10.10.2.3" xref="S3.E1.m1.10.10.2.3.cmml"><mi id="S3.E1.m1.10.10.2.3.2" xref="S3.E1.m1.10.10.2.3.2.cmml">p</mi><mi id="S3.E1.m1.10.10.2.3.3" xref="S3.E1.m1.10.10.2.3.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.10.10.2.2" xref="S3.E1.m1.10.10.2.2.cmml">â€‹</mo><mrow id="S3.E1.m1.10.10.2.1.1" xref="S3.E1.m1.10.10.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.10.10.2.1.1.2" xref="S3.E1.m1.10.10.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.10.10.2.1.1.1" xref="S3.E1.m1.10.10.2.1.1.1.cmml"><mi id="S3.E1.m1.10.10.2.1.1.1.2" xref="S3.E1.m1.10.10.2.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.E1.m1.10.10.2.1.1.1.1" xref="S3.E1.m1.10.10.2.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.10.10.2.1.1.1.3.2" xref="S3.E1.m1.10.10.2.1.1.1.3.1.cmml"><mi id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">V</mi><mo id="S3.E1.m1.10.10.2.1.1.1.3.2.1" xref="S3.E1.m1.10.10.2.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.8.8" xref="S3.E1.m1.8.8.cmml">Q</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.10.10.2.1.1.3" xref="S3.E1.m1.10.10.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.10b"><apply id="S3.E1.m1.10.10.cmml" xref="S3.E1.m1.10.10"><plus id="S3.E1.m1.10.10.3.cmml" xref="S3.E1.m1.10.10.3"></plus><apply id="S3.E1.m1.9.9.1.cmml" xref="S3.E1.m1.9.9.1"><times id="S3.E1.m1.9.9.1.2.cmml" xref="S3.E1.m1.9.9.1.2"></times><apply id="S3.E1.m1.9.9.1.3.cmml" xref="S3.E1.m1.9.9.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.3.1.cmml" xref="S3.E1.m1.9.9.1.3">subscript</csymbol><ci id="S3.E1.m1.9.9.1.3.2.cmml" xref="S3.E1.m1.9.9.1.3.2">ğ¸</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">ğ‘</ci><ci id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3">italic-Ï•</ci></apply><interval closure="open" id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘„</ci><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">ğ´</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">ğ‘‰</ci></apply></interval></apply></apply><apply id="S3.E1.m1.9.9.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.9.9.1.1.2.1.cmml" xref="S3.E1.m1.9.9.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.9.9.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1"><minus id="S3.E1.m1.9.9.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.2"></minus><apply id="S3.E1.m1.9.9.1.1.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3"><times id="S3.E1.m1.9.9.1.1.1.1.3.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.1"></times><apply id="S3.E1.m1.9.9.1.1.1.1.3.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2"><log id="S3.E1.m1.9.9.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2.1"></log><apply id="S3.E1.m1.9.9.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.9.9.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.E1.m1.9.9.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.2.2.3">ğœƒ</ci></apply></apply><vector id="S3.E1.m1.9.9.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.3.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ‘‰</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğ‘„</ci><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">ğ´</ci></vector></apply><apply id="S3.E1.m1.9.9.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1"><times id="S3.E1.m1.9.9.1.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.2"></times><apply id="S3.E1.m1.9.9.1.1.1.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3"><log id="S3.E1.m1.9.9.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3.1"></log><apply id="S3.E1.m1.9.9.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.9.9.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E1.m1.9.9.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.3.2.3">italic-Ï•</ci></apply></apply><interval closure="open" id="S3.E1.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1"><ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">ğ‘„</ci><apply id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.2">ğ´</ci><ci id="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.1.1.1.1.1.1.3">ğ‘‰</ci></apply></interval></apply></apply></apply></apply><apply id="S3.E1.m1.10.10.2.cmml" xref="S3.E1.m1.10.10.2"><times id="S3.E1.m1.10.10.2.2.cmml" xref="S3.E1.m1.10.10.2.2"></times><apply id="S3.E1.m1.10.10.2.3.cmml" xref="S3.E1.m1.10.10.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.2.3.1.cmml" xref="S3.E1.m1.10.10.2.3">subscript</csymbol><ci id="S3.E1.m1.10.10.2.3.2.cmml" xref="S3.E1.m1.10.10.2.3.2">ğ‘</ci><ci id="S3.E1.m1.10.10.2.3.3.cmml" xref="S3.E1.m1.10.10.2.3.3">ğœ“</ci></apply><apply id="S3.E1.m1.10.10.2.1.1.1.cmml" xref="S3.E1.m1.10.10.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.10.10.2.1.1.1.1.cmml" xref="S3.E1.m1.10.10.2.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.10.10.2.1.1.1.2.cmml" xref="S3.E1.m1.10.10.2.1.1.1.2">ğ´</ci><list id="S3.E1.m1.10.10.2.1.1.1.3.1.cmml" xref="S3.E1.m1.10.10.2.1.1.1.3.2"><ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">ğ‘‰</ci><ci id="S3.E1.m1.8.8.cmml" xref="S3.E1.m1.8.8">ğ‘„</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.10c">E_{q_{\phi}(Q,A|V)}[\log p_{\theta}(V,Q,A)-\log q_{\phi}(Q,A|V)]+p_{\psi}(A|V,Q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p2.10" class="ltx_p">where <math id="S3.p2.6.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S3.p2.6.m1.2a"><mrow id="S3.p2.6.m1.2.2" xref="S3.p2.6.m1.2.2.cmml"><msub id="S3.p2.6.m1.2.2.3" xref="S3.p2.6.m1.2.2.3.cmml"><mi id="S3.p2.6.m1.2.2.3.2" xref="S3.p2.6.m1.2.2.3.2.cmml">q</mi><mi id="S3.p2.6.m1.2.2.3.3" xref="S3.p2.6.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.6.m1.2.2.2" xref="S3.p2.6.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.p2.6.m1.2.2.1.1" xref="S3.p2.6.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.p2.6.m1.2.2.1.1.2" xref="S3.p2.6.m1.2.2.1.2.cmml">(</mo><mi id="S3.p2.6.m1.1.1" xref="S3.p2.6.m1.1.1.cmml">Q</mi><mo id="S3.p2.6.m1.2.2.1.1.3" xref="S3.p2.6.m1.2.2.1.2.cmml">,</mo><mrow id="S3.p2.6.m1.2.2.1.1.1" xref="S3.p2.6.m1.2.2.1.1.1.cmml"><mi id="S3.p2.6.m1.2.2.1.1.1.2" xref="S3.p2.6.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.p2.6.m1.2.2.1.1.1.1" xref="S3.p2.6.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.p2.6.m1.2.2.1.1.1.3" xref="S3.p2.6.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.p2.6.m1.2.2.1.1.4" xref="S3.p2.6.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m1.2b"><apply id="S3.p2.6.m1.2.2.cmml" xref="S3.p2.6.m1.2.2"><times id="S3.p2.6.m1.2.2.2.cmml" xref="S3.p2.6.m1.2.2.2"></times><apply id="S3.p2.6.m1.2.2.3.cmml" xref="S3.p2.6.m1.2.2.3"><csymbol cd="ambiguous" id="S3.p2.6.m1.2.2.3.1.cmml" xref="S3.p2.6.m1.2.2.3">subscript</csymbol><ci id="S3.p2.6.m1.2.2.3.2.cmml" xref="S3.p2.6.m1.2.2.3.2">ğ‘</ci><ci id="S3.p2.6.m1.2.2.3.3.cmml" xref="S3.p2.6.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S3.p2.6.m1.2.2.1.2.cmml" xref="S3.p2.6.m1.2.2.1.1"><ci id="S3.p2.6.m1.1.1.cmml" xref="S3.p2.6.m1.1.1">ğ‘„</ci><apply id="S3.p2.6.m1.2.2.1.1.1.cmml" xref="S3.p2.6.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.p2.6.m1.2.2.1.1.1.1.cmml" xref="S3.p2.6.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.p2.6.m1.2.2.1.1.1.2.cmml" xref="S3.p2.6.m1.2.2.1.1.1.2">ğ´</ci><ci id="S3.p2.6.m1.2.2.1.1.1.3.cmml" xref="S3.p2.6.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m1.2c">q_{\phi}(Q,A|V)</annotation></semantics></math> is the generator that generates corresponding QA pairs for given images and can be factorised into answer generator <math id="S3.p2.7.m2.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S3.p2.7.m2.1a"><mrow id="S3.p2.7.m2.1.1" xref="S3.p2.7.m2.1.1.cmml"><msub id="S3.p2.7.m2.1.1.3" xref="S3.p2.7.m2.1.1.3.cmml"><mi id="S3.p2.7.m2.1.1.3.2" xref="S3.p2.7.m2.1.1.3.2.cmml">q</mi><mi id="S3.p2.7.m2.1.1.3.3" xref="S3.p2.7.m2.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.7.m2.1.1.2" xref="S3.p2.7.m2.1.1.2.cmml">â€‹</mo><mrow id="S3.p2.7.m2.1.1.1.1" xref="S3.p2.7.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.7.m2.1.1.1.1.2" xref="S3.p2.7.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.p2.7.m2.1.1.1.1.1" xref="S3.p2.7.m2.1.1.1.1.1.cmml"><mi id="S3.p2.7.m2.1.1.1.1.1.2" xref="S3.p2.7.m2.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.p2.7.m2.1.1.1.1.1.1" xref="S3.p2.7.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.p2.7.m2.1.1.1.1.1.3" xref="S3.p2.7.m2.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.p2.7.m2.1.1.1.1.3" xref="S3.p2.7.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m2.1b"><apply id="S3.p2.7.m2.1.1.cmml" xref="S3.p2.7.m2.1.1"><times id="S3.p2.7.m2.1.1.2.cmml" xref="S3.p2.7.m2.1.1.2"></times><apply id="S3.p2.7.m2.1.1.3.cmml" xref="S3.p2.7.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.7.m2.1.1.3.1.cmml" xref="S3.p2.7.m2.1.1.3">subscript</csymbol><ci id="S3.p2.7.m2.1.1.3.2.cmml" xref="S3.p2.7.m2.1.1.3.2">ğ‘</ci><ci id="S3.p2.7.m2.1.1.3.3.cmml" xref="S3.p2.7.m2.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.p2.7.m2.1.1.1.1.1.cmml" xref="S3.p2.7.m2.1.1.1.1"><csymbol cd="latexml" id="S3.p2.7.m2.1.1.1.1.1.1.cmml" xref="S3.p2.7.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.p2.7.m2.1.1.1.1.1.2.cmml" xref="S3.p2.7.m2.1.1.1.1.1.2">ğ´</ci><ci id="S3.p2.7.m2.1.1.1.1.1.3.cmml" xref="S3.p2.7.m2.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m2.1c">q_{\phi}(A|V)</annotation></semantics></math> and question generator <math id="S3.p2.8.m3.3" class="ltx_Math" alttext="q_{\phi}(Q|V,A)" display="inline"><semantics id="S3.p2.8.m3.3a"><mrow id="S3.p2.8.m3.3.3" xref="S3.p2.8.m3.3.3.cmml"><msub id="S3.p2.8.m3.3.3.3" xref="S3.p2.8.m3.3.3.3.cmml"><mi id="S3.p2.8.m3.3.3.3.2" xref="S3.p2.8.m3.3.3.3.2.cmml">q</mi><mi id="S3.p2.8.m3.3.3.3.3" xref="S3.p2.8.m3.3.3.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.8.m3.3.3.2" xref="S3.p2.8.m3.3.3.2.cmml">â€‹</mo><mrow id="S3.p2.8.m3.3.3.1.1" xref="S3.p2.8.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p2.8.m3.3.3.1.1.2" xref="S3.p2.8.m3.3.3.1.1.1.cmml">(</mo><mrow id="S3.p2.8.m3.3.3.1.1.1" xref="S3.p2.8.m3.3.3.1.1.1.cmml"><mi id="S3.p2.8.m3.3.3.1.1.1.2" xref="S3.p2.8.m3.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.p2.8.m3.3.3.1.1.1.1" xref="S3.p2.8.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p2.8.m3.3.3.1.1.1.3.2" xref="S3.p2.8.m3.3.3.1.1.1.3.1.cmml"><mi id="S3.p2.8.m3.1.1" xref="S3.p2.8.m3.1.1.cmml">V</mi><mo id="S3.p2.8.m3.3.3.1.1.1.3.2.1" xref="S3.p2.8.m3.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p2.8.m3.2.2" xref="S3.p2.8.m3.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.p2.8.m3.3.3.1.1.3" xref="S3.p2.8.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.8.m3.3b"><apply id="S3.p2.8.m3.3.3.cmml" xref="S3.p2.8.m3.3.3"><times id="S3.p2.8.m3.3.3.2.cmml" xref="S3.p2.8.m3.3.3.2"></times><apply id="S3.p2.8.m3.3.3.3.cmml" xref="S3.p2.8.m3.3.3.3"><csymbol cd="ambiguous" id="S3.p2.8.m3.3.3.3.1.cmml" xref="S3.p2.8.m3.3.3.3">subscript</csymbol><ci id="S3.p2.8.m3.3.3.3.2.cmml" xref="S3.p2.8.m3.3.3.3.2">ğ‘</ci><ci id="S3.p2.8.m3.3.3.3.3.cmml" xref="S3.p2.8.m3.3.3.3.3">italic-Ï•</ci></apply><apply id="S3.p2.8.m3.3.3.1.1.1.cmml" xref="S3.p2.8.m3.3.3.1.1"><csymbol cd="latexml" id="S3.p2.8.m3.3.3.1.1.1.1.cmml" xref="S3.p2.8.m3.3.3.1.1.1.1">conditional</csymbol><ci id="S3.p2.8.m3.3.3.1.1.1.2.cmml" xref="S3.p2.8.m3.3.3.1.1.1.2">ğ‘„</ci><list id="S3.p2.8.m3.3.3.1.1.1.3.1.cmml" xref="S3.p2.8.m3.3.3.1.1.1.3.2"><ci id="S3.p2.8.m3.1.1.cmml" xref="S3.p2.8.m3.1.1">ğ‘‰</ci><ci id="S3.p2.8.m3.2.2.cmml" xref="S3.p2.8.m3.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m3.3c">q_{\phi}(Q|V,A)</annotation></semantics></math>;
<math id="S3.p2.9.m4.3" class="ltx_Math" alttext="p_{\theta}(V,Q,A)" display="inline"><semantics id="S3.p2.9.m4.3a"><mrow id="S3.p2.9.m4.3.4" xref="S3.p2.9.m4.3.4.cmml"><msub id="S3.p2.9.m4.3.4.2" xref="S3.p2.9.m4.3.4.2.cmml"><mi id="S3.p2.9.m4.3.4.2.2" xref="S3.p2.9.m4.3.4.2.2.cmml">p</mi><mi id="S3.p2.9.m4.3.4.2.3" xref="S3.p2.9.m4.3.4.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.9.m4.3.4.1" xref="S3.p2.9.m4.3.4.1.cmml">â€‹</mo><mrow id="S3.p2.9.m4.3.4.3.2" xref="S3.p2.9.m4.3.4.3.1.cmml"><mo stretchy="false" id="S3.p2.9.m4.3.4.3.2.1" xref="S3.p2.9.m4.3.4.3.1.cmml">(</mo><mi id="S3.p2.9.m4.1.1" xref="S3.p2.9.m4.1.1.cmml">V</mi><mo id="S3.p2.9.m4.3.4.3.2.2" xref="S3.p2.9.m4.3.4.3.1.cmml">,</mo><mi id="S3.p2.9.m4.2.2" xref="S3.p2.9.m4.2.2.cmml">Q</mi><mo id="S3.p2.9.m4.3.4.3.2.3" xref="S3.p2.9.m4.3.4.3.1.cmml">,</mo><mi id="S3.p2.9.m4.3.3" xref="S3.p2.9.m4.3.3.cmml">A</mi><mo stretchy="false" id="S3.p2.9.m4.3.4.3.2.4" xref="S3.p2.9.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.9.m4.3b"><apply id="S3.p2.9.m4.3.4.cmml" xref="S3.p2.9.m4.3.4"><times id="S3.p2.9.m4.3.4.1.cmml" xref="S3.p2.9.m4.3.4.1"></times><apply id="S3.p2.9.m4.3.4.2.cmml" xref="S3.p2.9.m4.3.4.2"><csymbol cd="ambiguous" id="S3.p2.9.m4.3.4.2.1.cmml" xref="S3.p2.9.m4.3.4.2">subscript</csymbol><ci id="S3.p2.9.m4.3.4.2.2.cmml" xref="S3.p2.9.m4.3.4.2.2">ğ‘</ci><ci id="S3.p2.9.m4.3.4.2.3.cmml" xref="S3.p2.9.m4.3.4.2.3">ğœƒ</ci></apply><vector id="S3.p2.9.m4.3.4.3.1.cmml" xref="S3.p2.9.m4.3.4.3.2"><ci id="S3.p2.9.m4.1.1.cmml" xref="S3.p2.9.m4.1.1">ğ‘‰</ci><ci id="S3.p2.9.m4.2.2.cmml" xref="S3.p2.9.m4.2.2">ğ‘„</ci><ci id="S3.p2.9.m4.3.3.cmml" xref="S3.p2.9.m4.3.3">ğ´</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m4.3c">p_{\theta}(V,Q,A)</annotation></semantics></math> is the cross-modal distribution that regularises the QA samples, and <math id="S3.p2.10.m5.3" class="ltx_Math" alttext="p_{\psi}(A|V,Q)" display="inline"><semantics id="S3.p2.10.m5.3a"><mrow id="S3.p2.10.m5.3.3" xref="S3.p2.10.m5.3.3.cmml"><msub id="S3.p2.10.m5.3.3.3" xref="S3.p2.10.m5.3.3.3.cmml"><mi id="S3.p2.10.m5.3.3.3.2" xref="S3.p2.10.m5.3.3.3.2.cmml">p</mi><mi id="S3.p2.10.m5.3.3.3.3" xref="S3.p2.10.m5.3.3.3.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.10.m5.3.3.2" xref="S3.p2.10.m5.3.3.2.cmml">â€‹</mo><mrow id="S3.p2.10.m5.3.3.1.1" xref="S3.p2.10.m5.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p2.10.m5.3.3.1.1.2" xref="S3.p2.10.m5.3.3.1.1.1.cmml">(</mo><mrow id="S3.p2.10.m5.3.3.1.1.1" xref="S3.p2.10.m5.3.3.1.1.1.cmml"><mi id="S3.p2.10.m5.3.3.1.1.1.2" xref="S3.p2.10.m5.3.3.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.p2.10.m5.3.3.1.1.1.1" xref="S3.p2.10.m5.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p2.10.m5.3.3.1.1.1.3.2" xref="S3.p2.10.m5.3.3.1.1.1.3.1.cmml"><mi id="S3.p2.10.m5.1.1" xref="S3.p2.10.m5.1.1.cmml">V</mi><mo id="S3.p2.10.m5.3.3.1.1.1.3.2.1" xref="S3.p2.10.m5.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p2.10.m5.2.2" xref="S3.p2.10.m5.2.2.cmml">Q</mi></mrow></mrow><mo stretchy="false" id="S3.p2.10.m5.3.3.1.1.3" xref="S3.p2.10.m5.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.10.m5.3b"><apply id="S3.p2.10.m5.3.3.cmml" xref="S3.p2.10.m5.3.3"><times id="S3.p2.10.m5.3.3.2.cmml" xref="S3.p2.10.m5.3.3.2"></times><apply id="S3.p2.10.m5.3.3.3.cmml" xref="S3.p2.10.m5.3.3.3"><csymbol cd="ambiguous" id="S3.p2.10.m5.3.3.3.1.cmml" xref="S3.p2.10.m5.3.3.3">subscript</csymbol><ci id="S3.p2.10.m5.3.3.3.2.cmml" xref="S3.p2.10.m5.3.3.3.2">ğ‘</ci><ci id="S3.p2.10.m5.3.3.3.3.cmml" xref="S3.p2.10.m5.3.3.3.3">ğœ“</ci></apply><apply id="S3.p2.10.m5.3.3.1.1.1.cmml" xref="S3.p2.10.m5.3.3.1.1"><csymbol cd="latexml" id="S3.p2.10.m5.3.3.1.1.1.1.cmml" xref="S3.p2.10.m5.3.3.1.1.1.1">conditional</csymbol><ci id="S3.p2.10.m5.3.3.1.1.1.2.cmml" xref="S3.p2.10.m5.3.3.1.1.1.2">ğ´</ci><list id="S3.p2.10.m5.3.3.1.1.1.3.1.cmml" xref="S3.p2.10.m5.3.3.1.1.1.3.2"><ci id="S3.p2.10.m5.1.1.cmml" xref="S3.p2.10.m5.1.1">ğ‘‰</ci><ci id="S3.p2.10.m5.2.2.cmml" xref="S3.p2.10.m5.2.2">ğ‘„</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.10.m5.3c">p_{\psi}(A|V,Q)</annotation></semantics></math> is the multi-label classification loss for VQA.
Note that the lower bound will act as a confidence measure of generate QA pairs and will be used to reweigh the cross-entropy loss while training the downstream VQA model.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The overall architecture of our generative model is illustrated in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The training steps are summarised as follows:
the joint distribution of three modalities (V, Q, and A) is modelled to learn prior knowledge on cross-modal interactions;
optimise the generator to generate QA pairs by first predicting answers from the image and then generating question;
sample QA pairs for additional unlabelled images to assist large-scale training for VQA.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generative Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The first term in the training objective is the variational lower bound that is optimised for learning cross-modal distributions.
In order to construct this lower bound, we need to model both the generative distribution <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><msub id="S3.SS1.p1.1.m1.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.3.2" xref="S3.SS1.p1.1.m1.2.2.3.2.cmml">q</mi><mi id="S3.SS1.p1.1.m1.2.2.3.3" xref="S3.SS1.p1.1.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Q</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.1.1.4" xref="S3.SS1.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><times id="S3.SS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2"></times><apply id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.3.2">ğ‘</ci><ci id="S3.SS1.p1.1.m1.2.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S3.SS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘„</ci><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2">ğ´</ci><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">q_{\phi}(Q,A|V)</annotation></semantics></math> and the cross-modal distribution <math id="S3.SS1.p1.2.m2.3" class="ltx_Math" alttext="p_{\theta}(V,Q,A)" display="inline"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.4" xref="S3.SS1.p1.2.m2.3.4.cmml"><msub id="S3.SS1.p1.2.m2.3.4.2" xref="S3.SS1.p1.2.m2.3.4.2.cmml"><mi id="S3.SS1.p1.2.m2.3.4.2.2" xref="S3.SS1.p1.2.m2.3.4.2.2.cmml">p</mi><mi id="S3.SS1.p1.2.m2.3.4.2.3" xref="S3.SS1.p1.2.m2.3.4.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.3.4.1" xref="S3.SS1.p1.2.m2.3.4.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.2.m2.3.4.3.2" xref="S3.SS1.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.3.4.3.2.1" xref="S3.SS1.p1.2.m2.3.4.3.1.cmml">(</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">V</mi><mo id="S3.SS1.p1.2.m2.3.4.3.2.2" xref="S3.SS1.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">Q</mi><mo id="S3.SS1.p1.2.m2.3.4.3.2.3" xref="S3.SS1.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml">A</mi><mo stretchy="false" id="S3.SS1.p1.2.m2.3.4.3.2.4" xref="S3.SS1.p1.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><apply id="S3.SS1.p1.2.m2.3.4.cmml" xref="S3.SS1.p1.2.m2.3.4"><times id="S3.SS1.p1.2.m2.3.4.1.cmml" xref="S3.SS1.p1.2.m2.3.4.1"></times><apply id="S3.SS1.p1.2.m2.3.4.2.cmml" xref="S3.SS1.p1.2.m2.3.4.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.4.2.1.cmml" xref="S3.SS1.p1.2.m2.3.4.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.4.2.2.cmml" xref="S3.SS1.p1.2.m2.3.4.2.2">ğ‘</ci><ci id="S3.SS1.p1.2.m2.3.4.2.3.cmml" xref="S3.SS1.p1.2.m2.3.4.2.3">ğœƒ</ci></apply><vector id="S3.SS1.p1.2.m2.3.4.3.1.cmml" xref="S3.SS1.p1.2.m2.3.4.3.2"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘‰</ci><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">ğ‘„</ci><ci id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3">ğ´</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">p_{\theta}(V,Q,A)</annotation></semantics></math>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Generative Distribution</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The training of the generator <math id="S3.SS1.SSS1.p1.1.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.2a"><mrow id="S3.SS1.SSS1.p1.1.m1.2.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.cmml"><msub id="S3.SS1.SSS1.p1.1.m1.2.2.3" xref="S3.SS1.SSS1.p1.1.m1.2.2.3.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.2.2.3.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.p1.1.m1.2.2.3.3" xref="S3.SS1.SSS1.p1.1.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.1.m1.2.2.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.p1.1.m1.2.2.1.1" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">Q</mi><mo id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.4" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.2b"><apply id="S3.SS1.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2"><times id="S3.SS1.SSS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.2"></times><apply id="S3.SS1.SSS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.2.2.3.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.2.2.3.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.p1.1.m1.2.2.3.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S3.SS1.SSS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">ğ‘„</ci><apply id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.2">ğ´</ci><ci id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.2c">q_{\phi}(Q,A|V)</annotation></semantics></math> can be decomposed into:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="q_{\phi}(Q,A|V)=q_{\phi}(Q|V,A)q_{\phi}(A|V)" display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.cmml"><msub id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.4.4.1.3.cmml"><mi id="S3.E2.m1.4.4.1.3.2" xref="S3.E2.m1.4.4.1.3.2.cmml">q</mi><mi id="S3.E2.m1.4.4.1.3.3" xref="S3.E2.m1.4.4.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">Q</mi><mo id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.cmml">,</mo><mrow id="S3.E2.m1.4.4.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.4.4.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.4" xref="S3.E2.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.4" xref="S3.E2.m1.6.6.4.cmml">=</mo><mrow id="S3.E2.m1.6.6.3" xref="S3.E2.m1.6.6.3.cmml"><msub id="S3.E2.m1.6.6.3.4" xref="S3.E2.m1.6.6.3.4.cmml"><mi id="S3.E2.m1.6.6.3.4.2" xref="S3.E2.m1.6.6.3.4.2.cmml">q</mi><mi id="S3.E2.m1.6.6.3.4.3" xref="S3.E2.m1.6.6.3.4.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.3.3" xref="S3.E2.m1.6.6.3.3.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.2.1.1" xref="S3.E2.m1.5.5.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.2.1.1.2" xref="S3.E2.m1.5.5.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.2.1.1.1" xref="S3.E2.m1.5.5.2.1.1.1.cmml"><mi id="S3.E2.m1.5.5.2.1.1.1.2" xref="S3.E2.m1.5.5.2.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.E2.m1.5.5.2.1.1.1.1" xref="S3.E2.m1.5.5.2.1.1.1.1.cmml">|</mo><mrow id="S3.E2.m1.5.5.2.1.1.1.3.2" xref="S3.E2.m1.5.5.2.1.1.1.3.1.cmml"><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">V</mi><mo id="S3.E2.m1.5.5.2.1.1.1.3.2.1" xref="S3.E2.m1.5.5.2.1.1.1.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.5.5.2.1.1.3" xref="S3.E2.m1.5.5.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.3.3a" xref="S3.E2.m1.6.6.3.3.cmml">â€‹</mo><msub id="S3.E2.m1.6.6.3.5" xref="S3.E2.m1.6.6.3.5.cmml"><mi id="S3.E2.m1.6.6.3.5.2" xref="S3.E2.m1.6.6.3.5.2.cmml">q</mi><mi id="S3.E2.m1.6.6.3.5.3" xref="S3.E2.m1.6.6.3.5.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.3.3b" xref="S3.E2.m1.6.6.3.3.cmml">â€‹</mo><mrow id="S3.E2.m1.6.6.3.2.1" xref="S3.E2.m1.6.6.3.2.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.3.2.1.2" xref="S3.E2.m1.6.6.3.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.3.2.1.1" xref="S3.E2.m1.6.6.3.2.1.1.cmml"><mi id="S3.E2.m1.6.6.3.2.1.1.2" xref="S3.E2.m1.6.6.3.2.1.1.2.cmml">A</mi><mo fence="false" id="S3.E2.m1.6.6.3.2.1.1.1" xref="S3.E2.m1.6.6.3.2.1.1.1.cmml">|</mo><mi id="S3.E2.m1.6.6.3.2.1.1.3" xref="S3.E2.m1.6.6.3.2.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.E2.m1.6.6.3.2.1.3" xref="S3.E2.m1.6.6.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.4.cmml" xref="S3.E2.m1.6.6.4"></eq><apply id="S3.E2.m1.4.4.1.cmml" xref="S3.E2.m1.4.4.1"><times id="S3.E2.m1.4.4.1.2.cmml" xref="S3.E2.m1.4.4.1.2"></times><apply id="S3.E2.m1.4.4.1.3.cmml" xref="S3.E2.m1.4.4.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.3.1.cmml" xref="S3.E2.m1.4.4.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.3.2.cmml" xref="S3.E2.m1.4.4.1.3.2">ğ‘</ci><ci id="S3.E2.m1.4.4.1.3.3.cmml" xref="S3.E2.m1.4.4.1.3.3">italic-Ï•</ci></apply><interval closure="open" id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ‘„</ci><apply id="S3.E2.m1.4.4.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2">ğ´</ci><ci id="S3.E2.m1.4.4.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3">ğ‘‰</ci></apply></interval></apply><apply id="S3.E2.m1.6.6.3.cmml" xref="S3.E2.m1.6.6.3"><times id="S3.E2.m1.6.6.3.3.cmml" xref="S3.E2.m1.6.6.3.3"></times><apply id="S3.E2.m1.6.6.3.4.cmml" xref="S3.E2.m1.6.6.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.3.4.1.cmml" xref="S3.E2.m1.6.6.3.4">subscript</csymbol><ci id="S3.E2.m1.6.6.3.4.2.cmml" xref="S3.E2.m1.6.6.3.4.2">ğ‘</ci><ci id="S3.E2.m1.6.6.3.4.3.cmml" xref="S3.E2.m1.6.6.3.4.3">italic-Ï•</ci></apply><apply id="S3.E2.m1.5.5.2.1.1.1.cmml" xref="S3.E2.m1.5.5.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.2.1.1.1.1.cmml" xref="S3.E2.m1.5.5.2.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.5.5.2.1.1.1.2.cmml" xref="S3.E2.m1.5.5.2.1.1.1.2">ğ‘„</ci><list id="S3.E2.m1.5.5.2.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.2.1.1.1.3.2"><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğ‘‰</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ´</ci></list></apply><apply id="S3.E2.m1.6.6.3.5.cmml" xref="S3.E2.m1.6.6.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.3.5.1.cmml" xref="S3.E2.m1.6.6.3.5">subscript</csymbol><ci id="S3.E2.m1.6.6.3.5.2.cmml" xref="S3.E2.m1.6.6.3.5.2">ğ‘</ci><ci id="S3.E2.m1.6.6.3.5.3.cmml" xref="S3.E2.m1.6.6.3.5.3">italic-Ï•</ci></apply><apply id="S3.E2.m1.6.6.3.2.1.1.cmml" xref="S3.E2.m1.6.6.3.2.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.3.2.1.1.1.cmml" xref="S3.E2.m1.6.6.3.2.1.1.1">conditional</csymbol><ci id="S3.E2.m1.6.6.3.2.1.1.2.cmml" xref="S3.E2.m1.6.6.3.2.1.1.2">ğ´</ci><ci id="S3.E2.m1.6.6.3.2.1.1.3.cmml" xref="S3.E2.m1.6.6.3.2.1.1.3">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">q_{\phi}(Q,A|V)=q_{\phi}(Q|V,A)q_{\phi}(A|V)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS1.p1.8" class="ltx_p">Specifically, given a triplet (<math id="S3.SS1.SSS1.p1.2.m1.3" class="ltx_Math" alttext="V,Q,A" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m1.3a"><mrow id="S3.SS1.SSS1.p1.2.m1.3.4.2" xref="S3.SS1.SSS1.p1.2.m1.3.4.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m1.1.1" xref="S3.SS1.SSS1.p1.2.m1.1.1.cmml">V</mi><mo id="S3.SS1.SSS1.p1.2.m1.3.4.2.1" xref="S3.SS1.SSS1.p1.2.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.2.m1.2.2" xref="S3.SS1.SSS1.p1.2.m1.2.2.cmml">Q</mi><mo id="S3.SS1.SSS1.p1.2.m1.3.4.2.2" xref="S3.SS1.SSS1.p1.2.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.2.m1.3.3" xref="S3.SS1.SSS1.p1.2.m1.3.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m1.3b"><list id="S3.SS1.SSS1.p1.2.m1.3.4.1.cmml" xref="S3.SS1.SSS1.p1.2.m1.3.4.2"><ci id="S3.SS1.SSS1.p1.2.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m1.1.1">ğ‘‰</ci><ci id="S3.SS1.SSS1.p1.2.m1.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m1.2.2">ğ‘„</ci><ci id="S3.SS1.SSS1.p1.2.m1.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m1.3.3">ğ´</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m1.3c">V,Q,A</annotation></semantics></math>), we build an answer generator <math id="S3.SS1.SSS1.p1.3.m2.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m2.1a"><mrow id="S3.SS1.SSS1.p1.3.m2.1.1" xref="S3.SS1.SSS1.p1.3.m2.1.1.cmml"><msub id="S3.SS1.SSS1.p1.3.m2.1.1.3" xref="S3.SS1.SSS1.p1.3.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.3.m2.1.1.3.2" xref="S3.SS1.SSS1.p1.3.m2.1.1.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.p1.3.m2.1.1.3.3" xref="S3.SS1.SSS1.p1.3.m2.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.3.m2.1.1.2" xref="S3.SS1.SSS1.p1.3.m2.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.p1.3.m2.1.1.1.1" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.2" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.2" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.1" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.3" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.3" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m2.1b"><apply id="S3.SS1.SSS1.p1.3.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1"><times id="S3.SS1.SSS1.p1.3.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.2"></times><apply id="S3.SS1.SSS1.p1.3.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.p1.3.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.2">ğ´</ci><ci id="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m2.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m2.1c">q_{\phi}(A|V)</annotation></semantics></math> and a conditional question generator <math id="S3.SS1.SSS1.p1.4.m3.3" class="ltx_Math" alttext="q_{\phi}(Q|V,A)" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m3.3a"><mrow id="S3.SS1.SSS1.p1.4.m3.3.3" xref="S3.SS1.SSS1.p1.4.m3.3.3.cmml"><msub id="S3.SS1.SSS1.p1.4.m3.3.3.3" xref="S3.SS1.SSS1.p1.4.m3.3.3.3.cmml"><mi id="S3.SS1.SSS1.p1.4.m3.3.3.3.2" xref="S3.SS1.SSS1.p1.4.m3.3.3.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.p1.4.m3.3.3.3.3" xref="S3.SS1.SSS1.p1.4.m3.3.3.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.4.m3.3.3.2" xref="S3.SS1.SSS1.p1.4.m3.3.3.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.p1.4.m3.3.3.1.1" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.2" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.2" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.1" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.2" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.1.cmml"><mi id="S3.SS1.SSS1.p1.4.m3.1.1" xref="S3.SS1.SSS1.p1.4.m3.1.1.cmml">V</mi><mo id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.2.1" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.4.m3.2.2" xref="S3.SS1.SSS1.p1.4.m3.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.3" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m3.3b"><apply id="S3.SS1.SSS1.p1.4.m3.3.3.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3"><times id="S3.SS1.SSS1.p1.4.m3.3.3.2.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.2"></times><apply id="S3.SS1.SSS1.p1.4.m3.3.3.3.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.4.m3.3.3.3.1.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p1.4.m3.3.3.3.2.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.p1.4.m3.3.3.3.3.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.2">ğ‘„</ci><list id="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.4.m3.3.3.1.1.1.3.2"><ci id="S3.SS1.SSS1.p1.4.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m3.1.1">ğ‘‰</ci><ci id="S3.SS1.SSS1.p1.4.m3.2.2.cmml" xref="S3.SS1.SSS1.p1.4.m3.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m3.3c">q_{\phi}(Q|V,A)</annotation></semantics></math>.
The original image <math id="S3.SS1.SSS1.p1.5.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m4.1a"><mi id="S3.SS1.SSS1.p1.5.m4.1.1" xref="S3.SS1.SSS1.p1.5.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m4.1b"><ci id="S3.SS1.SSS1.p1.5.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m4.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m4.1c">V</annotation></semantics></math> and the generated <math id="S3.SS1.SSS1.p1.6.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.SSS1.p1.6.m5.1a"><mi id="S3.SS1.SSS1.p1.6.m5.1.1" xref="S3.SS1.SSS1.p1.6.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.6.m5.1b"><ci id="S3.SS1.SSS1.p1.6.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m5.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m5.1c">Q</annotation></semantics></math> are then combined to obtain an answer prediction <math id="S3.SS1.SSS1.p1.7.m6.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.SSS1.p1.7.m6.1a"><mi id="S3.SS1.SSS1.p1.7.m6.1.1" xref="S3.SS1.SSS1.p1.7.m6.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.7.m6.1b"><ci id="S3.SS1.SSS1.p1.7.m6.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m6.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.7.m6.1c">A</annotation></semantics></math> using the conventional VQA model <math id="S3.SS1.SSS1.p1.8.m7.3" class="ltx_Math" alttext="p_{\psi}(A|V,Q)" display="inline"><semantics id="S3.SS1.SSS1.p1.8.m7.3a"><mrow id="S3.SS1.SSS1.p1.8.m7.3.3" xref="S3.SS1.SSS1.p1.8.m7.3.3.cmml"><msub id="S3.SS1.SSS1.p1.8.m7.3.3.3" xref="S3.SS1.SSS1.p1.8.m7.3.3.3.cmml"><mi id="S3.SS1.SSS1.p1.8.m7.3.3.3.2" xref="S3.SS1.SSS1.p1.8.m7.3.3.3.2.cmml">p</mi><mi id="S3.SS1.SSS1.p1.8.m7.3.3.3.3" xref="S3.SS1.SSS1.p1.8.m7.3.3.3.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.8.m7.3.3.2" xref="S3.SS1.SSS1.p1.8.m7.3.3.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.p1.8.m7.3.3.1.1" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.2" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.2" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.1" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.2" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.1.cmml"><mi id="S3.SS1.SSS1.p1.8.m7.1.1" xref="S3.SS1.SSS1.p1.8.m7.1.1.cmml">V</mi><mo id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.2.1" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.8.m7.2.2" xref="S3.SS1.SSS1.p1.8.m7.2.2.cmml">Q</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.3" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.8.m7.3b"><apply id="S3.SS1.SSS1.p1.8.m7.3.3.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3"><times id="S3.SS1.SSS1.p1.8.m7.3.3.2.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.2"></times><apply id="S3.SS1.SSS1.p1.8.m7.3.3.3.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.8.m7.3.3.3.1.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p1.8.m7.3.3.3.2.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.p1.8.m7.3.3.3.3.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.3.3">ğœ“</ci></apply><apply id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.2">ğ´</ci><list id="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.8.m7.3.3.1.1.1.3.2"><ci id="S3.SS1.SSS1.p1.8.m7.1.1.cmml" xref="S3.SS1.SSS1.p1.8.m7.1.1">ğ‘‰</ci><ci id="S3.SS1.SSS1.p1.8.m7.2.2.cmml" xref="S3.SS1.SSS1.p1.8.m7.2.2">ğ‘„</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.8.m7.3c">p_{\psi}(A|V,Q)</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">The design of our generative components is based on two hypotheses.
First, a model that can predict possible answer candidates directly from a given image has better understanding of the visual content and the dependency between the image and the answers.
Second, assuming the predicted answers have high correlation with the image, the conditional language model can generate valid questions considering both the image and the predicted answers.</p>
</div>
<section id="S3.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Answer Generator - <math id="S3.SS1.SSS1.Px1.1.m1.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S3.SS1.SSS1.Px1.1.m1.1b"><mrow id="S3.SS1.SSS1.Px1.1.m1.1.1" xref="S3.SS1.SSS1.Px1.1.m1.1.1.cmml"><msub id="S3.SS1.SSS1.Px1.1.m1.1.1.3" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.Px1.1.m1.1.1.3.2" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.Px1.1.m1.1.1.3.3" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.Px1.1.m1.1.1.2" xref="S3.SS1.SSS1.Px1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px1.1.m1.1c"><apply id="S3.SS1.SSS1.Px1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1"><times id="S3.SS1.SSS1.Px1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.2"></times><apply id="S3.SS1.SSS1.Px1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.Px1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.Px1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.2">ğ´</ci><ci id="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px1.1.m1.1d">q_{\phi}(A|V)</annotation></semantics></math>
</h5>

<div id="S3.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px1.p1.1" class="ltx_p">As illustrated in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> step (1), first we need to build an image classification model to predict possible answers to a given image.
Following typical setups in VQA models, we encode the input images as regional visual feature representations.
Specifically, these features are extracted from a bottom-up approach <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, where the input image is passed through a ResNet CNN within the Faster R-CNN framework to obtain a vector representation.
We take pretrained visual features as a preprocessing step for efficiency, and keep them fixed during the training of the VQA model.
We follow the same setup for both labelled and unlabelled images in our experiments.</p>
</div>
<div id="S3.SS1.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.Px1.p2.1" class="ltx_p">In our generative framework, the predicted answer distributions given images can be modelled as <math id="S3.SS1.SSS1.Px1.p2.1.m1.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S3.SS1.SSS1.Px1.p2.1.m1.1a"><mrow id="S3.SS1.SSS1.Px1.p2.1.m1.1.1" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.cmml"><msub id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1"><times id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.2"></times><apply id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.2">ğ´</ci><ci id="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p2.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px1.p2.1.m1.1c">q_{\phi}(A|V)</annotation></semantics></math>.
We build the classifier by using a non-linear fully connected layer and batch normalisation on the pretrained image features, followed by a projection to the space of all possible answers and a softmax layer.
This can be seen as a multi-label image classification task where the labels are possible VQA answers corresponding to the given image instead of image classification categories.
Without seeing the questions with strong language priors, the model can directly learn the mapping from the image to answer candidates.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Question Generator - <math id="S3.SS1.SSS1.Px2.1.m1.3" class="ltx_Math" alttext="q_{\phi}(Q|V,A)" display="inline"><semantics id="S3.SS1.SSS1.Px2.1.m1.3b"><mrow id="S3.SS1.SSS1.Px2.1.m1.3.3" xref="S3.SS1.SSS1.Px2.1.m1.3.3.cmml"><msub id="S3.SS1.SSS1.Px2.1.m1.3.3.3" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3.cmml"><mi id="S3.SS1.SSS1.Px2.1.m1.3.3.3.2" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.Px2.1.m1.3.3.3.3" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.Px2.1.m1.3.3.2" xref="S3.SS1.SSS1.Px2.1.m1.3.3.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.2" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.2" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.1" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.2" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.SS1.SSS1.Px2.1.m1.1.1" xref="S3.SS1.SSS1.Px2.1.m1.1.1.cmml">V</mi><mo id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.2.1" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.Px2.1.m1.2.2" xref="S3.SS1.SSS1.Px2.1.m1.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.3" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px2.1.m1.3c"><apply id="S3.SS1.SSS1.Px2.1.m1.3.3.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3"><times id="S3.SS1.SSS1.Px2.1.m1.3.3.2.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.2"></times><apply id="S3.SS1.SSS1.Px2.1.m1.3.3.3.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px2.1.m1.3.3.3.1.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.Px2.1.m1.3.3.3.2.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.Px2.1.m1.3.3.3.3.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.2">ğ‘„</ci><list id="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.Px2.1.m1.3.3.1.1.1.3.2"><ci id="S3.SS1.SSS1.Px2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px2.1.m1.1.1">ğ‘‰</ci><ci id="S3.SS1.SSS1.Px2.1.m1.2.2.cmml" xref="S3.SS1.SSS1.Px2.1.m1.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px2.1.m1.3d">q_{\phi}(Q|V,A)</annotation></semantics></math>
</h5>

<div id="S3.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p1.1" class="ltx_p">The second stage of our generative process is to generate questions conditioned on the given image and sampled answers, as shown in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> step (2).
The goal of our question generator is to define a conditional language model <math id="S3.SS1.SSS1.Px2.p1.1.m1.3" class="ltx_Math" alttext="q_{\phi}(Q|V,A)" display="inline"><semantics id="S3.SS1.SSS1.Px2.p1.1.m1.3a"><mrow id="S3.SS1.SSS1.Px2.p1.1.m1.3.3" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.cmml"><msub id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.cmml"><mi id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.3" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.SS1.SSS1.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.1.1.cmml">V</mi><mo id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.2.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.Px2.p1.1.m1.2.2" xref="S3.SS1.SSS1.Px2.p1.1.m1.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.3" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px2.p1.1.m1.3b"><apply id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3"><times id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.2.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.2"></times><apply id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.2.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.3.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.2">ğ‘„</ci><list id="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.3.3.1.1.1.3.2"><ci id="S3.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.1.1">ğ‘‰</ci><ci id="S3.SS1.SSS1.Px2.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px2.p1.1.m1.3c">q_{\phi}(Q|V,A)</annotation></semantics></math> to learn the transformations from one modality (<span id="S3.SS1.SSS1.Px2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>image) to another (<span id="S3.SS1.SSS1.Px2.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span>question) with the help from possible answers.
This is similar to a visual question generation (VQG) task that is aimed at generating not only relevant but diverse questions to each image.
However, typical VQG models only take images as input, and thus are not goal-driven and do not guarantee that the generated question corresponds to a specific type of answer.
We follow the common setup in <cite class="ltx_cite ltx_citemacro_citep">(Shah etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Krishna etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> to encode the answer along with the image before generating the question. Such an approach allows the model to condition its question on the answer.</p>
</div>
<div id="S3.SS1.SSS1.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p2.1" class="ltx_p">We model the question generator by an image encoder, answer encoder, and an LSTM decoder.
With predicted answer distributions, the model can generate various plausible questions for the sampled answers.
The image encoder transforms the attended image features to lower dimensional feature vectors, and the answer encoder takes the distribution over the answer space as input and outputs the vector representation of the answer.
The image and answer representation are then fused together and passed through the LSTM decoder to generate a question, a process that is optimised by minimising the negative log likelihood with teacher-forcing.</p>
</div>
</section>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Cross-Modal Distribution</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">We need to model the cross-modal distributions on the existing VQA dataset, where all the modalities are observed.
The cross-modal distribution can be decomposed into:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.3" class="ltx_Math" alttext="\displaystyle p_{\theta}(V,Q,A)=" display="inline"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.4" xref="S3.Ex1.m1.3.4.cmml"><mrow id="S3.Ex1.m1.3.4.2" xref="S3.Ex1.m1.3.4.2.cmml"><msub id="S3.Ex1.m1.3.4.2.2" xref="S3.Ex1.m1.3.4.2.2.cmml"><mi id="S3.Ex1.m1.3.4.2.2.2" xref="S3.Ex1.m1.3.4.2.2.2.cmml">p</mi><mi id="S3.Ex1.m1.3.4.2.2.3" xref="S3.Ex1.m1.3.4.2.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.4.2.1" xref="S3.Ex1.m1.3.4.2.1.cmml">â€‹</mo><mrow id="S3.Ex1.m1.3.4.2.3.2" xref="S3.Ex1.m1.3.4.2.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.4.2.3.2.1" xref="S3.Ex1.m1.3.4.2.3.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">V</mi><mo id="S3.Ex1.m1.3.4.2.3.2.2" xref="S3.Ex1.m1.3.4.2.3.1.cmml">,</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">Q</mi><mo id="S3.Ex1.m1.3.4.2.3.2.3" xref="S3.Ex1.m1.3.4.2.3.1.cmml">,</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">A</mi><mo stretchy="false" id="S3.Ex1.m1.3.4.2.3.2.4" xref="S3.Ex1.m1.3.4.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.3.4.1" xref="S3.Ex1.m1.3.4.1.cmml">=</mo><mi id="S3.Ex1.m1.3.4.3" xref="S3.Ex1.m1.3.4.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.4.cmml" xref="S3.Ex1.m1.3.4"><eq id="S3.Ex1.m1.3.4.1.cmml" xref="S3.Ex1.m1.3.4.1"></eq><apply id="S3.Ex1.m1.3.4.2.cmml" xref="S3.Ex1.m1.3.4.2"><times id="S3.Ex1.m1.3.4.2.1.cmml" xref="S3.Ex1.m1.3.4.2.1"></times><apply id="S3.Ex1.m1.3.4.2.2.cmml" xref="S3.Ex1.m1.3.4.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.4.2.2.1.cmml" xref="S3.Ex1.m1.3.4.2.2">subscript</csymbol><ci id="S3.Ex1.m1.3.4.2.2.2.cmml" xref="S3.Ex1.m1.3.4.2.2.2">ğ‘</ci><ci id="S3.Ex1.m1.3.4.2.2.3.cmml" xref="S3.Ex1.m1.3.4.2.2.3">ğœƒ</ci></apply><vector id="S3.Ex1.m1.3.4.2.3.1.cmml" xref="S3.Ex1.m1.3.4.2.3.2"><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">ğ‘‰</ci><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">ğ‘„</ci><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">ğ´</ci></vector></apply><csymbol cd="latexml" id="S3.Ex1.m1.3.4.3.cmml" xref="S3.Ex1.m1.3.4.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\displaystyle p_{\theta}(V,Q,A)=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.5" class="ltx_Math" alttext="\displaystyle p_{\theta}(V|Q,A)p_{\theta}(Q,A)" display="inline"><semantics id="S3.Ex1.m2.5a"><mrow id="S3.Ex1.m2.5.5" xref="S3.Ex1.m2.5.5.cmml"><msub id="S3.Ex1.m2.5.5.3" xref="S3.Ex1.m2.5.5.3.cmml"><mi id="S3.Ex1.m2.5.5.3.2" xref="S3.Ex1.m2.5.5.3.2.cmml">p</mi><mi id="S3.Ex1.m2.5.5.3.3" xref="S3.Ex1.m2.5.5.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.5.5.2" xref="S3.Ex1.m2.5.5.2.cmml">â€‹</mo><mrow id="S3.Ex1.m2.5.5.1.1" xref="S3.Ex1.m2.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.5.5.1.1.2" xref="S3.Ex1.m2.5.5.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.5.5.1.1.1" xref="S3.Ex1.m2.5.5.1.1.1.cmml"><mi id="S3.Ex1.m2.5.5.1.1.1.2" xref="S3.Ex1.m2.5.5.1.1.1.2.cmml">V</mi><mo fence="false" id="S3.Ex1.m2.5.5.1.1.1.1" xref="S3.Ex1.m2.5.5.1.1.1.1.cmml">|</mo><mrow id="S3.Ex1.m2.5.5.1.1.1.3.2" xref="S3.Ex1.m2.5.5.1.1.1.3.1.cmml"><mi id="S3.Ex1.m2.1.1" xref="S3.Ex1.m2.1.1.cmml">Q</mi><mo id="S3.Ex1.m2.5.5.1.1.1.3.2.1" xref="S3.Ex1.m2.5.5.1.1.1.3.1.cmml">,</mo><mi id="S3.Ex1.m2.2.2" xref="S3.Ex1.m2.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.Ex1.m2.5.5.1.1.3" xref="S3.Ex1.m2.5.5.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.5.5.2a" xref="S3.Ex1.m2.5.5.2.cmml">â€‹</mo><msub id="S3.Ex1.m2.5.5.4" xref="S3.Ex1.m2.5.5.4.cmml"><mi id="S3.Ex1.m2.5.5.4.2" xref="S3.Ex1.m2.5.5.4.2.cmml">p</mi><mi id="S3.Ex1.m2.5.5.4.3" xref="S3.Ex1.m2.5.5.4.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.5.5.2b" xref="S3.Ex1.m2.5.5.2.cmml">â€‹</mo><mrow id="S3.Ex1.m2.5.5.5.2" xref="S3.Ex1.m2.5.5.5.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.5.5.5.2.1" xref="S3.Ex1.m2.5.5.5.1.cmml">(</mo><mi id="S3.Ex1.m2.3.3" xref="S3.Ex1.m2.3.3.cmml">Q</mi><mo id="S3.Ex1.m2.5.5.5.2.2" xref="S3.Ex1.m2.5.5.5.1.cmml">,</mo><mi id="S3.Ex1.m2.4.4" xref="S3.Ex1.m2.4.4.cmml">A</mi><mo stretchy="false" id="S3.Ex1.m2.5.5.5.2.3" xref="S3.Ex1.m2.5.5.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.5b"><apply id="S3.Ex1.m2.5.5.cmml" xref="S3.Ex1.m2.5.5"><times id="S3.Ex1.m2.5.5.2.cmml" xref="S3.Ex1.m2.5.5.2"></times><apply id="S3.Ex1.m2.5.5.3.cmml" xref="S3.Ex1.m2.5.5.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.5.5.3.1.cmml" xref="S3.Ex1.m2.5.5.3">subscript</csymbol><ci id="S3.Ex1.m2.5.5.3.2.cmml" xref="S3.Ex1.m2.5.5.3.2">ğ‘</ci><ci id="S3.Ex1.m2.5.5.3.3.cmml" xref="S3.Ex1.m2.5.5.3.3">ğœƒ</ci></apply><apply id="S3.Ex1.m2.5.5.1.1.1.cmml" xref="S3.Ex1.m2.5.5.1.1"><csymbol cd="latexml" id="S3.Ex1.m2.5.5.1.1.1.1.cmml" xref="S3.Ex1.m2.5.5.1.1.1.1">conditional</csymbol><ci id="S3.Ex1.m2.5.5.1.1.1.2.cmml" xref="S3.Ex1.m2.5.5.1.1.1.2">ğ‘‰</ci><list id="S3.Ex1.m2.5.5.1.1.1.3.1.cmml" xref="S3.Ex1.m2.5.5.1.1.1.3.2"><ci id="S3.Ex1.m2.1.1.cmml" xref="S3.Ex1.m2.1.1">ğ‘„</ci><ci id="S3.Ex1.m2.2.2.cmml" xref="S3.Ex1.m2.2.2">ğ´</ci></list></apply><apply id="S3.Ex1.m2.5.5.4.cmml" xref="S3.Ex1.m2.5.5.4"><csymbol cd="ambiguous" id="S3.Ex1.m2.5.5.4.1.cmml" xref="S3.Ex1.m2.5.5.4">subscript</csymbol><ci id="S3.Ex1.m2.5.5.4.2.cmml" xref="S3.Ex1.m2.5.5.4.2">ğ‘</ci><ci id="S3.Ex1.m2.5.5.4.3.cmml" xref="S3.Ex1.m2.5.5.4.3">ğœƒ</ci></apply><interval closure="open" id="S3.Ex1.m2.5.5.5.1.cmml" xref="S3.Ex1.m2.5.5.5.2"><ci id="S3.Ex1.m2.3.3.cmml" xref="S3.Ex1.m2.3.3">ğ‘„</ci><ci id="S3.Ex1.m2.4.4.cmml" xref="S3.Ex1.m2.4.4">ğ´</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.5c">\displaystyle p_{\theta}(V|Q,A)p_{\theta}(Q,A)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S3.E3.m1.1a"><mo id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><eq id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.5" class="ltx_Math" alttext="\displaystyle p_{\theta}(V|Q,A)p_{\theta}(Q|A)p_{\theta}(A)" display="inline"><semantics id="S3.E3.m2.5a"><mrow id="S3.E3.m2.5.5" xref="S3.E3.m2.5.5.cmml"><msub id="S3.E3.m2.5.5.4" xref="S3.E3.m2.5.5.4.cmml"><mi id="S3.E3.m2.5.5.4.2" xref="S3.E3.m2.5.5.4.2.cmml">p</mi><mi id="S3.E3.m2.5.5.4.3" xref="S3.E3.m2.5.5.4.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.5.5.3" xref="S3.E3.m2.5.5.3.cmml">â€‹</mo><mrow id="S3.E3.m2.4.4.1.1" xref="S3.E3.m2.4.4.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.4.4.1.1.2" xref="S3.E3.m2.4.4.1.1.1.cmml">(</mo><mrow id="S3.E3.m2.4.4.1.1.1" xref="S3.E3.m2.4.4.1.1.1.cmml"><mi id="S3.E3.m2.4.4.1.1.1.2" xref="S3.E3.m2.4.4.1.1.1.2.cmml">V</mi><mo fence="false" id="S3.E3.m2.4.4.1.1.1.1" xref="S3.E3.m2.4.4.1.1.1.1.cmml">|</mo><mrow id="S3.E3.m2.4.4.1.1.1.3.2" xref="S3.E3.m2.4.4.1.1.1.3.1.cmml"><mi id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml">Q</mi><mo id="S3.E3.m2.4.4.1.1.1.3.2.1" xref="S3.E3.m2.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.E3.m2.2.2" xref="S3.E3.m2.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.E3.m2.4.4.1.1.3" xref="S3.E3.m2.4.4.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m2.5.5.3a" xref="S3.E3.m2.5.5.3.cmml">â€‹</mo><msub id="S3.E3.m2.5.5.5" xref="S3.E3.m2.5.5.5.cmml"><mi id="S3.E3.m2.5.5.5.2" xref="S3.E3.m2.5.5.5.2.cmml">p</mi><mi id="S3.E3.m2.5.5.5.3" xref="S3.E3.m2.5.5.5.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.5.5.3b" xref="S3.E3.m2.5.5.3.cmml">â€‹</mo><mrow id="S3.E3.m2.5.5.2.1" xref="S3.E3.m2.5.5.2.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.5.5.2.1.2" xref="S3.E3.m2.5.5.2.1.1.cmml">(</mo><mrow id="S3.E3.m2.5.5.2.1.1" xref="S3.E3.m2.5.5.2.1.1.cmml"><mi id="S3.E3.m2.5.5.2.1.1.2" xref="S3.E3.m2.5.5.2.1.1.2.cmml">Q</mi><mo fence="false" id="S3.E3.m2.5.5.2.1.1.1" xref="S3.E3.m2.5.5.2.1.1.1.cmml">|</mo><mi id="S3.E3.m2.5.5.2.1.1.3" xref="S3.E3.m2.5.5.2.1.1.3.cmml">A</mi></mrow><mo stretchy="false" id="S3.E3.m2.5.5.2.1.3" xref="S3.E3.m2.5.5.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m2.5.5.3c" xref="S3.E3.m2.5.5.3.cmml">â€‹</mo><msub id="S3.E3.m2.5.5.6" xref="S3.E3.m2.5.5.6.cmml"><mi id="S3.E3.m2.5.5.6.2" xref="S3.E3.m2.5.5.6.2.cmml">p</mi><mi id="S3.E3.m2.5.5.6.3" xref="S3.E3.m2.5.5.6.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.5.5.3d" xref="S3.E3.m2.5.5.3.cmml">â€‹</mo><mrow id="S3.E3.m2.5.5.7.2" xref="S3.E3.m2.5.5.cmml"><mo stretchy="false" id="S3.E3.m2.5.5.7.2.1" xref="S3.E3.m2.5.5.cmml">(</mo><mi id="S3.E3.m2.3.3" xref="S3.E3.m2.3.3.cmml">A</mi><mo stretchy="false" id="S3.E3.m2.5.5.7.2.2" xref="S3.E3.m2.5.5.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.5b"><apply id="S3.E3.m2.5.5.cmml" xref="S3.E3.m2.5.5"><times id="S3.E3.m2.5.5.3.cmml" xref="S3.E3.m2.5.5.3"></times><apply id="S3.E3.m2.5.5.4.cmml" xref="S3.E3.m2.5.5.4"><csymbol cd="ambiguous" id="S3.E3.m2.5.5.4.1.cmml" xref="S3.E3.m2.5.5.4">subscript</csymbol><ci id="S3.E3.m2.5.5.4.2.cmml" xref="S3.E3.m2.5.5.4.2">ğ‘</ci><ci id="S3.E3.m2.5.5.4.3.cmml" xref="S3.E3.m2.5.5.4.3">ğœƒ</ci></apply><apply id="S3.E3.m2.4.4.1.1.1.cmml" xref="S3.E3.m2.4.4.1.1"><csymbol cd="latexml" id="S3.E3.m2.4.4.1.1.1.1.cmml" xref="S3.E3.m2.4.4.1.1.1.1">conditional</csymbol><ci id="S3.E3.m2.4.4.1.1.1.2.cmml" xref="S3.E3.m2.4.4.1.1.1.2">ğ‘‰</ci><list id="S3.E3.m2.4.4.1.1.1.3.1.cmml" xref="S3.E3.m2.4.4.1.1.1.3.2"><ci id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1">ğ‘„</ci><ci id="S3.E3.m2.2.2.cmml" xref="S3.E3.m2.2.2">ğ´</ci></list></apply><apply id="S3.E3.m2.5.5.5.cmml" xref="S3.E3.m2.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m2.5.5.5.1.cmml" xref="S3.E3.m2.5.5.5">subscript</csymbol><ci id="S3.E3.m2.5.5.5.2.cmml" xref="S3.E3.m2.5.5.5.2">ğ‘</ci><ci id="S3.E3.m2.5.5.5.3.cmml" xref="S3.E3.m2.5.5.5.3">ğœƒ</ci></apply><apply id="S3.E3.m2.5.5.2.1.1.cmml" xref="S3.E3.m2.5.5.2.1"><csymbol cd="latexml" id="S3.E3.m2.5.5.2.1.1.1.cmml" xref="S3.E3.m2.5.5.2.1.1.1">conditional</csymbol><ci id="S3.E3.m2.5.5.2.1.1.2.cmml" xref="S3.E3.m2.5.5.2.1.1.2">ğ‘„</ci><ci id="S3.E3.m2.5.5.2.1.1.3.cmml" xref="S3.E3.m2.5.5.2.1.1.3">ğ´</ci></apply><apply id="S3.E3.m2.5.5.6.cmml" xref="S3.E3.m2.5.5.6"><csymbol cd="ambiguous" id="S3.E3.m2.5.5.6.1.cmml" xref="S3.E3.m2.5.5.6">subscript</csymbol><ci id="S3.E3.m2.5.5.6.2.cmml" xref="S3.E3.m2.5.5.6.2">ğ‘</ci><ci id="S3.E3.m2.5.5.6.3.cmml" xref="S3.E3.m2.5.5.6.3">ğœƒ</ci></apply><ci id="S3.E3.m2.3.3.cmml" xref="S3.E3.m2.3.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.5c">\displaystyle p_{\theta}(V|Q,A)p_{\theta}(Q|A)p_{\theta}(A)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.3" class="ltx_p">We first learn to model the joint distribution of QA pairs.
We build the model to learn the prior knowledge of QA pairs <math id="S3.SS1.SSS2.p2.1.m1.2" class="ltx_Math" alttext="p_{\theta}(Q,A)" display="inline"><semantics id="S3.SS1.SSS2.p2.1.m1.2a"><mrow id="S3.SS1.SSS2.p2.1.m1.2.3" xref="S3.SS1.SSS2.p2.1.m1.2.3.cmml"><msub id="S3.SS1.SSS2.p2.1.m1.2.3.2" xref="S3.SS1.SSS2.p2.1.m1.2.3.2.cmml"><mi id="S3.SS1.SSS2.p2.1.m1.2.3.2.2" xref="S3.SS1.SSS2.p2.1.m1.2.3.2.2.cmml">p</mi><mi id="S3.SS1.SSS2.p2.1.m1.2.3.2.3" xref="S3.SS1.SSS2.p2.1.m1.2.3.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.1.m1.2.3.1" xref="S3.SS1.SSS2.p2.1.m1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p2.1.m1.2.3.3.2" xref="S3.SS1.SSS2.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.1.m1.2.3.3.2.1" xref="S3.SS1.SSS2.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">Q</mi><mo id="S3.SS1.SSS2.p2.1.m1.2.3.3.2.2" xref="S3.SS1.SSS2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS2.p2.1.m1.2.2" xref="S3.SS1.SSS2.p2.1.m1.2.2.cmml">A</mi><mo stretchy="false" id="S3.SS1.SSS2.p2.1.m1.2.3.3.2.3" xref="S3.SS1.SSS2.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.2b"><apply id="S3.SS1.SSS2.p2.1.m1.2.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3"><times id="S3.SS1.SSS2.p2.1.m1.2.3.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.1"></times><apply id="S3.SS1.SSS2.p2.1.m1.2.3.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.1.m1.2.3.2.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS2.p2.1.m1.2.3.2.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.2.2">ğ‘</ci><ci id="S3.SS1.SSS2.p2.1.m1.2.3.2.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.2.3">ğœƒ</ci></apply><interval closure="open" id="S3.SS1.SSS2.p2.1.m1.2.3.3.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.3.3.2"><ci id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1">ğ‘„</ci><ci id="S3.SS1.SSS2.p2.1.m1.2.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.2.2">ğ´</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.2c">p_{\theta}(Q,A)</annotation></semantics></math> through <math id="S3.SS1.SSS2.p2.2.m2.2" class="ltx_Math" alttext="p_{\theta}(Q|A)p_{\theta}(A)" display="inline"><semantics id="S3.SS1.SSS2.p2.2.m2.2a"><mrow id="S3.SS1.SSS2.p2.2.m2.2.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.cmml"><msub id="S3.SS1.SSS2.p2.2.m2.2.2.3" xref="S3.SS1.SSS2.p2.2.m2.2.2.3.cmml"><mi id="S3.SS1.SSS2.p2.2.m2.2.2.3.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.3.2.cmml">p</mi><mi id="S3.SS1.SSS2.p2.2.m2.2.2.3.3" xref="S3.SS1.SSS2.p2.2.m2.2.2.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.2.m2.2.2.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p2.2.m2.2.2.1.1" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.cmml">Q</mi><mo fence="false" id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.1" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.3" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.3.cmml">A</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.3" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.2.m2.2.2.2a" xref="S3.SS1.SSS2.p2.2.m2.2.2.2.cmml">â€‹</mo><msub id="S3.SS1.SSS2.p2.2.m2.2.2.4" xref="S3.SS1.SSS2.p2.2.m2.2.2.4.cmml"><mi id="S3.SS1.SSS2.p2.2.m2.2.2.4.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.4.2.cmml">p</mi><mi id="S3.SS1.SSS2.p2.2.m2.2.2.4.3" xref="S3.SS1.SSS2.p2.2.m2.2.2.4.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.2.m2.2.2.2b" xref="S3.SS1.SSS2.p2.2.m2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p2.2.m2.2.2.5.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.2.m2.2.2.5.2.1" xref="S3.SS1.SSS2.p2.2.m2.2.2.cmml">(</mo><mi id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">A</mi><mo stretchy="false" id="S3.SS1.SSS2.p2.2.m2.2.2.5.2.2" xref="S3.SS1.SSS2.p2.2.m2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.2b"><apply id="S3.SS1.SSS2.p2.2.m2.2.2.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2"><times id="S3.SS1.SSS2.p2.2.m2.2.2.2.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.2"></times><apply id="S3.SS1.SSS2.p2.2.m2.2.2.3.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.2.m2.2.2.3.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS2.p2.2.m2.2.2.3.2.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.3.2">ğ‘</ci><ci id="S3.SS1.SSS2.p2.2.m2.2.2.3.3.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.3.3">ğœƒ</ci></apply><apply id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.2">ğ‘„</ci><ci id="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.1.1.1.3">ğ´</ci></apply><apply id="S3.SS1.SSS2.p2.2.m2.2.2.4.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.2.m2.2.2.4.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.4">subscript</csymbol><ci id="S3.SS1.SSS2.p2.2.m2.2.2.4.2.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.4.2">ğ‘</ci><ci id="S3.SS1.SSS2.p2.2.m2.2.2.4.3.cmml" xref="S3.SS1.SSS2.p2.2.m2.2.2.4.3">ğœƒ</ci></apply><ci id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.1.1">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.2c">p_{\theta}(Q|A)p_{\theta}(A)</annotation></semantics></math>.
Firstly, the prior distribution of answer candidates <math id="S3.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="p_{\theta}(A)" display="inline"><semantics id="S3.SS1.SSS2.p2.3.m3.1a"><mrow id="S3.SS1.SSS2.p2.3.m3.1.2" xref="S3.SS1.SSS2.p2.3.m3.1.2.cmml"><msub id="S3.SS1.SSS2.p2.3.m3.1.2.2" xref="S3.SS1.SSS2.p2.3.m3.1.2.2.cmml"><mi id="S3.SS1.SSS2.p2.3.m3.1.2.2.2" xref="S3.SS1.SSS2.p2.3.m3.1.2.2.2.cmml">p</mi><mi id="S3.SS1.SSS2.p2.3.m3.1.2.2.3" xref="S3.SS1.SSS2.p2.3.m3.1.2.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.3.m3.1.2.1" xref="S3.SS1.SSS2.p2.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p2.3.m3.1.2.3.2" xref="S3.SS1.SSS2.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.3.m3.1.2.3.2.1" xref="S3.SS1.SSS2.p2.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.SSS2.p2.3.m3.1.1" xref="S3.SS1.SSS2.p2.3.m3.1.1.cmml">A</mi><mo stretchy="false" id="S3.SS1.SSS2.p2.3.m3.1.2.3.2.2" xref="S3.SS1.SSS2.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.3.m3.1b"><apply id="S3.SS1.SSS2.p2.3.m3.1.2.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2"><times id="S3.SS1.SSS2.p2.3.m3.1.2.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2.1"></times><apply id="S3.SS1.SSS2.p2.3.m3.1.2.2.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.3.m3.1.2.2.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p2.3.m3.1.2.2.2.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2.2.2">ğ‘</ci><ci id="S3.SS1.SSS2.p2.3.m3.1.2.2.3.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.2.2.3">ğœƒ</ci></apply><ci id="S3.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.3.m3.1c">p_{\theta}(A)</annotation></semantics></math> is obtained directly from the dataset.
With a sampled answer from the prior distribution, we build a question generator conditioned on the answer.
Note that instead of obtaining a high quality generative distribution for QA pairs, we learn a joint distribution of the two modalities as the prior knowledge of QA pairs.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">The QA pairs are expected to have high dependency and correlation with the given image.
However, not all the QA pairs are coherent and consistent with the image, which leads to negative noises during training.
Inspired by <cite class="ltx_cite ltx_citemacro_citet">Shah etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, we overcome this issue by modelling the conditional distribution of images given QA pairs.
As shown in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> part (3), we build a neural network to model <math id="S3.SS1.SSS2.p3.1.m1.3" class="ltx_Math" alttext="p_{\theta}(V|Q,A)" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.3a"><mrow id="S3.SS1.SSS2.p3.1.m1.3.3" xref="S3.SS1.SSS2.p3.1.m1.3.3.cmml"><msub id="S3.SS1.SSS2.p3.1.m1.3.3.3" xref="S3.SS1.SSS2.p3.1.m1.3.3.3.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.3.3.3.2" xref="S3.SS1.SSS2.p3.1.m1.3.3.3.2.cmml">p</mi><mi id="S3.SS1.SSS2.p3.1.m1.3.3.3.3" xref="S3.SS1.SSS2.p3.1.m1.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p3.1.m1.3.3.2" xref="S3.SS1.SSS2.p3.1.m1.3.3.2.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p3.1.m1.3.3.1.1" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.2.cmml">V</mi><mo fence="false" id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.2" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml">Q</mi><mo id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.2.1" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.SSS2.p3.1.m1.2.2" xref="S3.SS1.SSS2.p3.1.m1.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.3b"><apply id="S3.SS1.SSS2.p3.1.m1.3.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3"><times id="S3.SS1.SSS2.p3.1.m1.3.3.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.2"></times><apply id="S3.SS1.SSS2.p3.1.m1.3.3.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.3.3.3.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.3.3.3.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS2.p3.1.m1.3.3.3.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.3.3">ğœƒ</ci></apply><apply id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.2">ğ‘‰</ci><list id="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.3.3.1.1.1.3.2"><ci id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1">ğ‘„</ci><ci id="S3.SS1.SSS2.p3.1.m1.2.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.3c">p_{\theta}(V|Q,A)</annotation></semantics></math> and score the correlation/relevance between each QA pair and the image.
We add a layer to fuse question and answer representations together, then multiply the fused QA vector with the image representation through another fusion layer.
Having the representations of both QA pairs and images, we feed them into a binary classifier to output a relevance score of the image and QA pairs after a sigmoid layer.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>VQA Classification</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">Following the conventional setup in VQA, the image feature <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">V</annotation></semantics></math> and question representation <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">Q</annotation></semantics></math> are extracted from the image encoder and the question encoder.
The VQA task is constructed as a classification problem to output the most likely answer <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">A</annotation></semantics></math> from a fixed set of answers based on the content of the image <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">V</annotation></semantics></math> and question <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">Q</annotation></semantics></math>.
Following <cite class="ltx_cite ltx_citemacro_citet">Teney etÂ al. (<a href="#bib.bib48" title="" class="ltx_ref">2018</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>, instead of softmax we use sigmoid outputs in our VQA training to cast it as a multi-label training objective:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.7" class="ltx_Math" alttext="p_{\psi}(A|V,Q)=\rm{CE}\ (f_{\rm{VQA}}(V,Q),A)" display="block"><semantics id="S3.E4.m1.7a"><mrow id="S3.E4.m1.7.7" xref="S3.E4.m1.7.7.cmml"><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.cmml"><msub id="S3.E4.m1.6.6.1.3" xref="S3.E4.m1.6.6.1.3.cmml"><mi id="S3.E4.m1.6.6.1.3.2" xref="S3.E4.m1.6.6.1.3.2.cmml">p</mi><mi id="S3.E4.m1.6.6.1.3.3" xref="S3.E4.m1.6.6.1.3.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.6.6.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.6.6.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.E4.m1.6.6.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.cmml">|</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.3.2" xref="S3.E4.m1.6.6.1.1.1.1.3.1.cmml"><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">V</mi><mo id="S3.E4.m1.6.6.1.1.1.1.3.2.1" xref="S3.E4.m1.6.6.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">Q</mi></mrow></mrow><mo stretchy="false" id="S3.E4.m1.6.6.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.7.7.3" xref="S3.E4.m1.7.7.3.cmml">=</mo><mrow id="S3.E4.m1.7.7.2" xref="S3.E4.m1.7.7.2.cmml"><mi id="S3.E4.m1.7.7.2.3" xref="S3.E4.m1.7.7.2.3.cmml">CE</mi><mo lspace="0.500em" rspace="0em" id="S3.E4.m1.7.7.2.2" xref="S3.E4.m1.7.7.2.2.cmml">â€‹</mo><mrow id="S3.E4.m1.7.7.2.1.1" xref="S3.E4.m1.7.7.2.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.7.7.2.1.1.2" xref="S3.E4.m1.7.7.2.1.2.cmml">(</mo><mrow id="S3.E4.m1.7.7.2.1.1.1" xref="S3.E4.m1.7.7.2.1.1.1.cmml"><msub id="S3.E4.m1.7.7.2.1.1.1.2" xref="S3.E4.m1.7.7.2.1.1.1.2.cmml"><mi mathvariant="normal" id="S3.E4.m1.7.7.2.1.1.1.2.2" xref="S3.E4.m1.7.7.2.1.1.1.2.2.cmml">f</mi><mi id="S3.E4.m1.7.7.2.1.1.1.2.3" xref="S3.E4.m1.7.7.2.1.1.1.2.3.cmml">VQA</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.2.1.1.1.1" xref="S3.E4.m1.7.7.2.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E4.m1.7.7.2.1.1.1.3.2" xref="S3.E4.m1.7.7.2.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E4.m1.7.7.2.1.1.1.3.2.1" xref="S3.E4.m1.7.7.2.1.1.1.3.1.cmml">(</mo><mi mathvariant="normal" id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">V</mi><mo id="S3.E4.m1.7.7.2.1.1.1.3.2.2" xref="S3.E4.m1.7.7.2.1.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">Q</mi><mo stretchy="false" id="S3.E4.m1.7.7.2.1.1.1.3.2.3" xref="S3.E4.m1.7.7.2.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.7.7.2.1.1.3" xref="S3.E4.m1.7.7.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">A</mi><mo stretchy="false" id="S3.E4.m1.7.7.2.1.1.4" xref="S3.E4.m1.7.7.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.7b"><apply id="S3.E4.m1.7.7.cmml" xref="S3.E4.m1.7.7"><eq id="S3.E4.m1.7.7.3.cmml" xref="S3.E4.m1.7.7.3"></eq><apply id="S3.E4.m1.6.6.1.cmml" xref="S3.E4.m1.6.6.1"><times id="S3.E4.m1.6.6.1.2.cmml" xref="S3.E4.m1.6.6.1.2"></times><apply id="S3.E4.m1.6.6.1.3.cmml" xref="S3.E4.m1.6.6.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.3.1.cmml" xref="S3.E4.m1.6.6.1.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.3.2.cmml" xref="S3.E4.m1.6.6.1.3.2">ğ‘</ci><ci id="S3.E4.m1.6.6.1.3.3.cmml" xref="S3.E4.m1.6.6.1.3.3">ğœ“</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.6.6.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1">conditional</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.2">ğ´</ci><list id="S3.E4.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.3.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘‰</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘„</ci></list></apply></apply><apply id="S3.E4.m1.7.7.2.cmml" xref="S3.E4.m1.7.7.2"><times id="S3.E4.m1.7.7.2.2.cmml" xref="S3.E4.m1.7.7.2.2"></times><ci id="S3.E4.m1.7.7.2.3.cmml" xref="S3.E4.m1.7.7.2.3">CE</ci><interval closure="open" id="S3.E4.m1.7.7.2.1.2.cmml" xref="S3.E4.m1.7.7.2.1.1"><apply id="S3.E4.m1.7.7.2.1.1.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1"><times id="S3.E4.m1.7.7.2.1.1.1.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1"></times><apply id="S3.E4.m1.7.7.2.1.1.1.2.cmml" xref="S3.E4.m1.7.7.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.2.1.1.1.2.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.7.7.2.1.1.1.2.2.cmml" xref="S3.E4.m1.7.7.2.1.1.1.2.2">f</ci><ci id="S3.E4.m1.7.7.2.1.1.1.2.3.cmml" xref="S3.E4.m1.7.7.2.1.1.1.2.3">VQA</ci></apply><interval closure="open" id="S3.E4.m1.7.7.2.1.1.1.3.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.3.2"><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">V</ci><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">Q</ci></interval></apply><ci id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">A</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.7c">p_{\psi}(A|V,Q)=\rm{CE}\ (f_{\rm{VQA}}(V,Q),A)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.6" class="ltx_p">where CEÂ represents the cross-entropy loss in VQA models.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.7" class="ltx_Math" alttext="p_{\psi}(A|V,Q)=\rm{CE}\ (f_{\rm{VQA}}(V,Q),A)*\mathcal{R}" display="block"><semantics id="S3.E5.m1.7a"><mrow id="S3.E5.m1.7.7" xref="S3.E5.m1.7.7.cmml"><mrow id="S3.E5.m1.6.6.1" xref="S3.E5.m1.6.6.1.cmml"><msub id="S3.E5.m1.6.6.1.3" xref="S3.E5.m1.6.6.1.3.cmml"><mi id="S3.E5.m1.6.6.1.3.2" xref="S3.E5.m1.6.6.1.3.2.cmml">p</mi><mi id="S3.E5.m1.6.6.1.3.3" xref="S3.E5.m1.6.6.1.3.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.1.2" xref="S3.E5.m1.6.6.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.6.6.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.6.6.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.6.6.1.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.cmml"><mi id="S3.E5.m1.6.6.1.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.E5.m1.6.6.1.1.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.1.cmml">|</mo><mrow id="S3.E5.m1.6.6.1.1.1.1.3.2" xref="S3.E5.m1.6.6.1.1.1.1.3.1.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">V</mi><mo id="S3.E5.m1.6.6.1.1.1.1.3.2.1" xref="S3.E5.m1.6.6.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">Q</mi></mrow></mrow><mo stretchy="false" id="S3.E5.m1.6.6.1.1.1.3" xref="S3.E5.m1.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.7.7.3" xref="S3.E5.m1.7.7.3.cmml">=</mo><mrow id="S3.E5.m1.7.7.2" xref="S3.E5.m1.7.7.2.cmml"><mrow id="S3.E5.m1.7.7.2.1" xref="S3.E5.m1.7.7.2.1.cmml"><mi id="S3.E5.m1.7.7.2.1.3" xref="S3.E5.m1.7.7.2.1.3.cmml">CE</mi><mo lspace="0.500em" rspace="0em" id="S3.E5.m1.7.7.2.1.2" xref="S3.E5.m1.7.7.2.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.7.7.2.1.1.1" xref="S3.E5.m1.7.7.2.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.7.7.2.1.1.1.2" xref="S3.E5.m1.7.7.2.1.1.2.cmml">(</mo><mrow id="S3.E5.m1.7.7.2.1.1.1.1" xref="S3.E5.m1.7.7.2.1.1.1.1.cmml"><msub id="S3.E5.m1.7.7.2.1.1.1.1.2" xref="S3.E5.m1.7.7.2.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S3.E5.m1.7.7.2.1.1.1.1.2.2" xref="S3.E5.m1.7.7.2.1.1.1.1.2.2.cmml">f</mi><mi id="S3.E5.m1.7.7.2.1.1.1.1.2.3" xref="S3.E5.m1.7.7.2.1.1.1.1.2.3.cmml">VQA</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.7.7.2.1.1.1.1.1" xref="S3.E5.m1.7.7.2.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E5.m1.7.7.2.1.1.1.1.3.2" xref="S3.E5.m1.7.7.2.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E5.m1.7.7.2.1.1.1.1.3.2.1" xref="S3.E5.m1.7.7.2.1.1.1.1.3.1.cmml">(</mo><mi mathvariant="normal" id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml">V</mi><mo id="S3.E5.m1.7.7.2.1.1.1.1.3.2.2" xref="S3.E5.m1.7.7.2.1.1.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml">Q</mi><mo stretchy="false" id="S3.E5.m1.7.7.2.1.1.1.1.3.2.3" xref="S3.E5.m1.7.7.2.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.7.7.2.1.1.1.3" xref="S3.E5.m1.7.7.2.1.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.E5.m1.5.5" xref="S3.E5.m1.5.5.cmml">A</mi><mo rspace="0.055em" stretchy="false" id="S3.E5.m1.7.7.2.1.1.1.4" xref="S3.E5.m1.7.7.2.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E5.m1.7.7.2.2" xref="S3.E5.m1.7.7.2.2.cmml">âˆ—</mo><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.7.7.2.3" xref="S3.E5.m1.7.7.2.3.cmml">â„›</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.7b"><apply id="S3.E5.m1.7.7.cmml" xref="S3.E5.m1.7.7"><eq id="S3.E5.m1.7.7.3.cmml" xref="S3.E5.m1.7.7.3"></eq><apply id="S3.E5.m1.6.6.1.cmml" xref="S3.E5.m1.6.6.1"><times id="S3.E5.m1.6.6.1.2.cmml" xref="S3.E5.m1.6.6.1.2"></times><apply id="S3.E5.m1.6.6.1.3.cmml" xref="S3.E5.m1.6.6.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.1.3.1.cmml" xref="S3.E5.m1.6.6.1.3">subscript</csymbol><ci id="S3.E5.m1.6.6.1.3.2.cmml" xref="S3.E5.m1.6.6.1.3.2">ğ‘</ci><ci id="S3.E5.m1.6.6.1.3.3.cmml" xref="S3.E5.m1.6.6.1.3.3">ğœ“</ci></apply><apply id="S3.E5.m1.6.6.1.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.6.6.1.1.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1">conditional</csymbol><ci id="S3.E5.m1.6.6.1.1.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.1.1.2">ğ´</ci><list id="S3.E5.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1.3.2"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ‘‰</ci><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">ğ‘„</ci></list></apply></apply><apply id="S3.E5.m1.7.7.2.cmml" xref="S3.E5.m1.7.7.2"><times id="S3.E5.m1.7.7.2.2.cmml" xref="S3.E5.m1.7.7.2.2"></times><apply id="S3.E5.m1.7.7.2.1.cmml" xref="S3.E5.m1.7.7.2.1"><times id="S3.E5.m1.7.7.2.1.2.cmml" xref="S3.E5.m1.7.7.2.1.2"></times><ci id="S3.E5.m1.7.7.2.1.3.cmml" xref="S3.E5.m1.7.7.2.1.3">CE</ci><interval closure="open" id="S3.E5.m1.7.7.2.1.1.2.cmml" xref="S3.E5.m1.7.7.2.1.1.1"><apply id="S3.E5.m1.7.7.2.1.1.1.1.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1"><times id="S3.E5.m1.7.7.2.1.1.1.1.1.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.1"></times><apply id="S3.E5.m1.7.7.2.1.1.1.1.2.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.7.7.2.1.1.1.1.2.1.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.7.7.2.1.1.1.1.2.2.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.2.2">f</ci><ci id="S3.E5.m1.7.7.2.1.1.1.1.2.3.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.2.3">VQA</ci></apply><interval closure="open" id="S3.E5.m1.7.7.2.1.1.1.1.3.1.cmml" xref="S3.E5.m1.7.7.2.1.1.1.1.3.2"><ci id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3">V</ci><ci id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4">Q</ci></interval></apply><ci id="S3.E5.m1.5.5.cmml" xref="S3.E5.m1.5.5">A</ci></interval></apply><ci id="S3.E5.m1.7.7.2.3.cmml" xref="S3.E5.m1.7.7.2.3">â„›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.7c">p_{\psi}(A|V,Q)=\rm{CE}\ (f_{\rm{VQA}}(V,Q),A)*\mathcal{R}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">One of the assumptions of our proposed generative training scheme is that the generated QA pairs are always semantically and syntactically correct and have high correlation with the image.
However, this is not always the case.
In order to overcome this issue, we propose a reweighing mechanism based on a reliability score obtained from our generative objective, as in Equation <a href="#S3.E5" title="In 3.2 VQA Classification â€£ 3 Model â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We define the reliability and confidence of each generated QA pair as <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{R}</annotation></semantics></math> using the generative objective in Equation <a href="#S3.E1" title="In 3 Model â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and use it to reweigh the cross-entropy loss during VQA training.
We add this reliability score to make the model aware of the confidence and quality of the augmented samples.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We train and evaluate our models on the VQA-CP-v2 <cite class="ltx_cite ltx_citemacro_citep">(Agrawal etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> and VQA-v2 <cite class="ltx_cite ltx_citemacro_citep">(Antol etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> datasets.
These are the most commonly used VQA benchmarks, where VQA-CP-v2 is created from VQA-v2 to evaluate robustness and generalizability of VQA models, by reorganising the train and validation splits.
We also report the results of our model on the validation split of VQA-v2, which contains a strong language prior. VQA-CP-v2 contains 121K images, 438K questions, and 4.4M answers for training and 98K images, 220K questions and 2.2M answers for testing.
We evaluate our model on the validation split of VQA-v2, which contains 83K images, 444K questions, and 4.4M answers for training and 41K images, 210K questions and 2.1M answers for validation.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The unlabelled images come from the Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> and MS COCO 2017-unlabelled <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2014</a>)</cite> datasets.
We directly use the images from these datasets and generate QA pairs relating to the images using proposed generator.
The number of additional images is 101K from VG and 120K from MS COCO 2017-unlabelled data, which are combined to provide 221K unlabelled images in total.
Note that we do not filter out VG images that overlap with original VQA dataset as our generator can generate new QA pairs relating to these images.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Benchmark Models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Most prior works on VQA, such as the ones we compared our model against â€“ GVQA <cite class="ltx_cite ltx_citemacro_citep">(Agrawal etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, RUBI <cite class="ltx_cite ltx_citemacro_citep">(Cadene etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, SCR <cite class="ltx_cite ltx_citemacro_citep">(Wu and Mooney, <a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>, LMH <cite class="ltx_cite ltx_citemacro_citep">(Clark etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, CSS <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite> â€“ are all built based on UpDn <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>.
Therefore, we also build our generative model using UpDn <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> as its backbone and investigate the efficacy of the extension under the generative paradigm.
UpDn incorporates bottom-up attention in VQA by extracting features associated with image regions proposed by Faster RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite> trained on Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>.
We use the evaluation code from official VQA challenge <cite class="ltx_cite ltx_citemacro_citep">(Antol etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">Following previous work, we take the pretrained image features extracted from a ResNet CNN within a Faster R-CNN framework.
Each image is transformed into a <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="Kx2048" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1a" xref="S4.SS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S4.SS3.p1.1.m1.1.1.4" xref="S4.SS3.p1.1.m1.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">ğ¾</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">ğ‘¥</ci><cn type="integer" id="S4.SS3.p1.1.m1.1.1.4.cmml" xref="S4.SS3.p1.1.m1.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">Kx2048</annotation></semantics></math> dimensional vector representation where <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="K=36" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">K</mi><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">36</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><eq id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></eq><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">ğ¾</ci><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">K=36</annotation></semantics></math> in our setup represents a set of objects in the image.
Each question is trimmed into a sequence with a maximum length of 14 words and initialised using the 300-dimensional word embeddings from GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2014</a>)</cite>.
A sentence-level representation is obtained by feeding the word embeddings into a GRU <cite class="ltx_cite ltx_citemacro_citep">(Chung etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite> with 1280 dimensions.
While using sampled answers to guide question generation, we add an answer encoder to transform the samples answer to a representation of dimension 512.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We pre-train the model with the proposed generative objective for 10 epochs (<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\sim</annotation></semantics></math>4 hours) and fix it for generating QA pairs to assist downstream VQA training.
This number of epochs is chosen because our generative model converges and the quality of generated questions does not improve after these epochs of training.
The number of parameters is 90M and the model is trained on 2 RTX 2080Ti.
We use batch size of 256 and adapt the Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> with the initial learning rate of 0.001.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section we present our experimental results including quantitative and qualitative analysis.
We first compare the results with previous approaches including strong baseline models and SOTA benchmarks;
then we validate the efficacy of each component proposed in our framework in ablation studies;
finally we provide a qualitative analysis of the generated QA pairs.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:168.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.7pt,32.9pt) scale(0.718477677455563,0.718477677455563) ;">
<table id="S5.T1.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S5.T1.2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<td id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<span id="S5.T1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">VQA-CP-v2 test (%) </span><math id="S5.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.1.1.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S5.T1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T1.2.2.2.2.4" class="ltx_td ltx_border_tt"></td>
<td id="S5.T1.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<span id="S5.T1.2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">VQA-v2 test (%) </span><math id="S5.T1.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.2.2.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S5.T1.2.2.2.2.2.m1.1.1" xref="S5.T1.2.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T1.2.2.2.2.5" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S5.T1.2.2.2.3.1" class="ltx_tr">
<td id="S5.T1.2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S5.T1.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.2.1" class="ltx_text" style="font-size:90%;">Yes/No</span></td>
<td id="S5.T1.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.3.1" class="ltx_text" style="font-size:90%;">Num</span></td>
<td id="S5.T1.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.4.1" class="ltx_text" style="font-size:90%;">Other</span></td>
<td id="S5.T1.2.2.2.3.1.5" class="ltx_td"></td>
<td id="S5.T1.2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.6.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S5.T1.2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.7.1" class="ltx_text" style="font-size:90%;">Yes/No</span></td>
<td id="S5.T1.2.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.8.1" class="ltx_text" style="font-size:90%;">Num</span></td>
<td id="S5.T1.2.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.3.1.9.1" class="ltx_text" style="font-size:90%;">Other</span></td>
<td id="S5.T1.2.2.2.3.1.10" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.4.2" class="ltx_tr">
<th id="S5.T1.2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">
<span id="S5.T1.2.2.2.4.2.1.1" class="ltx_text" style="font-size:90%;">GVQAÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.4.2.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Agrawal etÂ al.<span id="S5.T1.2.2.2.4.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib2" title="" class="ltx_ref">2018</a><span id="S5.T1.2.2.2.4.2.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.2.1" class="ltx_text" style="font-size:90%;">31.30</span></td>
<td id="S5.T1.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.3.1" class="ltx_text" style="font-size:90%;">57.99</span></td>
<td id="S5.T1.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.4.1" class="ltx_text" style="font-size:90%;">13.68</span></td>
<td id="S5.T1.2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.5.1" class="ltx_text" style="font-size:90%;">22.14</span></td>
<td id="S5.T1.2.2.2.4.2.6" class="ltx_td ltx_border_tt"></td>
<td id="S5.T1.2.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.7.1" class="ltx_text" style="font-size:90%;">48.24</span></td>
<td id="S5.T1.2.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.8.1" class="ltx_text" style="font-size:90%;">72.03</span></td>
<td id="S5.T1.2.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.9.1" class="ltx_text" style="font-size:90%;">31.17</span></td>
<td id="S5.T1.2.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.4.2.10.1" class="ltx_text" style="font-size:90%;">34.65</span></td>
<td id="S5.T1.2.2.2.4.2.11" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S5.T1.2.2.2.5.3" class="ltx_tr">
<th id="S5.T1.2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.5.3.1.1" class="ltx_text" style="font-size:90%;">RUBiÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.5.3.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Cadene etÂ al.<span id="S5.T1.2.2.2.5.3.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib7" title="" class="ltx_ref">2019</a><span id="S5.T1.2.2.2.5.3.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.5.3.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.2.1" class="ltx_text" style="font-size:90%;">47.11</span></td>
<td id="S5.T1.2.2.2.5.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.3.1" class="ltx_text" style="font-size:90%;">68.65</span></td>
<td id="S5.T1.2.2.2.5.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.4.1" class="ltx_text" style="font-size:90%;">20.28</span></td>
<td id="S5.T1.2.2.2.5.3.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.5.1" class="ltx_text" style="font-size:90%;">43.18</span></td>
<td id="S5.T1.2.2.2.5.3.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.5.3.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.7.1" class="ltx_text" style="font-size:90%;">63.10</span></td>
<td id="S5.T1.2.2.2.5.3.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.5.3.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.9.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.5.3.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.5.3.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.5.3.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.6.4" class="ltx_tr">
<th id="S5.T1.2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.6.4.1.1" class="ltx_text" style="font-size:90%;">SCRÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.6.4.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Wu and Mooney<span id="S5.T1.2.2.2.6.4.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib55" title="" class="ltx_ref">2019</a><span id="S5.T1.2.2.2.6.4.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.6.4.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.2.1" class="ltx_text" style="font-size:90%;">48.47</span></td>
<td id="S5.T1.2.2.2.6.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.3.1" class="ltx_text" style="font-size:90%;">70.41</span></td>
<td id="S5.T1.2.2.2.6.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.4.1" class="ltx_text" style="font-size:90%;">10.42</span></td>
<td id="S5.T1.2.2.2.6.4.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.5.1" class="ltx_text" style="font-size:90%;">47.29</span></td>
<td id="S5.T1.2.2.2.6.4.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.6.4.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.7.1" class="ltx_text" style="font-size:90%;">62.30</span></td>
<td id="S5.T1.2.2.2.6.4.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.8.1" class="ltx_text" style="font-size:90%;">77.40</span></td>
<td id="S5.T1.2.2.2.6.4.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.9.1" class="ltx_text" style="font-size:90%;">40.90</span></td>
<td id="S5.T1.2.2.2.6.4.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.6.4.10.1" class="ltx_text" style="font-size:90%;">56.50</span></td>
<td id="S5.T1.2.2.2.6.4.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.7.5" class="ltx_tr">
<th id="S5.T1.2.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.7.5.1.1" class="ltx_text" style="font-size:90%;">LMHÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.7.5.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Clark etÂ al.<span id="S5.T1.2.2.2.7.5.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2019</a><span id="S5.T1.2.2.2.7.5.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.7.5.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.2.1" class="ltx_text" style="font-size:90%;">52.45</span></td>
<td id="S5.T1.2.2.2.7.5.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.3.1" class="ltx_text" style="font-size:90%;">69.81</span></td>
<td id="S5.T1.2.2.2.7.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.4.1" class="ltx_text" style="font-size:90%;">44.46</span></td>
<td id="S5.T1.2.2.2.7.5.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.5.1" class="ltx_text" style="font-size:90%;">45.54</span></td>
<td id="S5.T1.2.2.2.7.5.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.7.5.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.7.1" class="ltx_text" style="font-size:90%;">61.64</span></td>
<td id="S5.T1.2.2.2.7.5.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.8.1" class="ltx_text" style="font-size:90%;">77.85</span></td>
<td id="S5.T1.2.2.2.7.5.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.9.1" class="ltx_text" style="font-size:90%;">40.03</span></td>
<td id="S5.T1.2.2.2.7.5.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.7.5.10.1" class="ltx_text" style="font-size:90%;">55.04</span></td>
<td id="S5.T1.2.2.2.7.5.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.8.6" class="ltx_tr">
<th id="S5.T1.2.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.8.6.1.1" class="ltx_text" style="font-size:90%;">CSSÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.8.6.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Chen etÂ al.<span id="S5.T1.2.2.2.8.6.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib8" title="" class="ltx_ref">2020a</a><span id="S5.T1.2.2.2.8.6.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.8.6.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.2.1" class="ltx_text" style="font-size:90%;">58.95</span></td>
<td id="S5.T1.2.2.2.8.6.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.3.1" class="ltx_text" style="font-size:90%;">84.37</span></td>
<td id="S5.T1.2.2.2.8.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.4.1" class="ltx_text" style="font-size:90%;">49.42</span></td>
<td id="S5.T1.2.2.2.8.6.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.5.1" class="ltx_text" style="font-size:90%;">48.21</span></td>
<td id="S5.T1.2.2.2.8.6.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.8.6.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.7.1" class="ltx_text" style="font-size:90%;">59.91</span></td>
<td id="S5.T1.2.2.2.8.6.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.8.1" class="ltx_text" style="font-size:90%;">73.25</span></td>
<td id="S5.T1.2.2.2.8.6.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.9.1" class="ltx_text" style="font-size:90%;">39.77</span></td>
<td id="S5.T1.2.2.2.8.6.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.8.6.10.1" class="ltx_text" style="font-size:90%;">55.11</span></td>
<td id="S5.T1.2.2.2.8.6.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.9.7" class="ltx_tr">
<th id="S5.T1.2.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">
<span id="S5.T1.2.2.2.9.7.1.1" class="ltx_text" style="font-size:90%;">LXMERTÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.9.7.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Tan and Bansal<span id="S5.T1.2.2.2.9.7.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib45" title="" class="ltx_ref">2019</a><span id="S5.T1.2.2.2.9.7.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.2.1" class="ltx_text" style="font-size:90%;">46.23</span></td>
<td id="S5.T1.2.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.3.1" class="ltx_text" style="font-size:90%;">42.84</span></td>
<td id="S5.T1.2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.4.1" class="ltx_text" style="font-size:90%;">18.91</span></td>
<td id="S5.T1.2.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.5.1" class="ltx_text" style="font-size:90%;">55.51</span></td>
<td id="S5.T1.2.2.2.9.7.6" class="ltx_td ltx_border_tt"></td>
<td id="S5.T1.2.2.2.9.7.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">74.16</span></td>
<td id="S5.T1.2.2.2.9.7.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">89.31</span></td>
<td id="S5.T1.2.2.2.9.7.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">56.85</span></td>
<td id="S5.T1.2.2.2.9.7.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.2.2.2.9.7.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.14</span></td>
<td id="S5.T1.2.2.2.9.7.11" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S5.T1.2.2.2.10.8" class="ltx_tr">
<th id="S5.T1.2.2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.10.8.1.1" class="ltx_text" style="font-size:90%;">MUTANTÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.10.8.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Gokhale etÂ al.<span id="S5.T1.2.2.2.10.8.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2020</a><span id="S5.T1.2.2.2.10.8.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.10.8.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">61.72</span></td>
<td id="S5.T1.2.2.2.10.8.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.3.1" class="ltx_text" style="font-size:90%;">88.90</span></td>
<td id="S5.T1.2.2.2.10.8.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">49.68</span></td>
<td id="S5.T1.2.2.2.10.8.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">50.78</span></td>
<td id="S5.T1.2.2.2.10.8.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.10.8.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.7.1" class="ltx_text" style="font-size:90%;">62.56</span></td>
<td id="S5.T1.2.2.2.10.8.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.8.1" class="ltx_text" style="font-size:90%;">82.07</span></td>
<td id="S5.T1.2.2.2.10.8.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.9.1" class="ltx_text" style="font-size:90%;">42.52</span></td>
<td id="S5.T1.2.2.2.10.8.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.10.8.10.1" class="ltx_text" style="font-size:90%;">53.28</span></td>
<td id="S5.T1.2.2.2.10.8.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.11.9" class="ltx_tr">
<th id="S5.T1.2.2.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T1.2.2.2.11.9.1.1" class="ltx_text" style="font-size:90%;">Ours</span></th>
<td id="S5.T1.2.2.2.11.9.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.2.1" class="ltx_text" style="font-size:90%;">46.93</span></td>
<td id="S5.T1.2.2.2.11.9.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.3.1" class="ltx_text" style="font-size:90%;">43.25</span></td>
<td id="S5.T1.2.2.2.11.9.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.4.1" class="ltx_text" style="font-size:90%;">20.03</span></td>
<td id="S5.T1.2.2.2.11.9.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.5.1" class="ltx_text" style="font-size:90%;">54.56</span></td>
<td id="S5.T1.2.2.2.11.9.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.11.9.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.7.1" class="ltx_text" style="font-size:90%;">59.32</span></td>
<td id="S5.T1.2.2.2.11.9.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.8.1" class="ltx_text" style="font-size:90%;">80.10</span></td>
<td id="S5.T1.2.2.2.11.9.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.9.1" class="ltx_text" style="font-size:90%;">40.23</span></td>
<td id="S5.T1.2.2.2.11.9.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.11.9.10.1" class="ltx_text" style="font-size:90%;">49.58</span></td>
<td id="S5.T1.2.2.2.11.9.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.12.10" class="ltx_tr">
<th id="S5.T1.2.2.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T1.2.2.2.12.10.1.1" class="ltx_text" style="font-size:90%;">UpDnÂ </span><cite class="ltx_cite ltx_citemacro_cite">Anderson etÂ al. <span id="S5.T1.2.2.2.12.10.1.2.1.1.1" class="ltx_text" style="font-size:90%;">(</span><a href="#bib.bib3" title="" class="ltx_ref">2018</a><span id="S5.T1.2.2.2.12.10.1.3.2.2.1" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.2.1" class="ltx_text" style="font-size:90%;">41.58</span></td>
<td id="S5.T1.2.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.3.1" class="ltx_text" style="font-size:90%;">43.07</span></td>
<td id="S5.T1.2.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.4.1" class="ltx_text" style="font-size:90%;">13.58</span></td>
<td id="S5.T1.2.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.5.1" class="ltx_text" style="font-size:90%;">48.48</span></td>
<td id="S5.T1.2.2.2.12.10.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.2.2.2.12.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.7.1" class="ltx_text" style="font-size:90%;">63.48</span></td>
<td id="S5.T1.2.2.2.12.10.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.8.1" class="ltx_text" style="font-size:90%;">81.18</span></td>
<td id="S5.T1.2.2.2.12.10.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.9.1" class="ltx_text" style="font-size:90%;">42.14</span></td>
<td id="S5.T1.2.2.2.12.10.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.12.10.10.1" class="ltx_text" style="font-size:90%;">55.66</span></td>
<td id="S5.T1.2.2.2.12.10.11" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S5.T1.2.2.2.13.11" class="ltx_tr">
<th id="S5.T1.2.2.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T1.2.2.2.13.11.1.1" class="ltx_text" style="font-size:90%;">UpDn + SSLÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T1.2.2.2.13.11.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Zhu etÂ al.<span id="S5.T1.2.2.2.13.11.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib60" title="" class="ltx_ref">2020</a><span id="S5.T1.2.2.2.13.11.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T1.2.2.2.13.11.2" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.2.1" class="ltx_text" style="font-size:90%;">57.59</span></td>
<td id="S5.T1.2.2.2.13.11.3" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.3.1" class="ltx_text" style="font-size:90%;">86.53</span></td>
<td id="S5.T1.2.2.2.13.11.4" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.4.1" class="ltx_text" style="font-size:90%;">29.87</span></td>
<td id="S5.T1.2.2.2.13.11.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.5.1" class="ltx_text" style="font-size:90%;">50.03</span></td>
<td id="S5.T1.2.2.2.13.11.6" class="ltx_td"></td>
<td id="S5.T1.2.2.2.13.11.7" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.7.1" class="ltx_text" style="font-size:90%;">63.73</span></td>
<td id="S5.T1.2.2.2.13.11.8" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.13.11.9" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.9.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.13.11.10" class="ltx_td ltx_align_center"><span id="S5.T1.2.2.2.13.11.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T1.2.2.2.13.11.11" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T1.2.2.2.14.12" class="ltx_tr">
<th id="S5.T1.2.2.2.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T1.2.2.2.14.12.1.1" class="ltx_text" style="font-size:90%;">Ours</span></th>
<td id="S5.T1.2.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.2.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">60.70</span></td>
<td id="S5.T1.2.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:90%;">89.73</span></td>
<td id="S5.T1.2.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.4.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">45.89</span></td>
<td id="S5.T1.2.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.5.1" class="ltx_text" style="font-size:90%;">48.36</span></td>
<td id="S5.T1.2.2.2.14.12.6" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.2.2.2.14.12.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">64.09</span></td>
<td id="S5.T1.2.2.2.14.12.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.8.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">81.78</span></td>
<td id="S5.T1.2.2.2.14.12.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">44.57</span></td>
<td id="S5.T1.2.2.2.14.12.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.2.2.2.14.12.10.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:90%;">55.84</span></td>
<td id="S5.T1.2.2.2.14.12.11" class="ltx_td ltx_nopad_r ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracies on VQA-CP-v2 test and VQA-v2 validation sets. â€œOurs" represents the final model build on LXMERT/UpDn with augmentation sampler, cross-modal joint distribution and reweighing augmented loss.
Overall best scores are <span id="S5.T1.9.1" class="ltx_text ltx_font_bold">bold</span>, and our best ones are <span id="S5.T1.10.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span>.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with Alternative Approaches</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In Table <a href="#S5.T1" title="Table 1 â€£ 5 Results â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare our proposed framework with previous SOTA approaches on two benchmarks: VQA-CP-v2 and VQA-v2.
We compare our generative VQA model against existing models.
RUBi<cite class="ltx_cite ltx_citemacro_citep">(Cadene etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, SCR<cite class="ltx_cite ltx_citemacro_citep">(Wu and Mooney, <a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>, LMH<cite class="ltx_cite ltx_citemacro_citep">(Clark etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, and CSS<cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite> are built on UpDn<cite class="ltx_cite ltx_citemacro_cite">Anderson etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> by adding different de-biasing components to mitigate superficial language biases <cite class="ltx_cite ltx_citemacro_cite">Niu etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> and improve robustness of VQA models.
Besides, CSS<cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite> and MUTANT<cite class="ltx_cite ltx_citemacro_citep">(Gokhale etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> make use of data augmentation to provide large-scale data sizes for VQA training.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We show that our generative VQA model outperforms most of the alternative approaches above.
For VQA-CP-v2, our method achieves 60.70 accuracy on all question types, competitive with the top results from MUTANT.
Our method shows improvements with 3.20 on the Yes-No category, 15.52 on Number-based questions; also 0.83 compared to MUTANT on the Yes-No category.
Our model improves substantially on â€œNumâ€ questions.
When built on UpDn and evaluated on the VQA-v2 val dataset, our framework achieves better performance (%64.09) than UpDn and outperforms MUTANT (%62.56).
This indicates that our generative augmentation model has higher generalisation ability to different VQA datasets, compared to MUTANT.
</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">To further prove the effectiveness of our generative model, we conduct experiments based on pre-trained visual-language framework - LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib45" title="" class="ltx_ref">2019</a>)</cite>, which is also the backbone of the state-of-the-art model on VQA-CP-v2 - MUTANT <cite class="ltx_cite ltx_citemacro_citep">(Gokhale etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> using data augmentation.
Specifically, our generated QA pairs are used to fine-tune the pre-trained LXMERT model.
Our model reaches some improvement over LXMERT on VQA-CP-v2, which further shows the efficacy of our generated QA pairs.
Although MUTANT is also focused on data augmentation and achieves significant improvements over LXMERT, their augmented data is from manually introduced mutations such as removing object instances and colour inversion for a strong contrastive learning signal.
The augmentation relies on low-level operations with deterministic heuristics and applies costly extensive transformations on original samples independently of VQA training.
We believe this is intentionally designed for VQA-CP-v2 dataset and limits the generalisation ability, while our goal is to introduce a generative approach that can be generalised to different multimodal tasks for cross-modal data augmentation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation of Model Components</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In order to evaluate the efficacy of each component of our generative model, we conduct an ablation study. Table <a href="#S5.T2" title="Table 2 â€£ 5.2 Evaluation of Model Components â€£ 5 Results â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results against two strong base models: Updn <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> and SSL <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>. SSL is built on UpDn and uses negative sampling to replace images in the triplets of (image, question, answer) to create irrelevant samples.
We can observe that in both cases, the components in our proposed framework can be beneficial and improve the performance by a reasonable margin.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:186.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(3.7pt,-3.1pt) scale(1.03487608276497,1.03487608276497) ;">
<table id="S5.T2.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.4.4.4.5.1" class="ltx_tr">
<th id="S5.T2.4.4.4.5.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S5.T2.4.4.4.5.1.2.1" class="ltx_text" style="font-size:90%;">VQA-CP-v2 test (%)</span></th>
</tr>
<tr id="S5.T2.4.4.4.6.2" class="ltx_tr">
<th id="S5.T2.4.4.4.6.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.4.4.4.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.4.4.4.6.2.2.1" class="ltx_text" style="font-size:80%;">All</span></th>
<th id="S5.T2.4.4.4.6.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.4.4.4.6.2.3.1" class="ltx_text" style="font-size:80%;">Yes/No</span></th>
<th id="S5.T2.4.4.4.6.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.4.4.4.6.2.4.1" class="ltx_text" style="font-size:80%;">Num</span></th>
<th id="S5.T2.4.4.4.6.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.4.4.4.6.2.5.1" class="ltx_text" style="font-size:80%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.4.4.4.7.1" class="ltx_tr">
<th id="S5.T2.4.4.4.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T2.4.4.4.7.1.1.1" class="ltx_text" style="font-size:90%;">UpDn</span></th>
<td id="S5.T2.4.4.4.7.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.4.4.4.7.1.2.1" class="ltx_text" style="font-size:90%;">41.58</span></td>
<td id="S5.T2.4.4.4.7.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.4.4.4.7.1.3.1" class="ltx_text" style="font-size:90%;">43.07</span></td>
<td id="S5.T2.4.4.4.7.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.4.4.4.7.1.4.1" class="ltx_text" style="font-size:90%;">13.58</span></td>
<td id="S5.T2.4.4.4.7.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.4.4.4.7.1.5.1" class="ltx_text" style="font-size:90%;">48.48</span></td>
</tr>
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T2.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">+ </span><math id="S5.T2.1.1.1.1.1.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S5.T2.1.1.1.1.1.m1.2a"><mrow id="S5.T2.1.1.1.1.1.m1.2.2" xref="S5.T2.1.1.1.1.1.m1.2.2.cmml"><msub id="S5.T2.1.1.1.1.1.m1.2.2.3" xref="S5.T2.1.1.1.1.1.m1.2.2.3.cmml"><mi mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.3.2" xref="S5.T2.1.1.1.1.1.m1.2.2.3.2.cmml">q</mi><mi mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.3.3" xref="S5.T2.1.1.1.1.1.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S5.T2.1.1.1.1.1.m1.2.2.2" xref="S5.T2.1.1.1.1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S5.T2.1.1.1.1.1.m1.2.2.1.1" xref="S5.T2.1.1.1.1.1.m1.2.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.2" xref="S5.T2.1.1.1.1.1.m1.2.2.1.2.cmml">(</mo><mi mathsize="90%" id="S5.T2.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">Q</mi><mo mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.3" xref="S5.T2.1.1.1.1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.2" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.1" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.1.cmml">|</mo><mi mathsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.3" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.4" xref="S5.T2.1.1.1.1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.2b"><apply id="S5.T2.1.1.1.1.1.m1.2.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2"><times id="S5.T2.1.1.1.1.1.m1.2.2.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.2"></times><apply id="S5.T2.1.1.1.1.1.m1.2.2.3.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S5.T2.1.1.1.1.1.m1.2.2.3.1.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.3">subscript</csymbol><ci id="S5.T2.1.1.1.1.1.m1.2.2.3.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.3.2">ğ‘</ci><ci id="S5.T2.1.1.1.1.1.m1.2.2.3.3.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S5.T2.1.1.1.1.1.m1.2.2.1.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">ğ‘„</ci><apply id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.2">ğ´</ci><ci id="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.3.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.2c">q_{\phi}(Q,A|V)</annotation></semantics></math>
</th>
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">42.84</span></td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">44.57</span></td>
<td id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">24.93</span></td>
<td id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">47.05</span></td>
</tr>
<tr id="S5.T2.2.2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T2.2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">+ </span><math id="S5.T2.2.2.2.2.1.m1.3" class="ltx_Math" alttext="p_{\theta}(V,Q,A)" display="inline"><semantics id="S5.T2.2.2.2.2.1.m1.3a"><mrow id="S5.T2.2.2.2.2.1.m1.3.4" xref="S5.T2.2.2.2.2.1.m1.3.4.cmml"><msub id="S5.T2.2.2.2.2.1.m1.3.4.2" xref="S5.T2.2.2.2.2.1.m1.3.4.2.cmml"><mi mathsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.2.2" xref="S5.T2.2.2.2.2.1.m1.3.4.2.2.cmml">p</mi><mi mathsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.2.3" xref="S5.T2.2.2.2.2.1.m1.3.4.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S5.T2.2.2.2.2.1.m1.3.4.1" xref="S5.T2.2.2.2.2.1.m1.3.4.1.cmml">â€‹</mo><mrow id="S5.T2.2.2.2.2.1.m1.3.4.3.2" xref="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.3.2.1" xref="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml">(</mo><mi mathsize="90%" id="S5.T2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">V</mi><mo mathsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.3.2.2" xref="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="S5.T2.2.2.2.2.1.m1.2.2" xref="S5.T2.2.2.2.2.1.m1.2.2.cmml">Q</mi><mo mathsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.3.2.3" xref="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="S5.T2.2.2.2.2.1.m1.3.3" xref="S5.T2.2.2.2.2.1.m1.3.3.cmml">A</mi><mo maxsize="90%" minsize="90%" id="S5.T2.2.2.2.2.1.m1.3.4.3.2.4" xref="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.3b"><apply id="S5.T2.2.2.2.2.1.m1.3.4.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4"><times id="S5.T2.2.2.2.2.1.m1.3.4.1.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.1"></times><apply id="S5.T2.2.2.2.2.1.m1.3.4.2.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.2"><csymbol cd="ambiguous" id="S5.T2.2.2.2.2.1.m1.3.4.2.1.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.2">subscript</csymbol><ci id="S5.T2.2.2.2.2.1.m1.3.4.2.2.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.2.2">ğ‘</ci><ci id="S5.T2.2.2.2.2.1.m1.3.4.2.3.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.2.3">ğœƒ</ci></apply><vector id="S5.T2.2.2.2.2.1.m1.3.4.3.1.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.3.2"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">ğ‘‰</ci><ci id="S5.T2.2.2.2.2.1.m1.2.2.cmml" xref="S5.T2.2.2.2.2.1.m1.2.2">ğ‘„</ci><ci id="S5.T2.2.2.2.2.1.m1.3.3.cmml" xref="S5.T2.2.2.2.2.1.m1.3.3">ğ´</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.3c">p_{\theta}(V,Q,A)</annotation></semantics></math>
</th>
<td id="S5.T2.2.2.2.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">42.96</span></td>
<td id="S5.T2.2.2.2.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">45.88</span></td>
<td id="S5.T2.2.2.2.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.2.2.4.1" class="ltx_text" style="font-size:90%;">24.82</span></td>
<td id="S5.T2.2.2.2.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.2.2.5.1" class="ltx_text" style="font-size:90%;">47.66</span></td>
</tr>
<tr id="S5.T2.4.4.4.8.2" class="ltx_tr">
<th id="S5.T2.4.4.4.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.4.4.4.8.2.1.1" class="ltx_text" style="font-size:90%;">+ reweighted</span></th>
<td id="S5.T2.4.4.4.8.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.8.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">43.12</span></td>
<td id="S5.T2.4.4.4.8.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.8.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">45.76</span></td>
<td id="S5.T2.4.4.4.8.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.8.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.71</span></td>
<td id="S5.T2.4.4.4.8.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.8.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">48.51</span></td>
</tr>
<tr id="S5.T2.4.4.4.9.3" class="ltx_tr">
<th id="S5.T2.4.4.4.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.4.4.4.9.3.1.1" class="ltx_text" style="font-size:90%;">SSL</span></th>
<td id="S5.T2.4.4.4.9.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.4.4.9.3.2.1" class="ltx_text" style="font-size:90%;">57.59</span></td>
<td id="S5.T2.4.4.4.9.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.4.4.9.3.3.1" class="ltx_text" style="font-size:90%;">86.53</span></td>
<td id="S5.T2.4.4.4.9.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.4.4.9.3.4.1" class="ltx_text" style="font-size:90%;">29.87</span></td>
<td id="S5.T2.4.4.4.9.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.4.4.9.3.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">50.03</span></td>
</tr>
<tr id="S5.T2.3.3.3.3" class="ltx_tr">
<th id="S5.T2.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T2.3.3.3.3.1.1" class="ltx_text" style="font-size:90%;">+ </span><math id="S5.T2.3.3.3.3.1.m1.2" class="ltx_Math" alttext="q_{\phi}(Q,A|V)" display="inline"><semantics id="S5.T2.3.3.3.3.1.m1.2a"><mrow id="S5.T2.3.3.3.3.1.m1.2.2" xref="S5.T2.3.3.3.3.1.m1.2.2.cmml"><msub id="S5.T2.3.3.3.3.1.m1.2.2.3" xref="S5.T2.3.3.3.3.1.m1.2.2.3.cmml"><mi mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.3.2" xref="S5.T2.3.3.3.3.1.m1.2.2.3.2.cmml">q</mi><mi mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.3.3" xref="S5.T2.3.3.3.3.1.m1.2.2.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S5.T2.3.3.3.3.1.m1.2.2.2" xref="S5.T2.3.3.3.3.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S5.T2.3.3.3.3.1.m1.2.2.1.1" xref="S5.T2.3.3.3.3.1.m1.2.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.2" xref="S5.T2.3.3.3.3.1.m1.2.2.1.2.cmml">(</mo><mi mathsize="90%" id="S5.T2.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">Q</mi><mo mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.3" xref="S5.T2.3.3.3.3.1.m1.2.2.1.2.cmml">,</mo><mrow id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.2" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.2.cmml">A</mi><mo fence="false" mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.1" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.1.cmml">|</mo><mi mathsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.3" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.3.cmml">V</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.4" xref="S5.T2.3.3.3.3.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.2b"><apply id="S5.T2.3.3.3.3.1.m1.2.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2"><times id="S5.T2.3.3.3.3.1.m1.2.2.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.2"></times><apply id="S5.T2.3.3.3.3.1.m1.2.2.3.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.3"><csymbol cd="ambiguous" id="S5.T2.3.3.3.3.1.m1.2.2.3.1.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.3">subscript</csymbol><ci id="S5.T2.3.3.3.3.1.m1.2.2.3.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.3.2">ğ‘</ci><ci id="S5.T2.3.3.3.3.1.m1.2.2.3.3.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.3.3">italic-Ï•</ci></apply><interval closure="open" id="S5.T2.3.3.3.3.1.m1.2.2.1.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1"><ci id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">ğ‘„</ci><apply id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.1">conditional</csymbol><ci id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.2">ğ´</ci><ci id="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.3.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2.1.1.1.3">ğ‘‰</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.2c">q_{\phi}(Q,A|V)</annotation></semantics></math>
</th>
<td id="S5.T2.3.3.3.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.3.3.3.3.2.1" class="ltx_text" style="font-size:90%;">59.80</span></td>
<td id="S5.T2.3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.3.3.3.3.3.1" class="ltx_text" style="font-size:90%;">89.54</span></td>
<td id="S5.T2.3.3.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.3.3.3.3.4.1" class="ltx_text" style="font-size:90%;">43.80</span></td>
<td id="S5.T2.3.3.3.3.5" class="ltx_td ltx_align_center"><span id="S5.T2.3.3.3.3.5.1" class="ltx_text" style="font-size:90%;">48.61</span></td>
</tr>
<tr id="S5.T2.4.4.4.4" class="ltx_tr">
<th id="S5.T2.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T2.4.4.4.4.1.1" class="ltx_text" style="font-size:90%;">+ </span><math id="S5.T2.4.4.4.4.1.m1.3" class="ltx_Math" alttext="p_{\theta}(V,Q,A)" display="inline"><semantics id="S5.T2.4.4.4.4.1.m1.3a"><mrow id="S5.T2.4.4.4.4.1.m1.3.4" xref="S5.T2.4.4.4.4.1.m1.3.4.cmml"><msub id="S5.T2.4.4.4.4.1.m1.3.4.2" xref="S5.T2.4.4.4.4.1.m1.3.4.2.cmml"><mi mathsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.2.2" xref="S5.T2.4.4.4.4.1.m1.3.4.2.2.cmml">p</mi><mi mathsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.2.3" xref="S5.T2.4.4.4.4.1.m1.3.4.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S5.T2.4.4.4.4.1.m1.3.4.1" xref="S5.T2.4.4.4.4.1.m1.3.4.1.cmml">â€‹</mo><mrow id="S5.T2.4.4.4.4.1.m1.3.4.3.2" xref="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.3.2.1" xref="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml">(</mo><mi mathsize="90%" id="S5.T2.4.4.4.4.1.m1.1.1" xref="S5.T2.4.4.4.4.1.m1.1.1.cmml">V</mi><mo mathsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.3.2.2" xref="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="S5.T2.4.4.4.4.1.m1.2.2" xref="S5.T2.4.4.4.4.1.m1.2.2.cmml">Q</mi><mo mathsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.3.2.3" xref="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="S5.T2.4.4.4.4.1.m1.3.3" xref="S5.T2.4.4.4.4.1.m1.3.3.cmml">A</mi><mo maxsize="90%" minsize="90%" id="S5.T2.4.4.4.4.1.m1.3.4.3.2.4" xref="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.1.m1.3b"><apply id="S5.T2.4.4.4.4.1.m1.3.4.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4"><times id="S5.T2.4.4.4.4.1.m1.3.4.1.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.1"></times><apply id="S5.T2.4.4.4.4.1.m1.3.4.2.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.2"><csymbol cd="ambiguous" id="S5.T2.4.4.4.4.1.m1.3.4.2.1.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.2">subscript</csymbol><ci id="S5.T2.4.4.4.4.1.m1.3.4.2.2.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.2.2">ğ‘</ci><ci id="S5.T2.4.4.4.4.1.m1.3.4.2.3.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.2.3">ğœƒ</ci></apply><vector id="S5.T2.4.4.4.4.1.m1.3.4.3.1.cmml" xref="S5.T2.4.4.4.4.1.m1.3.4.3.2"><ci id="S5.T2.4.4.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.4.4.1.m1.1.1">ğ‘‰</ci><ci id="S5.T2.4.4.4.4.1.m1.2.2.cmml" xref="S5.T2.4.4.4.4.1.m1.2.2">ğ‘„</ci><ci id="S5.T2.4.4.4.4.1.m1.3.3.cmml" xref="S5.T2.4.4.4.4.1.m1.3.3">ğ´</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.1.m1.3c">p_{\theta}(V,Q,A)</annotation></semantics></math>
</th>
<td id="S5.T2.4.4.4.4.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.4.2.1" class="ltx_text" style="font-size:90%;">60.37</span></td>
<td id="S5.T2.4.4.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.4.3.1" class="ltx_text" style="font-size:90%;">89.49</span></td>
<td id="S5.T2.4.4.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.4.4.1" class="ltx_text" style="font-size:90%;">44.43</span></td>
<td id="S5.T2.4.4.4.4.5" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.4.5.1" class="ltx_text" style="font-size:90%;">48.72</span></td>
</tr>
<tr id="S5.T2.4.4.4.10.4" class="ltx_tr">
<th id="S5.T2.4.4.4.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.4.4.4.10.4.1.1" class="ltx_text" style="font-size:90%;">+ reweighted</span></th>
<td id="S5.T2.4.4.4.10.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.4.4.4.10.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.70</span></td>
<td id="S5.T2.4.4.4.10.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.4.4.4.10.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">89.73</span></td>
<td id="S5.T2.4.4.4.10.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.4.4.4.10.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">45.89</span></td>
<td id="S5.T2.4.4.4.10.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.4.4.4.10.4.5.1" class="ltx_text" style="font-size:90%;">48.36</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Study on the benefits of each component of the proposed approach: augmentation sampler, cross-modal joint distribution, and reweighed loss. The best accuracies are in <span id="S5.T2.9.1" class="ltx_text ltx_font_bold">bold</span>.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p">Specifically, the incorporation of generative training for augmentation, i.e. answer generation <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="q(A|V)" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S5.SS2.p2.1.m1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS2.p2.1.m1.1.1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p2.1.m1.1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S5.SS2.p2.1.m1.1.1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S5.SS2.p2.1.m1.1.1.1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S5.SS2.p2.1.m1.1.1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><times id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2"></times><ci id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">ğ‘</ci><apply id="S5.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S5.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.2">ğ´</ci><ci id="S5.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">q(A|V)</annotation></semantics></math> and question generation <math id="S5.SS2.p2.2.m2.3" class="ltx_Math" alttext="q(Q|V,A)" display="inline"><semantics id="S5.SS2.p2.2.m2.3a"><mrow id="S5.SS2.p2.2.m2.3.3" xref="S5.SS2.p2.2.m2.3.3.cmml"><mi id="S5.SS2.p2.2.m2.3.3.3" xref="S5.SS2.p2.2.m2.3.3.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.2.m2.3.3.2" xref="S5.SS2.p2.2.m2.3.3.2.cmml">â€‹</mo><mrow id="S5.SS2.p2.2.m2.3.3.1.1" xref="S5.SS2.p2.2.m2.3.3.1.1.1.cmml"><mo stretchy="false" id="S5.SS2.p2.2.m2.3.3.1.1.2" xref="S5.SS2.p2.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S5.SS2.p2.2.m2.3.3.1.1.1" xref="S5.SS2.p2.2.m2.3.3.1.1.1.cmml"><mi id="S5.SS2.p2.2.m2.3.3.1.1.1.2" xref="S5.SS2.p2.2.m2.3.3.1.1.1.2.cmml">Q</mi><mo fence="false" id="S5.SS2.p2.2.m2.3.3.1.1.1.1" xref="S5.SS2.p2.2.m2.3.3.1.1.1.1.cmml">|</mo><mrow id="S5.SS2.p2.2.m2.3.3.1.1.1.3.2" xref="S5.SS2.p2.2.m2.3.3.1.1.1.3.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">V</mi><mo id="S5.SS2.p2.2.m2.3.3.1.1.1.3.2.1" xref="S5.SS2.p2.2.m2.3.3.1.1.1.3.1.cmml">,</mo><mi id="S5.SS2.p2.2.m2.2.2" xref="S5.SS2.p2.2.m2.2.2.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S5.SS2.p2.2.m2.3.3.1.1.3" xref="S5.SS2.p2.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.3b"><apply id="S5.SS2.p2.2.m2.3.3.cmml" xref="S5.SS2.p2.2.m2.3.3"><times id="S5.SS2.p2.2.m2.3.3.2.cmml" xref="S5.SS2.p2.2.m2.3.3.2"></times><ci id="S5.SS2.p2.2.m2.3.3.3.cmml" xref="S5.SS2.p2.2.m2.3.3.3">ğ‘</ci><apply id="S5.SS2.p2.2.m2.3.3.1.1.1.cmml" xref="S5.SS2.p2.2.m2.3.3.1.1"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.3.3.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.3.3.1.1.1.1">conditional</csymbol><ci id="S5.SS2.p2.2.m2.3.3.1.1.1.2.cmml" xref="S5.SS2.p2.2.m2.3.3.1.1.1.2">ğ‘„</ci><list id="S5.SS2.p2.2.m2.3.3.1.1.1.3.1.cmml" xref="S5.SS2.p2.2.m2.3.3.1.1.1.3.2"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">ğ‘‰</ci><ci id="S5.SS2.p2.2.m2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2">ğ´</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.3c">q(Q|V,A)</annotation></semantics></math>, improves 1.26 and 2.21 accuracy points on
the two base models, respectively.
When introducing the cross-modal joint modelling to provide prior knowledge of multimodal distribution, the performance is further improved by 0.12 and 0.57 accuracy points, respectively.
The overall better results observed with SSL can be attributed to the negative sampling feature in this model.
The most significant improvement across all the categories is observed in â€œNum" questions.
With the generative objective as reward to reweigh the VQA loss on the augmented data, the best performance of our model can achieve is 60.70 accuracy, outperforming the strong SSL model by 3.11 points.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2105.04780/assets/figures/analysis.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="334" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S5.F3.2.1" class="ltx_text ltx_font_bold">Qualitative Analysis:</span> Examples comparing generated QA pairs against those from the original dataset annotations for the same image.
Generated QA pairs are informative if they are out of distribution compared with the original QA pairs.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Examples of Generated QA pairs</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To qualitatively demonstrate the effectiveness of the proposed approach, Figure <a href="#S5.F3" title="Figure 3 â€£ 5.2 Evaluation of Model Components â€£ 5 Results â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows examples of generated QA pairs and compares them with the original ones from the VQA-CP-v2 dataset.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In the four examples, we present five generated QA pairs from the proposed generator and five QA pairs from the original dataset, for each image.
For the generated QA pairs, we have a column <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="q_{\phi}(A|V)" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><msub id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml"><mi id="S5.SS3.p2.1.m1.1.1.3.2" xref="S5.SS3.p2.1.m1.1.1.3.2.cmml">q</mi><mi id="S5.SS3.p2.1.m1.1.1.3.3" xref="S5.SS3.p2.1.m1.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S5.SS3.p2.1.m1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p2.1.m1.1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S5.SS3.p2.1.m1.1.1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S5.SS3.p2.1.m1.1.1.1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><times id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2"></times><apply id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.3.1.cmml" xref="S5.SS3.p2.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS3.p2.1.m1.1.1.3.2.cmml" xref="S5.SS3.p2.1.m1.1.1.3.2">ğ‘</ci><ci id="S5.SS3.p2.1.m1.1.1.3.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3.3">italic-Ï•</ci></apply><apply id="S5.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S5.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.1.2">ğ´</ci><ci id="S5.SS3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">q_{\phi}(A|V)</annotation></semantics></math> showing the probabilities
of sampled answers given only the input image, in descending order.
It can be seen from the <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">Answer</span> column that most of the answers can be directly related to the image, but some are not.
This is because we use ground-truth answers from the original dataset to train the image-answer pipeline, which contains some answers that have high correlations with the questions but not with the image.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">In the column of <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">Questions</span>, we show the generated questions that are conditioned on the sampled answers and the image, which have high diversity.
To quantify diversity, we measure type-token ratio on a per-image basis.
The average ratio is 0.53, which is â€“ as expected â€“ lower than the average ratio for the original QA pairs (0.79), but Figure <a href="#S5.F3" title="Figure 3 â€£ 5.2 Evaluation of Model Components â€£ 5 Results â€£ Cross-Modal Generative Augmentation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that generated questions are meaningful and related to the answers and the image.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Additionally, the <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">Reliability</span> <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><ci id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\mathcal{R}</annotation></semantics></math> of each QA pair is illustrated, showing that most QA pairs have high reliability and relevance to the image.
Note that high probabilities of answers given an image cannot guarantee that the augmented QA pairs have high correlation with the image.
In the column of <span id="S5.SS3.p4.1.2" class="ltx_text ltx_font_typewriter">Novel</span>, we use green check to indicate the novel generated questions when compared to the set of question in the column of <span id="S5.SS3.p4.1.3" class="ltx_text ltx_font_typewriter">Original QA pairs</span>.
The questions are novel when they convey different information from the original ones.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">We also inspect the generated data for a better understanding of the generated QA pairs for different types of questions.
The improvements over â€œNUMâ€ questions can be due to the answer generator.
Our answer generator can sample diverse possible numbers first given an image and the subsequently generated questions are always â€œhow manyâ€ questions, which is not always the case for â€œYes/Noâ€ and â€œOtherâ€ types.
This provides diverse â€œNUMâ€ QA pairs which are not always â€œ1â€ as the answers in the original dataset.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper introduces a generative model for cross-modal data augmentation on VQA.
We learn a generator to generate reliable QA pairs given images under a generative framework.
The augmented QA pairs are trained and evaluated by the generative distribution pretrained on VQA dataset, which are in turn employed in a downstream VQA task with confidence scores to selectively improve the classification performance.
Without low-level operations or specific heuristics, our proposed model is able to augment unlabelled images with large-scale reasonable QA pairs, which boosts a vanilla model to achieve benchmark results.
The strong generalisation ability of our model open avenues for extension to other multimodal machine learning tasks.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work received support from the MultiMT project (H2020 ERC Starting Grant No. 678017) and the Air Force Office of Scientific Research (under award number FA8655-20-1-7006).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal etÂ al. (2020)</span>
<span class="ltx_bibblock">
V.Â Agarwal, Rakshith Shetty, and M.Â Fritz.

</span>
<span class="ltx_bibblock">Towards causal vqa: Revealing and reducing spurious correlations by
invariant and covariant semantic editing.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 9687â€“9695, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Donâ€™t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C.Â Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision (ICCV)</em>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaltruÅ¡aitis etÂ al. (2019)</span>
<span class="ltx_bibblock">
T.Â BaltruÅ¡aitis, C.Â Ahuja, and L.Â Morency.

</span>
<span class="ltx_bibblock">Multimodal machine learning: A survey and taxonomy.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 41(2):423â€“443, 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/TPAMI.2018.2798607" title="" class="ltx_ref">10.1109/TPAMI.2018.2798607</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce
Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich,
Nicolas Pinto, and Joseph Turian.

</span>
<span class="ltx_bibblock">Experience grounds language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 8718â€“8735, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.emnlp-main.703" title="" class="ltx_ref">10.18653/v1/2020.emnlp-main.703</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-main.703" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.emnlp-main.703</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadene etÂ al. (2019)</span>
<span class="ltx_bibblock">
Remi Cadene, Corentin Dancette, Hedi BenÂ younes, Matthieu Cord, and Devi
Parikh.

</span>
<span class="ltx_bibblock">Rubi: Reducing unimodal biases for visual question answering.

</span>
<span class="ltx_bibblock">In H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, volumeÂ 32, pages 841â€“852. Curran
Associates, Inc., 2019.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2019/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2019/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.

</span>
<span class="ltx_bibblock">Counterfactual samples synthesizing for robust visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020a.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, AhmedÂ El Kholy, Faisal Ahmed, Zhe Gan,
YuÂ Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2014)</span>
<span class="ltx_bibblock">
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Empirical evaluation of gated recurrent neural networks on sequence
modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.3555</em>, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Mark Yatskar, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Donâ€™t take the easy way out: Ensemble based methods for avoiding
known dataset biases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 4069â€“4082, Hong Kong,
China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/D19-1418" title="" class="ltx_ref">10.18653/v1/D19-1418</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/D19-1418" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/D19-1418</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2015)</span>
<span class="ltx_bibblock">
Hao Fang, Saurabh Gupta, Forrest Iandola, RupeshÂ K. Srivastava, LiÂ Deng, Piotr
Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, JohnÂ C. Platt,
C.Â LawrenceÂ Zitnick, and Geoffrey Zweig.

</span>
<span class="ltx_bibblock">From captions to visual concepts and back.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, YuÂ Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Large-scale adversarial training for vision-and-language
representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokhale etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang.

</span>
<span class="ltx_bibblock">MUTANT: A training paradigm for out-of-distribution generalization
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 878â€“892, Online, November 2020.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.emnlp-main.63" title="" class="ltx_ref">10.18653/v1/2020.emnlp-main.63</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-main.63" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.emnlp-main.63</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jiuxiang Gu, Jianfei Cai, ShafiqÂ R. Joty, LiÂ Niu, and Gang Wang.

</span>
<span class="ltx_bibblock">Look, imagine and match: Improving textual-visual cross-modal
retrieval with generative models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yutai Hou, Yijia Liu, W.Â Che, and Ting Liu.

</span>
<span class="ltx_bibblock">Sequence-to-sequence data augmentation for dialogue language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1807.01554, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2017)</span>
<span class="ltx_bibblock">
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and EricÂ P. Xing.

</span>
<span class="ltx_bibblock">Toward controlled generation of text.

</span>
<span class="ltx_bibblock">In Doina Precup and YeeÂ Whye Teh, editors, <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
34th International Conference on Machine Learning</em>, volumeÂ 70 of
<em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1587â€“1596,
International Convention Centre, Sydney, Australia, 06â€“11 Aug 2017. PMLR.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v70/hu17e.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v70/hu17e.html</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain etÂ al. (2017)</span>
<span class="ltx_bibblock">
Unnat Jain, Ziyu Zhang, and AlexanderÂ G. Schwing.

</span>
<span class="ltx_bibblock">Creativity: Generating diverse questions using variational
autoencoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, July 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle etÂ al. (2017)</span>
<span class="ltx_bibblock">
Kushal Kafle, Mohammed Yousefhussien, and Christopher Kanan.

</span>
<span class="ltx_bibblock">Data augmentation for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Natural
Language Generation</em>, pages 198â€“202, Santiago de Compostela, Spain,
September 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/W17-3529" title="" class="ltx_ref">10.18653/v1/W17-3529</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/W17-3529" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/W17-3529</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kant etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yash Kant, A.Â Moudgil, Dhruv Batra, D.Â Parikh, and Harsh Agrawal.

</span>
<span class="ltx_bibblock">Contrast and classify: Alternate training for robust vqa.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.06087, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
DiederikÂ P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2014)</span>
<span class="ltx_bibblock">
DiederikÂ P Kingma and Max Welling.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2014.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma etÂ al. (2014)</span>
<span class="ltx_bibblock">
DurkÂ P Kingma, Shakir Mohamed, Danilo JimenezÂ Rezende, and Max Welling.

</span>
<span class="ltx_bibblock">Semi-supervised learning with deep generative models.

</span>
<span class="ltx_bibblock">In Z.Â Ghahramani, M.Â Welling, C.Â Cortes, N.Â Lawrence, and K.Â Q.
Weinberger, editors, <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, volumeÂ 27, pages 3581â€“3589. Curran Associates, Inc., 2014.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiros etÂ al. (2014)</span>
<span class="ltx_bibblock">
Ryan Kiros, Ruslan Salakhutdinov, and RichardÂ S Zemel.

</span>
<span class="ltx_bibblock">Unifying visual-semantic embeddings with multimodal neural language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1411.2539</em>, 2014.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A. Shamma,
MichaelÂ S. Bernstein, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):32â€“73, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/s11263-016-0981-7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11263-016-0981-7</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Michael Bernstein, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Information maximizing visual question generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>,
2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2020)</span>
<span class="ltx_bibblock">
DongÂ Bok Lee, Seanie Lee, WooÂ Tae Jeong, Donghwan Kim, and SungÂ Ju Hwang.

</span>
<span class="ltx_bibblock">Generating diverse and consistent QA pairs from contexts with
information-maximizing hierarchical conditional VAEs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 208â€“224, Online, July 2020.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.acl-main.20" title="" class="ltx_ref">10.18653/v1/2020.acl-main.20</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.acl-main.20" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.acl-main.20</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and
Ming Zhou.

</span>
<span class="ltx_bibblock">Visual question generation as dual task of visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,
James Hays, Pietro Perona, Deva Ramanan, C.Â Lawrence Zitnick, and Piotr
DollÃ¡r.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, 2014.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1405.0312" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1405.0312</a>.

</span>
<span class="ltx_bibblock">cite arxiv:1405.0312Comment: 1) updated annotation pipeline
description and figures; 2) added new section describing datasets splits; 3)
updated author list.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In D.Â Lee, M.Â Sugiyama, U.Â Luxburg, I.Â Guyon, and R.Â Garnett,
editors, <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 29,
pages 289â€“297. Curran Associates, Inc., 2016.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2016/file/9dcb88e0137649590b755372b040afad-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2016/file/9dcb88e0137649590b755372b040afad-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao and Blunsom (2016)</span>
<span class="ltx_bibblock">
Yishu Miao and Phil Blunsom.

</span>
<span class="ltx_bibblock">Language as a latent variable: Discrete generative models for
sentence compression.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 319â€“328, Austin, Texas, November 2016.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/D16-1031" title="" class="ltx_ref">10.18653/v1/D16-1031</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/D16-1031" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/D16-1031</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao etÂ al. (2016)</span>
<span class="ltx_bibblock">
Yishu Miao, Lei Yu, and Phil Blunsom.

</span>
<span class="ltx_bibblock">Neural variational inference for text processing.

</span>
<span class="ltx_bibblock">In MariaÂ Florina Balcan and KilianÂ Q. Weinberger, editors,
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 33rd International Conference on Machine Learning</em>,
volumeÂ 48 of <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages
1727â€“1736, New York, New York, USA, 20â€“22 Jun 2016. PMLR.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v48/miao16.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v48/miao16.html</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih and Gregor (2014)</span>
<span class="ltx_bibblock">
Andriy Mnih and Karol Gregor.

</span>
<span class="ltx_bibblock">Neural variational inference and learning in belief networks.

</span>
<span class="ltx_bibblock">In EricÂ P. Xing and Tony Jebara, editors, <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
31st International Conference on Machine Learning</em>, volumeÂ 32 of
<em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1791â€“1799, Bejing,
China, 22â€“24 Jun 2014. PMLR.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v32/mnih14.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v32/mnih14.html</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mogadala etÂ al. (2019)</span>
<span class="ltx_bibblock">
Aditya Mogadala, M.Â Kalimuthu, and D.Â Klakow.

</span>
<span class="ltx_bibblock">Trends in integration of vision and language research: A survey of
tasks, datasets, and methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1907.09358, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mostafazadeh etÂ al. (2016)</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He,
and Lucy Vanderwende.

</span>
<span class="ltx_bibblock">Generating natural questions about an image.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1802â€“1813,
Berlin, Germany, August 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P16-1170" title="" class="ltx_ref">10.18653/v1/P16-1170</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/P16-1170" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/P16-1170</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ngiam etÂ al. (2011)</span>
<span class="ltx_bibblock">
J.Â Ngiam, A.Â Khosla, Mingyu Kim, Juhan Nam, H.Â Lee, and A.Â Ng.

</span>
<span class="ltx_bibblock">Multimodal deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2011.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong
Wen.

</span>
<span class="ltx_bibblock">Counterfactual vqa: A cause-effect look at language bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington etÂ al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning.

</span>
<span class="ltx_bibblock">GloVe: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1532â€“1543, Doha, Qatar,
October 2014. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.3115/v1/D14-1162" title="" class="ltx_ref">10.3115/v1/D14-1162</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/D14-1162" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/D14-1162</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ray etÂ al. (2019)</span>
<span class="ltx_bibblock">
Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas.

</span>
<span class="ltx_bibblock">Sunny and dark outside?! improving answer consistency in VQA
through entailed question generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5860â€“5865, Hong Kong,
China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/D19-1596" title="" class="ltx_ref">10.18653/v1/D19-1596</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/D19-1596" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/D19-1596</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In C.Â Cortes, N.Â Lawrence, D.Â Lee, M.Â Sugiyama, and R.Â Garnett,
editors, <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 28,
pages 91â€“99. Curran Associates, Inc., 2015.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rezende etÂ al. (2014)</span>
<span class="ltx_bibblock">
DaniloÂ Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.

</span>
<span class="ltx_bibblock">Stochastic backpropagation and approximate inference in deep
generative models.

</span>
<span class="ltx_bibblock">In EricÂ P. Xing and Tony Jebara, editors, <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
31st International Conference on Machine Learning</em>, volumeÂ 32 of
<em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1278â€“1286, Bejing,
China, 22â€“24 Jun 2014. PMLR.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v32/rezende14.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v32/rezende14.html</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah etÂ al. (2019)</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.

</span>
<span class="ltx_bibblock">Cycle-consistency for robust visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2019 Conference on Computer Vision and Pattern Recognition
(CVPR)</em>. IEEE, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Socher etÂ al. (2014)</span>
<span class="ltx_bibblock">
Richard Socher, Andrej Karpathy, QuocÂ V. Le, ChristopherÂ D. Manning, and
AndrewÂ Y. Ng.

</span>
<span class="ltx_bibblock">Grounded compositional semantics for finding and describing images
with sentences.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
2:207â€“218, 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1162/tacl_a_00177" title="" class="ltx_ref">10.1162/tacl_a_00177</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/Q14-1017" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/Q14-1017</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2020)</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=SygXPaEYvH" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=SygXPaEYvH</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal.

</span>
<span class="ltx_bibblock">LXMERT: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5100â€“5111, Hong Kong,
China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref">10.18653/v1/D19-1514</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/D19-1514" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/D19-1514</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Ruixue Tang, Chao Ma, WeiÂ Emma Zhang, QiÂ Wu, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Semantic equivalent adversarial data augmentation for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>,
2020a.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Ruixue Tang, Chifeng Ma, W.Â Zhang, QiÂ Wu, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Semantic equivalent adversarial data augmentation for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020b.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney etÂ al. (2018)</span>
<span class="ltx_bibblock">
Damien Teney, Peter Anderson, Xiaodong He, and Anton Van DenÂ Hengel.

</span>
<span class="ltx_bibblock">Tips and tricks for visual question answering: Learnings from the
2017 challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4223â€“4232, 2018.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uppal etÂ al. (2020)</span>
<span class="ltx_bibblock">
Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya
Poria, R.Â Zimmermann, and Amir Zadeh.

</span>
<span class="ltx_bibblock">Emerging trends of multimodal research in vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.09522, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedd etÂ al. (2021)</span>
<span class="ltx_bibblock">
Nihir Vedd, Zixu Wang, Marek Rei, Yishu Miao, and Lucia Specia.

</span>
<span class="ltx_bibblock">Guiding visual question generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08226</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vendrov etÂ al. (2016)</span>
<span class="ltx_bibblock">
Ivan Vendrov, Ryan Kiros, S.Â Fidler, and R.Â Urtasun.

</span>
<span class="ltx_bibblock">Order-embeddings of images and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1511.06361, 2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2016)</span>
<span class="ltx_bibblock">
Liwei Wang, Yin Li, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Learning deep structure-preserving image-text embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2016.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zihao Wang, Xihui Liu, Hongsheng Li, LuÂ Sheng, Junjie Yan, Xiaogang Wang, and
Jing Shao.

</span>
<span class="ltx_bibblock">Camp: Cross-modal adaptive message passing for text-image retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">The IEEE International Conference on Computer Vision
(ICCV)</em>, October 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams (1992)</span>
<span class="ltx_bibblock">
RonaldÂ J. Williams.

</span>
<span class="ltx_bibblock">Simple statistical gradient-following algorithms for connectionist
reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Mach. Learn.</em>, 8(3â€“4):229â€“256, May
1992.

</span>
<span class="ltx_bibblock">ISSN 0885-6125.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/BF00992696" title="" class="ltx_ref">10.1007/BF00992696</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/BF00992696" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/BF00992696</a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Mooney (2019)</span>
<span class="ltx_bibblock">
Jialin Wu and Raymond Mooney.

</span>
<span class="ltx_bibblock">Self-critical reasoning for robust visual question answering.

</span>
<span class="ltx_bibblock">In H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, editors, <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, volumeÂ 32, pages 8604â€“8614. Curran
Associates, Inc., 2019.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2019/file/33b879e7ab79f56af1e88359f9314a10-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2019/file/33b879e7ab79f56af1e88359f9314a10-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong and Wu (2020)</span>
<span class="ltx_bibblock">
Peixi Xiong and Ying Wu.

</span>
<span class="ltx_bibblock">Ta-student vqa: Multi-agents training by self-questioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, LiÂ Deng, and AlexanderÂ J. Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 21â€“29. IEEE Computer Society, 2016.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="http://dblp.uni-trier.de/db/conf/cvpr/cvpr2016.html#YangHGDS16" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dblp.uni-trier.de/db/conf/cvpr/cvpr2016.html#YangHGDS16</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo etÂ al. (2019)</span>
<span class="ltx_bibblock">
KangÂ Min Yoo, Youhyun Shin, and Sang-goo Lee.

</span>
<span class="ltx_bibblock">Data augmentation for spoken language understanding via joint
variational generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volumeÂ 33, pages 7402â€“7409, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu Jiang* etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yu Jiang*, Vivek Natarajan*, Xinlei Chen*, Marcus Rohrbach, Dhruv Batra,
and Devi Parikh.

</span>
<span class="ltx_bibblock">Pythia v0.1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09956</em>, 2018.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2020)</span>
<span class="ltx_bibblock">
XiÂ Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong Zhang.

</span>
<span class="ltx_bibblock">Overcoming language priors with self-supervised learning for visual
question answering.

</span>
<span class="ltx_bibblock">In Christian Bessiere, editor, <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence, IJCAI-20</em>, pages
1083â€“1089. International Joint Conferences on Artificial Intelligence
Organization, 7 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.24963/ijcai.2020/151" title="" class="ltx_ref">10.24963/ijcai.2020/151</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.24963/ijcai.2020/151" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.24963/ijcai.2020/151</a>.

</span>
<span class="ltx_bibblock">Main track.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.04779" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.04780" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.04780">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.04780" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.04781" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 12:59:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
