<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.05798] Aesthetic Visual Question Answering of Photographs</title><meta property="og:description" content="Aesthetic assessment of images can be categorized into two main forms: numerical assessment and language assessment. Aesthetics caption of photographs is the only task of aesthetic language assessment that has been add…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aesthetic Visual Question Answering of Photographs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Aesthetic Visual Question Answering of Photographs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.05798">

<!--Generated on Wed Mar 13 18:21:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content=" Multimodal learning,  Aesthetic computing,  Visual question and answering,  Semi-automatic labeling,  Transformer">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Aesthetic Visual Question Answering of Photographs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin Jin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jinxinbesti@foxmail.com">jinxinbesti@foxmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_affiliation_institution">Beijing Electronic Science and Technology Institute</span><span id="id3.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id4.3.id3" class="ltx_text ltx_affiliation_state">Beijing</span><span id="id5.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wu Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhouwu_nj@126.com">zhouwu_nj@126.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_affiliation_institution">Beijing Electronic Science and Technology Institute</span><span id="id7.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id8.3.id3" class="ltx_text ltx_affiliation_state">Beijing</span><span id="id9.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinghui Zhou<sup id="id10.2.id1" class="ltx_sup"><span id="id10.2.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:graydove@mail.ustc.edu.cn">graydove@mail.ustc.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id11.3.id1" class="ltx_text ltx_affiliation_institution">University of Science and Technology of China</span><span id="id12.4.id2" class="ltx_text ltx_affiliation_city">Hefei</span><span id="id13.5.id3" class="ltx_text ltx_affiliation_state">Hefei</span><span id="id14.6.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuai Cui
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:shucui@ucdavis.edu">shucui@ucdavis.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_affiliation_institution">University of California, Davis</span><span id="id16.2.id2" class="ltx_text ltx_affiliation_city">Davis</span><span id="id17.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Le Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lezhang.thu@gmail.com">lezhang.thu@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id18.1.id1" class="ltx_text ltx_affiliation_institution">Beijing Electronic Science and Technology Institute</span><span id="id19.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id20.3.id3" class="ltx_text ltx_affiliation_state">Beijing</span><span id="id21.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianwen Lv
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:513415184@qq.com">513415184@qq.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id22.1.id1" class="ltx_text ltx_affiliation_institution">Beijing Electronic Science and Technology Institute</span><span id="id23.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id24.3.id3" class="ltx_text ltx_affiliation_state">Beijing</span><span id="id25.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shu Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhaoshu0104@163.com">zhaoshu0104@163.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id26.1.id1" class="ltx_text ltx_affiliation_institution">Beijing Electronic Science and Technology Institute</span><span id="id27.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id28.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id29.id1" class="ltx_p">Aesthetic assessment of images can be categorized into two main forms: numerical assessment and language assessment. Aesthetics caption of photographs is the only task of aesthetic language assessment that has been addressed. In this paper, we propose a new task of aesthetic language assessment: aesthetic visual question and answering (AVQA) of images. If we give a question of images aesthetics, model can predict the answer. We use images from <span id="id29.id1.1" class="ltx_text ltx_font_italic">www.flickr.com</span>. The objective QA pairs are generated by the proposed aesthetic attributes analysis algorithms. Moreover, we introduce subjective QA pairs that are converted from aesthetic numerical labels and sentiment analysis from large-scale pre-train models. We build the first aesthetic visual question answering dataset, AesVQA, that contains 72,168 high-quality images and 324,756 pairs of aesthetic questions. Two methods for adjusting the data distribution have been proposed and proved to improve the accuracy of existing models. This is the first work that both addresses the task of aesthetic VQA and introduces subjectiveness into VQA tasks. The experimental results reveal that our methods outperform other VQA models on this new task.</p>
</div>
<div class="ltx_keywords"> Multimodal learning, Aesthetic computing, Visual question and answering, Semi-automatic labeling, Transformer
</div>
<div class="ltx_keywords">aesthetic mixed dataset with attributes, multitasking, external attribute features, ECA channel attention
</div>
<div class="ltx_acknowledgements">*Corresponding authors
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/1122445.1122456</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>JACM</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the researches of computer vision and natural language processing, researchers always focus on direct and obvious goals instead of image aesthetics with different attributes. In the past decade, many researchers were interested in image aesthetics. However, the popular methods were to score images and describe attributes of images. Either giving a score ranging from 0 to 10 to evaluate aesthetics of images or describing a images’ attributes<cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, such as color, lighting, and composition, in one sentence are limited and lack description in details.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The state-of-the-art model of visual question answering (VQA) has achieved multiple purposes on many large-scale datasets. According to the category of the datasets, the subtask of VQA can be divided into task-based, inference-based, and text-based, like TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> dataset. Many famous models <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> have been developed in these visual question answering tasks. However, they lack the overall investigation and reasoning of the picture.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Many attribute labels related to image aesthetics can be collected in photography (for example, image classification and evaluation labels available in subjectively scored datasets). Although some studies have used image tags for regression, multi-task evaluation, and image captioning, they ignore the available information of images in multiple information dimensions.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2208.05798/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>VQA and Aesthetic VQA. The AVQA focuses on the overall aesthetic attributes of the image such as shot, lighting and colors.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Some large-scale visual question answering datasets, such as VQA2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, include some questions related to the overall image; GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> and NLVR2 <cite class="ltx_cite ltx_citemacro_citep">(Suhr et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> include some questions about designing image content reasoning. By a lot of computational cost and testing, we can get the answer related to the question about overall characteristics of the image. There is no dataset that focuses on visual question answering based on the overall image.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Most of current VQA researches focus on objective QA pairs. This paper mainly addresses aesthetic visual question answering (AVQA) of images, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This task is both aesthetic and subjective; also, it is a complementary task for the previous QA studies. This problem is important since subjective questions and answers are very prevalent among human discourses. In this paper, we present a dataset with aesthetic QA pairs, aesthetic visual question answering (AesVQA). Images in the new dataset are all marked with aesthetic labels through series of multiple computer vision sub-tasks. These labels cover composition, color, subject, lighting, genres, techniques and emotions of photos.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To obtain reliable labels, we develop some unsupervised and semi-supervised computer vision methods based on image processing and large restraint model. At the same time, we propose a threshold function and design a small amount of manual labelling image adjustment function to get a uniform and credible dataset distribution.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In particularly, the labels of lights and colors are designed based on the specific objects in the image. The labels of scene and composition are designed based on the whole picture. The other labels based on subjective evaluations of images, especially techniques and emotions, provide extra dimensional information in question and answering. The task becomes more challenging by adding some human psychological evaluations. The main contributions of this paper can be summarized as:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><code id="S1.I1.i1.p1.1.1" class="ltx_verbatim ltx_font_typewriter">New Task</code><span id="S1.I1.i1.p1.1.2" class="ltx_text">: The new task of image aesthetics question and answer, and it is the first work that introduces subjective answering to VQA problems.</span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><code id="S1.I1.i2.p1.1.1" class="ltx_verbatim ltx_font_typewriter">New Labels</code><span id="S1.I1.i2.p1.1.2" class="ltx_text">: Semi-automatic QA labelling of images gets more information when answering and methods of dataset distribution adjustment.</span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><code id="S1.I1.i3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">New Points of VQA</code><span id="S1.I1.i3.p1.1.2" class="ltx_text">: The traditional visual question answering method based on objective detection is optimized to half-object-detection-based (such as lighting and color labels) and half-whole-image-based (such as subject and composition labels).</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Aesthetics and computer vision are inextricably linked, and people are demanding higher quality of image data. How to process the images into a form that is more compatible with human preferences has become a critical issue. It is feasible to use computer technology to study the aesthetics of images<cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The earliest image aesthetic quality evaluation was only the two-category evaluation of scores and good or bad, based on which a considerable number of scholars have made considerable contributions on how to score and score on multiple attributes of images. The earliest study of aesthetic image quality evaluation is this paper<cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Visual question answering is a difficult problem that straddles computer vision and natural language processing, and its task requires the extraction of not only image features but also textual partial features. Recent VQA datasets such as DAQUAR<cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>, Visual7W<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2016</a>)</cite>, Visual Madlibs<cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2015</a>)</cite>, FM-IQA<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>, VQA<cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>, etc. are available. Unlike look-and-talk tasks, simply fusing image and text features often does not yield the desired features, i.e., answers.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Visual question answering requires the model to process from image and language to combine and give the answer effectively. So, VQA is both interdisciplinary and challenging. Also, due to characteristics of image aesthetics, there is no professional image aesthetics Q &amp; A(Question and Answer) dataset. Therefore, it is important to propose a common aesthetic Q &amp; A datasets. By the visual model, we use classical Q&amp; A datasets as reference to generate our dataset. The classic Q &amp; A datasets are always generated by machines, such as: VQA<cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>, TextVQA<cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, EST-VQA<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite> and VizWiz-VQA<cite class="ltx_cite ltx_citemacro_citep">(Bigham et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">One of the difficulties of the image aesthetics task is the disadvantage of inaccurate subjective evaluations but low number of objective ones. Borrowing a large amount of data as a foundation, it will be possible to mine a sufficient number of images and corresponding comments with a high enough standard, and then further convert the comments into the desired Q&amp;A pairs. Proxy objectification with statistical features of subjective evaluation is a common approach in current image aesthetics tasks.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">To date, there is no comprehensive aesthetic VQA dataset. Previous research work proposed the AQUA<cite class="ltx_cite ltx_citemacro_citep">(Garcia et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> dataset, which focuses more on studying the artistic aspects of paintings, while our dataset focuses more on the aesthetic aspects of photographs. In previous studies, researchers would apply attention mechanisms to images or text to obtain better results, but it is difficult to obtain the desired features from the many image features because of the different feature spaces and the presence of tensor features with ultra-high dimensionality of images.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>VQA datasets’ answer words length accounted for the proportion of all answers table. Answers with two words or more are 94.6% in AesVQA dataset, which leads a more difficult VQA task.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">The Length of The Answer Words</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">One Word</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Yes/No</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Two Words or More</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">The Number of QA Pairs</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">VQAv2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">51.2%</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">45.0%</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.8%</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">1,105,904</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">51.2%</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">21.2%</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.6%</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">139,868</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">52.5%</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">21.2%</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.3%</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">1,445,233</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">AesVQA</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">5.4%</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S2.T1.1.6.5.3.1" class="ltx_text ltx_font_bold">0%</span></td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.6.5.4.1" class="ltx_text ltx_font_bold">94.6%</span></td>
<td id="S2.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">324,756</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2208.05798/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="441" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Pictures, Questions and Answers in the Aesthetic VQA dataset.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Aesthetic Visual Question Answering</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To study the answers to questions related to image aesthetics and the overall content of the image, and to reduce the cost of manual annotation, we proposed AesVQA, as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2. Related work ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table 1. This dataset is an image aesthetic question and answer dataset based on an unsupervised model. We will start by describing how to select the images used in AesVQA. Then, we explain our data collection pipeline to collect questions and answers.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2208.05798/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="334" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The distribution of the basic aesthetic labels in the AesVQA dataset.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2208.05798/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The distribution of the genres labels in the AesVQA dataset.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2208.05798/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The distribution of photos’s subjective labels in the AesVQA dataset. Including tender, magic, lovely, calm, angry, adventurous and many other attributes.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Images of AesVQA</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use pictures from the Flickr website as our source pictures, which conforms to developing a VQA model based on image aesthetics and the overall content of the image. We are most interested in pictures in categories such as landscapes, people, still lifes, and animals. Several categories in the Explore section of the Flickr website meet this condition.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To obtain accurate images, to determine the aesthetic information of the image and the overall characteristics of the image, we deleted the pictures with borders and the pictures with a too large aspect ratio (if the aspect ratio of the picture is greater than 1:2, delete it). We use the OCR model Rosetta on these images to delete some images that contain watermarks that may be involved in the infringement.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">To automate this process to identify categories that tend to contain images with text, we select 100 random images for each category (if the maximum number of images for that category is less than 100, select all images). We run the state-of-the-art OCR model Rosetta [6] on these images and calculate the average number of OCR boxes in the category.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Based on the 190,000 Flickr images we crawled, after artificially filtering out abstract images and unrealistic images from image photography, filtering these (and a small amount of noisy data from manual annotations) can obtain 72168 images. image. Manual screening requires that the image must be taken from the real world, not obtained through PS or other software editing tools.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Generation and filtering of AesVQA</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In order to generate question and answer pairs, we designed our own model. Our model based on sequence to sequence method, including three modules: text content encoder, answer encoder and decoder. In addition, the quality of the generated AesVQA data is uneven, and there is a big gap with the common VQA dataset. Therefore, the data needs to be screened again after QA is generated. We propose an aesthetic question and answer filtering method using LDA<cite class="ltx_cite ltx_citemacro_citep">(Blei et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2003</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The text content coder is used to process the input comments. At the same time, it needs to calculate the attention weight of different words and the weight of specific words in the replication mechanism. The answer encoder is used to process the selected answer words or phrases, filter the words according to the attention weight, and encode the words into a format suitable for the decoder. The decoder is responsible for using the cyclic neural network to process the input tensor data, and generating questions according to the weight data between the answer and the comment.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For the generated question and answer pairs, the manual filtering method is used to remove the inappropriate question and answer pairs, so as to ensure that at least one question and answer pair generated by each picture is reasonable. These question and answer pairs are formed into a small text set, and the correlation between the text set and multiple topic words obtained from previous LDA topic calculation is calculated, which is used as the threshold to evaluate whether a question and answer pair meets the aesthetic question and answer model.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Calculate the relevance between the remaining sentences and the topics selected by LDA, and judge whether the relevance of the selected sentences is greater than the topic threshold. When the relevance value of the selected sentence is greater than the threshold, it is determined as a question and answer pair that meets the requirements, otherwise it is determined as a question and answer pair that does not meet the requirements.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Finally, since the quality of the generated issues is not very high, the solution is to make manual corrections.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Basic Aesthetic Labels</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The first part of the AADB<cite class="ltx_cite ltx_citemacro_citep">(Kong et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> dataset we constructed is the basic image aesthetic tags, including composition, color, subject, and light. Among them, in order not to reduce the VQA task to a simple classification task, and to ensure that the specific image attributes are associated with specific objects, we design light labels and color labels to be related to the content of the image, while composition labels and subjects labels are related to the overall image. Here, the whole image is defined as multiple targets or content obtained by the fusion of background and foreground information. The biggest feature of this content is that each part of the image is required to obtain it, rather than part-based objects.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Among them, the symmetry of the composition labels is obtained by the SSIM<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2004</a>)</cite> calculation of the image. When the left and right parts of the image or the upper and lower parts of the image after the mirror symmetry, the SSIM is higher than the threshold, and it is judged to be an asymmetric image. The rule of thirds and the center composition use the faster R-CNN based on the visual genome to obtain the position of the most distinctive object in the image. Midas is used for the foreground composition, and the image is divided into three parts: the upper, middle, and lower parts, and judge whether there is a significant difference in the depth of field between the upper part and the lower part.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Color labels depend on the type and color of objects in the image obtained by faster R-CNN. To enrich the diversity of colors, we have added warm tone and cold tone, using target detection to cut out the selected objects in the image, and classify their average colors, and judge based on the correlation between the average color and other known main colors warm tone or cold tone.
The attributes of the subject are labeled using the CLIP<cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> pre-training model. According to the classification results of the subject on the AVA<cite class="ltx_cite ltx_citemacro_citep">(Murray et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2012</a>)</cite>, it can be known that CLIP can deal with the category labeling of multiple subjects. At the same time, to make up for the errors that may be caused by automatic marking, some manually marked image attributes are subsequently used to correct the results.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The attribute of Light is judged by judging the grayscale histogram of the image. When the image is not black and white, but still has more black parts, it is marked as backlight; if the histogram part of the image is on both sides, it is marked as split light; If the histogram of the image is evenly distributed, it is marked as broad light. Similar to the Subject attribute, this part also uses some artificially marked image attributes to correct the result.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2208.05798/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Question answers which before and after the adjustment of image’s confidence and the adjustment of the distribution of answers.The <span id="S3.F6.3.1" class="ltx_text ltx_font_bold">AoC</span> means the adjustment of image’s classify confidence. The <span id="S3.F6.4.2" class="ltx_text ltx_font_bold">AoDA</span> means the adjustment of distribution of answers. These two tricks will help the model achieving a higher accuracy.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Distribution in VQA Dataset</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The distribution of the dataset is extremely important for the VQA task. Some work <cite class="ltx_cite ltx_citemacro_citep">(Teney et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Zellers et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> have proved it. For the aesthetic question and answer task based on the whole image, three dimensions of data distribution need to be considered: the distribution of the whole picture and the specific objects in the image in the question, the distribution of pictures with obvious objects, and the overall scenery in the data set, the answer to the question is Distribution under the influence of subjective factors.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">For the impact of the first data distribution, we roughly divide the labels of the data set into two categories: ”related to specific objects” and ”related to the image as a whole”, and ensure that the number of images and the number of questions in the two categories are consistent.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">In response to the impact of the second data distribution, we have added two more general and adaptable categories: genre and skill. These two shooting-related categories will be further divided into more than 10 sub-categories, each of which has enough questions and answers. The genre and technique will be able to balance the effects of pictures with obvious objects and overall scenery on the model.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">In response to the impact of data distribution in Chapter Four, we add a subjective evaluation. Mark the content of the picture as emotionally related. At the same time, to prevent excessively strong subjective distribution deviations, in the labeling of some attributes, we introduced 7000 additional labeled pictures to constrain the generation of questions and answers.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">We show the distributions of the basic aesthetic attribute labels, the genre labels, the subjective labels and photography labels in Figure <a href="#S3.F3" title="Figure 3 ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Figure <a href="#S3.F4" title="Figure 4 ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Figure <a href="#S3.F5" title="Figure 5 ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Subjective Labels and Photography Component</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">To overcome the possible impact of data distribution, we divide the attributes of images and aesthetics into subjective feelings and objective photography techniques. The subjective description comes from people’s direct perception of the image, and the label of this part comes from the pre-training label of CLIP. Including tender, magic, lovely, calm, angry, adventurous, and many other attributes. The reason for selecting these words is that the results of the sample survey show that these words can get a more balanced data distribution.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">On the other hand, we provide related attributes of photography, including genres and techniques, which contain multiple subcategories. Same as subjective, we use CLIP to mark images. Some properties are shown in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3. Basic Aesthetic Labels ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Many attributes of this part have the limitation of vague definition. Therefore, each category has a confidence level. Only when the confidence level is high enough, the mark of the picture is considered effective.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">CLIP’s image classification model is unsupervised, multiple subcategories can be used as input, and the model is required to classify them. Specifically, some attributes such as ”Multiple Exposure” require more professional photography skills, so the confidence threshold for this category is higher; such as ”Night Photography”, which can better distinguish various attributes, so for this the confidence threshold of the class is low.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Bias of Data</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Because of the use of unsupervised models such as CLIP to predict the types of pictures, how to design the distribution of options in answers for pictures is the key.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Usually, we will give up pictures with too high an accuracy rate and too low an accuracy rate, that is, a picture can be judged as B category on the A attribute, or it is difficult to judge which category it is. We use the design threshold and the accuracy of a certain type of problem model to estimate how the threshold of this type of problem should be, including two parameters, the mean and variance. Only when the mean and the standard deviation of answers’ distribution meet the accuracy of this type of baseline model (LXMERT for example) higher than 50 %, answers contribution is efficacious.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\text{accuracy}(V,Q,A,D(\text{avg}),D(\text{std}))&gt;0.5" display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml"><mrow id="S3.E1.m1.7.7.2" xref="S3.E1.m1.7.7.2.cmml"><mtext id="S3.E1.m1.7.7.2.4" xref="S3.E1.m1.7.7.2.4a.cmml">accuracy</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.2.3" xref="S3.E1.m1.7.7.2.3.cmml">​</mo><mrow id="S3.E1.m1.7.7.2.2.2" xref="S3.E1.m1.7.7.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.2.2.2.3" xref="S3.E1.m1.7.7.2.2.3.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">V</mi><mo id="S3.E1.m1.7.7.2.2.2.4" xref="S3.E1.m1.7.7.2.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Q</mi><mo id="S3.E1.m1.7.7.2.2.2.5" xref="S3.E1.m1.7.7.2.2.3.cmml">,</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">A</mi><mo id="S3.E1.m1.7.7.2.2.2.6" xref="S3.E1.m1.7.7.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.6.6.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.3.2" xref="S3.E1.m1.1.1a.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1a.cmml">(</mo><mtext id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">avg</mtext><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1a.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.2.2.2.7" xref="S3.E1.m1.7.7.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.7.7.2.2.2.2" xref="S3.E1.m1.7.7.2.2.2.2.cmml"><mi id="S3.E1.m1.7.7.2.2.2.2.2" xref="S3.E1.m1.7.7.2.2.2.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.2.2.2.2.1" xref="S3.E1.m1.7.7.2.2.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.2.2.2.2.3.2" xref="S3.E1.m1.2.2a.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.2.2.2.2.3.2.1" xref="S3.E1.m1.2.2a.cmml">(</mo><mtext id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">std</mtext><mo stretchy="false" id="S3.E1.m1.7.7.2.2.2.2.3.2.2" xref="S3.E1.m1.2.2a.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.7.7.2.2.2.8" xref="S3.E1.m1.7.7.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.3" xref="S3.E1.m1.7.7.3.cmml">&gt;</mo><mn id="S3.E1.m1.7.7.4" xref="S3.E1.m1.7.7.4.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7"><gt id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.3"></gt><apply id="S3.E1.m1.7.7.2.cmml" xref="S3.E1.m1.7.7.2"><times id="S3.E1.m1.7.7.2.3.cmml" xref="S3.E1.m1.7.7.2.3"></times><ci id="S3.E1.m1.7.7.2.4a.cmml" xref="S3.E1.m1.7.7.2.4"><mtext id="S3.E1.m1.7.7.2.4.cmml" xref="S3.E1.m1.7.7.2.4">accuracy</mtext></ci><vector id="S3.E1.m1.7.7.2.2.3.cmml" xref="S3.E1.m1.7.7.2.2.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑉</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑄</ci><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝐴</ci><apply id="S3.E1.m1.6.6.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1"></times><ci id="S3.E1.m1.6.6.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.2">𝐷</ci><ci id="S3.E1.m1.1.1a.cmml" xref="S3.E1.m1.6.6.1.1.1.1.3.2"><mtext id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">avg</mtext></ci></apply><apply id="S3.E1.m1.7.7.2.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2.2"><times id="S3.E1.m1.7.7.2.2.2.2.1.cmml" xref="S3.E1.m1.7.7.2.2.2.2.1"></times><ci id="S3.E1.m1.7.7.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2.2.2">𝐷</ci><ci id="S3.E1.m1.2.2a.cmml" xref="S3.E1.m1.7.7.2.2.2.2.3.2"><mtext id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">std</mtext></ci></apply></vector></apply><cn type="float" id="S3.E1.m1.7.7.4.cmml" xref="S3.E1.m1.7.7.4">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\text{accuracy}(V,Q,A,D(\text{avg}),D(\text{std}))&gt;0.5</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS6.p3.2" class="ltx_p">where <math id="S3.SS6.p3.1.m1.1" class="ltx_Math" alttext="D(\text{avg})" display="inline"><semantics id="S3.SS6.p3.1.m1.1a"><mrow id="S3.SS6.p3.1.m1.1.2" xref="S3.SS6.p3.1.m1.1.2.cmml"><mi id="S3.SS6.p3.1.m1.1.2.2" xref="S3.SS6.p3.1.m1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.2.1" xref="S3.SS6.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS6.p3.1.m1.1.2.3.2" xref="S3.SS6.p3.1.m1.1.1a.cmml"><mo stretchy="false" id="S3.SS6.p3.1.m1.1.2.3.2.1" xref="S3.SS6.p3.1.m1.1.1a.cmml">(</mo><mtext id="S3.SS6.p3.1.m1.1.1" xref="S3.SS6.p3.1.m1.1.1.cmml">avg</mtext><mo stretchy="false" id="S3.SS6.p3.1.m1.1.2.3.2.2" xref="S3.SS6.p3.1.m1.1.1a.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.1.m1.1b"><apply id="S3.SS6.p3.1.m1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.2"><times id="S3.SS6.p3.1.m1.1.2.1.cmml" xref="S3.SS6.p3.1.m1.1.2.1"></times><ci id="S3.SS6.p3.1.m1.1.2.2.cmml" xref="S3.SS6.p3.1.m1.1.2.2">𝐷</ci><ci id="S3.SS6.p3.1.m1.1.1a.cmml" xref="S3.SS6.p3.1.m1.1.2.3.2"><mtext id="S3.SS6.p3.1.m1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1">avg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.1.m1.1c">D(\text{avg})</annotation></semantics></math> means the average of the answers’ distribution, the <math id="S3.SS6.p3.2.m2.1" class="ltx_Math" alttext="D(\text{std})" display="inline"><semantics id="S3.SS6.p3.2.m2.1a"><mrow id="S3.SS6.p3.2.m2.1.2" xref="S3.SS6.p3.2.m2.1.2.cmml"><mi id="S3.SS6.p3.2.m2.1.2.2" xref="S3.SS6.p3.2.m2.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.2.m2.1.2.1" xref="S3.SS6.p3.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS6.p3.2.m2.1.2.3.2" xref="S3.SS6.p3.2.m2.1.1a.cmml"><mo stretchy="false" id="S3.SS6.p3.2.m2.1.2.3.2.1" xref="S3.SS6.p3.2.m2.1.1a.cmml">(</mo><mtext id="S3.SS6.p3.2.m2.1.1" xref="S3.SS6.p3.2.m2.1.1.cmml">std</mtext><mo stretchy="false" id="S3.SS6.p3.2.m2.1.2.3.2.2" xref="S3.SS6.p3.2.m2.1.1a.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.2.m2.1b"><apply id="S3.SS6.p3.2.m2.1.2.cmml" xref="S3.SS6.p3.2.m2.1.2"><times id="S3.SS6.p3.2.m2.1.2.1.cmml" xref="S3.SS6.p3.2.m2.1.2.1"></times><ci id="S3.SS6.p3.2.m2.1.2.2.cmml" xref="S3.SS6.p3.2.m2.1.2.2">𝐷</ci><ci id="S3.SS6.p3.2.m2.1.1a.cmml" xref="S3.SS6.p3.2.m2.1.2.3.2"><mtext id="S3.SS6.p3.2.m2.1.1.cmml" xref="S3.SS6.p3.2.m2.1.1">std</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.2.m2.1c">D(\text{std})</annotation></semantics></math> means the standard deviation of the answers’ distribution. 72,168 pictures satisfy the above formula.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>The <span id="S3.T2.3.1" class="ltx_text ltx_font_bold">AoC</span> in the table means the data with adjustment of image’s class confidences, and the <span id="S3.T2.4.2" class="ltx_text ltx_font_bold">AoDA</span> in the table means the adjustment of the distribution of answers for each question. ”M1” means method 1, it is the LXMERT, the baseline model. ”M2” means method 2, it is the Visual BERT. ”M3” means method 3, it is the UNITER, the state-of-the art model in VQA. The training set in the AesVQA database was 58,168 images, and the test set and validation set were both 7,000 images, which were randomly mixed and assigned.</figcaption>
<div id="S3.T2.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:132.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.1pt,14.7pt) scale(0.818278055108567,0.818278055108567) ;">
<table id="S3.T2.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.5.1.1.1" class="ltx_tr">
<td id="S3.T2.5.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T2.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M1</th>
<th id="S3.T2.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M1&amp;AoC</th>
<th id="S3.T2.5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M1&amp;AoDA</th>
<th id="S3.T2.5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M2</th>
<th id="S3.T2.5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M2&amp;AoC</th>
<th id="S3.T2.5.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M2&amp;AoDA</th>
<th id="S3.T2.5.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M3</th>
<th id="S3.T2.5.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M3&amp;AoC</th>
<th id="S3.T2.5.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M3&amp;AoDA</th>
</tr>
<tr id="S3.T2.5.1.2.2" class="ltx_tr">
<td id="S3.T2.5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Composition</td>
<td id="S3.T2.5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">53.4%</td>
<td id="S3.T2.5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">54.6%</td>
<td id="S3.T2.5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">54.8%</td>
<td id="S3.T2.5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">55.6%</td>
<td id="S3.T2.5.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">56.3%</td>
<td id="S3.T2.5.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">56.5%</td>
<td id="S3.T2.5.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">57.6%</td>
<td id="S3.T2.5.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">59.2%</td>
<td id="S3.T2.5.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.2.2.10.1" class="ltx_text ltx_font_bold">60.3%</span></td>
</tr>
<tr id="S3.T2.5.1.3.3" class="ltx_tr">
<td id="S3.T2.5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">Color</td>
<td id="S3.T2.5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">55.3%</td>
<td id="S3.T2.5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">57.6%</td>
<td id="S3.T2.5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">58.8%</td>
<td id="S3.T2.5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">57.2%</td>
<td id="S3.T2.5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">59.5%</td>
<td id="S3.T2.5.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">60.2%</td>
<td id="S3.T2.5.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">58.2%</td>
<td id="S3.T2.5.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">58.5%</td>
<td id="S3.T2.5.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.3.3.10.1" class="ltx_text ltx_font_bold">60.5%</span></td>
</tr>
<tr id="S3.T2.5.1.4.4" class="ltx_tr">
<td id="S3.T2.5.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t">Light</td>
<td id="S3.T2.5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">70.6%</td>
<td id="S3.T2.5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">73.5%</td>
<td id="S3.T2.5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">67.3%</td>
<td id="S3.T2.5.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">72.5%</td>
<td id="S3.T2.5.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.4.4.6.1" class="ltx_text ltx_font_bold">75.8%</span></td>
<td id="S3.T2.5.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">69.0%</td>
<td id="S3.T2.5.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">71.5%</td>
<td id="S3.T2.5.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">72.5%</td>
<td id="S3.T2.5.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">67.8%</td>
</tr>
<tr id="S3.T2.5.1.5.5" class="ltx_tr">
<td id="S3.T2.5.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">Subject</td>
<td id="S3.T2.5.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">45.9%</td>
<td id="S3.T2.5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">50.2%</td>
<td id="S3.T2.5.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">55.5%</td>
<td id="S3.T2.5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">48.3%</td>
<td id="S3.T2.5.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">53.2%</td>
<td id="S3.T2.5.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">57.1%</td>
<td id="S3.T2.5.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">49.5%</td>
<td id="S3.T2.5.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">53.9%</td>
<td id="S3.T2.5.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.5.5.10.1" class="ltx_text ltx_font_bold">58.0%</span></td>
</tr>
<tr id="S3.T2.5.1.6.6" class="ltx_tr">
<td id="S3.T2.5.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t">Genres</td>
<td id="S3.T2.5.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">48.5%</td>
<td id="S3.T2.5.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">56.6%</td>
<td id="S3.T2.5.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">54.7%</td>
<td id="S3.T2.5.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">50.3%</td>
<td id="S3.T2.5.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">58.5%</td>
<td id="S3.T2.5.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">56.5%</td>
<td id="S3.T2.5.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">52.2%</td>
<td id="S3.T2.5.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.6.6.9.1" class="ltx_text ltx_font_bold">61.2%</span></td>
<td id="S3.T2.5.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t">60.7%</td>
</tr>
<tr id="S3.T2.5.1.7.7" class="ltx_tr">
<td id="S3.T2.5.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t">Techniques</td>
<td id="S3.T2.5.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">42.4%</td>
<td id="S3.T2.5.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">49.9%</td>
<td id="S3.T2.5.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">50.3%</td>
<td id="S3.T2.5.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">45.9%</td>
<td id="S3.T2.5.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">51.2%</td>
<td id="S3.T2.5.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">53.0%</td>
<td id="S3.T2.5.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t">47.2%</td>
<td id="S3.T2.5.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t">53.6%</td>
<td id="S3.T2.5.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.7.7.10.1" class="ltx_text ltx_font_bold">54.9%</span></td>
</tr>
<tr id="S3.T2.5.1.8.8" class="ltx_tr">
<td id="S3.T2.5.1.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Subject</td>
<td id="S3.T2.5.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">51.8%</td>
<td id="S3.T2.5.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">52.6%</td>
<td id="S3.T2.5.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">53.5%</td>
<td id="S3.T2.5.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">53.9%</td>
<td id="S3.T2.5.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">54.4%</td>
<td id="S3.T2.5.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t">55.8%</td>
<td id="S3.T2.5.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">55.6%</td>
<td id="S3.T2.5.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t">57.8%</td>
<td id="S3.T2.5.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.5.1.8.8.10.1" class="ltx_text ltx_font_bold">59.4%</span></td>
</tr>
<tr id="S3.T2.5.1.9.9" class="ltx_tr">
<td id="S3.T2.5.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">All</td>
<td id="S3.T2.5.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.7%</td>
<td id="S3.T2.5.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">58.6%</td>
<td id="S3.T2.5.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">59.3%</td>
<td id="S3.T2.5.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">59.8%</td>
<td id="S3.T2.5.1.9.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">60.7%</td>
<td id="S3.T2.5.1.9.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">62.2%</td>
<td id="S3.T2.5.1.9.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">60.3%</td>
<td id="S3.T2.5.1.9.9.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">61.4%</td>
<td id="S3.T2.5.1.9.9.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T2.5.1.9.9.10.1" class="ltx_text ltx_font_bold">61.9%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>By randomly extracting 1000 pictures from each category, the confidence adjustment operation needed to be judged. The following table describes the pre-adjusted and post-adjusted range of each sub-category in the categories of genres and techniques:</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Genres</span></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Before Adjustment</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">After Adjustment</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<td id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Abstract</td>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.536</td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.536</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<td id="S3.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">Architectural</td>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.912</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.615</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<td id="S3.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">Astrophotography</td>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.990</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.653</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<td id="S3.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">Conceptual photography</td>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">0.760</td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.760</td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<td id="S3.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_t">Fashion</td>
<td id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">0.450</td>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.450</td>
</tr>
<tr id="S3.T3.1.7.6" class="ltx_tr">
<td id="S3.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_border_t">Fine-art photography</td>
<td id="S3.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t">0.784</td>
<td id="S3.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t">0.635</td>
</tr>
<tr id="S3.T3.1.8.7" class="ltx_tr">
<td id="S3.T3.1.8.7.1" class="ltx_td ltx_align_center ltx_border_t">High-speed photography</td>
<td id="S3.T3.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">0.856</td>
<td id="S3.T3.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.654</td>
</tr>
<tr id="S3.T3.1.9.8" class="ltx_tr">
<td id="S3.T3.1.9.8.1" class="ltx_td ltx_align_center ltx_border_t">Landscape</td>
<td id="S3.T3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t">0.750</td>
<td id="S3.T3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">0.653</td>
</tr>
<tr id="S3.T3.1.10.9" class="ltx_tr">
<td id="S3.T3.1.10.9.1" class="ltx_td ltx_align_center ltx_border_t">Nature</td>
<td id="S3.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_t">0.803</td>
<td id="S3.T3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t">0.664</td>
</tr>
<tr id="S3.T3.1.11.10" class="ltx_tr">
<td id="S3.T3.1.11.10.1" class="ltx_td ltx_align_center ltx_border_t">Portrait photography</td>
<td id="S3.T3.1.11.10.2" class="ltx_td ltx_align_center ltx_border_t">0.869</td>
<td id="S3.T3.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t">0.685</td>
</tr>
<tr id="S3.T3.1.12.11" class="ltx_tr">
<td id="S3.T3.1.12.11.1" class="ltx_td ltx_align_center ltx_border_t">Sports</td>
<td id="S3.T3.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t">0.427</td>
<td id="S3.T3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t">0.427</td>
</tr>
<tr id="S3.T3.1.13.12" class="ltx_tr">
<td id="S3.T3.1.13.12.1" class="ltx_td ltx_align_center ltx_border_t">Still life</td>
<td id="S3.T3.1.13.12.2" class="ltx_td ltx_align_center ltx_border_t">0.752</td>
<td id="S3.T3.1.13.12.3" class="ltx_td ltx_align_center ltx_border_t">0.645</td>
</tr>
<tr id="S3.T3.1.14.13" class="ltx_tr">
<td id="S3.T3.1.14.13.1" class="ltx_td ltx_align_center ltx_border_t">Underwater photography</td>
<td id="S3.T3.1.14.13.2" class="ltx_td ltx_align_center ltx_border_t">0.416</td>
<td id="S3.T3.1.14.13.3" class="ltx_td ltx_align_center ltx_border_t">0.416</td>
</tr>
<tr id="S3.T3.1.15.14" class="ltx_tr">
<td id="S3.T3.1.15.14.1" class="ltx_td ltx_align_center ltx_border_t">Wildlife</td>
<td id="S3.T3.1.15.14.2" class="ltx_td ltx_align_center ltx_border_t">0.947</td>
<td id="S3.T3.1.15.14.3" class="ltx_td ltx_align_center ltx_border_t">0.703</td>
</tr>
<tr id="S3.T3.1.16.15" class="ltx_tr">
<td id="S3.T3.1.16.15.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.16.15.1.1" class="ltx_text ltx_font_bold">Techniques</span></td>
<td id="S3.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_t">Before Adjustment</td>
<td id="S3.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_t">After Adjustment</td>
</tr>
<tr id="S3.T3.1.17.16" class="ltx_tr">
<td id="S3.T3.1.17.16.1" class="ltx_td ltx_align_center ltx_border_t">Bokeh</td>
<td id="S3.T3.1.17.16.2" class="ltx_td ltx_align_center ltx_border_t">0.514</td>
<td id="S3.T3.1.17.16.3" class="ltx_td ltx_align_center ltx_border_t">0.514</td>
</tr>
<tr id="S3.T3.1.18.17" class="ltx_tr">
<td id="S3.T3.1.18.17.1" class="ltx_td ltx_align_center ltx_border_t">Burst mode</td>
<td id="S3.T3.1.18.17.2" class="ltx_td ltx_align_center ltx_border_t">0.272</td>
<td id="S3.T3.1.18.17.3" class="ltx_td ltx_align_center ltx_border_t">0.452</td>
</tr>
<tr id="S3.T3.1.19.18" class="ltx_tr">
<td id="S3.T3.1.19.18.1" class="ltx_td ltx_align_center ltx_border_t">Contre-jour</td>
<td id="S3.T3.1.19.18.2" class="ltx_td ltx_align_center ltx_border_t">0.634</td>
<td id="S3.T3.1.19.18.3" class="ltx_td ltx_align_center ltx_border_t">0.634</td>
</tr>
<tr id="S3.T3.1.20.19" class="ltx_tr">
<td id="S3.T3.1.20.19.1" class="ltx_td ltx_align_center ltx_border_t">High-dynamic-range imaging</td>
<td id="S3.T3.1.20.19.2" class="ltx_td ltx_align_center ltx_border_t">0.595</td>
<td id="S3.T3.1.20.19.3" class="ltx_td ltx_align_center ltx_border_t">0.595</td>
</tr>
<tr id="S3.T3.1.21.20" class="ltx_tr">
<td id="S3.T3.1.21.20.1" class="ltx_td ltx_align_center ltx_border_t">Holography</td>
<td id="S3.T3.1.21.20.2" class="ltx_td ltx_align_center ltx_border_t">0.402</td>
<td id="S3.T3.1.21.20.3" class="ltx_td ltx_align_center ltx_border_t">0.402</td>
</tr>
<tr id="S3.T3.1.22.21" class="ltx_tr">
<td id="S3.T3.1.22.21.1" class="ltx_td ltx_align_center ltx_border_t">Long-exposure photography</td>
<td id="S3.T3.1.22.21.2" class="ltx_td ltx_align_center ltx_border_t">0.308</td>
<td id="S3.T3.1.22.21.3" class="ltx_td ltx_align_center ltx_border_t">0.502</td>
</tr>
<tr id="S3.T3.1.23.22" class="ltx_tr">
<td id="S3.T3.1.23.22.1" class="ltx_td ltx_align_center ltx_border_t">Macro photography</td>
<td id="S3.T3.1.23.22.2" class="ltx_td ltx_align_center ltx_border_t">0.869</td>
<td id="S3.T3.1.23.22.3" class="ltx_td ltx_align_center ltx_border_t">0.652</td>
</tr>
<tr id="S3.T3.1.24.23" class="ltx_tr">
<td id="S3.T3.1.24.23.1" class="ltx_td ltx_align_center ltx_border_t">Multiple exposure</td>
<td id="S3.T3.1.24.23.2" class="ltx_td ltx_align_center ltx_border_t">0.894</td>
<td id="S3.T3.1.24.23.3" class="ltx_td ltx_align_center ltx_border_t">0.675</td>
</tr>
<tr id="S3.T3.1.25.24" class="ltx_tr">
<td id="S3.T3.1.25.24.1" class="ltx_td ltx_align_center ltx_border_t">Night photography</td>
<td id="S3.T3.1.25.24.2" class="ltx_td ltx_align_center ltx_border_t">0.613</td>
<td id="S3.T3.1.25.24.3" class="ltx_td ltx_align_center ltx_border_t">0.613</td>
</tr>
<tr id="S3.T3.1.26.25" class="ltx_tr">
<td id="S3.T3.1.26.25.1" class="ltx_td ltx_align_center ltx_border_t">Panning</td>
<td id="S3.T3.1.26.25.2" class="ltx_td ltx_align_center ltx_border_t">0.597</td>
<td id="S3.T3.1.26.25.3" class="ltx_td ltx_align_center ltx_border_t">0.597</td>
</tr>
<tr id="S3.T3.1.27.26" class="ltx_tr">
<td id="S3.T3.1.27.26.1" class="ltx_td ltx_align_center ltx_border_t">Panoramic photography</td>
<td id="S3.T3.1.27.26.2" class="ltx_td ltx_align_center ltx_border_t">0.749</td>
<td id="S3.T3.1.27.26.3" class="ltx_td ltx_align_center ltx_border_t">0.678</td>
</tr>
<tr id="S3.T3.1.28.27" class="ltx_tr">
<td id="S3.T3.1.28.27.1" class="ltx_td ltx_align_center ltx_border_t">Stereoscopy</td>
<td id="S3.T3.1.28.27.2" class="ltx_td ltx_align_center ltx_border_t">0.875</td>
<td id="S3.T3.1.28.27.3" class="ltx_td ltx_align_center ltx_border_t">0.658</td>
</tr>
<tr id="S3.T3.1.29.28" class="ltx_tr">
<td id="S3.T3.1.29.28.1" class="ltx_td ltx_align_center ltx_border_t">Time-lapse photography</td>
<td id="S3.T3.1.29.28.2" class="ltx_td ltx_align_center ltx_border_t">0.605</td>
<td id="S3.T3.1.29.28.3" class="ltx_td ltx_align_center ltx_border_t">0.605</td>
</tr>
<tr id="S3.T3.1.30.29" class="ltx_tr">
<td id="S3.T3.1.30.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Vignetting</td>
<td id="S3.T3.1.30.29.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.634</td>
<td id="S3.T3.1.30.29.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.634</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2208.05798/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="423" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>The pictures on the left show that these pictures have more biased attributes, which can easily cause the model to fall into overfitting; the pictures on the right have appropriate ”controversial” labels, which can allow the model to obtain better training results.</figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7. </span>Influence from Artificial labels</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Complete automatic program marking is not considered to be reliable, and there may also be cases of inconsistency with the facts in automatic program marking. Therefore, we selected 7000 pictures for manual marking. When the result of manual marking does not match the result of machine marking, the picture will be corrected to the result of manual marking.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">Another important significance of manually marked pictures is to determine whether the distribution of answers of this type conforms to the real situation. Taking the composition category as an example, the number of symmetrical pictures is usually much lower than that of the rule of thirds and the central composition. This feature can be reflected in the sampled pictures and all pictures. Considering the huge workload of manual marking, we only added manual markings to the composition, color, scene, and lighting attributes. Genres and techniques are more complicated and prone to controversy, so they are not marked.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In the experiment, we used three models to train and test each category. The ratio of the training set and validation set is 8:1, and the number of test sets is the same as the validation set. To test the influence of data distribution on the model, we constantly adjusted the proportions of each sub-category during training and obtained more balanced training results.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The data distribution without artificial interference comes from all pictures. Such a data distribution will not be conducive to the training of the neural network. The operation of adjusting the data set mainly includes: adjusting the confidence threshold of each sub-category in a large category, and the distribution of the question and answer results of the pictures in each large category. The number of pictures is limited by the different types of pictures that have different requirements in the selection process, so it is impossible to achieve close results. The results of the comparative experiment with AoC (Adjustment of Confidence) and AoDA (Adjustment of the Distribution of Answers) are shown in Table 2. It can be seen that UNITER, based on multiple datasets has achieved the best results in AesVQA.</p>
</div>
<section id="S4.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Adjustment of Confidence</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Taking genres and techniques as examples, we adjusted the confidence of different categories through training and test results, called it adjustment of confidence (AoC). For different genres and techniques, it is generally believed that the judgment criteria are also different, which is reflected in the labeling process with unsupervised learning as the main method, and the confidence level needs to be adjusted according to the specific class. As is shown in Table 3, The adjusted result is artificially limited to the range of 0.3 to 0.7, this range will help the training and prediction of the model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Adjustment of Distribution of Answers</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Limited to automatic labeling, our proposed dataset requires multiple models to label images. To prevent the occurrence of some over-fitting phenomena, ten answers are set for each question. For adjusting the distribution of answers (AoDA), we let questions have no controversial answers and salient answers.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The salient answers mean that it is obvious to know which is the right choice in all options. The controversial answers mean there are two or more answers with high confidence. For example, a picture may be classified in ”landscape” and ”wildlife” at the same time, such a picture needs to be removed, as shown in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3. Basic Aesthetic Labels ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The adjustment of the distribution of answers will help eliminate images in the data set that lack diversity in labels. These images will affect the model’s preference and make the model more prone to overfitting. The example in the Figure <a href="#S3.F7" title="Figure 7 ‣ 3.6. Bias of Data ‣ 3. Aesthetic Visual Question Answering ‣ Aesthetic Visual Question Answering of Photographs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates which pictures should be discarded and which should be kept during adjustment.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose and solve a new task of aesthetic quality evaluation: VQA of image aesthetics. This paper constructs the VQA dataset for image aesthetics. We get basic image aesthetics labels, genres labels, and techniques labels from photography, and subjective emotional labels, and transfer these labels to question-answers pairs. We used three advanced VQA models for training and testing and obtained further performance improvement by adjusting the confidence in image classification and the distribution of answers in VQA. It can be proved by experiments that exporting the answer to aesthetic questions and answers is available by training on our datasets. As a result, display performance model and the advantage of databases, which filled in the blank in the field of aesthetics in the VQA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv
Batra, Devi Parikh, and Aniruddha
Kembhavi. 2018.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming
priors for visual question answering. In
<em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 4971–4980.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Jeffrey P Bigham,
Chandrika Jayant, Hanjie Ji,
Greg Little, Andrew Miller,
Robert C Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White,
Samual White, et al<span id="bib.bib3.3.1" class="ltx_text">.</span>
2010.

</span>
<span class="ltx_bibblock">Vizwiz: nearly real-time answers to visual
questions. In <em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 23nd annual ACM
symposium on User interface software and technology</em>.
333–342.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blei et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2003)</span>
<span class="ltx_bibblock">
David M Blei, Andrew Y
Ng, and Michael I Jordan.
2003.

</span>
<span class="ltx_bibblock">Latent dirichlet allocation.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Journal of machine Learning research</em>
3, Jan (2003),
993–1022.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Kuang-Yu Chang, Kung-Hung
Lu, and Chu-Song Chen. 2017.

</span>
<span class="ltx_bibblock">Aesthetic critiques generation for photos. In
<em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on
computer vision</em>. 3514–3523.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li,
Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.
2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation
learning. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>. Springer, 104–120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao,
Jie Zhou, Zhiheng Huang,
Lei Wang, and Wei Xu.
2015.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods
for multilingual image question.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 28 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Noa Garcia, Chentao Ye,
Zihua Liu, Qingtao Hu,
Mayu Otani, Chenhui Chu,
Yuta Nakashima, and Teruko Mitamura.
2020.

</span>
<span class="ltx_bibblock">A dataset and baselines for visual question
answering on art. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">European Conference on
Computer Vision</em>. Springer, 92–108.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot,
Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role
of Image Understanding in Visual Question Answering. In
<em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern
Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A Hudson and
Christopher D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 6700–6709.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xin Jin, Le Wu,
Geng Zhao, Xiaodong Li,
Xiaokun Zhang, Shiming Ge,
Dongqing Zou, Bin Zhou, and
Xinghui Zhou. 2019.

</span>
<span class="ltx_bibblock">Aesthetic attributes assessment of images. In
<em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM International
Conference on Multimedia</em>. 311–319.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Shu Kong, Xiaohui Shen,
Zhe Lin, Radomir Mech, and
Charless Fowlkes. 2016.

</span>
<span class="ltx_bibblock">Photo aesthetics ranking network with attributes
and content adaptation. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">European conference on
computer vision</em>. Springer, 662–679.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu,
Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz,
Stephanie Chen, Yannis Kalantidis,
Li-Jia Li, David A Shamma,
et al<span id="bib.bib13.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using
crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.4.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>
123, 1 (2017),
32–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark
Yatskar, Da Yin, Cho-Jui Hsieh, and
Kai-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for
vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin,
Chunyuan Li, Pengchuan Zhang,
Xiaowei Hu, Lei Zhang,
Lijuan Wang, Houdong Hu,
Li Dong, Furu Wei, et al<span id="bib.bib15.3.1" class="ltx_text">.</span>
2020.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for
vision-language tasks. In <em id="bib.bib15.4.1" class="ltx_emph ltx_font_italic">European Conference on
Computer Vision</em>. Springer, 121–137.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Xin Lu, Zhe Lin,
Hailin Jin, Jianchao Yang, and
James Z Wang. 2015.

</span>
<span class="ltx_bibblock">Rating image aesthetics using deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>
17, 11 (2015),
2021–2034.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murray et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Naila Murray, Luca
Marchesotti, and Florent Perronnin.
2012.

</span>
<span class="ltx_bibblock">AVA: A large-scale database for aesthetic visual
analysis. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">2012 IEEE conference on computer
vision and pattern recognition</em>. IEEE, 2408–2415.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook
Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark,
et al<span id="bib.bib18.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural
language supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.00020</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros,
and Richard Zemel. 2015.

</span>
<span class="ltx_bibblock">Exploring models and data for image question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 28 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek
Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra,
Devi Parikh, and Marcus Rohrbach.
2019.

</span>
<span class="ltx_bibblock">Towards vqa models that can read. In
<em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 8317–8326.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Alane Suhr, Mike Lewis,
James Yeh, and Yoav Artzi.
2017.

</span>
<span class="ltx_bibblock">A corpus of natural language for visual reasoning.
In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers)</em>.
217–223.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Damien Teney, Ehsan
Abbasnejad, and Anton van den Hengel.
2020.

</span>
<span class="ltx_bibblock">Unshuffling data for improved generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.11894</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xinyu Wang, Yuliang Liu,
Chunhua Shen, Chun Chet Ng,
Canjie Luo, Lianwen Jin,
Chee Seng Chan, Anton van den Hengel,
and Liangwei Wang. 2020.

</span>
<span class="ltx_bibblock">On the general value of evidence, and bilingual
scene-text visual question answering. In
<em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 10126–10135.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Zhou Wang, Alan C Bovik,
Hamid R Sheikh, and Eero P Simoncelli.
2004.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to
structural similarity.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on image processing</em>
13, 4 (2004),
600–612.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Licheng Yu, Eunbyung
Park, Alexander C Berg, and Tamara L
Berg. 2015.

</span>
<span class="ltx_bibblock">Visual madlibs: Fill in the blank description
generation and question answering. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the ieee international conference on computer vision</em>.
2461–2469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Rowan Zellers, Yonatan
Bisk, Roy Schwartz, and Yejin Choi.
2018.

</span>
<span class="ltx_bibblock">SWAG: A Large-Scale Adversarial Dataset for
Grounded Commonsense Inference. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing</em>.
93–104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth,
Michael Bernstein, and Li Fei-Fei.
2016.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.
In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>. 4995–5004.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.05797" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.05798" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.05798">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.05798" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.05801" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 18:21:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
