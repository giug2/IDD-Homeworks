<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly.</title>
<!--Generated on Tue Jul 30 15:33:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.20910v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S1" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Motivation: Existing stance detection methods fall short</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS1" title="In 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Need for Granularity (R1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS2" title="In 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Need for Claim Invariancy (R2)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS3" title="In 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Need for contrastive context awareness (R3)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Contrastive Textual Deviation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.SS1" title="In 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.SS2" title="In 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Bootstrapping CTD using LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS1" title="In 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Fine tuning FLAN-T5 for CTD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS2" title="In 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison with existing baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS3" title="In 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model size and performance tradeoff</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS4" title="In 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Integrating CTD into Lambretta</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S6" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S7" title="In Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\mdfdefinestyle</span>
<p class="ltx_p" id="p1.2">MyFrame
linecolor=black,
outerlinewidth=0.5pt,
roundcorner=5pt,
innertopmargin=2pt,
innerbottommargin=2pt,
innerrightmargin=-10pt,
innerleftmargin=-15pt,
leftmargin = -10pt,
rightmargin = -10pt,
backgroundcolor=gray!10,
userdefinedwidth=220pt



</p>
</div>
<h1 class="ltx_title ltx_title_document">Enabling Contextual Soft Moderation on Social Media
<br class="ltx_break"/>through Contrastive Textual Deviation <span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pujan Paudel, Mohammad Hammas Saeed, Rebecca Auger, Chris Wells, and Gianluca Stringhini
<br class="ltx_break"/>Boston University
<br class="ltx_break"/>{ppaudel,hammas,raauger,cfwells,gian}@bu.edu
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Automated soft moderation systems are unable to ascertain if a post supports or refutes a false claim, resulting in a large number of contextual false positives.
This limits their effectiveness, for example undermining trust in health experts by adding warnings to their posts or resorting to vague warnings instead of granular fact-checks, which result in desensitizing users.
In this paper, we propose to incorporate stance detection into existing automated soft-moderation pipelines, with the goal of ruling out contextual false positives and providing more precise recommendations for social media content that should receive warnings.
We develop a textual deviation task called Contrastive Textual Deviation (CTD) and show that it outperforms existing stance detection approaches when applied to soft moderation.
We then integrate CTD into the state-of-the-art system for automated soft moderation Lambretta, showing that our approach can reduce contextual false positives from 20% to 2.1%, providing another important building block towards deploying reliable automated soft moderation tools on social media.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="S1.F1.sf1.g1" src="extracted/5764336/figs/moderation_correct.png" width="698"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S1.F1.sf1.3.2" style="font-size:90%;">Correct application of warning label.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="159" id="S1.F1.sf2.g1" src="extracted/5764336/figs/moderation_topical_falsepositive.png" width="698"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S1.F1.sf2.3.2" style="font-size:90%;">Topical False Positive.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S1.F1.sf3.g1" src="extracted/5764336/figs/moderation_contextual_falsepositive.png" width="698"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S1.F1.sf3.3.2" style="font-size:90%;">Contextual False Positive.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Three tweets discussing the debunked claim that COVID-19 is caused by 5G. Existing moderation systems might suffer from topical false positives as well as contextual false positives.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">“<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">Get the facts about COVID-19</em>.”
This message became a familiar sight for the users of Twitter during the pandemic surge in 2020.
Added as a warning label by the platform to accompany potentially misleading tweets about COVID-19 (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1(a)</span></a>), its goal was to allow users to seek more information and not fall for dangerous conspiracy theories and misinformation which could have compromised public safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib73" title="">73</a>]</cite>.
This was an early example of <em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">soft moderation</em>, which was later applied by Twitter on other important topics (e.g., the 2020 US Presidential election <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib83" title="">83</a>]</cite>) and has since been adopted by other platforms like Facebook <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib21" title="">21</a>]</cite>, Instagram <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib9" title="">9</a>]</cite>, and TikTok <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Platforms do not release details on how they identify tweets that need to receive soft moderation labels, but this includes getting input from reputable authorities like fact-checking organizations (e.g., Snopes) and public agencies (e.g., the CDC), applying automated tools like semantic matching and information retrieval to identify posts about known misleading claims, and relying on human moderators to vet content that should receive a warning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib74" title="">74</a>]</cite>.
Despite these efforts, however, recent work has showed that the methods adopted by online platforms fail to flag a large amount of misleading content that should be moderated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib83" title="">83</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Fully automated attempts by platforms to identify misleading content had shortcomings.
We illustrate an example of this in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1</span></a>.
To curb the spread of a conspiracy theory linking 5G technology to the spread of COVID-19, Twitter decided to add a warning label to any tweet that contained the words “<em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">5G</em>” and “<em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">oxygen</em>” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib31" title="">31</a>]</cite>.
Due to the generality of these keywords, this caused the platform to flag tweets that were unrelated to the conspiracy theory, like the one in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1(b)</span></a>.
We call these irrelevant tweets mistakenly flagged for moderation <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">topical false positives</em>.
In addition to irrelevant tweets, Twitter also applied warning labels to tweets that debunked the conspiracy theory, like the one in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1(c)</span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib82" title="">82</a>]</cite>.
This can have adverse effects, for example by having the platform undermine the public trust in experts (e.g., health professionals) by mistakenly flagging their informative posts as misleading <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib37" title="">37</a>]</cite>.
It would also discourage diligent users who actively debunk misinformation on social media (i.e., “wisdom of the crowds” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib43" title="">43</a>]</cite>), who would see their fact-checks marked as misleading.
Finally, it can cause <em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">warning fatigue</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib45" title="">45</a>]</cite> in social media users, where users are bombarded with warning labels attached to the posts and stop paying attention to them, thereby reducing the intended effectiveness of the labels.
We call this type of mistakenly flagged posts <em class="ltx_emph ltx_font_italic" id="S1.p3.1.5">contextual false positives</em>.
We argue that an effective approach for the automated identification of content that should receive moderation should address both topical and contextual false positives.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recently, the computer security community has focused on developing automated systems to flag content that should receive moderation labels.
Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite> is a system that leverages information retrieval techniques (i.e., Learning to Rank) to identify the optimal set of keywords that represent a misleading claim and identify social media posts that are discussing that claim.
Lambretta outperforms alternative keyword retrieval and semantic search approaches, and largely addresses the problems of topical false positives, reporting a false detection rate of 3.93% on a dataset of tweets discussing the 2020 US Presidential Election.
However, Lambretta does not address the problem of contextual false positives, being unable to ascertain if a social media post is supporting a false claim (and therefore should be moderated) or refuting it.
In fact, the false detection rate from the perspective of contextual false positives on its original dataset is 20%.
This limitation is not unique to Lambretta, but is also present in other content moderation systems powered by retrieval or semantic similarity approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To reduce contextual false positives, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">stance detection</em> techniques can be integrated into soft moderation systems.
Stance detection is a task where the objective is to learn a model that can identify whether a given piece of text is in favor of or against a set of target(s) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib1" title="">1</a>]</cite>.
In the context of fact-checking, the problem aims to automatically detect if a piece of text supports or refutes a misleading claim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib23" title="">23</a>]</cite>.
We find, however, that existing stance detection techniques are limited in their ability to generalize across unseen claims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib70" title="">70</a>]</cite> and different social media platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib40" title="">40</a>]</cite>, making them unfit to be applied on social media platforms at scale.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address the shortcomings of existing stance detection approaches when dealing with content moderation on social media, we reframe the problem as a textual deviation detection one and propose <em class="ltx_emph ltx_font_italic" id="S1.p6.1.1">Contrastive Textual Deviation</em> (CTD).
Our approach consists of developing, for each claim that we wish to moderate, a triplet composed of a <span class="ltx_text" id="S1.p6.1.2" style="color:#0000FF;">consensus statement</span> that is confirmed to be true (e.g., by health authorities or fact-checking organizations) and two contrastive markers, a piece of <span class="ltx_text" id="S1.p6.1.3" style="color:#FF0000;">refuting evidence</span> and one of <span class="ltx_text" id="S1.p6.1.4" style="color:#228B22;">supporting evidence</span>.
Our approach then leverages the <em class="ltx_emph ltx_font_italic" id="S1.p6.1.5">emergent</em> abilities of Large Language Models (LLMs) like zero-shot learning to identify if a piece of text supports or refutes the consensus statement.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Compared to conventional stance detection approaches, our method has two major advantages.
First, we build a model that can identify patterns of deviation from an anchor text (i.e., the <span class="ltx_text" id="S1.p7.1.1" style="color:#0000FF;">consensus statement</span>) in a topic-invariant fashion, without depending on features specific to a certain topic (e.g., climate change denial or vaccine misinformation).
This overcomes one of the main issues of traditional supervised stance detection pipelines, where researchers need to annotate samples for each claim as these methods fail to generalize well across multiple claims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib70" title="">70</a>]</cite>.
Second, CTD can leverage the consensus statements curated by fact checkers and expert authorities as part of the stance detection process (through the use of the <span class="ltx_text" id="S1.p7.1.2" style="color:#0000FF;">consensus statement</span>), allowing us to keep experts in the loop in the case of evolving events like a global pandemic or misinformation stories that threaten public safety.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">We first motivate the task of textual deviation detection by identifying the shortcomings of existing approaches for stance detection in the context of content moderation.
We then perform preliminary experiments on the advantages of reframing stance detection as textual deviation by bootstrapping the task using the zero-shot learning capabilities of Large Language Models (LLMs).
Motivated by the better performance of LLMs over alternative solutions and by the ease of generalizing across datasets, we further fine-tune the LLM for the task of CTD to boost its performance.
Finally, we integrate our fine-tuned unsupervised stance detection model into the analysis pipeline of the state-of-the-art soft moderation system Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>, to improve its detection capabilities.
We make all the labeled datasets curated in the work and fine-tuned models publicly available <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/collections/ppaudel/contrastive-textual-deviation-65e20c48680724cc9a809062" title="">https://huggingface.co/collections/ppaudel/contrastive-textual-deviation-65e20c48680724cc9a809062</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">In summary, this paper makes the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose Contrastive Textual Deviation (CTD) as a new task to perform stance detection on social media.
We show that CTD overcomes the limitations of existing stance detection approaches and that it generalizes across platforms by testing it on datasets collected from different platforms (Twitter and Reddit) and covering different topics (COVID-19, climate change, and politics).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We fine-tune a LLM dedicated to the task of CTD and show that our approach improves over baseline methods, with an average improvement on the F1-score of over 7%.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We integrate CTD into the automated soft moderation pipeline of the state-of-the-art approach Lambretta, reducing contextual false positives from 20% to 2.1%, with a minimum impact on false negatives.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We curate and collect multiple datasets throughout our work that i) come from multiple social media platforms (Reddit, Twitter), ii) cover different topics (e.g, Climate Change, COVID-19), and iii) contain different levels of granularity (i.e, fine-grained or coarse-grained claims).
A summary of the datasets used in our work is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T1" title="Table 1 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.2">
<tr class="ltx_tr" id="S2.T1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.2.1">Platform</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.3.1">Topic</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.4.1">#Claims</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.5.1">#Refute</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.1.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.6.1">#Support</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.2.1">GWSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib38" title="">38</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.2">News</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.3">Climate</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.4">N/A</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.5">400</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.6">777</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.3.1">Climate Skepticism</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.3.2">Reddit</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.3.3">Climate</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.3.4">3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.3.5">1,277</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.3.6">1,650</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.4.1">COVID-FACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib66" title="">66</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.4.2">Reddit</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.4.3">COVID-19</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.4.4">4,086</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.4.5">2,790</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.4.6">1,296</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.5.1">COVID-CQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.5.2">Twitter</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.5.3">COVID-19</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.5.4">1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.5.5">3,488</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.5.6">3,515</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.6.1">Stanceosaurus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib85" title="">85</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.6.2">Twitter</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.6.3">Fact-Checking</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.6.4">190</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.6.5">1,442</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.6.6">3,025</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.7.1">Election Denial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.7.2">Twitter</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.7.3">Election</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.7.4">3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.7.5">128</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.7.6">454</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.8.1">PERSPECTRUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib14" title="">14</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.8.2">Debates</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.8.3">Argument Mining</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.8.4">907</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.8.5">2,468</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.8.6">2,627</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.4.2" style="font-size:90%;">Summary of datasets used.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Global Warming Stance Dataset (GWSD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib38" title="">38</a>]</cite>.</span>
This dataset contains 1,177 “opinion spans” of news headlines about Global Warming annotated by human annotators as either accepting of global warming (e.g.,
“<em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">We can’t afford to wait until everyone is feeling the pain of the climate emergency before we do something about it</em>”) or skeptical of it (e.g., “<em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">Humans have negligible impact on the climate</em>”).
The opinion spans are extracted from 56,000 news articles spanning a period of 20 years from news outlets such as the New York Times, Fox, Washington Post, Forbes, etc.
We characterize this dataset as a “coarse-grained” stance dataset since the opinion spans in this dataset discuss climate change from the broader perspective of climate emergency and skepticism, and do not deal with granular causes or effects of climate change (e.g., <em class="ltx_emph ltx_font_italic" id="S2.p2.1.4">Urban Heat Island effect on Global Warming is negligible</em>.)
This dataset is slightly imbalanced in its class labels as it contains 777 opinion spans supporting climate change
and 400 opinion spans skeptical of climate change.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Climate Skepticism.</span>
The majority of stance detection datasets are topic-based or target-based in nature, with a very limited set of datasets on claim-based stance detection occurring on social media text.
This existing gap motivates us to prepare a comprehensive claim-based stance detection dataset.
We curate a dataset of climate change denial discussions, containing Reddit posts that support or refute three different claims related to different arguments used in climate change discussions.
We query the Pushshift Reddit dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib7" title="">7</a>]</cite> with curated keywords related to the claims, retrieving posts from Reddit that are discussing narratives related to three of the most popular climate denier claims: i) <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">Cosmic rays are causing global warming (GW)</em>, ii) <em class="ltx_emph ltx_font_italic" id="S2.p3.1.3">Antarctica is gaining ice</em>, and iii) <em class="ltx_emph ltx_font_italic" id="S2.p3.1.4">Urban Heat island (UHI) effect exaggerate global warming trends</em>.
These claims are fine-grained and objective in nature, unlike high-level “topics” in the GWSD dataset such as “Global warming is a hoax,” or “Climate change is not happening.” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib38" title="">38</a>]</cite>
For the goal of scalable and tractable soft moderation, these claims are very representative of misleading claims propagating on social media that can be easily refuted from scientific consensus, or authoritative sources (e.g. Skeptical Science).
We then set out to annotate 1,000 Reddit posts for each claim by developing a codebook dedicated to climate skepticism.
Our codebook was informed by the crowdsourced resource Skeptical Science, which provides pointers for understanding different narratives used by climate skeptics to deny a claim as well as examples of scientific support for confirmed claims.
We do not include posts that are inquiring about the claim in question, are neutral towards it, or are topically irrelevant.
Two researchers performed multiple rounds of annotation and reached a near-perfect agreement of <math alttext="\kappa" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_κ</annotation></semantics></math> = 0.865 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib41" title="">41</a>]</cite>) Cohen’s Kappa (i.e., strong agreement).
This dataset also has value in evaluating the transferability of stance detection approaches as it contains multiple targets (claims), and different types of label distribution within the claims.
A summary of the climate denial claims and the number of annotated Reddit posts for each claim is provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T2" title="Table 2 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tr class="ltx_tr" id="S2.T2.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1">Claim</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.2.1">#Refute</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.3.1">#Support</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.2.1">Cosmic Rays cause GW</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.2.2">577</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.2.3">439</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.3.1">UHI effect exaggerate GW trends</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.3.2">390</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.3.3">495</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.4.1">Antarctica is gaining ice</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.4.2">683</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.4.3">343</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.4.2" style="font-size:90%;">Summary of climate skeptic claims annotated with stance labels.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">COVID-FACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib66" title="">66</a>]</cite>.</span>
This dataset belongs to a family of datasets in the field of automated fact-checking known as FEVER (Fact Extraction and Verification).
What makes it interesting for our work is that the FEVER dataset contains labeled text known as “evidences” that either supports or refutes a given claim.
More precisely, the COVID-FACT dataset contains 4,086 claims concerning the COVID-19 pandemic, and both refuting (3,488) and supporting evidence (3,515) for the claims.
These claims are obtained from the <span class="ltx_text ltx_font_italic" id="S2.p4.1.2">/r/COVID19</span> community of Reddit and are “fine-grained” in nature.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">COVID-CQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib46" title="">46</a>]</cite>.</span>
In addition to creating our own dataset for identifying stance in climate denial claims, we also make use of an existing stance detection dataset of COVID-19 treatment tweets.
The dataset, called COVID-CQ, contains 7,003 tweets and their respective stance on the efficacy of hydroxychloroquine as a treatment for COVID-19.
The annotation criteria of this work closely align with the annotation criteria of our own data collection process, and this dataset fits the problem definition of claim-based stance detection very closely.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p" id="S2.p6.1"><span class="ltx_text ltx_font_bold" id="S2.p6.1.1">Stanceosaurus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib85" title="">85</a>]</cite>.</span>
This dataset is a multi-lingual and multi-cultural corpus of tweets annotated with a stance towards 4,467 tweets discussing 190 misinformation claims from 9 fact-checking sources (Snopes, Poynter, FullFact, etc.)
We use a subset of the dataset containing fact-checked claims and tweets in English, which either support or refute the claims, ignoring the tweets that are either <span class="ltx_text ltx_font_italic" id="S2.p6.1.2">irrelevant</span> to the claim, or simply <span class="ltx_text ltx_font_italic" id="S2.p6.1.3">querying</span> about the claim.
For the posts that are <span class="ltx_text ltx_font_italic" id="S2.p6.1.4">discussing</span> the claims in question, the dataset also contains annotation if the tweets are leaning towards supporting or refuting the claim, and we use tweets that are annotated as such with the label of their leaning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold" id="S2.p7.1.1">Election denial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>.</span>
This dataset contains 499 election denial claims and 101,353 tweets discussing the claims retrieved by the corresponding soft-moderation system Lambretta.
We select the three most popular election denial claims by frequency present in the dataset and annotate a sample of 200 random tweets discussing the three claims for the stance of the tweets concerning the claim in question.
We adapt the codebook and annotation process used for annotating climate denial-related claims.
A near-perfect agreement of <math alttext="\kappa" class="ltx_Math" display="inline" id="S2.p7.1.m1.1"><semantics id="S2.p7.1.m1.1a"><mi id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><ci id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="S2.p7.1.m1.1d">italic_κ</annotation></semantics></math> = 0.866 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib41" title="">41</a>]</cite> was reached among the annotators.
A summary of the claims, and the number of supporting, and refuting tweets for each claim is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T3" title="Table 3 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T3.2" style="width:227.3pt;height:57.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.4pt,7.2pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.2.1">
<tr class="ltx_tr" id="S2.T3.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.2.1.1.1.1">Claim</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.2.1.1.2.1">#Refute</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T3.2.1.1.3.1">#Support</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.2.1">Wisconsin Voter Turnout above 90%</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.2.2">132</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.2.3">32</td>
</tr>
<tr class="ltx_tr" id="S2.T3.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.3.1">Illegal suitcase of ballots in Georgia</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.3.2">161</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.2.1.3.3">55</td>
</tr>
<tr class="ltx_tr" id="S2.T3.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.4.1">Dead Voters voted in Michigan</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T3.2.1.4.2">161</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T3.2.1.4.3">41</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S2.T3.4.2" style="font-size:90%;">Summary of election denial claims from  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite> annotated with stance labels</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p8">
<p class="ltx_p" id="S2.p8.1"><span class="ltx_text ltx_font_bold" id="S2.p8.1.1">PERSPECTRUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib14" title="">14</a>]</cite>.</span>
Finally, we use a dataset from argument mining called PERSPECTRUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib14" title="">14</a>]</cite>.
PERSPECTRUM contains 907 claims from online debate topics and 5,095 “perspectives” from search engine results presenting diversifying viewpoints about the claims.
These “perspectives” are annotated with stance as either supporting or refuting the claim and cover more than 10 different topics such as Politics, Freedom of Speech, Environment, Science, Health etc.
The structure of this dataset provides value in augmenting a large-scale stance dataset as it contains claims and multiple sides (perspectives) of the claims from different topics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p9">
<p class="ltx_p" id="S2.p9.1"><span class="ltx_text ltx_font_bold" id="S2.p9.1.1">Summary.</span> A summary of all the datasets curated and collected is listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T1" title="Table 1 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">1</span></a>.
We can observe that the datasets used by our work span across different use cases of misinformation such as climate denial, public health emergencies, civic processes such as elections, and general purpose fact-checking.
This way, we aim to comprehensively evaluate our method on a variety of misleading claims occurring across two different social media platforms.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Motivation: Existing stance detection methods fall short</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We identify three requirements that effective stance detection approaches should satisfy to successfully improve content moderation systems.
Ideally, we would be able to leverage existing stance detection methods for this purpose.
However, we find that previous work, including more sophisticated entailment-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib22" title="">22</a>]</cite>, fall short in achieving one or more of these requirements.
In a nutshell, the requirements and shortcomings that we identify are the following:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<svg class="ltx_picture" height="88.36" id="S3.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,88.36) matrix(1 0 0 -1 0 0)"><g fill="#BF0000" fill-opacity="1.0"><path d="M 0 5.91 L 0 82.46 C 0 85.72 2.64 88.36 5.91 88.36 L 594.09 88.36 C 597.36 88.36 600 85.72 600 82.46 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 82.46 C 1.97 84.63 3.73 86.4 5.91 86.4 L 594.09 86.4 C 596.27 86.4 598.03 84.63 598.03 82.46 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="60.81" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_itemize" id="S3.I1">
<span class="ltx_item" id="S3.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I1.i1.p1">
<span class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">R1. Need for Granularity</span>: Supervised methods trained on coarse-grained claims fail on fine-grained claims.</span>
</span></span>
<span class="ltx_item" id="S3.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I1.i2.p1">
<span class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">R2. Need for Claim Invariancy</span>: Supervised methods trained on one claim do not generalize well on other claims.</span>
</span></span>
<span class="ltx_item" id="S3.I1.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I1.i3.p1">
<span class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">R3. Need for Contrastive Context Awareness</span>: Supervised entailment-based methods fail to identify stance on social media despite being given context-aware hypothesis statements.</span>
</span></span>
</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">In the rest of this section, we discuss the shortcomings of existing approaches in achieving the three requirements, showcasing our preliminary experiments.
Then, based on these observations, in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4" title="4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a> we present our solution, which overcomes the limitations of previous methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Need for Granularity (R1)</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">While coarse-grained claims (e.g., “<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">Climate change isn’t real</em>”) may generally describe broad categories of misinformation, finer-grained ones (e.g., “<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">Cosmic rays are causing global warming</em>”) are often encountered online.
An effective stance detection approach should be able to operate on both categories of claims.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To investigate the ability of stance detection to generalize between coarse-grained and fine-grained claims, we use the GWSD and Climate Skepticism datasets.
These two datasets both pertain to climate change, but GWSD contains coarse-grained claims while the Climate Skepticism one is built from fine-grained ones.
We train a supervised model on the GWSD dataset and test whether this model can identify the stance of climate skepticism on the fine-grained claims of the Climate Skepticism dataset.
Following the standard BERT fine-tuning recipe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib64" title="">64</a>]</cite>, we fine-tune a DistilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib68" title="">68</a>]</cite> model for five epochs with a learning rate of <math alttext="5e^{-5}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">5</mn><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml"><mo id="S3.SS1.p2.1.m1.1.1.3.3a" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS1.p2.1.m1.1.1.3.3.2" xref="S3.SS1.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><cn id="S3.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.2">5</cn><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3"><minus id="S3.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3"></minus><cn id="S3.SS1.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">5e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">5 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> until the training loss converges.
We balance the dataset by randomly undersampling the supporting claims, as the original dataset is imbalanced.
We repeat the experiment five times and report the average evaluation of the fine-tuned BERT model on the three claims from the Climate Skepticism dataset.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.2">
<tr class="ltx_tr" id="S3.T4.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1">Claim</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.2.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.2.1">F1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.2.1">Antarctica is gaining ice</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.2.2.2">0.61</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.3.1">UHI exaggerate GW trends</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.2.3.2">0.44</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.4.1">Cosmic rays cause GW</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.4.2">0.41</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;">Performance of BERT model on GWSD claims evaluated on Climate Skepticism dataset.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Takeaways.</span> The fine-tuning procedure on the GWSD dataset performs well when cross-evaluated on coarser claims (F1 score of 0.766 on 5-fold cross-validation).
However, the model performance drops drastically when evaluated on the fine-grained climate claims from the Climate Skepticism dataset as seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.T4" title="Table 4 ‣ 3.1 Need for Granularity (R1) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a>.
This experiment shows that supervised models trained on coarse-level claims fail to evaluate the stance on fine-grained ones, despite having high domain and topic overlap with the training data.
This motivates the need to design stance detection methods that are highly granular and specific in nature.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Need for Claim Invariancy (R2)</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The second requirement that we identify is that a stance detection approach trained on a fine-grained claim about a topic must generalize to detect stance on other fine-grained claims about the same topic.
This is important because false information is not static and new claims emerge all the time (e.g., the emergence of narratives advocating for Vitamin C, Hydroxychloroquine, and Ivermectin as effective cures against COVID-19 at different points in time during the pandemic).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">To investigate if existing stance detection approaches can generalize between claims, we again use the Climate Skepticism dataset, as it contains three fine-grained claims.
It is to note that all three corpora for the individual claims come from the same social media platform (Reddit), thus we can expect the corpus distribution of the evaluation setting to be similar to the training setting.
As in the previous step, we train a DistilBERT model for each claim and evaluate the model on detecting the stance of posts on the other two claims that it was not trained on.
The results are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.T5" title="Table 5 ‣ 3.2 Need for Claim Invariancy (R2) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Takeaways.</span> We find that in all three cases, a model trained on one claim fails to generalize on other claims as the performance drops drastically.
This motivates the need for building claim invariant stance detection methods that are not only learning features to detect stance specific to the training data of the claim it is trained on, but generic representations of support or refute towards a claim.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T5.2" style="width:248.7pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.8pt,3.6pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.2.1">
<tr class="ltx_tr" id="S3.T5.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.1.1">Training Claim</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.2.1">F1 #1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.3.1">F1 #2</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.4.1">F1 #3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.1.2.1">#1 Antarctica is gaining ice</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.2.2"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.2.2.1">0.828</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.2.3">0.429</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.2.4">0.551</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.1.3.1">#2 UHI exaggerate GW trends</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.3.2">0.470</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.3.3.1">0.7315</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.2.1.3.4">0.578</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.1.4.1">#3 Cosmic rays cause GW</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.1.4.2">0.4183</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.1.4.3">0.589</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.1.4.4"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.4.4.1">0.817</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.3.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S3.T5.4.2" style="font-size:90%;">Stance detection performance across claims. Each line shows the F1 score for the model trained on one claim and tested on the three claims in the Climate Skepticism dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Need for contrastive context awareness (R3)</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">An effective stance detection approach must be able to accurately identify text that supports or refutes a given claim.
A promising approach to achieve this is Natural Language Inference (NLI), which is also known as Recognizing Textual Entailment (RTE).
NLI is a widely popular approach for detecting stance in NLP, where a claim is treated as a <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">premise</em> and a piece of evidence is treated as a <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">hypothesis</em>.
The task then consists in checking if the premise entails (supports) or contradicts (refutes) the hypothesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib22" title="">22</a>]</cite>.
Compared to the approach of fine-tuning BERT-based models for classification tasks, NLI can be adapted with a much more granular objective for stance detection, as each claim being evaluated can be directly subjected to the most relevant piece of evidence associated with the claim.
One of the major reasons existing stance detection methods fail to generalize well on detecting stance on a new topic during inference is the lack of enough contextual information about the topic or target they are subjected in the out-of-domain or zero-shot setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib13" title="">13</a>]</cite>.
Prior research showed that providing enough context about the topic or claim being evaluated can bridge this context gap, improving stance detection models to generalize well on unseen topics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib8" title="">8</a>]</cite>.
Based on this, we investigate if we can leverage NLI methods for stance detection on social media by providing the most “context-aware” set of evidence (hypothesis) statements related to a claim (premise) getting evaluated.
Our assumption is that if there is <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.3">sufficient</em> context provided in the <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.4">hypothesis</em> statement, NLI models should be able to detect stance well on new claims and topics.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We first train an NLI model on COVID-19 claims using the COVID-FACT dataset.
We then evaluate the trained model on the COVID-CQ dataset.
The FEVER setup of the COVID-FACT dataset is perfect for this task, as it already contains pieces of evidence supporting or refuting a claim.
In addition to that, these claims are “context-aware,” (e.g., about the lack of studies confirming the effectiveness of Hydroxychloroquine to treat COVID-19) instead of being generic like “Covid-19 and mask” or “Covid-19 and HCQ.”
Also, COVID-FACT consists of claims and evidence that are semantically close to those being evaluated in the COVID-CQ dataset (e.g., “homemade remedies of COVID-19” or “alternative treatments of COVID-19”), therefore we can expect a high level of domain overlap, similar granularity, and semantic overlap between the training data and the evaluation data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Following a similar methodology as in prior sections, we train a DistilBERT model for textual entailment using these pairs and evaluate the trained model on the COVID-CQ dataset.
Since NLI only allows to specify one hypothesis for each premise, we evaluate the trained model on two different configurations using the following hypotheses: i) Another study has confirmed hydroxychloroquine to be effective in the treatment of COVID-19 (coronavirus), ii) No clinical studies have confirmed hydroxychloroquine as a cure for COVID-19 (coronavirus).
We then consider a tweet as supporting the misleading narrative if it either entails with first hypothesis or contradicts the second hypothesis.
Similarly, a tweet debunks the misleading narrative if it either contradicts the first hypothesis or entails the second hypothesis.
This way, we setup an evaluation setting for a model that is trained on context-aware sentence pairs, and evaluate it under an identical setting, allowing us to understand if satisfying the criteria of “context-awareness” is sufficient enough for stance detection for content moderation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Takeaways.</span> We evaluate the trained DistilBERT model on the COVID-CQ dataset and find that the best-performing F1 score of this method is 0.53, which is only slightly better than random chance, despite having a high topical overlap with the training dataset, and further taking a granular approach of training entailment or contradiction aligned on claims.
Moreover, we discover that the NLI model evaluated on the “context-aware” setting, providing the best set of hypothesis statements with “sufficient” information about the claim, is not adequate for stance detection on social media text.
These results suggest that providing a context-aware hypothesis statement is not enough to build NLI models for precise stance detection.
We argue that while context awareness through the best set of hypothesis statements gives a model important contextual signals about the claim, it would be beneficial for the model to be “contrastively context-aware,” i.e., exposed to contradicting hypotheses, one supporting and one refuting the claim, which is what commonly occurs on social media.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Contrastive Textual Deviation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Based on the three requirements defined in the previous section, we aim to build an unsupervised stance detection model that overcomes the limitations of previous approaches by: i) detecting stance at a fine-grained level, ii) learning semantic representations of stance on a claim invariant fashion, and iii) encoding contrastive context awareness to learn higher level representations of stance.
We first describe our proposed solution by formally defining a new task called Contrastive Textual Deviation (CTD) and discuss the different components of the task.
We then bootstrap CTD by leveraging the zero-shot learning capabilities of Large Language Models (LLMs) through prompt engineering.
Finally, we conclude the section by demonstrating the successful performance of bootstrapped CTD by comparing against multiple baselines from supervised learning.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task Definition</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We propose to reframe the problem of stance detection as a new task of “textual deviation” detection, which aims to satisfy all three requirements.
First, for every misleading claim that we wish to moderate, we start from a <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1" style="color:#0000FF;">consensus statement</em> that has been confirmed to be true, for example by health authorities like the World Health Organization (WHO) in cases of public health emergencies or by fact-checking organizations in case of political events.
Our intuition is that posts that refute the claim, spreading false information while doing so, will deviate from the consensus statement, while those that support (debunk) the claim will stick closer to the messaging of the consensus statement.
We then extend the idea of “context aware” textual entailment to “contrastively context-aware” textual entailment by providing a pair of “contrastive markers” for each consensus statement associated with the misleading claim: a piece of <span class="ltx_text" id="S4.SS1.p1.1.2" style="color:#FF0000;">refuting evidence</span> and one of <span class="ltx_text" id="S4.SS1.p1.1.3" style="color:#228B22;">supporting evidence</span>.
Our idea is to leverage the zero-shot capabilities of Large Language Models (LLMs) to align social media posts discussing a claim with the piece of anchor “consensus” statement specific to the claim, while using the “contrastive markers” as additional context to aide the alignment decision.
We call this task <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.4">Contrastive Textual Deviation</em> (CTD).</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F2.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F2.3">[style=MyFrame,nobreak=true]</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<blockquote class="ltx_quote ltx_figure_panel" id="S4.F2.4">
<p class="ltx_p" id="S4.F2.4.1"><span class="ltx_text ltx_font_bold" id="S4.F2.4.1.1">Dataset: Climate Skepticism</span></p>
<p class="ltx_p" id="S4.F2.4.2"><span class="ltx_text" id="S4.F2.4.2.1" style="color:#0000FF;">Urban Heat Island Effect has no significant influence on the record of global temperature trends.</span></p>
<p class="ltx_p" id="S4.F2.4.3"><span class="ltx_text" id="S4.F2.4.3.1" style="color:#FF0000;">Urban Heat Island effect significantly exaggerates global warming trends.</span></p>
<p class="ltx_p" id="S4.F2.4.4"><span class="ltx_text" id="S4.F2.4.4.1" style="color:#228B22;">Urban Heat Island effect has been properly factored in climate models and does not biases global warming measurements.</span></p>
<p class="ltx_p" id="S4.F2.4.5"><span class="ltx_rule" style="width:433.6pt;height:0.4pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S4.F2.4.6"><span class="ltx_text ltx_font_bold" id="S4.F2.4.6.1">Dataset: COVID-CQ</span></p>
<p class="ltx_p" id="S4.F2.4.7"><span class="ltx_text" id="S4.F2.4.7.1" style="color:#0000FF;">No clinical studies have confirmed hydroxychloroquine as a cure for COVID-19 (coronavirus).</span></p>
<p class="ltx_p" id="S4.F2.4.8"><span class="ltx_text" id="S4.F2.4.8.1" style="color:#FF0000;">Another study has confirmed hydroxychloroquine to be effective in the treatment of COVID-19 (coronavirus).</span></p>
<p class="ltx_p" id="S4.F2.4.9"><span class="ltx_text" id="S4.F2.4.9.1" style="color:#228B22;">It is not medically proven that Hydroxychloroquine (HCQ) can treat COVID-19 (coronavirus).</span></p>
<p class="ltx_p" id="S4.F2.4.10"><span class="ltx_rule" style="width:433.6pt;height:0.4pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S4.F2.4.11"><span class="ltx_text ltx_font_bold" id="S4.F2.4.11.1">Dataset: Stanceosaurus</span></p>
<p class="ltx_p" id="S4.F2.4.12"><span class="ltx_text" id="S4.F2.4.12.1" style="color:#0000FF;">There is no proof that the US military purchased Muslim app user data.</span></p>
<p class="ltx_p" id="S4.F2.4.13"><span class="ltx_text" id="S4.F2.4.13.1" style="color:#FF0000;">Recent news sources reveal that the U.S. military has purchased user location data on popular Muslim phone apps such as Muslim Pro.</span></p>
<p class="ltx_p" id="S4.F2.4.14"><span class="ltx_text" id="S4.F2.4.14.1" style="color:#228B22;">Untrue: Muslim Pro app denies selling user data to US military.</span></p>
</blockquote>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.6.2" style="font-size:90%;">Example claims from evaluation dataset and corresponding triplets.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The core of CTD lies on formulating this triplet of i) a <span class="ltx_text" id="S4.SS1.p2.1.1" style="color:#0000FF;">consensus statement</span>, ii) a <span class="ltx_text" id="S4.SS1.p2.1.2" style="color:#FF0000;">refuting evidence</span>, and iii)  a <span class="ltx_text" id="S4.SS1.p2.1.3" style="color:#228B22;">supporting evidence</span> for each claim that we want to infer the stance of social media posts.
Compared to traditional supervised learning approaches which need hundreds or even thousands of examples for training claim-specific or topic-specific stance models, we argue that curating this triplet is a much easier and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.4">tractable</em> human effort, which could be easily carried out by fact-checking organizations or social media platforms.
The <span class="ltx_text" id="S4.SS1.p2.1.5" style="color:#0000FF;">consensus statements</span> used to anchor the triplets are the “truth values” about the false claim in question.
For each false claim in the datasets, we use the corresponding fact-checks and scientific explanations associated with the claim.
For Climate Skepticism claims, we use fact-checks from <a class="ltx_ref ltx_url ltx_font_typewriter" href="SkepticalScience.com" title="">SkepticalScience.com</a> available under the “<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.6">What the science says</span>” snippet for every climate skepticism claim.
Stanceosaurus contains fact-checked claims coming from several reputable sources (e.g. PolitiFact, Snopes, etc.), and we used the debunking article headline from the corresponding fact-checking sources as the consensus statement.
For the COVID-CQ dataset, we used Google Fact Check Explorer to identify fact-checks and used their debunking title as the consensus statement.
To complete the triplet for each claim, we formulate the contrastive markers as a positive affirmation and a negative reframing of the corresponding <span class="ltx_text" id="S4.SS1.p2.1.7" style="color:#0000FF;">consensus statement</span>.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.F2" title="Figure 2 ‣ 4.1 Task Definition ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">2</span></a> shows examples of triplets from three different claims from our evaluation datasets, with <span class="ltx_text" id="S4.SS1.p2.1.8" style="color:#0000FF;">consensus statement</span>, <span class="ltx_text" id="S4.SS1.p2.1.9" style="color:#228B22;">supporting evidence</span>, and <span class="ltx_text" id="S4.SS1.p2.1.10" style="color:#FF0000;">refuting evidence</span> for each of the claims.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2">CTD allows us to satisfy all three requirements discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3" title="3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a>.
R1 is addressed by allowing us to use customizable and well-informed <span class="ltx_text" id="S4.SS1.p3.2.1" style="color:#0000FF;">consensus statements</span> directly discussing the claim being moderated.
To address R2, CTD learns semantic features of deviation from an anchor piece of text (<span class="ltx_text" id="S4.SS1.p3.2.2" style="color:#0000FF;">consensus statements</span>), rather than semantic features of support or refute towards a particular claim that the model is trained on, allowing us to scale our method across claims and achieve the property of claim invariancy.
Finally, CTD addresses R3 by using a pair of contrasting evidence as context input for the classification task (<span class="ltx_text" id="S4.SS1.p3.2.3" style="color:#FF0000;">refuting evidence</span> and <span class="ltx_text" id="S4.SS1.p3.2.4" style="color:#228B22;">supporting evidence</span>).
This allows our approach to improve over entailment-based methods, which only use one piece of hypothesis statement and thus do not generalize to the case of content moderation on social media, as we showed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS3" title="3.3 Need for contrastive context awareness (R3) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.3</span></a>.
Before bootstrapping Contrastive Textual Deviation as a NLP task, we empirically validate the underlying intuition motivating CTD by visualizing if tweets supporting a claim are semantically different than tweets refuting it.
We utilize the tweets from the COVID-CQ dataset for this experiment and embed the tweets using the Sentence-T5 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib50" title="">50</a>]</cite>, producing 768-dimensional embeddings.
Note that these sentence embeddings are produced using the same encoder component of the LLM we will use for bootstrapping purposes i.e. (FLAN-T5-XXL), and thus serve as a useful tool for empirically testing our intuition.
To this end, we center the embeddings of tweets from the COVID-CQ dataset using the embedding of the <span class="ltx_text" id="S4.SS1.p3.2.5" style="color:#0000FF;">consensus statement</span> and project the T-SNE embeddings of the supporting and refuting tweets.
The scatterplot in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.F3" title="Figure 3 ‣ 4.1 Task Definition ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a> shows a strong separation between the supporting and refuting tweets, with minimal overlap, indicating that the supporting tweets and refuting tweets are indeed semantically different.
Additionally, we also compute the cosine similarity of the embeddings of the supporting tweets and refuting tweets with the <span class="ltx_text" id="S4.SS1.p3.2.6" style="color:#0000FF;">consensus</span> statement.
Statistical analysis using a two-sample t-test to compare the means of the supporting and refuting tweets allows us to reject the null hypothesis (with <math alttext="p&lt;&lt;&lt;0.01" class="ltx_math_unparsed" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1b"><mi id="S4.SS1.p3.1.m1.1.1">p</mi><mo id="S4.SS1.p3.1.m1.1.2" rspace="0em">&lt;</mo><mo id="S4.SS1.p3.1.m1.1.3" lspace="0em" rspace="0em">&lt;</mo><mo id="S4.SS1.p3.1.m1.1.4" lspace="0em">&lt;</mo><mn id="S4.SS1.p3.1.m1.1.5">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">p&lt;&lt;&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_p &lt; &lt; &lt; 0.01</annotation></semantics></math> and <math alttext="t-statistics&gt;&gt;&gt;0" class="ltx_math_unparsed" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1b"><mi id="S4.SS1.p3.2.m2.1.1">t</mi><mo id="S4.SS1.p3.2.m2.1.2">−</mo><mi id="S4.SS1.p3.2.m2.1.3">s</mi><mi id="S4.SS1.p3.2.m2.1.4">t</mi><mi id="S4.SS1.p3.2.m2.1.5">a</mi><mi id="S4.SS1.p3.2.m2.1.6">t</mi><mi id="S4.SS1.p3.2.m2.1.7">i</mi><mi id="S4.SS1.p3.2.m2.1.8">s</mi><mi id="S4.SS1.p3.2.m2.1.9">t</mi><mi id="S4.SS1.p3.2.m2.1.10">i</mi><mi id="S4.SS1.p3.2.m2.1.11">c</mi><mi id="S4.SS1.p3.2.m2.1.12">s</mi><mo id="S4.SS1.p3.2.m2.1.13" rspace="0em">&gt;</mo><mo id="S4.SS1.p3.2.m2.1.14" lspace="0em" rspace="0em">&gt;</mo><mo id="S4.SS1.p3.2.m2.1.15" lspace="0em">&gt;</mo><mn id="S4.SS1.p3.2.m2.1.16">0</mn></mrow><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">t-statistics&gt;&gt;&gt;0</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_t - italic_s italic_t italic_a italic_t italic_i italic_s italic_t italic_i italic_c italic_s &gt; &gt; &gt; 0</annotation></semantics></math>).
This further validates our intuition that the posts supporting the claim align more with the consensus than the posts that refute the claim.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="311" id="S4.F3.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">T-SNE embeddings of supporting and refuting statements from COVID-CQ</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Bootstrapping CTD using LLMs</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Due to their zero-shot learning abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib79" title="">79</a>]</cite>, Large Language Models (LLMs) can be used off-the-shelf to leverage the triplet structure defined by CTD to detect stance in an unsupervised fashion.
We argue that LLMs are particularly well suited for this task because: i) they have demonstrated performance competitive to supervised baselines in multiple NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib60" title="">60</a>]</cite>, ii) they can incorporate the proposed contrastive markers through in-context learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib80" title="">80</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Background.</span> Recent approaches showed that <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.2">prompting</em> a LLM to a specific <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.3">prompt</em> that defines the downstream task (e.g., sentiment analysis, textual entailment), and the possible label spaces for the tasks (e.g., positive, negative, or entailment, contradiction) can lead to performance comparable to supervised methods for text classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib60" title="">60</a>]</cite>.
This ability of LLMs has been shown to be “emergent:” as the size and scale of these language models increases, so does their ability to generalize to unseen tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib80" title="">80</a>]</cite>.
Another emergent ability of LLMs is their capability to perform “in-context learning.”
By extending the <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.4">prompts</em> to include a few examples of the downstream task and the classification labels, LLMs can learn new tasks.
Subjecting LLMs to demonstrative examples can give the model an idea of the label space, of the mapping between input and label space, and help generalize LLMs to a variety of tasks without the need for task-specific fine-tuning.
In-context learning does not require any model update or fine-tuning and performs competitively with state-of-the-art methods for text classification problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Prompt engineering to bootstrap LLMs.</span>
To show the feasibility of our approach for soft moderation, we first bootstrap CTD with LLMs.
To this end, we build our prompts as follows:
For every social media post discussing a claim, we subject it to a <span class="ltx_text" id="S4.SS2.p3.1.2" style="color:#0000FF;">consensus statement</span> that explains the established consensus (e.g., a fact-check or a statement on the scientific consensus about the claim).
Then, we proceed to show an example statement that <span class="ltx_text" id="S4.SS2.p3.1.3" style="color:#FF0000;">refutes</span> the consensus.
Similarly, we show an example statement that <span class="ltx_text" id="S4.SS2.p3.1.4" style="color:#228B22;">supports</span> the consensus.
Finally, we end the prompt with the text that we wish to classify using the LLM, where the LLM is expected to return one of the stance labels (support, or refute).
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.F4" title="Figure 4 ‣ 4.2 Bootstrapping CTD using LLMs ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the prompt structure that we use for bootstrapping CTD.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F4.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F4.3">[style=MyFrame,nobreak=true]</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<blockquote class="ltx_quote ltx_figure_panel" id="S4.F4.4">
<p class="ltx_p" id="S4.F4.4.1">Classify if a statement supports or refutes the consensus statement: <span class="ltx_text" id="S4.F4.4.1.1" style="color:#0000FF;">&lt;Consensus statement&gt;.</span></p>
<p class="ltx_p" id="S4.F4.4.2">Statement: <span class="ltx_text" id="S4.F4.4.2.1" style="color:#FF0000;">&lt;Refuting evidence&gt;.</span></p>
<p class="ltx_p" id="S4.F4.4.3">Response: Refutes.</p>
<p class="ltx_p" id="S4.F4.4.4">Statement: <span class="ltx_text" id="S4.F4.4.4.1" style="color:#228B22;">&lt;Supporting evidence&gt;.</span></p>
<p class="ltx_p" id="S4.F4.4.5">Response: Supports.</p>
<p class="ltx_p" id="S4.F4.4.6">Statement: &lt;Text from the test sample to classify.&gt;</p>
<p class="ltx_p" id="S4.F4.4.7">Response:</p>
</blockquote>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.6.2" style="font-size:90%;">Prompting structure for bootstrapping CTD.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Our intuition is that by doing so we can leverage the zero-shot classification capabilities of LLMs, while at the same time <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.1">ground</em> them towards incorporating the consensus and the contrasting evidence as additional context for classification.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Validating CTD bootstrapping.</span>
We use the open-source encoder-decoder model FLAN-T5 by Google for bootstrapping purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib15" title="">15</a>]</cite>, prompting, for each claim, the LLM with the prompt structure defined above.
The underlying architecture of FLAN-T5 is a T5 model (Text-to-Text Transfer Transformer), a new unified paradigm of transfer learning for text classification.
By formulating challenging tasks like machine translation, summarization, etc. to be a text-to-text task, T5 has shown promising results in multiple NLP benchmark problems over encoder-based models like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib63" title="">63</a>]</cite>.
FLAN-T5 is an improved version of the T5 model that has been “instruction finetuned” on more than 1,000 NLP tasks and thus can be used to perform zero-shot text classification by just “prompting” the model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">As a summary of our validation results, we present the results of the task bootstrapping on the Climate Skepticism dataset.
We will comprehensively evaluate our task with a fine-tuned model against all three evaluation datasets and claims later in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5" title="5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">As baselines, we compare our bootstrapped task against two different prompt setups: i) LLM without contrastive markers, ii) LLM without consensus definition.
Additionally, we also compare our method against two supervised methods for stance detection i) a DistilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib68" title="">68</a>]</cite> model fine-tuned on each claim, and ii) a fine-tuned NLI model on ClimateFever dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib16" title="">16</a>]</cite> (following the identical procedure we fine-tuned NLI model on COVID-FACT dataset in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS3" title="3.3 Need for contrastive context awareness (R3) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.3</span></a>).
The purpose of evaluating the two added prompt configurations alongside the prompt configuration for CTD is to understand the role that each component of the triplets (consensus statement and the contrastive markers) plays in identifying stance.
The prompt configuration of evaluating LLMs without contrastive markers will help us understand if the contrastive markers are an improvement over entailment-based approaches.
For the DistilBERT model, which uses supervised fine-tuning, we perform 5-fold cross-validation for each of the claims and report the average score across the folds.
We use the weighted F1 score as the evaluation metric as the label distribution of supporting and refuting labels for each of the three claims in the Climate Skepticism dataset is imbalanced.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.2">
<tr class="ltx_tr" id="S4.T6.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.2.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.2.1.2.1">Mean F1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.2.1">BERT Fine Tuning</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.2.2.2">0.786</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.3.1">Climate Fever NLI</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.2.3.2">0.626</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.4.1">LLM w/o Contrastive Markers</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.2.4.2">0.795</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.5.1">LLM w/o Consensus</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.2.5.2">0.771</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.2.6.1"><span class="ltx_text ltx_font_bold" id="S4.T6.2.6.1.1">Bootstrapped CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.2.6.2"><span class="ltx_text ltx_font_bold" id="S4.T6.2.6.2.1">0.836</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.3.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.4.2" style="font-size:90%;">Summary of F1 Scores for Different Methods</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p8.1.1">Results.</span> A summary of the bootstrapping results with the mean F1 scores for the three Climate Skepticism claims is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.T6" title="Table 6 ‣ 4.2 Bootstrapping CTD using LLMs ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">6</span></a>.
We find that our bootstrapped method (which is unsupervised) performs better than supervised BERT-based fine-tuning baselines.
Additionally, we find that our approach provides better performance over using the LLMs in their default state: without consensus grounding (LLM without consensus), and without contrastive markers (LLM without contrastive markers).
The results also suggest that the triplet setup of CTD performs better than the conventional setup of entailment for stance detection.
Additionally, we observe that having the contrastive markers by themselves without the consensus statement (LLM without consensus) is not nearly as effective for detecting stance.
Overall, these results validate that the task of CTD can outperform existing methods for supervised stance detection.
Moreover, the utility of our approach is validated in being fully unsupervised and requiring minimal setup needed for a claim (i.e. a <span class="ltx_text" id="S4.SS2.p8.1.2" style="color:#0000FF;">consensus</span> statement, and a pair of <span class="ltx_text" id="S4.SS2.p8.1.3" style="color:#228B22;">supporting</span> and <span class="ltx_text" id="S4.SS2.p8.1.4" style="color:#FF0000;">refuting</span> in-context examples).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Following the successful bootstrapping of CTD through LLMs, we now further fine-tune the FLAN-T5 model to improve upon the bootstrapping performance and build a robust model for the task.
First, we discuss the setup of this fine-tuning procedure and the process of augmenting an argument-mining dataset for the task of CTD.
Next, we comprehensively evaluate the performance of this fine-tuned model against the prior bootstrapped LLM and various other baselines for the downstream task of stance detection in all three of our evaluation datasets.
We then assess how the runtime and detection performance of CTD varies with increasing model size.
Finally, we integrate our approach with the state-of-the-art soft moderation pipeline Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>, showing that it can improve its ability to limit contextual false positives.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Fine tuning FLAN-T5 for CTD</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The bootstrapping experiments from Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4" title="4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a> suggest that CTD can be a reliable approach to identify if a social media post is supporting or refuting a claim, enabling better content moderation.
While LLMs bootstrapped on this task already perform better than multiple existing baselines by leveraging in-context learning, we can improve them further by fine-tuning LLMs that are specifically dedicated to the task of CTD.
Previous work showed that even with little training data fine-tuned LLMs perform better on tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib76" title="">76</a>]</cite>.Additionally, fine-tuning can potentially allow the LLM to build a more specialized model representation associated with the CTD task, enabling the model to better generalize and also be adversarially robust <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib52" title="">52</a>]</cite>.
Previous work showed that the performance of LLMs can be heavily affected by the components in the prompt, in our case the triplets used by CTD.
For this reason, we aim to use the large number of triplet combinations available in our fine-tuning dataset, to learn a model that is decoupled from the phrasing of the prompt, and more focused and consistent on the CTD task itself.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">There are two different approaches to fine-tuning LLMs: i) task-adaptive fine-tuning, and ii) behavioral fine-tuning.
Fine-tuning a LLM directly on the downstream task is called task-adaptive fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib65" title="">65</a>]</cite>.
Behavioral fine-tuning aims to teach a model to learn higher-level language representation by fine-tuning on a task that is behaviorally similar, or close to the downstream task, rather than fine-tuning on the downstream task itself.
This approach is also called intermediate-task fine-tuning, and it works best with tasks that require high-level inference and reasoning capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib56" title="">56</a>]</cite>, like CTD.
Thus, we argue that instead of fine-tuning a LLM for the task of stance detection (task-adaptive fine-tuning), which would suffer from similar limitations as BERT fined-tuned on the stance detection task discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3" title="3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a>, a LLM fine-tuned for textual deviation on a large number of claims will be able to reliably perform stance detection on general claims.
Additionally, behavioral fine-tuning on textual deviation tasks solves the problem of the amount of data available for fine-tuning, as the augmentation of argument mining datasets for textual deviation allows us to generate a dataset that is orders of magnitude larger than any dataset available for specific stance detection topics.
This way, our fine-tuned model is learning what <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">deviation</em> or <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.2">affirmation</em> to any piece of anchor claim looks like without relying on target-specific or claim-specific features, further helping us to achieve the need for claim invariancy (R2) (see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS2" title="3.2 Need for Claim Invariancy (R2) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">We use the PERSPECTRUM dataset to fine-tune LLMs for the CTD task.
This dataset contains both claims and multiple examples of evidences called <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.1">perspectives</em> either agreeing with the claims or disagreeing with them, as required by CTD.
We augment this dataset to create a large-scale one (expanding up to millions of elements) containing the triplets for the textual deviation task.
An example of a claim sampled from the PERSPECTRUM dataset alongside two randomly sampled supporting and refuting perspectives to the claim is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.F5" title="Figure 5 ‣ 5.1 Fine tuning FLAN-T5 for CTD ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5</span></a>.
This particular claim contains 13 supporting perspectives and 22 refuting perspectives in total.
With the claim being used as the anchor (<span class="ltx_text" id="S5.SS1.p3.1.2" style="color:#0000FF;">consensus</span>) statement, and the contrasting set of <span class="ltx_text" id="S5.SS1.p3.1.3" style="color:#FF0000;">refuting</span> and <span class="ltx_text" id="S5.SS1.p3.1.4" style="color:#228B22;">supporting</span> perspectives, we can generate a combination of 9,438 triplets for this claim alone.
This way, the PERSPECTRUM dataset containing 907 claims yields a total of 3,311,548 triplets that we use for fine-tuning a FLAN-T5 model for the CTD task.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">It is to note that the claims present in this dataset are argumentative sentences used for debates and unrelated to the task of fact-checking or misinformation.
We argue that the normalized argumentative structures used to fine-tune the model for CTD will help the model to better capture the intricacies of textual deviation from a canonical perspective, allowing CTD to generalize well and achieve claim invariancy (R2) when applied in the wild.
As we will show later in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS2" title="5.2 Comparison with existing baselines ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.2</span></a>, the representations learned through these triplets built from normalized argumentative structures indeed generalize very well in out-of-domain data on Reddit posts and tweets, confirming that higher-level language representations are learned via behavioral fine-tuning.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F5.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F5.3">[style=MyFrame,nobreak=true]</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<blockquote class="ltx_quote ltx_figure_panel" id="S5.F5.4">
<p class="ltx_p" id="S5.F5.4.1"><span class="ltx_text ltx_font_bold" id="S5.F5.4.1.1">Claim: <span class="ltx_text" id="S5.F5.4.1.1.1" style="color:#0000FF;">All countries should have the right to pursue a nuclear defence.</span></span></p>
<p class="ltx_p" id="S5.F5.4.2"><span class="ltx_rule" style="width:433.6pt;height:0.1pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S5.F5.4.3">Supporting Perspective #1: <span class="ltx_text" id="S5.F5.4.3.1" style="color:#228B22;">All countries are entitled to self defense with nuclear weapons, even when they do not have the capacity to carry conventional weapons.</span></p>
<p class="ltx_p" id="S5.F5.4.4">Supporting Perspective #2: <span class="ltx_text" id="S5.F5.4.4.1" style="color:#228B22;">The pursuit of nuclear defence (respectively the possession of nuclear weapons) by more countries is a guarantee for peace.</span></p>
<p class="ltx_p" id="S5.F5.4.5"><span class="ltx_rule" style="width:433.6pt;height:0.4pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S5.F5.4.6">Refuting Perspective #1: <span class="ltx_text" id="S5.F5.4.6.1" style="color:#FF0000;">The threat of a state developing nuclear weapons could instigate pre-emptive strikes from its neighbours and rivals to prevent the acquisition of such weapons.</span></p>
<p class="ltx_p" id="S5.F5.4.7">Refuting Perspective #2: <span class="ltx_text" id="S5.F5.4.7.1" style="color:#FF0000;">It is very difficult to intercede in humanitarian crises in states wherein nuclear weapons are present.</span></p>
</blockquote>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.5.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.6.2" style="font-size:90%;">Example claims and perspectives from PERSPECTRUM dataset.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">We fine-tune the FLAN-T5 model using a Parameter Efficient Fine-tuning (PEFT) technique known as LoRA (Low-Rank domain adaptation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib25" title="">25</a>]</cite>.
LLMs like FLAN-T5 contain billions of parameters (11B) and fine-tuning the entire network is not feasible even under expensive GPU memory configurations.
LoRA works by freezing the model weights of LLMs and injecting trainable rank decomposition matrices into each layer of the transformers, greatly reducing the number of trainable parameters for downstream tasks while achieving comparable or even better performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib26" title="">26</a>]</cite>.
In our case, LoRA reduces the number of trainable parameters of the 11B FLAN-T5 LLM by 84% (from 11B to 18.8M).</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">Then, using the PEFT configuration of LoRA, we fine-tune the FLAN-T5 model on the augmented fine-tuning dataset.
To make the task more manageable, we randomly sample 4 contrastive examples per claim to fine-tune the model, resulting in a dataset size of 36,000 triplets from 905 claims.
We divide the fine-tuning dataset into 85-15 training and validation splits and fine-tune the FLAN-T5 model for 5 epochs until the validation loss stops decreasing further.
Finally, we test this fine-tuned FLAN model on all three of our evaluation datasets (one coming from Reddit and two from Twitter).
The results of this experiment are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.T7" title="Table 7 ‣ 5.2 Comparison with existing baselines ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">7</span></a>.
As it can be seen, the fine-tuned CTD model performs well, reporting F1-scores between 0.84 and 0.90 on the different datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with existing baselines</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We further evaluate the fine-tuned FLAN-T5 model against three types of baselines: task fine-tuned BERT, multi-task deep neural networks, and behaviorally fine-tuned baselines.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Task Fine-tuned BERT.</span> This is a supervised learning setup where the BERT model is directly trained on a fraction of the evaluation dataset in a five-fold cross-validation setup.
The evaluation configuration for the Climate Skepticism and COVID-CQ datasets has already been discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3" title="3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a>.
It is to note that the third evaluation dataset we use, Stanceosaurus contains a large number of claims, and only a few tweets (in the order of 100) discussing a single claim.
Fine-tuning a BERT-based model for classification for each claim with a limited number of tweets per claim is not possible, thus we fine-tune a BERT-based model for textual entailment instead (identical to NLI model trained on COVID-Fact in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS3" title="3.3 Need for contrastive context awareness (R3) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.3</span></a>).
We refer to this setup as “Task-BERT.”</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Multi-Task Deep Neural Networks (MT-DNN) based stance detection.</span>
A promising solution to DNN methods generalizing poorly outside their training domain (as demonstrated in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS2" title="3.2 Need for Claim Invariancy (R2) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.2</span></a>) is a framework known as Multi-Task Deep Neural Network (MT-DNN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib35" title="">35</a>]</cite>, where a variety of datasets from different domains are used to train a model to increase its robustness.
This has helped in learning powerful higher-level representations across multiple Natural Language Understanding (NLU) tasks, and obtain state-of-the-art results in them.
The MT-DNN framework has been adapted for stance detection by Schiller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib70" title="">70</a>]</cite>, reporting improvement in stance detection performance.
We use the best-performing MT-DNN model available in their work that is fine-tuned across 10 different datasets as a baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Behaviorally fine-tuned baselines.</span> The concept of using contrastive markers and an anchor statement proposed in our work is very closely associated with a framework in Deep Learning known as Triplet learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib71" title="">71</a>]</cite>, which has been successfully applied in computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib84" title="">84</a>]</cite> and NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib61" title="">61</a>]</cite>.
The fine-tuned BERT model we evaluated and compared on sequence classification (in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS2" title="3.2 Need for Claim Invariancy (R2) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.2</span></a>) and sentence-pair classification (in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S3.SS3" title="3.3 Need for contrastive context awareness (R3) ‣ 3 Motivation: Existing stance detection methods fall short ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3.3</span></a>) are pre-trained with the objective of Masked Language Modeling (MLM), and do not consider any type of contrastive loss in their design.
However, a recent framework known as POLITICS (Pretraining Objective Leveraging Inter-article Triplet-loss using Ideological Content and Story) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib36" title="">36</a>]</cite> leverages the concept of Triplet Loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib71" title="">71</a>]</cite> to pre-train a language model.
This model modifies BERT to be trained on a large corpus of news articles discussing the same story, but from different ideologies and uses triplet loss to capture the ideological (dis)similarity among articles on the same story <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib36" title="">36</a>]</cite>.
Furthermore, POLITICS has been fine-tuned for downstream tasks like stance detection, showing improved performance on various datasets over conventional BERT models.
We therefore use POLITICS as an additional baseline to compare against CTD.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">POLITICS needs to be fine-tuned on downstream tasks to be useful.
To this end, we fine-tune the POLITICS model on the same dataset we fine-tuned the FLAN-T5 model on (PERSPECTRUM), adopting Natural Language Inference (NLI) as the downstream task setup.
Similar to how we extracted triplets from PERSPECTRUM in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS1" title="5.1 Fine tuning FLAN-T5 for CTD ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.1</span></a>, we generated combinations of around 11K pairs of claims and perspectives for fine-tuning the POLITICS model.
The process of fine-tuning the POLITICS model closely mirrors that of the FLAN-T5 model, except for the PEFT configuration, which was not necessary for POLITICS.</p>
</div>
<figure class="ltx_table" id="S5.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T7.2">
<tr class="ltx_tr" id="S5.T7.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T7.2.1.1.1">Claim / Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.2.1.3.1">F1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.2.2.1" rowspan="5"><span class="ltx_text" id="S5.T7.2.2.1.1">Climate Skepticism (Reddit)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.2.2">Task-BERT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.2.3">0.786</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.3.1">MT-DNN</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.3.2">0.645</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.4.1">POLITICS</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.4.2">0.734</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.5.1">Bootstrapped CTD</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.5.2">0.836</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.6.1"><span class="ltx_text ltx_font_bold" id="S5.T7.2.6.1.1">Fine-tuned CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.6.2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.6.2.1">0.871</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.2.7.1" rowspan="5"><span class="ltx_text" id="S5.T7.2.7.1.1">COVID-CQ (Twitter)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.7.2">Task-BERT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.7.3">0.900</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.8.1">MT-DNN</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.8.2">0.586</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.9.1">POLITICS</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.9.2">0.831</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.10.1">Bootstrapped CTD</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.10.2">0.810</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.11.1"><span class="ltx_text ltx_font_bold" id="S5.T7.2.11.1.1">Fine-tuned CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.11.2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.11.2.1">0.904</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.12">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.2.12.1" rowspan="5"><span class="ltx_text" id="S5.T7.2.12.1.1">Stanceosaurus (Twitter)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.12.2">Task-BERT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.2.12.3">0.731</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.13.1">MT-DNN</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.13.2">0.656</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.14.1">POLITICS</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.14.2">0.670</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.15.1">Bootstrapped CTD</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.2.15.2">0.773</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.16">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T7.2.16.1"><span class="ltx_text ltx_font_bold" id="S5.T7.2.16.1.1">Fine-tuned CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T7.2.16.2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.16.2.1">0.848</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T7.3.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S5.T7.4.2" style="font-size:90%;">Results of comprehensive evaluation.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T8">
<table class="ltx_tabular ltx_align_middle" id="S5.T8.2">
<tr class="ltx_tr" id="S5.T8.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T8.2.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.2.1.2.1"># params</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T8.2.1.3.1">Runtime (s)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T8.2.1.4.1">Mean F1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.2.1">FLAN-T5-Small</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.2.2">60M</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.2.3">0.008</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.2.4">0.492</td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.3.1">FLAN-T5-Base</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.3.2">250M</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.3.3">0.012</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.3.4">0.555</td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.4.1">FLAN-T5-Large</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.4.2">780M</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.4.3">0.018</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.4.4">0.589</td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.5.1">FLAN-T5-XL</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.5.2">3B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.5.3">0.043</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.2.5.4">0.811</td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.2.6.1"><span class="ltx_text ltx_font_bold" id="S5.T8.2.6.1.1">FLAN-T5-XXL</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T8.2.6.2">11B</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T8.2.6.3">0.078</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T8.2.6.4">0.874</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S5.T8.4.2" style="font-size:90%;">Performance of different FLAN-T5 models on Contrastive Textual Deviation.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">Results.</span>
The summary of evaluation results of the fine-tuned FLAN model and other methods is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.T7" title="Table 7 ‣ 5.2 Comparison with existing baselines ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">7</span></a>.
We can observe that the fine-tuned FLAN model for CTD performs consistently better than all other baselines on all evaluation datasets, outperforming not only Task-BERT and other baselines but also the results from the bootstrapped CTD.
First, this confirms our hypothesis that the zero-shot learning capabilities of LLMs can be further bolstered by behaviorally fine-tuning a LLM on the CTD task.
Secondly, fine-tuned CTD has better performance than other stance detection approaches such as MT-DNN and POLITICS while being fine-tuned on the same dataset.
This proves that the task formulation of Contrastive Textual Deviation leveraged by the fine-tuned CTD model meaningfully outperforms existing systems for stance detection.
Additionally, it is interesting to note that the FLAN-T5 model, which was fine-tuned on normalized argumentative structures that are domain and platform-independent (as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS1" title="5.1 Fine tuning FLAN-T5 for CTD ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.1</span></a>), consistently outperforms the Task-BERT models that were specifically fine-tuned on claim specific posts from social media text.
This again affirms our hypothesis that a granular, topic-independent, and platform-independent stance detection model can be created by reframing the problem as a task of Contrastive Textual Deviation, which captures the semantics of stance detection much better on a foundation level.
In summary, these results show that the FLAN-T5 model fine-tuned on the CTD task is a reliable, robust, and scalable stance detection method for the soft moderation of social media text.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model size and performance tradeoff</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Finally, we study how the runtime and performance of CTD varies based on the different models available in the FLAN-T5 family.
Following the same methodology discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS1" title="5.1 Fine tuning FLAN-T5 for CTD ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.1</span></a>, we fine-tune four smaller models of CTD: i) FLAN-T5-Small, ii) FLAN-T5-Base, iii) FLAN-T5-Large, and iv) FLAN-T5-XL.
This helps us better understand the scaling properties of the CTD task.
Moreover, researchers and practitioners can use this to guide them in appropriate model selection based on their resource constraints and runtime requirements.
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.T8" title="Table 8 ‣ 5.2 Comparison with existing baselines ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">8</span></a> shows that the performance of CTD increases linearly as the model size increases along with the tradeoff on runtime.
On the other hand, CTD fine-tuned on FLAN-T5-XL can be a viable option for practitioners as the performance dropoff from the best model (i.e. FLAN-T5-XXL) is relatively minor compared to the substantial reduction in model size (11B to 3B).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Integrating CTD into Lambretta</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">At this point, we have experimentally validated that a fine-tuned FLAN model on the CTD task has better performance than existing supervised baselines and LLMs bootstrapped for the task, making it the state-of-the-art approach in claim-based stance detection.
Our motivation for designing CTD, however, is to enable soft moderation approaches to get rid of contextual false positives, allowing platforms to deploy more effective warnings that are only applied to content that is supporting a certain false claim.
The state-of-the-art soft moderation tool Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite> is unable to discern this contextual information, and while it performs well in discarding candidate posts that are irrelevant to a given claim, it still flags a large fraction of posts that refute false claims as candidates for moderation: looking at the results reported by Lambretta on three false claims related to the 2020 US Presidential Election (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T3" title="Table 3 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a>), we find that 20% of the candidates flagged by the system are contextual false positives.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="187" id="S5.F6.g1" src="x2.png" width="415"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.3.2" style="font-size:90%;">Our envisioned integration of CTD as a downstream component to Lambretta’s soft moderation system.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">To evaluate how much our approach can aid existing soft moderation approaches to incorporate contextual knowledge into their decisions, we add CTD as a downstream component to Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>, and use its stance detection capabilities to further refine the posts that it flags as candidates for soft moderation, filtering out posts that have a debunking (supporting) stance.
Our goal is to reduce contextual false positives while keeping false negatives at a minimum.
Our envisioned architecture is showed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.F6" title="Figure 6 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">6</span></a>.
It is to note that while we evaluate the approach on Lambretta, CTD can be added as a post-retrieval filter in any content moderation system after posts relevant to a claim or event for flagging are retrieved.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Setup.</span>
We use the annotated dataset of election denial tweets discussed in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S2.T3" title="Table 3 ‣ 2 Datasets ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">3</span></a> for this purpose.
These tweets were flagged by Lambretta as part of the evaluation in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>.
We can observe that the class of stance in the tweets is heavily skewed towards spreading the misinformation (<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.p3.1.2">Refuting</span> the fact-checks), and contains a smaller proportion of tweets that are debunking the misinformation (<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.p3.1.3">Supporting</span> the fact-checks).
As discussed, Lambretta has no contextual understanding of its candidates for soft moderation, and when evaluating for contextual false positives, we find that the system reports a 20% False Detection Rate on average, with an F1 score on the three claims that ranges between 0.88 and 0.89 depending on the claim (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.T9" title="Table 9 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">9</span></a>).
Note that the False Negative Rate for the Lambretta baseline is 0 since the system does not perform any contextual filtering on the moderation candidates, and all retrieved tweets are considered matches for moderation.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S5.F7.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F7.3">[style=MyFrame,nobreak=true]</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<blockquote class="ltx_quote ltx_figure_panel" id="S5.F7.4">
<p class="ltx_p" id="S5.F7.4.1"><span class="ltx_text ltx_font_bold" id="S5.F7.4.1.1">Claim: Wisconsin voter turnout above 90%</span></p>
<p class="ltx_p" id="S5.F7.4.2"><span class="ltx_text" id="S5.F7.4.2.1" style="color:#0000FF;">The voter turnout in Wisconsin is within historical averages of 73% and does not indicate any voter fraud.</span></p>
<p class="ltx_p" id="S5.F7.4.3"><span class="ltx_text" id="S5.F7.4.3.1" style="color:#FF0000;">Fraud in Wisconsin as there were more votes than registered voters and irregular voter turnout around 90%.</span></p>
<p class="ltx_p" id="S5.F7.4.4"><span class="ltx_text" id="S5.F7.4.4.1" style="color:#228B22;">Wisconsin did not have more votes than people registered and their voter turnout figures is 73%.</span></p>
<p class="ltx_p" id="S5.F7.4.5"><span class="ltx_rule" style="width:433.6pt;height:0.4pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S5.F7.4.6"><span class="ltx_text ltx_font_bold" id="S5.F7.4.6.1">Claim: Illegal suitcase of ballots in Georgia.</span></p>
<p class="ltx_p" id="S5.F7.4.7"><span class="ltx_text" id="S5.F7.4.7.1" style="color:#0000FF;">Recently circulating viral video on social media doesn’t show ‘suitcases’ of illegal ballots in Georgia.</span></p>
<p class="ltx_p" id="S5.F7.4.8"><span class="ltx_text" id="S5.F7.4.8.1" style="color:#FF0000;">Suitcases filled with illegal ballots were pulled out from underneath tables after election observers left in Georgia.</span></p>
<p class="ltx_p" id="S5.F7.4.9"><span class="ltx_text" id="S5.F7.4.9.1" style="color:#228B22;">Officials have confirmed that there were no suitcases full of illegal ballots counted in the absence of election observers.</span></p>
<p class="ltx_p" id="S5.F7.4.10"><span class="ltx_rule" style="width:433.6pt;height:0.4pt;background:black;display:inline-block;"> </span></p>
<p class="ltx_p" id="S5.F7.4.11"><span class="ltx_text ltx_font_bold" id="S5.F7.4.11.1">Claim: Dead voters voted in Michigan.</span></p>
<p class="ltx_p" id="S5.F7.4.12"><span class="ltx_text" id="S5.F7.4.12.1" style="color:#0000FF;">There is no credible evidence that dead people voted in the election or ballots were cast fraudulently by deceased voters.</span></p>
<p class="ltx_p" id="S5.F7.4.13"><span class="ltx_text" id="S5.F7.4.13.1" style="color:#FF0000;">There were many cases of voter fraud nationwide due to dead people’s votes getting counted.</span></p>
<p class="ltx_p" id="S5.F7.4.14"><span class="ltx_text" id="S5.F7.4.14.1" style="color:#228B22;">No evidence that 14,000 dead people cast ballots in Wayne County, Michigan.</span></p>
</blockquote>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.5.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S5.F7.6.2" style="font-size:90%;">Triplets used in our experiments on integrating CTD with Lambretta.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">To evaluate CTD on the candidate tweets flagged by Lambretta, we follow the same methodology discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4" title="4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a>, curating a triplet of <span class="ltx_text" id="S5.SS4.p4.1.1" style="color:#0000FF;">consensus statement</span>, <span class="ltx_text" id="S5.SS4.p4.1.2" style="color:#FF0000;">supporting evidence</span>, and <span class="ltx_text" id="S5.SS4.p4.1.3" style="color:#228B22;">refuting evidence</span> for each of the three election denial claims being evaluated, as showed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.F7" title="Figure 7 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">7</span></a>.
Note that in deployment, the only input that platform moderators need to use for our system is the triplet structure pertaining to a specific social media claim that they wish to moderate.
In the case of a single social media post being the starting point, moderators need to first extract the claim contained in the post, which can be done by leveraging Lambretta’s Claim Extraction component or other claim extraction tools such as OpenIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib18" title="">18</a>]</cite>.
We then use the fine-tuned FLAN-T5 model presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS1" title="5.1 Fine tuning FLAN-T5 for CTD ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.1</span></a> with the three curated triplets to identify the stance on the tweets discussing the three claims.
Note that for this experiment CTD is not fine-tuned on the tweets flagged by Lambretta, nor on any tweet discussing election fraud claims, further showcasing that our approach is context agnostic and generalizes across different topics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p5">
<p class="ltx_p" id="S5.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p5.1.1">Results.</span>
The summary of the evaluation is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.T9" title="Table 9 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">9</span></a>.
As it can be seen, integrating CTD with Lambretta largely reduces the rate of contextual false positives, bringing down the average false detection rate by an order of magnitude from 20% to 2.1%.
For the “Wisconsin voter turnout above 90%,” the false detection rate after applying our approach is actually zero.
At the same time, the false negative rate remains small, being 1.8% on average.
This translates in F1 scores between 0.98 and 0.96, showing an improvement of about 10% over the baseline, showing that CTD can be effectively used to improve soft moderation systems for social media.</p>
</div>
<figure class="ltx_table" id="S5.T9">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T9.2" style="width:264.6pt;height:94.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.1pt,15.8pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T9.2.1">
<tr class="ltx_tr" id="S5.T9.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T9.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.1.1.1">Claim</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.1.3.1">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.1.4.1">FDR</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.1.5.1">FNR</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T9.2.1.2.1" rowspan="2"><span class="ltx_text" id="S5.T9.2.1.2.1.1">GA suitcase of ballots</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.2.2">Lambretta</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.2.3">0.877</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.2.4">0.219</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.2.5"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.2.5.1">0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.3.1"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.3.1.1">Lambretta + CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.3.2"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.3.2.1">0.987</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.3.3.1">0.015</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.3.4">0.010</td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T9.2.1.4.1" rowspan="2"><span class="ltx_text" id="S5.T9.2.1.4.1.1">Dead Voters voted in MI</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.4.2">Lambretta</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.4.3">0.887</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.4.4">0.203</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.4.5"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.4.5.1">0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.5.1"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.5.1.1">Lambretta + CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.5.2"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.5.2.1">0.9632</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.5.3"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.5.3.1">0.048</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.5.4">0.024</td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T9.2.1.6.1" rowspan="2"><span class="ltx_text" id="S5.T9.2.1.6.1.1">WI Voter Turnout above 90%</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.6.2">Lambretta</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.6.3">0.891</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.6.4">0.195</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T9.2.1.6.5"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.6.5.1">0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.2.1.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T9.2.1.7.1"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.7.1.1">Lambretta + CTD</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T9.2.1.7.2"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.7.2.1">0.988</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T9.2.1.7.3"><span class="ltx_text ltx_font_bold" id="S5.T9.2.1.7.3.1">0</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T9.2.1.7.4">0.022</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T9.3.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S5.T9.4.2" style="font-size:90%;">Evaluation of the end-to-end component.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We review related work on stance detection, LLMs for text classification, and use of stance detection for integrated fact-checking.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Stance detection.</span>
Stance detection is a foundational technique for various natural language understanding tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib78" title="">78</a>]</cite> and has been used under various settings like argument mining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib33" title="">33</a>]</cite>, rumor detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib87" title="">87</a>]</cite>, and fake news detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib57" title="">57</a>]</cite>.
The majority of the existing work on stance detection focuses on topic or target-specific stance detection, where they aim to detect the stance of a text towards topic such as “gun rights,” “atheism” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib3" title="">3</a>]</cite> and public figures like “Donald Trump” or “Hillary Clinton.” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib44" title="">44</a>]</cite>
Only a few works focus on claim-based stance detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib87" title="">87</a>]</cite>, which is the focus of this work.
Works on claim-based stance detection are mostly geared towards checking facts on formal text like Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib75" title="">75</a>]</cite> or scientific knowledge bases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib77" title="">77</a>]</cite>.
Multiple works have used textual entailment for verifying claims on Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib75" title="">75</a>]</cite>, scientific knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib10" title="">10</a>]</cite>, and climate change conversations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib16" title="">16</a>]</cite>.
There are a few works on claim-based stance detection in multi-lingual settings like Arabic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib2" title="">2</a>]</cite>, and Crotian <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">LLMs for text classification.</span>
With the explosion of Large Language Models and rapid development of powerful LLM models like Open AI’s GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib53" title="">53</a>]</cite>, Google’s BARD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib39" title="">39</a>]</cite>, Anthropic’s Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib5" title="">5</a>]</cite> etc., there has been a paradigm shift in approaching text classification problems.
The massive amount of internet-scale data that LLMs see during their pretraining has been harnessed to fine-tune LLM models, producing state-of-the-art results in challenging benchmarks in Natural Language Understanding (NLU) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib79" title="">79</a>]</cite>.
In addition to the zero-shot capabilities of LLMs, researchers have also used strategies like in-context learning to improve their performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib42" title="">42</a>]</cite>, making weakly supervised learning scalable and effective.
This has made bootstrapping LLMs for custom text classification much easier, a process that we demonstrated successfully in our work.
This way, LLMs have presented themselves as a viable alternative for multiple Natural Language Processing tasks such as classification, and summarization, while relaxing the constraint of task-specific training data needed for conventional NLP methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Stance detection as a fact-checking component.</span>
Prior works have used stance detection as a component of automated fact-checking pipelines.
Works in this domain use stance prediction as one of the many components of their fact-checking pipeline, alongside other components and metadata such as user features and features of conversation threads.
Zubiaga et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib86" title="">86</a>]</cite> incorporated stance classification to detect the stance of tweets in a four-step rumor verification pipeline.
Dungs et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib17" title="">17</a>]</cite> leveraged the stance of conversational threads to predict veracity of rumors.
CredEye, a system proposed by Popat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib58" title="">58</a>]</cite> used stance detection to automatically predict the credibility of textual claims retrieved from the Web.
Another tool, developed by Nguyen et al. to assist fact-checkers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib49" title="">49</a>]</cite>, uses stance predicted from multiple articles alongside other attributes such as the reputation of the news sources to assess a claim’s veracity.
Similarly, FAKTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib47" title="">47</a>]</cite>, a system for end-to-end fact-checking of claims, uses a stance detection model trained on FEVER setting alongside linguistic metadata for automatic fact-checking.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p5">
<p class="ltx_p" id="S6.p5.1"><span class="ltx_text ltx_font_bold" id="S6.p5.1.1">Remarks.</span> In this paper, we showed that CTD outperforms existing stance detection mechanisms and that it could be easily integrated into existing moderation and analysis pipelines that make use of stance detection.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we presented Contrastive Textual Deviation, a new framework for detecting stance in social media posts discussing misleading claims.
We tested the ability of existing stance detection systems to aid content moderation on social media and identified three major limitations of these systems in the context of applying them for content moderation.
Motivated by the shortcomings, we developed the CTD framework as an unsupervised, platform, and topic-agnostic solution.
By experimenting on datasets from two social network platforms (Twitter and Reddit) and multiple topics (e.g., politics, health, climate) we showed that our method consistently outperforms both supervised and unsupervised baselines.
Most importantly, we demonstrated that CTD can be easily integrated into an end-to-end content moderation system, improving the performance of the state-of-the-art soft moderation system Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite> by reducing its contextual false positives tenfold.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We believe that CTD will both serve as a new paradigm for unsupervised claim-based stance detection and will be a valuable tool for researchers and online platforms aiming to improve their existing content moderation systems.
Enabling context-aware soft moderation systems can go a long way in making our information ecosystems healthier, minimizing warning fatigue, and increasing the intended effectiveness of warning labels.
We now discuss the ethical concerns of our work, design implications and limitations of our approach, and avenues for future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Ethics.</span>
All datasets used in this work were either publicly released by other researchers or were collected using publicly available APIs and following those API’s terms of service.
This work is not considered human subjects research by our institution, since we do not interact with humans and do not collect any private information.
Nonetheless, we adhere to ethical standards by removing any personally identifying information when reporting examples of social media posts in the paper.
While we advocate that our approach should be used to benefit society, following the <em class="ltx_emph ltx_font_italic" id="S7.p3.1.2">respect for public interest</em> and <em class="ltx_emph ltx_font_italic" id="S7.p3.1.3">beneficence</em> principles of the Menlo report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib30" title="">30</a>]</cite>, CTD could be misused by malicious parties.
Potential adversarial misuse includes biased platform providers using CTD to identify and downrank dissident users, or state-actors applying CTD to amplify false narratives of interest or identify expert accounts that are correcting/debunking false narratives to be silenced.
While these threats are real, automatically generating and posting content on social media at a large scale produces other artifacts that can be identified by alternative approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib67" title="">67</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">Design implications.</span>
We envision CTD to be applied as a post-retrieval filtering tool for content moderation systems on social media after topically relevant candidate posts for a claim are retrieved, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.F6" title="Figure 6 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">6</span></a>.
Since the approach of Contrastive Textual Deviation is designed for the task of claim-specific stance detection, CTD is mostly intended for claim-specific content moderation systems like Lambretta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib54" title="">54</a>]</cite>.
The process of integrating CTD as a tool to existing soft moderation systems is seamless, as we demonstrated in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.SS4" title="5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">5.4</span></a>.
For each claim a platform moderator wishes to apply warning labels to, the only input from their end is to craft the triplet consisting of the <span class="ltx_text" id="S7.p4.1.2" style="color:#0000FF;">consensus statement</span>, a <span class="ltx_text" id="S7.p4.1.3" style="color:#FF0000;">refuting evidence</span> and a <span class="ltx_text" id="S7.p4.1.4" style="color:#228B22;">supporting evidence</span>.
The triplet can then be used with the evaluation prompt in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.F4" title="Figure 4 ‣ 4.2 Bootstrapping CTD using LLMs ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">4</span></a> alongside the fine-tuned FLAN-T5 model for inference.
It is to note that one of the major advantages of using CTD is that it does not need any further fine-tuning for new claims and new platforms.
As demonstrated in the example triplets throughout our work (i.e. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S5.F7" title="Figure 7 ‣ 5.4 Integrating CTD into Lambretta ‣ 5 Evaluation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">7</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#S4.F2" title="Figure 2 ‣ 4.1 Task Definition ‣ 4 Contrastive Textual Deviation ‣ Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation This paper is accepted for publication at the 2024 USENIX Security Symposium. Please cite accordingly."><span class="ltx_text ltx_ref_tag">2</span></a>), the <span class="ltx_text" id="S7.p4.1.5" style="color:#0000FF;">consensus statement</span> for a claim can be formulated with the most succinct fact-check or scientific consensus about the misleading claim.
Similarly, the contrastive markers can be simple statements that are positive affirmation and negative reframing of the same <span class="ltx_text" id="S7.p4.1.6" style="color:#0000FF;">consensus statement</span>.
The final model was fine-tuned on normalized pieces of argumentative structures, and we expect it to be robust enough to handle different quality or phrasings of triplet semantic structure expected of a CTD triplet.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Limitations.</span>
There are some limitations that come with the task setup of CTD for stance detection.
First, we expect that any claim a platform aims to contextually moderate has been determined to be false, and has an accompanying fact-check statement associated with it.
This requirement of a corresponding fact-check statement used to build the <span class="ltx_text" id="S7.p5.1.2" style="color:#0000FF;">consensus statement</span> in a CTD triplet poses a practical challenge in cases of quickly emerging false claims or novel skeptical narratives, for which a consensus about the claim has not yet been established.
For these cases, moderators could potentially use high-quality crowdsourced truth statements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib4" title="">4</a>]</cite>, or resort to applying previously proposed soft moderation techniques that do not take stance into account (e.g., Lambretta).
The formulation of CTD can only handle posts that support or refute a misleading claim, while social media posts discussing misinformation and rumors might also contain posts that are of “querying” or “commenting” nature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib62" title="">62</a>]</cite>.
However, for the purpose of soft moderation where the objective is to apply warning labels to posts that are spreading the misleading claim, we argue that finer-grained distinction within the nature of “support” of a claim might not be necessary.
Upon manual inspection, we find that the majority of the misclassifications by CTD happen on posts that are sarcastic and satirical about the misleading claim being discussed, which are often misclassified as refuting the claim.
Identifying satire and sarcasm is a challenging NLP task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib29" title="">29</a>]</cite>, and future work can explore fine-tuning CTD models to handle more nuanced cases of stance occurring on social media text.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p6">
<p class="ltx_p" id="S7.p6.1"><span class="ltx_text ltx_font_bold" id="S7.p6.1.1">Future Work.</span>
In the future, we plan to extend CTD on evaluating claim-based stance detection in multi-lingual settings.
As LLMs become more powerful beyond the English Language, we can expect their learning capabilities to improve on multiple languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib81" title="">81</a>]</cite>.
We will also explore strategies to reduce the manual costs of curating the triplet structure needed for CTD by automatically matching the best set of fact checks and <span class="ltx_text" id="S7.p6.1.2" style="color:#0000FF;">consensus statements</span> for misleading claims spreading in the wild, and generating the best set of triplets automatically.
The ClaimReview structured markup introduced by Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20910v1#bib.bib20" title="">20</a>]</cite> is a promising avenue for this direction, and we will explore multiple ways of semantically matching a misleading claim with ClaimReview markups, and generating the piece of contrastive markers from the fact-check document.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p7">
<p class="ltx_p" id="S7.p7.1"><span class="ltx_text ltx_font_bold" id="S7.p7.1.1">Acknowledgments.</span> We would like to thank the anonymous reviewers for their feedback.
This work was funded by the Institute for Global Sustainability and the Rafik B. Hariri Institute for Computing and Computational Science &amp; Engineering at Boston University under the <em class="ltx_emph ltx_font_italic" id="S7.p7.1.2">Data and Misinformation in an Era of Sustainability and Climate Change Crises</em> project, and by the NSF under grants CNS-1942610, CNS-2114407, CNS-2247868, and DEB-2200052.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abeer AlDayel and Walid Magdy.

</span>
<span class="ltx_bibblock">Stance detection on social media: State of the art and trends.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Information Processing &amp; Management</span>, 58(4), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Tariq Alhindi, Amal Alabdulkarim, Ali Alshehri, Muhammad Abdul-Mageed, and
Preslav Nakov.

</span>
<span class="ltx_bibblock">Arastance: A multi-country and multi-domain dataset of arabic stance
detection for fact checking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Workshop on NLP for Internet Freedom: Censorship,
Disinformation, and Propaganda</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Emily Allaway, Malavika Srikanth, and Kathleen Mckeown.

</span>
<span class="ltx_bibblock">Adversarial learning for zero-shot stance detection on social media.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jennifer Allen, Antonio A Arechar, Gordon Pennycook, and David G Rand.

</span>
<span class="ltx_bibblock">Scaling up fact-checking using the wisdom of crowds.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Science advances</span>, 7(36), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/index/introducing-claude" title="">https://www.anthropic.com/index/introducing-claude</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ramy Baly, Mitra Mohtarami, James Glass, Lluís Màrquez, Alessandro
Moschitti, and Preslav Nakov.

</span>
<span class="ltx_bibblock">Integrating stance detection and fact checking in a unified corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy
Blackburn.

</span>
<span class="ltx_bibblock">The pushshift reddit dataset.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">AAAI International Conference on Web and Social Media</span>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Tilman Beck, Andreas Waldis, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Robust integration of contextual information for cross-target stance
detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Joint Conference on Lexical and Computational Semantics (*
SEM 2023)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Karissa Bell.

</span>
<span class="ltx_bibblock">Instagram adds ’false information’ labels to prevent fake news from
going viral.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mashable.com/article/instagram-false-information-labels" title="">https://mashable.com/article/instagram-false-information-labels</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Iz Beltagy, Kyle Lo, and Arman Cohan.

</span>
<span class="ltx_bibblock">Scibert: A pretrained language model for scientific text.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Mihaela Bošnjak and Mladen Karan.

</span>
<span class="ltx_bibblock">Data set for stance and sentiment analysis from user comments on
croatian news.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Workshop on Balto-Slavic Natural Language Processing</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Advances in neural information processing systems</span>, 33, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Michael Burnham.

</span>
<span class="ltx_bibblock">Stance detection with supervised, zero-shot, and few-shot
applications.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth.

</span>
<span class="ltx_bibblock">Seeing things from a different angle: Discovering diverse
perspectives about claims.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita,
and Markus Leippold.

</span>
<span class="ltx_bibblock">Climate-fever: A dataset for verification of real-world climate
claims.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Sebastian Dungs, Ahmet Aker, Norbert Fuhr, and Kalina Bontcheva.

</span>
<span class="ltx_bibblock">Can rumour stance alone predict veracity?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">International conference on computational linguistics</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld.

</span>
<span class="ltx_bibblock">Open information extraction from the web.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Communications of the ACM</span>, 51(12), 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jaynil Gaglani, Yash Gandhi, Shubham Gogate, and Aparna Halbe.

</span>
<span class="ltx_bibblock">Unsupervised whatsapp fake news detection using semantic search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">International Conference on Intelligent Computing and Control
Systems (ICICCS)</span>. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Fact check (claimreview) structured data.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.google.com/search/docs/appearance/structured-data/factcheck" title="">https://developers.google.com/search/docs/appearance/structured-data/factcheck</a>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Megan Graham and Salvador Rodriguez.

</span>
<span class="ltx_bibblock">Twitter and facebook race to label a slew of posts making false
election claims before all votes counted.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnbc.com/2020/11/04/twitter-and-facebook-label-trump-posts-claiming-election-stolen.html" title="">https://www.cnbc.com/2020/11/04/twitter-and-facebook-label-trump-posts-claiming-election-stolen.html</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Andreas Hanselowski, Hao Zhang, Zile Li, Daniil Sorokin, Benjamin Schiller,
Claudia Schulz, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Ukp-athene: Multi-sentence textual entailment for claim verification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">EMNLP 2018</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Momchil Hardalov, Arnav Arora, Preslav Nakov, and Isabelle Augenstein.

</span>
<span class="ltx_bibblock">A survey on stance detection for mis-and disinformation
identification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">NAACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Tamanna Hossain, Robert L Logan IV, Arjuna Ugarte, Yoshitomo Matsubara, Sean
Young, and Sameer Singh.

</span>
<span class="ltx_bibblock">Covidlies: Detecting covid-19 misinformation on social media.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee,
Lidong Bing, and Soujanya Poria.

</span>
<span class="ltx_bibblock">Llm-adapters: An adapter family for parameter-efficient fine-tuning
of large language models.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Muhammad Ikram, Lucky Onwuzurike, Shehroze Farooqi, Emiliano De Cristofaro,
Arik Friedman, Guillaume Jourjon, Mohammed Ali Kaafar, and M Zubair Shafiq.

</span>
<span class="ltx_bibblock">Measuring, characterizing, and detecting facebook like farms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">ACM Transactions on Privacy and Security (TOPS)</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yan Jiang, Jinhua Gao, Huawei Shen, and Xueqi Cheng.

</span>
<span class="ltx_bibblock">Zero-shot stance detection via multi-perspective contrastive learning
with unlabeled data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Information Processing &amp; Management</span>, 60(4), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Aditya Joshi, Pushpak Bhattacharyya, and Mark J Carman.

</span>
<span class="ltx_bibblock">Automatic sarcasm detection: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">ACM Computing Surveys (CSUR)</span>, 50(5), 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Erin Kenneally and David Dittrich.

</span>
<span class="ltx_bibblock">The menlo report: Ethical principles guiding information and
communication technology research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Available at SSRN 2445102</span>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Paige Leskin.

</span>
<span class="ltx_bibblock">Twitter has apologized for slapping a COVID-19 label on tweets about
5G, but experts say the platform’s algorithm could be encouraging the spread
of conspiracy theories.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.businessinsider.com/twitter-5g-coronavirus-label-blames-algorithm-encourages-conspiracy-theories-2020-6" title="">https://www.businessinsider.com/twitter-5g-coronavirus-label-blames-algorithm-encourages-conspiracy-theories-2020-6</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Chen Ling, Krishna P Gummadi, and Savvas Zannettou.

</span>
<span class="ltx_bibblock">“learn the facts about covid-19”: Analyzing the use of warning
labels on tiktok videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">International AAAI Conference on Web and Social Media
(ICWSM)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Marco Lippi and Paolo Torroni.

</span>
<span class="ltx_bibblock">Argumentation mining: State of the art and emerging trends.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">ACM Transactions on Internet Technology (TOIT)</span>, 16(2), 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
Bansal, and Colin A Raffel.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than
in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</span>, 35, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Multi-task deep neural networks for natural language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Annual Meeting of the Association for Computational
Linguistics</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Yujian Liu, Xinliang Frederick Zhang, David Wegsman, Nicholas Beauchamp, and
Lu Wang.

</span>
<span class="ltx_bibblock">Politics: Pretraining with same-story article comparison for ideology
prediction and stance detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">NAACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Taylor Lorentz.

</span>
<span class="ltx_bibblock">Twitter labeled factual information about covid-19 as
misinformation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.washingtonpost.com/technology/2022/08/25/twitter-factual-covid-info-labeled-misinformation/" title="">https://www.washingtonpost.com/technology/2022/08/25/twitter-factual-covid-info-labeled-misinformation/</a>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yiwei Luo, Dallas Card, and Dan Jurafsky.

</span>
<span class="ltx_bibblock">Detecting stance in media on global warming.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">EMNLP</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
James Manyika.

</span>
<span class="ltx_bibblock">An overview of Bard: an early experiment with generative AI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/static/documents/google-about-bard.pdf" title="">https://ai.google/static/documents/google-about-bard.pdf</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Tom McCoy, Ellie Pavlick, and Tal Linzen.

</span>
<span class="ltx_bibblock">Right for the wrong reasons: Diagnosing syntactic heuristics in
natural language inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Annual Meeting of the Association for Computational
Linguistics</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Mary L McHugh.

</span>
<span class="ltx_bibblock">Interrater reliability: the kappa statistic.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Biochemia medica</span>, 22(3), 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
Hajishirzi, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Rethinking the role of demonstrations: What makes in-context learning
work?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Conference on Empirical Methods in Natural Language
Processing</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Kunihiro Miyazaki, Takayuki Uchiba, Kenji Tanaka, Jisun An, Haewoon Kwak, and
Kazutoshi Sasahara.

</span>
<span class="ltx_bibblock">" this is fake news": Characterizing the spontaneous debunking from
twitter users to covid-19 false information.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">International AAAI Conference on Web and Social Media</span>,
volume 17, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin
Cherry.

</span>
<span class="ltx_bibblock">Semeval-2016 task 6: Detecting stance in tweets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">International workshop on semantic evaluation
(SemEval-2016)</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Garrett Morrow, Briony Swire-Thompson, Jessica Montgomery Polny, Matthew Kopec,
and John P Wihbey.

</span>
<span class="ltx_bibblock">The emerging science of content labeling: Contextualizing social
media content moderation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Journal of the Association for Information Science and
Technology</span>, 73(10), 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ece C Mutlu, Toktam Oghaz, Jasser Jasser, Ege Tutunculer, Amirarsalan Rajabi,
Aida Tayebi, Ozlem Ozmen, and Ivan Garibay.

</span>
<span class="ltx_bibblock">A stance data set on polarized conversations on twitter about the
efficacy of hydroxychloroquine as a treatment for covid-19.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Data in brief</span>, 33, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, and James Glass.

</span>
<span class="ltx_bibblock">Fakta: An automatic end-to-end fact checking system.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Lynnette Hui Xian Ng and Kathleen M Carley.

</span>
<span class="ltx_bibblock">Is my stance the same as your stance? a cross validation study of
stance detection datasets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Information Processing &amp; Management</span>, 59(6), 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
An Nguyen, Aditya Kharosekar, Matthew Lease, and Byron Wallace.

</span>
<span class="ltx_bibblock">An interpretable joint graphical model for fact-checking from crowds.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">AAAI Conference on Artificial Intelligence</span>, volume 32, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel
Cer, and Yinfei Yang.

</span>
<span class="ltx_bibblock">Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">ACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Shirin Nilizadeh, François Labrèche, Alireza Sedighian, Ali Zand,
José Fernandez, Christopher Kruegel, Gianluca Stringhini, and Giovanni
Vigna.

</span>
<span class="ltx_bibblock">Poised: Spotting twitter spam off the beaten paths.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">ACM SIGSAC Conference on Computer and Communications
Security</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Venkata Prabhakara Sarath Nookala, Gaurav Verma, Subhabrata Mukherjee, and
Srijan Kumar.

</span>
<span class="ltx_bibblock">Adversarial robustness of prompt-based few-shot learning for natural
language understanding.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">ArXiv</span>, abs/2303.08774, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Pujan Paudel, Jeremy Blackburn, Emiliano De Cristofaro, Savvas Zannettou, and
Gianluca Stringhini.

</span>
<span class="ltx_bibblock">Lambretta: Learning to rank for twitter soft moderation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">IEEE Symposium on Security and Privacy</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Gordon Pennycook and David G Rand.

</span>
<span class="ltx_bibblock">Fighting misinformation on social media using crowdsourced judgments
of news source quality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the National Academy of Sciences</span>, 116(7), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara
Vania, Katharina Kann, and Samuel R Bowman.

</span>
<span class="ltx_bibblock">English intermediate-task training improves zero-shot cross-lingual
transfer too.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Dean Pomerleau and Delip Rao.

</span>
<span class="ltx_bibblock">Fake news challenge stage 1 (fnc-i): Stance detection.

</span>
<span class="ltx_bibblock">15, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum.

</span>
<span class="ltx_bibblock">Credeye: A credibility lens for analyzing and explaining
misinformation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Companion Proceedings of the The Web Conference 2018</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang,
Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel Bowman.

</span>
<span class="ltx_bibblock">Intermediate-task transfer learning with pretrained language models:
When and why does it work?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Raul Puri and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Zero-shot text classification with generative language models.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.

</span>
<span class="ltx_bibblock">From humor recognition to irony detection: The figurative language of
social media.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">Data &amp; Knowledge Engineering</span>, 74, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer,
Peter J Liu, Sharan Narang, Wei Li, and Yanqi Zhou.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.

</span>
<span class="ltx_bibblock">A primer in bertology: What we know about how bert works.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Transactions of the Association for Computational Linguistics</span>,
8, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Sebastian Ruder.

</span>
<span class="ltx_bibblock">Recent Advances in Language Model Fine-tuning.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ruder.io/recent-advances-lm-fine-tuning" title="">http://ruder.io/recent-advances-lm-fine-tuning</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan.

</span>
<span class="ltx_bibblock">Covid-fact: Fact extraction and verification of real-world claims on
covid-19 pandemic.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language
Processing</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Mohammad Hammas Saeed, Shiza Ali, Jeremy Blackburn, Emiliano De Cristofaro,
Savvas Zannettou, and Gianluca Stringhini.

</span>
<span class="ltx_bibblock">Trollmagnifier: Detecting state-sponsored troll accounts on reddit.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">IEEE Symposium on Security and Privacy (SP)</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
Yvon, Matthias Gallé, et al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Benjamin Schiller, Johannes Daxenberger, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Stance detection benchmark: How robust is your stance detection?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">KI-Künstliche Intelligenz</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Florian Schroff, Dmitry Kalenichenko, and James Philbin.

</span>
<span class="ltx_bibblock">Facenet: A unified embedding for face recognition and clustering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">IEEE conference on computer vision and pattern recognition</span>,
2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Chengcheng Shao, Giovanni Luca Ciampaglia, Alessandro Flammini, and Filippo
Menczer.

</span>
<span class="ltx_bibblock">Hoaxy: A platform for tracking online misinformation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Proceedings of the international conference companion on
world wide web</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Filipo Sharevski, Raniem Alsaadi, Peter Jachim, and Emma Pieroni.

</span>
<span class="ltx_bibblock">Misinformation warning labels: Twitter’s soft moderation effects on
covid-19 vaccine belief echoes.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Scott Simon and Emma Bowman.

</span>
<span class="ltx_bibblock">Propaganda, hate speech, violence: The working lives of facebook’s
content moderators.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.npr.org/2019/03/02/699663284/the-working-lives-of-facebooks-content-moderators" title="">https://www.npr.org/2019/03/02/699663284/the-working-lives-of-facebooks-content-moderators</a>,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.

</span>
<span class="ltx_bibblock">Fever: a large-scale dataset for fact extraction and verification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Robert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,
Jianfeng Gao, and Hoifung Poon.

</span>
<span class="ltx_bibblock">Fine-tuning large neural language models for biomedical natural
language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Patterns (New York, NY)</span>, 4(4), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman
Cohan, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Fact or fiction: Verifying scientific claims.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">EMNLP Workshop BlackboxNLP</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei
Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al.

</span>
<span class="ltx_bibblock">Polylm: An open source polyglot large language model.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Queenie Wong.

</span>
<span class="ltx_bibblock">More harm than good? Twitter struggles to label misleading COVID-19
tweets.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnet.com/tech/mobile/more-harm-than-good-twitter-struggles-to-label-misleading-covid-19-tweets/" title="">https://www.cnet.com/tech/mobile/more-harm-than-good-twitter-struggles-to-label-misleading-covid-19-tweets/</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Savvas Zannettou.

</span>
<span class="ltx_bibblock">I won the election: An empirical analysis of soft moderation
interventions on twitter.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">International AAAI Conference on Web and Social Media</span>,
volume 15, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.

</span>
<span class="ltx_bibblock">Hierarchical clustering with hard-batch triplet loss for person
re-identification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Jonathan Zheng, Ashutosh Baheti, Tarek Naous, Wei Xu, and Alan Ritter.

</span>
<span class="ltx_bibblock">Stanceosaurus: Classifying stance towards multicultural
misinformation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">Conference on Empirical Methods in Natural Language
Processing</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and Rob Procter.

</span>
<span class="ltx_bibblock">Detection and resolution of rumours in social media: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">ACM Computing Surveys (CSUR)</span>, 51(2), 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, and Peter
Tolmie.

</span>
<span class="ltx_bibblock">Analysing how people orient to and spread rumours in social media by
looking at conversational threads.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">PloS one</span>, 11(3), 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 30 15:33:54 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
