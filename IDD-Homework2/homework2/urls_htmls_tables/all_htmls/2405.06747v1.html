<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Music Emotion Prediction Using Recurrent Neural Networks</title>
<!--Generated on Fri May 10 17:55:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %Uncomment␣to␣remove␣the␣date .-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.06747v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S1" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S2" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Relative Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.SS1" title="In 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.SS1.SSS1" title="In 3.1 Datasets ‣ 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>4Q audio emotion dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.SS1.SSS2" title="In 3.1 Datasets ‣ 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Augmented dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.SS1.SSS3" title="In 3.1 Datasets ‣ 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>MTG-Jamendo Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.SS2" title="In 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Visualization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS1" title="In 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Hypothesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS2" title="In 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Baseline Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS3" title="In 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>RNNs Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS3.SSS1" title="In 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>RNNs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS3.SSS2" title="In 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>BRNNs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS3.SSS3" title="In 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>LSTM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS4" title="In 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Conclusions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.SS4.SSS1" title="In 4.4 Conclusions ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Baseline Models vs Neural Networks</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Further Works 1 - Augment Original Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.SS1" title="In 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.SS2" title="In 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.SS2.SSS1" title="In 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.SS2.SSS2" title="In 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>RNN-Related Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.SS3" title="In 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Conclusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6" title="In Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Further Works 2 - Larger Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS1" title="In 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS2" title="In 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS2.SSS1" title="In 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS2.SSS2" title="In 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>RNNs-Related Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS3" title="In 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS4" title="In 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.SS5" title="In 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Other Notes</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Music Emotion Prediction 
<br class="ltx_break"/>Using Recurrent Neural Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> <a class="ltx_ref ltx_href" href="https://xiyahc.github.io/about/" title=""><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="8" id="id1.1.1.g1" src="x1.png" width="8"/> Xinyu Chang</a>
<br class="ltx_break"/>Johns Hopkins University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id1">xchang23@jh.edu</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id6.6.id2">\And</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="8" id="id2.2.g1" src="x2.png" width="8"/> Xiangyu Zhang 
<br class="ltx_break"/>Johns Hopkins University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id7.7.id3">xzhan344@jh.edu</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id8.8.id4">\AND</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="8" id="id3.3.g2" src="x3.png" width="8"/> Haoruo Zhang 
<br class="ltx_break"/>Johns Hopkins University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id5">hzhan237@jh.edu</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id10.10.id6">\And</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="8" id="id4.4.g3" src="x4.png" width="8"/> Yulu Ran 
<br class="ltx_break"/>Johns Hopkins University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.11.id7">yran2@jh.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">This study explores the application of recurrent neural networks to recognize emotions conveyed in music, aiming to enhance music recommendation systems and support therapeutic interventions by tailoring music to fit listeners’ emotional states. We utilize Russell’s Emotion Quadrant to categorize music into four distinct emotional regions and develop models capable of accurately predicting these categories. Our approach involves extracting a comprehensive set of audio features using Librosa and applying various recurrent neural network architectures, including standard RNNs, Bidirectional RNNs, and Long Short-Term Memory (LSTM) networks. Initial experiments are conducted using a dataset of 900 audio clips, labeled according to the emotional quadrants. We compare the performance of our neural network models against a set of baseline classifiers and analyze their effectiveness in capturing the temporal dynamics inherent in musical expression. The results indicate that simpler RNN architectures may perform comparably or even superiorly to more complex models, particularly in smaller datasets. We’ve also applied the following experiments on larger datasets: one is augmented based on our original dataset, and the other is from other sources. This research not only enhances our understanding of the emotional impact of music but also demonstrates the potential of neural networks in creating more personalized and emotionally resonant music recommendation and therapy systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.7"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.7.1">Keywords</em> Music Emotion Recognition  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.1.m1.1"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.1.m1.1d">⋅</annotation></semantics></math>
Neural Networks  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.2.m2.1"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.2.m2.1d">⋅</annotation></semantics></math>
Music Therapy  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.3.m3.1"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.3.m3.1d">⋅</annotation></semantics></math>
Machine Learning in Music  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">⋅</annotation></semantics></math>
Bidirectional Recurrent Neural Networks(BRNNs)  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.5.m5.1"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.5.m5.1d">⋅</annotation></semantics></math>
Long Short-Term Memory Networks (LSTM)  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.6.m6.1"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.6.m6.1d">⋅</annotation></semantics></math>
Russell’s Emotion Quadrant  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.7.m7.1"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.7.m7.1d">⋅</annotation></semantics></math>
Music Recommendation System</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Our project aims to leverage neural networks to analyze the emotional expressions conveyed by music. Consider the profound impact music has on our emotions, such as evoking feelings of happiness or sadness. Our objective is to employ these techniques to enhance music recommendation systems, tailoring selections more closely to listeners’ moods. Additionally, we anticipate that our findings could support therapeutic interventions, where specific musical moods are used to aid individuals dealing with emotional challenges. Our methodology involves developing robust techniques for labeling and predicting the emotional categories of music, based on Russell’s Emotion Quadrant. Through this research, we seek to establish a more accurate framework for classifying the emotional content of arbitrary audio samples into specific emotional regions. This endeavor not only advances our understanding of music’s emotional dimensions but also enhances the personalization of music therapy and recommendation systems.

<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Relative Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p ltx_align_center" id="S2.p1.1">Novel Audio Features for Music Emotion Recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#bib.bib3" title="">3</a>]</cite></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In this paper, Renato Panda and colleagues introduce a set of novel audio features specifically designed for music emotion recognition. These features encompass melodic, dynamic, rhythmic, musical texture, and expressivity aspects. The authors elaborate on the methods used to extract these features from audio clips and describe the algorithms they developed for this process. Additionally, they provide well-managed datasets, which we have chosen to utilize in our project.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p ltx_align_center" id="S2.p3.1">A Novel Music Emotion Recognition Model Using Neural Network Technology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#bib.bib4" title="">4</a>]</cite></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In this paper, Jing Yang discusses the limitations of neural networks, particularly the ease with which they can fall into local solutions and their low efficiency due to backpropagation. To address these issues, Yang introduces the Artificial Bee Colony Algorithm as a potential solution. The paper presents the idea of using neural network models for music emotion recognition tasks and explores the possibility of incorporating the Artificial Bee Colony Algorithm to enhance the performance of these models, which we have chosen to attempt in our project.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We used 3 datasets to test our model: 4Q audio emotion dataset (small size), the augmented 4Q audio emotion dataset (medium size), and the MTG-Jamendo Dataset (large size).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>4Q audio emotion dataset</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The dataset contains 900 audio clips, gathered from AllMusic API. According to Russell’s model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#bib.bib2" title="">2</a>]</cite>, the datasets are annotated into 4 quadrants.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Augmented dataset</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">To generate syntactic data for audio, we applied noise injection, shifting time, changing pitch, and speed. After data augmentation, we have 3600 audio clips in total.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">By using Librosa, we computed the chromagram of each audio clip and converted all 3600 30-sec audio clips into 14 features [’chroma_stft’, ’chroma_cqt’, ’chroma_vqt’, ’melspectrogram’, ’mfcc’, ’rms’, ’spectral_centroid’, ’spectral_bandwidth’, ’spectral_contrast’, ’spectral_flatness’, ’spectral_rolloff’, ’tonnetz’, ’zero_crossing_rate’, ’tempo’]. To take advantage of Recurrent Neural Networks (RNNs) to process time series data, we stacked 14 features into a single feature. Therefore, after processing, X is in the shape of (3600, 204, 1295), while y comprises four labels and exhibits a shape of (3600, ).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>MTG-Jamendo Dataset</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">The dataset contains over 55,000 full audio tracks with 195 tags from genre, instrument, and mood/theme categories. We test our models and baseline model on subsets of the MTG-Jamendo Dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#bib.bib1" title="">1</a>]</cite>, which contains nearly 14,000 audio clips and 59 tags.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Visualization</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The following is a t-SNE plot obtained by embedding 14 features into 2 dimensions, which presents a good visual representation of how the features are distributed. From Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S3.F1" title="Figure 1 ‣ 3.2 Data Visualization ‣ 3 Method ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a> we can find that label 2 (green), and label 3 (yellow) points tend to be concentrated in the upper-middle right position, and label 1 (light blue) in the lower-middle right position. Although the clustering is not very tight, points of certain classes seem to tend to cluster together, suggesting that there may be some inherent structure in the multidimensional space.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="276" id="S3.F1.g1" src="extracted/5589538/images/aug_dataset.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data Visualization for Augmented 4Q Dataset</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Hypothesis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Audio clips contain a lot of temporal information. Models based on recurrent neural networks can connect previous and subsequent time steps. Therefore, we tested Recurrent Neural Networks (RNNs), Bidirectional Recurrent Neural Networks (BRNNs), and Long Short-Term Memory (LSTM) on our dataset. We hypothesize that by extracting 14 spectral and rhythmic features, RNNs-related models(RNNs, BRNNs, LSTM) can perform better than traditional Machine Learning models to classify audio clips into 4 Russell’s Emotion Quadrants.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baseline Models</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The baseline classifiers we select encompass a diverse set of machine-learning algorithms that include both traditional statistical models and more advanced machine-learning techniques. This collection comprises Logistic Regression (LR), RidgeClassifier (RC), Linear Support Vector Classification (LSVM), and Support Vector Machine (SVM), which are commonly used for linear and non-linear decision boundary classification. Additionally, the ensemble-based Random Forest Classifier (RFC) and AdaBoost Classifier (ABC) offer improved predictive performance through the aggregation of multiple learners. Gaussian Naive Bayes (GNB), Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis (QDA) apply probabilistic frameworks to classify data based on statistical distributions. One-vs-Rest Classifier (OvRC) simplifies multi-class problems into multiple binary classifications. The K-Neighbors Classifier (KNC) utilizes neighborhood-based learning, while the Multilayer Perceptron (MLP) represents a deep learning approach for more complex patterns. We suppose these classifiers provide a comprehensive and varied set of benchmarks for comparative analysis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">From the result of 12 scikit-learn classifiers shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F2" title="Figure 2 ‣ 4.2 Baseline Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>, the top three best models are Logistic Regression, Linear Discriminant Analysis, and One-vs-Rest Classifier with accuracy 0.633, 0.622, 0.622 respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="394" id="S4.F2.g1" src="extracted/5589538/images/Baseline.png" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Plots of Test Accuracy of Baseline Models</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>RNNs Models</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.3">Recurrent Neural Networks(RNNs) are specialized for processing sequential data <math alttext="x(t)=x(1),…,x(\tau)" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.6"><semantics id="S4.SS3.p1.1.m1.6a"><mrow id="S4.SS3.p1.1.m1.6.6" xref="S4.SS3.p1.1.m1.6.6.cmml"><mrow id="S4.SS3.p1.1.m1.6.6.4" xref="S4.SS3.p1.1.m1.6.6.4.cmml"><mi id="S4.SS3.p1.1.m1.6.6.4.2" xref="S4.SS3.p1.1.m1.6.6.4.2.cmml">x</mi><mo id="S4.SS3.p1.1.m1.6.6.4.1" xref="S4.SS3.p1.1.m1.6.6.4.1.cmml">⁢</mo><mrow id="S4.SS3.p1.1.m1.6.6.4.3.2" xref="S4.SS3.p1.1.m1.6.6.4.cmml"><mo id="S4.SS3.p1.1.m1.6.6.4.3.2.1" stretchy="false" xref="S4.SS3.p1.1.m1.6.6.4.cmml">(</mo><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">t</mi><mo id="S4.SS3.p1.1.m1.6.6.4.3.2.2" stretchy="false" xref="S4.SS3.p1.1.m1.6.6.4.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p1.1.m1.6.6.3" xref="S4.SS3.p1.1.m1.6.6.3.cmml">=</mo><mrow id="S4.SS3.p1.1.m1.6.6.2.2" xref="S4.SS3.p1.1.m1.6.6.2.3.cmml"><mrow id="S4.SS3.p1.1.m1.5.5.1.1.1" xref="S4.SS3.p1.1.m1.5.5.1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.5.5.1.1.1.2" xref="S4.SS3.p1.1.m1.5.5.1.1.1.2.cmml">x</mi><mo id="S4.SS3.p1.1.m1.5.5.1.1.1.1" xref="S4.SS3.p1.1.m1.5.5.1.1.1.1.cmml">⁢</mo><mrow id="S4.SS3.p1.1.m1.5.5.1.1.1.3.2" xref="S4.SS3.p1.1.m1.5.5.1.1.1.cmml"><mo id="S4.SS3.p1.1.m1.5.5.1.1.1.3.2.1" stretchy="false" xref="S4.SS3.p1.1.m1.5.5.1.1.1.cmml">(</mo><mn id="S4.SS3.p1.1.m1.2.2" xref="S4.SS3.p1.1.m1.2.2.cmml">1</mn><mo id="S4.SS3.p1.1.m1.5.5.1.1.1.3.2.2" stretchy="false" xref="S4.SS3.p1.1.m1.5.5.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p1.1.m1.6.6.2.2.3" xref="S4.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mi id="S4.SS3.p1.1.m1.4.4" mathvariant="normal" xref="S4.SS3.p1.1.m1.4.4.cmml">…</mi><mo id="S4.SS3.p1.1.m1.6.6.2.2.4" xref="S4.SS3.p1.1.m1.6.6.2.3.cmml">,</mo><mrow id="S4.SS3.p1.1.m1.6.6.2.2.2" xref="S4.SS3.p1.1.m1.6.6.2.2.2.cmml"><mi id="S4.SS3.p1.1.m1.6.6.2.2.2.2" xref="S4.SS3.p1.1.m1.6.6.2.2.2.2.cmml">x</mi><mo id="S4.SS3.p1.1.m1.6.6.2.2.2.1" xref="S4.SS3.p1.1.m1.6.6.2.2.2.1.cmml">⁢</mo><mrow id="S4.SS3.p1.1.m1.6.6.2.2.2.3.2" xref="S4.SS3.p1.1.m1.6.6.2.2.2.cmml"><mo id="S4.SS3.p1.1.m1.6.6.2.2.2.3.2.1" stretchy="false" xref="S4.SS3.p1.1.m1.6.6.2.2.2.cmml">(</mo><mi id="S4.SS3.p1.1.m1.3.3" xref="S4.SS3.p1.1.m1.3.3.cmml">τ</mi><mo id="S4.SS3.p1.1.m1.6.6.2.2.2.3.2.2" stretchy="false" xref="S4.SS3.p1.1.m1.6.6.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.6b"><apply id="S4.SS3.p1.1.m1.6.6.cmml" xref="S4.SS3.p1.1.m1.6.6"><eq id="S4.SS3.p1.1.m1.6.6.3.cmml" xref="S4.SS3.p1.1.m1.6.6.3"></eq><apply id="S4.SS3.p1.1.m1.6.6.4.cmml" xref="S4.SS3.p1.1.m1.6.6.4"><times id="S4.SS3.p1.1.m1.6.6.4.1.cmml" xref="S4.SS3.p1.1.m1.6.6.4.1"></times><ci id="S4.SS3.p1.1.m1.6.6.4.2.cmml" xref="S4.SS3.p1.1.m1.6.6.4.2">𝑥</ci><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑡</ci></apply><list id="S4.SS3.p1.1.m1.6.6.2.3.cmml" xref="S4.SS3.p1.1.m1.6.6.2.2"><apply id="S4.SS3.p1.1.m1.5.5.1.1.1.cmml" xref="S4.SS3.p1.1.m1.5.5.1.1.1"><times id="S4.SS3.p1.1.m1.5.5.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.5.5.1.1.1.1"></times><ci id="S4.SS3.p1.1.m1.5.5.1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.5.5.1.1.1.2">𝑥</ci><cn id="S4.SS3.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.2.2">1</cn></apply><ci id="S4.SS3.p1.1.m1.4.4.cmml" xref="S4.SS3.p1.1.m1.4.4">…</ci><apply id="S4.SS3.p1.1.m1.6.6.2.2.2.cmml" xref="S4.SS3.p1.1.m1.6.6.2.2.2"><times id="S4.SS3.p1.1.m1.6.6.2.2.2.1.cmml" xref="S4.SS3.p1.1.m1.6.6.2.2.2.1"></times><ci id="S4.SS3.p1.1.m1.6.6.2.2.2.2.cmml" xref="S4.SS3.p1.1.m1.6.6.2.2.2.2">𝑥</ci><ci id="S4.SS3.p1.1.m1.3.3.cmml" xref="S4.SS3.p1.1.m1.3.3">𝜏</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.6c">x(t)=x(1),…,x(\tau)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.6d">italic_x ( italic_t ) = italic_x ( 1 ) , … , italic_x ( italic_τ )</annotation></semantics></math>, where <math alttext="t" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_t</annotation></semantics></math> represents the time step index ranging from 1 to <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">italic_τ</annotation></semantics></math>. In this paper, we will explore three specific RNN models: standard RNNs, Bidirectional RNNs(BRNNs), and Long Short-Term Memory(LSTM), which is a specialized variant of RNNs. We will discuss these models in greater detail in the following sections.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">While implementing the code of these models, we will use the AdamW optimizer instead of Adam due to its improved implementation of weight decay. Additionally, we will compute cross-entropy loss, a prevalent method for classification tasks which is effective for calculating loss when outputs are probabilities. For RNNs and BRNNs, gradient clipping is applied to prevent gradient explosion or vanishing, whereas LSTM already prevents such issues as a highlight for its model, therefore gradient clipping is not applied in LSTM. Early-stopping is not used in this experiment, instead, we manually choose the iteration number(epoch number) to stop where the evaluation accuracy won’t increase after that stop point.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>RNNs</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">RNNs possess a form of “memory” that captures information from previous computations. Essentially, an RNN can be un-rolled to multiple copies of the same network, where each instance passes information to its successor.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S4.F3.g1" src="extracted/5589538/images/UnrolledRNN.png" width="510"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Unrolled RNNs Models</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F3" title="Figure 3 ‣ 4.3.1 RNNs ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">3</span></a> displays RNNs after unrolling. We will use numerical features extracted from 900 audio clips using Librosa as inputs for these RNNs. Since the basic structures are similar, these inputs will also be used for the other two RNNs models we plan to explore.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p3.1.1">Model Input</span>: After data preparation, the 14 features of our original dataset, transformed using the Librosa package, will be organized into a vector of shape (204, 1295) for each of the 900 samples, where 1295 represents the number of time frames analyzed by Librosa. Consequently, our dataset will have the shape ([900, 204, 1295]), and the labels will be shaped ([900]), containing values 0, 1, 2, 3. These values represent the 1st, 2nd, 3rd, and 4th quadrants of Russell’s Emotion Quadrant, respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p4">
<p class="ltx_p" id="S4.SS3.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p4.1.1">Model Output</span>: The output will be one of the 0, 1, 2, 3 which is predicted by the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p5">
<p class="ltx_p" id="S4.SS3.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p5.1.1">Model Architecture</span>: We used Dropout with probability 0.2 and RELU as the activation function within the fully connected layer. Since we are using many-to-one RNNs, The hidden state of the last time step will be passed to the fully connected layer instead of the output of the model to get the final output.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p6">
<p class="ltx_p" id="S4.SS3.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p6.1.1">Train</span>: For the basic RNNs model, we run 2000 epochs and get the train accuracy and evaluation accuracy shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F4" title="Figure 4 ‣ 4.3.1 RNNs ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S4.F4.g1" src="extracted/5589538/images/RNNs_train_eval_acc_2000epoch.png" width="393"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Train and Evaluation Accuracy vs. Iterations for RNNs</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p7">
<p class="ltx_p" id="S4.SS3.SSS1.p7.1">We pick the epoch numbers to stop by viewing if the evaluation accuracy starts to decrease on a large scale, or if the training accuracy stops increasing.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>BRNNs</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Bidirectional Recurrent Neural Networks (BRNNs) can connect to both previous and subsequent time steps, unlike basic RNNs, which only link to previous time steps. We plan to explore whether this model offers an improvement over the traditional RNNs.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="110" id="S4.F5.g1" src="extracted/5589538/images/bidir-RNN.png" width="118"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Bidirectional RNNs</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F5" title="Figure 5 ‣ 4.3.2 BRNNs ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the typical architecture of bidirectional RNNs. Notably, the hidden states are passed both from the initial to the final time steps and vice versa. This dual passage facilitates the formation of forward and backward connections, enhancing the network’s understanding of the data sequence.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p3.1.1">Model Architecture</span>: BRNNs share the same architecture as RNNs except it is bidirectional rather than mono-directional.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p4">
<p class="ltx_p" id="S4.SS3.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p4.1.1">Train</span>: We run 1000 epochs and plot the train accuracy and evaluation accuracy as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F6" title="Figure 6 ‣ 4.3.2 BRNNs ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="381" id="S4.F6.g1" src="extracted/5589538/images/BiRNNs_train_eval_acc_1000.png" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Train and Evaluation Accuracy vs. Iterations for BRNNs</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p5">
<p class="ltx_p" id="S4.SS3.SSS2.p5.1">After monitoring the training and evaluation accuracy across 10,000 epochs, we observed that the evaluation accuracy plateaued after 1,000 epochs. Consequently, we decided to halt the training at this point.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>LSTM</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Long Short-Term Memory (LSTM) networks are designed to address issues of gradient explosion and vanishing. They have the capability to forget non-essential information and retain important content. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S4.F7.sf1.g1" src="extracted/5589538/images/LSTM_2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>LSTM Model</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S4.F7.sf2.g1" src="extracted/5589538/images/zCompute.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Formula of z in LSTM model</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>LSTM</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.6">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F7.sf1" title="In Figure 7 ‣ 4.3.3 LSTM ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">7(a)</span></a> depicts the structure of an LSTM model. Initially, the <math alttext="z^{f}" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.1.m1.1"><semantics id="S4.SS3.SSS3.p2.1.m1.1a"><msup id="S4.SS3.SSS3.p2.1.m1.1.1" xref="S4.SS3.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS3.p2.1.m1.1.1.2" xref="S4.SS3.SSS3.p2.1.m1.1.1.2.cmml">z</mi><mi id="S4.SS3.SSS3.p2.1.m1.1.1.3" xref="S4.SS3.SSS3.p2.1.m1.1.1.3.cmml">f</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.1.m1.1b"><apply id="S4.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1.2">𝑧</ci><ci id="S4.SS3.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.1.m1.1c">z^{f}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.1.m1.1d">italic_z start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT</annotation></semantics></math> gate controls the aspects of the previous state, <math alttext="c^{t-1}" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.2.m2.1"><semantics id="S4.SS3.SSS3.p2.2.m2.1a"><msup id="S4.SS3.SSS3.p2.2.m2.1.1" xref="S4.SS3.SSS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.SSS3.p2.2.m2.1.1.2" xref="S4.SS3.SSS3.p2.2.m2.1.1.2.cmml">c</mi><mrow id="S4.SS3.SSS3.p2.2.m2.1.1.3" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.cmml"><mi id="S4.SS3.SSS3.p2.2.m2.1.1.3.2" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S4.SS3.SSS3.p2.2.m2.1.1.3.1" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.1.cmml">−</mo><mn id="S4.SS3.SSS3.p2.2.m2.1.1.3.3" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.2.m2.1b"><apply id="S4.SS3.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS3.SSS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1.2">𝑐</ci><apply id="S4.SS3.SSS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1.3"><minus id="S4.SS3.SSS3.p2.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.1"></minus><ci id="S4.SS3.SSS3.p2.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.2">𝑡</ci><cn id="S4.SS3.SSS3.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS3.SSS3.p2.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.2.m2.1c">c^{t-1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.2.m2.1d">italic_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, that should be forgotten, selectively erasing unimportant elements. Next, the <math alttext="z^{i}" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.3.m3.1"><semantics id="S4.SS3.SSS3.p2.3.m3.1a"><msup id="S4.SS3.SSS3.p2.3.m3.1.1" xref="S4.SS3.SSS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.SSS3.p2.3.m3.1.1.2" xref="S4.SS3.SSS3.p2.3.m3.1.1.2.cmml">z</mi><mi id="S4.SS3.SSS3.p2.3.m3.1.1.3" xref="S4.SS3.SSS3.p2.3.m3.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.3.m3.1b"><apply id="S4.SS3.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1">superscript</csymbol><ci id="S4.SS3.SSS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1.2">𝑧</ci><ci id="S4.SS3.SSS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.SSS3.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.3.m3.1c">z^{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.3.m3.1d">italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> gate determines which new information to retain. This selected data, combined with the input from the previous state, <math alttext="z" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.4.m4.1"><semantics id="S4.SS3.SSS3.p2.4.m4.1a"><mi id="S4.SS3.SSS3.p2.4.m4.1.1" xref="S4.SS3.SSS3.p2.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.4.m4.1b"><ci id="S4.SS3.SSS3.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS3.p2.4.m4.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.4.m4.1c">z</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.4.m4.1d">italic_z</annotation></semantics></math>, and the result of the “forgetting” process, is passed to the next stage, <math alttext="c^{t}" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.5.m5.1"><semantics id="S4.SS3.SSS3.p2.5.m5.1a"><msup id="S4.SS3.SSS3.p2.5.m5.1.1" xref="S4.SS3.SSS3.p2.5.m5.1.1.cmml"><mi id="S4.SS3.SSS3.p2.5.m5.1.1.2" xref="S4.SS3.SSS3.p2.5.m5.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS3.p2.5.m5.1.1.3" xref="S4.SS3.SSS3.p2.5.m5.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.5.m5.1b"><apply id="S4.SS3.SSS3.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.SSS3.p2.5.m5.1.1">superscript</csymbol><ci id="S4.SS3.SSS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.SSS3.p2.5.m5.1.1.2">𝑐</ci><ci id="S4.SS3.SSS3.p2.5.m5.1.1.3.cmml" xref="S4.SS3.SSS3.p2.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.5.m5.1c">c^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.5.m5.1d">italic_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>. Lastly, the zo gate manages the output <math alttext="y^{t}" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.6.m6.1"><semantics id="S4.SS3.SSS3.p2.6.m6.1a"><msup id="S4.SS3.SSS3.p2.6.m6.1.1" xref="S4.SS3.SSS3.p2.6.m6.1.1.cmml"><mi id="S4.SS3.SSS3.p2.6.m6.1.1.2" xref="S4.SS3.SSS3.p2.6.m6.1.1.2.cmml">y</mi><mi id="S4.SS3.SSS3.p2.6.m6.1.1.3" xref="S4.SS3.SSS3.p2.6.m6.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.6.m6.1b"><apply id="S4.SS3.SSS3.p2.6.m6.1.1.cmml" xref="S4.SS3.SSS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS3.p2.6.m6.1.1.1.cmml" xref="S4.SS3.SSS3.p2.6.m6.1.1">superscript</csymbol><ci id="S4.SS3.SSS3.p2.6.m6.1.1.2.cmml" xref="S4.SS3.SSS3.p2.6.m6.1.1.2">𝑦</ci><ci id="S4.SS3.SSS3.p2.6.m6.1.1.3.cmml" xref="S4.SS3.SSS3.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.6.m6.1c">y^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.6.m6.1d">italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, controlling what is ultimately transmitted to the next layer or used for predictions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p3.1.1">Model Architecture</span>: For LSTM, we construct 4 fully connected layers to make the model more complex so that it can help learn more details about our data. The first three layers are followed by batch normalization, RELU activation, and dropout. The final linear layer shapes the final output for the classification.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p4">
<p class="ltx_p" id="S4.SS3.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p4.1.1">Train</span>: We run 250 epochs and plot the train accuracy and evaluation accuracy as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.F8" title="Figure 8 ‣ 4.3.3 LSTM ‣ 4.3 RNNs Models ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S4.F8.g1" src="extracted/5589538/images/LSTMacc.png" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Train and Evaluation Accuracy vs. Iterations for LSTM</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p5">
<p class="ltx_p" id="S4.SS3.SSS3.p5.1">We limited training to 250 epochs because we observed that the evaluation accuracy showed no significant changes beyond this point. However, it is worth noting that the training accuracy continued to exhibit a positive upward trend.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Conclusions</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S4.T1" title="Table 1 ‣ 4.4 Conclusions ‣ 4 Experiments ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a> compares the test accuracy of three RNNs-related models: RNNs, BRNNs, and LSTM. Surprisingly, RNNs achieved the highest test accuracy at 53.33%. Initially, we anticipated that LSTM would perform the best. However, the results shown in Table 1 may be influenced by our small datasets, which contain only 900 samples in total—720 for training and 90 each for evaluation and testing.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Test Accuracy of RNNs, BRNNs, LSTM</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">RNNs-Related Models</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">Test Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.1">RNNs</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.2">53.33%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.1">BRNNs</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.2">48.89%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.4.3.1">LSTM</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.4.3.2">51.11%</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Given that RNNs are the simplest model among the three, this outcome suggests that simpler models often have better generalization capabilities, particularly with small datasets. Although the results were unexpected, the positive trend in training accuracy indicates that our models are performing well. We plan to test these models with a much larger dataset to see if they align more closely with our expectations.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Baseline Models vs Neural Networks </h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">Comparing the neural networks and baseline models, the neural network models generally perform no better than the traditional machine learning models. This outcome may be attributed to the constrained size of the dataset. Simpler models are potentially more effective in extracting information from the dataset, whereas neural networks could lead to overfitting. Larger datasets will further be considered to test our model.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Further Works 1 - Augment Original Data</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We suspect that the low model performance is due to the limited size of our dataset. To address this issue, we decide to augment the original dataset. There are many options to enlarge the dataset, we will use the techniques of adding random noise, time shifting, and pitch changing. These modifications will be applied to the original data to create a more robust dataset. Specifically, we expect to see positive outcomes with the expanded dataset, which will comprise 3600 data samples, each featuring 14 attributes and one label column. This augmented dataset should provide a more comprehensive basis for training our model, potentially leading to more accurate and reliable results.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To enhance our dataset and improve the performance of our neural network, we have employed three different augmentation techniques on the original set of 900 samples. As a result, our dataset now includes a total of 3600 samples, with each of the four emotional quadrants—Q1, Q2, Q3, and Q4—equally represented by 900 samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To prepare this augmented data for neural network training, we have standardized the dataset format. Specifically, we padded and reshaped the data to ensure uniformity across samples. The resulting dataset is structured into an array of shapes (3600, 204, 1290), where 3600 represents the total number of samples, 204 is the number of features per sample, and 1290 denotes the number of time steps per sample. This uniform structuring is critical for training our neural network effectively, as it ensures that each input is consistently formatted and fully compatible with the network architecture.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Models</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Baseline</h4>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">We tested our baseline models on the augmented dataset. From the result of 12 scikit-learn classifiers shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.F9" title="Figure 9 ‣ 5.2.1 Baseline ‣ 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">9</span></a>, the top three best models are One-vs-Rest Classifier, Logistic Regression, and RidgeClassifier with accuracy 0.836, 0.824, 0.822 respectively. Compared with the original 4Q audio emotion dataset, test accuracy has generally improved, and it has increased by 20% on models with good performance.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="S5.F9.g1" src="extracted/5589538/images/BaselineAugment.png" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Plots of Test Accuracy of Baseline Models on Augmented Dataset</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>RNN-Related Models</h4>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">To ensure the robustness and generalizability of our models, we implemented a 5-fold cross-validation approach during training for all three models: RNNs, BRNNs, and LSTM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">In this process, the dataset was divided into five distinct subsets. For each fold, one subset was used as the validation set while the remaining four were used for training. To prevent data leakage and ensure that the models generalize well to new data, we normalized the data in each fold using the statistical parameters (mean and standard deviation) calculated from the training subsets only.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Throughout the training process, we tracked and recorded the average validation loss for each fold. This measure helps us assess the performance and stability of the models across different subsets of the data, providing insights into their effectiveness and potential areas for improvement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p4">
<p class="ltx_p" id="S5.SS2.SSS2.p4.1">Different from our original experiment, we used early-stopping techniques at this time. This is aimed to prevent over-training, which might negatively affect the results.</p>
</div>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="S5.F10.sf1.g1" src="extracted/5589538/images/RNNAug.png" width="658"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>RNN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="S5.F10.sf2.g1" src="extracted/5589538/images/BRNNAug.png" width="658"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>BRNN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F10.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="344" id="S5.F10.sf3.g1" src="extracted/5589538/images/LSTMAug.png" width="658"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>LSTM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Evaluation Loss vs. Folds</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p5">
<p class="ltx_p" id="S5.SS2.SSS2.p5.1">Through the three plots about evaluation loss versus folds numbers shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.F10" title="Figure 10 ‣ 5.2.2 RNN-Related Models ‣ 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">10</span></a>, we can notice that reaching the 5th fold, BRNNs show the lowest evaluation loss, then LSTM, then RNNs. From Table <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.T2" title="Table 2 ‣ 5.2.2 RNN-Related Models ‣ 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a> below, we can notice that the test performance for all three models improved. From the original data to augmented data, the test accuracy improved by 20%, 19%, and 30% for RNNs, BRNNs, and LSTM, respectively. We fixed our model while applying the modifications to our dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS2.p6">
<p class="ltx_p" id="S5.SS2.SSS2.p6.1">In our analysis, represented through three plots of evaluation loss versus fold numbers, the BRNNs demonstrated the lowest evaluation loss by the 5th fold, followed by LSTM, and RNNs. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S5.T2" title="Table 2 ‣ 5.2.2 RNN-Related Models ‣ 5.2 Models ‣ 5 Further Works 1 - Augment Original Data ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>, transitioning from the original to the augmented dataset significantly improved test accuracy: RNNs by 20%, BRNNs by 19%, and LSTMs by 30%. Throughout this evaluation, our model architecture remained constant, highlighting that improvements were solely due to the augmented dataset’s enhanced quality and diversity. This confirms that dataset augmentation effectively boosts model performance.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Test Accuracy of RNNs, BRNNs, LSTM between 900 and Augmented Data</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1">RNNs-Related Models</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2">Test Accuracy(Original)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3">Test Accuracy(Augmented)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.1">RNNs</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.2">53.33%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.3">64.03% ↑</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.1">BRNNs</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.2">48.89%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.3">58.33% ↑</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.4.3.1">LSTM</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.4.3.2">51.11%</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.4.3.3">65.97% ↑</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Conclusion</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Comparing the neural networks and baseline models, the neural network models generally perform no better than the traditional machine learning models. However, the effect of data augmentation is very significant. The test accuracy increased by more than 10% on all three neural networks.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Further Works 2 - Larger Dataset</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We intend to implement the same baseline models alongside Recurrent Neural Networks (RNNs), Bidirectional Recurrent Neural Networks (BRNNs), and Long Short-Term Memory networks (LSTM) on larger datasets. Additionally, we aim to classify the dataset into more granular categories.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Datasets</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We utilized the MTG-Jamendo Dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#bib.bib1" title="">1</a>]</cite>, a recently released open dataset for music auto-tagging which is accessible on GitHub. Given our limited computational resources, we opted for a subset of the dataset named “autotagging-moodtheme/audio-low,” comprising low-quality audio files specific to mood and theme. This subset includes approximately 14,000 audio tracks, each varying in length. The dataset is categorized into 59s distinct sub-genres, as outlined in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.F11" title="Figure 11 ‣ 6.1 Datasets ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">11</span></a>. These sub-genres serve as labels for training our model. Upon completion of the training process, we will manually map these sub-genre labels onto the four quadrants of Russell’s Emotion Quadrant to interpret our model’s output in terms of emotional content.</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S6.F11.g1" src="extracted/5589538/images/59labels.png" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>59 Mood Genres and Their Encodings</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Models</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Baselines</h4>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">We applied a Logistic Regression model as the baseline model for the 59 classes classification and used AOC-ROC plot as Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.F12" title="Figure 12 ‣ 6.2.1 Baselines ‣ 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">12</span></a> to show the result. The model achieves 43.75% accuracy on the test set.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="721" id="S6.F12.g1" src="extracted/5589538/images/ROC.png" width="471"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>ROC Curves For Multiclass Classification Through LR</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>RNNs-Related Models</h4>
<figure class="ltx_figure" id="S6.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F13.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S6.F13.sf1.g1" src="extracted/5589538/images/14kRNNs.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>RNN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F13.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S6.F13.sf2.g1" src="extracted/5589538/images/14kBRNNs.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>BRNN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F13.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S6.F13.sf3.g1" src="extracted/5589538/images/14kLSTM.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>LSTM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Train and Evaluation Accuracy vs. Iterations</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.F13.sf1" title="In Figure 13 ‣ 6.2.2 RNNs-Related Models ‣ 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">13(a)</span></a> displays a plot of training and evaluation accuracy versus 2,000 epochs trained for RNNs model. After the evaluation accuracy passed 0.5, the growth shown on the curve starts to stop.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">From Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.F13.sf2" title="In Figure 13 ‣ 6.2.2 RNNs-Related Models ‣ 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">13(b)</span></a> we can notice that for BRNNs, 400 epochs is a fairly reasonable number to stop training. Though after 400 iterations, the training accuracy still increases, the evaluation accuracy is a line after that showing no tendency to increase anymore.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1">LSTM shows a similar situation as BRNNs. We can notice that the evaluation accuracy stops climbing after 400 iterations from Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.F13.sf3" title="In Figure 13 ‣ 6.2.2 RNNs-Related Models ‣ 6.2 Models ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">13(c)</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Results</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Table 2 shows a comparison of the test accuracy on three RNN-related models between our original 900 samples dataset and later 14,000 samples larger dataset.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Test Accuracy of RNNs, BRNNs, LSTM between 900 and 14,000 Data</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.4.1.1">RNNs-Related Models</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.4.1.2">Test Accuracy (900)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.4.1.3">Test Accuracy (14,000)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.1.1.2">RNNs</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.1.1.3">53.33%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.1.1.1">50.18% <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T3.1.1.1.m1.1"><semantics id="S6.T3.1.1.1.m1.1a"><mo id="S6.T3.1.1.1.m1.1.1" stretchy="false" xref="S6.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.2">
<td class="ltx_td ltx_align_left" id="S6.T3.2.2.2">BRNNs</td>
<td class="ltx_td ltx_align_left" id="S6.T3.2.2.3">48.89%</td>
<td class="ltx_td ltx_align_left" id="S6.T3.2.2.1">53.97% <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.2.2.1.m1.1"><semantics id="S6.T3.2.2.1.m1.1a"><mo id="S6.T3.2.2.1.m1.1.1" stretchy="false" xref="S6.T3.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.1.m1.1b"><ci id="S6.T3.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.1.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.3.3.2">LSTM</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.3.3.3">51.11%</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.3.3.1">52.61% <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.3.3.1.m1.1"><semantics id="S6.T3.3.3.1.m1.1a"><mo id="S6.T3.3.3.1.m1.1.1" stretchy="false" xref="S6.T3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.1.m1.1b"><ci id="S6.T3.3.3.1.m1.1.1.cmml" xref="S6.T3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.3.3.1.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2405.06747v1#S6.T3" title="Table 3 ‣ 6.3 Results ‣ 6 Further Works 2 - Larger Dataset ‣ Music Emotion Prediction Using Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">3</span></a> we can clearly see the test accuracy of RNNs decreased as the dataset becomes larger, whereas both Bidirectional RNNs and LSTM have an increased test accuracy as we have more data samples for training the models. For a larger dataset, we have BRNNs as the model with the best performance among the three, where LSTM leads the second place, and lastly the RNNs model. This proved our hypothesis that larger dataset performs better on more complex models, whereas RNNs cannot capture many complex details within larger datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Conclusions</h3>
<div class="ltx_para ltx_noindent" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Comparing the result of Logistic Regression and neural networks, our model performs significantly better than logistic regression, which indicates that traditional models may be more suitable for small and medium-size datasets. Neural Network models are better suited to capture the complex information of large data sets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">When comparing the results across datasets of varying sizes, we observed that RNNs, BRNNs, and LSTMs performed best on the medium-sized dataset containing 3,600 audio clips. This outcome may be attributed to the fact that larger datasets necessitate more complex models (either deeper or with more intricate structures) to effectively capture information. Our current model is best suited for medium-sized datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Other Notes</h3>
<div class="ltx_para ltx_noindent" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1"><span class="ltx_text ltx_font_italic" id="S6.SS5.p1.1.1">Whether for the larger dataset, a more complex model shows a better performance?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">Within the three RNN models, in the result section of further works 1, LSTM do have the best test accuracy among the three with the most complex model of the three. However, some traditional machine learning models are still outperforming the three RNN models. It is worth testing whether a deep neural network will perform better in the future.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p3">
<p class="ltx_p" id="S6.SS5.p3.1"><span class="ltx_text ltx_font_italic" id="S6.SS5.p3.1.1">Whether RNN-related models are good for music emotion classification?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p4">
<p class="ltx_p" id="S6.SS5.p4.1">From our comparison, RNN-related models perform no better than the traditional machine learning models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p5">
<p class="ltx_p" id="S6.SS5.p5.1"><span class="ltx_text ltx_font_italic" id="S6.SS5.p5.1.1">What are the possible biases in this project?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p6">
<p class="ltx_p" id="S6.SS5.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS5.p6.1.1">Data bias</span>: only takes about the first 30 seconds’ features into training due to limited resources</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p7">
<p class="ltx_p" id="S6.SS5.p7.1"><span class="ltx_text ltx_font_bold" id="S6.SS5.p7.1.1">Labeling bias</span>: cannot manually change 59 mood genres into 4 quadrants, therefore using 59 labels to train the large dataset and record the results in Further Works 2.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS5.p8">
<p class="ltx_p" id="S6.SS5.p8.1">There are still many aspects worth improving in our project for future work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogdanov et al. [2019]</span>
<span class="ltx_bibblock">
Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra.

</span>
<span class="ltx_bibblock">The mtg-jamendo dataset for automatic music tagging.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019)</em>, Long Beach, CA, United States, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://hdl.handle.net/10230/42015" title="">http://hdl.handle.net/10230/42015</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panda et al. [2018a]</span>
<span class="ltx_bibblock">
R. Panda, R. Malheiro, and R. P. Paiva.

</span>
<span class="ltx_bibblock">Musical texture and expressivity features for music emotion recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR 2018)</em>, page unknown, Paris, France, 2018a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panda et al. [2018b]</span>
<span class="ltx_bibblock">
R. Panda, R. Malheiro, and R. P. Paiva.

</span>
<span class="ltx_bibblock">Novel audio features for music emotion recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Affective Computing</em>, 2018b.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TAFFC.2018.2820691" title="">10.1109/TAFFC.2018.2820691</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang [2021]</span>
<span class="ltx_bibblock">
J. Yang.

</span>
<span class="ltx_bibblock">A novel music emotion recognition model using neural network technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Frontiers in Psychology</em>, 2021.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fpsyg.2021.760060" title="">10.3389/fpsyg.2021.760060</a>.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="David S. Hippocampus, Elias D. Striatum" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="First keyword, Second keyword, More" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="q-bio.NC, q-bio.QM" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="A template for the arxiv style" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 10 17:55:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
