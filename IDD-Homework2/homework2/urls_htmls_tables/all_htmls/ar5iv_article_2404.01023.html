<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Large Language Model Evaluation Via Multi AI Agents: Preliminary results
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zeeshan Rasheed
    <br class="ltx_break"/>
    Department of Computer Science
    <br class="ltx_break"/>
    Cranberry-Lemon University
    <br class="ltx_break"/>
    Pittsburgh, PA 15213, USA
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id1.1.id1">
     {hippo,brain,jen}@cs.cranberry-lemon.edu
    </span>
    <br class="ltx_break"/>
    &amp;Ji Q. Ren &amp; Yevgeny LeNet
    <br class="ltx_break"/>
    Department of Computational Neuroscience
    <br class="ltx_break"/>
    University of the Witwatersrand
    <br class="ltx_break"/>
    Joburg, South Africa
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id2.2.id2">
     {robot,net}@wits.ac.za
    </span>
    <br class="ltx_break"/>
    <span class="ltx_ERROR undefined" id="id3.3.id3">
     \AND
    </span>
    Coauthor
    <br class="ltx_break"/>
    Affiliation
    <br class="ltx_break"/>
    Address
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id4.4.id4">
     email
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id5.id1">
   The increasing significance of Large Language Models (LLMs) in both the academic and industrial sectors can be attributed to their exceptional performance across a diverse range of applications. As LLMs have become integral to both research and daily operations, rigorous evaluation is crucial. This assessment is important not only for individual tasks but also for understanding their societal impact and potential risks. Despite extensive efforts to examine LLMs from various perspectives, there is a noticeable lack of multi-agent AI models specifically designed to evaluate the performance of different LLMs. To address this gap, we introduce a novel multi-agent AI model that aims to assess and compare the performance of various LLMs.
Our model consists of eight distinct AI agents, each responsible for retrieving code based on a common description from different advanced language models, including GPT-3.5, GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Google Bard, LLAMA, and Hugging Face. Our developed model utilizes the API of each language model to retrieve code for a given high-level description. Additionally, we developed a verification agent, tasked with the critical role of evaluating the code generated by its counterparts. We integrate the HumanEval benchmark into our verification agent to assess the generated code’s performance, providing insights into their respective capabilities and efficiencies. Our initial results indicate that the GPT-3.5 Turbo model’s performance is comparatively better than the other models. Initially, we provided ten common high-level input descriptions to our proposed model. This preliminary analysis serves as a benchmark, comparing their performances side by side.
Our future goal is to enhance the evaluation process by incorporating the Massively Multitask Benchmark for Python (MBPP) benchmark, which is expected to further refine our assessment. Additionally, we plan to share our developed model with twenty practitioners from various backgrounds to test our model and collect their feedback for further improvement.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent advancements in the field of Large Language Models (LLMs) have brought about significant changes in academia and several industries (
    <cite class="ltx_cite ltx_citemacro_cite">
     Hou et al. (
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     )
    </cite>
    ). LLMs have become crucial subjects of study. For example, LLMs such as OpenAI’s GPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Radford et al. (
     <a class="ltx_ref" href="#bib.bib16" title="">
      2018
     </a>
     )
    </cite>
    , Google’s Bard
    <cite class="ltx_cite ltx_citemacro_cite">
     Thoppilan et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2022
     </a>
     )
    </cite>
    , Bing Chat Enterprise
    <cite class="ltx_cite ltx_citemacro_cite">
     Rudolph et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023
     </a>
     )
    </cite>
    , LLama
    <cite class="ltx_cite ltx_citemacro_cite">
     Touvron et al. (
     <a class="ltx_ref" href="#bib.bib23" title="">
      2023
     </a>
     )
    </cite>
    , and Hugging Face
    <cite class="ltx_cite ltx_citemacro_cite">
     Wolf et al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2019
     </a>
     )
    </cite>
    demonstrate exceptional skills in understanding, interpreting, and generating text that closely resembles human communication, proving to be invaluable assets in various domains. The importance of LLMs extends beyond their technical capabilities to their exceptional proficiency in processing and analyzing vast amounts of data effectively
    <cite class="ltx_cite ltx_citemacro_cite">
     Rasheed et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2024a
     </a>
     )
    </cite>
    . This groundbreaking technology is carving new paths in both research and practical applications, particularly in the domain of Software Engineering (SE)
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    LLMs have recently demonstrated remarkable abilities in processing and interpreting programming languages, a key aspect of source code modeling
    <cite class="ltx_cite ltx_citemacro_cite">
     Alon et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2020
     </a>
     )
    </cite>
    ,
    <cite class="ltx_cite ltx_citemacro_cite">
     Hellendoorn &amp; Devanbu (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2017
     </a>
     )
    </cite>
    ,
    <cite class="ltx_cite ltx_citemacro_cite">
     Karampatsis et al. (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2020
     </a>
     )
    </cite>
    ,
    <cite class="ltx_cite ltx_citemacro_cite">
     Rasheed et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2024b
     </a>
     )
    </cite>
    .
These models, driven by sophisticated algorithms and expansive datasets, have demonstrated an unprecedented ability to comprehend and generate code, offering substantial potential for SE applications
    <cite class="ltx_cite ltx_citemacro_cite">
     Xu et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2022
     </a>
     )
    </cite>
    ,
    <cite class="ltx_cite ltx_citemacro_cite">
     Rasheed et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    . With the proliferation of various LLMs by leading AI research institutions and corporations, such as OpenAI’s GPT series, Google’s Bard, DeepMind’s AlphaCode, and others, It becomes crucial to develop a strong evaluation framework to assess their performance and effectiveness in code generation tasks.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    This paper proposes a novel multi-agent AI model designed for a crucial task. The model utilizes the strengths of eight distinct AI agents, each interacting with a different LLM to retrieve programming code based on the same input. This approach aims to capitalize on the unique characteristics and methodologies of each LLM, offering a comprehensive view of the capabilities in automated code generation. We use the API key of each model to generate code. The agents work with various LLMs including GPT-4, GPT-4 Turbo, GPT-3.5, GPT-3.5 Turbo, Google Bard, LLAMA, and Hugging Face. Our proposed AI agent model facilitates code generation and conducts initial assessments to guarantee the quality and pertinence of the results. This paper shares early insight into how the model works in practice.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    The crux of our model lies in the eighth agent, the verification agent, whose primary objective is to perform a meticulous comparison of the code produced by its seven counterparts. This agent employs HumanEval benchmark
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2021
     </a>
     )
    </cite>
    to assesses various attributes of the generated code, such as syntactic correctness, adherence to the prompt, computational efficiency, and code accuracy. We utilized pass@k metric into our proposed AI model, enabling a direct evaluation of the code generated by various models. Pass@k metric is a method in HumanEval benchmark and a key component of the HumanEval benchmark, serves as a standard measure specifically tailored for appraising the proficiency of code generation models in creating functional and accurate code
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2022
     </a>
     )
    </cite>
    . This benchmark is instrumental in providing a comprehensive and objective assessment of model performance, ensuring the reliability and effectiveness of our AI model in code generation tasks.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Our initial result demonstrates that the GPT-3.5 Turbo model exhibits superior performance in generating accurate code compared to the other models discussed earlier. We tested this by providing ten common, high-level descriptions as inputs, with each model generating code based on these descriptions. Remarkably, GPT-3.5 Turbo achieved a 70% accuracy rate in code generation, outperforming its counterparts. Following closely, GPT-4 Turbo emerged as the second most effective model, yielding satisfactory results in 6 out of 10 cases. This study significantly contributes to the ongoing discourse on the practical applications of LLMs and aims to guide stakeholders in making informed, data-driven decisions when integrating these models into their development workflows.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    Our future endeavors are focused on augmenting the evaluation process. We aim to incorporate the MBPP benchmark
    <cite class="ltx_cite ltx_citemacro_cite">
     Austin et al. (
     <a class="ltx_ref" href="#bib.bib2" title="">
      2021
     </a>
     )
    </cite>
    into our multi AI agent model, which we anticipate will significantly enhance the precision of our evaluations. To this end, we will develop a specialized AI agent that integrates the MBPP benchmark for a more thorough examination of the generated code. Furthermore, we intend to expand our input description set from the current 10 to 50, thereby enabling a more robust and comprehensive analysis of our proposed model. In addition to these technical enhancements, we are committed to engaging with the wider community by sharing our model with twenty practitioners from diverse backgrounds. This collaborative approach will not only provide us with valuable real-world testing but also facilitate the collection of insightful feedback, which is essential for the continuous improvement of our model. This final aspect of our plan underscores our commitment to both technical excellence and practical relevance in the evolving field of language model evaluation.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    Our contribution can be summarized as follow:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p8">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       Our study introduces an innovative model for code generation, offering a unique approach to the evaluation of various LLMs such as GPT-3.5, GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Google Bard, LLama, and Hugging Face.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p9">
   <ul class="ltx_itemize" id="S1.I2">
    <li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I2.i1.p1">
      <p class="ltx_p" id="S1.I2.i1.p1.1">
       We integrated the HumanEval benchmark into our verification agent and provided a detailed comparison of each model’s code generation abilities, highlighting their strengths and weaknesses. Empirical tests with 10 common project descriptions show GPT-3.5 Turbo leading in accuracy, outperforming others in 7 out of 10 cases.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p10">
   <ul class="ltx_itemize" id="S1.I3">
    <li class="ltx_item" id="S1.I3.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I3.i1.p1">
      <p class="ltx_p" id="S1.I3.i1.p1.1">
       Our future goal is to enhance the evaluation process by integrating the MBPP benchmark into our specialized AI agent for comprehensive code examination, and increase input descriptions from 10 to 50 for more detailed analysis. Additionally, we will share the model with twenty diverse practitioners for real-world testing and feedback, underlining our commitment to technical excellence and practical application in language model evaluation
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p11">
   <p class="ltx_p" id="S1.p11.1">
    The rest of the paper is organized as follows: Section
    <a class="ltx_ref" href="#S2" title="2 Related Work ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    reviews related work, and Section
    <a class="ltx_ref" href="#S3" title="3 Research Methodology for Preliminary Analysis ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    describes the study’s methodology. Primary results are presented in Section
    <a class="ltx_ref" href="#S4" title="4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    , and details of our future plans are outlined in Section
    <a class="ltx_ref" href="#S5" title="5 Future Work ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    . The paper concludes in Section
    <a class="ltx_ref" href="#S6" title="6 Conclusions ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    .
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    In this section, we briefly present the related work of the study, focusing on existing research. Section
    <a class="ltx_ref" href="#S2.SS1" title="2.1 Large Language Models ‣ 2 Related Work ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      2.1
     </span>
    </a>
    provides an overview of studies concerning Large Language Models (LLMs). Section
    <a class="ltx_ref" href="#S2.SS2" title="2.2 Large Language Models Evaluations ‣ 2 Related Work ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      2.2
     </span>
    </a>
    examines works that have developed models for evaluating LLMs.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Large Language Models
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     In recent years, LLMs have shown promise in various SE applications
     <cite class="ltx_cite ltx_citemacro_cite">
      Feng et al. (
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
      )
     </cite>
     . LLMs are language models consisting of billions of parameters trained from a significant amount of data and have impressive performance in language processing tasks, including both natural languages and programming languages
     <cite class="ltx_cite ltx_citemacro_cite">
      Feng et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2020
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_cite">
      Guo et al. (
      <a class="ltx_ref" href="#bib.bib10" title="">
       2022
      </a>
      )
     </cite>
     . LLMs designed for processing and generating human-like text and code, ChatGPT, Google Bard, LLAMA, and Bing Chat Enterprise are prominent examples within this category
     <cite class="ltx_cite ltx_citemacro_cite">
      Eloundou et al. (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     . These LLM models are particularly notable for their ability to perform a wide range of language tasks, from translation and summarizing to question-answering and creative writing, without needing task-specific training.
The core module behind many LLMs such as GPT and BERT are the self-attention module in Transformer
     <cite class="ltx_cite ltx_citemacro_cite">
      Vaswani et al. (
      <a class="ltx_ref" href="#bib.bib24" title="">
       2017
      </a>
      )
     </cite>
     that serves as the fundamental building block for language modeling tasks. The introduction of transformer models, as exemplified by OpenAI’s GPT series, marked a new era in code generation. These models, with their ability to process vast amounts of data and learn from a wide range of programming languages and styles, significantly outperformed their predecessors. GPT-3, in particular, demonstrated an unprecedented ability to generate human-like code, offering not only code completion but also bug fixing, code translation, and even generation of complex algorithms from natural language descriptions
     <cite class="ltx_cite ltx_citemacro_cite">
      Brown et al. (
      <a class="ltx_ref" href="#bib.bib3" title="">
       2020
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Over the past few years, LLMs have demonstrated remarkable efficiency in enabling automatic programming
     <cite class="ltx_cite ltx_citemacro_cite">
      Xu et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_cite">
      Sami et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2024
      </a>
      )
     </cite>
     . For example, OpenAI’s suite of models has emerged as a notable force, exhibiting a profound ability in both understanding and generating code with high accuracy and efficiency
     <cite class="ltx_cite ltx_citemacro_cite">
      Radford et al. (
      <a class="ltx_ref" href="#bib.bib16" title="">
       2018
      </a>
      )
     </cite>
     . Concurrently, Google’s Bard and Bing Enterprises have introduced their own iterations of LLMs, tailored to enhance programming automation with distinct features and methodologies
     <cite class="ltx_cite ltx_citemacro_cite">
      Rudolph et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2023
      </a>
      )
     </cite>
     . Equally important in this space are the LLaMA and LLaMA2 models, which have carved out a niche in code generation, noted for their robustness and adaptability
     <cite class="ltx_cite ltx_citemacro_cite">
      Touvron et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     . Hugging Face’s contributions are equally significant, offering a diverse array of models that specialize in code generation
     <cite class="ltx_cite ltx_citemacro_cite">
      Wolf et al. (
      <a class="ltx_ref" href="#bib.bib26" title="">
       2019
      </a>
      )
     </cite>
     . These models stand out for their user-friendly interfaces and community-driven approach, making them accessible to a wider range of developers and researchers
     <cite class="ltx_cite ltx_citemacro_cite">
      Chang et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     . Each of these LLMs has propelled the field of automated code generation forward, presenting a variety of tools and platforms that cater to different aspects of programming and software development. This proliferation of models not only highlights the rapid advancements in AI-assisted programming but also underscores the potential for further innovation and exploration within this domain
     <cite class="ltx_cite ltx_citemacro_cite">
      Guo et al. (
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      )
     </cite>
     . The comparative analysis of these models reveals a landscape rich with possibilities, offering numerous avenues for research and application in the broader context of artificial intelligence and machine learning.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Large Language Models Evaluations
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     It’s important to assess how well LLMs perform in creating code. This helps make sure these sophisticated AI models are useful and dependable for actual software development tasks. Checking their performance is essential to confirm that the code they generate is accurate, efficient, and secure, matching the strict quality demands of the software field. Additionally, regular evaluations help in identifying areas for improvement, ensuring continuous advancement in AI-driven coding. This process not only boosts the confidence of developers in using LLMs but also contributes to the overall growth and innovation in the technology industry. Several researchers, such as
     <cite class="ltx_cite ltx_citemacro_cite">
      Xu et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_cite">
      Chang et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_cite">
      Guo et al. (
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      )
     </cite>
     , have conducted surveys of existing LLM models evaluation for code across various programming languages. The primary goal of these surveys is to shed more light on the landscape of code modeling design decisions by comparing and contrasting LLM models.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     The development of metrics for evaluating code generated by LLMs has seen significant advancements in recent years, particularly with the introduction of models like CodeBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      Feng et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2020
      </a>
      )
     </cite>
     , BertScore
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib28" title="">
       2019
      </a>
      )
     </cite>
     , Codex
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib6" title="">
       2021
      </a>
      )
     </cite>
     , PolyCoder
     <cite class="ltx_cite ltx_citemacro_cite">
      Xu et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     , and the development of new evaluation metrics like ICE-Score
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhuo (
      <a class="ltx_ref" href="#bib.bib29" title="">
       2023
      </a>
      )
     </cite>
     . CodeBERT developed by Microsoft, which stands out as a significant milestone in code evaluation. As a bimodal model trained on both natural language and programming languages, it offers a nuanced approach to assess the semantic correctness of code snippets
     <cite class="ltx_cite ltx_citemacro_cite">
      Mashhadi &amp; Hemmati (
      <a class="ltx_ref" href="#bib.bib15" title="">
       2021
      </a>
      )
     </cite>
     . CodeBERT’s ability to understand the context and functionality of the code has made it a valuable tool for evaluating the quality of LLM-generated code. After one year. Following CodeBERT, OpenAI introduced Codex, an AI model based on GPT-3 but fine-tuned for programming tasks. Codex marked a significant step forward in code generation, capable of understanding and generating complex code sequences. Its performance necessitated the development of advanced metrics that could effectively evaluate both the syntactic and logical aspects of the code it generated
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib6" title="">
       2021
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_cite">
      Xu et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     introduce PolyCoder, an autoregressive language model specifically designed for coding, further advanced the field. PolyCoder’s specialization in code generation emphasized the need for robust metrics capable of assessing intricate code structures and the model’s comprehension of programming paradigms. According to
     <cite class="ltx_cite ltx_citemacro_cite">
      Xu et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     , PolyCoder outperformed codex. Furthermore, using human-written test suites to evaluate the functional correctness of LLM-generated code can be challenging, especially in domains where resources are scarce. To address these challenges and limitations, a new metric, ICE-Score, was proposed. ICE-Score revolutionizes code evaluation by instructing LLMs for code assessments, offering a novel approach to overcome the obstacles of traditional metrics
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhuo (
      <a class="ltx_ref" href="#bib.bib29" title="">
       2023
      </a>
      )
     </cite>
     . It represents a significant step forward in accurately assessing the practical utility and reliability of code produced by advanced AI models.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.1">
     However, when it comes to multi-AI agent models, there is a notable gap in the development of multi-agent systems that can directly retrieve code using an API key and then evaluate the code accuracy of various LLMs. Addressing this gap, we introduce a novel multi-agent AI model designed to assess and compare the performance of different LLMs. Our work is pioneering in integrating the pass@k metric into an AI agent for a comparative analysis of code generated by these models. This approach not only enhances the precision of our evaluation but also provides a more nuanced understanding of each model’s capabilities. Looking towards the future, we aim to further improve our evaluation process by incorporating the MBPP benchmark. We believe that this addition will significantly refine our assessment methods. Furthermore, to ensure the practical applicability and robustness of our model, we plan to collaborate with twenty practitioners from diverse backgrounds. By engaging these practitioners in testing our model and collecting their feedback, we seek to gain valuable insights that will guide further improvements.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Research Methodology for Preliminary Analysis
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    This research aims to evaluate the performance of various language models through the use of AI agents. For this purpose, we initially developed a multi-agent model. We utilized an API key for each LLMs to obtain outputs. By providing a common description, we were able to receive code generated by different LLMs as outputs. Continuing with this approach, the methodology of our research is structured into distinct phases. Each phase is intentionally designed to rigorously test and analyze the capabilities of the LLMs in generating code responses to a common description, using our multi-agent model system.
We formulate the following research questions (RQs):
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p2">
   <svg class="ltx_picture" height="73.07" id="S3.p2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,73.07) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 67.16 C 0 70.42 2.64 73.07 5.91 73.07 L 594.09 73.07 C 597.36 73.07 600 70.42 600 67.16 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FAFFFA" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 67.16 C 1.97 69.34 3.73 71.1 5.91 71.1 L 594.09 71.1 C 596.27 71.1 598.03 69.34 598.03 67.16 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="S3.p2.pic1.1.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p2.pic1.1.1.1.1.1.1.1">
          RQ1.
          <span class="ltx_text ltx_font_medium" id="S3.p2.pic1.1.1.1.1.1.1.1.1">
           How does a multi-agent AI system, with each agent utilizing a different advanced language model, compare in terms of efficiency and accuracy in generating code from a common description?
          </span>
         </span>
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">
     Motivation
    </span>
    : The motivation behind this research question stems from the growing importance of AI in software development and the need to understand the efficacy of different AI approaches. By exploring how a multi-agent AI system, where each agent employs a distinct advanced language model, performs in generating code from a common description, we aim to shed light on the efficiency and accuracy of these models. This comparison is crucial in an era where the speed and reliability of automated coding are becoming increasingly vital. It also offers insights into the strengths and weaknesses of various models, guiding developers and researchers in selecting the most appropriate AI tools for their specific needs. This research not only contributes to the field of AI in coding but also has the potential to revolutionize the way we approach software development and problem-solving in the digital age.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p4">
   <svg class="ltx_picture" height="56.46" id="S3.p4.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,56.46) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 50.56 C 0 53.82 2.64 56.46 5.91 56.46 L 594.09 56.46 C 597.36 56.46 600 53.82 600 50.56 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FAFFFA" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 50.56 C 1.97 52.73 3.73 54.49 5.91 54.49 L 594.09 54.49 C 596.27 54.49 598.03 52.73 598.03 50.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="S3.p4.pic1.1.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p4.pic1.1.1.1.1.1.1.1">
          RQ2.
          <span class="ltx_text ltx_font_medium" id="S3.p4.pic1.1.1.1.1.1.1.1.1">
           What criteria and methods does the verification agent employ to evaluate the performance of various language models in code generation tasks?
          </span>
         </span>
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p5">
   <p class="ltx_p" id="S3.p5.1">
    <span class="ltx_text ltx_font_bold" id="S3.p5.1.1">
     Motivation
    </span>
    : The purpose of this research is to understand how a verification agent checks the work of different language models when they are used to write code. We want to know what methods and rules the agent uses to see if the code is good or not. This is important because as we use AI more for writing code, we need to make sure that the code is correct and works well. By learning how the agent checks the code, we can make better AI models for coding. This will help people who make software to use AI in a way that is safe and effective.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     In this section, we detail our use of API keys from various language models for code generation. Our primary objective is to assess the accuracy and efficiency of different LLMs. Here, we explain the process of how these models generate code. In a subsequent section, we will discuss the methodology we adopted to evaluate the performance of the generated code. This approach allows for a comprehensive analysis of the capabilities of different LLMs in coding tasks.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     In this paper, we introduce a novel multi-agent AI model consisting of eight distinct AI agents. Each agent is uniquely configured to interact with a specific advanced language model, including GPT-4, GPT-4 Turbo, GPT-3.5, GPT-3.5 Turbo, Google Bard, Hugging Face, and LLAMA. The primary function of these agents is to generate code based on a common description provided to all. This ensures uniformity in the task and allows for a fair comparison of the output from each language model. The agents operate simultaneously, retrieving code based on the same set of instructions. The collected data from each agent is then prepared for subsequent evaluation. Below, we discuss the entire process of how we utilized each LLM model to generate and evaluate code from given high-level descriptions.
Figure
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.1 Experimental Setup ‣ 3 Research Methodology for Preliminary Analysis ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     illustrates the entire process of how the API key is used to handle requests and generate the final code.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="255" id="S3.F1.g1" src="/html/2404.01023/assets/x1.jpg" width="608"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     An overview of API request and response processes across different LLMs for Code generation
    </figcaption>
   </figure>
   <section class="ltx_subsubsection" id="S3.SS1.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.1
     </span>
     OpenAI series
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
     <p class="ltx_p" id="S3.SS1.SSS1.p1.1">
      We developed an AI agent model that integrates with multiple versions of the OpenAI GPT models, including GPT-4, GPT-4 Turbo, GPT-3.5, and GPT-3.5 Turbo. Each AI agent in our model was configured to interact with a specific version of the OpenAI GPT model. We initiated the process by providing customized prompts to each agent, embedding a particular project description into these prompts to maintain consistency across different models. The AI agents, each linked to a distinct GPT version, were tasked with processing these prompts and generating responses.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p2">
     <p class="ltx_p" id="S3.SS1.SSS1.p2.1">
      Our model efficiently managed the communication with the OpenAI API for each agent, ensuring that the correct version of the GPT model was utilized for each request. Figure
      <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.1 Experimental Setup ‣ 3 Research Methodology for Preliminary Analysis ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      demonstrates how the API key receives requests and generates responses. The API key provided secure access to these models, enabling our agents to send and receive data. The responses from the various GPT versions were then collected and processed to maintain a coherent and continuous dialogue flow.
By structuring the interaction in several rounds and alternating between the different GPT models, our model was able to capture a diverse range of AI-generated responses. This setup was pivotal in assessing the comparative performance and output variations among the different models. The final outcome of this methodology was a comprehensive collection of responses from each GPT model, offering a unique opportunity to analyze and understand the nuances in AI-generated content based on the model version and initial input conditions.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.2
     </span>
     Google Bard
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p1">
     <p class="ltx_p" id="S3.SS1.SSS2.p1.1">
      Google Bard, a sophisticated language model created by Google, played a crucial role in our research methodology. We employed the Google Bard API to produce descriptive content. Our method involved using the Bard API alongside Bard Cookies for interaction with Google Bard’s AI system. This process necessitated authentication via specific cookies, supplied in the form of a dictionary to the Bard Cookies class. After successful authentication, the system could process and respond to text-based queries. For example, we input a high-level description into the Bard system, which then generated code relevant to the input. The responses from Bard offered insightful reflections on the AI’s ability to comprehend and create intricate, contextually relevant textual content, showcasing its effectiveness in automated content creation and AI-centric research methodologies.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.3
     </span>
     LLAMA
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p1">
     <p class="ltx_p" id="S3.SS1.SSS3.p1.1">
      LLAMA, another prominent language model in our study, was accessed through its designated API key. This key enabled our algorithm to send high-level descriptions to LLAMA, which then employed its language understanding capabilities to produce code. We incorporated the use of the replicate Python library to interact with advanced AI models for code generation. Specifically, we utilized this library to communicate with the Meta LLAMA-2-70b model. The process involved sending a high-level prompt to the AI model and model processed the request and generated a corresponding output. This output was then iteratively printed, providing us with AI-generated Python code. This approach demonstrated the model’s capability to understand and generate complex code based on simple text prompts, showcasing its potential in automating the code generation process and aiding in software development research.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.4
     </span>
     Hugging Face
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS4.p1">
     <p class="ltx_p" id="S3.SS1.SSS4.p1.1">
      In our research, we also leveraged Hugging Face’s extensive suite of language models through its API, focusing specifically on the CodeBERT model. Our method involved providing a high-level description to the model. Utilizing its advanced natural language processing capabilities, Hugging Face’s model generated relevant code. This interaction was made possible through the API key, which served as a conduit, granting our algorithm access to various models under the Hugging Face platform. Each model contributed distinct strengths to the code generation process, demonstrating the versatility and efficacy of Hugging Face’s AI solutions in our research.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Input of Code Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     In this research, we focused on the development of an AI agent model capable of robustly interpreting natural language prompts and generating corresponding code based on the provided descriptions. Our methodology involved utilizing common natural language descriptions as inputs, which were then processed by various state-of-the-art LLMs to generate code outputs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     To effectively evaluate the performance of these LLMs in code generation, we curated a diverse set of 10 descriptive prompts, each originating from different project backgrounds. This diversity in input descriptions was deliberately chosen to encompass a wide range of scenarios and complexities that developers might encounter in real-world applications. The specifics of these input descriptions are presented below.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     The box below displays the input prompts used for evaluating various LLMs, thereby ensuring a comprehensive assessment of the LLMs’ code generation capabilities. By encompassing a wide array of subjects and complexities in our input descriptions, our approach aims to rigorously test the adaptability and accuracy of the LLMs in converting natural language instructions into functional code across different contexts. This methodological approach provides a robust framework for assessing the efficacy of LLMs in understanding and translating human language into executable code.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p4">
    <svg class="ltx_picture" height="255.72" id="S3.SS2.p4.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,255.72) matrix(1 0 0 -1 0 0)">
      <g fill="#000000" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 249.81 C 0 253.07 2.64 255.72 5.91 255.72 L 594.09 255.72 C 597.36 255.72 600 253.07 600 249.81 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FCFCFC" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 249.81 C 1.97 251.98 3.73 253.75 5.91 253.75 L 594.09 253.75 C 596.27 253.75 598.03 251.98 598.03 249.81 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="228.16" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="S3.SS2.p4.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1">
           01.
           <span class="ltx_text ltx_font_medium" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.1">
            A simple Python program for creating and using flashcards for study, allowing users to test their knowledge on various subjects
            <br class="ltx_break"/>
           </span>
           02.
           <span class="ltx_text ltx_font_medium" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.2">
            An interactive Python game that tests users on various academic subjects and provides instant feedback
            <br class="ltx_break"/>
           </span>
           03.
           <span class="ltx_text ltx_font_medium" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.3">
            A short, interactive, text-based adventure game written in Python, where players make choices that influence the outcome of the story.
            <br class="ltx_break"/>
           </span>
           04.
           <span class="ltx_text ltx_font_medium" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.4">
            A Python-based Pomodoro timer that helps users apply the Pomodoro technique (25 minutes of work followed by a 5-minute break) to boost productivity.
            <br class="ltx_break"/>
           </span>
           <span class="ltx_text ltx_font_upright" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.5">
            .
            <br class="ltx_break"/>
            .
            <br class="ltx_break"/>
            .
            <br class="ltx_break"/>
            .
            <br class="ltx_break"/>
           </span>
           10.
           <span class="ltx_text ltx_font_medium" id="S3.SS2.p4.pic1.1.1.1.1.1.1.1.6">
            A Python-based basic arithmetic calculator capable of performing operations like addition, subtraction, multiplication, and division
           </span>
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Methodology for Code Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     The evaluation process involves analyzing various aspects of the code generated by each language model. Key metrics include the accuracy of the code in relation to the given description, efficiency in terms of execution and resource usage, and the innovation demonstrated in the solutions. This study employs a structured approach to assess the code generation capabilities of various advanced LLMs, including GPT-4, GPT-4 Turbo, GPT-3.5, GPT-3.5 Turbo, LLAMA, Hugging Face, and Google BARD. We developed a specialized AI agent with the primary objective of evaluating the accuracy and reliability of code produced by these models. The cornerstone of our evaluation methodology is the integration of the HumanEval benchmark within our AI agent.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     This model utilized the HumanEval benchmark, a widely recognized standard for assessing code quality and effectiveness. For evaluation, we employed the pass@k metric, a rigorous method to determine the viability of generated code. Our model receives a description as input, and in response, it obtains code from various LLMs. We provided ten distinct descriptions, each generating a set of potential solutions. Using the pass@k metric, we evaluated these solutions to determine their correctness. Mathematically, pass@k can be expressed as the probability that at least one of the top k solutions generated by the model is correct.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p3">
    <table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="\text{pass}@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]" class="ltx_Math" display="block" id="S3.Ex1.m1.5">
         <semantics id="S3.Ex1.m1.5a">
          <mrow id="S3.Ex1.m1.5.5" xref="S3.Ex1.m1.5.5.cmml">
           <mrow id="S3.Ex1.m1.5.5.3" xref="S3.Ex1.m1.5.5.3.cmml">
            <mtext id="S3.Ex1.m1.5.5.3.2" xref="S3.Ex1.m1.5.5.3.2a.cmml">
             pass
            </mtext>
            <mo id="S3.Ex1.m1.5.5.3.1" lspace="0em" rspace="0em" xref="S3.Ex1.m1.5.5.3.1.cmml">
             ​
            </mo>
            <mi id="S3.Ex1.m1.5.5.3.3" mathvariant="normal" xref="S3.Ex1.m1.5.5.3.3.cmml">
             @
            </mi>
            <mo id="S3.Ex1.m1.5.5.3.1a" lspace="0em" rspace="0em" xref="S3.Ex1.m1.5.5.3.1.cmml">
             ​
            </mo>
            <mi id="S3.Ex1.m1.5.5.3.4" xref="S3.Ex1.m1.5.5.3.4.cmml">
             k
            </mi>
           </mrow>
           <mo id="S3.Ex1.m1.5.5.2" lspace="0.278em" rspace="0.278em" xref="S3.Ex1.m1.5.5.2.cmml">
            :=
           </mo>
           <mrow id="S3.Ex1.m1.5.5.1" xref="S3.Ex1.m1.5.5.1.cmml">
            <msub id="S3.Ex1.m1.5.5.1.3" xref="S3.Ex1.m1.5.5.1.3.cmml">
             <mi id="S3.Ex1.m1.5.5.1.3.2" xref="S3.Ex1.m1.5.5.1.3.2.cmml">
              𝔼
             </mi>
             <mtext id="S3.Ex1.m1.5.5.1.3.3" xref="S3.Ex1.m1.5.5.1.3.3a.cmml">
              Problems
             </mtext>
            </msub>
            <mo id="S3.Ex1.m1.5.5.1.2" lspace="0em" rspace="0em" xref="S3.Ex1.m1.5.5.1.2.cmml">
             ​
            </mo>
            <mrow id="S3.Ex1.m1.5.5.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.cmml">
             <mo id="S3.Ex1.m1.5.5.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.1.cmml">
              [
             </mo>
             <mrow id="S3.Ex1.m1.5.5.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml">
              <mn id="S3.Ex1.m1.5.5.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.cmml">
               1
              </mn>
              <mo id="S3.Ex1.m1.5.5.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.cmml">
               −
              </mo>
              <mfrac id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml">
               <mrow id="S3.Ex1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.3.cmml">
                <mo id="S3.Ex1.m1.2.2.2.4.1" xref="S3.Ex1.m1.2.2.2.3.1.cmml">
                 (
                </mo>
                <mfrac id="S3.Ex1.m1.2.2.2.2.2.2" linethickness="0pt" xref="S3.Ex1.m1.2.2.2.3.cmml">
                 <mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">
                  <mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml">
                   n
                  </mi>
                  <mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml">
                   −
                  </mo>
                  <mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">
                   c
                  </mi>
                 </mrow>
                 <mi id="S3.Ex1.m1.2.2.2.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.2.2.2.1.cmml">
                  k
                 </mi>
                </mfrac>
                <mo id="S3.Ex1.m1.2.2.2.4.2" xref="S3.Ex1.m1.2.2.2.3.1.cmml">
                 )
                </mo>
               </mrow>
               <mrow id="S3.Ex1.m1.4.4.4.4" xref="S3.Ex1.m1.4.4.4.3.cmml">
                <mo id="S3.Ex1.m1.4.4.4.4.1" xref="S3.Ex1.m1.4.4.4.3.1.cmml">
                 (
                </mo>
                <mfrac id="S3.Ex1.m1.4.4.4.2.2.2" linethickness="0pt" xref="S3.Ex1.m1.4.4.4.3.cmml">
                 <mi id="S3.Ex1.m1.3.3.3.1.1.1.1.1" xref="S3.Ex1.m1.3.3.3.1.1.1.1.1.cmml">
                  n
                 </mi>
                 <mi id="S3.Ex1.m1.4.4.4.2.2.2.2.1" xref="S3.Ex1.m1.4.4.4.2.2.2.2.1.cmml">
                  k
                 </mi>
                </mfrac>
                <mo id="S3.Ex1.m1.4.4.4.4.2" xref="S3.Ex1.m1.4.4.4.3.1.cmml">
                 )
                </mo>
               </mrow>
              </mfrac>
             </mrow>
             <mo id="S3.Ex1.m1.5.5.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.1.cmml">
              ]
             </mo>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.5b">
           <apply id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5">
            <csymbol cd="latexml" id="S3.Ex1.m1.5.5.2.cmml" xref="S3.Ex1.m1.5.5.2">
             assign
            </csymbol>
            <apply id="S3.Ex1.m1.5.5.3.cmml" xref="S3.Ex1.m1.5.5.3">
             <times id="S3.Ex1.m1.5.5.3.1.cmml" xref="S3.Ex1.m1.5.5.3.1">
             </times>
             <ci id="S3.Ex1.m1.5.5.3.2a.cmml" xref="S3.Ex1.m1.5.5.3.2">
              <mtext id="S3.Ex1.m1.5.5.3.2.cmml" xref="S3.Ex1.m1.5.5.3.2">
               pass
              </mtext>
             </ci>
             <ci id="S3.Ex1.m1.5.5.3.3.cmml" xref="S3.Ex1.m1.5.5.3.3">
              @
             </ci>
             <ci id="S3.Ex1.m1.5.5.3.4.cmml" xref="S3.Ex1.m1.5.5.3.4">
              𝑘
             </ci>
            </apply>
            <apply id="S3.Ex1.m1.5.5.1.cmml" xref="S3.Ex1.m1.5.5.1">
             <times id="S3.Ex1.m1.5.5.1.2.cmml" xref="S3.Ex1.m1.5.5.1.2">
             </times>
             <apply id="S3.Ex1.m1.5.5.1.3.cmml" xref="S3.Ex1.m1.5.5.1.3">
              <csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.3.1.cmml" xref="S3.Ex1.m1.5.5.1.3">
               subscript
              </csymbol>
              <ci id="S3.Ex1.m1.5.5.1.3.2.cmml" xref="S3.Ex1.m1.5.5.1.3.2">
               𝔼
              </ci>
              <ci id="S3.Ex1.m1.5.5.1.3.3a.cmml" xref="S3.Ex1.m1.5.5.1.3.3">
               <mtext id="S3.Ex1.m1.5.5.1.3.3.cmml" mathsize="70%" xref="S3.Ex1.m1.5.5.1.3.3">
                Problems
               </mtext>
              </ci>
             </apply>
             <apply id="S3.Ex1.m1.5.5.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1">
              <csymbol cd="latexml" id="S3.Ex1.m1.5.5.1.1.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.2">
               delimited-[]
              </csymbol>
              <apply id="S3.Ex1.m1.5.5.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1">
               <minus id="S3.Ex1.m1.5.5.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1">
               </minus>
               <cn id="S3.Ex1.m1.5.5.1.1.1.1.2.cmml" type="integer" xref="S3.Ex1.m1.5.5.1.1.1.1.2">
                1
               </cn>
               <apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4">
                <divide id="S3.Ex1.m1.4.4.5.cmml" xref="S3.Ex1.m1.4.4">
                </divide>
                <apply id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.4">
                 <csymbol cd="latexml" id="S3.Ex1.m1.2.2.2.3.1.cmml" xref="S3.Ex1.m1.2.2.2.4.1">
                  binomial
                 </csymbol>
                 <apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1">
                  <minus id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2">
                  </minus>
                  <ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3">
                   𝑛
                  </ci>
                  <ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.4">
                   𝑐
                  </ci>
                 </apply>
                 <ci id="S3.Ex1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.2.1">
                  𝑘
                 </ci>
                </apply>
                <apply id="S3.Ex1.m1.4.4.4.3.cmml" xref="S3.Ex1.m1.4.4.4.4">
                 <csymbol cd="latexml" id="S3.Ex1.m1.4.4.4.3.1.cmml" xref="S3.Ex1.m1.4.4.4.4.1">
                  binomial
                 </csymbol>
                 <ci id="S3.Ex1.m1.3.3.3.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.3.1.1.1.1.1">
                  𝑛
                 </ci>
                 <ci id="S3.Ex1.m1.4.4.4.2.2.2.2.1.cmml" xref="S3.Ex1.m1.4.4.4.2.2.2.2.1">
                  𝑘
                 </ci>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.Ex1.m1.5c">
           \text{pass}@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
      </tr>
     </tbody>
    </table>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p4">
    <p class="ltx_p" id="S3.SS3.p4.1">
     If one solution out of the k attempts is found correct, we mark the instance as successful and proceed to the next description. In this way, we systematically assess the performance of LLMs across multiple coding tasks, ensuring a comprehensive analysis of their capabilities in code generation. The methodology’s implementation involves iterating over the set of descriptions, generating code for each, and applying the pass@k metric to assess each set of solutions. This approach allows for an empirical evaluation of LLMs in terms of both accuracy and efficiency in coding tasks.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Preliminary Results
  </h2>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we present the initial results of the proposed multi agents model for evaluation of various LLM-based model. The primarily results are shown in Table
    <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Initial Evaluation Framework (RQ2) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    . So far our findings indicate that the GPT 3.5 Turbo model perform relatively better than others LLM based model for generating accurate code by given high level description. Below, we present the results of our proposed model in Section
    <a class="ltx_ref" href="#S4.SS1" title="4.1 Proposed Multi Agent Model (RQ1) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      4.1
     </span>
    </a>
    , specifically reporting the outcomes of RQ1. Additionally, we also provide the detail criteria and methods that we employed in verification agent to evaluate the performance of various language models in Section
    <a class="ltx_ref" href="#S4.SS2" title="4.2 Initial Evaluation Framework (RQ2) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
     <span class="ltx_text ltx_ref_tag">
      4.2
     </span>
    </a>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Proposed Multi Agent Model (RQ1)
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     In this section of our research, we conducted an evaluation of code performance across various language models. This assessment was carried out by developing a sophisticated algorithm that utilized the API keys of the respective language models. The core objective was to methodically analyze and compare the output quality and efficiency of each model when tasked with generating code from high-level descriptions. In the pursuit of understanding the capabilities of various LLMs in the context of code generation, we conducted a comprehensive evaluation using the HumanEval benchmark to gauge the accuracy of the code produced by each model. Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Initial Evaluation Framework (RQ2) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     presents a synthesized view of this evaluation, showcasing the performance of different LLMs across identical input descriptions.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     Upon early testing and analysis, our results indicated that GPT-3.5 Turbo exhibited superior performance compared to other language models like GPT-4, GPT-4 Turbo, Google Bard, Hugging Face, and LLAMA. Specifically, GPT 3.5 Turbo provided accurate results for seven out of the ten inputs. This achievement marks a significant margin of excellence compared to its counterparts. In contrast, other LLM models in the study produced fewer accurate outputs, as can be observed from the comparative data in the Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Initial Evaluation Framework (RQ2) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Initial Evaluation Framework (RQ2) ‣ 4 Preliminary Results ‣ Large Language Model Evaluation Via Multi AI Agents: Preliminary results">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     enumerates seven different LLMs, each produced by renowned organizations with varying parameter sizes—from 355M by Hugging Face to 1.96 trillion by GPT-4 Turbo. Each model was provided with the same set of ten input descriptions to process and generate code. The primary focus of the evaluation was on the accuracy of the results, which refers to the functional correctness of the generated code as per the given description. Additionally, a quality rating, depicted with stars, provides a subjective assessment of the code based on criteria such as readability, efficiency, and adherence to best practices.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     The data shows a varied performance across models. GPT-3.5 Turbo, with 154 billion parameters, leads with the highest number of accurate results, successfully generating correct code for seven out of ten descriptions. This is followed closely by GPT-4 Turbo, which each returned six accurate pieces of code. Notably, despite Google Bard’s significantly larger parameter count, it did not outperform GPT-3.5 Turbo, indicating that larger model size does not necessarily equate to better performance in specific tasks such as code generation.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p5">
    <p class="ltx_p" id="S4.SS1.p5.1">
     The results underscore the efficacy of GPT-3.5 Turbo in this domain, with a strong blend of accuracy and high quality, as reflected by its four-star rating. The other models, while demonstrating varying levels of proficiency, highlight the competitive landscape of LLMs and their potential for software engineering applications.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p6">
    <p class="ltx_p" id="S4.SS1.p6.1">
     This analysis provides initial insights into the current state of LLMs in code generation tasks, underscoring the importance of specialized benchmarks like HumanEval in evaluating AI agents. It also sets the stage for further discussions on model optimization, the impact of parameter scale, and the potential for LLMs to revolutionize the field of automated code generation.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Initial Evaluation Framework (RQ2)
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In the Initial Evaluation Framework section, our focus shifted towards the validation and verification of the code generated by these models. In our study, we deployed an AI multi-agent model to evaluate the code generation capabilities of various LLMs, including GPT-4, GPT-4 Turbo, GPT-3.5, GPT-3.5 Turbo, Google Bard, LLaMA, and Hugging Face. The performance assessment of these models was based on their ability to generate accurate and functional code in response to a set of predefined descriptions.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Our evaluation strategy involved presenting each LLM with 10 unique code generation tasks. The performance of each model was assessed using the HumanEval benchmark, which is a widely recognized method for evaluating code synthesis. In this project, we developed multi AI agent model to generate and evaluate the generated code. The task of one agent was to employ the HumanEval benchmark and autonomously test the code generation capabilities of AI models Each task in HumanEval requires the AI to write a function that satisfies a given specification, and the models’ responses are evaluated based on their correctness and efficiency.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     One of the key metrics used in our assessment was the pass@k metric, a standard measure for evaluating code generation models. The pass@k metric evaluates whether a correct solution is found within k attempts, where k is a predefined number of generated code samples. This metric is particularly useful in scenarios where multiple solutions are possible, as it accounts for the probabilistic nature of code generation by LLMs.
In our experiment, we utilized a pass@1 approach, meaning that we evaluated the models based on their first attempt at generating a solution. This approach was chosen to reflect a more realistic scenario where a developer would use the model’s first output.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     The results were revealing: GPT-3.5 Turbo outperformed other models in our tests, delivering correct and efficient code solutions in 7 out of 10 tasks. This indicates a 70% success rate on the first attempt, which is a significant achievement considering the complexity of the tasks involved. In comparison, other models like GPT-4 and GPT-4 Turbo showed varying levels of performance, but none matched the consistency of GPT-3.5 Turbo.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     This evaluation highlights the effectiveness of GPT-3.5 Turbo in code generation tasks, particularly in its ability to understand and translate complex task descriptions into functional code. The use of HumanEval benchmark and the ‘pass@k‘ metric provided a robust framework for this assessment, allowing for a fair and comprehensive comparison across different LLMs.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Workflow of the proposed model: Application across diverse datasets and output formats
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S4.T1.1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1" style="background-color:#EFEFEF;">
         LLM Model
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1" style="background-color:#EFEFEF;">
         Product
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1" style="background-color:#EFEFEF;">
         Parameter
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1" style="background-color:#EFEFEF;">
         I/P Description
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.5" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1" style="background-color:#EFEFEF;">
         Accurate Result
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.6" style="background-color:#EFEFEF;">
        <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.6.1" style="background-color:#EFEFEF;">
         Quality
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T1.1.2.1">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.1.1" style="background-color:#EFEFEF;">
         GPT-4 Turbo
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.2.1" style="background-color:#EFEFEF;">
         OpenAI
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.3.1" style="background-color:#EFEFEF;">
         1.96 trillion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.5.1" style="background-color:#EFEFEF;">
         6
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.2.1.6.1" style="background-color:#EFEFEF;">
         ★★★✩✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.3.2">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.1.1" style="background-color:#EFEFEF;">
         GPT-4
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.3.2.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.2.1" style="background-color:#EFEFEF;">
         OpenAI
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.3.2.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.3.1" style="background-color:#EFEFEF;">
         1.76 trillion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.3.2.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.3.2.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.5.1" style="background-color:#EFEFEF;">
         5
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.3.2.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.3.2.6.1" style="background-color:#EFEFEF;">
         ★★★✩✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.4.3">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.3.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.1.1" style="background-color:#EFEFEF;">
         GPT-3.5 Turbo
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.4.3.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.2.1" style="background-color:#EFEFEF;">
         OpenAI
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.4.3.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.3.1" style="background-color:#EFEFEF;">
         154 billion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.4.3.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.4.3.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.5.1" style="background-color:#EFEFEF;">
         7
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.4.3.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.4.3.6.1" style="background-color:#EFEFEF;">
         ★★★★✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.5.4">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.4.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.1.1" style="background-color:#EFEFEF;">
         GPT-3.5
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.2.1" style="background-color:#EFEFEF;">
         OpenAI
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.3.1" style="background-color:#EFEFEF;">
         125 billion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.5.1" style="background-color:#EFEFEF;">
         5
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.5.4.6.1" style="background-color:#EFEFEF;">
         ★★★✩✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.6.5">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.6.5.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.1.1" style="background-color:#EFEFEF;">
         Google Bard
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.6.5.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.2.1" style="background-color:#EFEFEF;">
         Google
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.6.5.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.3.1" style="background-color:#EFEFEF;">
         1.56 trillion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.6.5.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.6.5.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.5.1" style="background-color:#EFEFEF;">
         4
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.6.5.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.6.5.6.1" style="background-color:#EFEFEF;">
         ★★✩✩✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.7.6">
       <td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.7.6.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.1.1" style="background-color:#EFEFEF;">
         LLama
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.7.6.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.2.1" style="background-color:#EFEFEF;">
         Meta
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.7.6.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.3.1" style="background-color:#EFEFEF;">
         70 billion
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.7.6.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.7.6.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.5.1" style="background-color:#EFEFEF;">
         2
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.7.6.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.7.6.6.1" style="background-color:#EFEFEF;">
         ★✩✩✩✩
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.1.8.7">
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.8.7.1" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.1.1" style="background-color:#EFEFEF;">
         Hugging Face
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.8.7.2" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.2.1" style="background-color:#EFEFEF;">
         Hugging Face
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.8.7.3" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.3.1" style="background-color:#EFEFEF;">
         355M
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.8.7.4" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.4.1" style="background-color:#EFEFEF;">
         10
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.8.7.5" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.5.1" style="background-color:#EFEFEF;">
         2
        </span>
       </td>
       <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.8.7.6" style="background-color:#EFEFEF;">
        <span class="ltx_text" id="S4.T1.1.8.7.6.1" style="background-color:#EFEFEF;">
         ★✩✩✩✩
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Future Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    As we work to improve how we evaluate LLMs for code generation, our current methodology employs the HumanEval benchmark within our AI agent framework. HumanEval has served as a robust starting point, offering a suite of programming problems that models must solve, thereby providing a quantitative measure of their code generation abilities. However, to broaden our evaluation’s scope and depth, we are preparing to incorporate the MBPP. The MBPP benchmark, with its larger and more diverse set of programming tasks, will enable a more extensive assessment of LLMs’ capabilities.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    In conjunction with the adoption of MBPP, we recognize the indispensable value of human judgment in model evaluation. To this end, we have embarked on a collaboration with twenty practitioners from a spectrum of backgrounds. These industry experts, academics, and developers will bring their unique perspectives to bear on the task of manually evaluating our proposed model’s performance. Their insights will not only validate the findings from the MBPP benchmark but also provide nuanced understanding of the model’s practical effectiveness and areas for improvement.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    This dual approach, combining automated benchmarks with human expertise, wil offer a comprehensive evaluation framework. It will ensure that the strengths and weaknesses of LLMs in code generation are fully understood, paving the way for targeted enhancements. With this initiative, we aim to set a new standard for the evaluation of LLMs, one that balances the scalability of automated testing with the depth of human review.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusions
  </h2>
  <div class="ltx_para ltx_noindent" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we introduced a novel multi-agent AI model designed to assess and compare the performance of various LLMs. Our system comprises eight distinct AI agents, each tasked with the retrieval of code based on a shared high-level description from several advanced language models, including GPT-3.5, GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Google Bard, LLAMA, and Hugging Face. Utilizing the APIs provided by these models, our agents can retrieve code that is then evaluated by a specialized verification agent. This verification agent employs the HumanEval benchmark to measure performance, offering valuable insights into the strengths and efficiencies of each model. Initial findings have highlighted GPT-3.5 Turbo’s superior performance relative to its counterparts.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    As we look to the future, our ambition is to further enhance the evaluation framework. The integration of the MBPP represents a significant next step in this process, promising to sharpen our assessment tools and provide even more granular insights. Moreover, we are committed to engaging with the broader community by involving twenty practitioners from a variety of backgrounds to interact with and evaluate our model. Their feedback will be indispensable, providing a practical edge to our theoretical findings and helping to guide the model’s continuous improvement.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    Our work lays the groundwork for a dynamic and robust evaluation platform that not only benchmarks the current state of LLMs but also adapts and evolves with the field. The insights gained through our research will serve as a cornerstone for future advancements in AI-driven code generation, facilitating developments that are as practical as they are innovative. Through rigorous assessment and community engagement, we aspire to drive the field forward, setting new standards for performance and applicability in the ever-growing expanse of language model technology.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Acknowledgment
  </h2>
  <div class="ltx_para ltx_noindent" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    We express our sincere gratitude to Business Finland for their generous support and funding of our project. Their commitment to fostering innovation and supporting research initiatives has been instrumental in the success of our work.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alon et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav.
    </span>
    <span class="ltx_bibblock">
     Structural language models of code.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      International conference on machine learning
     </em>
     , pp.  245–256. PMLR, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Austin et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
    </span>
    <span class="ltx_bibblock">
     Program synthesis with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2108.07732
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Advances in neural information processing systems
     </em>
     , 33:1877–1901, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al.
    </span>
    <span class="ltx_bibblock">
     A survey on evaluation of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      ACM Transactions on Intelligent Systems and Technology
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     Codet: Code generation with generated tests.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2207.10397
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2107.03374
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Eloundou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock.
    </span>
    <span class="ltx_bibblock">
     Gpts are gpts: An early look at the labor market impact potential of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2303.10130
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yunhe Feng, Sreecharan Vanam, Manasa Cherukupally, Weijian Zheng, Meikang Qiu, and Haihua Chen.
    </span>
    <span class="ltx_bibblock">
     Investigating code generation performance of chat-gpt with crowdsourcing social data.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Proceedings of the 47th IEEE Computer Software and Applications Conference
     </em>
     , pp.  1–10, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al.
    </span>
    <span class="ltx_bibblock">
     Codebert: A pre-trained model for programming and natural languages.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2002.08155
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin.
    </span>
    <span class="ltx_bibblock">
     Unixcoder: Unified cross-modal pre-training for code representation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2203.03850
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models: A comprehensive survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:2310.19736
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hellendoorn &amp; Devanbu (2017)
    </span>
    <span class="ltx_bibblock">
     Vincent J Hellendoorn and Premkumar Devanbu.
    </span>
    <span class="ltx_bibblock">
     Are deep neural networks the best choice for modeling source code?
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Proceedings of the 2017 11th Joint meeting on foundations of software engineering
     </em>
     , pp.  763–773, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.
    </span>
    <span class="ltx_bibblock">
     Large language models for software engineering: A systematic literature review.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      arXiv preprint arXiv:2308.10620
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Karampatsis et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes.
    </span>
    <span class="ltx_bibblock">
     Big code!= big vocabulary: Open-vocabulary models for source code.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
     </em>
     , pp.  1073–1085, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mashhadi &amp; Hemmati (2021)
    </span>
    <span class="ltx_bibblock">
     Ehsan Mashhadi and Hadi Hemmati.
    </span>
    <span class="ltx_bibblock">
     Applying codebert for automated program repair of java simple bugs.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)
     </em>
     , pp.  505–509. IEEE, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Improving language understanding by generative pre-training.
    </span>
    <span class="ltx_bibblock">
     2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rasheed et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zeeshan Rasheed, Muhammad Waseem, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, Kari Systä, and Pekka Abrahamsson.
    </span>
    <span class="ltx_bibblock">
     Autonomous agents in software development: A vision paper.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2311.18440
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rasheed et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Zeeshan Rasheed, Muhammad Waseem, Aakash Ahmad, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, and Pekka Abrahamsson.
    </span>
    <span class="ltx_bibblock">
     Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2402.01386
     </em>
     , 2024a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rasheed et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Zeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Systä, and Pekka Abrahamsson.
    </span>
    <span class="ltx_bibblock">
     Codepori: Large scale model for autonomous software development by using multi-agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2402.01411
     </em>
     , 2024b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rudolph et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jürgen Rudolph, Shannon Tan, and Samson Tan.
    </span>
    <span class="ltx_bibblock">
     War of the chatbots: Bard, bing chat, chatgpt, ernie and beyond. the new ai gold rush and its impact on higher education.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Journal of Applied Learning and Teaching
     </em>
     , 6(1), 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sami et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, Anh Nguyen Duc, Kari Systä, and Pekka Abrahamsson.
    </span>
    <span class="ltx_bibblock">
     System for systematic literature review using multiple ai agents: Concept and an empirical evaluation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2403.08399
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Thoppilan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
    </span>
    <span class="ltx_bibblock">
     Lamda: Language models for dialog applications.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      arXiv preprint arXiv:2201.08239
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
    </span>
    <span class="ltx_bibblock">
     Attention is all you need.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Advances in neural information processing systems
     </em>
     , 30, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang.
    </span>
    <span class="ltx_bibblock">
     Software testing with large language model: Survey, landscape, and vision.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2307.07221
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wolf et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al.
    </span>
    <span class="ltx_bibblock">
     Huggingface’s transformers: State-of-the-art natural language processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      arXiv preprint arXiv:1910.03771
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn.
    </span>
    <span class="ltx_bibblock">
     A systematic evaluation of large language models of code.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming
     </em>
     , pp.  1–10, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.
    </span>
    <span class="ltx_bibblock">
     Bertscore: Evaluating text generation with bert.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:1904.09675
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhuo (2023)
    </span>
    <span class="ltx_bibblock">
     Terry Yue Zhuo.
    </span>
    <span class="ltx_bibblock">
     Large language models are state-of-the-art evaluators of code generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2304.14317
     </em>
     , 2023.
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
</article>
