<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</title>
<!--Generated on Wed Sep 25 15:22:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.17025v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S2" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S3" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset description</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S3.SS1" title="In 3 Dataset description ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Videos</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S3.SS2" title="In 3 Dataset description ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Instrument annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S3.SS3" title="In 3 Dataset description ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Surgical skill assessments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1" title="In 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Instrument segmentation and tracking</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1.SSS1" title="In 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>PRINTNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1.SSS2" title="In 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Real-time implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1.SSS3" title="In 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1.SSS4" title="In 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Dataset split</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS1.SSS5" title="In 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.5 </span>Implementation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.SS2" title="In 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Surgical skill assessment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.SS1" title="In 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Instrument tracking and segmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.SS1.SSS1" title="In 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Instrument segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.SS1.SSS2" title="In 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Instrument tracking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.SS1.SSS3" title="In 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Real-time implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.SS2" title="In 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Surgical skill assessment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S6" title="In Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1" lang="en">\historydate</span>
<p class="ltx_p" id="p1.2"><span class="ltx_text" id="p1.2.1" lang="en">Pre-print: 25 September 2024</span></p>
</div>
<h1 class="ltx_title ltx_title_document" lang="en">Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id1.1.1">Adrito Das<sup class="ltx_sup" id="id1.1.1.1"><span class="ltx_text ltx_font_italic" id="id1.1.1.1.1">1</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id2.1.1">Bilal Sidiqi<sup class="ltx_sup" id="id2.1.1.1"><span class="ltx_text ltx_font_italic" id="id2.1.1.1.1">1</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id3.1.1">Laurent Mennillo<sup class="ltx_sup" id="id3.1.1.1"><span class="ltx_text ltx_font_italic" id="id3.1.1.1.1">1</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id4.1.1">Zhehua Mao<sup class="ltx_sup" id="id4.1.1.1"><span class="ltx_text ltx_font_italic" id="id4.1.1.1.1">1</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id19.2.id1">Mikael Brudfors</span><sup class="ltx_sup" id="id20.3.id2"><span class="ltx_text ltx_font_italic" id="id20.3.id2.1">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id6.1.1">Miguel Xochicale<sup class="ltx_sup" id="id6.1.1.1"><span class="ltx_text ltx_font_italic" id="id6.1.1.1.1">1,3</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id7.1.1">Danyal Z. Khan<sup class="ltx_sup" id="id7.1.1.1"><span class="ltx_text ltx_font_italic" id="id7.1.1.1.1">1,4</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id8.1.1">Nicola Newall<sup class="ltx_sup" id="id8.1.1.1"><span class="ltx_text ltx_font_italic" id="id8.1.1.1.1">1,4</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id9.1.1">John G. Hanrahan<sup class="ltx_sup" id="id9.1.1.1"><span class="ltx_text ltx_font_italic" id="id9.1.1.1.1">1,4</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id10.1.1">Matthew J. Clarkson<sup class="ltx_sup" id="id10.1.1.1"><span class="ltx_text ltx_font_italic" id="id10.1.1.1.1">1,5</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id11.1.1">Danail Stoyanov<sup class="ltx_sup" id="id11.1.1.1"><span class="ltx_text ltx_font_italic" id="id11.1.1.1.1">1</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span class="ltx_text" id="id12.1.1">Hani J. Marcus<sup class="ltx_sup" id="id12.1.1.1"><span class="ltx_text ltx_font_italic" id="id12.1.1.1.1">1,4</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
and
<span class="ltx_text" id="id13.1.1">Sophia Bano<sup class="ltx_sup" id="id13.1.1.1"><span class="ltx_text ltx_font_italic" id="id13.1.1.1.1">1</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address" lang="en"><sup class="ltx_sup" id="id21.7.id1"><span class="ltx_text ltx_font_italic" id="id21.7.id1.1">1</span></sup> Wellcome / EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK
<br class="ltx_break"/><sup class="ltx_sup" id="id22.8.id2"><span class="ltx_text ltx_font_italic" id="id22.8.id2.1">2</span></sup> NVIDIA
<br class="ltx_break"/><sup class="ltx_sup" id="id23.9.id3"><span class="ltx_text ltx_font_italic" id="id23.9.id3.1">3</span></sup> School of Biomedical Engineering and Imaging Sciences, King’s College London, London, UK
<br class="ltx_break"/><sup class="ltx_sup" id="id24.10.id4"><span class="ltx_text ltx_font_italic" id="id24.10.id4.1">4</span></sup> Department of Neurosurgery, National Hospital for Neurology and Neurosurgery, London, UK 
<br class="ltx_break"/><sup class="ltx_sup" id="id25.11.id5"><span class="ltx_text ltx_font_italic" id="id25.11.id5.1">5</span></sup> Department of Medical Physics &amp; Biomedical Engineering, University College London, London, UK

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id26.id1"><span class="ltx_text" id="id26.id1.1" lang="en">Improved surgical skill is generally associated with improved patient outcomes, although assessment is subjective; labour-intensive; and requires domain specific expertise. Automated data driven metrics can alleviate these difficulties, as demonstrated by existing machine learning instrument tracking models in minimally invasive surgery. However, these models have been tested on limited datasets of laparoscopic surgery, with a focus on isolated tasks and robotic surgery. In this paper, a new public dataset is introduced, focusing on simulated surgery, using the nasal phase of endoscopic pituitary surgery as an exemplar. Simulated surgery allows for a realistic yet repeatable environment, meaning the insights gained from automated assessment can be used by novice surgeons to hone their skills on the simulator before moving to real surgery. PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as a baseline model for this automated assessment. Consisting of DeepLabV3 for classification and segmentation; StrongSORT for tracking; and the NVIDIA Holoscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object Tracking Precision running at 22 Frames Per Second. Using this tracking output, a Multilayer Perceptron achieved 87% accuracy in predicting surgical skill level (novice or expert), with the ‘ratio of total procedure time to instrument visible time’ correlated with higher surgical skill. This therefore demonstrates the feasibility of automated surgical skill assessment in simulated endoscopic pituitary surgery. The new publicly available dataset can be found here: <span class="ltx_text" id="id26.id1.1.1" style="color:#0000FF;">https://doi.org/10.5522/04/26511049</span>.</span></p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S1.F1.g1" src="extracted/5879470/01instruments.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Representative images of the 4-instrument-classes used in the nasal phase of endoscopic pituitary surgery.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S1.F2.g1" src="extracted/5879470/02folds.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of instruments: (a) Total number of images before data balancing; (b) Number of images per fold after data balancing.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Benign tumours of the pituitary gland, pituitary adenomas, are common, associated with systemic morbidity and mortality, and the majority are curable with surgery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib1" title="">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib2" title="">2</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib3" title="">3</a>]</cite>. The <span class="ltx_glossaryref" title="">endoscopic TransSphenoidal Approach (eTSA)</span>, is a minimally invasive surgery where these tumours are removed by entering through a nostril <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib4" title="">4</a>]</cite>. However, this surgery has a steep learning curve, with superior surgical skill generally associated with superior patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib3" title="">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib5" title="">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_glossaryref" title="">Objective Structured Assessment of Technical Skills (OSATS)</span> measures surgical skill by assessing how well aspects of a surgical task are performed on a scale of 1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib7" title="">7</a>]</cite>. For example, for the aspect of instrument handling, a value of 1 indicates ‘Repeatedly makes tentative or awkward moves with instruments’, and a value of 5 indicates ‘Fluid moves with instruments and no awkwardness’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib8" title="">8</a>]</cite>. However, it is not operation specific; liable to interpreter variability; and is a time-consuming manual process requiring surgical experts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib10" title="">10</a>]</cite>. Data driven metrics may be more specific; objective; reproducible; and easier to automate.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Neural networks can automatically and accurately determine surgical skill <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib12" title="">12</a>]</cite>. More specifically, instrument tracking has been shown to be associated with <span class="ltx_glossaryref" title="">OSATS</span> in minimally invasive surgeries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib11" title="">11</a>]</cite>. However, the models have been tested on limited datasets with a focus on laparoscopic surgeries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib11" title="">11</a>]</cite>. Pedrett et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib11" title="">11</a>]</cite> provides a comprehensive list of these datasets, which are videos of: isolated tasks (e.g. peg transfers in JIGSAWS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib13" title="">13</a>]</cite>); real surgery (e.g. Cholec-80 with no publicly available surgical skill assessment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib14" title="">14</a>]</cite>); on robotic surgery (e.g. ROSMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib15" title="">15</a>]</cite>); or include instrument tracking data from built in methods (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib16" title="">16</a>]</cite>) or wearable sensors (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib17" title="">17</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, this previous work is extended to be tested on videos of a high-fidelity bench-top phantom of the full nasal phase of <span class="ltx_glossaryref" title="">eTSA</span>. These videos are therefore of a non-laparoscopic; non-private; non-robotic; and non-task-isolated surgery with no tracking data. This phantom is commonly used in neurosurgical training to simulate real surgery, and so surgical skill is an important measure to track a novice surgeon’s progress until they are able to perform real surgery. Additionally, the insights gained from the automated assessment can be used to isolate specific areas of improvement for the novice surgeon. In real surgery, surgeons are already of sufficient skill, and surgical skill assessment has the alternative use of correlating certain practices with patient outcomes.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Moreover, instrument tracking in <span class="ltx_glossaryref" title="">eTSA</span> provides a unique computer vision challenge due to: (I) A non-fixed endoscope leading to large camera movements; (II) The frequent withdrawal of instruments leading to instruments having a range of sizes; (III) The use of niche instruments leading to heavy class imbalance; (IV) The smaller working space requiring the use of a wide lens, distorting images (see <span class="ltx_text" id="S1.p5.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">1</span></a></span>). To overcome these challenges, <span class="ltx_glossaryref" title="">Pituitary Real-time INstrument Tracking Network (PRINTNet)</span> has been created, and the output is used to demonstrate correlations between instrument tracking and surgical skill. Therefore, this paper’s contribution are:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The first public dataset containing both instrument and surgical skill assessment annotations in a high-fidelity bench-top phantom of <span class="ltx_glossaryref" title="">eTSA</span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A baseline network capable of automated classification; segmentation; and tracking of the instruments in the nasal phase of <span class="ltx_glossaryref" title="">eTSA</span>, integrated on a NVIDIA Clara AGX for real-time assistance in surgical training sessions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Statistical analysis between instrument tracking and surgical skill assessment in <span class="ltx_glossaryref" title="">eTSA</span>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Instrument classification in <span class="ltx_glossaryref" title="">eTSA</span> has been attempted in the PitVis-EndoVis MICCAI-2023 sub-challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib18" title="">18</a>]</cite>, where 25-videos and 8-videos of real <span class="ltx_glossaryref" title="">eTSA</span> (complete videos) were used for training and testing respectively.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Instrument segmentation and tracking is yet to be explored for <span class="ltx_glossaryref" title="">eTSA</span>, though it has been attempted in minimally invasive surgeries since 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib19" title="">19</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib20" title="">20</a>]</cite>. Modern models use encoder-decoder architectures, utilising U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib21" title="">21</a>]</cite> and its variants for segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib19" title="">19</a>]</cite>, and early forms of SORT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib22" title="">22</a>]</cite> for tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The most similar study to this paper linking instrument tracking to surgical skill assessment is one conducted on robotic thyroid surgery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>. 23-videos (simulation and real) were used for training the 4-instrument-class tracking model, and 40-simulation-videos were used for training the surgical assessment model, with 12-simulation-videos used for testing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>. Mask R-CNN and DeepSORT were used for segmentation and tracking respectively, achieving 70.0% <span class="ltx_glossaryref" title="">Area Under Curve (AUC)</span> for tracking a tool tip within 1mm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>. A <span class="ltx_glossaryref" title="">Random Forest (RF)</span> model was shown to be the best predictor of surgical skill, achieving 83% accuracy in distinguishing between novice; intermediate; and expert surgeons <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>. It was found ‘economy of motion’ was the most important predictive factor in where camera motion is minimal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Other studies that use tool tracking for surgical skill assessment include one on real non-robotic laparoscopic cholecystectomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib24" title="">24</a>]</cite>. Here, instruments in 80-videos (15-test) of the calot triangle dissection phase in were tracked <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib24" title="">24</a>]</cite>. The model consisted of YoloV5 for detection, followed by a Kalman filter and the Hungarian algorithm for tracking, achieving 83% <span class="ltx_glossaryref" title="">Multiple Object Tracking Accuracy (MOTA)</span> and 83% accuracy in binary skill assessment via <span class="ltx_glossaryref" title="">RF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib24" title="">24</a>]</cite>. Alternative models, such as those utilising aggregation of local features, have also been used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. This model consisted of stacked <span class="ltx_glossaryref" title="">Convolution Neural Networks (CNNs)</span> followed by bidirectional <span class="ltx_glossaryref" title="">Long Short-Term Memorys (LSTMs)</span> and temporal pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. On 24-videos (4-fold) of the calot triangle and gallbladder dissection phases of real non-robotic laparoscopic cholecystectomy the model achieved 46% Spearman’s rank correlation on a 1-5 scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. An identical model trained on 30-videos (4-fold) of the 3 isolated robotic tasks found in the JIGSAWS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib13" title="">13</a>]</cite> achieved 83% Spearman’s rank correlation on a 1-6 scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. This paper extends these methods to a new and unique dataset, in order to test their capability.</p>
</div>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset description</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Videos</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">During a surgical training course at <span class="ltx_text" id="S3.SS1.p1.1.1">the National Hospital for Neurology and Neurosurgery, London, UK<span class="ltx_text" id="S3.SS1.p1.1.1.1">, 15 simulated surgeries videos (11426-images) were recorded, one per participating surgeon, using a commercially available high-fidelity bench-top phantom of the nasal phase of <span class="ltx_glossaryref" title="">eTSA</span> <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>www.store.upsurgeon.com/products/tnsbox/</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib8" title="">8</a>]</cite>.
The participants were recruited from multiple neurosurgical centres within <span class="ltx_text" id="S3.SS1.p1.1.1.1.1">the United Kingdom<span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1">, with self-reported skill levels (10-novice, 5-expert), receiving tutorials and teaching beforehand. A high-definition endoscope (<span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1">Olympus S200 visera elite endoscope<span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1.1">) was used to record the surgeries at 25-<span class="ltx_glossaryref" title="">frames per second (FPS)</span> with <math alttext="720\times 1080" class="ltx_Math" display="inline" id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.2.cmml">720</mn><mo id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1"><times id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.1"></times><cn id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.2">720</cn><cn id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.1.1.1.1.1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1c">720\times 1080</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.1.1.1.1.1.1.m1.1d">720 × 1080</annotation></semantics></math> resolution, and stored as .mp4 files in a <span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1.1.1">surgical video management and analytics platform (Medtronic, Touch Surgery<sup class="ltx_sup" id="S3.SS1.p1.1.1.1.1.1.1.1.1.1">TM</sup> Ecosystem<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.touchsurgery.com/</span></span></span>)<span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1.1.1.2">. Ethical approval was granted <span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1.1.1.2.1">by the <span class="ltx_glossaryref" title="">Institutional Review Board (IRB)</span> at <span class="ltx_glossaryref" title="">University College London (UCL)</span> (17819/011)<span class="ltx_text" id="S3.SS1.p1.1.1.1.1.1.1.1.1.2.1.1"> with informed participation consent.</span></span></span></span></span></span></span></span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Instrument annotations</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Each video was sampled at 1-<span class="ltx_glossaryref" title="">FPS</span> with <math alttext="720\times 1080" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">720</mn><mo id="S3.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn id="S3.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.2">720</cn><cn id="S3.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">720\times 1080</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">720 × 1080</annotation></semantics></math> resolution, and stored as .png files. Third party annotators (<span class="ltx_text" id="S3.SS2.p1.1.1">Anolytics<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.anolytics.ai/</span></span></span><span class="ltx_text" id="S3.SS2.p1.1.1.1">) manually annotated each image for instrument boundary and class, which was then verified by two neurosurgical trainees and one consultant neurosurgeon. No image contained multiple instruments, and only visible parts of the instrument were annotated if obscured. <span class="ltx_text" id="S3.SS2.p1.1.1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">2</span></a></span>a displays the distribution of the instruments.</span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Surgical skill assessments</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_glossaryref" title="">Modified OSATS (mOSATS)</span>, <span class="ltx_glossaryref" title="">OSATS</span> curated for pituitary videos, was created, leading to 10-aspects each measured between 1-5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib8" title="">8</a>]</cite>. Each video was assessed by two neurosurgical trainees and verified by one consultant neurosurgeon. <span class="ltx_text" id="S3.SS3.p1.1.1">Inter-rater reliability was calculated using Cohen’s Kappa, resulting in 0.949 (<span class="ltx_glossaryref" title="">Confidence Interval (CI)</span> 0.983–0.853) for the 6 general surgical aspects and 0.945 (<span class="ltx_glossaryref" title="">CI</span> 0.981–0.842) for the 4 <span class="ltx_glossaryref" title="">eTSA</span> specific aspects, as defined in the first and second column respectively under ‘<span class="ltx_glossaryref" title="">mOSATS</span> Assessment’ in <span class="ltx_text" id="S3.SS3.p1.1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F3" title="Figure 3 ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">3</span></a></span>.<span class="ltx_text" id="S3.SS3.p1.1.1.2"> <span class="ltx_text" id="S3.SS3.p1.1.1.2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F4" title="Figure 4 ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">4</span></a></span> displays the <span class="ltx_glossaryref" title="">mOSATS</span> distribution.</span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S4.F3.g1" src="extracted/5879470/04workflow.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Complete workflow diagram of this study.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="53" id="S4.F4.g1" src="extracted/5879470/03osats.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution of mOSATS (10-aspects, max 50) across the 15-videos.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Instrument segmentation and tracking</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>PRINTNet</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The simplified diagram of the created architecture is displayed in the dashed green box of <span class="ltx_text" id="S4.SS1.SSS1.p1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F3" title="Figure 3 ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">3</span></a></span>. The encoder is ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib26" title="">26</a>]</cite>, with no pre-training: a well understood; strong performing; and lightweight <span class="ltx_glossaryref" title="">CNN</span> commonly used for medical imaging tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib20" title="">20</a>]</cite>, particularly for <span class="ltx_glossaryref" title="">eTSA</span> recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib27" title="">27</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib28" title="">28</a>]</cite>. The decoder is DeepLabV3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib29" title="">29</a>]</cite>, commonly used in <span class="ltx_glossaryref" title="">eTSA</span> segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib30" title="">30</a>]</cite>, which utilises Atrous (also called dilation) convolutions, as opposed to skip connections found in other decoders. These convolutions skip a certain number of pixels (the dilation rate), which increases the receptive field without sacrificing spatial resolution or increasing the number of weights (and so computationally efficient), allowing object features to be captured on multiple spatial scales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib29" title="">29</a>]</cite>. This is particularly important for instrument segmentation in <span class="ltx_glossaryref" title="">eTSA</span>, given the frequency in which instruments are entering and exiting the endoscopic view, and so the same instrument will be found in a variety of sizes.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_glossaryref" title="">Simple Online and Realtime Tracking (SORT)</span> begins with object detection using a <span class="ltx_glossaryref" title="">CNN</span> as a feature extractor, followed by object estimating via velocity predictions, and finally ensuring the new objects detected and predicted trajectories of the old objects match <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib22" title="">22</a>]</cite>. DeepSORT extends <span class="ltx_glossaryref" title="">SORT</span> through the use of a feature bank (storing features from previous frames), and matching these with the previous predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib31" title="">31</a>]</cite>. StrongSORT extends DeepSORT through the use of an improved: feature extractor; feature bank (now updater); velocity prediction algorithm; and matching algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib32" title="">32</a>]</cite>. Moreover, StrongSORT compensates for camera motion by estimating global rotation and translation between frames <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib32" title="">32</a>]</cite>, which is of importance for instrument tracking in <span class="ltx_glossaryref" title="">eTSA</span>. <span class="ltx_glossaryref" title="">PRINTNet</span> utilises StrongSORT, replacing the object detection model with DeepLabV3.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Real-time implementation</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The implementation is done via the NVIDIA Holoscan SDK<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/nvidia-holoscan/holoscan-sdk</span></span></span> and runs on a NVIDIA Clara AGX<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://www.nvidia.com/en-gb/clara/intelligent-medical-instruments</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib33" title="">33</a>]</cite>. The Holoscan SDK builds a TensorRT<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://developer.nvidia.com/tensorrt</span></span></span> engine, which optimises models through reductions in floating point precision; smaller model size; and dynamic memory allocation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1"><span class="ltx_glossaryref" title="">Mean IoU (mIoU)</span> was the evaluation metric for segmentation models. <span class="ltx_glossaryref" title="">Multiple Object Tracking Precision (MOTP)</span> was the evaluation metric for tracking models, and <span class="ltx_glossaryref" title="">MOTA</span> is given as a secondary metric. <span class="ltx_glossaryref" title="">MOTA</span> is calculated on every frame, and for frames where the ground-truth classification is unknown, it is assumed the ground-truth classification is unchanged since last known. <span class="ltx_glossaryref" title="">MOTP</span> is calculated only on frames where ground-truth segmentations, and hence bounding-boxes, are known. <span class="ltx_text" id="S4.SS1.SSS3.p1.1.1">For these segmentation and tracking metrics a 100% score indicates perfect overlap between the predicted and ground-truth annotation, with 0% indicating no overlap or a missclassification.<span class="ltx_text" id="S4.SS1.SSS3.p1.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_glossaryref" title="">FPS</span> was the metric used to compare the speeds of the models. <span class="ltx_text" id="S4.SS1.SSS3.p2.1.1">A 25-<span class="ltx_glossaryref" title="">FPS</span> model would match the native video frame rate and allow for real-time tracking, whereas a lower frame rate model would mean some frames in the video will be skipped.<span class="ltx_text" id="S4.SS1.SSS3.p2.1.1.1"></span></span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Dataset split</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">4-fold cross-validation was implemented, as 15-videos is not sufficiently large for a reliable training to testing split. The folds were chosen such that each fold contained approximately the same number of images of a given instrument, but images from one video were only present in one fold. Five instrument classes (Blakesly; Irrigation Syringe; Retractable Knife; Dual Scissors; Surgical Drill) were removed from the analysis as they appeared in less than 4-videos, and so could not be present in each fold. This left four instrument classes (Blunt Dissector; Cup Forceps; Kerrisons; Pituitary Ronguers) as displayed in <span class="ltx_text" id="S4.SS1.SSS4.p1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">1</span></a></span>. <span class="ltx_text" id="S4.SS1.SSS4.p1.1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">2</span></a></span>a displays the stark data imbalance between the instrument classes. To mitigate the effect of overtraining on dominant classes, images of the Blunt Dissector and Kerrisons were downsampled by 600 and 1200 respectively, and images of the Cup Forceps and Pituitary Ronguers were upsampled by 400. This was done per fold, and sampled images were chosen at random. <span class="ltx_text" id="S4.SS1.SSS4.p1.1.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">2</span></a></span>b displays the resampled dataset (per fold).</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S4.F5.g1" src="extracted/5879470/05results.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative results of PRINTNet: (a) a strong example where the classification, segmentation, and tracking are accurate; (b) a common example where the classification and tracking are accurate, but the segmentation could be improved at the instrument tip; (c) an uncommon example where classification, segmentation, and tracking are all inaccurate. (See the Supplementary Material for the full video.)</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Segmentation models’ mIoU for each of the four instrument classes across the 4-folds. The highest mIoU for a given instrument is displayed in bold.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:429.3pt;height:60.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.8pt,6.3pt) scale(0.824272039764712,0.824272039764712) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.2">Blunt Dissector</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.3">Cup Forceps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.4">Kerrisons</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.5">Pituitary Ronguers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1.6">All Instruments</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.1.7">No instrument</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.1">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.2">63.8±09.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.3">22.1±18.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.4">62.1±23.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.5">18.6±14.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.1.6">41.6±9.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.7">98.4±0.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.1">SegFormer</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.2">63.4±12.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.2.3.1">24.4±17.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.4">60.2±21.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.5">31.9±24.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.3.2.6">45.0±11.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.7">98.2±0.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.1.1">DeepLabV3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.2.1">66.9±15.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.3">11.8±10.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.4.1">73.4±28.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.5.1">31.9±22.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.6.1">46.0±09.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.3.7.1">98.7±0.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5 </span>Implementation details</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1"><span class="ltx_text" id="S4.SS1.SSS5.p1.1.1">To improve segmentation model training and generalisation, the following augmentation techniques were applied in sequence at random: horizontal flips; vertical flips; rotation; and colour jitters. As a compromise between having a sufficiently large batch size for finding optimal weights during gradient descent and a sufficiently high image resolution for meaningful feature extraction, models were training with a batch size of 16 with training images resized to <math alttext="288\times 512" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p1.1.1.m1.1"><semantics id="S4.SS1.SSS5.p1.1.1.m1.1a"><mrow id="S4.SS1.SSS5.p1.1.1.m1.1.1" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS5.p1.1.1.m1.1.1.2" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.2.cmml">288</mn><mo id="S4.SS1.SSS5.p1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS5.p1.1.1.m1.1.1.3" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p1.1.1.m1.1b"><apply id="S4.SS1.SSS5.p1.1.1.m1.1.1.cmml" xref="S4.SS1.SSS5.p1.1.1.m1.1.1"><times id="S4.SS1.SSS5.p1.1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS5.p1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.2">288</cn><cn id="S4.SS1.SSS5.p1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS5.p1.1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p1.1.1.m1.1c">288\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p1.1.1.m1.1d">288 × 512</annotation></semantics></math> pixels<sup class="ltx_sup" id="S4.SS1.SSS5.p1.1.1.1">2</sup>, which was able to run on a single NVIDIA Tesla V100 Tensor Core 32-GB GPU.<span class="ltx_text" id="S4.SS1.SSS5.p1.1.1.2"></span></span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.2"><span class="ltx_text" id="S4.SS1.SSS5.p2.2.2">Cross-entropy was the loss function and Adam with learning rate 0.00006 was the optimiser, as these choices resulted in improved convergence over focal loss; and dice loss; and other optimiser variations. Each model was run for 50-epochs where the loss function was shown to be sufficiently small (<math alttext="&lt;0.04" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p2.1.1.m1.1"><semantics id="S4.SS1.SSS5.p2.1.1.m1.1a"><mrow id="S4.SS1.SSS5.p2.1.1.m1.1.1" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS5.p2.1.1.m1.1.1.2" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.SSS5.p2.1.1.m1.1.1.1" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS5.p2.1.1.m1.1.1.3" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p2.1.1.m1.1b"><apply id="S4.SS1.SSS5.p2.1.1.m1.1.1.cmml" xref="S4.SS1.SSS5.p2.1.1.m1.1.1"><lt id="S4.SS1.SSS5.p2.1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS1.SSS5.p2.1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.2">absent</csymbol><cn id="S4.SS1.SSS5.p2.1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS5.p2.1.1.m1.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p2.1.1.m1.1c">&lt;0.04</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p2.1.1.m1.1d">&lt; 0.04</annotation></semantics></math>) across all folds with minimal changes in subsequent epochs (<math alttext="&lt;0.005" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p2.2.2.m2.1"><semantics id="S4.SS1.SSS5.p2.2.2.m2.1a"><mrow id="S4.SS1.SSS5.p2.2.2.m2.1.1" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.cmml"><mi id="S4.SS1.SSS5.p2.2.2.m2.1.1.2" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.2.cmml"></mi><mo id="S4.SS1.SSS5.p2.2.2.m2.1.1.1" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS5.p2.2.2.m2.1.1.3" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p2.2.2.m2.1b"><apply id="S4.SS1.SSS5.p2.2.2.m2.1.1.cmml" xref="S4.SS1.SSS5.p2.2.2.m2.1.1"><lt id="S4.SS1.SSS5.p2.2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.1"></lt><csymbol cd="latexml" id="S4.SS1.SSS5.p2.2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.2">absent</csymbol><cn id="S4.SS1.SSS5.p2.2.2.m2.1.1.3.cmml" type="float" xref="S4.SS1.SSS5.p2.2.2.m2.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p2.2.2.m2.1c">&lt;0.005</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p2.2.2.m2.1d">&lt; 0.005</annotation></semantics></math> change after 100-epochs), and so restricting training to 50-epochs limits overfitting and reduces computational time. The model weights of the final (50<sup class="ltx_sup" id="S4.SS1.SSS5.p2.2.2.1">th</sup>) epoch was evaluated on the testing dataset with no early stopping procedure as to be a consistent choice which would not bias the model on any given fold.<span class="ltx_text" id="S4.SS1.SSS5.p2.2.2.2"></span></span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p3">
<p class="ltx_p" id="S4.SS1.SSS5.p3.1">The code is written in Python 3.8 using PyTorch 1.8.1 using CUDA 11.2, and is available at <span class="ltx_text" id="S4.SS1.SSS5.p3.1.1">https://github.com/dreets/printnet<span class="ltx_text" id="S4.SS1.SSS5.p3.1.1.1">. All videos and annotations are available at <span class="ltx_text" id="S4.SS1.SSS5.p3.1.1.1.1">https://doi.org/10.5522/04/26511049<span class="ltx_text" id="S4.SS1.SSS5.p3.1.1.1.1.1">.</span></span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Surgical skill assessment</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In total, 34-metrics were extracted from the tracking data (see <span class="ltx_text" id="S4.SS2.p1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.F6" title="Figure 6 ‣ 5.1.3 Real-time implementation ‣ 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">6</span></a></span>). In summary, it consisted of: time (e.g. instrument visible time); motion (e.g. acceleration); and usage metrics (e.g. number of instrument switches).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For each metric, a <span class="ltx_glossaryref" title="">Pearson Correlation Coefficient (PCC)</span> was calculated against each <span class="ltx_glossaryref" title="">mOSATS</span> aspect and summed <span class="ltx_glossaryref" title="">mOSATS</span>. <span class="ltx_text" id="S4.SS2.p2.1.1">A <span class="ltx_glossaryref" title="">PCC</span> of 1.0 or -1.0 indicates direct positive or negative correlation respectively, with 0.0 indicating no correlation.<span class="ltx_text" id="S4.SS2.p2.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Then, two classification tasks were then performed: multi-class <span class="ltx_glossaryref" title="">mOSATS</span> (mean-averaged and rounded) and binary-class skill level (novice/expert). For each task, a Linear, <span class="ltx_glossaryref" title="">Support Vector Machine (SVM)</span>; <span class="ltx_glossaryref" title="">RF</span>; and <span class="ltx_glossaryref" title="">MultiLayer Perceptron (MLP)</span> model were trained, and boosted via <span class="ltx_glossaryref" title="">Analysis of Variance (ANOVA)</span> feature selection. <span class="ltx_text" id="S4.SS2.p3.1.1">A naïve classifier that only predicts the dominant class would achieve 33.3% accuracy in multi-class by predicting ‘3’ and 66.7% accuracy in binary-class by predicting ‘novice’.<span class="ltx_text" id="S4.SS2.p3.1.1.1"></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Instrument tracking and segmentation</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Instrument segmentation</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">It is found that Blunt Dissector and Kerrisons are segmented well, with much worse performances for Cup Forceps and Pituitary Ronguers (see <span class="ltx_text" id="S5.SS1.SSS1.p1.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.T1" title="Table 1 ‣ 4.1.4 Dataset split ‣ 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">1</span></a></span>). This is due to the heavy data imbalance (<span class="ltx_glossaryref" title="">mIoU</span> =0 for misclassifications), which is difficult to account for given the small number of images used for testing, even if balance sampling was implemented during training (see <span class="ltx_text" id="S5.SS1.SSS1.p1.1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">2</span></a></span>).</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">This difficulty in classification is likely because instrument handles are very similar, and take up a large portion of an image due to the image distortion, and so instruments must be distinguished by their relatively small tips. This can be more clearly seen in <span class="ltx_text" id="S5.SS1.SSS1.p2.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F5" title="Figure 5 ‣ 4.1.4 Dataset split ‣ 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">5</span></a></span>b where PRINTNet struggles in identifying the boundary of the Pituitary Rongeur, but is able to identify the boundary of Kerrisons (<span class="ltx_text" id="S5.SS1.SSS1.p2.1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F5" title="Figure 5 ‣ 4.1.4 Dataset split ‣ 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">5</span></a></span>a), a dominant class. This again implies poor classification rather than poor segmentation, which is verified by ablation studies showing 82.2±0.2% mIoU in binary segmentation.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">When compared to other segmentation models, DeepLabV3 has the highest overall <span class="ltx_glossaryref" title="">mIoU</span>, although closely followed by SegFormer, which also has a significantly higher Cup Forceps <span class="ltx_glossaryref" title="">mIoU</span>. Given more data, it is likely SegFormer will outperform DeepLabV3, as the transformer encoder performs better with larger datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib34" title="">34</a>]</cite>, extracting both local and global spatial features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib35" title="">35</a>]</cite>. U-Net performs worse, as the skip connections between the <span class="ltx_glossaryref" title="">CNN</span> encoder and upsampling decoder prevents derogation of local and not global spatial information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figure class="ltx_figure ltx_minipage ltx_align_middle" id="S5.T2.fig1" style="width:214.6pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Tracking models’ performance across the 4-folds. The highest value for a given evaluation metric is displayed in bold. Note detection frequency was set to 5.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.fig1.1" style="width:214.6pt;height:58.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.0pt,6.8pt) scale(0.810907506148742,0.810907506148742) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.fig1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.fig1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.fig1.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.fig1.1.1.1.1.2">MOTP (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.fig1.1.1.1.1.3">MOTA (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.fig1.1.1.1.1.4">FPS (mean)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.fig1.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.fig1.1.1.2.1.1">SORT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.fig1.1.1.2.1.2">59.1±03.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.fig1.1.1.2.1.3">77.9±07.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.fig1.1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.fig1.1.1.2.1.4.1">24.7±00.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.fig1.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.3.2.1">DeepSORT</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.3.2.2">62.9±05.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.3.2.3">77.9±07.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.fig1.1.1.3.2.4">12.8±00.7</td>
</tr>
<tr class="ltx_tr" id="S5.T2.fig1.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S5.T2.fig1.1.1.4.3.1.1">StrongSORT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T2.fig1.1.1.4.3.2.1">71.9±05.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.fig1.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.fig1.1.1.4.3.3.1">77.9±07.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.fig1.1.1.4.3.4">10.6±02.9</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Instrument tracking</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">StrongSORT has the highest <span class="ltx_glossaryref" title="">MOTP</span> as it accounts for camera motion, although at a lower <span class="ltx_glossaryref" title="">FPS</span> when compared to SORT due to this extra computation (see <span class="ltx_text" id="S5.SS1.SSS2.p1.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.T2" title="Table 2 ‣ 5.1.1 Instrument segmentation ‣ 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">2</span></a></span>). All models have an identical and high <span class="ltx_glossaryref" title="">MOTA</span> as classification is determined by the same DeepLabV3 backbone.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">Moreover, occasionally, <span class="ltx_glossaryref" title="">PRINTNet</span> incorrectly predicts an instrument’s classification; segmentation; and tracking; such as in <span class="ltx_text" id="S5.SS1.SSS2.p2.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S4.F5" title="Figure 5 ‣ 4.1.4 Dataset split ‣ 4.1 Instrument segmentation and tracking ‣ 4 Methods ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">5</span></a></span>c, caused by overpredicting the Blunt Dissector tracking paths from previous frames. These incorrect predictions increase the difficulty of surgical skill analysis as some metrics, such as time of instrument usage, may not be reliable.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Real-time implementation</h4>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S5.F6.g1" src="extracted/5879470/06pcc.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Pearson Correlation Coefficient of the 34-metrics for summed mOSATS.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<figure class="ltx_figure ltx_minipage ltx_align_middle" id="S5.T3.fig1" style="width:214.6pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Accuracy in surgical skill classification across the 4-folds. The highest value for a given metric is displayed in bold.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.fig1.1" style="width:214.6pt;height:78.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.0pt,14.9pt) scale(0.723514802380923,0.723514802380923) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.fig1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.fig1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T3.fig1.1.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.1.1.2">Multi-class</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig1.1.1.1.1.3">Binary-class</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.2.2.1">mean mOSATS (%)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig1.1.1.2.2.2">skill level (%)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.fig1.1.1.3.3.1">Linear</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.fig1.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.3.3.2.1">39.9±24.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.fig1.1.1.3.3.3">80.0±16.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.4.4.1">Support Vector Machine</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.4.4.2.1">46.7±26.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig1.1.1.4.4.3">80.0±26.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.5.5.1">Random Forest</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.5.5.2">40.0±38.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig1.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.5.5.3.1">73.3±24.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.6.6.1">MultiLayer Perceptron</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig1.1.1.6.6.2">26.7±24.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig1.1.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.6.6.3.1">86.7±16.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</figure>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.1">The accelerated <span class="ltx_glossaryref" title="">PRINTNet</span> runs at 22-<span class="ltx_glossaryref" title="">FPS</span> with a 100-millisecond delay at FP16 precision on the NVIDIA Clara AGX. This is sufficient for real-time use, so PRINTNet can be used during surgical training courses. (See Supplementary Material for a live demonstration of this setup.)</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Surgical skill assessment</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Distinguishing between expert and novice skill level achieved a high <math alttext="87\%" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mn id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">87</mn><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1">percent</csymbol><cn id="S5.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS2.p1.1.m1.1.1.2">87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">87\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">87 %</annotation></semantics></math> accuracy (see <span class="ltx_text" id="S5.SS2.p1.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.T3" title="Table 3 ‣ 5.1.3 Real-time implementation ‣ 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">3</span></a></span>), in line with similar studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib24" title="">24</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. However, there was poor accuracy in multi-class mean <span class="ltx_glossaryref" title="">mOSATS</span> classification, although comparable to similar studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib25" title="">25</a>]</cite>. This highlights the complexity of the problem, with the implication that more data is required.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Across the 10-aspects, time based metrics were stronger predictors than motion based metrics. This is seen in <span class="ltx_text" id="S5.SS2.p2.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#S5.F6" title="Figure 6 ‣ 5.1.3 Real-time implementation ‣ 5.1 Instrument tracking and segmentation ‣ 5 Results and Discussion ‣ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom"><span class="ltx_text ltx_ref_tag">6</span></a></span> where <span class="ltx_glossaryref" title="">PCC</span> for summed <span class="ltx_glossaryref" title="">mOSATS</span> is shown. Specifically, ‘ratio of total procedure time to instrument visible time’ is found to be positively correlated with mOSATS, indicating instrument efficiency (i.e. a reduced idle time) is correlated with higher surgical skill. Interestingly, it is found the use of a Blunt Dissector or Cup Forceps is negatively correlated with mOSATS whereas Kerrisons and Pituitary Rongeurs are positively correlated.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">The limited correlation between motion based metrics and mOSATS is an opposing result to that found in robotic thyroid surgery, where instrument motion in the absence of camera motion was a strong predictor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib23" title="">23</a>]</cite>. Removing this camera motion is tricky, as large endoscope movements are required to navigate through the nasal phase of <span class="ltx_glossaryref" title="">eTSA</span> in order to get through the nostril (for both novice and expert surgeons), which outweighs the more subtle movements of the instruments. Although StrongSORT does compensate for this motion, more sophisticated models are needed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Rating surgical skill via instrument tracking during minimally invasive surgery in an objective and reproducible manor remains a difficult task. Existing models have focused on real and robotic laparoscopic surgery, and these models have now been extended to simulated endoscopic surgery. 15-videos of the nasal phase of <span class="ltx_glossaryref" title="">eTSA</span> were performed on a high-fidelity bench-top phantom during a training course and were recorded. They were later assessed for surgical skill by expert surgeons, and instruments were manually segmented. The created model, <span class="ltx_glossaryref" title="">PRINTNet</span>, designed to classify; segment; and track the instruments during the nasal phase of <span class="ltx_glossaryref" title="">eTSA</span> achieved 67% and 73% <span class="ltx_glossaryref" title="">mIoU</span> for the dominant Blunt Dissector and Kerrisons classes, with 72% <span class="ltx_glossaryref" title="">MOTP</span>. 87% accuracy was achieved with a <span class="ltx_glossaryref" title="">MLP</span> when using the <span class="ltx_glossaryref" title="">PRINTNet</span> tracking output to predict whether a surgeon was a novice or expert. Moreover, real-time speeds were achieved when run on a NVIDIA Clara AGX, allowing for real-time feedback for surgeons during training courses. This continuous monitoring of surgical skill allows novice surgeons to consistently improve their skill on simulated surgery before they are sufficiently skilled to perform real surgery. Future work will involve: modifying the model, such as with the use of temporal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib24" title="">24</a>]</cite> or anchor free methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17025v1#bib.bib36" title="">36</a>]</cite>; collecting a larger dataset; and extending this work to real <span class="ltx_glossaryref" title="">eTSA</span>- linking instrument tracking to both surgical skill and real patient outcomes. For now, this paper provides a new and unique publicly available dataset and baseline network, which can be improved on by the community.</p>
</div>
<div class="ltx_para" id="S6.p2">
<span class="ltx_ERROR undefined" id="S6.p2.1">\ack</span>
<p class="ltx_p" id="S6.p2.2"><span class="ltx_text" id="S6.p2.2.1">With thanks to Digital Surgery Ltd, a Medtronic company, for access to Touch Surgery Ecosystem for video recording, annotation and storage.
<span class="ltx_text" id="S6.p2.2.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S6.p3">
<span class="ltx_ERROR undefined" id="S6.p3.1">\fundingandinterests</span>
<p class="ltx_p" id="S6.p3.2"><span class="ltx_text" id="S6.p3.2.1">This work was supported in whole, or in part, by the <span class="ltx_glossaryref" title="">Wellcome/EPSRC Centre for Interventional and Surgical Sciences203145/Z/16/Z (WEISS)</span>, the <span class="ltx_glossaryref" title="">Engineering and Physical Sciences Research CouncilEP/W00805X/1, EP/Y01958X/1, EP/P012841/1 (EPSRC)</span>, the Horizon 2020 FET [GA863146], the Department of Science, Innovation and Technology (DSIT) and the Royal Academy of Engineering under the Chair in Emerging Technologies programme. Adrito Das is supported by the <span class="ltx_glossaryref" title="">EPSRCEP/S021612/1</span>. Danyal Z. Khan is supported by a <span class="ltx_glossaryref" title="">National Institute for Health and Care Research (NIHR)</span> Academic Clinical Fellowship and the <span class="ltx_glossaryref" title="">Cancer Research UK (CRUK)</span> Pre-doctoral Fellowship. Hani J. Marcus is supported by <span class="ltx_glossaryref" title="">WEISSNS/A000050/1</span> and by the <span class="ltx_glossaryref" title="">NIHR</span> Biomedical Research Centre at <span class="ltx_glossaryref" title="">UCL</span>.
<span class="ltx_text" id="S6.p3.2.1.1"></span></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. L. Asa, “Practical pituitary pathology: What does the pathologist need to know?” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Archives of Pathology and Laboratory Medicine</em>, vol. 132, no. 8, p. 1231–1240, Aug. 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Ezzat, S. L. Asa, W. T. Couldwell, C. E. Barr, W. E. Dodge, M. L. Vance, and I. E. McCutcheon, “The prevalence of pituitary adenomas: A systematic review,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Cancer</em>, vol. 101, no. 3, p. 613–619, Jun. 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. Z. Khan, J. G. Hanrahan, S. E. Baldeweg, N. L. Dorward, D. Stoyanov, and H. J. Marcus, “Current and future advances in surgical therapy for pituitary adenoma,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Endocrine Reviews</em>, vol. 44, no. 5, p. 947–959, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. J. Marcus, D. Z. Khan, A. Borg, M. Buchfelder, J. S. Cetas, J. W. Collins, N. L. Dorward, M. Fleseriu, M. Gurnell, M. Javadpour, P. S. Jones, C. H. Koh, H. Layard Horsfall, A. N. Mamelak, P. Mortini, W. Muirhead, N. M. Oyesiku, T. H. Schwartz, S. Sinha, D. Stoyanov, L. V. Syro, G. Tsermoulas, A. Williams, M. J. Winder, G. Zada, and E. R. Laws, “Pituitary society expert delphi consensus: operative workflow in endoscopic transsphenoidal pituitary adenoma resection,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Pituitary</em>, vol. 24, no. 6, p. 839–853, Jul. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. Leach, A. H. Abou-Zeid, T. Kearney, J. Davis, P. J. Trainer, and K. K. Gnanalingham, “Endoscopic transsphenoidal pituitary surgery: Evidence of an operative learning curve,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Neurosurgery</em>, vol. 67, no. 5, p. 1205–1212, Nov. 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N. McLaughlin, E. R. Laws, N. M. Oyesiku, L. Katznelson, and D. F. Kelly, “Pituitary centers of excellence,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Neurosurgery</em>, vol. 71, no. 5, p. 916–926, Nov. 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. A. Martin, G. Regehr, R. Reznick, H. Macrae, J. Murnaghan, C. Hutchison, and M. Brown, “Objective structured assessment of technical skill (osats) for surgical residents: Objective structured assessment of technical skill,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">British Journal of Surgery</em>, vol. 84, no. 2, p. 273–278, Feb. 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N. Newall, D. Z. Khan, J. G. Hanrahan, J. Booker, A. Borg, J. Davids, F. Nicolosi, S. Sinha, N. Dorward, and H. J. Marcus, “High fidelity simulation of the endoscopic transsphenoidal approach: Validation of the upsurgeon tns box,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Frontiers in Surgery</em>, vol. 9, Dec. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Niitsu, N. Hirabayashi, M. Yoshimitsu, T. Mimura, J. Taomoto, Y. Sugiyama, S. Murakami, S. Saeki, H. Mukaida, and W. Takiyama, “Using the objective structured assessment of technical skills (osats) global rating scale to evaluate the skills of surgical trainees in the operating room,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Surgery Today</em>, vol. 43, no. 3, p. 271–275, Sep. 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D. Z. Khan, C. H. Koh, A. Das, A. Valetopolou, J. G. Hanrahan, H. L. Horsfall, S. E. Baldeweg, S. Bano, A. Borg, N. L. Dorward, O. Olukoya, D. Stoyanov, and H. J. Marcus, “Video-based performance analysis in pituitary surgery - part 1: Surgical outcomes,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">World Neurosurgery</em>, Aug. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.wneu.2024.07.218" title="">http://dx.doi.org/10.1016/j.wneu.2024.07.218</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Pedrett, P. Mascagni, G. Beldi, N. Padoy, and J. L. Lavanchy, “Technical skill assessment in minimally invasive surgery using artificial intelligence: a systematic review,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Surgical Endoscopy</em>, vol. 37, no. 10, p. 7412–7424, Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Z. Khan, N. Newall, C. H. Koh, A. Das, S. Aapan, H. L. Horsfall, S. E. Baldeweg, S. Bano, A. Borg, A. Chari, N. L. Dorward, A. Elserius, T. Giannis, A. Jain, D. Stoyanov, and H. J. Marcus, “Video-based performance analysis in pituitary surgery - part 2: Artificial intelligence assisted surgical coaching,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">World Neurosurgery</em>, Aug. 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.wneu.2024.07.219" title="">http://dx.doi.org/10.1016/j.wneu.2024.07.219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Gao, S. S. Vedula, C. E. Reiley, A. Narges, B. Varadarajan, H. C. Lin, L. Tao, L. Zappella, B. Bejar, D. D. Yuh, C. C. G. Chen, R. Vidal, S. Khudanpur, and G. D. Hager, “Jhu-isi gesture and skill assessment working set (jigsaws): a surgical activity dataset for human motion modeling.” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Modeling and monitoring of computer assisted interventions (M2CAI)—MICCAI Workshop</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. L. Lavanchy, J. Zindel, K. Kirtac, I. Twick, E. Hosgor, D. Candinas, and G. Beldi, “Automation of surgical skill assessment using a three-stage machine learning algorithm,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Scientific Reports</em>, vol. 11, no. 1, March 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
I. Rivas-Blanco, C. J. Pérez-del Pulgar, A. Mariani, G. Tortora, and A. J. Reina, “A surgical dataset from the da vinci research kit for task automation and recognition,” 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2102.03643" title="">https://arxiv.org/abs/2102.03643</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
F. Pérez-Escamirosa, A. Alarcón-Paredes, G. A. Alonso-Silverio, I. Oropesa, O. Camacho-Nieto, D. Lorias-Espinoza, and A. Minor-Martínez, “Objective classification of psychomotor laparoscopic skills of surgeons based on three different approaches,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Journal of Computer Assisted Radiology and Surgery</em>, vol. 15, no. 1, p. 27–40, Oct. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. Soangra, R. Sivakumar, E. R. Anirudh, S. V. Reddy Y., and E. B. John, “Evaluation of surgical skill using machine learning with optimal wearable sensor locations,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">PLOS ONE</em>, vol. 17, no. 6, p. e0267936, Jun. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Das, D. Z. Khan, D. Psychogyios, Y. Zhang, J. G. Hanrahan, F. Vasconcelos, Y. Pang, Z. Chen, J. Wu, X. Zou, G. Zheng, A. Qayyum, M. Mazher, I. Razzak, T. Li, J. Ye, J. He, S. Płotka, J. Kaleta, A. Yamlahi, A. Jund, P. Godau, S. Kondo, S. Kasai, K. Hirasawa, D. Rivoir, A. Pérez, S. Rodriguez, P. Arbeláez, D. Stoyanov, H. J. Marcus, and S. Bano, “Pitvis-2023 challenge: Workflow recognition in videos of endoscopic pituitary surgery,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2409.01184" title="">https://arxiv.org/abs/2409.01184</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Rueckert, D. Rueckert, and C. Palm, “Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Computers in Biology and Medicine</em>, vol. 169, p. 107929, Feb. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Wang, Q. Sun, Z. Liu, and L. Gu, “Visual detection and tracking algorithms for minimally invasive surgical instruments: A comprehensive review of the state-of-the-art,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Robotics and Autonomous Systems</em>, vol. 149, p. 103945, Mar. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” 2015. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1505.04597" title="">https://arxiv.org/abs/1505.04597</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and realtime tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">2016 IEEE International Conference on Image Processing (ICIP)</em>.   IEEE, Sep. 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Lee, H. W. Yu, H. Kwon, H.-J. Kong, K. E. Lee, and H. C. Kim, “Evaluation of surgical skills during robotic surgery by deep learning-based multiple surgical instrument tracking in training and actual operations,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Journal of Clinical Medicine</em>, vol. 9, no. 6, p. 1964, Jun. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Fathollahi, M. H. Sarhan, R. Pena, L. DiMonte, A. Gupta, A. Ataliwala, and J. Barker, <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Video-Based Surgical Skills Assessment Using Long Term Tool Tracking</em>.   Springer Nature Switzerland, 2022, p. 541–550.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Li, L. Gu, W. Wang, R. Nakamura, and Y. Sato, <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Surgical Skill Assessment via Video Semantic Aggregation</em>.   Springer Nature Switzerland, 2022, p. 410–420.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2015. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1512.03385" title="">https://arxiv.org/abs/1512.03385</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Das, S. Bano, F. Vasconcelos, D. Z. Khan, H. J. Marcus, and D. Stoyanov, “Reducing prediction volatility in the surgical workflow recognition of endoscopic pituitary surgery,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International Journal of Computer Assisted Radiology and Surgery</em>, vol. 17, no. 8, p. 1445–1452, Apr. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Das, D. Z. Khan, J. G. Hanrahan, H. J. Marcus, and D. Stoyanov, “Automatic generation of operation notes in endoscopic pituitary surgery videos using workflow recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Intelligence-Based Medicine</em>, vol. 8, p. 100107, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous convolution for semantic image segmentation,” 2017. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1706.05587" title="">https://arxiv.org/abs/1706.05587</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Das, D. Z. Khan, S. C. Williams, J. G. Hanrahan, A. Borg, N. L. Dorward, S. Bano, H. J. Marcus, and D. Stoyanov, <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery</em>.   Springer Nature Switzerland, 2023, p. 472–482.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime tracking with a deep association metric,” 2017. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1703.07402" title="">https://arxiv.org/abs/1703.07402</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Du, Z. Zhao, Y. Song, Y. Zhao, F. Su, T. Gong, and H. Meng, “Strongsort: Make deepsort great again,” 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.13514" title="">https://arxiv.org/abs/2202.13514</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Sinha, S. Dwivedi, and M. Azizian, “Towards deterministic end-to-end latency for medical ai systems in nvidia holoscan,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.04466" title="">https://arxiv.org/abs/2402.04466</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. Sourget, S. N. Hasany, F. Mériaudeau, and C. Petitjean, <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Can SegFormer be a True Competitor to U-Net for Medical Image Segmentation?</em>   Springer Nature Switzerland, Dec. 2023, p. 111–118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,” 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.15203" title="">https://arxiv.org/abs/2105.15203</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
G. Ding, X. Zhao, C. Peng, L. Li, J. Guo, D. Li, and X. Jiang, “Anchor-free feature aggregation network for instrument detection in endoscopic surgery,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">IEEE Access</em>, vol. 11, p. 29464–29473, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 15:22:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
