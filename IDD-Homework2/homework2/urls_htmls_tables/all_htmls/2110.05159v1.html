<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.05159] Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking</title><meta property="og:description" content="On â€ â€ * Authors contributed equally the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for speciâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.05159">

<!--Generated on Wed Mar  6 21:03:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dirk VÃ¤th
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pascal Tilli
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ngoc Thang Vu 
<br class="ltx_break">University of Stuttgart 
<br class="ltx_break">Germany 
<br class="ltx_break">{dirk.vaeth, pascal.tilli, thang.vu}@ims.uni-stuttgart.de
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">On <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup>* Authors contributed equally</span></span></span> the way towards general <span title="" class="ltx_glossaryref">Visual Question Answering (VQA)</span> systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets.
To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of <span title="" class="ltx_glossaryref">VQA</span>.
Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise.
Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior.
Interactive filtering facilitates discovery of problematic behavior, down to the data sample level.
As proof of concept, we perform a case study on four models.
We find that state-of-the-art <span title="" class="ltx_glossaryref">VQA</span> models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they cannot recognize text in images.
Our metrics allow us to quantify which image and question embeddings provide most robustness to a model.
All code<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/patilli/vqa_benchmarking</span></span></span> is publicly available.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span title="" class="ltx_glossaryref">VQA</span> refers to the multi-modal task of answering free-form, natural language questions about images - a task sometimes referred to as a visual Turing test <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.05159/assets/img/intro_ov_cropped.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Tool landing page with aggregated metrics for each model (larger version in Appendix A). Statistics can be expanded per model to view performance on each dataset.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The number and variety of datasets for evaluating such systems has continued to increase over the last years <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>); Hudson and Manning (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>); Agrawal etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Kervadec etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Johnson etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>.
These datasets aim to test modelsâ€™ abilities with respect to different skills, such as commonsense or external knowledge reasoning, visual reasoning, or reading text in images.
Traditionally, evaluation relies solely on answering accuracy.
However, it is misleading to believe that a single number, like high accuracy on a given benchmark, corresponds to a systemâ€™s ability to answer arbitrary questions with high quality.
Each dataset contains biases which state-of-the-art deep neural networks are prone to exploit, resulting in higher accuracy scores <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>); Das etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>); Agrawal etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>); Jabri etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite>.
Thus, most <span title="" class="ltx_glossaryref">VQA</span> models, if evaluated on multiple benchmarks at all, are re-trained per dataset to achieve higher numbers in specialized leaderboards.
Further shortcomings of current leaderboards include ignoring prediction cost and robustness, as discussed in <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh and Jurafsky (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> for NLP.
In <span title="" class="ltx_glossaryref">VQA</span>, we need even more specialized evaluation due to the challenges inherent to open-ended, multi-modal reasoning.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In order to successfully develop <span title="" class="ltx_glossaryref">VQA</span> systems that are able to answer arbitrary questions with human-like performance, we should overcome the previously mentioned shortcomings of current leaderboards as one of the first essential steps.
To this end, we propose a benchmarking tool, to our knowledge the first of its kind in the <span title="" class="ltx_glossaryref">VQA</span> domain, that goes beyond current leaderboard evaluations. It follows the four following principles:</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">1. Realism</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">To better simulate real-world conditions, we test robustness to semantic-preserving input perturbations to images and questions.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">2. Generalizability</h4>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">We include six carefully chosen benchmark datasets, each evaluating different abilities as well as model behavior on changing distributions to test model generalizability.
Additionally, we provide easy python interfaces for adding new benchmarking datasets and state-of-the-art models as they arise. The full tool is released under an open-source license.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3. Explainability</h4>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p1.1" class="ltx_p">To provide more insight into model behavior and overcome the problem of single-metric comparisons, we measure scores such as biases and uncertainty, in addition to accuracy.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">4. Interactivity</h4>

<div id="S1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p1.1" class="ltx_p">Aggregated statistics can be drilled down and filtered, providing interactive exploration of model behavior from a global dataset perspective down to individual data samples.
The above functionalities not only support detailed model comparison, but also facilitate development and debugging of models in the <span title="" class="ltx_glossaryref">VQA</span> domain.</p>
</div>
<div id="S1.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p2.1" class="ltx_p">As proof of concept, we integrate several popular and state-of-the-art models from public code repositories and investigate their abilities and weaknesses.
Through our case study, we demonstrate that all of these models fail to generalize, even to other in-domain test sets.
Our metrics quantify the influence of model architecture decisions, which accuracy cannot capture, such as the effect of image and question embeddings on model robustness.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">VQA Benchmarks</span> Benchmarks often emphasize certain sub-tasks of the general <span title="" class="ltx_glossaryref">VQA</span> problem.
For example, CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> tests visual reasoning abilities such as shape recognition and spatial relationships between objects, rather than real-world scenarios.
Other approaches change the answer distributions of existing datasets, such as VQA-CP <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> originating from VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> or GQA-OOD from GQA <cite class="ltx_cite ltx_citemacro_cite">Kervadec etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
These changes are intended to mitigate learnable bias. Another approach to mitigating biases was proposed by <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>, who created a dataset for real world visual reasoning with a tightly controlled answer distribution.
<cite class="ltx_cite ltx_citemacro_citet">Kervadec etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> went one step further by analyzing and changing the test sets to evaluate on rarely occurring question-answer pairs rather than on frequent ones. Finally, <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> proposed an adversarial benchmarking dataset to evaluate system robustness.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">For more automated, dataset-level insight, many methods try to analyze single aspects of <span title="" class="ltx_glossaryref">VQA</span> models. For example, <cite class="ltx_cite ltx_citemacro_citet">Halbe (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> use feature attribution to assess the influence of individual question words on model predictions.
<cite class="ltx_cite ltx_citemacro_citet">Das etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> compare human attention to machine attention to explore whether they focus on the same image regions.
To measure robustness w.r.t. question input, <cite class="ltx_cite ltx_citemacro_citet">Huang etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite> collect a dataset of semantically relevant questions, and rank them by similarity, feeding the top-3 into the network to observe changes in prediction.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Identifying Biases</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Agrawal etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite> measured multiple properties: generalization to novel instances as selected by dissimilarity, question understanding based on length and POS-tags, and image understanding by selecting a subset of questions which share an answer but have different images.
Another approach to analyze bias towards one modality is to train a uni-modal network that excludes the other modality in the training phase <cite class="ltx_cite ltx_citemacro_cite">Cadene etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>.
However, this requires training one model per modality and cannot be applied easily to all architectures, e.g. to attention mechanisms computed on joint feature spaces.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Robustness and Adversarial Examples</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Adversarial examples originate from image classification, where perturbations barely visible to a human fool the classifier and cause sudden prediction changes <cite class="ltx_cite ltx_citemacro_cite">Szegedy etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2014</a>)</cite>.
The same idea was later applied to NLP, where, e.g., appending distracting text to the context in a question answering scenario resulted in F1-score dropping by more than half <cite class="ltx_cite ltx_citemacro_cite">Jia and Liang (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Benchmarking Tools</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> propose a leaderboard for NLP tasks to compare model performance.
They differentiate among several NLP tasks and datasets.
All methods are applied post-hoc to analyze the predictions a model outputs.
Other benchmarking tools, for example, focus on runtime comparisons <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2016</a>); Liu etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. Our benchmarking tool not only analyzes system outputs, but also modifies input modalities as well as feature spaces and provides metrics beyond just accuracy.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VQA Benchmark Tool</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our tool facilitates global evaluation of model performance w.r.t general and specific tasks (<span id="S3.p1.1.1" class="ltx_text ltx_font_bold">generalizability</span>), such as real-world images and reading capabilities.
To simplify integration of future benchmark datasets and models, we provide a well-documented python API.
We measure model-inherent properties, such as biases and uncertainty (<span id="S3.p1.1.2" class="ltx_text ltx_font_bold">explainability</span>) as well as robustness against input perturbations (<span id="S3.p1.1.3" class="ltx_text ltx_font_bold">realism</span>).
Model behavior can be further inspected using interactive diagrams and filtering methods for all metrics, supporting sample-level exploration of suspicious model behavior (<span id="S3.p1.1.4" class="ltx_text ltx_font_bold">interactivity</span>).
All data is collected post-hoc and can be explored in a web application, eliminating the need to re-train existing models.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">In this section, we describe the datasets supported out-of-the-box.
These serve the principle of benchmarking <span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_bold">generalizability</span>, by including real-world scenarios as well as task-specific and even synthetic datasets.
Where labels are publicly available, we rely on test sets, otherwise on validation sets (marked with <sup id="S3.SS1.p1.2.2" class="ltx_sup">âˆ—</sup>).
To reduce computational cost and resources (including environmental impact), we limit each dataset to a maximum of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mo id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\sim</annotation></semantics></math>15,000 randomly drawn samples , which is referred to in the following paragraphs as a sub-set.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA2<sup id="S3.SS1.SSS0.Px1.2.1" class="ltx_sup">âˆ—</sup>
</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">This dataset <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> represents a balanced version of the vanilla VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>.
It is intended to mirror real-world scenarios and used as the de-facto baseline for model comparisons.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">GQA</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The GQA dataset <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>, derived from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, is designed to test modelsâ€™ real-word visual reasoning capabilities, in particular, robustness, consistency and semantic understanding of vision and language.
Similar to VQA2, it also provides a balanced version.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">GQA-OOD</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_citet">Kervadec etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, evaluating on rare instead of frequent question-answer pairs is better suited for measuring reasoning abilities.
Hence, they introduce the GQA-OOD dataset as a new split of the original GQA dataset to evaluate out-of-distribution questions with imbalanced distributions.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CLEVR<sup id="S3.SS1.SSS0.Px4.2.1" class="ltx_sup">âˆ—</sup>
</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> is a synthetic dataset, containing images of multiple geometric objects in different colors, materials and arrangements.
It aims to test modelsâ€™ visual reasoning abilities by asking questions that require a model to identify objects based on attributes, presence, count, and spacial relationships.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OK-VQA<sup id="S3.SS1.SSS0.Px5.2.1" class="ltx_sup">âˆ—</sup>
</h4>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Marino etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> introduce a dataset that requires external knowledge to answer its questions, thereby motivating the integration of additional knowledge pools.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text VQA<sup id="S3.SS1.SSS0.Px6.2.1" class="ltx_sup">âˆ—</sup>
</h4>

<div id="S3.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px6.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Singh etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite> consider the problem of understanding text in images,
an important problem to consider in <span title="" class="ltx_glossaryref">VQA</span> benchmarking systems, as one application of <span title="" class="ltx_glossaryref">VQA</span> is intended to aid the visually impaired.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In addition to the evaluation of accuracy across datasets with different distributions and focuses, we implement metrics such as bias of models towards one modality and uncertainty (<span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">explainability</span>), as well as robustness to noise and adversarial questions (<span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">realism</span>). All metrics are in range <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="[0,100]" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">100</mn><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="closed" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">100</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">[0,100]</annotation></semantics></math>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Accuracy</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Our tool supports multiple ground truth answers with different scores per sample, providing the flexibility to evaluate for single-answer accuracy as well as e.g. the official VQA2 accuracy measure <math id="S3.SS2.SSS0.Px1.p1.1.m1.3" class="ltx_Math" alttext="acc(a)=min(1,\frac{\#\text{humans}(a)}{3})" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.3a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.3.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1b" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.5.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.5.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.5.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1b" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.1.cmml">(</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">1</mn><mo id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.1.cmml">,</mo><mfrac id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.3.cmml">#</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2.cmml">â€‹</mo><mtext id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4a.cmml">humans</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2a" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.5.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.5.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">a</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.5.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">)</mo></mrow></mrow><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">3</mn></mfrac><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.3b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4"><eq id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2"><times id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.3">ğ‘</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.2.4">ğ‘</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2">ğ‘</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3"><times id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.2">ğ‘š</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.3">ğ‘–</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.4">ğ‘›</ci><interval closure="open" id="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.4.3.5.2"><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3">1</cn><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><divide id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"></divide><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.2"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.3">#</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4a.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.4">humans</mtext></ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1">ğ‘</ci></apply><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">3</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.3c">acc(a)=min(1,\frac{\#\text{humans}(a)}{3})</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Modality Bias</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.12" class="ltx_p">Here, we refer to a modelâ€™s focus on one modality over the other.
Given an image of a zoo and the question <em id="S3.SS2.SSS0.Px2.p1.12.1" class="ltx_emph ltx_font_italic">â€œWhat animals are shown?â€</em>, if we replace this picture with a fruit bowl, we would expect the model to change its prediction.
However, if the prediction stays unaltered, the modelâ€™s answer cannot depend on the image input.
For each prediction on altered inputs <math id="S3.SS2.SSS0.Px2.p1.1.m1.2" class="ltx_Math" alttext="(i^{\prime},q)" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.2a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml">(</mo><msup id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml">â€²</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">q</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.2b"><interval closure="open" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2">ğ‘–</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3">â€²</ci></apply><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.2c">(i^{\prime},q)</annotation></semantics></math> or <math id="S3.SS2.SSS0.Px2.p1.2.m2.2" class="ltx_Math" alttext="(i,q^{\prime})" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.2a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml">,</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.2b"><interval closure="open" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">ğ‘–</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3">â€²</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.2c">(i,q^{\prime})</annotation></semantics></math>, we evaluate how many times the answer <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="a^{\prime}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><msup id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">a</mi><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">a^{\prime}</annotation></semantics></math> of the replacement pairs is the same as answer <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">a</annotation></semantics></math> predicted on the original inputs <math id="S3.SS2.SSS0.Px2.p1.5.m5.2" class="ltx_Math" alttext="(i,q)" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.2a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.2.3.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.5.m5.2.3.2.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.2.cmml">q</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.5.m5.2.3.2.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.2b"><interval closure="open" id="S3.SS2.SSS0.Px2.p1.5.m5.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">ğ‘–</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.2.2">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.2c">(i,q)</annotation></semantics></math>.
Averaging across <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">N</annotation></semantics></math> trials yields a Monte-Carlo estimate of the bias towards one modality as <math id="S3.SS2.SSS0.Px2.p1.7.m7.4" class="ltx_Math" alttext="1/N\sum_{q^{\prime}}\mathds{1}_{f(q,i)=f(q^{\prime},i)}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.4a"><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.5" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.cmml"><mn id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.2.cmml">1</mn><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.1.cmml">/</mo><mi id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.3.cmml">N</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.cmml"><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.2.cmml">âˆ‘</mo><msup id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.2.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.3.cmml">â€²</mo></msup></msub><msub id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.cmml"><mn id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.2.cmml">ğŸ™</mn><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.2.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.1.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.7.m7.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.2.2.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.2.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.5" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.5.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.2.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.2.cmml">(</mo><msup id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.2.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.2.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.7.m7.3.3.3.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.3.3.3.3.cmml">i</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.4" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.2.cmml">)</mo></mrow></mrow></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.4b"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5"><times id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.1"></times><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2"><divide id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.1"></divide><cn type="integer" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.2">1</cn><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.2.3">ğ‘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1">subscript</csymbol><sum id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.2"></sum><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.1.3.3">â€²</ci></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2">subscript</csymbol><cn type="integer" id="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.5.3.2.2">1</cn><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4"><eq id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.5.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.5"></eq><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6"><times id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.1"></times><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.2">ğ‘“</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.6.3.2"><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.1">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.2.2.2.2">ğ‘–</ci></interval></apply><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4"><times id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.2"></times><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.3">ğ‘“</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.4.4.4.4.1.1.1.3">â€²</ci></apply><ci id="S3.SS2.SSS0.Px2.p1.7.m7.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.3.3.3.3">ğ‘–</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.4c">1/N\sum_{q^{\prime}}\mathds{1}_{f(q,i)=f(q^{\prime},i)}</annotation></semantics></math>.
Heuristics, such as ensuring no overlap between subjects and objects of <math id="S3.SS2.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.1a"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.1b"><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.1c">q</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="q^{\prime}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.9.m9.1a"><msup id="S3.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m9.1b"><apply id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m9.1c">q^{\prime}</annotation></semantics></math>, help reduce cases where <math id="S3.SS2.SSS0.Px2.p1.10.m10.1" class="ltx_Math" alttext="q^{\prime}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.10.m10.1a"><msup id="S3.SS2.SSS0.Px2.p1.10.m10.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml">q</mi><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m10.1c">q^{\prime}</annotation></semantics></math> would just be a rephrasing of <math id="S3.SS2.SSS0.Px2.p1.11.m11.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.11.m11.1a"><mi id="S3.SS2.SSS0.Px2.p1.11.m11.1.1" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.11.m11.1b"><ci id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.11.m11.1c">q</annotation></semantics></math>.
High values in modality bias correspond to models ignoring input from one modality for many samples, e.g. a question bias of <math id="S3.SS2.SSS0.Px2.p1.12.m12.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.12.m12.1a"><mn id="S3.SS2.SSS0.Px2.p1.12.m12.1.1" xref="S3.SS2.SSS0.Px2.p1.12.m12.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.12.m12.1b"><cn type="integer" id="S3.SS2.SSS0.Px2.p1.12.m12.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m12.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.12.m12.1c">100</annotation></semantics></math> indicates a model that completely ignores images.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Robustness to Noise</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">An important consideration when deploying a model in the real world, is its susceptibility to noise.
Noise might be induced naturally by data acquisition methods (<span title="" class="ltx_glossaryref">VQA</span>-setting: camera), for example a color-question should not be affected by subtle tone shifts between two cameras.
On the question side, semantic-preserving input changes can be induced through paraphrases, synonyms or region-dependent spelling.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p2.1" class="ltx_p">For measuring robustness to noise in images, we support adding Gaussian-, Poisson-, salt&amp;pepper- and speckle-noise to the original input image.
We also support adding Gaussian noise in image feature space. To obtain a realistic input range, we calculate the standard deviation from 500 randomly sampled image feature vectors.
After multiple trials, we average how often the prediction on the noisy inputs matches the original prediction.
Applying noise to the original image input tests the robustness of the image feature extractor, which, in many models, is external and thus easy to swap and interesting to compare.
On the other hand, applying noise in feature space tests model robustness towards noisy feature extractors.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p3.1" class="ltx_p">Measuring robustness to question noise is done by adding Gaussian noise in embedding space, a reasonable approach under the assumption that similar vectors in embedding space have similar meaning.
Again, multiple trials are performed.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p4" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p4.1" class="ltx_p">High values in robustness correspond to models unaffected by noise in one modality for many samples, e.g. a question robustness of <math id="S3.SS2.SSS0.Px3.p4.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS0.Px3.p4.1.m1.1a"><mn id="S3.SS2.SSS0.Px3.p4.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p4.1.m1.1b"><cn type="integer" id="S3.SS2.SSS0.Px3.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p4.1.m1.1c">100</annotation></semantics></math> indicates a model that never changed its predictions due to noise added in question embedding space.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Robustness to Adversarial Questions</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p"><span title="" class="ltx_glossaryref">Semantically Equivalent Adversarial Ruless (SEARs)</span> alter textual input according to a set of rules, while preserving original semantics <cite class="ltx_cite ltx_citemacro_cite">Ribeiro etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>.
For the questions in the <span title="" class="ltx_glossaryref">VQA</span> dataset, the authors come up with the four rules that most affect the predictions in their tests, using a combination of <span title="" class="ltx_glossaryref">Part-of-Speech (POS)</span>-Tags and vocabulary entries:</p>
</div>
<div id="S3.SS2.SSS0.Px4.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Rule 1</em> <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\text{WP VBZ}\rightarrow\text{WP's}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mtext id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2a.cmml">WP VBZ</mtext><mo stretchy="false" id="S3.I1.i1.p1.1.m1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.cmml">â†’</mo><mtext id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3a.cmml">WPâ€™s</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><ci id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1">â†’</ci><ci id="S3.I1.i1.p1.1.m1.1.1.2a.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2"><mtext id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">WP VBZ</mtext></ci><ci id="S3.I1.i1.p1.1.m1.1.1.3a.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"><mtext id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">WPâ€™s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\text{WP VBZ}\rightarrow\text{WP's}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Rule 2</em> <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\text{What NOUN}\rightarrow\text{Which NOUN}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mtext id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2a.cmml">What NOUN</mtext><mo stretchy="false" id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">â†’</mo><mtext id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3a.cmml">Which NOUN</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><ci id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1">â†’</ci><ci id="S3.I1.i2.p1.1.m1.1.1.2a.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2"><mtext id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">What NOUN</mtext></ci><ci id="S3.I1.i2.p1.1.m1.1.1.3a.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><mtext id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">Which NOUN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\text{What NOUN}\rightarrow\text{Which NOUN}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Rule 3</em> <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\text{color}\rightarrow\text{colour}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mtext id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2a.cmml">color</mtext><mo stretchy="false" id="S3.I1.i3.p1.1.m1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.cmml">â†’</mo><mtext id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3a.cmml">colour</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><ci id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1">â†’</ci><ci id="S3.I1.i3.p1.1.m1.1.1.2a.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"><mtext id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">color</mtext></ci><ci id="S3.I1.i3.p1.1.m1.1.1.3a.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3"><mtext id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">colour</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\text{color}\rightarrow\text{colour}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><em id="S3.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Rule 4</em> <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="\text{ADV VBZ}\rightarrow\text{ADV's}" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mtext id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2a.cmml">ADV VBZ</mtext><mo stretchy="false" id="S3.I1.i4.p1.1.m1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">â†’</mo><mtext id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3a.cmml">ADVâ€™s</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><ci id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1">â†’</ci><ci id="S3.I1.i4.p1.1.m1.1.1.2a.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2"><mtext id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">ADV VBZ</mtext></ci><ci id="S3.I1.i4.p1.1.m1.1.1.3a.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3"><mtext id="S3.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3">ADVâ€™s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\text{ADV VBZ}\rightarrow\text{ADV's}</annotation></semantics></math></p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS0.Px4.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p3.1" class="ltx_p">High values in robustness against <span title="" class="ltx_glossaryref">SEARs</span> correspond to models unaffected by adversarial questions, e.g. a robustness of <math id="S3.SS2.SSS0.Px4.p3.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS0.Px4.p3.1.m1.1a"><mn id="S3.SS2.SSS0.Px4.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px4.p3.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p3.1.m1.1b"><cn type="integer" id="S3.SS2.SSS0.Px4.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p3.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p3.1.m1.1c">100</annotation></semantics></math> indicates a model that never changed its predictions due to the application of any of the above rules. Therefore, higher values are preferable.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Uncertainty</h4>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">To measure model certainty, we leverage the dropout-based Monte-Carlo method <cite class="ltx_cite ltx_citemacro_cite">Gal and Ghahramani (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>.
Forwarding a sample multiple times with active dropout, the averaged output vector <math id="S3.SS2.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\frac{1}{N}\sum_{n=1}^{N}f(x)" display="inline"><semantics id="S3.SS2.SSS0.Px5.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.cmml"><mfrac id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.cmml"><mn id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.2.cmml">1</mn><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.cmml"><msubsup id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.2.cmml">n</mi><mo id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.3.cmml">N</mi></msubsup><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.3.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.3.2.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.3.2.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2"><times id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.1"></times><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2"><divide id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2"></divide><cn type="integer" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.2">1</cn><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.2.3">ğ‘</ci></apply><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3"><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1">subscript</csymbol><sum id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.2"></sum><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3"><eq id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.1"></eq><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.2">ğ‘›</ci><cn type="integer" id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.1.3">ğ‘</ci></apply><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2"><times id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.1"></times><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.2.3.2.2">ğ‘“</ci><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1">ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px5.p1.1.m1.1c">\frac{1}{N}\sum_{n=1}^{N}f(x)</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Views</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We support inspection of the included metrics at different levels of granularity, from comparisons across multiple datasets to filtering of individual samples (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">interactivity</span>).
On each level, we supplement the accuracy measure by additional metrics helpful for understanding and debugging <span title="" class="ltx_glossaryref">VQA</span> models (<span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_bold">explainability</span>).</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gobal View</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">The global view (see figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) acts as the main entry to our tool.
At a glance, it shows a leaderboard with statistics averaged on all datasets, providing users with an impression of the modelsâ€™ performance and properties across tasks and distributions.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2110.05159/assets/img/overview_detail_view.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Expanded details on the overview page (larger version in Appendix A).</figcaption>
</figure>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p2.1" class="ltx_p">All columns are sortable to allow easy comparisons between models for each metric.
Each row in the overview table describes a modelâ€™s average performance and can be expanded to provide additional information on a per-dataset level (see figure <a href="#S3.F2" title="Figure 2 â€£ Gobal View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Metrics View</h4>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2110.05159/assets/img/metric_view_gqa_img_bias.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Metric view, showing bias towards images on the GQA dataset for the MDETR model (larger version in Appendix A).</figcaption>
</figure>
<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">Clicking a model row in the global view navigates to the metrics view, which provides graphs on all metrics and datasets for the selected model in detail (see figure <a href="#S3.F3" title="Figure 3 â€£ Metrics View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Users have the choice to change dataset and metric via selection boxes.
For easy comparison between datasets of different sizes, all values are recorded in percentages of the dataset.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Filter View</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">Our tool supports searching for patterns of suspicious model behavior by providing a filter view (see figure <a href="#S3.F4" title="Figure 4 â€£ Filter View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
Once model, dataset and metric are selected, users are presented a list of all samples within the chosen range.
The range can be adjusted using a slider, which updates the list of matching data samples in real-time.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2110.05159/assets/img/filter_view.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Filter view. Enables filtering for unexpected model behavior on sample level (larger version in Appendix A).</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Sample View</h4>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px4.p1.1" class="ltx_p">Finally, once the desired range of samples has been filtered, clicking a data sample navigates to the sample view (see figure <a href="#S3.F5" title="Figure 5 â€£ Sample View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
There, the original input image and question are displayed, along with ground truth and the modelâ€™s top-3 predictions.
Additionally, the scores and answers for each single metric are shown.
For example, if applying noise to the image changed the prediction multiple times, we show all the answers that were predicted using those noisy inputs.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2110.05159/assets/img/sample_view.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sample view with <span id="S3.F5.2.1" class="ltx_text ltx_font_italic">Image Bias Word Space</span> metric. Similar cards exist for each metric (larger version in Appendix A).</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Case Study</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As a case study, we explore a range of models from well-established, previously high ranking entries in the VQA2 competition to more recent, transformer-based architectures and report the insights we gained by inspecting them with our tool.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluated Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We chose a widely used <span title="" class="ltx_glossaryref">VQA</span>-baseline BAN, two transformer-based architectures MDETR and MCAN, and MMNASNET.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BAN</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Kim etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> is a strong baseline using bilinear attention. It
won third place in the VQA2 2018 challenge and was still in the top-10 entries in 2019. We use the 8-layer version.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MCAN</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> improves BAN with a co-attention feature fusion mechanism.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MMNASNET</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Yu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> is a more recent state-of-the-art model constructed using neural architecture search. It is one of the top-10 entries of the VQA2 2020 challenge.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MDETR</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Kamath etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> is a state-of-the art transformer using more recent question <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> and image embedding approaches <cite class="ltx_cite ltx_citemacro_cite">Carion etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.
MDETR achieves competitive accuracies on both GQA and CLEVR.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and Lessons Learned</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Results and Lessons Learned â€£ 4 Case Study â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> contains the aggregated results of all models, averaged across the development (sub-)splits of all datasets.
For details about the computation of each metric, see section <a href="#S3.SS2" title="3.2 Metrics â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:110.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.5pt,8.2pt) scale(0.869751328183474,0.869751328183474) ;">
<table id="S4.T1.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1.1" class="ltx_tr">
<td id="S4.T1.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="10"><span id="S4.T1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Average Results</span></td>
</tr>
<tr id="S4.T1.3.1.2.2" class="ltx_tr">
<td id="S4.T1.3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S4.T1.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T1.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Modality Bias</td>
<td id="S4.T1.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Robustness Image</td>
<td id="S4.T1.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Robustness</td>
<td id="S4.T1.3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SEARs</td>
<td id="S4.T1.3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Uncer-</td>
<td id="S4.T1.3.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Parameters</td>
</tr>
<tr id="S4.T1.3.1.3.3" class="ltx_tr">
<td id="S4.T1.3.1.3.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.3.1.3.3.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">Image</td>
<td id="S4.T1.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Quest.</td>
<td id="S4.T1.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">Image</td>
<td id="S4.T1.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Feature</td>
<td id="S4.T1.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r">Question</td>
<td id="S4.T1.3.1.3.3.8" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r">tainty</td>
<td id="S4.T1.3.1.3.3.10" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T1.3.1.4.4" class="ltx_tr">
<td id="S4.T1.3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">MCAN</td>
<td id="S4.T1.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.1.4.4.2.1" class="ltx_text ltx_font_bold">41.30</span></td>
<td id="S4.T1.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">4.83</td>
<td id="S4.T1.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.57</td>
<td id="S4.T1.3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">99.98</td>
<td id="S4.T1.3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">81.09</td>
<td id="S4.T1.3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.14</td>
<td id="S4.T1.3.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.45</td>
<td id="S4.T1.3.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.77</td>
<td id="S4.T1.3.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">201,723,191</td>
</tr>
<tr id="S4.T1.3.1.5.5" class="ltx_tr">
<td id="S4.T1.3.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">MMNASNET</td>
<td id="S4.T1.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">40.80</td>
<td id="S4.T1.3.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.5.5.3.1" class="ltx_text ltx_font_bold">4.67</span></td>
<td id="S4.T1.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.1.5.5.4.1" class="ltx_text ltx_font_bold">2.51</span></td>
<td id="S4.T1.3.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.5.5.5.1" class="ltx_text ltx_font_bold">99.99</span></td>
<td id="S4.T1.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">79.90</td>
<td id="S4.T1.3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">65.18</td>
<td id="S4.T1.3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">59.65</td>
<td id="S4.T1.3.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r">31.75</td>
<td id="S4.T1.3.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r">211,166,871</td>
</tr>
<tr id="S4.T1.3.1.6.6" class="ltx_tr">
<td id="S4.T1.3.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">BAN-8</td>
<td id="S4.T1.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">38.62</td>
<td id="S4.T1.3.1.6.6.3" class="ltx_td ltx_align_center">5.21</td>
<td id="S4.T1.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">3.31</td>
<td id="S4.T1.3.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.6.6.5.1" class="ltx_text ltx_font_bold">99.99</span></td>
<td id="S4.T1.3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">82.22</td>
<td id="S4.T1.3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r">70.09</td>
<td id="S4.T1.3.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r">53.53</td>
<td id="S4.T1.3.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r">66.10</td>
<td id="S4.T1.3.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r">112,167,258</td>
</tr>
<tr id="S4.T1.3.1.7.7" class="ltx_tr">
<td id="S4.T1.3.1.7.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">MDETR</td>
<td id="S4.T1.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">38.82</td>
<td id="S4.T1.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b">4.77</td>
<td id="S4.T1.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2.60</td>
<td id="S4.T1.3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b">36.33</td>
<td id="S4.T1.3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.1.7.7.6.1" class="ltx_text ltx_font_bold">90.10</span></td>
<td id="S4.T1.3.1.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.1.7.7.7.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.3.1.7.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.1.7.7.8.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.3.1.7.7.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.3.1.7.7.9.1" class="ltx_text ltx_font_bold">24.58</span></td>
<td id="S4.T1.3.1.7.7.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">185,847,022</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average results of our evaluated models across all development (sub-)splits.
We split the columns modality bias and robustness against image modifications into two sub-columns.
These columns should be read as the modality bias (lower values are better) measured for the image space and question space.
Robustness (higher values are better) against image changes is divided into alterations on the image itself as well as modifications inside image feature space .
All metrics are in range <math id="S4.T1.2.m1.2" class="ltx_Math" alttext="[0,100]" display="inline"><semantics id="S4.T1.2.m1.2b"><mrow id="S4.T1.2.m1.2.3.2" xref="S4.T1.2.m1.2.3.1.cmml"><mo stretchy="false" id="S4.T1.2.m1.2.3.2.1" xref="S4.T1.2.m1.2.3.1.cmml">[</mo><mn id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml">0</mn><mo id="S4.T1.2.m1.2.3.2.2" xref="S4.T1.2.m1.2.3.1.cmml">,</mo><mn id="S4.T1.2.m1.2.2" xref="S4.T1.2.m1.2.2.cmml">100</mn><mo stretchy="false" id="S4.T1.2.m1.2.3.2.3" xref="S4.T1.2.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.2c"><interval closure="closed" id="S4.T1.2.m1.2.3.1.cmml" xref="S4.T1.2.m1.2.3.2"><cn type="integer" id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">0</cn><cn type="integer" id="S4.T1.2.m1.2.2.cmml" xref="S4.T1.2.m1.2.2">100</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.2d">[0,100]</annotation></semantics></math>. </figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 â€£ 4.2 Results and Lessons Learned â€£ 4 Case Study â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows model accuracy per dataset.
Unsurprisingly, models performed best when evaluated on the development (sub-)split of the dataset they were originally trained on, and worse on datasets they were not trained on.
These performance drops are observable for all models, suggesting that <span title="" class="ltx_glossaryref">VQA</span> models cannot yet generalize well across tasks.
Low performance of current highly ranked <span title="" class="ltx_glossaryref">VQA</span> models on new datasets can partially be attributed to their fixed answer spaces.
This implies the need for more research into systems that are able to generate answers instead of treating <span title="" class="ltx_glossaryref">VQA</span> as a multiple-choice problem.
However, even changing distributions of the same dataset leads to a large performance drop, as we observe, for example, in GQA and its out-of-distribution variants, GQA-OOD-HEAD and GQA-OOD-TAIL.
By swapping the original GQA dataset for the GQA-OOD-TAIL distribution, MDETR accuracy decreased by more than <math id="S4.SS2.p2.1.m1.2" class="ltx_Math" alttext="11,6\%" display="inline"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.2.1" xref="S4.SS2.p2.1.m1.2.2.2.cmml"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">11</mn><mo id="S4.SS2.p2.1.m1.2.2.1.2" xref="S4.SS2.p2.1.m1.2.2.2.cmml">,</mo><mrow id="S4.SS2.p2.1.m1.2.2.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.cmml"><mn id="S4.SS2.p2.1.m1.2.2.1.1.2" xref="S4.SS2.p2.1.m1.2.2.1.1.2.cmml">6</mn><mo id="S4.SS2.p2.1.m1.2.2.1.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><list id="S4.SS2.p2.1.m1.2.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">11</cn><apply id="S4.SS2.p2.1.m1.2.2.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.2">6</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">11,6\%</annotation></semantics></math>.
That out-of-distribution testing causes such high losses in accuracy indicates models are still relying on biases learned from the training dataset.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:68.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-70.0pt,11.0pt) scale(0.756019764931883,0.756019764931883) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">CLEVR</span></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">GQA</span></th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">GQA-OOD-ALL</span></th>
<th id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">GQA-OOD-HEAD</span></th>
<th id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.6.1" class="ltx_text ltx_font_bold">GQA-OOD-TAIL</span></th>
<th id="S4.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.7.1" class="ltx_text ltx_font_bold">OK-VQA</span></th>
<th id="S4.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.8.1" class="ltx_text ltx_font_bold">TextVQA</span></th>
<th id="S4.T2.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.9.1" class="ltx_text ltx_font_bold">VQA2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">MCAN</th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.87</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.58</td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.67</td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.78</td>
<td id="S4.T2.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.59</td>
<td id="S4.T2.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.46</td>
<td id="S4.T2.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.49</td>
<td id="S4.T2.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.9.1" class="ltx_text ltx_font_bold">85.94</span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<th id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">MMNASNET</th>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">31.95</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">44.50</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">40.24</td>
<td id="S4.T2.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">42.01</td>
<td id="S4.T2.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">37.35</td>
<td id="S4.T2.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">35.09</td>
<td id="S4.T2.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">8.81</td>
<td id="S4.T2.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.3.2.9.1" class="ltx_text ltx_font_bold">86.51</span></td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<th id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">BAN-8</th>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">28.64</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">41.86</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">38.95</td>
<td id="S4.T2.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">40.97</td>
<td id="S4.T2.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">35.65</td>
<td id="S4.T2.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">32.88</td>
<td id="S4.T2.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r">8.40</td>
<td id="S4.T2.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.4.3.9.1" class="ltx_text ltx_font_bold">81.60</span></td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<th id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">MDETR</th>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25.44</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.1.5.4.3.1" class="ltx_text ltx_font_bold">61.42</span></td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">55.76</td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">59.43</td>
<td id="S4.T2.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">49.76</td>
<td id="S4.T2.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">9.83</td>
<td id="S4.T2.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4.70</td>
<td id="S4.T2.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">44.22</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy across the development (sub-)splits of different datasets. Bold entries mark best accuracy per model and coincides in all cases with the dataset it was trained on.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">All systems struggled to read text in images, in fact, the highest accuracy score on TextVQA was only <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="8.81\%" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">8.81</mn><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">8.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">8.81\%</annotation></semantics></math>, achieved by MMNASNET.
This might be improved by extending existing <span title="" class="ltx_glossaryref">VQA</span>-architectures with additional inputs, e.g. from optical character recognition, or adapting the training of currently used image feature extractors.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Applying noise in image space has almost no impact on models using bottom-up-topdown feature extraction <cite class="ltx_cite ltx_citemacro_cite">Anderson etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, in contrast to MDETR, the only model using an alternative approach.
In feature space, all models are similarly stable, which could imply that the feature extractor in MDETR could be made more robust by augmenting training with noisy images.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">All models show highest modality bias and low accuracy on the CLEVR dataset. Given that no models were trained on synthetic images or questions involving such complex selection and spatial reasoning, this hints at the models not understanding either modality well.
Inspection using the filter view on modality biases provides more evidence of understanding problems here, showing that for example BAN-8 nearly always guesses <em id="S4.SS2.p5.1.1" class="ltx_emph ltx_font_italic">yes</em> or <em id="S4.SS2.p5.1.2" class="ltx_emph ltx_font_italic">no</em>, regardless of the question asked or the image given.
In general, BAN-8 displays the highest modality bias, indicating more recent models have become better at jointly reasoning over image and text.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span title="" class="ltx_glossaryref">SEAR</span> and question robustness metrics show that RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> provides substantial robustness to question perturbation;
there were zero cases causing MDETR to change predictions, suggesting that context-aware embeddings should be a standard consideration for future <span title="" class="ltx_glossaryref">VQA</span> models.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Our metrics show that state-of-the-art <span title="" class="ltx_glossaryref">VQA</span> models are optimized for specific tasks or datasets, but fail to generalize even across other in-domain datasets.
In order to be successful in real-world applications, systems must demonstrate a variety of abilities, not merely good performance on a single-purpose test set.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our proposed benchmarking tool is the first of its kind in the domain of <span title="" class="ltx_glossaryref">VQA</span> and addresses the problems of current single-metric leaderboards in this domain.
It provides easy to use and fast comparison of integrated models on a global level.
The performance of each model is evaluated across multiple special-purpose as well as general-purpose datasets to test generalizability and capabilities.
Each model can be quantified by metrics such as accuracy, biases, robustness, and uncertainty, revealing strengths and weaknesses w.r.t to given tasks, i.e. measuring the properties models offer as well as their real-world robustness.
Exploration via filtering can be used to identify suspicious behaviour down to single data sample level.
Through this, our tool provides deeper insights into the strengths and weaknesses of each model across tasks and metrics and how architectural choices can affect behavior, encouraging researchers to develop <span title="" class="ltx_glossaryref">VQA</span> systems with rich sets of abilities that stand up to real-world environments.
The open-source tool itself can be installed as a package and extended with new models, datasets and metrics using our python API.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In the future, we plan to extend this tool with new datasets as they are released.
Moreover, we are looking for more metrics for model evaluation as well as more detailed dataset analysis, e.g. answer space overlap.
Last but not least, interactivity could be extended towards live model feedback, allowing to change inputs, e.g. the image noise level, and observe model outputs at runtime.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research was funded by the Cluster of Excellence EXC 2075 "Data-Integrated Simulation Science" at the University of Stuttgart.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2016)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1955â€“1960.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock">Donâ€™t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 4971â€“4980.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077â€“6086.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
CÂ Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425â€“2433.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadene etÂ al. (2019)</span>
<span class="ltx_bibblock">
Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, etÂ al. 2019.

</span>
<span class="ltx_bibblock">Rubi: Reducing unimodal biases for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
841â€“852.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion etÂ al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko. 2020.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 213â€“229.
Springer.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock">Human attention in visual question answering: Do humans and deep
networks look at the same regions?

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 163:90 â€“ 100.

</span>
<span class="ltx_bibblock">Language in Vision.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh and Jurafsky (2020)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh and Dan Jurafsky. 2020.

</span>
<span class="ltx_bibblock">Utility is in the eye of the user: A critique of nlp leaderboard
design.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4846â€“4853.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal and Ghahramani (2016)</span>
<span class="ltx_bibblock">
Yarin Gal and Zoubin Ghahramani. 2016.

</span>
<span class="ltx_bibblock">Dropout as a bayesian approximation: Representing model uncertainty
in deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">international conference on machine learning</em>, pages
1050â€“1059.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6904â€“6913.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halbe (2020)</span>
<span class="ltx_bibblock">
Shaunak Halbe. 2020.

</span>
<span class="ltx_bibblock">Exploring weaknesses of vqa models through attribution driven
insights.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ACL 2020</em>, pageÂ 64.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, CuongÂ Duc Dao, Modar Alfadly, and Bernard Ghanem. 2019.

</span>
<span class="ltx_bibblock">A novel framework for robustness analysis of visual qa models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 33, pages 8449â€“8456.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
DrewÂ A Hudson and ChristopherÂ D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6700â€“6709.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri etÂ al. (2016)</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens Van DerÂ Maaten. 2016.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 727â€“739.
Springer.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia and Liang (2017)</span>
<span class="ltx_bibblock">
Robin Jia and Percy Liang. 2017.

</span>
<span class="ltx_bibblock">Adversarial examples for evaluating reading comprehension systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2021â€“2031.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens Van DerÂ Maaten, LiÂ Fei-Fei,
CÂ LawrenceÂ Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 2901â€“2910.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath etÂ al. (2021)</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and
Nicolas Carion. 2021.

</span>
<span class="ltx_bibblock">Mdetrâ€“modulated detection for end-to-end multi-modal understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.12763</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kervadec etÂ al. (2021)</span>
<span class="ltx_bibblock">
Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. 2021.

</span>
<span class="ltx_bibblock">Roses are red, violets are blueâ€¦ but should vqa expect them to?

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 2776â€“2785.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
1564â€“1574.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma, etÂ al.
2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32â€“73.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2021)</span>
<span class="ltx_bibblock">
Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. 2021.

</span>
<span class="ltx_bibblock">Adversarial vqa: A new benchmark for evaluating the robustness of vqa
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.00245</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Ling Liu, Yanzhao Wu, Wenqi Wei, Wenqi Cao, Semih Sahin, and QiÂ Zhang. 2018.

</span>
<span class="ltx_bibblock">Benchmarking deep learning frameworks: Design considerations, metrics
and beyond.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2018 IEEE 38th International Conference on Distributed
Computing Systems (ICDCS)</em>, pages 1258â€“1269. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai,
Yixin Liu, Zihuiwen Ye, and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock">Explainaboard: An explainable leaderboard for nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.06387</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly
optimized bert pretraining approach</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 3195â€“3204.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro etÂ al. (2018)</span>
<span class="ltx_bibblock">
MarcoÂ Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/p18-1079" title="" class="ltx_ref ltx_href">Semantically
Equivalent Adversarial Rules for Debugging NLP models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics</em>, pages 856â€“865.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2016)</span>
<span class="ltx_bibblock">
Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016.

</span>
<span class="ltx_bibblock">Benchmarking state-of-the-art deep learning software tools.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2016 7th International Conference on Cloud Computing and Big
Data (CCBD)</em>, pages 99â€“104. IEEE.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarjan, Meet Shah, YuÂ Jiang, Xinlei Chen, Devi Parikh,
and Marcus Rohrbach. 2019.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 8317â€“8326.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy etÂ al. (2014)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1312.6199" title="" class="ltx_ref ltx_href">Intriguing properties of
neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, and Dawn
Song. 2018.

</span>
<span class="ltx_bibblock">Fooling vision and language models despite localization and attention
mechanism.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 4951â€“4961.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, and QiÂ Tian. 2020.

</span>
<span class="ltx_bibblock">Deep multimodal neural architecture search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</em>, pages 3743â€“3752.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and QiÂ Tian. 2019.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6281â€“6290.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<figure id="A1.F6" class="ltx_figure"><img src="/html/2110.05159/assets/img/metric_view_gqa_img_bias_test.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Zoom into Figure <a href="#S3.F3" title="Figure 3 â€£ Metrics View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</figcaption>
</figure>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2110.05159/assets/img/intro_ov.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in full size.</figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2110.05159/assets/img/filter_view.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Figure <a href="#S3.F4" title="Figure 4 â€£ Filter View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in full size.</figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure"><img src="/html/2110.05159/assets/img/sample_view.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="402" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Figure <a href="#S3.F5" title="Figure 5 â€£ Sample View â€£ 3.3 Views â€£ 3 VQA Benchmark Tool â€£ Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in full size.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.05158" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.05159" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.05159">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.05159" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.05160" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 21:03:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
