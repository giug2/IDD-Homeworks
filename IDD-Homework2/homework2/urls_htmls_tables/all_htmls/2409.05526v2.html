<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks</title>
<!--Generated on Tue Sep 10 16:45:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender Systems,  Evaluation Framework,  Benchmarking,  Reproducibility." lang="en" name="keywords"/>
<base href="/html/2409.05526v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#S1" title="In RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#S2" title="In RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Key features and Innovations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#S3" title="In RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>System Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#S4" title="In RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Workflow And Use cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#S5" title="In RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions and Future Works</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinyang Shao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Huawei Ireland Research Centre</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Ireland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xinyang.shao@huawei-partners.com">xinyang.shao@huawei-partners.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Edoardo D’Amico
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Huawei Ireland Research Centre</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Ireland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:edoardo.damico@huawei-partners.com">edoardo.damico@huawei-partners.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gabor Fodor
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Huawei Ireland Research Centre</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Ireland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:gabor.fodor@huawei-partners.com">gabor.fodor@huawei-partners.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tri Kurniawan Wijaya
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Huawei Ireland Research Centre</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Ireland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tri.kurniawan.wijaya@huawei.com">tri.kurniawan.wijaya@huawei.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(20 February 2007; 12 March 2009; 5 June 2009)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Recommender systems research lacks standardized benchmarks for reproducibility and algorithm comparisons. We introduce RBoard, a novel framework addressing these challenges by providing a comprehensive platform for benchmarking diverse recommendation tasks, including CTR prediction, Top-N recommendation, and others. RBoard’s primary objective is to enable fully reproducible and reusable experiments across these scenarios. The framework evaluates algorithms across multiple datasets within each task, aggregating results for a holistic performance assessment. It implements standardized evaluation protocols, ensuring consistency and comparability. To facilitate reproducibility, all user-provided code can be easily downloaded and executed, allowing researchers to reliably replicate studies and build upon previous work. By offering a unified platform for rigorous, reproducible evaluation across various recommendation scenarios, RBoard aims to accelerate progress in the field and establish a new standard for recommender systems benchmarking in both academia and industry. The platform is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://rboard.org" title="">https://rboard.org</a> and the demo video can be found at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bit.ly/rboard-demo" title="">https://bit.ly/rboard-demo</a>.</p>
</div>
<div class="ltx_keywords">Recommender Systems, Evaluation Framework, Benchmarking, Reproducibility.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Evaluation of retrieval results</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems play a crucial role in shaping our digital experiences, from product discovery to content consumption <cite class="ltx_cite ltx_citemacro_citep">(Ricci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib7" title="">2010</a>)</cite>. As this field rapidly evolves, the need for standardized evaluation methods and replicable experiments has become critical. Despite algorithmic advancements, the research community struggles to effectively compare approaches and validate results across diverse tasks and datasets <cite class="ltx_cite ltx_citemacro_citep">(Said and Bellogín, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib8" title="">2014</a>; Beel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib3" title="">2016</a>; Ferrari Dacrema et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib5" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The recommender systems domain encompasses a wide range of tasks, including click-through rate (CTR) prediction, top-N recommendation, and sequential and session-based recommendations <cite class="ltx_cite ltx_citemacro_citep">(Yang and Zhai, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib10" title="">2022</a>; Cremonesi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib4" title="">2010</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib9" title="">2021</a>)</cite>. Each of these tasks presents unique challenges and requires specific evaluation metrics. However, the lack of a unified benchmarking framework has led to inconsistent evaluation protocols <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib11" title="">2022</a>)</cite>, limited reproducibility <cite class="ltx_cite ltx_citemacro_citep">(Ferrari Dacrema et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib5" title="">2019</a>)</cite>, and potential dataset biases. These challenges impede progress on two fronts: researchers struggle to build upon each other’s work, while companies face difficulties in identifying and integrating the most effective algorithms for their unique business needs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these issues, we introduce RBoard, a novel framework that offers a systematic approach to evaluating and comparing recommender systems. RBoard distinguishes itself by providing a unified platform for benchmarking diverse recommender tasks, with a primary focus on enabling fully reproducible and reusable experiments. The framework implements consistent evaluation protocols across multiple datasets for each task, ensuring fair comparisons and offering a comprehensive assessment of algorithm performance. By facilitating downloading and execution of user-provided code, RBoard streamlines experiment replication and promotes transparency in the research process. This approach aims to establish a new standard for benchmarking recommender systems algorithms.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Key features and Innovations</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="S2.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Architecture overview of the RBoard Framework.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Recommender system research is shaped by two main tools: full-pipeline frameworks and benchmarking platforms. Full-pipeline frameworks like Elliot <cite class="ltx_cite ltx_citemacro_citep">(Anelli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib2" title="">2021</a>)</cite>, RecBole <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib12" title="">2021</a>)</cite>, and SSLRec <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib6" title="">2024</a>)</cite> offer comprehensive solutions for dataset preprocessing, algorithm implementation, and evaluation. While these frameworks have advanced standardized workflows, challenges in ensuring reproducibility and reusability persist due to variations in implementation details and the need for modifications in complex experiments.
Existing benchmarking platforms <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05526v2#bib.bib13" title="">2022</a>)</cite> often fall short in providing truly reproducible benchmarks by overlooking critical aspects of the recommendation process, particularly hyperparameter tuning. This oversight limits the practical relevance of benchmarks and fails to capture important real-world considerations, such as the time required for hyperparameter optimization.
RBoard addresses these challenges by introducing a flexible, task-agnostic benchmarking environment that accommodates a wide range of recommender system scenarios and frameworks. This versatility allows RBoard to complement existing tools while focusing on two core objectives:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Reproducibility:</span> Ensuring that all experiments can be easily replicated, regardless of the underlying implementation approach.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Reusability:</span> Facilitating the reuse of research code and methodologies across different studies and contexts.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.p1.2">RBoard’s design accommodates various implementation approaches, from custom code to full-pipeline frameworks, while providing a standardized evaluation environment. This flexibility allows researchers to use their preferred methods while still benefiting from consistent benchmarking. By enabling direct comparison of experiments and enhancing the reusability of research code, RBoard aims to improve the reproducibility of results across diverse recommender system studies.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>System Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">RBoard’s architecture is fundamentally designed to ensure reproducibility and reusability in recommender system research. This is achieved through the following design choices:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Data Handling and Preprocessing.</span>
Central to RBoard’s design is its approach to data handling and preprocessing. The platform manages dataset preprocessing and splitting, ensuring consistency across experiments and eliminating variations in data preparation that could affect reproducibility. Users receive preprocessed input data for model training and prediction, while test sets remain hidden to maintain evaluation integrity. To foster transparency, RBoard makes preprocessing code available for review, but withholds specific randomization element to prevent overfitting and ensure result generalizability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Task Evaluation.</span>
The evaluation process provides task-specific metrics with openly available evaluation code, allowing researchers to understand precisely how their algorithms are assessed. A key innovation of RBoard is its aggregation of results across multiple datasets within each task, offering a more comprehensive and generalizable view of algorithm performance. This multi-dataset evaluation approach helps mitigate dataset-specific biases and provides a more robust assessment of recommender systems across varied contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">User Code Integration.</span>
User code integration follows a standardized approach. Researchers upload their code with a main.py file as the entry point, which the platform executes with necessary inputs as command-line arguments. This uniform method ensures consistency across submissions and simplifies experiment reproduction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Hyper-parameters tuning.</span>
RBoard identifies hyperparameter tuning as an essential component of reproducible experiments. Users must include tuning processes in their main.py file, ensuring the entire experimental pipeline can be replicated. This approach not only captures the final performance metrics but also assesses the time and computational resources required for optimization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Code Availability and Reusability.</span>
To promote reusability and collaboration, RBoard makes all submitted code available for download, allowing researchers to build upon existing work and verify results independently.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Workflow And Use cases</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.05526v2#S2.F1" title="Figure 1 ‣ 2. Key features and Innovations ‣ RBoard: A Unified Platform for Reproducible and Reusable Recommender System Benchmarks"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, RBoard’s workflow begins with users uploading their code, including required hyperparameter tuning. The submission manager executes this code for each dataset within a task, collecting and evaluating predictions on separate test data. RBoard then aggregates results to compute overall performance metrics. Both aggregated and single dataset results are displayed on a public leaderboard, offering a comprehensive view of algorithm effectiveness across diverse data contexts.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">RBoard serves diverse use cases in the recommender systems community. Researchers can benchmark their algorithms against state-of-the-art approaches, gaining insights into performance across various scenarios. Industry practitioners can identify top-performing algorithms and adapt them to their specific needs, bridging the gap between academic research and real-world applications.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions and Future Works</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">RBoard addresses critical challenges in recommender system research by providing a standardized platform for benchmarking diverse tasks. Its key contributions include a unified evaluation environment, standardized protocols, and open code availability, all enhancing reproducibility and reusability. Future work will focus on expanding the range of supported tasks, exploring different splitting and preprocessing protocols, and developing diverse strategies for aggregating benchmark results across datasets.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anelli et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Vito Walter Anelli, Alejandro Bellogín, Antonio Ferrara, Daniele Malitesta, Felice Antonio Merra, Claudio Pomo, Francesco Maria Donini, and Tommaso Di Noia. 2021.

</span>
<span class="ltx_bibblock">Elliot: A comprehensive and rigorous framework for reproducible recommender systems evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval</em>. 2405–2414.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beel et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Joeran Beel, Corinna Breitinger, Stefan Langer, Andreas Lommatzsch, and Bela Gipp. 2016.

</span>
<span class="ltx_bibblock">Towards reproducibility in recommender-systems research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">User modeling and user-adapted interaction</em> 26, 1 (2016), 69–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cremonesi et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. 2010.

</span>
<span class="ltx_bibblock">Performance of recommender algorithms on top-n recommendation tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the fourth ACM conference on Recommender systems</em>. 39–46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrari Dacrema et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019.

</span>
<span class="ltx_bibblock">Are we really making much progress? A worrying analysis of recent neural recommendation approaches. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 13th ACM Conference on Recommender Systems</em> (Copenhagen, Denmark) <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(RecSys ’19)</em>. Association for Computing Machinery, New York, NY, USA, 101–109.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3298689.3347058" title="">https://doi.org/10.1145/3298689.3347058</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, and Chao Huang. 2024.

</span>
<span class="ltx_bibblock">Sslrec: A self-supervised learning framework for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>. 567–575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ricci et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Francesco Ricci, Lior Rokach, and Bracha Shapira. 2010.

</span>
<span class="ltx_bibblock">Introduction to recommender systems handbook.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Recommender systems handbook</em>. Springer, 1–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Said and Bellogín (2014)</span>
<span class="ltx_bibblock">
Alan Said and Alejandro Bellogín. 2014.

</span>
<span class="ltx_bibblock">Comparative recommender system evaluation: benchmarking recommendation frameworks. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 8th ACM Conference on Recommender systems</em>. 129–136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shoujin Wang, Longbing Cao, Yan Wang, Quan Z Sheng, Mehmet A Orgun, and Defu Lian. 2021.

</span>
<span class="ltx_bibblock">A survey on session-based recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">ACM Computing Surveys (CSUR)</em> 54, 7 (2021), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Zhai (2022)</span>
<span class="ltx_bibblock">
Yanwu Yang and Panyu Zhai. 2022.

</span>
<span class="ltx_bibblock">Click-through rate prediction in online advertising: A literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Information Processing &amp; Management</em> 59, 2 (2022), 102853.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Zihan Lin, Zhichao Feng, Pengfei Wang, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">A Revisiting Study of Appropriate Offline Evaluation for Top-N Recommendation Algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">ACM Trans. Inf. Syst.</em> 41, 2, Article 32 (dec 2022), 41 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3545796" title="">https://doi.org/10.1145/3545796</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">proceedings of the 30th acm international conference on information &amp; knowledge management</em>. 4653–4664.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022.

</span>
<span class="ltx_bibblock">BARS: Towards Open Benchmarking for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">SIGIR ’22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 2912–2923.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3477495.3531723" title="">https://doi.org/10.1145/3477495.3531723</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 16:45:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
