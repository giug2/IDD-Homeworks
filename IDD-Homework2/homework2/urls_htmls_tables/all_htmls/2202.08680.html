<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2202.08680] Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund.</title><meta property="og:description" content="Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by p…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2202.08680">

<!--Generated on Thu Mar  7 18:24:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Computer Vision Synthetic Data Polyp Segmentation Unsupervised Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Insight SFI Centre for Data Analytics, Ireland </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Dublin City University, Ireland
</span></span></span>
<h1 class="ltx_title ltx_title_document">Synthetic data for unsupervised polyp segmentation <span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enric Moreu 
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1336-6477" title="ORCID identifier" class="ltx_ref">0000-0003-1336-6477</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kevin McGuinness
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1336-6477" title="ORCID identifier" class="ltx_ref">0000-0003-1336-6477</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noel E. O’Connor
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4033-9135" title="ORCID identifier" class="ltx_ref">0000-0002-4033-9135</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by producing realistic synthetic images using a combination of 3D technologies and generative adversarial networks. We use zero annotations from medical professionals in our pipeline.
Our fully unsupervised method achieves promising results on five real polyp segmentation datasets.
As a part of this study we release Synth-Colon, an entirely synthetic dataset that includes <span id="id2.id1.1" class="ltx_text ltx_number">20 000</span> realistic colon images and additional details about depth and 3D geometry: <a target="_blank" href="https://enric1994.github.io/synth-colon" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://enric1994.github.io/synth-colon</a></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Computer Vision Synthetic Data Polyp Segmentation Unsupervised Learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Colorectal cancer is one of the most commonly diagnosed cancer types. It can be treated with an early intervention, which consists of detecting and removing polyps in the colon. The accuracy of the procedure strongly depends on the medical professionals experience and hand-eye coordination during the procedure, which can last up to 60 minutes. Computer vision can provide real-time support for doctors to ensure a reliable examination by double-checking all the tissues during the colonoscopy.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/mask.png" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/gan.png" id="S1.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/depth.png" id="S1.F1.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/mesh.png" id="S1.F1.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synth-Colon dataset samples include: synthetic image, annotation, realistic image, depth map, and 3D mesh (from left to right).</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The data obtained during a colonoscopy is accompanied by a set of issues that prevent creating datasets for computer vision applications.
First, there are privacy issues because it is considered personal data that can not be used without the consent of the patients.
Second, there are a wide range of cameras and lights used to perform colonoscopies. Every device has its own focal length, aperture, and resolution. There are no large datasets with standardized parameters.
Finally, polyp segmentation datasets are expensive because they depend on the annotations of qualified professionals with limited available time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose an unsupervised method to detect polyps that does not require annotations by combining 3D rendering and a CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
First, we produce artificial colons and polyps based on a set of parameters. Annotations of the location of the polyps are automatically generated by the 3D engine.
Second, the synthetic images are used alongside real images to train a CycleGAN. The CycleGAN is used to make the synthetic images appear more realistic.
Finally, we train a HarDNeT-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a state-of-the-art polyp segmentation architecture, with the realistic synthetic data and our self-generated synthetic labels.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, we are the first to train a polyp segmentation model with zero annotations from the real world.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a pipeline that preserves the self-generated annotations when shifting the domain from synthetic to real.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We release Synth-Colon (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), the largest synthetic dataset for polyp segmentation including additional data such as depth and 3D mesh.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of the paper is structured as follows: Section 2 reviews relevant work, Section 3 explains our method, Section 4 presents the Synth-Colon dataset, Section 5 describes our experiments, and Section 6 concludes the paper.”</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Here we briefly review some relevant works related to polyp segmentation and synthetic data.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Polyp segmentation</h3>

<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/22.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="85" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/22_mask.png" id="S2.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="85" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/69.png" id="S2.F2.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="85" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/69_mask.png" id="S2.F2.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="85" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Real samples from CVC-ColonDB with the corresponding annotation made by a medical professionals indicating the location of cancerous polyps.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Early polyp segmentation was based in the texture and shape of the polyps. For example, Hwang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> used ellipse fitting techniques based on shape. However, some corectal polyps can be small (5mm) and are not detected by these techniques. In addition, the texture is easily confused with other tissues in the colon as can be seen in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Polyp segmentation ‣ 2 Related work ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">With the rise of convolutional neural networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> the problem of the texture and shape of the polyps was solved and the accuracy was substantially increased. Several authors have applied deep convolutional networks to the polyp segmentation problem.
Brandao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed to use a fully convolutional neural network based on the VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> architecture to identify and segment polyps. Unfortunately, the small datasets available and the large number of parameters make these large networks prone to overfitting. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> used an encoder-decoder network with dense skip pathways between layers that prevented the vanishing gradient problem of VGG networks. They also significantly reduced the number of parameters, reducing the amount of overfitting. More recently, Chao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> reduced the number of shortcut connections in the network to speed-up inference time, a critical issue when performing real-time colonoscopies in high-resolution. They focused on reducing the memory traffic to access intermediate features, reducing the latency. Finally, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> improved the performance and inference time by combining HarDNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> with a cascaded partial decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> that discards larger resolution features of shallower layers to reduce latency.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The limitation of using large neural networks is that they often require large amounts of annotated data. This problem is particularly acute in medical imaging due to problems in privacy, standardization, and the lack of professional annotators. Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Synthetic data ‣ 2 Related work ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the limited size and resolution of the datasets used to train and evaluate existing polyp segmentation models.
The lack of large datasets for polyp segmentation can be addressed by generating synthetic data.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Thambawita et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> used a generative adversarial network (GAN) to produce new colonoscopy images and annotations. They added a fourth channel to SinGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to generate annotations that are consistent with the colon image. They then used style transfer to improve the realism of the textures. Their results are excellent considering the small quantity of real images and professional annotations that are used. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> used a CycleGAN to translate colonoscopy images to polyp masks. In their work, the generator learns how to segment polyps by trying to fool a discriminator.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Real polyp segmentation datasets size and resolution.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">#Images</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Resolution</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">CVC-T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">912</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">574 x 500</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left">CVC-ClinicDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_left">612</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_left">384 x 288</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left">CVC-ColonDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_left">380</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_left">574 x 500</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left">ETIS-LaribPolypDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_left">196</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_left">1225 x 966</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">Kvasir <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb">1000</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_bb">Variable</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Synthetic images combined with generative networks have also been widely used in the depth prediction task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This task helps doctors to verify that all the surfaces in the colon have been analyzed. Synthetic data is essential for this task because of the difficulties to obtain depth information in a real colonoscopy.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Unlike previous works, our method is entirely unsupervised and does not require any human annotations. We automatically generate the annotations by defining the structure of the colon and polyps and transferring the location of the polyps to a 2D mask. The key difference between our approach and other state-of-the-art is that we combine 3D rendering and generative networks. First, the 3D engine defines the structure of the image and generates the annotations. Second, the adversarial network makes the images realistic.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Similar unsupervised methods have also been successfully applied in other domains like crowd counting. For example, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> render crowd images from a video game and then use a CycleGAN to increase the realism.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our approach is composed of three steps: first, we procedurally generate colon images and annotations using a 3D engine; second, we feed a CycleGAN with images from real colonoscopies and our synthetic images; finally, we use the realistic images created by CycleGAN to train an image segmentation model.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>3D colon generation</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2202.08680/assets/images/section.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="277" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The structure of the colon is composed by 7 segments to simulate the curvature of the intestinal tract.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The 3D colon and polyps are procedurally generated using Blender, a 3D engine that can be automated via scripting.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Our 3D colons structure is a cone composed by 2454 faces. Vertices are randomly displaced following a normal distribution in order to simulate the tissues in the colon. Additionally, the colon structure is modified by displacing 7 segments as in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 3D colon generation ‣ 3 Method ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For the textures we used a base color [0.80, 0.13, 0.18] (RGB). For each sample we shift the color to other tones of brown, orange and pink. One single polyp is used on every image, which is placed inside the colon. It can be either in the colon’s walls or in the middle. Polyps are distorted spheres with 16384 faces. Samples with polyps occupying less than 20,000 pixels are removed.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Lighting is composed by a white ambient light, two white dynamic lights that project glare into the walls, and three negative lights that project black light at the end of the colon. We found that having a dark area at the end helps CycleGAN to understand the structure of the colon. The 3D scene must be similar to real colon images because otherwise, the CycleGAN will not translate properly the images to the real-world domain. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 3D colon generation ‣ 3 Method ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the images and ground truth generated by the 3D engine.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/synth_1.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/synth_1_mask.png" id="S3.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/synth_2.png" id="S3.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2202.08680/assets/images/synth_2_mask.png" id="S3.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="97" height="97" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Synthetic colons with corresponding annotations rendered using a 3D engine.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>CycleGAN</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A standard CycleGAN composed by two generators and two discriminators is trained using real images from colonoscopies and synthetic images generated using the 3D engine as depicted in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 CycleGAN ‣ 3 Method ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We train a CycleGAN for 200 epochs and then we infer real images in the “Generator Synth to Real” model, producing realistic colon images.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 CycleGAN ‣ 3 Method ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays synthetic images before and after the CycleGAN domain adaptation. Note that the position of the polyps is not altered. Hence, the ground truth information generated by the 3D engine is preserved.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_shadow_1.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_shadow_2.png" id="S3.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_shadow_3.png" id="S3.F5.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_real_1.png" id="S3.F5.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_real_2.png" id="S3.F5.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2202.08680/assets/images/synth_real_3.png" id="S3.F5.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="84" height="84" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Synthetic images (first row) and realistic images generated by our CycleGAN (second row).</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2202.08680/assets/images/cyclegan_synth_1.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="457" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Our CycleGAN architecture. We train two generator models that try to fool two discriminator models by changing the domain of the images.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Polyp segmentation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After creating a synthetic dataset that has been adapted to the real colon textures, we train an image segmentation model.
We used the HarDNeT-MSEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> model architecture because of its real-time performance and high accuracy. We use the same hyperparameter configuration as in the original paper.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Synth-Colon</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We publicly release Synth-Colon, a synthetic dataset for polyp segmentation. It is the first dataset generated using zero annotations from medical professionals. The dataset is composed of <span id="S4.p1.1.1" class="ltx_text ltx_number">20 000</span> images with a resolution of 500<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation></semantics></math>500. Synth-Colon additionally includes realistic colon images generated with our CycleGAN and the Kvasir training set images.
Synth-Colon can also be used for the colon depth estimation task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> because we provide depth and 3D information for each image. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows some examples from the dataset.
In summary, Synth-Colon includes:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Synthetic images of the colon and one polyp.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Masks indicating the location of the polyp.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Realistic images of the colon and polyps. Generated using CycleGAN and the Kvasir dataset.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Depth images of the colon and polyp.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">3D meshes of the colon and polyp in OBJ format.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Metrics</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.4" class="ltx_p">We use two common metrics for evaluation. The mean Dice score, given by:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.1" class="ltx_Math" alttext="\mathrm{mDice}=\frac{2\times tp}{2\times tp+fp+fn}," display="block"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mi id="S5.E1.m1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.2.cmml">mDice</mi><mo id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S5.E1.m1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.3.cmml"><mrow id="S5.E1.m1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.3.2.cmml"><mrow id="S5.E1.m1.1.1.1.1.3.2.2" xref="S5.E1.m1.1.1.1.1.3.2.2.cmml"><mn id="S5.E1.m1.1.1.1.1.3.2.2.2" xref="S5.E1.m1.1.1.1.1.3.2.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S5.E1.m1.1.1.1.1.3.2.2.1" xref="S5.E1.m1.1.1.1.1.3.2.2.1.cmml">×</mo><mi id="S5.E1.m1.1.1.1.1.3.2.2.3" xref="S5.E1.m1.1.1.1.1.3.2.2.3.cmml">t</mi></mrow><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.3.2.1" xref="S5.E1.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3" xref="S5.E1.m1.1.1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="S5.E1.m1.1.1.1.1.3.3" xref="S5.E1.m1.1.1.1.1.3.3.cmml"><mrow id="S5.E1.m1.1.1.1.1.3.3.2" xref="S5.E1.m1.1.1.1.1.3.3.2.cmml"><mrow id="S5.E1.m1.1.1.1.1.3.3.2.2" xref="S5.E1.m1.1.1.1.1.3.3.2.2.cmml"><mn id="S5.E1.m1.1.1.1.1.3.3.2.2.2" xref="S5.E1.m1.1.1.1.1.3.3.2.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S5.E1.m1.1.1.1.1.3.3.2.2.1" xref="S5.E1.m1.1.1.1.1.3.3.2.2.1.cmml">×</mo><mi id="S5.E1.m1.1.1.1.1.3.3.2.2.3" xref="S5.E1.m1.1.1.1.1.3.3.2.2.3.cmml">t</mi></mrow><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.3.3.2.1" xref="S5.E1.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.3.3.2.3" xref="S5.E1.m1.1.1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="S5.E1.m1.1.1.1.1.3.3.1" xref="S5.E1.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S5.E1.m1.1.1.1.1.3.3.3" xref="S5.E1.m1.1.1.1.1.3.3.3.cmml"><mi id="S5.E1.m1.1.1.1.1.3.3.3.2" xref="S5.E1.m1.1.1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.3.3.3.1" xref="S5.E1.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.3.3.3.3" xref="S5.E1.m1.1.1.1.1.3.3.3.3.cmml">p</mi></mrow><mo id="S5.E1.m1.1.1.1.1.3.3.1a" xref="S5.E1.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S5.E1.m1.1.1.1.1.3.3.4" xref="S5.E1.m1.1.1.1.1.3.3.4.cmml"><mi id="S5.E1.m1.1.1.1.1.3.3.4.2" xref="S5.E1.m1.1.1.1.1.3.3.4.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.3.3.4.1" xref="S5.E1.m1.1.1.1.1.3.3.4.1.cmml">​</mo><mi id="S5.E1.m1.1.1.1.1.3.3.4.3" xref="S5.E1.m1.1.1.1.1.3.3.4.3.cmml">n</mi></mrow></mrow></mfrac></mrow><mo id="S5.E1.m1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1"><eq id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"></eq><ci id="S5.E1.m1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.2">mDice</ci><apply id="S5.E1.m1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.3"><divide id="S5.E1.m1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3"></divide><apply id="S5.E1.m1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2"><times id="S5.E1.m1.1.1.1.1.3.2.1.cmml" xref="S5.E1.m1.1.1.1.1.3.2.1"></times><apply id="S5.E1.m1.1.1.1.1.3.2.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2.2"><times id="S5.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S5.E1.m1.1.1.1.1.3.2.2.1"></times><cn type="integer" id="S5.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2.2.2">2</cn><ci id="S5.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S5.E1.m1.1.1.1.1.3.2.2.3">𝑡</ci></apply><ci id="S5.E1.m1.1.1.1.1.3.2.3.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3">𝑝</ci></apply><apply id="S5.E1.m1.1.1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3"><plus id="S5.E1.m1.1.1.1.1.3.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.1"></plus><apply id="S5.E1.m1.1.1.1.1.3.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2"><times id="S5.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.1"></times><apply id="S5.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.2"><times id="S5.E1.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.2.1"></times><cn type="integer" id="S5.E1.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.2.2">2</cn><ci id="S5.E1.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.2.3">𝑡</ci></apply><ci id="S5.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2.3">𝑝</ci></apply><apply id="S5.E1.m1.1.1.1.1.3.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3"><times id="S5.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.1"></times><ci id="S5.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.2">𝑓</ci><ci id="S5.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.3">𝑝</ci></apply><apply id="S5.E1.m1.1.1.1.1.3.3.4.cmml" xref="S5.E1.m1.1.1.1.1.3.3.4"><times id="S5.E1.m1.1.1.1.1.3.3.4.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.4.1"></times><ci id="S5.E1.m1.1.1.1.1.3.3.4.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.4.2">𝑓</ci><ci id="S5.E1.m1.1.1.1.1.3.3.4.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.4.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\mathrm{mDice}=\frac{2\times tp}{2\times tp+fp+fn},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p1.5" class="ltx_p">and the mean intersection over union (IoU):</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="\mathrm{mIoU}=\frac{tp}{tp+fp+fn}," display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1.1.cmml"><mrow id="S5.E2.m1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.2.cmml">mIoU</mi><mo id="S5.E2.m1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S5.E2.m1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.3.cmml"><mrow id="S5.E2.m1.1.1.1.1.3.2" xref="S5.E2.m1.1.1.1.1.3.2.cmml"><mi id="S5.E2.m1.1.1.1.1.3.2.2" xref="S5.E2.m1.1.1.1.1.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.3.2.1" xref="S5.E2.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.3.2.3" xref="S5.E2.m1.1.1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="S5.E2.m1.1.1.1.1.3.3" xref="S5.E2.m1.1.1.1.1.3.3.cmml"><mrow id="S5.E2.m1.1.1.1.1.3.3.2" xref="S5.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S5.E2.m1.1.1.1.1.3.3.2.2" xref="S5.E2.m1.1.1.1.1.3.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.3.3.2.1" xref="S5.E2.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.3.3.2.3" xref="S5.E2.m1.1.1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="S5.E2.m1.1.1.1.1.3.3.1" xref="S5.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.1.1.3.3.3" xref="S5.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S5.E2.m1.1.1.1.1.3.3.3.2" xref="S5.E2.m1.1.1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.3.3.3.1" xref="S5.E2.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.3.3.3.3" xref="S5.E2.m1.1.1.1.1.3.3.3.3.cmml">p</mi></mrow><mo id="S5.E2.m1.1.1.1.1.3.3.1a" xref="S5.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.1.1.3.3.4" xref="S5.E2.m1.1.1.1.1.3.3.4.cmml"><mi id="S5.E2.m1.1.1.1.1.3.3.4.2" xref="S5.E2.m1.1.1.1.1.3.3.4.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.3.3.4.1" xref="S5.E2.m1.1.1.1.1.3.3.4.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.3.3.4.3" xref="S5.E2.m1.1.1.1.1.3.3.4.3.cmml">n</mi></mrow></mrow></mfrac></mrow><mo id="S5.E2.m1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"><eq id="S5.E2.m1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1"></eq><ci id="S5.E2.m1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.2">mIoU</ci><apply id="S5.E2.m1.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.3"><divide id="S5.E2.m1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.1.3"></divide><apply id="S5.E2.m1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.1.3.2"><times id="S5.E2.m1.1.1.1.1.3.2.1.cmml" xref="S5.E2.m1.1.1.1.1.3.2.1"></times><ci id="S5.E2.m1.1.1.1.1.3.2.2.cmml" xref="S5.E2.m1.1.1.1.1.3.2.2">𝑡</ci><ci id="S5.E2.m1.1.1.1.1.3.2.3.cmml" xref="S5.E2.m1.1.1.1.1.3.2.3">𝑝</ci></apply><apply id="S5.E2.m1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.1.3.3"><plus id="S5.E2.m1.1.1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.1.1.3.3.1"></plus><apply id="S5.E2.m1.1.1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.1.1.3.3.2"><times id="S5.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S5.E2.m1.1.1.1.1.3.3.2.1"></times><ci id="S5.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S5.E2.m1.1.1.1.1.3.3.2.2">𝑡</ci><ci id="S5.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S5.E2.m1.1.1.1.1.3.3.2.3">𝑝</ci></apply><apply id="S5.E2.m1.1.1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.1.1.3.3.3"><times id="S5.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S5.E2.m1.1.1.1.1.3.3.3.1"></times><ci id="S5.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S5.E2.m1.1.1.1.1.3.3.3.2">𝑓</ci><ci id="S5.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S5.E2.m1.1.1.1.1.3.3.3.3">𝑝</ci></apply><apply id="S5.E2.m1.1.1.1.1.3.3.4.cmml" xref="S5.E2.m1.1.1.1.1.3.3.4"><times id="S5.E2.m1.1.1.1.1.3.3.4.1.cmml" xref="S5.E2.m1.1.1.1.1.3.3.4.1"></times><ci id="S5.E2.m1.1.1.1.1.3.3.4.2.cmml" xref="S5.E2.m1.1.1.1.1.3.3.4.2">𝑓</ci><ci id="S5.E2.m1.1.1.1.1.3.3.4.3.cmml" xref="S5.E2.m1.1.1.1.1.3.3.4.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">\mathrm{mIoU}=\frac{tp}{tp+fp+fn},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p1.3" class="ltx_p">where in both forumlae, <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="tp" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">tp</annotation></semantics></math> is the number of true positives, <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="fp" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><times id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></times><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝑓</ci><ci id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">fp</annotation></semantics></math> the number of false positives, and <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="fn" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><times id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1"></times><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">𝑓</ci><ci id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">fn</annotation></semantics></math> the number of false negatives.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation on real polyp segmentation datasets</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We evaluate our approach on five real polyp segmentation datasets. Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Evaluation on real polyp segmentation datasets ‣ 5 Experiments ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results obtained when training HarDNeT-MSEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> using our synthetic data. Note that our method is not using any annotations.
Results are satisfactory considering the fact that labels have been generated automatically. We found that training the CycleGAN with only the images from the target dataset performs better than training the CycleGAN with all the datasets combined, indicating a domain gap among the real-world datasets.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation of our synthetic approach on real-world datasets. The metrics used are mean Dice similarity index (mDice) and mean Intersection over Union (mIoU).</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:85.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-102.9pt,20.3pt) scale(0.678074697471385,0.678074697471385) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">CVC-T</th>
<th id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">ColonDB</th>
<th id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">ClinicDB</th>
<th id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">ETIS</th>
<th id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2">Kvasir</th>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<th id="S5.T2.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th id="S5.T2.1.1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mDice</th>
<th id="S5.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mIoU</th>
<th id="S5.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mDice</th>
<th id="S5.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mIoU</th>
<th id="S5.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mDice</th>
<th id="S5.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mIoU</th>
<th id="S5.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mDice</th>
<th id="S5.T2.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mIoU</th>
<th id="S5.T2.1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mDice</th>
<th id="S5.T2.1.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">mIoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.3.1" class="ltx_tr">
<th id="S5.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></th>
<td id="S5.T2.1.1.3.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.710</td>
<td id="S5.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.627</td>
<td id="S5.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.512</td>
<td id="S5.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.444</td>
<td id="S5.T2.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.823</td>
<td id="S5.T2.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.755</td>
<td id="S5.T2.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.398</td>
<td id="S5.T2.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.335</td>
<td id="S5.T2.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.818</td>
<td id="S5.T2.1.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.746</td>
</tr>
<tr id="S5.T2.1.1.4.2" class="ltx_tr">
<th id="S5.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">SFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></th>
<td id="S5.T2.1.1.4.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.467</td>
<td id="S5.T2.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.329</td>
<td id="S5.T2.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.469</td>
<td id="S5.T2.1.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.347</td>
<td id="S5.T2.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.700</td>
<td id="S5.T2.1.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.607</td>
<td id="S5.T2.1.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.297</td>
<td id="S5.T2.1.1.4.2.9" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.217</td>
<td id="S5.T2.1.1.4.2.10" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.723</td>
<td id="S5.T2.1.1.4.2.11" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.611</td>
</tr>
<tr id="S5.T2.1.1.5.3" class="ltx_tr">
<th id="S5.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">PraNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></th>
<td id="S5.T2.1.1.5.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.871</td>
<td id="S5.T2.1.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.797</td>
<td id="S5.T2.1.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.709</td>
<td id="S5.T2.1.1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.640</td>
<td id="S5.T2.1.1.5.3.6" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.899</td>
<td id="S5.T2.1.1.5.3.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.849</td>
<td id="S5.T2.1.1.5.3.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.628</td>
<td id="S5.T2.1.1.5.3.9" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.567</td>
<td id="S5.T2.1.1.5.3.10" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.898</td>
<td id="S5.T2.1.1.5.3.11" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.840</td>
</tr>
<tr id="S5.T2.1.1.6.4" class="ltx_tr">
<th id="S5.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">HarDNet-MSEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></th>
<td id="S5.T2.1.1.6.4.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.887</td>
<td id="S5.T2.1.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.821</td>
<td id="S5.T2.1.1.6.4.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.731</td>
<td id="S5.T2.1.1.6.4.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.660</td>
<td id="S5.T2.1.1.6.4.6" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.932</td>
<td id="S5.T2.1.1.6.4.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.882</td>
<td id="S5.T2.1.1.6.4.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.677</td>
<td id="S5.T2.1.1.6.4.9" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.613</td>
<td id="S5.T2.1.1.6.4.10" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.912</td>
<td id="S5.T2.1.1.6.4.11" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.857</td>
</tr>
<tr id="S5.T2.1.1.7.5" class="ltx_tr">
<th id="S5.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">Synth-Colon (ours)</th>
<td id="S5.T2.1.1.7.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.703</td>
<td id="S5.T2.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.635</td>
<td id="S5.T2.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.521</td>
<td id="S5.T2.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.452</td>
<td id="S5.T2.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.551</td>
<td id="S5.T2.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.475</td>
<td id="S5.T2.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.257</td>
<td id="S5.T2.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.214</td>
<td id="S5.T2.1.1.7.5.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.759</td>
<td id="S5.T2.1.1.7.5.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.527</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Study with limited real data</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this section we evaluate how our approach based on synthetic imagery and domain adaptation compares with the fully supervised state-of-the-art HarDNeT-MSEG network when there are fewer training examples available. We train the CycleGAN used in the proposed approach, without ground truth segmentation labels, on progressively larger sets of imagery, and compare this with the supervised method trained on the same amount of labelled imagery. Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Study with limited real data ‣ 5 Experiments ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of the experiment, which demonstrates that synthetic data is extremely useful for domains where annotations are very scarce. While our CycleGAN can produce realistic images with a small sample of only five real images, supervised methods require many images and annotations to achieve good performance. Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Study with limited real data ‣ 5 Experiments ‣ Synthetic data for unsupervised polyp segmentation This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 765140. This publication has emanated from research supported by Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that our unsupervised approach is useful when there are less than 50 real images and annotations. Note that zero images here means there is no domain adaptation via the CycleGAN.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation of the proposed approach on the Kvasir dataset when few real images are available. The performance is measured using the mean Dice metric.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_tt">Synth-Colon (ours)</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_tt">HarDNeT-MSEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<td id="S5.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">0 images</td>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t">0.356</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<td id="S5.T3.1.3.3.1" class="ltx_td ltx_align_left">5 images</td>
<td id="S5.T3.1.3.3.2" class="ltx_td ltx_nopad_l ltx_align_left">0.642</td>
<td id="S5.T3.1.3.3.3" class="ltx_td ltx_nopad_l ltx_align_left">0.361</td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<td id="S5.T3.1.4.4.1" class="ltx_td ltx_align_left">10 images</td>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_nopad_l ltx_align_left">0.681</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_nopad_l ltx_align_left">0.512</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<td id="S5.T3.1.5.5.1" class="ltx_td ltx_align_left">25 images</td>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_nopad_l ltx_align_left">0.721</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_nopad_l ltx_align_left">0.718</td>
</tr>
<tr id="S5.T3.1.6.6" class="ltx_tr">
<td id="S5.T3.1.6.6.1" class="ltx_td ltx_align_left">50 images</td>
<td id="S5.T3.1.6.6.2" class="ltx_td ltx_nopad_l ltx_align_left">0.735</td>
<td id="S5.T3.1.6.6.3" class="ltx_td ltx_nopad_l ltx_align_left">0.781</td>
</tr>
<tr id="S5.T3.1.7.7" class="ltx_tr">
<td id="S5.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_border_bb">900 (all) images</td>
<td id="S5.T3.1.7.7.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb">0.759</td>
<td id="S5.T3.1.7.7.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb">0.912</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We successfully trained a polyp segmentation model without annotations from doctors. We used 3D rendering to generate the structure of the colon and generative adversarial networks to make the images realistic, and demonstrated that it can perform quite reasonably in several datasets, even outperforming some fully supervised methods in some cases.
We hope this study can help aligning synthetic data and medical imaging in future. As future work, we will explore how to include our synthetic annotations in the CycleGAN.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Bernal, J., Sánchez, F.J., Fernández-Esparrach, G., Gil, D.,
Rodríguez, C., Vilariño, F.: Wm-dova maps for accurate polyp
highlighting in colonoscopy: Validation vs. saliency maps from physicians.
Computerized Medical Imaging and Graphics <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">43</span>, 99–111 (2015)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Brandao, P., Mazomenos, E., Ciuti, G., Caliò, R., Bianchi, F., Menciassi,
A., Dario, P., Koulaouzidis, A., Arezzo, A., Stoyanov, D.: Fully
convolutional neural networks for polyp segmentation in colonoscopy. In:
Medical Imaging 2017: Computer-Aided Diagnosis. vol. 10134, p. 101340F.
International Society for Optics and Photonics (2017)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chao, P., Kao, C.Y., Ruan, Y.S., Huang, C.H., Lin, Y.L.: Hardnet: A low memory
traffic network. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3552–3561 (2019)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Fan, D.P., Ji, G.P., Zhou, T., Chen, G., Fu, H., Shen, J., Shao, L.: Pranet:
Parallel reverse attention network for polyp segmentation. In: International
Conference on Medical Image Computing and Computer-Assisted Intervention. pp.
263–273. Springer (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Fang, Y., Chen, C., Yuan, Y., Tong, K.y.: Selective feature aggregation network
with area-boundary constraints for polyp segmentation. In: International
Conference on Medical Image Computing and Computer-Assisted Intervention. pp.
302–310. Springer (2019)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Gao, H., Ogawara, K.: Adaptive data generation and bidirectional mapping for
polyp images. In: 2020 IEEE Applied Imagery Pattern Recognition Workshop
(AIPR). pp. 1–6. IEEE (2020)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Huang, C.H., Wu, H.Y., Lin, Y.L.: Hardnet-mseg: A simple encoder-decoder polyp
segmentation neural network that achieves over 0.9 mean dice and 86 fps
(2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Hwang, S., Oh, J., Tavanapong, W., Wong, J., De Groen, P.C.: Polyp detection in
colonoscopy video using elliptical shape feature. In: 2007 IEEE International
Conference on Image Processing. vol. 2, pp. II–465. IEEE (2007)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jha, D., Smedsrud, P.H., Riegler, M.A., Halvorsen, P., de Lange, T., Johansen,
D., Johansen, H.D.: Kvasir-seg: A segmented polyp dataset. In: International
Conference on Multimedia Modeling. pp. 451–462. Springer (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">521</span>(7553),
436–444 (2015)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Mahmood, F., Chen, R., Durr, N.J.: Unsupervised reverse domain adaptation for
synthetic medical images via adversarial training. IEEE transactions on
medical imaging <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">37</span>(12), 2572–2581 (2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Rau, A., Edwards, P.E., Ahmad, O.F., Riordan, P., Janatka, M., Lovat, L.B.,
Stoyanov, D.: Implicit domain adaptation with conditional generative
adversarial networks for depth prediction in endoscopy. International journal
of computer assisted radiology and surgery <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">14</span>(7), 1167–1176
(2019)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for
biomedical image segmentation. In: International Conference on Medical image
computing and computer-assisted intervention. pp. 234–241. Springer (2015)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rott Shaham, T., Dekel, T., Michaeli, T.: Singan: Learning a generative model
from a single natural image. In: Computer Vision (ICCV), IEEE International
Conference on (2019)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Silva, J., Histace, A., Romain, O., Dray, X., Granado, B.: Toward embedded
detection of polyps in wce images for early diagnosis of colorectal cancer.
International journal of computer assisted radiology and surgery
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">9</span>(2), 283–293 (2014)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tajbakhsh, N., Gurudu, S.R., Liang, J.: Automated polyp detection in
colonoscopy videos using shape and context information. IEEE transactions on
medical imaging <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">35</span>(2), 630–644 (2015)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Thambawita, V., Salehi, P., Sheshkal, S.A., Hicks, S.A., Hammer, H.L., Parasa,
S., de Lange, T., Halvorsen, P., Riegler, M.A.: Singan-seg: Synthetic
training data generation for medical image segmentation. arXiv preprint
arXiv:2107.00471 (2021)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Vázquez, D., Bernal, J., Sánchez, F.J., Fernández-Esparrach, G.,
López, A.M., Romero, A., Drozdzal, M., Courville, A.: A benchmark for
endoluminal scene segmentation of colonoscopy images. Journal of healthcare
engineering <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">2017</span> (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Wang, Q., Gao, J., Lin, W., Yuan, Y.: Learning from synthetic data for crowd
counting in the wild. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 8198–8207 (2019)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wu, Z., Su, L., Huang, Q.: Cascaded partial decoder for fast and accurate
salient object detection. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 3907–3916 (2019)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net
architecture for medical image segmentation. In: Deep learning in medical
image analysis and multimodal learning for clinical decision support, pp.
3–11. Springer (2018)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image
translation using cycle-consistent adversarial networks. In: Computer Vision
(ICCV), 2017 IEEE International Conference on (2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2202.08679" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2202.08680" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2202.08680">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2202.08680" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2202.08681" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 18:24:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
