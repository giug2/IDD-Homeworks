<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2202.00632] Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data</title><meta property="og:description" content="A number of studies have investigated the training of neural networks
with synthetic data for applications in the real world.
The aim of this study is to quantify how much real world data can be
saved when using a mixeâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2202.00632">

<!--Generated on Thu Mar  7 18:32:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Reducing the Amount of Real World Data for Object Detector Training
with Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sven Burdorf<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Karoline Plum<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, and Daniel Hasenklever<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"><sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Sven Burdorf and Daniel Hasenklever are
with dSPACE GmbH, Rathenaustr. 26, 33102 Paderborn, Germany
<span id="id10.10.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sburdorf@dspace.de, dhasenklever@dspace.de</span><sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Karoline Plum is with the Institute of Computer Sciences,
University OsnabrÃ¼ck, 49069 OsnabrÃ¼ck, Germany. Work was done while
being an intern at dSPACE GmbH
<span id="id12.12.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">kplum@uni-osnabrueck.de</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">A number of studies have investigated the training of neural networks
with synthetic data for applications in the real world.
The aim of this study is to quantify how much real world data can be
saved when using a mixed dataset of synthetic and real world data.
By modeling the relationship between the number of training examples
and detection performance by a simple power law, we find that
the need for real world data can be reduced by up to 70% without
sacrificing detection performance.
The training of object detection networks is especially enhanced
by enriching the mixed dataset with classes underrepresented
in the real world dataset.
The results indicate that mixed datasets
with real world data ratios between 5% and 20% reduce the need for real
world data the most without reducing the detection performance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Reliable and robust object detection is one of the key techniques for
autonomous driving.
However, the training of neural networks for object detection requires
a huge amount of labeled data for which collecting and labeling
in the real world is a time-consuming and expensive task.
Especially the collection of edge or corner cases for the dataset is
challenging, because it is virtually impossible to capture
all edge cases imaginable or even a significant amount of it.
A systematic overview of edge cases can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Another problem of real world automotive datasets is
that there is usually a strong
imbalance in the occurrence of classes: <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">car</em> is the dominant class while
vulnerable road users like pedestrians and bicycles occur less frequently.
An additional challenge is the need for anonymization of real world data to
comply with the General Data Protection Regulation (GDPR).
To be compliant with the General Data Protection Regulation (GDPR) real
world data has to be usually anonymized, which adds a
post-processing step to the data pipeline.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data can be easily constructed in ways, that it has
none of the drawbacks mentioned above:
Ground truth data can be generated with pixel level accuracy.
It is even possible to generate these data â€on the flyâ€ without the need to
store the data and thus storage costs can be traded for compute costs
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Even rare and dangerous scenarios can be simulated.
The class imbalance in synthetic datasets can also be circumvented by adding
additional instances of underrepresented classes to the synthetic dataset.
In addition, synthetic data does not suffer from any legal issues related to
the GDPR.
However, the domain gap prevents object detectors trained only
on synthetic data from reaching the same performance as their counterparts trained
on an equal number of real training examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome this problem mainly two different
training strategies are usually applied:
One kind of approaches uses pretraining and fine-tuning strategies to
pretrain a network on synthetic data and later fine-tune on real data.
Another group of approaches is mixed strategies, where the networks are trained
simultaneously on real and synthetic data in a mixed training dataset.
Usually for both types of strategies the training does not start from scratch but from
network backbones trained on real data as e.g. ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
or COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">More specifically, several concrete approaches to combinine synthetic
and real data for training have been applied so far.
For example
Trembaly et al. proposed pretraining on synthetic training examples, which are
generated by a domain randomization approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Objects with
random textures are placed in random backgrounds with additional random
objects as distractors.
Fine-tuning these pretrained networks on real data improved the performance
of the object detection network compared to only training on real data.
However, their study focused on the dominant car class in automotive datasets
and did not deal with vulnerable road users.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As an example for the second group of strategies,
Nowruzi et al. studied the effect of different ratios of real and synthetic
data in the training dataset as well as pretraining and fine-tuning vs. a
mixed strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. They used various synthetic datasets
(Synscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, GTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
Carla <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>)
and real datasets
(BDD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>,
KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, Nuscenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>)
and concluded that pretraining on synthetic data and fine-tuning on real data
provides better results than mixed training.
Although their results showed, that higher ratios of real data in the training
dataset are beneficial, only ratios of real data up to 10% were
investigated.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">However, when choosing generally mixed training sets as basic strategy,
it is still unclear, what the best ratio of real and synthetic
data would be.
Also, the actual criteria, to get the â€bestâ€ ratio can still be defined
in several ways.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">This study focuses on determining how much real data can be saved when
using synthetic data.
Therefore, an evaluation method based on a
simple power law is proposed to quantify
real data saving in mixed real and synthetic datasets.
In addition, the influence of adding synthetic instances of
underrepresented vulnerable road users (<em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">person</em> class) is studied.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Apart from 3D rendering approaches to produce synthetic data,
generative adversarial networks (GANs) are a promising alternative
to generate synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
To investigate the possibility of using GAN images for training neural
networks, an additional synthetic dataset is created from the Synscapes
semantic segmentation by an image-to-image translation GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
This dataset is referred to as â€GANscapesâ€.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">METHOD</span>
</h2>

<figure id="S2.F1" class="ltx_figure">
<div id="S2.F1.1" class="ltx_block ltx_parbox ltx_align_center ltx_align_middle" style="width:397.5pt;">
<img src="/html/2202.00632/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="422" height="321" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example images from the Cityscapes dataset (top row),
the Synscapes dataset (middle row) and our generated GANscapes
dataset from the Synscapes semantic segmentation (last row).</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To evaluate the proposed experiments real and synthetic datasets with
the same labeling specifications for ground truth labels are needed.
Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> was selected as the real world dataset and
the Synscapes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> with <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="N_{\mathrm{syn}}=25000" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><msub id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml"><mi id="S2.p1.1.m1.1.1.2.2" xref="S2.p1.1.m1.1.1.2.2.cmml">N</mi><mi id="S2.p1.1.m1.1.1.2.3" xref="S2.p1.1.m1.1.1.2.3.cmml">syn</mi></msub><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">25000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><eq id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></eq><apply id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.2.1.cmml" xref="S2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.2.cmml" xref="S2.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S2.p1.1.m1.1.1.2.3.cmml" xref="S2.p1.1.m1.1.1.2.3">syn</ci></apply><cn type="integer" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">25000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">N_{\mathrm{syn}}=25000</annotation></semantics></math> examples
was chosen as the synthetic dataset.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In addition to the labeling specifications,
many parameters in Synscapes like camera
pose and field of view are identical to the corresponding Cityscapes
parameters.
Some typical examples of Cityscapes and Synscapes images are shown in Fig.
<a href="#S2.F1" title="Figure 1 â€£ II METHOD â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The Synscapes dataset puts its emphasis on classes usually underrepresented
in autonomous driving datasets like persons.
Fig. <a href="#S2.F2" title="Figure 2 â€£ II METHOD â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows class imbalances of the two used datasets.
While in Cityscapes the car class is the dominant class, in Synscapes the
most frequent class is deliberately shifted towards the person class.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div id="S2.F2.1" class="ltx_block ltx_parbox ltx_align_center ltx_align_middle" style="width:209.6pt;">
<img src="/html/2202.00632/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_square" width="222" height="222" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Average instances per image for different classes in the
Cityscapes and Synscapes/GANscapes dataset.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Since the Synscapes dataset does not distinguish between riders and rideable
objects (bicycles and motorcycles) a preprocessing step was performed on the
Cityscapes dataset where riders were merged with the closest rideable object
to the class bicycle or motorcycle.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The Cityscapes dataset contains 2975 labeled training images, 500 labeled
validation images, and 1525 test images, for which the ground truth labels
are withheld for the official benchmark.
Therefore, the Hamburg sequence with 248 images was used as the new validation
set, which leaves
<math id="S2.p4.1.m1.1" class="ltx_Math" alttext="N_{\mathrm{city}}=2727" display="inline"><semantics id="S2.p4.1.m1.1a"><mrow id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><msub id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml"><mi id="S2.p4.1.m1.1.1.2.2" xref="S2.p4.1.m1.1.1.2.2.cmml">N</mi><mi id="S2.p4.1.m1.1.1.2.3" xref="S2.p4.1.m1.1.1.2.3.cmml">city</mi></msub><mo id="S2.p4.1.m1.1.1.1" xref="S2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml">2727</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><eq id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1.1"></eq><apply id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.2.1.cmml" xref="S2.p4.1.m1.1.1.2">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.2.cmml" xref="S2.p4.1.m1.1.1.2.2">ğ‘</ci><ci id="S2.p4.1.m1.1.1.2.3.cmml" xref="S2.p4.1.m1.1.1.2.3">city</ci></apply><cn type="integer" id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">2727</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">N_{\mathrm{city}}=2727</annotation></semantics></math>
examples in our real world training dataset.
The 500 original validation images were used as the new test set on which
all trained object detection networks were evaluated.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The GANscapes dataset was generated
fom the semantic and instance segmentation images of the
Synscapes dataset with a pix2pixHD network
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
For the experiments the original
implementation<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/NVIDIA/pix2pixHD</span></span></span>
with weights from training on Cityscapes was used.
Since Synscapes does not differentiate between
the rider class and the bicycle or motorcycle class,
the quality of GAN generated bicycles and motorcycles might be negatively
affected.
However, by looking at some generated bicycles and motorcycles (see e.g. last
row in Fig. <a href="#S2.F1" title="Figure 1 â€£ II METHOD â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), the quality is comparable to the
input from Cityscapes semantic and instance segmentation.
To further improve the quality of the generated images, the faces of semantic
segmentation covering the always visible front view of the recording car
are added to Synscapes semantic segmentation.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.11" class="ltx_p">To analyze the detection performance of the trained networks with the
proposed power law, the networks have to be trained on a varying
(ideally logarithmically spaced) number of total training examples
<math id="S2.p6.1.m1.2" class="ltx_Math" alttext="N_{r,i}" display="inline"><semantics id="S2.p6.1.m1.2a"><msub id="S2.p6.1.m1.2.3" xref="S2.p6.1.m1.2.3.cmml"><mi id="S2.p6.1.m1.2.3.2" xref="S2.p6.1.m1.2.3.2.cmml">N</mi><mrow id="S2.p6.1.m1.2.2.2.4" xref="S2.p6.1.m1.2.2.2.3.cmml"><mi id="S2.p6.1.m1.1.1.1.1" xref="S2.p6.1.m1.1.1.1.1.cmml">r</mi><mo id="S2.p6.1.m1.2.2.2.4.1" xref="S2.p6.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.p6.1.m1.2.2.2.2" xref="S2.p6.1.m1.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.2b"><apply id="S2.p6.1.m1.2.3.cmml" xref="S2.p6.1.m1.2.3"><csymbol cd="ambiguous" id="S2.p6.1.m1.2.3.1.cmml" xref="S2.p6.1.m1.2.3">subscript</csymbol><ci id="S2.p6.1.m1.2.3.2.cmml" xref="S2.p6.1.m1.2.3.2">ğ‘</ci><list id="S2.p6.1.m1.2.2.2.3.cmml" xref="S2.p6.1.m1.2.2.2.4"><ci id="S2.p6.1.m1.1.1.1.1.cmml" xref="S2.p6.1.m1.1.1.1.1">ğ‘Ÿ</ci><ci id="S2.p6.1.m1.2.2.2.2.cmml" xref="S2.p6.1.m1.2.2.2.2">ğ‘–</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.2c">N_{r,i}</annotation></semantics></math>, where <math id="S2.p6.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.p6.2.m2.1a"><mi id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><ci id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">r</annotation></semantics></math> denotes the ratio of real world examples
and an index <math id="S2.p6.3.m3.3" class="ltx_Math" alttext="i=1,\ldots,10" display="inline"><semantics id="S2.p6.3.m3.3a"><mrow id="S2.p6.3.m3.3.4" xref="S2.p6.3.m3.3.4.cmml"><mi id="S2.p6.3.m3.3.4.2" xref="S2.p6.3.m3.3.4.2.cmml">i</mi><mo id="S2.p6.3.m3.3.4.1" xref="S2.p6.3.m3.3.4.1.cmml">=</mo><mrow id="S2.p6.3.m3.3.4.3.2" xref="S2.p6.3.m3.3.4.3.1.cmml"><mn id="S2.p6.3.m3.1.1" xref="S2.p6.3.m3.1.1.cmml">1</mn><mo id="S2.p6.3.m3.3.4.3.2.1" xref="S2.p6.3.m3.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.p6.3.m3.2.2" xref="S2.p6.3.m3.2.2.cmml">â€¦</mi><mo id="S2.p6.3.m3.3.4.3.2.2" xref="S2.p6.3.m3.3.4.3.1.cmml">,</mo><mn id="S2.p6.3.m3.3.3" xref="S2.p6.3.m3.3.3.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.3.m3.3b"><apply id="S2.p6.3.m3.3.4.cmml" xref="S2.p6.3.m3.3.4"><eq id="S2.p6.3.m3.3.4.1.cmml" xref="S2.p6.3.m3.3.4.1"></eq><ci id="S2.p6.3.m3.3.4.2.cmml" xref="S2.p6.3.m3.3.4.2">ğ‘–</ci><list id="S2.p6.3.m3.3.4.3.1.cmml" xref="S2.p6.3.m3.3.4.3.2"><cn type="integer" id="S2.p6.3.m3.1.1.cmml" xref="S2.p6.3.m3.1.1">1</cn><ci id="S2.p6.3.m3.2.2.cmml" xref="S2.p6.3.m3.2.2">â€¦</ci><cn type="integer" id="S2.p6.3.m3.3.3.cmml" xref="S2.p6.3.m3.3.3">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m3.3c">i=1,\ldots,10</annotation></semantics></math>.
These training subsets are constructed by randomly drawing
(without replacement) <math id="S2.p6.4.m4.2" class="ltx_Math" alttext="rN_{r,i}" display="inline"><semantics id="S2.p6.4.m4.2a"><mrow id="S2.p6.4.m4.2.3" xref="S2.p6.4.m4.2.3.cmml"><mi id="S2.p6.4.m4.2.3.2" xref="S2.p6.4.m4.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p6.4.m4.2.3.1" xref="S2.p6.4.m4.2.3.1.cmml">â€‹</mo><msub id="S2.p6.4.m4.2.3.3" xref="S2.p6.4.m4.2.3.3.cmml"><mi id="S2.p6.4.m4.2.3.3.2" xref="S2.p6.4.m4.2.3.3.2.cmml">N</mi><mrow id="S2.p6.4.m4.2.2.2.4" xref="S2.p6.4.m4.2.2.2.3.cmml"><mi id="S2.p6.4.m4.1.1.1.1" xref="S2.p6.4.m4.1.1.1.1.cmml">r</mi><mo id="S2.p6.4.m4.2.2.2.4.1" xref="S2.p6.4.m4.2.2.2.3.cmml">,</mo><mi id="S2.p6.4.m4.2.2.2.2" xref="S2.p6.4.m4.2.2.2.2.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.4.m4.2b"><apply id="S2.p6.4.m4.2.3.cmml" xref="S2.p6.4.m4.2.3"><times id="S2.p6.4.m4.2.3.1.cmml" xref="S2.p6.4.m4.2.3.1"></times><ci id="S2.p6.4.m4.2.3.2.cmml" xref="S2.p6.4.m4.2.3.2">ğ‘Ÿ</ci><apply id="S2.p6.4.m4.2.3.3.cmml" xref="S2.p6.4.m4.2.3.3"><csymbol cd="ambiguous" id="S2.p6.4.m4.2.3.3.1.cmml" xref="S2.p6.4.m4.2.3.3">subscript</csymbol><ci id="S2.p6.4.m4.2.3.3.2.cmml" xref="S2.p6.4.m4.2.3.3.2">ğ‘</ci><list id="S2.p6.4.m4.2.2.2.3.cmml" xref="S2.p6.4.m4.2.2.2.4"><ci id="S2.p6.4.m4.1.1.1.1.cmml" xref="S2.p6.4.m4.1.1.1.1">ğ‘Ÿ</ci><ci id="S2.p6.4.m4.2.2.2.2.cmml" xref="S2.p6.4.m4.2.2.2.2">ğ‘–</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.m4.2c">rN_{r,i}</annotation></semantics></math> examples from the <math id="S2.p6.5.m5.1" class="ltx_Math" alttext="N_{\mathrm{city}}" display="inline"><semantics id="S2.p6.5.m5.1a"><msub id="S2.p6.5.m5.1.1" xref="S2.p6.5.m5.1.1.cmml"><mi id="S2.p6.5.m5.1.1.2" xref="S2.p6.5.m5.1.1.2.cmml">N</mi><mi id="S2.p6.5.m5.1.1.3" xref="S2.p6.5.m5.1.1.3.cmml">city</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.5.m5.1b"><apply id="S2.p6.5.m5.1.1.cmml" xref="S2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p6.5.m5.1.1.1.cmml" xref="S2.p6.5.m5.1.1">subscript</csymbol><ci id="S2.p6.5.m5.1.1.2.cmml" xref="S2.p6.5.m5.1.1.2">ğ‘</ci><ci id="S2.p6.5.m5.1.1.3.cmml" xref="S2.p6.5.m5.1.1.3">city</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.5.m5.1c">N_{\mathrm{city}}</annotation></semantics></math>
Cityscapes examples and <math id="S2.p6.6.m6.3" class="ltx_Math" alttext="(1-r)N_{r,i}" display="inline"><semantics id="S2.p6.6.m6.3a"><mrow id="S2.p6.6.m6.3.3" xref="S2.p6.6.m6.3.3.cmml"><mrow id="S2.p6.6.m6.3.3.1.1" xref="S2.p6.6.m6.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.p6.6.m6.3.3.1.1.2" xref="S2.p6.6.m6.3.3.1.1.1.cmml">(</mo><mrow id="S2.p6.6.m6.3.3.1.1.1" xref="S2.p6.6.m6.3.3.1.1.1.cmml"><mn id="S2.p6.6.m6.3.3.1.1.1.2" xref="S2.p6.6.m6.3.3.1.1.1.2.cmml">1</mn><mo id="S2.p6.6.m6.3.3.1.1.1.1" xref="S2.p6.6.m6.3.3.1.1.1.1.cmml">âˆ’</mo><mi id="S2.p6.6.m6.3.3.1.1.1.3" xref="S2.p6.6.m6.3.3.1.1.1.3.cmml">r</mi></mrow><mo stretchy="false" id="S2.p6.6.m6.3.3.1.1.3" xref="S2.p6.6.m6.3.3.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.p6.6.m6.3.3.2" xref="S2.p6.6.m6.3.3.2.cmml">â€‹</mo><msub id="S2.p6.6.m6.3.3.3" xref="S2.p6.6.m6.3.3.3.cmml"><mi id="S2.p6.6.m6.3.3.3.2" xref="S2.p6.6.m6.3.3.3.2.cmml">N</mi><mrow id="S2.p6.6.m6.2.2.2.4" xref="S2.p6.6.m6.2.2.2.3.cmml"><mi id="S2.p6.6.m6.1.1.1.1" xref="S2.p6.6.m6.1.1.1.1.cmml">r</mi><mo id="S2.p6.6.m6.2.2.2.4.1" xref="S2.p6.6.m6.2.2.2.3.cmml">,</mo><mi id="S2.p6.6.m6.2.2.2.2" xref="S2.p6.6.m6.2.2.2.2.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.6.m6.3b"><apply id="S2.p6.6.m6.3.3.cmml" xref="S2.p6.6.m6.3.3"><times id="S2.p6.6.m6.3.3.2.cmml" xref="S2.p6.6.m6.3.3.2"></times><apply id="S2.p6.6.m6.3.3.1.1.1.cmml" xref="S2.p6.6.m6.3.3.1.1"><minus id="S2.p6.6.m6.3.3.1.1.1.1.cmml" xref="S2.p6.6.m6.3.3.1.1.1.1"></minus><cn type="integer" id="S2.p6.6.m6.3.3.1.1.1.2.cmml" xref="S2.p6.6.m6.3.3.1.1.1.2">1</cn><ci id="S2.p6.6.m6.3.3.1.1.1.3.cmml" xref="S2.p6.6.m6.3.3.1.1.1.3">ğ‘Ÿ</ci></apply><apply id="S2.p6.6.m6.3.3.3.cmml" xref="S2.p6.6.m6.3.3.3"><csymbol cd="ambiguous" id="S2.p6.6.m6.3.3.3.1.cmml" xref="S2.p6.6.m6.3.3.3">subscript</csymbol><ci id="S2.p6.6.m6.3.3.3.2.cmml" xref="S2.p6.6.m6.3.3.3.2">ğ‘</ci><list id="S2.p6.6.m6.2.2.2.3.cmml" xref="S2.p6.6.m6.2.2.2.4"><ci id="S2.p6.6.m6.1.1.1.1.cmml" xref="S2.p6.6.m6.1.1.1.1">ğ‘Ÿ</ci><ci id="S2.p6.6.m6.2.2.2.2.cmml" xref="S2.p6.6.m6.2.2.2.2">ğ‘–</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.6.m6.3c">(1-r)N_{r,i}</annotation></semantics></math> examples from the
<math id="S2.p6.7.m7.1" class="ltx_Math" alttext="N_{\mathrm{syn}}" display="inline"><semantics id="S2.p6.7.m7.1a"><msub id="S2.p6.7.m7.1.1" xref="S2.p6.7.m7.1.1.cmml"><mi id="S2.p6.7.m7.1.1.2" xref="S2.p6.7.m7.1.1.2.cmml">N</mi><mi id="S2.p6.7.m7.1.1.3" xref="S2.p6.7.m7.1.1.3.cmml">syn</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.7.m7.1b"><apply id="S2.p6.7.m7.1.1.cmml" xref="S2.p6.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p6.7.m7.1.1.1.cmml" xref="S2.p6.7.m7.1.1">subscript</csymbol><ci id="S2.p6.7.m7.1.1.2.cmml" xref="S2.p6.7.m7.1.1.2">ğ‘</ci><ci id="S2.p6.7.m7.1.1.3.cmml" xref="S2.p6.7.m7.1.1.3">syn</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.7.m7.1c">N_{\mathrm{syn}}</annotation></semantics></math> synthetic examples.
For example, a training subset with
<math id="S2.p6.8.m8.2" class="ltx_Math" alttext="N_{20\%,1}=272" display="inline"><semantics id="S2.p6.8.m8.2a"><mrow id="S2.p6.8.m8.2.3" xref="S2.p6.8.m8.2.3.cmml"><msub id="S2.p6.8.m8.2.3.2" xref="S2.p6.8.m8.2.3.2.cmml"><mi id="S2.p6.8.m8.2.3.2.2" xref="S2.p6.8.m8.2.3.2.2.cmml">N</mi><mrow id="S2.p6.8.m8.2.2.2.2" xref="S2.p6.8.m8.2.2.2.3.cmml"><mrow id="S2.p6.8.m8.2.2.2.2.1" xref="S2.p6.8.m8.2.2.2.2.1.cmml"><mn id="S2.p6.8.m8.2.2.2.2.1.2" xref="S2.p6.8.m8.2.2.2.2.1.2.cmml">20</mn><mo id="S2.p6.8.m8.2.2.2.2.1.1" xref="S2.p6.8.m8.2.2.2.2.1.1.cmml">%</mo></mrow><mo id="S2.p6.8.m8.2.2.2.2.2" xref="S2.p6.8.m8.2.2.2.3.cmml">,</mo><mn id="S2.p6.8.m8.1.1.1.1" xref="S2.p6.8.m8.1.1.1.1.cmml">1</mn></mrow></msub><mo id="S2.p6.8.m8.2.3.1" xref="S2.p6.8.m8.2.3.1.cmml">=</mo><mn id="S2.p6.8.m8.2.3.3" xref="S2.p6.8.m8.2.3.3.cmml">272</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.8.m8.2b"><apply id="S2.p6.8.m8.2.3.cmml" xref="S2.p6.8.m8.2.3"><eq id="S2.p6.8.m8.2.3.1.cmml" xref="S2.p6.8.m8.2.3.1"></eq><apply id="S2.p6.8.m8.2.3.2.cmml" xref="S2.p6.8.m8.2.3.2"><csymbol cd="ambiguous" id="S2.p6.8.m8.2.3.2.1.cmml" xref="S2.p6.8.m8.2.3.2">subscript</csymbol><ci id="S2.p6.8.m8.2.3.2.2.cmml" xref="S2.p6.8.m8.2.3.2.2">ğ‘</ci><list id="S2.p6.8.m8.2.2.2.3.cmml" xref="S2.p6.8.m8.2.2.2.2"><apply id="S2.p6.8.m8.2.2.2.2.1.cmml" xref="S2.p6.8.m8.2.2.2.2.1"><csymbol cd="latexml" id="S2.p6.8.m8.2.2.2.2.1.1.cmml" xref="S2.p6.8.m8.2.2.2.2.1.1">percent</csymbol><cn type="integer" id="S2.p6.8.m8.2.2.2.2.1.2.cmml" xref="S2.p6.8.m8.2.2.2.2.1.2">20</cn></apply><cn type="integer" id="S2.p6.8.m8.1.1.1.1.cmml" xref="S2.p6.8.m8.1.1.1.1">1</cn></list></apply><cn type="integer" id="S2.p6.8.m8.2.3.3.cmml" xref="S2.p6.8.m8.2.3.3">272</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.8.m8.2c">N_{20\%,1}=272</annotation></semantics></math> examples consists of 54 examples randomly selected
from the <math id="S2.p6.9.m9.1" class="ltx_Math" alttext="N_{\mathrm{city}}" display="inline"><semantics id="S2.p6.9.m9.1a"><msub id="S2.p6.9.m9.1.1" xref="S2.p6.9.m9.1.1.cmml"><mi id="S2.p6.9.m9.1.1.2" xref="S2.p6.9.m9.1.1.2.cmml">N</mi><mi id="S2.p6.9.m9.1.1.3" xref="S2.p6.9.m9.1.1.3.cmml">city</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.9.m9.1b"><apply id="S2.p6.9.m9.1.1.cmml" xref="S2.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p6.9.m9.1.1.1.cmml" xref="S2.p6.9.m9.1.1">subscript</csymbol><ci id="S2.p6.9.m9.1.1.2.cmml" xref="S2.p6.9.m9.1.1.2">ğ‘</ci><ci id="S2.p6.9.m9.1.1.3.cmml" xref="S2.p6.9.m9.1.1.3">city</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.9.m9.1c">N_{\mathrm{city}}</annotation></semantics></math> examples.
The remaining 218 examples are either selected from the <math id="S2.p6.10.m10.1" class="ltx_Math" alttext="N_{\mathrm{syn}}" display="inline"><semantics id="S2.p6.10.m10.1a"><msub id="S2.p6.10.m10.1.1" xref="S2.p6.10.m10.1.1.cmml"><mi id="S2.p6.10.m10.1.1.2" xref="S2.p6.10.m10.1.1.2.cmml">N</mi><mi id="S2.p6.10.m10.1.1.3" xref="S2.p6.10.m10.1.1.3.cmml">syn</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.10.m10.1b"><apply id="S2.p6.10.m10.1.1.cmml" xref="S2.p6.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p6.10.m10.1.1.1.cmml" xref="S2.p6.10.m10.1.1">subscript</csymbol><ci id="S2.p6.10.m10.1.1.2.cmml" xref="S2.p6.10.m10.1.1.2">ğ‘</ci><ci id="S2.p6.10.m10.1.1.3.cmml" xref="S2.p6.10.m10.1.1.3">syn</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.10.m10.1c">N_{\mathrm{syn}}</annotation></semantics></math>
Synscapes examples, when training on a mixed Cityscapes+Synscapes
dataset or selected from <math id="S2.p6.11.m11.1" class="ltx_Math" alttext="N_{\mathrm{syn}}" display="inline"><semantics id="S2.p6.11.m11.1a"><msub id="S2.p6.11.m11.1.1" xref="S2.p6.11.m11.1.1.cmml"><mi id="S2.p6.11.m11.1.1.2" xref="S2.p6.11.m11.1.1.2.cmml">N</mi><mi id="S2.p6.11.m11.1.1.3" xref="S2.p6.11.m11.1.1.3.cmml">syn</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.11.m11.1b"><apply id="S2.p6.11.m11.1.1.cmml" xref="S2.p6.11.m11.1.1"><csymbol cd="ambiguous" id="S2.p6.11.m11.1.1.1.cmml" xref="S2.p6.11.m11.1.1">subscript</csymbol><ci id="S2.p6.11.m11.1.1.2.cmml" xref="S2.p6.11.m11.1.1.2">ğ‘</ci><ci id="S2.p6.11.m11.1.1.3.cmml" xref="S2.p6.11.m11.1.1.3">syn</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.11.m11.1c">N_{\mathrm{syn}}</annotation></semantics></math> GANscapes examples when
training on a mixed Cityscapes+GANscapes dataset.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">For the object detection network a YOLOv3 architecture<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
was chosen, which was pretrained on the COCO dataset.
The YOLOv3 is trained with a batch size of 16, an initial learning rate of
<math id="S2.p7.1.m1.1" class="ltx_Math" alttext="3\cdot 10^{-4}" display="inline"><semantics id="S2.p7.1.m1.1a"><mrow id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml"><mn id="S2.p7.1.m1.1.1.2" xref="S2.p7.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.p7.1.m1.1.1.1" xref="S2.p7.1.m1.1.1.1.cmml">â‹…</mo><msup id="S2.p7.1.m1.1.1.3" xref="S2.p7.1.m1.1.1.3.cmml"><mn id="S2.p7.1.m1.1.1.3.2" xref="S2.p7.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.p7.1.m1.1.1.3.3" xref="S2.p7.1.m1.1.1.3.3.cmml"><mo id="S2.p7.1.m1.1.1.3.3a" xref="S2.p7.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S2.p7.1.m1.1.1.3.3.2" xref="S2.p7.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><apply id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1"><ci id="S2.p7.1.m1.1.1.1.cmml" xref="S2.p7.1.m1.1.1.1">â‹…</ci><cn type="integer" id="S2.p7.1.m1.1.1.2.cmml" xref="S2.p7.1.m1.1.1.2">3</cn><apply id="S2.p7.1.m1.1.1.3.cmml" xref="S2.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p7.1.m1.1.1.3.1.cmml" xref="S2.p7.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.p7.1.m1.1.1.3.2.cmml" xref="S2.p7.1.m1.1.1.3.2">10</cn><apply id="S2.p7.1.m1.1.1.3.3.cmml" xref="S2.p7.1.m1.1.1.3.3"><minus id="S2.p7.1.m1.1.1.3.3.1.cmml" xref="S2.p7.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.p7.1.m1.1.1.3.3.2.cmml" xref="S2.p7.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">3\cdot 10^{-4}</annotation></semantics></math> and a cosine learning rate schedule without restarts
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is employed.
The training data is augmented via random color jitter, random crop, and
random mosaicking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
The network is trained until no improvement on the validation set is
observed for 20 epochs.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">For every number of training examples <math id="S2.p8.1.m1.2" class="ltx_Math" alttext="N_{r,i}" display="inline"><semantics id="S2.p8.1.m1.2a"><msub id="S2.p8.1.m1.2.3" xref="S2.p8.1.m1.2.3.cmml"><mi id="S2.p8.1.m1.2.3.2" xref="S2.p8.1.m1.2.3.2.cmml">N</mi><mrow id="S2.p8.1.m1.2.2.2.4" xref="S2.p8.1.m1.2.2.2.3.cmml"><mi id="S2.p8.1.m1.1.1.1.1" xref="S2.p8.1.m1.1.1.1.1.cmml">r</mi><mo id="S2.p8.1.m1.2.2.2.4.1" xref="S2.p8.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.p8.1.m1.2.2.2.2" xref="S2.p8.1.m1.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p8.1.m1.2b"><apply id="S2.p8.1.m1.2.3.cmml" xref="S2.p8.1.m1.2.3"><csymbol cd="ambiguous" id="S2.p8.1.m1.2.3.1.cmml" xref="S2.p8.1.m1.2.3">subscript</csymbol><ci id="S2.p8.1.m1.2.3.2.cmml" xref="S2.p8.1.m1.2.3.2">ğ‘</ci><list id="S2.p8.1.m1.2.2.2.3.cmml" xref="S2.p8.1.m1.2.2.2.4"><ci id="S2.p8.1.m1.1.1.1.1.cmml" xref="S2.p8.1.m1.1.1.1.1">ğ‘Ÿ</ci><ci id="S2.p8.1.m1.2.2.2.2.cmml" xref="S2.p8.1.m1.2.2.2.2">ğ‘–</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.1.m1.2c">N_{r,i}</annotation></semantics></math>
the training was repeated five times with different initializations
of the YOLOv3 detection head and different random selections of training
examples.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">RESULTS AND DISCUSSION</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Pretraining + fine-tuning vs. mixed training strategy</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">Results in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> indicated that pretraining
a neural network on synthetic data and fine-tuning on real data is a
better training strategy than training on a mixed dataset of
synthetic and real data.
However, no confidence intervals are reported for their findings.
Therefore, a YOLOv3 network pretrained on COCO was on the one hand
pretrained on 25000 synthetic examples and subsequently fine-tuned
on 2727 real examples and on the other hand trained on a mixed dataset
consisting of all synthetic and real examples.
Both training strategies were repeated five times with different
random seeds.
The performance of the trained networks on the test set with 500 real examples
is evaluated in terms of mean average precision (mAP) and
average precision (AP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
The metrics require an intersection over union (IoU) threshold
at which a predicted bounding box is considered a match with
a ground truth bounding box.
A threshold of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{IoU}\geq 50\%" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">â‰¥</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">50</mn><mo id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><geq id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></geq><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">IoU</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathrm{IoU}\geq 50\%</annotation></semantics></math> is chosen throughout all experiments
(mAP<sub id="S3.SS1.p1.5.1" class="ltx_sub"><span id="S3.SS1.p1.5.1.1" class="ltx_text ltx_font_italic">50</span></sub>, AP<sub id="S3.SS1.p1.5.2" class="ltx_sub"><span id="S3.SS1.p1.5.2.1" class="ltx_text ltx_font_italic">50</span></sub>).
Tab. <a href="#S3.T1" title="TABLE I â€£ III-A Pretraining + fine-tuning vs. mixed training strategy â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>
shows the resulting mAP<sub id="S3.SS1.p1.5.3" class="ltx_sub"><span id="S3.SS1.p1.5.3.1" class="ltx_text ltx_font_italic">50</span></sub> and AP<sub id="S3.SS1.p1.5.4" class="ltx_sub"><span id="S3.SS1.p1.5.4.1" class="ltx_text ltx_font_italic">50</span></sub> values
for the <em id="S3.SS1.p1.5.5" class="ltx_emph ltx_font_italic">car</em> and <em id="S3.SS1.p1.5.6" class="ltx_emph ltx_font_italic">person</em> class averaged over five training runs
with the standard deviation.
No significant difference between the pretraining + fine-tuning and the mixed
training strategy was observable.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
mAP<sub id="S3.T1.16.1" class="ltx_sub"><span id="S3.T1.16.1.1" class="ltx_text ltx_font_italic">50</span></sub> and AP<sub id="S3.T1.17.2" class="ltx_sub"><span id="S3.T1.17.2.1" class="ltx_text ltx_font_italic">50</span></sub> for the car and person class for both
considered training strategies with a ratio of 10% real data.</figcaption>
<table id="S3.T1.13.13" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.13.13.10.1" class="ltx_tr">
<td id="S3.T1.13.13.10.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T1.13.13.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.13.13.10.1.2.1" class="ltx_text ltx_font_bold">pretrain + fine-tune</span></th>
<th id="S3.T1.13.13.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.13.13.10.1.3.1" class="ltx_text ltx_font_bold">mixed</span></th>
</tr>
<tr id="S3.T1.7.7.3" class="ltx_tr">
<td id="S3.T1.5.5.1.1" class="ltx_td ltx_align_center ltx_border_t">mAP<sub id="S3.T1.5.5.1.1.1" class="ltx_sub"><span id="S3.T1.5.5.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T1.6.6.2.2" class="ltx_td ltx_align_center ltx_border_t">0.3706 <math id="S3.T1.6.6.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.6.6.2.2.m1.1a"><mo id="S3.T1.6.6.2.2.m1.1.1" xref="S3.T1.6.6.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.2.m1.1b"><csymbol cd="latexml" id="S3.T1.6.6.2.2.m1.1.1.cmml" xref="S3.T1.6.6.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.2.m1.1c">\pm</annotation></semantics></math> 0.0082</td>
<td id="S3.T1.7.7.3.3" class="ltx_td ltx_align_center ltx_border_t">0.3841 <math id="S3.T1.7.7.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.7.7.3.3.m1.1a"><mo id="S3.T1.7.7.3.3.m1.1.1" xref="S3.T1.7.7.3.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.3.3.m1.1b"><csymbol cd="latexml" id="S3.T1.7.7.3.3.m1.1.1.cmml" xref="S3.T1.7.7.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.3.3.m1.1c">\pm</annotation></semantics></math> 0.0132</td>
</tr>
<tr id="S3.T1.10.10.6" class="ltx_tr">
<td id="S3.T1.8.8.4.1" class="ltx_td ltx_align_center">car AP<sub id="S3.T1.8.8.4.1.1" class="ltx_sub"><span id="S3.T1.8.8.4.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T1.9.9.5.2" class="ltx_td ltx_align_center">0.6356 <math id="S3.T1.9.9.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.9.9.5.2.m1.1a"><mo id="S3.T1.9.9.5.2.m1.1.1" xref="S3.T1.9.9.5.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.5.2.m1.1b"><csymbol cd="latexml" id="S3.T1.9.9.5.2.m1.1.1.cmml" xref="S3.T1.9.9.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.5.2.m1.1c">\pm</annotation></semantics></math> 0.0065</td>
<td id="S3.T1.10.10.6.3" class="ltx_td ltx_align_center">0.6346 <math id="S3.T1.10.10.6.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.10.10.6.3.m1.1a"><mo id="S3.T1.10.10.6.3.m1.1.1" xref="S3.T1.10.10.6.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.6.3.m1.1b"><csymbol cd="latexml" id="S3.T1.10.10.6.3.m1.1.1.cmml" xref="S3.T1.10.10.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.6.3.m1.1c">\pm</annotation></semantics></math> 0.0059</td>
</tr>
<tr id="S3.T1.13.13.9" class="ltx_tr">
<td id="S3.T1.11.11.7.1" class="ltx_td ltx_align_center ltx_border_b">person AP<sub id="S3.T1.11.11.7.1.1" class="ltx_sub"><span id="S3.T1.11.11.7.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T1.12.12.8.2" class="ltx_td ltx_align_center ltx_border_b">0.3518 <math id="S3.T1.12.12.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.12.12.8.2.m1.1a"><mo id="S3.T1.12.12.8.2.m1.1.1" xref="S3.T1.12.12.8.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.8.2.m1.1b"><csymbol cd="latexml" id="S3.T1.12.12.8.2.m1.1.1.cmml" xref="S3.T1.12.12.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.8.2.m1.1c">\pm</annotation></semantics></math> 0.0128</td>
<td id="S3.T1.13.13.9.3" class="ltx_td ltx_align_center ltx_border_b">0.3778 <math id="S3.T1.13.13.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T1.13.13.9.3.m1.1a"><mo id="S3.T1.13.13.9.3.m1.1.1" xref="S3.T1.13.13.9.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.9.3.m1.1b"><csymbol cd="latexml" id="S3.T1.13.13.9.3.m1.1.1.cmml" xref="S3.T1.13.13.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.9.3.m1.1c">\pm</annotation></semantics></math> 0.0058</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Reduction of real data amount by adding synthetic data</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As outlined in the introduction, neural networks trained only
on synthetic data generalize poorly to real world data.
Considering Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the previous
statement can be confirmed:
When the mAP<sub id="S3.SS2.p1.1.1" class="ltx_sub"><span id="S3.SS2.p1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub> scores of networks trained on different ratios
of real and synthetic data are compared, networks trained on real data
only (red triangles, real data portion of 100%)
outperform significantly their counterparts trained on synthetic
data only (purple squares, real data portion of 0%).
These results are consistent for the Synscapes dataset
Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>a and the GANscapes dataset
Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>b.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_block ltx_parbox ltx_align_center ltx_align_middle" style="width:397.5pt;">
<img src="/html/2202.00632/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="422" height="241" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance of neural networks in terms of 1-mAP<sub id="S3.F3.5.1" class="ltx_sub"><span id="S3.F3.5.1.1" class="ltx_text ltx_font_italic">50</span></sub> (lower
is better) for various ratios of real data in the mixed dataset
consisting of real data and Synscapes (left side) and
GANscapes (right side). Values are presented as mean values and
standard deviations over five training runs. Dashed colored lines
are regression lines fitted with Eq. (<a href="#S3.E1" title="In III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The dashed horizontal black is an auxiliary line to help the reader
compare detection performance with the mean performance of the networks
trained on all 2727 real examples.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we compare the model performance
for different ratios <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">r</annotation></semantics></math> of real data dependent on the total number of
training examples.
In agreement with results by Hestness et al. the performance improvement
by increasing the number of training examples can be described by a power law
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> of the form</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="1-\mathrm{mAP}_{50}=10^{\beta}N_{r,i}^{\gamma}," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mn id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml">1</mn><mo id="S3.E1.m1.3.3.1.1.2.1" xref="S3.E1.m1.3.3.1.1.2.1.cmml">âˆ’</mo><msub id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.2.3.2.cmml">mAP</mi><mn id="S3.E1.m1.3.3.1.1.2.3.3" xref="S3.E1.m1.3.3.1.1.2.3.3.cmml">50</mn></msub></mrow><mo id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><msup id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml"><mn id="S3.E1.m1.3.3.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.3.2.2.cmml">10</mn><mi id="S3.E1.m1.3.3.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.3.2.3.cmml">Î²</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.1" xref="S3.E1.m1.3.3.1.1.3.1.cmml">â€‹</mo><msubsup id="S3.E1.m1.3.3.1.1.3.3" xref="S3.E1.m1.3.3.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.3.3.2.2" xref="S3.E1.m1.3.3.1.1.3.3.2.2.cmml">N</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">r</mi><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">i</mi></mrow><mi id="S3.E1.m1.3.3.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.3.3.3.cmml">Î³</mi></msubsup></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"></eq><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><minus id="S3.E1.m1.3.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1"></minus><cn type="integer" id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2">1</cn><apply id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.3.2">mAP</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3.3">50</cn></apply></apply><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><times id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1"></times><apply id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.2">superscript</csymbol><cn type="integer" id="S3.E1.m1.3.3.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2.2">10</cn><ci id="S3.E1.m1.3.3.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3">ğ›½</ci></apply><apply id="S3.E1.m1.3.3.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.3">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2.2">ğ‘</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘Ÿ</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘–</ci></list></apply><ci id="S3.E1.m1.3.3.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3.3">ğ›¾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">1-\mathrm{mAP}_{50}=10^{\beta}N_{r,i}^{\gamma},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.9" class="ltx_p">where <math id="S3.SS2.p2.2.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.p2.2.m1.1a"><mi id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b"><ci id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">\beta</annotation></semantics></math> and <math id="S3.SS2.p2.3.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.p2.3.m2.1a"><mi id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b"><ci id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">\gamma</annotation></semantics></math>
are the intercept and the slope of the linear fit in the log-log plot.
It can be seen in Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, that the
slope parameter <math id="S3.SS2.p2.4.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.p2.4.m3.1a"><mi id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b"><ci id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">\gamma</annotation></semantics></math> decreases when ratio <math id="S3.SS2.p2.5.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p2.5.m4.1a"><mi id="S3.SS2.p2.5.m4.1.1" xref="S3.SS2.p2.5.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b"><ci id="S3.SS2.p2.5.m4.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">r</annotation></semantics></math> of real
data is increased, indicating better training results when more real data
is included in the training dataset.
The mAP<sub id="S3.SS2.p2.9.1" class="ltx_sub"><span id="S3.SS2.p2.9.1.1" class="ltx_text ltx_font_italic">50</span></sub> scores of networks trained on mixed datasets surpass
the best mAP<sub id="S3.SS2.p2.9.2" class="ltx_sub"><span id="S3.SS2.p2.9.2.1" class="ltx_text ltx_font_italic">50</span></sub> score for networks trained only on the real data
(<math id="S3.SS2.p2.8.m7.1" class="ltx_Math" alttext="r=100\%" display="inline"><semantics id="S3.SS2.p2.8.m7.1a"><mrow id="S3.SS2.p2.8.m7.1.1" xref="S3.SS2.p2.8.m7.1.1.cmml"><mi id="S3.SS2.p2.8.m7.1.1.2" xref="S3.SS2.p2.8.m7.1.1.2.cmml">r</mi><mo id="S3.SS2.p2.8.m7.1.1.1" xref="S3.SS2.p2.8.m7.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.8.m7.1.1.3" xref="S3.SS2.p2.8.m7.1.1.3.cmml"><mn id="S3.SS2.p2.8.m7.1.1.3.2" xref="S3.SS2.p2.8.m7.1.1.3.2.cmml">100</mn><mo id="S3.SS2.p2.8.m7.1.1.3.1" xref="S3.SS2.p2.8.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m7.1b"><apply id="S3.SS2.p2.8.m7.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1"><eq id="S3.SS2.p2.8.m7.1.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1.1"></eq><ci id="S3.SS2.p2.8.m7.1.1.2.cmml" xref="S3.SS2.p2.8.m7.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.p2.8.m7.1.1.3.cmml" xref="S3.SS2.p2.8.m7.1.1.3"><csymbol cd="latexml" id="S3.SS2.p2.8.m7.1.1.3.1.cmml" xref="S3.SS2.p2.8.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.8.m7.1.1.3.2.cmml" xref="S3.SS2.p2.8.m7.1.1.3.2">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m7.1c">r=100\%</annotation></semantics></math>) with the maximum number of training images
(<math id="S3.SS2.p2.9.m8.2" class="ltx_Math" alttext="N_{100\%,10}=2727" display="inline"><semantics id="S3.SS2.p2.9.m8.2a"><mrow id="S3.SS2.p2.9.m8.2.3" xref="S3.SS2.p2.9.m8.2.3.cmml"><msub id="S3.SS2.p2.9.m8.2.3.2" xref="S3.SS2.p2.9.m8.2.3.2.cmml"><mi id="S3.SS2.p2.9.m8.2.3.2.2" xref="S3.SS2.p2.9.m8.2.3.2.2.cmml">N</mi><mrow id="S3.SS2.p2.9.m8.2.2.2.2" xref="S3.SS2.p2.9.m8.2.2.2.3.cmml"><mrow id="S3.SS2.p2.9.m8.2.2.2.2.1" xref="S3.SS2.p2.9.m8.2.2.2.2.1.cmml"><mn id="S3.SS2.p2.9.m8.2.2.2.2.1.2" xref="S3.SS2.p2.9.m8.2.2.2.2.1.2.cmml">100</mn><mo id="S3.SS2.p2.9.m8.2.2.2.2.1.1" xref="S3.SS2.p2.9.m8.2.2.2.2.1.1.cmml">%</mo></mrow><mo id="S3.SS2.p2.9.m8.2.2.2.2.2" xref="S3.SS2.p2.9.m8.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p2.9.m8.1.1.1.1" xref="S3.SS2.p2.9.m8.1.1.1.1.cmml">10</mn></mrow></msub><mo id="S3.SS2.p2.9.m8.2.3.1" xref="S3.SS2.p2.9.m8.2.3.1.cmml">=</mo><mn id="S3.SS2.p2.9.m8.2.3.3" xref="S3.SS2.p2.9.m8.2.3.3.cmml">2727</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m8.2b"><apply id="S3.SS2.p2.9.m8.2.3.cmml" xref="S3.SS2.p2.9.m8.2.3"><eq id="S3.SS2.p2.9.m8.2.3.1.cmml" xref="S3.SS2.p2.9.m8.2.3.1"></eq><apply id="S3.SS2.p2.9.m8.2.3.2.cmml" xref="S3.SS2.p2.9.m8.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m8.2.3.2.1.cmml" xref="S3.SS2.p2.9.m8.2.3.2">subscript</csymbol><ci id="S3.SS2.p2.9.m8.2.3.2.2.cmml" xref="S3.SS2.p2.9.m8.2.3.2.2">ğ‘</ci><list id="S3.SS2.p2.9.m8.2.2.2.3.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2"><apply id="S3.SS2.p2.9.m8.2.2.2.2.1.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2.1"><csymbol cd="latexml" id="S3.SS2.p2.9.m8.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.9.m8.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2.1.2">100</cn></apply><cn type="integer" id="S3.SS2.p2.9.m8.1.1.1.1.cmml" xref="S3.SS2.p2.9.m8.1.1.1.1">10</cn></list></apply><cn type="integer" id="S3.SS2.p2.9.m8.2.3.3.cmml" xref="S3.SS2.p2.9.m8.2.3.3">2727</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m8.2c">N_{100\%,10}=2727</annotation></semantics></math>).
However, by taking into account the standard deviations, the performance
increase is not significant in most cases.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The amount of training examples needed to reach the best performance
of the networks trained only on real data (intersection of colored dashed
lines with horizontal black dashed line in
Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>a and
Fig. <a href="#S3.F3" title="Figure 3 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>b)
can be easily obtained from Eq. (<a href="#S3.E1" title="In III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The calculated numbers of total training examples and real training
examples are given in Tab. <a href="#S3.T2" title="TABLE II â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
While there is a substantial increase in the total number of required
training examples, the amount of real data examples decreases dramatically.
In the best case (10% real images with Synscapes) only around 30% of
the original real data were needed.
The results from Tab. <a href="#S3.T2" title="TABLE II â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> suggest that the best
ratio of real data in a mixed training dataset is between 5% and 20%.
The GANscapes dataset achieves comparable
results in terms of real data reduction.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div id="S3.F4.1" class="ltx_block ltx_parbox ltx_align_center ltx_align_middle" style="width:397.5pt;">
<img src="/html/2202.00632/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_square" width="422" height="483" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance of neural networks in terms of 1-AP<sub id="S3.F4.5.1" class="ltx_sub"><span id="S3.F4.5.1.1" class="ltx_text ltx_font_italic">50</span></sub> (lower
is better) for various ratios of real data in the mixed dataset
consisting of real data and Synscapes (left side) and
GANscapes (right side). Data is given for the car class (upper row)
and the person class (lower row). Values are presented as mean values and
standard deviations over five training runs. Dashed colored lines
are regression lines fitted with Eq. (<a href="#S3.E1" title="In III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The dashed horizontal black is an auxiliary line to help the reader
to compare the detection performances with the mean performance
of the networks trained on all 2727 real examples.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>
Number of total images (synthetic + real images)
needed to reach the performance of the model trained only on
real data and the number of real images contained
in the mixed dataset.
Numbers are given for training with Synscapes and GANscapes
images.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Synscapes</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">GANscapes</span></th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row">
<span id="S3.T2.1.2.2.1.1" class="ltx_inline-block">
<span id="S3.T2.1.2.2.1.1.1" class="ltx_p"><span id="S3.T2.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Ratio of</span></span>
<span id="S3.T2.1.2.2.1.1.2" class="ltx_p"><span id="S3.T2.1.2.2.1.1.2.1" class="ltx_text ltx_font_bold">real data</span></span>
</span>
</th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T2.1.2.2.2.1" class="ltx_inline-block">
<span id="S3.T2.1.2.2.2.1.1" class="ltx_p"><span id="S3.T2.1.2.2.2.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T2.1.2.2.2.1.2" class="ltx_p"><span id="S3.T2.1.2.2.2.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T2.1.2.2.3.1" class="ltx_inline-block">
<span id="S3.T2.1.2.2.3.1.1" class="ltx_p"><span id="S3.T2.1.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T2.1.2.2.3.1.2" class="ltx_p"><span id="S3.T2.1.2.2.3.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T2.1.2.2.4.1" class="ltx_inline-block">
<span id="S3.T2.1.2.2.4.1.1" class="ltx_p"><span id="S3.T2.1.2.2.4.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T2.1.2.2.4.1.2" class="ltx_p"><span id="S3.T2.1.2.2.4.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T2.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T2.1.2.2.5.1" class="ltx_inline-block">
<span id="S3.T2.1.2.2.5.1.1" class="ltx_p"><span id="S3.T2.1.2.2.5.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T2.1.2.2.5.1.2" class="ltx_p"><span id="S3.T2.1.2.2.5.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<th id="S3.T2.1.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">5%</th>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">18166</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">908</td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">16804</td>
<td id="S3.T2.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">840</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<th id="S3.T2.1.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">10%</th>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_right">7783</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_right">778</td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_align_right">9335</td>
<td id="S3.T2.1.4.2.5" class="ltx_td ltx_align_right">933</td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<th id="S3.T2.1.5.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">20%</th>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_right">5685</td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_right">1137</td>
<td id="S3.T2.1.5.3.4" class="ltx_td ltx_align_right">5217</td>
<td id="S3.T2.1.5.3.5" class="ltx_td ltx_align_right">1043</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<th id="S3.T2.1.6.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">50%</th>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_right">2427</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_right">1214</td>
<td id="S3.T2.1.6.4.4" class="ltx_td ltx_align_right">2783</td>
<td id="S3.T2.1.6.4.5" class="ltx_td ltx_align_right">1392</td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<th id="S3.T2.1.7.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">100%</th>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T2.1.7.5.4" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T2.1.7.5.5" class="ltx_td ltx_align_right ltx_border_b">2727</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Class influence on data reduction</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">The results shown for the mAP<sub id="S3.SS3.p1.3.1" class="ltx_sub"><span id="S3.SS3.p1.3.1.1" class="ltx_text ltx_font_italic">50</span></sub> score were obtained by averaging
over all classes in the dataset, which masks the influence of specific
classes.
It is therefore interesting to analyze the detection performance for
different classes in terms of AP<sub id="S3.SS3.p1.3.2" class="ltx_sub"><span id="S3.SS3.p1.3.2.1" class="ltx_text ltx_font_italic">50</span></sub> score.
As an example the <em id="S3.SS3.p1.3.3" class="ltx_emph ltx_font_italic">car</em> class (most frequent class in real dataset)
and the <em id="S3.SS3.p1.3.4" class="ltx_emph ltx_font_italic">person</em> class (most frequent class in synthetic dataset)
were investigated in more detail.
From Fig. <a href="#S3.F4" title="Figure 4 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a and
Fig. <a href="#S3.F4" title="Figure 4 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b, it can be seen
that adding synthetic examples to the training dataset does not deteriorate
the detection performance of networks evaluated on the <em id="S3.SS3.p1.3.5" class="ltx_emph ltx_font_italic">car</em>
class significantly.
Fig. <a href="#S3.F4" title="Figure 4 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>c and
Fig. <a href="#S3.F4" title="Figure 4 â€£ III-B Reduction of real data amount by adding synthetic data â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>d show the
same for the <em id="S3.SS3.p1.3.6" class="ltx_emph ltx_font_italic">person</em> class.
The networks trained on mixed datasets exceed the detection
performance of the networks trained on real data for the <em id="S3.SS3.p1.3.7" class="ltx_emph ltx_font_italic">person</em> class.
However, the performance increase is not significant except for the
training with <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="r=10\%" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">r</mi><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mn id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">10</mn><mo id="S3.SS3.p1.3.m3.1.1.3.1" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><eq id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></eq><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">r=10\%</annotation></semantics></math> with Synscapes data.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The real data saving results are presented in
Tab. <a href="#S3.T3" title="TABLE III â€£ III-C Class influence on data reduction â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>
for the <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">car</em> class and in
Tab. <a href="#S3.T4" title="TABLE IV â€£ III-C Class influence on data reduction â€£ III RESULTS AND DISCUSSION â€£ Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>
for the <em id="S3.SS3.p2.1.2" class="ltx_emph ltx_font_italic">person</em> class.
A large discrepancy between the <em id="S3.SS3.p2.1.3" class="ltx_emph ltx_font_italic">car</em> and the
<em id="S3.SS3.p2.1.4" class="ltx_emph ltx_font_italic">person</em> class occurs when
the mixed datasets are compared in terms of real world data reduction.
For the <em id="S3.SS3.p2.1.5" class="ltx_emph ltx_font_italic">car</em> class the lowest amount of real data examples
required is still as much as 64%
while for the <em id="S3.SS3.p2.1.6" class="ltx_emph ltx_font_italic">person</em> class only 20% of the real world dataset is
needed (5% real with Synscapes).
This can be expected, because the real world dataset already contains a lot of
<em id="S3.SS3.p2.1.7" class="ltx_emph ltx_font_italic">car</em> instances and hence the benefit by adding synthetic instances
is small.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Since the <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">person</em> class is underrepresented in the real dataset,
the network benefits to a greater extent from the additional synthetic
instances.
Another interesting result is that the person assets, which were
generated in Synscapes with a lot of effort, can be exchanged with GAN
images without compromising too much reduction of real world data
examples in the training dataset.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>
Number of total images (synthetic + real images)
needed to reach the AP<sub id="S3.T3.4.1" class="ltx_sub"><span id="S3.T3.4.1.1" class="ltx_text ltx_font_italic">50</span></sub> of the model trained only on
real data for car class and the number of real images
contained in the mixed dataset.
Numbers are given for training with Synscapes and GANscapes
images.</figcaption>
<table id="S3.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.5.1.1" class="ltx_tr">
<th id="S3.T3.5.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T3.5.1.1.2.1" class="ltx_text ltx_font_bold">Car class</span></th>
</tr>
<tr id="S3.T3.5.2.2" class="ltx_tr">
<th id="S3.T3.5.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T3.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S3.T3.5.2.2.2.1" class="ltx_text ltx_font_bold">Synscapes</span></th>
<th id="S3.T3.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S3.T3.5.2.2.3.1" class="ltx_text ltx_font_bold">GANscapes</span></th>
</tr>
<tr id="S3.T3.5.3.3" class="ltx_tr">
<th id="S3.T3.5.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row">
<span id="S3.T3.5.3.3.1.1" class="ltx_inline-block">
<span id="S3.T3.5.3.3.1.1.1" class="ltx_p"><span id="S3.T3.5.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Ratio of</span></span>
<span id="S3.T3.5.3.3.1.1.2" class="ltx_p"><span id="S3.T3.5.3.3.1.1.2.1" class="ltx_text ltx_font_bold">real data</span></span>
</span>
</th>
<th id="S3.T3.5.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T3.5.3.3.2.1" class="ltx_inline-block">
<span id="S3.T3.5.3.3.2.1.1" class="ltx_p"><span id="S3.T3.5.3.3.2.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T3.5.3.3.2.1.2" class="ltx_p"><span id="S3.T3.5.3.3.2.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T3.5.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T3.5.3.3.3.1" class="ltx_inline-block">
<span id="S3.T3.5.3.3.3.1.1" class="ltx_p"><span id="S3.T3.5.3.3.3.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T3.5.3.3.3.1.2" class="ltx_p"><span id="S3.T3.5.3.3.3.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T3.5.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T3.5.3.3.4.1" class="ltx_inline-block">
<span id="S3.T3.5.3.3.4.1.1" class="ltx_p"><span id="S3.T3.5.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T3.5.3.3.4.1.2" class="ltx_p"><span id="S3.T3.5.3.3.4.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T3.5.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T3.5.3.3.5.1" class="ltx_inline-block">
<span id="S3.T3.5.3.3.5.1.1" class="ltx_p"><span id="S3.T3.5.3.3.5.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T3.5.3.3.5.1.2" class="ltx_p"><span id="S3.T3.5.3.3.5.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.5.4.1" class="ltx_tr">
<th id="S3.T3.5.4.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">5%</th>
<td id="S3.T3.5.4.1.2" class="ltx_td ltx_align_right ltx_border_t">34887</td>
<td id="S3.T3.5.4.1.3" class="ltx_td ltx_align_right ltx_border_t">1744</td>
<td id="S3.T3.5.4.1.4" class="ltx_td ltx_align_right ltx_border_t">40059</td>
<td id="S3.T3.5.4.1.5" class="ltx_td ltx_align_right ltx_border_t">2002</td>
</tr>
<tr id="S3.T3.5.5.2" class="ltx_tr">
<th id="S3.T3.5.5.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">10%</th>
<td id="S3.T3.5.5.2.2" class="ltx_td ltx_align_right">18437</td>
<td id="S3.T3.5.5.2.3" class="ltx_td ltx_align_right">1844</td>
<td id="S3.T3.5.5.2.4" class="ltx_td ltx_align_right">25230</td>
<td id="S3.T3.5.5.2.5" class="ltx_td ltx_align_right">2523</td>
</tr>
<tr id="S3.T3.5.6.3" class="ltx_tr">
<th id="S3.T3.5.6.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">20%</th>
<td id="S3.T3.5.6.3.2" class="ltx_td ltx_align_right">11683</td>
<td id="S3.T3.5.6.3.3" class="ltx_td ltx_align_right">2337</td>
<td id="S3.T3.5.6.3.4" class="ltx_td ltx_align_right">13901</td>
<td id="S3.T3.5.6.3.5" class="ltx_td ltx_align_right">2780</td>
</tr>
<tr id="S3.T3.5.7.4" class="ltx_tr">
<th id="S3.T3.5.7.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">50%</th>
<td id="S3.T3.5.7.4.2" class="ltx_td ltx_align_right">4673</td>
<td id="S3.T3.5.7.4.3" class="ltx_td ltx_align_right">2337</td>
<td id="S3.T3.5.7.4.4" class="ltx_td ltx_align_right">5112</td>
<td id="S3.T3.5.7.4.5" class="ltx_td ltx_align_right">2556</td>
</tr>
<tr id="S3.T3.5.8.5" class="ltx_tr">
<th id="S3.T3.5.8.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">100%</th>
<td id="S3.T3.5.8.5.2" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T3.5.8.5.3" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T3.5.8.5.4" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T3.5.8.5.5" class="ltx_td ltx_align_right ltx_border_b">2727</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>
Number of total images (synthetic + real images)
needed to reach the AP<sub id="S3.T4.4.1" class="ltx_sub"><span id="S3.T4.4.1.1" class="ltx_text ltx_font_italic">50</span></sub> of the model trained only on
real data for person class and the number of real images
contained in the mixed dataset.
Numbers are given for training with Synscapes and GANscapes
images.</figcaption>
<table id="S3.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.5.1.1" class="ltx_tr">
<th id="S3.T4.5.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T4.5.1.1.2.1" class="ltx_text ltx_font_bold">Person class</span></th>
</tr>
<tr id="S3.T4.5.2.2" class="ltx_tr">
<th id="S3.T4.5.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T4.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S3.T4.5.2.2.2.1" class="ltx_text ltx_font_bold">Synscapes</span></th>
<th id="S3.T4.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S3.T4.5.2.2.3.1" class="ltx_text ltx_font_bold">GANscapes</span></th>
</tr>
<tr id="S3.T4.5.3.3" class="ltx_tr">
<th id="S3.T4.5.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row">
<span id="S3.T4.5.3.3.1.1" class="ltx_inline-block">
<span id="S3.T4.5.3.3.1.1.1" class="ltx_p"><span id="S3.T4.5.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Ratio of</span></span>
<span id="S3.T4.5.3.3.1.1.2" class="ltx_p"><span id="S3.T4.5.3.3.1.1.2.1" class="ltx_text ltx_font_bold">real data</span></span>
</span>
</th>
<th id="S3.T4.5.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T4.5.3.3.2.1" class="ltx_inline-block">
<span id="S3.T4.5.3.3.2.1.1" class="ltx_p"><span id="S3.T4.5.3.3.2.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T4.5.3.3.2.1.2" class="ltx_p"><span id="S3.T4.5.3.3.2.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T4.5.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T4.5.3.3.3.1" class="ltx_inline-block">
<span id="S3.T4.5.3.3.3.1.1" class="ltx_p"><span id="S3.T4.5.3.3.3.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T4.5.3.3.3.1.2" class="ltx_p"><span id="S3.T4.5.3.3.3.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T4.5.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T4.5.3.3.4.1" class="ltx_inline-block">
<span id="S3.T4.5.3.3.4.1.1" class="ltx_p"><span id="S3.T4.5.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T4.5.3.3.4.1.2" class="ltx_p"><span id="S3.T4.5.3.3.4.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
<th id="S3.T4.5.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">
<span id="S3.T4.5.3.3.5.1" class="ltx_inline-block">
<span id="S3.T4.5.3.3.5.1.1" class="ltx_p"><span id="S3.T4.5.3.3.5.1.1.1" class="ltx_text ltx_font_bold">Real</span></span>
<span id="S3.T4.5.3.3.5.1.2" class="ltx_p"><span id="S3.T4.5.3.3.5.1.2.1" class="ltx_text ltx_font_bold">images</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.5.4.1" class="ltx_tr">
<th id="S3.T4.5.4.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">5%</th>
<td id="S3.T4.5.4.1.2" class="ltx_td ltx_align_right ltx_border_t">10752</td>
<td id="S3.T4.5.4.1.3" class="ltx_td ltx_align_right ltx_border_t">538</td>
<td id="S3.T4.5.4.1.4" class="ltx_td ltx_align_right ltx_border_t">17324</td>
<td id="S3.T4.5.4.1.5" class="ltx_td ltx_align_right ltx_border_t">866</td>
</tr>
<tr id="S3.T4.5.5.2" class="ltx_tr">
<th id="S3.T4.5.5.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">10%</th>
<td id="S3.T4.5.5.2.2" class="ltx_td ltx_align_right">7775</td>
<td id="S3.T4.5.5.2.3" class="ltx_td ltx_align_right">778</td>
<td id="S3.T4.5.5.2.4" class="ltx_td ltx_align_right">12620</td>
<td id="S3.T4.5.5.2.5" class="ltx_td ltx_align_right">1262</td>
</tr>
<tr id="S3.T4.5.6.3" class="ltx_tr">
<th id="S3.T4.5.6.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">20%</th>
<td id="S3.T4.5.6.3.2" class="ltx_td ltx_align_right">5612</td>
<td id="S3.T4.5.6.3.3" class="ltx_td ltx_align_right">1122</td>
<td id="S3.T4.5.6.3.4" class="ltx_td ltx_align_right">8491</td>
<td id="S3.T4.5.6.3.5" class="ltx_td ltx_align_right">1698</td>
</tr>
<tr id="S3.T4.5.7.4" class="ltx_tr">
<th id="S3.T4.5.7.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">50%</th>
<td id="S3.T4.5.7.4.2" class="ltx_td ltx_align_right">3214</td>
<td id="S3.T4.5.7.4.3" class="ltx_td ltx_align_right">1607</td>
<td id="S3.T4.5.7.4.4" class="ltx_td ltx_align_right">3935</td>
<td id="S3.T4.5.7.4.5" class="ltx_td ltx_align_right">1967</td>
</tr>
<tr id="S3.T4.5.8.5" class="ltx_tr">
<th id="S3.T4.5.8.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">100%</th>
<td id="S3.T4.5.8.5.2" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T4.5.8.5.3" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T4.5.8.5.4" class="ltx_td ltx_align_right ltx_border_b">2727</td>
<td id="S3.T4.5.8.5.5" class="ltx_td ltx_align_right ltx_border_b">2727</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This study extends the research on the use of synthetic data for
neural network training.
In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, no significant difference of the detection
performance was found, when pretraining on synthetic data first
and fine-tuning on real data or when training on a mixed dataset of real and
synthetic examples.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We showed that synthetic data can be used to reduce the need for real
world data in a mixed training dataset.
The proposed method enabled us to estimate that the most reduction of
real examples can be achieved with a ratio of real
examples between 5% and 20% in mixed training datasets.
In addition, we showed that neural networks can particularly benefit
from synthetic data, when the synthetic data is enriched with classes that
are underrepresented the real world dataset.
Since synthetic data can usually be produced in a much more cost-efficient
way, mostly because ground truth labeling comes for free in synthetic
data, this is a promising approach for autonomous driving, where
real world labeled data is still rather expensive.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">While synthetic datasets generated by GANs are not as good as
synthetic datasets produced by classical methods,
they are a promising alternative as the technology is still evolving.
Especially the need for modeling high fidelity 3D assets can be circumvented
by GANs. However, current GANs often need semantic (instance) segmentation
images for their training, which is even more expensive than
bounding box labeling.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Although the results are promising, our study only evaluated one
real dataset and two synthetic datasets for a single object detection
architecture.
Future work should therefore extend our proposed method for the evaluation
of reduction of real data in mixed training datasets to additional real and
synthetic datasets as well as additional object detection architectures.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Breitenstein, J.A. TermÃ¶hlen, D. Lipinski, and T. Fingscheidt,
â€Systematization of Corner Cases for Visual Perception in Automated Drivingâ€,
IEEE Intelligent Vehicles Symposium (IV), Las Vegas, 2020, pp. 986

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. Mason, S. Vejdan, and S. Grijalva,
â€An â€On The Flyâ€ Framework for Efficiently Generating Synthetic Big Data Setsâ€
2019 IEEE International Conference on Big Data, Los Angeles, 2019, pp. 3379

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
w. Chen, Z. Yu, Z. Wang, and A. Anandkumar,
â€Automated synthetic-to-real generalizationâ€, International Conference on
Machine Learning, PMLR 2020 pp. 1746 - 1756.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. Chen, Z. Yu, S. De Mello, S. Liu, J.M. Alvarez,
Z. Wang, A. Anandkumar,
â€Contrastive Syn-to-Real Generalizationâ€,
International Conference on Learning Representations, online, 2021

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li, and L. Fei-Fei,
â€ImageNet: A large-scale hierarchical image databaseâ€,
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Miami, 2009

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollar, and C.L. Zitnick,
â€Microsoft COCO: Common Objects in Contextâ€,
European Conference on Computer Vision (ECCV), Zurich, 2014, pp. 740

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil,
T. To, E. Cameracci, S. Boochoon, and S. Birchfield, â€Training
deep networks with synthetic data: Bridging the reality gap
by domain randomization,â€ in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
F.E. Nowruzi, P. Kapoor, D. Kolhatkar, F.A. Hassanat, R. Laganiere,
and J. Rebut, â€How much real data do we actually need: Analyzing object
detection performance using synthetic and real dataâ€,
ICML Workshop on AI for Autonomous Driving, Long Beach, 2019

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Wrenninge and J. Unger, â€Synscapes: A Photorealistic Synthetic Dataset for
Street Scene Parsingâ€, CoRR, abs/1810.08705, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.R. Richter, Z. Hayder, and V. Koltun,
â€Playing for benchmarksâ€,
in Proc. of the IEEE International Conference on Computer Vision (ICCV),
Venice, 2017, pp. 2213.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
â€CARLA: An open urban driving simulatorâ€,
Conference on Robotic Learning, Mountain View, 2017

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
â€BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learningâ€,
https://arxiv.org/abs/1805.04687, 2018

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Cordts, M. Omram, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele, â€The Cityscapes Dataset for Semantic
Urban Scene Understandingâ€, in Proc. of the IEEE Conference for Computer Vision
and Pattern Recognition (CVPR), Las Vegas, 2016, pp. 3213.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, and R. Urtasun,
â€Are we ready for autonomous driving? The KITTI vision benchmark suiteâ€,
in Proc. of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), Providence, 2012, pp. 3354.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A.H. Lang, S. Vora, V.E. Liong, Q. Xu,
A. Krishnan, Y. Pan, G. Baldan, O. Beijbom,
â€nuScenes: A Multimodal Dataset for Autonomous Drivingâ€,
in Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), online, 2020, pp. 11621

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger,
and H. Greenspan,
â€GAN-based synthetic medical image augmentation for increased CNN performance
in liver lesion classificationâ€,
Neurocomputing, 2018, Vol. 321, pp. 321

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. Park, M.Y. Liu, T.C. Wang, and J.Y. Zhu,
â€Semantic Image Synthesis with Spatially-Adaptive Normalizationâ€,
in Proc. of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), Long Beach, 2019, pp. 2337

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro,
â€High-resolution image synthesis and semantic manipulation with conditional
gansâ€, in Proc. of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), Salt Lake City, 2018, pp. 8798.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Redmon, A. Farhadi, â€Yolov3: an incremental improvementâ€,
CoRR, abs/1804.02767, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter, â€SGDR: Stochastic Gradient Descent with Warm
Restartsâ€, in 5th International Conference on Learning Representations (ICLR),
Toulon, 2017

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.Y. Wang, and H.Y. Liao,
â€YOLOv4: Optimal Speed and Accuracy of Object Detectionâ€,
https://arxiv.org/abs/2004.10934, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Everingham, L. Van Gol, K.C. Williams, J. Winn and A. Zisserman,
â€The Pascal Visual Object Classes (VOC) Challengeâ€, International Journal
of Computer Vision, vol. 88, p. 303â€“338, 2010.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Padilla, S.L. Netto and E.A.B. da Silva,
â€A Survey on Performance Metrics for Object-Detection Algorithmsâ€,
in 2020 International Conference on Systems, Signals and Image Processing,
Online, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad,
M.M.A. Patwary, Y. Yang and Y. Zhou,
â€Deep Learning Scaling is Predictable, Empiricallyâ€,
CoRR, abs/1712.00409, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2202.00631" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2202.00632" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2202.00632">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2202.00632" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2202.00633" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 18:32:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
