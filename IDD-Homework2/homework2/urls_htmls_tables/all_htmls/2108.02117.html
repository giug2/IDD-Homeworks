<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2108.02117] FedJAX: Federated learning simulation with JAX</title><meta property="og:description" content="Federated learning is a machine learning technique that enables training across decentralized data. Recently, federated learning has become an active area of research due to an increased focus on privacy and security. …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedJAX: Federated learning simulation with JAX">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedJAX: Federated learning simulation with JAX">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2108.02117">

<!--Generated on Sat Mar  9 01:58:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_smallcaps">FedJAX</span>: Federated learning simulation with JAX</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jae Hun Ro 
<br class="ltx_break">Google Research 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">jaero@google.com</span> 
<br class="ltx_break">&amp;Ananda Theertha Suresh 
<br class="ltx_break">Google Research 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_typewriter">theertha@google.com</span> 
<br class="ltx_break">&amp;Ke Wu 
<br class="ltx_break">Google Research 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_text ltx_font_typewriter">wuke@google.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Federated learning is a machine learning technique that enables training across decentralized data. Recently, federated learning has become an active area of research due to an increased focus on privacy and security. In light of this, a variety of open source federated learning libraries have been developed and released.
We introduce <span id="id5.id1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>, a JAX-based open source library for federated learning simulations that emphasizes ease-of-use in research. With its simple primitives for implementing federated learning algorithms, prepackaged datasets, models and algorithms, and fast simulation speed, <span id="id5.id1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> aims to make developing and evaluating federated algorithms faster and easier for researchers. Our benchmark results show that <span id="id5.id1.3" class="ltx_text ltx_font_smallcaps">FedJAX</span> can be used to train models with federated averaging on the EMNIST dataset in a few minutes and the Stack Overflow dataset in roughly an hour with standard hyperparameters using TPUs.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated learning is a machine learning setting where many clients collaboratively train a model under the orchestration of a central server, while keeping the training data decentralized. Clients can be either mobile devices or whole organizations depending on the task at hand <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al., <a href="#bib.bib24" title="" class="ltx_ref">2016b</a>, <a href="#bib.bib23" title="" class="ltx_ref">a</a>; McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>; Yang et al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>. Federated learning is typically studied in two scenarios: <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">cross-silo</em> and <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">cross-device</em>. In cross-silo federated learning, the number of clients is small, where as in cross-device, the number of clients is very large and can be in the order of millions. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> highlights key characteristics of most federated learning algorithms in the cross-device settings.
Typically, federated learning algorithms first initialize the model at the server and then complete three key steps for each round of training:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The server selects a subset of clients to participate in training and sends the model to these clients.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Each selected client completes some steps of training on their local data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">After training, the clients send their updated models to the server and the server aggregates them together.</p>
</div>
</li>
</ol>
<p id="S1.p1.2" class="ltx_p">For example, Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the popular <em id="S1.p1.2.1" class="ltx_emph ltx_font_italic">federated averaging</em> algorithm <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, which follows the above three steps. Federated learning has demonstrated usefulness in a variety of contexts, including next word prediction <cite class="ltx_cite ltx_citemacro_citep">(Hard et al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Yang et al., <a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite> and healthcare applications <cite class="ltx_cite ltx_citemacro_citep">(Brisimi et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>. We refer to <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib26" title="" class="ltx_ref">2019a</a>; Kairouz et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> for a more detailed survey of federated learning.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Federated learning poses several interesting challenges. For example, training typically occurs mostly on small devices, limiting the size of models that can be trained. Furthermore, the devices may have low communication bandwidth and require model updates to be compressed. Data is also distributed in a non i.i.d. fashion across devices which raises several optimization questions. Finally, privacy and security are of utmost importance in federated learning and addressing them requires techniques ranging from differential privacy to cryptography.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Given these challenges, federated learning has become an increasingly active area of research. This includes new learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Mohri et al., <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, optimization algorithms <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>; Yu et al., <a href="#bib.bib43" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2019b</a>; Haddadpour and Mahdavi, <a href="#bib.bib12" title="" class="ltx_ref">2019</a>; Khaled et al., <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Karimireddy et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>; Ro et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, compression algorithms <cite class="ltx_cite ltx_citemacro_citep">(Suresh et al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>; Caldas et al., <a href="#bib.bib8" title="" class="ltx_ref">2018a</a>; Xu et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, differentially private algorithms <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Peterson et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>; Sattler et al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>, cryptography techniques <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al., <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>, and algorithms that incorporate fairness <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Du et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Huang et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>. Motivated by this, there are several libraries for federated learning, including, TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">(TFF, <a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite>, PySyft <cite class="ltx_cite ltx_citemacro_citep">(Ryffel et al., <a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, FedTorch <cite class="ltx_cite ltx_citemacro_citep">(Ludwig et al., <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>, and Flower <cite class="ltx_cite ltx_citemacro_citep">(Ludwig et al., <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Recently, JAX <cite class="ltx_cite ltx_citemacro_citep">(Bradbury et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> was introduced to provide utilities to convert Python functions into Accelerated Linear Algebra (XLA) optimized kernels, where compilation and automatic differentiation can be composed arbitrarily. This enables expressiveness for sophisticated algorithms and efficient performance without leaving Python. Given its ease-of-use, several libraries to support machine learning have been built on top of JAX, including, but not limited to, Flax <cite class="ltx_cite ltx_citemacro_citep">(Heek et al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, Objax <cite class="ltx_cite ltx_citemacro_citep">(Objax Developers, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>, Jraph <cite class="ltx_cite ltx_citemacro_citep">(Godwin* et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, and Haiku <cite class="ltx_cite ltx_citemacro_citep">(Hennigan et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> for neural network architectures and training, Optax <cite class="ltx_cite ltx_citemacro_citep">(Hessel et al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> for optimizers, Chex <cite class="ltx_cite ltx_citemacro_citep">(Budden et al., <a href="#bib.bib6" title="" class="ltx_ref">2020a</a>)</cite> for testing, and RLax <cite class="ltx_cite ltx_citemacro_citep">(Budden et al., <a href="#bib.bib7" title="" class="ltx_ref">2020b</a>)</cite> for reinforcement learning.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We present <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>, a JAX and Python based library for federated learning simulation for research.
<span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> is designed for ease-of-use for research and is not intended to be deployed over distributed devices. Focusing on ease-of-use, the <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">FedJAX</span> API is structured to reduce the amount of new concepts that users have to learn to get started and comes packaged with several standard datasets, models, and algorithms that can be used straight out of the box. Additionally, since it is based on JAX, <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">FedJAX</span> can run on accelerators (GPU and TPU) with minimal additional effort.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">The rest of the paper is organized as follows. In Section <a href="#S2" title="2 System design ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we overview the system design, in Section <a href="#S3" title="3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we demonstrate a sample federated learning algorithm with <span id="S1.p6.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>, and in Section <a href="#S4" title="4 Benchmarks ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we benchmark training with <span id="S1.p6.1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> on two datasets.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2108.02117/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="222" height="77" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example federated learning algorithm with four clients.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System design</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">A typical federated learning experiment consists of a federated dataset, model and optimizer, local client training strategy, and server aggregation strategy, so we structure <span id="S2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> accordingly.
The design was primarily driven by ease-of-use and performance when addressing the challenges uniquely attributed to using JAX for federated learning.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets and models</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated dataset</h4>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">In the context of federated learning, data is decentralized and distributed across clients, with each client having their own local set of examples. We refer to two levels of organization for datasets:</p>
</div>
<div id="S2.SS1.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Federated dataset: A collection of clients, each with their own local dataset and metadata.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Client dataset: The set of local examples for a particular client.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px1.p3.1" class="ltx_p">In its simplest form, federated datasets are just mappings from clients to their local examples.
Specifically, clients have a unique identifier for querying their local dataset, which is essentially treated as a list of examples. Furthermore, these local client datasets are typically small as seen in the EMNIST example in Figure <a href="#S2.F2" title="Figure 2 ‣ Federated dataset ‣ 2.1 Datasets and models ‣ 2 System design ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This is in contrast to standard centralized machine learning which requires iterating over a single dataset in a large number of batches. With this difference in mind, we designed <span id="S2.SS1.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_typewriter">fedjax.FederatedData</span> and <span id="S2.SS1.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_typewriter">fedjax.ClientDataset</span>
to rely mostly on NumPy and Python, making it
easy to use and troubleshoot. Finally, in order to take full advantage of JAX, we also provide several helpful functions for accessing and iterating over federated and client datasets. We refer readers to the tutorial<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://fedjax.readthedocs.io/en/latest/notebooks/dataset_tutorial.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fedjax.readthedocs.io/en/latest/notebooks/dataset_tutorial.html</a></span></span></span> for an overview of these functionalities.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2108.02117/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="132" height="87" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2108.02117/assets/x3.png" id="S2.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="130" height="88" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>EMNIST dataset characteristics. Left: Histogram of number of clients as a function of number of samples. Right: Histogram of number of clients as a function of the size of the final batch, with batch size <math id="S2.F2.2.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.F2.2.m1.1b"><mn id="S2.F2.2.m1.1.1" xref="S2.F2.2.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.F2.2.m1.1c"><cn type="integer" id="S2.F2.2.m1.1.1.cmml" xref="S2.F2.2.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.m1.1d">10</annotation></semantics></math>.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure">
<table id="S2.F3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.F3.3.3" class="ltx_tr">
<td id="S2.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2108.02117/assets/x4.png" id="S2.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="87" height="47" alt="Refer to caption"></td>
<td id="S2.F3.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2108.02117/assets/x5.png" id="S2.F3.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="88" height="50" alt="Refer to caption"></td>
<td id="S2.F3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2108.02117/assets/x6.png" id="S2.F3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="88" height="51" alt="Refer to caption"></td>
</tr>
<tr id="S2.F3.3.4.1" class="ltx_tr">
<td id="S2.F3.3.4.1.1" class="ltx_td ltx_align_center">(a) <span id="S2.F3.3.4.1.1.1" class="ltx_text ltx_font_typewriter">batch</span>
</td>
<td id="S2.F3.3.4.1.2" class="ltx_td ltx_align_center">(b) <span id="S2.F3.3.4.1.2.1" class="ltx_text ltx_font_typewriter">padded_batch</span>
</td>
<td id="S2.F3.3.4.1.3" class="ltx_td ltx_align_center">(c) <span id="S2.F3.3.4.1.3.1" class="ltx_text ltx_font_typewriter">shuffle_repeat_batch</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Resulting batches for three clients of various sizes with different batching strategies applied.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">JIT efficient batching</h4>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">Despite the small size of client datasets, there is no guarantee that their sizes will be the same. As a result, when we run batch evaluation on client datasets, the sizes of the final batch can vary greatly depending on the client, as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ Federated dataset ‣ 2.1 Datasets and models ‣ 2 System design ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This poses a practical challenge when using JAX. For best performance, <span id="S2.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">jax.jit<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><a target="_blank" href="https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html" title="" class="ltx_ref ltx_url">https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html</a></span></span></span></span> is used to perform Just In Time (JIT) compilation of a JAX Python function into XLA compiled machine code. <span id="S2.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">jax.jit</span> invokes the XLA compiler for each unique combination of input shapes. Left alone, the large number of possible final batch sizes results in excessive JIT recompilations, significantly slowing execution time. In response to this, we implemented three different batching strategies:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter">batch</span> produces batches in a fixed sequential order without padding the final batch. This is included mostly for illustration purposes.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter">padded_batch</span> produces padded batches in a fixed sequential order for evaluation. By padding the final batch to a small number of fixed sizes, we can set a limit on the maximum number of possible JIT recompilations. This is typically used in evaluation.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter">shuffle_repeat_batch</span> produces batches in a shuffled and repeated order for training where shuffling is done without replacement and batches are always the same size. This is typically used in training.</p>
</div>
</li>
</ul>
<p id="S2.SS1.SSS0.Px2.p1.2" class="ltx_p">Figure <a href="#S2.F3" title="Figure 3 ‣ Federated dataset ‣ 2.1 Datasets and models ‣ 2 System design ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showcases the resulting batches using each of these batching strategies for different client dataset sizes.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model and optimizer</h4>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">The model and optimizer described in this section are unchanged between the standard centralized learning setting and the federated learning setting. There are already numerous JAX based libraries for neural networks and optimizers, such as Flax, Haiku, Objax, and Optax. Thus, for convenience, <span id="S2.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> provides an implementation-agnostic wrapper to make porting existing models and optimizers into <span id="S2.SS1.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> as simple as possible. For example, using an existing Haiku model in <span id="S2.SS1.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">FedJAX</span> is as easy as wrapping the module in a <span id="S2.SS1.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_typewriter">fedjax.Model</span>. We refer readers to the tutorial<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://fedjax.readthedocs.io/en/latest/notebooks/model_tutorial.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fedjax.readthedocs.io/en/latest/notebooks/model_tutorial.html</a></span></span></span> for an overview of these functionalities.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Included datasets and models</h4>

<div id="S2.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px4.p1.1" class="ltx_p">Currently in federated learning research, there are a variety of commonly used datasets and models, such as image recognition, language modeling, and more. A growing set of these datasets and models can be used straight out of the box in <span id="S2.SS1.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>. This not only encourages valid comparisons between various federated algorithms but also accelerates the development of new algorithms since the preprocessed datasets and models are readily available for use and do not have to be written from scratch.</p>
</div>
<div id="S2.SS1.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px4.p2.1" class="ltx_p">At present, <span id="S2.SS1.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> comes packaged with the following datasets and sample models:</p>
</div>
<div id="S2.SS1.SSS0.Px4.p3" class="ltx_para ltx_noindent">
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">EMNIST-62 <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib9" title="" class="ltx_ref">2018b</a>)</cite>, a character recognition task</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">Shakespeare <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, a next character prediction task</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p">Stack Overflow <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, a next word prediction task</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.SSS0.Px4.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px4.p4.1" class="ltx_p"><span id="S2.SS1.SSS0.Px4.p4.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> also provides tools to create new datasets and models that can be used with the rest of the library along with implementations of federated averaging <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite> and other algorithms, such as adaptive federated optimizers <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, agnostic federated averaging <cite class="ltx_cite ltx_citemacro_citep">(Ro et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, and Mime <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>

<div id="S2.SS1.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px5.p1.1" class="ltx_p">When evaluating accuracy on a dataset, we wish to know the proportion of examples that are correctly predicted by the model. In federated learning specifically, we typically want to know the accuracy for each client and over all clients. Thus, we need to divide the work across clients, evaluate each separately, and finally somehow combine the results.</p>
</div>
<div id="S2.SS1.SSS0.Px5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px5.p2.1" class="ltx_p">Assuming the metric evaluation logic is JIT compiled, this faces the same issue of excessive recompilations due to different subset sizes resulting in shape differences. While <span id="S2.SS1.SSS0.Px5.p2.1.1" class="ltx_text ltx_font_typewriter">padded_batch</span> can be used to address this issue, it will also result in batches padded with empty examples which should not be counted in the metric calculation. In an effort to improve ease-of-use while maintaining performance, we designed <span id="S2.SS1.SSS0.Px5.p2.1.2" class="ltx_text ltx_font_typewriter">fedjax.Metric</span> to be defined on a single example rather than a batch of examples. This way, the metric calculation can be freely vectorized with <span id="S2.SS1.SSS0.Px5.p2.1.3" class="ltx_text ltx_font_typewriter">jax.vmap<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_serif">4</span></span><a target="_blank" href="https://jax.readthedocs.io/en/latest/jax.html?highlight=vmap##jax.vmap" title="" class="ltx_ref ltx_url">https://jax.readthedocs.io/en/latest/jax.html?highlight=vmap##jax.vmap</a></span></span></span></span> and empty examples in padded batches ignored without users having to explicitly account for this themselves.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Client training and aggregators</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Client training</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">As mentioned previously, federated learning experiments typically include a step of training across decentralized and distributed clients. <span id="S2.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> provides core functions for expressing this step. Because simulation is the focus, there is no need to introduce any design or technical overhead for distributed machine communication. Instead, the entire per-client work can run on a single machine, making the API simpler and the execution faster in most cases.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px1.p2.3" class="ltx_p">In its simplest form, a basic for-loop can be used for conducting training across multiple clients in <span id="S2.SS2.SSS0.Px1.p2.3.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>. For even faster training speeds, <span id="S2.SS2.SSS0.Px1.p2.3.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> provides the <span id="S2.SS2.SSS0.Px1.p2.3.3" class="ltx_text ltx_markedasmath ltx_font_typewriter">fedjax.for_each_client</span> primitive backed by <span id="S2.SS2.SSS0.Px1.p2.3.4" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.jit</span> and <span id="S2.SS2.SSS0.Px1.p2.3.5" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.pmap</span><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://jax.readthedocs.io/en/latest/jax.html?highlight=pmap##jax.pmap" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jax.readthedocs.io/en/latest/jax.html?highlight=pmap##jax.pmap</a></span></span></span>, which enables the simulation work to easily run on one or more accelerators such as GPU and TPU. We refer readers to the tutorial<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://fedjax.readthedocs.io/en/latest/notebooks/algorithms_tutorial.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fedjax.readthedocs.io/en/latest/notebooks/algorithms_tutorial.html</a></span></span></span> for an overview of these functionalities.
By defining client work in terms of <span id="S2.SS2.SSS0.Px1.p2.3.6" class="ltx_text ltx_font_typewriter">fedjax.for_each_client</span>, we are able to arbitrarily group cohorts of clients to be executed in parallel for greater performance without additional burden on the user.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Server aggregation</h4>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">The final step of server aggregation is often what differs the most significantly between federated learning algorithms. Thus, making this step as easily expressible and interpretable as possible is a core design goal of <span id="S2.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>. This is achieved by providing basic underlying functions for working with model parameters, which are usually structured as pytrees<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://jax.readthedocs.io/en/latest/pytrees.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jax.readthedocs.io/en/latest/pytrees.html</a></span></span></span> in JAX, as well as high-level pre-defined common aggregators in <span id="S2.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">fedjax.aggregators</span>. These aggregators can also be used to implement compression and differentially private federated learning algorithms.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Example</h2>

<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated averaging algorithm <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite> </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.3" class="ltx_listing ltx_figure_panel ltx_minipage ltx_align_top ltx_listing" style="width:216.8pt;">
<div id="alg1.l1" class="ltx_listingline">
<span id="alg1.l1.1" class="ltx_text ltx_font_bold">procedure</span> <span id="alg1.l1.2" class="ltx_text ltx_font_smallcaps">FederatedAveraging</span>

</div>
<div id="alg1.l2" class="ltx_listingline">     <math id="alg1.l2.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l2.m1.1a"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">T</annotation></semantics></math>: total number of rounds, <math id="alg1.l2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="alg1.l2.m2.1a"><mi id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><ci id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">c</annotation></semantics></math>: number of clients per round, <math id="alg1.l2.m3.1" class="ltx_Math" alttext="\eta_{s}" display="inline"><semantics id="alg1.l2.m3.1a"><msub id="alg1.l2.m3.1.1" xref="alg1.l2.m3.1.1.cmml"><mi id="alg1.l2.m3.1.1.2" xref="alg1.l2.m3.1.1.2.cmml">η</mi><mi id="alg1.l2.m3.1.1.3" xref="alg1.l2.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m3.1b"><apply id="alg1.l2.m3.1.1.cmml" xref="alg1.l2.m3.1.1"><csymbol cd="ambiguous" id="alg1.l2.m3.1.1.1.cmml" xref="alg1.l2.m3.1.1">subscript</csymbol><ci id="alg1.l2.m3.1.1.2.cmml" xref="alg1.l2.m3.1.1.2">𝜂</ci><ci id="alg1.l2.m3.1.1.3.cmml" xref="alg1.l2.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m3.1c">\eta_{s}</annotation></semantics></math>: server learning rate.

</div>
<div id="alg1.l3" class="ltx_listingline">     Initialize parameters: <math id="alg1.l3.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">w</mi><mn id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑤</ci><cn type="integer" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">w_{0}</annotation></semantics></math>

</div>
<div id="alg1.l4" class="ltx_listingline">     <span id="alg1.l4.1" class="ltx_text ltx_font_bold">for</span> round <math id="alg1.l4.m1.1" class="ltx_Math" alttext="t=1" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">t</mi><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">=</mo><mn id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><eq id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></eq><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">t=1</annotation></semantics></math> to <math id="alg1.l4.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l4.m2.1a"><mi id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b"><ci id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.1c">T</annotation></semantics></math> <span id="alg1.l4.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">         <math id="alg1.l5.m1.1" class="ltx_Math" alttext="C_{t}\leftarrow" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><msub id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml"><mi id="alg1.l5.m1.1.1.2.2" xref="alg1.l5.m1.1.1.2.2.cmml">C</mi><mi id="alg1.l5.m1.1.1.2.3" xref="alg1.l5.m1.1.1.2.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">←</mo><mi id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">←</ci><apply id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.2.1.cmml" xref="alg1.l5.m1.1.1.2">subscript</csymbol><ci id="alg1.l5.m1.1.1.2.2.cmml" xref="alg1.l5.m1.1.1.2.2">𝐶</ci><ci id="alg1.l5.m1.1.1.2.3.cmml" xref="alg1.l5.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">C_{t}\leftarrow</annotation></semantics></math> (random set of <math id="alg1.l5.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="alg1.l5.m2.1a"><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">c</annotation></semantics></math> clients)

</div>
<div id="alg1.l6" class="ltx_listingline">         <span id="alg1.l6.1" class="ltx_text ltx_font_bold">for</span> client <math id="alg1.l6.m1.1" class="ltx_Math" alttext="k\in C_{t}" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">k</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">∈</mo><msub id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">C</mi><mi id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><in id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></in><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑘</ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3">subscript</csymbol><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝐶</ci><ci id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">k\in C_{t}</annotation></semantics></math> <span id="alg1.l6.2" class="ltx_text ltx_font_bold">do</span>

<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\Delta_{k},n_{k}\leftarrow\textsc{ClientUpdate}(k,w_{t-1})" display="block"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mrow id="S3.Ex1.m1.3.3.2.2" xref="S3.Ex1.m1.3.3.2.3.cmml"><msub id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.cmml"><mi mathvariant="normal" id="S3.Ex1.m1.2.2.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">Δ</mi><mi id="S3.Ex1.m1.2.2.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.3.cmml">k</mi></msub><mo id="S3.Ex1.m1.3.3.2.2.3" xref="S3.Ex1.m1.3.3.2.3.cmml">,</mo><msub id="S3.Ex1.m1.3.3.2.2.2" xref="S3.Ex1.m1.3.3.2.2.2.cmml"><mi id="S3.Ex1.m1.3.3.2.2.2.2" xref="S3.Ex1.m1.3.3.2.2.2.2.cmml">n</mi><mi id="S3.Ex1.m1.3.3.2.2.2.3" xref="S3.Ex1.m1.3.3.2.2.2.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m1.4.4.4" xref="S3.Ex1.m1.4.4.4.cmml">←</mo><mrow id="S3.Ex1.m1.4.4.3" xref="S3.Ex1.m1.4.4.3.cmml"><mtext class="ltx_font_smallcaps" id="S3.Ex1.m1.4.4.3.3" xref="S3.Ex1.m1.4.4.3.3a.cmml">ClientUpdate</mtext><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.3.2" xref="S3.Ex1.m1.4.4.3.2.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.3.1.1" xref="S3.Ex1.m1.4.4.3.1.2.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.3.1.1.2" xref="S3.Ex1.m1.4.4.3.1.2.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">k</mi><mo id="S3.Ex1.m1.4.4.3.1.1.3" xref="S3.Ex1.m1.4.4.3.1.2.cmml">,</mo><msub id="S3.Ex1.m1.4.4.3.1.1.1" xref="S3.Ex1.m1.4.4.3.1.1.1.cmml"><mi id="S3.Ex1.m1.4.4.3.1.1.1.2" xref="S3.Ex1.m1.4.4.3.1.1.1.2.cmml">w</mi><mrow id="S3.Ex1.m1.4.4.3.1.1.1.3" xref="S3.Ex1.m1.4.4.3.1.1.1.3.cmml"><mi id="S3.Ex1.m1.4.4.3.1.1.1.3.2" xref="S3.Ex1.m1.4.4.3.1.1.1.3.2.cmml">t</mi><mo id="S3.Ex1.m1.4.4.3.1.1.1.3.1" xref="S3.Ex1.m1.4.4.3.1.1.1.3.1.cmml">−</mo><mn id="S3.Ex1.m1.4.4.3.1.1.1.3.3" xref="S3.Ex1.m1.4.4.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.Ex1.m1.4.4.3.1.1.4" xref="S3.Ex1.m1.4.4.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><ci id="S3.Ex1.m1.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4">←</ci><list id="S3.Ex1.m1.3.3.2.3.cmml" xref="S3.Ex1.m1.3.3.2.2"><apply id="S3.Ex1.m1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.2">Δ</ci><ci id="S3.Ex1.m1.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3">𝑘</ci></apply><apply id="S3.Ex1.m1.3.3.2.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.2.2.2.1.cmml" xref="S3.Ex1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.2.2.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2.2.2">𝑛</ci><ci id="S3.Ex1.m1.3.3.2.2.2.3.cmml" xref="S3.Ex1.m1.3.3.2.2.2.3">𝑘</ci></apply></list><apply id="S3.Ex1.m1.4.4.3.cmml" xref="S3.Ex1.m1.4.4.3"><times id="S3.Ex1.m1.4.4.3.2.cmml" xref="S3.Ex1.m1.4.4.3.2"></times><ci id="S3.Ex1.m1.4.4.3.3a.cmml" xref="S3.Ex1.m1.4.4.3.3"><mtext class="ltx_font_smallcaps" id="S3.Ex1.m1.4.4.3.3.cmml" xref="S3.Ex1.m1.4.4.3.3">ClientUpdate</mtext></ci><interval closure="open" id="S3.Ex1.m1.4.4.3.1.2.cmml" xref="S3.Ex1.m1.4.4.3.1.1"><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝑘</ci><apply id="S3.Ex1.m1.4.4.3.1.1.1.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.3.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.4.4.3.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1.2">𝑤</ci><apply id="S3.Ex1.m1.4.4.3.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1.3"><minus id="S3.Ex1.m1.4.4.3.1.1.1.3.1.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1.3.1"></minus><ci id="S3.Ex1.m1.4.4.3.1.1.1.3.2.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.Ex1.m1.4.4.3.1.1.1.3.3.cmml" xref="S3.Ex1.m1.4.4.3.1.1.1.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\Delta_{k},n_{k}\leftarrow\textsc{ClientUpdate}(k,w_{t-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>

</div>
<div id="alg1.l7" class="ltx_listingline">         <span id="alg1.l7.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l7.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l8" class="ltx_listingline">         <math id="alg1.l8.m1.1" class="ltx_Math" alttext="w_{t}\leftarrow w_{t-1}-\eta_{\text{s}}\frac{\sum_{k\in C_{t}}n_{k}\Delta_{k}}{\sum_{k^{\prime}\in C_{t}}n_{k^{\prime}}}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><msub id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml"><mi id="alg1.l8.m1.1.1.2.2" xref="alg1.l8.m1.1.1.2.2.cmml">w</mi><mi id="alg1.l8.m1.1.1.2.3" xref="alg1.l8.m1.1.1.2.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">←</mo><mrow id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><msub id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml"><mi id="alg1.l8.m1.1.1.3.2.2" xref="alg1.l8.m1.1.1.3.2.2.cmml">w</mi><mrow id="alg1.l8.m1.1.1.3.2.3" xref="alg1.l8.m1.1.1.3.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.2.3.2" xref="alg1.l8.m1.1.1.3.2.3.2.cmml">t</mi><mo id="alg1.l8.m1.1.1.3.2.3.1" xref="alg1.l8.m1.1.1.3.2.3.1.cmml">−</mo><mn id="alg1.l8.m1.1.1.3.2.3.3" xref="alg1.l8.m1.1.1.3.2.3.3.cmml">1</mn></mrow></msub><mo id="alg1.l8.m1.1.1.3.1" xref="alg1.l8.m1.1.1.3.1.cmml">−</mo><mrow id="alg1.l8.m1.1.1.3.3" xref="alg1.l8.m1.1.1.3.3.cmml"><msub id="alg1.l8.m1.1.1.3.3.2" xref="alg1.l8.m1.1.1.3.3.2.cmml"><mi id="alg1.l8.m1.1.1.3.3.2.2" xref="alg1.l8.m1.1.1.3.3.2.2.cmml">η</mi><mtext id="alg1.l8.m1.1.1.3.3.2.3" xref="alg1.l8.m1.1.1.3.3.2.3a.cmml">s</mtext></msub><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.3.3.1" xref="alg1.l8.m1.1.1.3.3.1.cmml">​</mo><mfrac id="alg1.l8.m1.1.1.3.3.3" xref="alg1.l8.m1.1.1.3.3.3.cmml"><mrow id="alg1.l8.m1.1.1.3.3.3.2" xref="alg1.l8.m1.1.1.3.3.3.2.cmml"><mstyle displaystyle="false" id="alg1.l8.m1.1.1.3.3.3.2.1" xref="alg1.l8.m1.1.1.3.3.3.2.1.cmml"><msub id="alg1.l8.m1.1.1.3.3.3.2.1a" xref="alg1.l8.m1.1.1.3.3.3.2.1.cmml"><mo id="alg1.l8.m1.1.1.3.3.3.2.1.2" xref="alg1.l8.m1.1.1.3.3.3.2.1.2.cmml">∑</mo><mrow id="alg1.l8.m1.1.1.3.3.3.2.1.3" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.2.1.3.2" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.2.cmml">k</mi><mo id="alg1.l8.m1.1.1.3.3.3.2.1.3.1" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.1.cmml">∈</mo><msub id="alg1.l8.m1.1.1.3.3.3.2.1.3.3" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.2" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3.2.cmml">C</mi><mi id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.3" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3.3.cmml">t</mi></msub></mrow></msub></mstyle><mrow id="alg1.l8.m1.1.1.3.3.3.2.2" xref="alg1.l8.m1.1.1.3.3.3.2.2.cmml"><msub id="alg1.l8.m1.1.1.3.3.3.2.2.2" xref="alg1.l8.m1.1.1.3.3.3.2.2.2.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.2.2.2.2" xref="alg1.l8.m1.1.1.3.3.3.2.2.2.2.cmml">n</mi><mi id="alg1.l8.m1.1.1.3.3.3.2.2.2.3" xref="alg1.l8.m1.1.1.3.3.3.2.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.3.3.3.2.2.1" xref="alg1.l8.m1.1.1.3.3.3.2.2.1.cmml">​</mo><msub id="alg1.l8.m1.1.1.3.3.3.2.2.3" xref="alg1.l8.m1.1.1.3.3.3.2.2.3.cmml"><mi mathvariant="normal" id="alg1.l8.m1.1.1.3.3.3.2.2.3.2" xref="alg1.l8.m1.1.1.3.3.3.2.2.3.2.cmml">Δ</mi><mi id="alg1.l8.m1.1.1.3.3.3.2.2.3.3" xref="alg1.l8.m1.1.1.3.3.3.2.2.3.3.cmml">k</mi></msub></mrow></mrow><mrow id="alg1.l8.m1.1.1.3.3.3.3" xref="alg1.l8.m1.1.1.3.3.3.3.cmml"><mstyle displaystyle="false" id="alg1.l8.m1.1.1.3.3.3.3.1" xref="alg1.l8.m1.1.1.3.3.3.3.1.cmml"><msub id="alg1.l8.m1.1.1.3.3.3.3.1a" xref="alg1.l8.m1.1.1.3.3.3.3.1.cmml"><mo id="alg1.l8.m1.1.1.3.3.3.3.1.2" xref="alg1.l8.m1.1.1.3.3.3.3.1.2.cmml">∑</mo><mrow id="alg1.l8.m1.1.1.3.3.3.3.1.3" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.cmml"><msup id="alg1.l8.m1.1.1.3.3.3.3.1.3.2" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.2" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2.2.cmml">k</mi><mo id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.3" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2.3.cmml">′</mo></msup><mo id="alg1.l8.m1.1.1.3.3.3.3.1.3.1" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.1.cmml">∈</mo><msub id="alg1.l8.m1.1.1.3.3.3.3.1.3.3" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.2" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3.2.cmml">C</mi><mi id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.3" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3.3.cmml">t</mi></msub></mrow></msub></mstyle><msub id="alg1.l8.m1.1.1.3.3.3.3.2" xref="alg1.l8.m1.1.1.3.3.3.3.2.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.3.2.2" xref="alg1.l8.m1.1.1.3.3.3.3.2.2.cmml">n</mi><msup id="alg1.l8.m1.1.1.3.3.3.3.2.3" xref="alg1.l8.m1.1.1.3.3.3.3.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.3.3.3.2.3.2" xref="alg1.l8.m1.1.1.3.3.3.3.2.3.2.cmml">k</mi><mo id="alg1.l8.m1.1.1.3.3.3.3.2.3.3" xref="alg1.l8.m1.1.1.3.3.3.3.2.3.3.cmml">′</mo></msup></msub></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">←</ci><apply id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.2.1.cmml" xref="alg1.l8.m1.1.1.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.2.2.cmml" xref="alg1.l8.m1.1.1.2.2">𝑤</ci><ci id="alg1.l8.m1.1.1.2.3.cmml" xref="alg1.l8.m1.1.1.2.3">𝑡</ci></apply><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><minus id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.1"></minus><apply id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2">𝑤</ci><apply id="alg1.l8.m1.1.1.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.3"><minus id="alg1.l8.m1.1.1.3.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.2.3.1"></minus><ci id="alg1.l8.m1.1.1.3.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.2.3.2">𝑡</ci><cn type="integer" id="alg1.l8.m1.1.1.3.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="alg1.l8.m1.1.1.3.3.cmml" xref="alg1.l8.m1.1.1.3.3"><times id="alg1.l8.m1.1.1.3.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.1"></times><apply id="alg1.l8.m1.1.1.3.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.2.2">𝜂</ci><ci id="alg1.l8.m1.1.1.3.3.2.3a.cmml" xref="alg1.l8.m1.1.1.3.3.2.3"><mtext mathsize="70%" id="alg1.l8.m1.1.1.3.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.3.2.3">s</mtext></ci></apply><apply id="alg1.l8.m1.1.1.3.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3"><divide id="alg1.l8.m1.1.1.3.3.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3"></divide><apply id="alg1.l8.m1.1.1.3.3.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2"><apply id="alg1.l8.m1.1.1.3.3.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.2.1.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1">subscript</csymbol><sum id="alg1.l8.m1.1.1.3.3.3.2.1.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.2"></sum><apply id="alg1.l8.m1.1.1.3.3.3.2.1.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3"><in id="alg1.l8.m1.1.1.3.3.3.2.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.1"></in><ci id="alg1.l8.m1.1.1.3.3.3.2.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.2">𝑘</ci><apply id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3.2">𝐶</ci><ci id="alg1.l8.m1.1.1.3.3.3.2.1.3.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="alg1.l8.m1.1.1.3.3.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2"><times id="alg1.l8.m1.1.1.3.3.3.2.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.1"></times><apply id="alg1.l8.m1.1.1.3.3.3.2.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.2.2.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.2.2.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.2.2">𝑛</ci><ci id="alg1.l8.m1.1.1.3.3.3.2.2.2.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.2.3">𝑘</ci></apply><apply id="alg1.l8.m1.1.1.3.3.3.2.2.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.2.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.2.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.3.2">Δ</ci><ci id="alg1.l8.m1.1.1.3.3.3.2.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.2.2.3.3">𝑘</ci></apply></apply></apply><apply id="alg1.l8.m1.1.1.3.3.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3"><apply id="alg1.l8.m1.1.1.3.3.3.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.3.1.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1">subscript</csymbol><sum id="alg1.l8.m1.1.1.3.3.3.3.1.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.2"></sum><apply id="alg1.l8.m1.1.1.3.3.3.3.1.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3"><in id="alg1.l8.m1.1.1.3.3.3.3.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.1"></in><apply id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2">superscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2.2">𝑘</ci><ci id="alg1.l8.m1.1.1.3.3.3.3.1.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.2.3">′</ci></apply><apply id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3.2">𝐶</ci><ci id="alg1.l8.m1.1.1.3.3.3.3.1.3.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="alg1.l8.m1.1.1.3.3.3.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2.2">𝑛</ci><apply id="alg1.l8.m1.1.1.3.3.3.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.3.3.3.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2.3">superscript</csymbol><ci id="alg1.l8.m1.1.1.3.3.3.3.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2.3.2">𝑘</ci><ci id="alg1.l8.m1.1.1.3.3.3.3.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.3.3.3.2.3.3">′</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">w_{t}\leftarrow w_{t-1}-\eta_{\text{s}}\frac{\sum_{k\in C_{t}}n_{k}\Delta_{k}}{\sum_{k^{\prime}\in C_{t}}n_{k^{\prime}}}</annotation></semantics></math>

</div>
<div id="alg1.l9" class="ltx_listingline">     <span id="alg1.l9.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span id="alg1.l10.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l10.2" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.4" class="ltx_listing ltx_figure_panel ltx_minipage ltx_align_top ltx_listing" style="width:216.8pt;">
<div id="alg1.l1a" class="ltx_listingline">
<span id="alg1.l1a.1" class="ltx_text ltx_font_bold">procedure</span> <span id="alg1.l1a.2" class="ltx_text ltx_font_smallcaps">ClientUpdate</span>(<math id="alg1.l1a.m1.2" class="ltx_Math" alttext="k,w" display="inline"><semantics id="alg1.l1a.m1.2a"><mrow id="alg1.l1a.m1.2.3.2" xref="alg1.l1a.m1.2.3.1.cmml"><mi id="alg1.l1a.m1.1.1" xref="alg1.l1a.m1.1.1.cmml">k</mi><mo id="alg1.l1a.m1.2.3.2.1" xref="alg1.l1a.m1.2.3.1.cmml">,</mo><mi id="alg1.l1a.m1.2.2" xref="alg1.l1a.m1.2.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1a.m1.2b"><list id="alg1.l1a.m1.2.3.1.cmml" xref="alg1.l1a.m1.2.3.2"><ci id="alg1.l1a.m1.1.1.cmml" xref="alg1.l1a.m1.1.1">𝑘</ci><ci id="alg1.l1a.m1.2.2.cmml" xref="alg1.l1a.m1.2.2">𝑤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1a.m1.2c">k,w</annotation></semantics></math>) 
</div>
<div id="alg1.l2a" class="ltx_listingline">     <math id="alg1.l2a.m1.1" class="ltx_Math" alttext="S_{k}" display="inline"><semantics id="alg1.l2a.m1.1a"><msub id="alg1.l2a.m1.1.1" xref="alg1.l2a.m1.1.1.cmml"><mi id="alg1.l2a.m1.1.1.2" xref="alg1.l2a.m1.1.1.2.cmml">S</mi><mi id="alg1.l2a.m1.1.1.3" xref="alg1.l2a.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l2a.m1.1b"><apply id="alg1.l2a.m1.1.1.cmml" xref="alg1.l2a.m1.1.1"><csymbol cd="ambiguous" id="alg1.l2a.m1.1.1.1.cmml" xref="alg1.l2a.m1.1.1">subscript</csymbol><ci id="alg1.l2a.m1.1.1.2.cmml" xref="alg1.l2a.m1.1.1.2">𝑆</ci><ci id="alg1.l2a.m1.1.1.3.cmml" xref="alg1.l2a.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m1.1c">S_{k}</annotation></semantics></math>: dataset of client <math id="alg1.l2a.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l2a.m2.1a"><mi id="alg1.l2a.m2.1.1" xref="alg1.l2a.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l2a.m2.1b"><ci id="alg1.l2a.m2.1.1.cmml" xref="alg1.l2a.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m2.1c">k</annotation></semantics></math>, <math id="alg1.l2a.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg1.l2a.m3.1a"><mi id="alg1.l2a.m3.1.1" xref="alg1.l2a.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg1.l2a.m3.1b"><ci id="alg1.l2a.m3.1.1.cmml" xref="alg1.l2a.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m3.1c">B</annotation></semantics></math>: batch size, <math id="alg1.l2a.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l2a.m4.1a"><mi id="alg1.l2a.m4.1.1" xref="alg1.l2a.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l2a.m4.1b"><ci id="alg1.l2a.m4.1.1.cmml" xref="alg1.l2a.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m4.1c">E</annotation></semantics></math>: Number of epochs, <math id="alg1.l2a.m5.1" class="ltx_Math" alttext="\eta_{c}" display="inline"><semantics id="alg1.l2a.m5.1a"><msub id="alg1.l2a.m5.1.1" xref="alg1.l2a.m5.1.1.cmml"><mi id="alg1.l2a.m5.1.1.2" xref="alg1.l2a.m5.1.1.2.cmml">η</mi><mi id="alg1.l2a.m5.1.1.3" xref="alg1.l2a.m5.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l2a.m5.1b"><apply id="alg1.l2a.m5.1.1.cmml" xref="alg1.l2a.m5.1.1"><csymbol cd="ambiguous" id="alg1.l2a.m5.1.1.1.cmml" xref="alg1.l2a.m5.1.1">subscript</csymbol><ci id="alg1.l2a.m5.1.1.2.cmml" xref="alg1.l2a.m5.1.1.2">𝜂</ci><ci id="alg1.l2a.m5.1.1.3.cmml" xref="alg1.l2a.m5.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m5.1c">\eta_{c}</annotation></semantics></math>: client learning rate.

</div>
<div id="alg1.l3a" class="ltx_listingline">     <math id="alg1.l3a.m1.1" class="ltx_Math" alttext="w^{\prime}\leftarrow w" display="inline"><semantics id="alg1.l3a.m1.1a"><mrow id="alg1.l3a.m1.1.1" xref="alg1.l3a.m1.1.1.cmml"><msup id="alg1.l3a.m1.1.1.2" xref="alg1.l3a.m1.1.1.2.cmml"><mi id="alg1.l3a.m1.1.1.2.2" xref="alg1.l3a.m1.1.1.2.2.cmml">w</mi><mo id="alg1.l3a.m1.1.1.2.3" xref="alg1.l3a.m1.1.1.2.3.cmml">′</mo></msup><mo stretchy="false" id="alg1.l3a.m1.1.1.1" xref="alg1.l3a.m1.1.1.1.cmml">←</mo><mi id="alg1.l3a.m1.1.1.3" xref="alg1.l3a.m1.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3a.m1.1b"><apply id="alg1.l3a.m1.1.1.cmml" xref="alg1.l3a.m1.1.1"><ci id="alg1.l3a.m1.1.1.1.cmml" xref="alg1.l3a.m1.1.1.1">←</ci><apply id="alg1.l3a.m1.1.1.2.cmml" xref="alg1.l3a.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l3a.m1.1.1.2.1.cmml" xref="alg1.l3a.m1.1.1.2">superscript</csymbol><ci id="alg1.l3a.m1.1.1.2.2.cmml" xref="alg1.l3a.m1.1.1.2.2">𝑤</ci><ci id="alg1.l3a.m1.1.1.2.3.cmml" xref="alg1.l3a.m1.1.1.2.3">′</ci></apply><ci id="alg1.l3a.m1.1.1.3.cmml" xref="alg1.l3a.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3a.m1.1c">w^{\prime}\leftarrow w</annotation></semantics></math>

</div>
<div id="alg1.l4a" class="ltx_listingline">     <math id="alg1.l4a.m1.1" class="ltx_Math" alttext="\mathcal{B}\leftarrow" display="inline"><semantics id="alg1.l4a.m1.1a"><mrow id="alg1.l4a.m1.1.1" xref="alg1.l4a.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l4a.m1.1.1.2" xref="alg1.l4a.m1.1.1.2.cmml">ℬ</mi><mo stretchy="false" id="alg1.l4a.m1.1.1.1" xref="alg1.l4a.m1.1.1.1.cmml">←</mo><mi id="alg1.l4a.m1.1.1.3" xref="alg1.l4a.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4a.m1.1b"><apply id="alg1.l4a.m1.1.1.cmml" xref="alg1.l4a.m1.1.1"><ci id="alg1.l4a.m1.1.1.1.cmml" xref="alg1.l4a.m1.1.1.1">←</ci><ci id="alg1.l4a.m1.1.1.2.cmml" xref="alg1.l4a.m1.1.1.2">ℬ</ci><csymbol cd="latexml" id="alg1.l4a.m1.1.1.3.cmml" xref="alg1.l4a.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4a.m1.1c">\mathcal{B}\leftarrow</annotation></semantics></math> (split <math id="alg1.l4a.m2.1" class="ltx_Math" alttext="S_{k}" display="inline"><semantics id="alg1.l4a.m2.1a"><msub id="alg1.l4a.m2.1.1" xref="alg1.l4a.m2.1.1.cmml"><mi id="alg1.l4a.m2.1.1.2" xref="alg1.l4a.m2.1.1.2.cmml">S</mi><mi id="alg1.l4a.m2.1.1.3" xref="alg1.l4a.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l4a.m2.1b"><apply id="alg1.l4a.m2.1.1.cmml" xref="alg1.l4a.m2.1.1"><csymbol cd="ambiguous" id="alg1.l4a.m2.1.1.1.cmml" xref="alg1.l4a.m2.1.1">subscript</csymbol><ci id="alg1.l4a.m2.1.1.2.cmml" xref="alg1.l4a.m2.1.1.2">𝑆</ci><ci id="alg1.l4a.m2.1.1.3.cmml" xref="alg1.l4a.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4a.m2.1c">S_{k}</annotation></semantics></math> into batches of size <math id="alg1.l4a.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg1.l4a.m3.1a"><mi id="alg1.l4a.m3.1.1" xref="alg1.l4a.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg1.l4a.m3.1b"><ci id="alg1.l4a.m3.1.1.cmml" xref="alg1.l4a.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4a.m3.1c">B</annotation></semantics></math>)

</div>
<div id="alg1.l5a" class="ltx_listingline">     <span id="alg1.l5a.1" class="ltx_text ltx_font_bold">for</span> epoch <math id="alg1.l5a.m1.1" class="ltx_Math" alttext="e=1" display="inline"><semantics id="alg1.l5a.m1.1a"><mrow id="alg1.l5a.m1.1.1" xref="alg1.l5a.m1.1.1.cmml"><mi id="alg1.l5a.m1.1.1.2" xref="alg1.l5a.m1.1.1.2.cmml">e</mi><mo id="alg1.l5a.m1.1.1.1" xref="alg1.l5a.m1.1.1.1.cmml">=</mo><mn id="alg1.l5a.m1.1.1.3" xref="alg1.l5a.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5a.m1.1b"><apply id="alg1.l5a.m1.1.1.cmml" xref="alg1.l5a.m1.1.1"><eq id="alg1.l5a.m1.1.1.1.cmml" xref="alg1.l5a.m1.1.1.1"></eq><ci id="alg1.l5a.m1.1.1.2.cmml" xref="alg1.l5a.m1.1.1.2">𝑒</ci><cn type="integer" id="alg1.l5a.m1.1.1.3.cmml" xref="alg1.l5a.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5a.m1.1c">e=1</annotation></semantics></math> to <math id="alg1.l5a.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l5a.m2.1a"><mi id="alg1.l5a.m2.1.1" xref="alg1.l5a.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l5a.m2.1b"><ci id="alg1.l5a.m2.1.1.cmml" xref="alg1.l5a.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5a.m2.1c">E</annotation></semantics></math> <span id="alg1.l5a.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l6a" class="ltx_listingline">         <span id="alg1.l6a.1" class="ltx_text ltx_font_bold">for</span> batch <math id="alg1.l6a.m1.1" class="ltx_Math" alttext="b\in\mathcal{B}" display="inline"><semantics id="alg1.l6a.m1.1a"><mrow id="alg1.l6a.m1.1.1" xref="alg1.l6a.m1.1.1.cmml"><mi id="alg1.l6a.m1.1.1.2" xref="alg1.l6a.m1.1.1.2.cmml">b</mi><mo id="alg1.l6a.m1.1.1.1" xref="alg1.l6a.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="alg1.l6a.m1.1.1.3" xref="alg1.l6a.m1.1.1.3.cmml">ℬ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6a.m1.1b"><apply id="alg1.l6a.m1.1.1.cmml" xref="alg1.l6a.m1.1.1"><in id="alg1.l6a.m1.1.1.1.cmml" xref="alg1.l6a.m1.1.1.1"></in><ci id="alg1.l6a.m1.1.1.2.cmml" xref="alg1.l6a.m1.1.1.2">𝑏</ci><ci id="alg1.l6a.m1.1.1.3.cmml" xref="alg1.l6a.m1.1.1.3">ℬ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6a.m1.1c">b\in\mathcal{B}</annotation></semantics></math> <span id="alg1.l6a.2" class="ltx_text ltx_font_bold">do</span>

<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.5" class="ltx_Math" alttext="w^{\prime}\leftarrow w^{\prime}-\eta_{\text{c}}\nabla\sum_{(x_{i},y_{i})\in b}L(w^{\prime},x_{i},y_{i})" display="block"><semantics id="S3.Ex2.m1.5a"><mrow id="S3.Ex2.m1.5.5" xref="S3.Ex2.m1.5.5.cmml"><msup id="S3.Ex2.m1.5.5.5" xref="S3.Ex2.m1.5.5.5.cmml"><mi id="S3.Ex2.m1.5.5.5.2" xref="S3.Ex2.m1.5.5.5.2.cmml">w</mi><mo id="S3.Ex2.m1.5.5.5.3" xref="S3.Ex2.m1.5.5.5.3.cmml">′</mo></msup><mo stretchy="false" id="S3.Ex2.m1.5.5.4" xref="S3.Ex2.m1.5.5.4.cmml">←</mo><mrow id="S3.Ex2.m1.5.5.3" xref="S3.Ex2.m1.5.5.3.cmml"><msup id="S3.Ex2.m1.5.5.3.5" xref="S3.Ex2.m1.5.5.3.5.cmml"><mi id="S3.Ex2.m1.5.5.3.5.2" xref="S3.Ex2.m1.5.5.3.5.2.cmml">w</mi><mo id="S3.Ex2.m1.5.5.3.5.3" xref="S3.Ex2.m1.5.5.3.5.3.cmml">′</mo></msup><mo id="S3.Ex2.m1.5.5.3.4" xref="S3.Ex2.m1.5.5.3.4.cmml">−</mo><mrow id="S3.Ex2.m1.5.5.3.3" xref="S3.Ex2.m1.5.5.3.3.cmml"><msub id="S3.Ex2.m1.5.5.3.3.5" xref="S3.Ex2.m1.5.5.3.3.5.cmml"><mi id="S3.Ex2.m1.5.5.3.3.5.2" xref="S3.Ex2.m1.5.5.3.3.5.2.cmml">η</mi><mtext id="S3.Ex2.m1.5.5.3.3.5.3" xref="S3.Ex2.m1.5.5.3.3.5.3a.cmml">c</mtext></msub><mo lspace="0.167em" rspace="0em" id="S3.Ex2.m1.5.5.3.3.4" xref="S3.Ex2.m1.5.5.3.3.4.cmml">​</mo><mo id="S3.Ex2.m1.5.5.3.3.6" xref="S3.Ex2.m1.5.5.3.3.6.cmml">∇</mo><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.5.5.3.3.4a" xref="S3.Ex2.m1.5.5.3.3.4.cmml">​</mo><mrow id="S3.Ex2.m1.5.5.3.3.3" xref="S3.Ex2.m1.5.5.3.3.3.cmml"><munder id="S3.Ex2.m1.5.5.3.3.3.4" xref="S3.Ex2.m1.5.5.3.3.3.4.cmml"><mo movablelimits="false" id="S3.Ex2.m1.5.5.3.3.3.4.2" xref="S3.Ex2.m1.5.5.3.3.3.4.2.cmml">∑</mo><mrow id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml"><mrow id="S3.Ex2.m1.2.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.2.2.2.3" xref="S3.Ex2.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.2.2.2.2.2.4" xref="S3.Ex2.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex2.m1.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.2.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.Ex2.m1.2.2.2.2.2.2.3" xref="S3.Ex2.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex2.m1.2.2.2.2.2.5" xref="S3.Ex2.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.Ex2.m1.2.2.2.3" xref="S3.Ex2.m1.2.2.2.3.cmml">∈</mo><mi id="S3.Ex2.m1.2.2.2.4" xref="S3.Ex2.m1.2.2.2.4.cmml">b</mi></mrow></munder><mrow id="S3.Ex2.m1.5.5.3.3.3.3" xref="S3.Ex2.m1.5.5.3.3.3.3.cmml"><mi id="S3.Ex2.m1.5.5.3.3.3.3.5" xref="S3.Ex2.m1.5.5.3.3.3.3.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.5.5.3.3.3.3.4" xref="S3.Ex2.m1.5.5.3.3.3.3.4.cmml">​</mo><mrow id="S3.Ex2.m1.5.5.3.3.3.3.3.3" xref="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.3.3.3.3.3.3.4" xref="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml">(</mo><msup id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml">w</mi><mo id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.Ex2.m1.5.5.3.3.3.3.3.3.5" xref="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml">,</mo><msub id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.2.cmml">x</mi><mi id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.3" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.5.5.3.3.3.3.3.3.6" xref="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml">,</mo><msub id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.cmml"><mi id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.2" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.2.cmml">y</mi><mi id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.3" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex2.m1.5.5.3.3.3.3.3.3.7" xref="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.5b"><apply id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5"><ci id="S3.Ex2.m1.5.5.4.cmml" xref="S3.Ex2.m1.5.5.4">←</ci><apply id="S3.Ex2.m1.5.5.5.cmml" xref="S3.Ex2.m1.5.5.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.5.1.cmml" xref="S3.Ex2.m1.5.5.5">superscript</csymbol><ci id="S3.Ex2.m1.5.5.5.2.cmml" xref="S3.Ex2.m1.5.5.5.2">𝑤</ci><ci id="S3.Ex2.m1.5.5.5.3.cmml" xref="S3.Ex2.m1.5.5.5.3">′</ci></apply><apply id="S3.Ex2.m1.5.5.3.cmml" xref="S3.Ex2.m1.5.5.3"><minus id="S3.Ex2.m1.5.5.3.4.cmml" xref="S3.Ex2.m1.5.5.3.4"></minus><apply id="S3.Ex2.m1.5.5.3.5.cmml" xref="S3.Ex2.m1.5.5.3.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3.5.1.cmml" xref="S3.Ex2.m1.5.5.3.5">superscript</csymbol><ci id="S3.Ex2.m1.5.5.3.5.2.cmml" xref="S3.Ex2.m1.5.5.3.5.2">𝑤</ci><ci id="S3.Ex2.m1.5.5.3.5.3.cmml" xref="S3.Ex2.m1.5.5.3.5.3">′</ci></apply><apply id="S3.Ex2.m1.5.5.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3"><times id="S3.Ex2.m1.5.5.3.3.4.cmml" xref="S3.Ex2.m1.5.5.3.3.4"></times><apply id="S3.Ex2.m1.5.5.3.3.5.cmml" xref="S3.Ex2.m1.5.5.3.3.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3.3.5.1.cmml" xref="S3.Ex2.m1.5.5.3.3.5">subscript</csymbol><ci id="S3.Ex2.m1.5.5.3.3.5.2.cmml" xref="S3.Ex2.m1.5.5.3.3.5.2">𝜂</ci><ci id="S3.Ex2.m1.5.5.3.3.5.3a.cmml" xref="S3.Ex2.m1.5.5.3.3.5.3"><mtext mathsize="70%" id="S3.Ex2.m1.5.5.3.3.5.3.cmml" xref="S3.Ex2.m1.5.5.3.3.5.3">c</mtext></ci></apply><ci id="S3.Ex2.m1.5.5.3.3.6.cmml" xref="S3.Ex2.m1.5.5.3.3.6">∇</ci><apply id="S3.Ex2.m1.5.5.3.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3.3"><apply id="S3.Ex2.m1.5.5.3.3.3.4.cmml" xref="S3.Ex2.m1.5.5.3.3.3.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3.3.3.4.1.cmml" xref="S3.Ex2.m1.5.5.3.3.3.4">subscript</csymbol><sum id="S3.Ex2.m1.5.5.3.3.3.4.2.cmml" xref="S3.Ex2.m1.5.5.3.3.3.4.2"></sum><apply id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2"><in id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.3"></in><interval closure="open" id="S3.Ex2.m1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.2.2"><apply id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.Ex2.m1.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.Ex2.m1.2.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.3">𝑖</ci></apply></interval><ci id="S3.Ex2.m1.2.2.2.4.cmml" xref="S3.Ex2.m1.2.2.2.4">𝑏</ci></apply></apply><apply id="S3.Ex2.m1.5.5.3.3.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3"><times id="S3.Ex2.m1.5.5.3.3.3.3.4.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.4"></times><ci id="S3.Ex2.m1.5.5.3.3.3.3.5.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.5">𝐿</ci><vector id="S3.Ex2.m1.5.5.3.3.3.3.3.4.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3"><apply id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.2">𝑤</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.3">′</ci></apply><apply id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.2">𝑥</ci><ci id="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.1.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3">subscript</csymbol><ci id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.2.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.2">𝑦</ci><ci id="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3.3.3.3.3.3.3">𝑖</ci></apply></vector></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.5c">w^{\prime}\leftarrow w^{\prime}-\eta_{\text{c}}\nabla\sum_{(x_{i},y_{i})\in b}L(w^{\prime},x_{i},y_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>

</div>
<div id="alg1.l7a" class="ltx_listingline">         <span id="alg1.l7a.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l7a.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l8a" class="ltx_listingline">     <span id="alg1.l8a.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l8a.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l9a" class="ltx_listingline">     <span id="alg1.l9a.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l9a.m1.2" class="ltx_Math" alttext="w-w^{\prime},|S_{k}|" display="inline"><semantics id="alg1.l9a.m1.2a"><mrow id="alg1.l9a.m1.2.2.2" xref="alg1.l9a.m1.2.2.3.cmml"><mrow id="alg1.l9a.m1.1.1.1.1" xref="alg1.l9a.m1.1.1.1.1.cmml"><mi id="alg1.l9a.m1.1.1.1.1.2" xref="alg1.l9a.m1.1.1.1.1.2.cmml">w</mi><mo id="alg1.l9a.m1.1.1.1.1.1" xref="alg1.l9a.m1.1.1.1.1.1.cmml">−</mo><msup id="alg1.l9a.m1.1.1.1.1.3" xref="alg1.l9a.m1.1.1.1.1.3.cmml"><mi id="alg1.l9a.m1.1.1.1.1.3.2" xref="alg1.l9a.m1.1.1.1.1.3.2.cmml">w</mi><mo id="alg1.l9a.m1.1.1.1.1.3.3" xref="alg1.l9a.m1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo id="alg1.l9a.m1.2.2.2.3" xref="alg1.l9a.m1.2.2.3.cmml">,</mo><mrow id="alg1.l9a.m1.2.2.2.2.1" xref="alg1.l9a.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="alg1.l9a.m1.2.2.2.2.1.2" xref="alg1.l9a.m1.2.2.2.2.2.1.cmml">|</mo><msub id="alg1.l9a.m1.2.2.2.2.1.1" xref="alg1.l9a.m1.2.2.2.2.1.1.cmml"><mi id="alg1.l9a.m1.2.2.2.2.1.1.2" xref="alg1.l9a.m1.2.2.2.2.1.1.2.cmml">S</mi><mi id="alg1.l9a.m1.2.2.2.2.1.1.3" xref="alg1.l9a.m1.2.2.2.2.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="alg1.l9a.m1.2.2.2.2.1.3" xref="alg1.l9a.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9a.m1.2b"><list id="alg1.l9a.m1.2.2.3.cmml" xref="alg1.l9a.m1.2.2.2"><apply id="alg1.l9a.m1.1.1.1.1.cmml" xref="alg1.l9a.m1.1.1.1.1"><minus id="alg1.l9a.m1.1.1.1.1.1.cmml" xref="alg1.l9a.m1.1.1.1.1.1"></minus><ci id="alg1.l9a.m1.1.1.1.1.2.cmml" xref="alg1.l9a.m1.1.1.1.1.2">𝑤</ci><apply id="alg1.l9a.m1.1.1.1.1.3.cmml" xref="alg1.l9a.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9a.m1.1.1.1.1.3.1.cmml" xref="alg1.l9a.m1.1.1.1.1.3">superscript</csymbol><ci id="alg1.l9a.m1.1.1.1.1.3.2.cmml" xref="alg1.l9a.m1.1.1.1.1.3.2">𝑤</ci><ci id="alg1.l9a.m1.1.1.1.1.3.3.cmml" xref="alg1.l9a.m1.1.1.1.1.3.3">′</ci></apply></apply><apply id="alg1.l9a.m1.2.2.2.2.2.cmml" xref="alg1.l9a.m1.2.2.2.2.1"><abs id="alg1.l9a.m1.2.2.2.2.2.1.cmml" xref="alg1.l9a.m1.2.2.2.2.1.2"></abs><apply id="alg1.l9a.m1.2.2.2.2.1.1.cmml" xref="alg1.l9a.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l9a.m1.2.2.2.2.1.1.1.cmml" xref="alg1.l9a.m1.2.2.2.2.1.1">subscript</csymbol><ci id="alg1.l9a.m1.2.2.2.2.1.1.2.cmml" xref="alg1.l9a.m1.2.2.2.2.1.1.2">𝑆</ci><ci id="alg1.l9a.m1.2.2.2.2.1.1.3.cmml" xref="alg1.l9a.m1.2.2.2.2.1.1.3">𝑘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9a.m1.2c">w-w^{\prime},|S_{k}|</annotation></semantics></math>

</div>
<div id="alg1.l10a" class="ltx_listingline">
<span id="alg1.l10a.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l10a.2" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</div>
</div>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we demonstrate how to implement federated averaging with <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>. Because <span id="S3.p1.1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> only introduces a few core concepts and is clear and straightforward, code written in <span id="S3.p1.1.3" class="ltx_text ltx_font_smallcaps">FedJAX</span> tends to resemble the pseudo-code used to describe novel algorithms in research papers, making it easy to get started with. While <span id="S3.p1.1.4" class="ltx_text ltx_font_smallcaps">FedJAX</span> provides primitives for federated learning, they are not required and can be replaced with just NumPy and JAX. The advantage of building on top of JAX is that even the most basic implementations can still be reasonably fast.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">Below, we walk through a simple example of federated averaging (Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) for linear regression implemented in <span id="S3.p2.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>.
The first steps are to set up the experiment by loading the federated dataset, initializing the model parameters, and defining the loss and gradient functions.
The code for these steps is given in Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><svg id="S3.F4.pic1" class="ltx_picture" height="107.81" overflow="visible" version="1.1" width="600"><g transform="translate(0,107.81) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 101.91 C 0 105.17 2.64 107.81 5.91 107.81 L 594.09 107.81 C 597.36 107.81 600 105.17 600 101.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 101.91 C 1.97 104.08 3.73 105.85 5.91 105.85 L 594.09 105.85 C 596.27 105.85 598.03 104.08 598.03 101.91 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject class="ltx_minipage" width="402.3pt" height="80.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><pre id="S3.F4.pic1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter">
import jax
import jax.numpy as jnp
import fedjax

# {"client_id": client_dataset}
federated_data = fedjax.FederatedData()
# Initialize model parameters
server_params = jnp.array(0.5)
# Mean squared error
mse_loss = lambda params, batch: jnp.mean(
        (jnp.dot(batch["x"], params) - batch["y"])**2)
# jax.jit for XLA and jax.grad for autograd
grad_fn = jax.jit(jax.grad(mse_loss))
</pre></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Dataset and model initialization.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.7" class="ltx_p">Next, we use <span id="S3.p3.7.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">fedjax.for_each_client</span> to coordinate the training that occurs across multiple clients. For federated averaging, <span id="S3.p3.7.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">client_init</span> initializes the client model using the server model, <span id="S3.p3.7.3" class="ltx_text ltx_markedasmath ltx_font_typewriter">client_step</span> completes one step of local mini-batch SGD, and <span id="S3.p3.7.4" class="ltx_text ltx_markedasmath ltx_font_typewriter">client_final</span> returns the difference between the initial server model and the trained client model. By using <span id="S3.p3.7.5" class="ltx_text ltx_markedasmath ltx_font_typewriter">fedjax.for_each_client</span>, this work will run on any available accelerators and possibly in parallel because it is backed by <span id="S3.p3.7.6" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.jit</span> and <span id="S3.p3.7.7" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.pmap</span>. While this is already simple to implement, the same could also be written out as a basic for-loop over clients if desired. The code for these steps is given in Figure <a href="#S3.F5" title="Figure 5 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This code implements the <span id="S3.p3.7.8" class="ltx_text ltx_font_smallcaps">ClientUpdate</span> procedure of federated averaging from Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><svg id="S3.F5.pic1" class="ltx_picture" height="91.21" overflow="visible" version="1.1" width="600"><g transform="translate(0,91.21) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 85.3 C 0 88.57 2.64 91.21 5.91 91.21 L 594.09 91.21 C 597.36 91.21 600 88.57 600 85.3 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 85.3 C 1.97 87.48 3.73 89.24 5.91 89.24 L 594.09 89.24 C 596.27 89.24 598.03 87.48 598.03 85.3 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject class="ltx_minipage" width="402.3pt" height="63.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><pre id="S3.F5.pic1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter">
# For-loop over clients with client learning rate 0.1
for_each_client = fedjax.for_each_client(
  client_init=lambda server_params, _: server_params,
  client_step=(
    lambda params, batch: params - grad_fn(params, batch) * 0.1),
  client_final=lambda server_params, params: server_params - params)
</pre></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Client update.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.2" class="ltx_p">Finally, we run federated averaging for <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.p4.1.m1.1a"><mn id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><cn type="integer" id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">100</annotation></semantics></math> training rounds by sampling clients from the federated dataset, training across these clients using <span id="S3.p4.2.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">fedjax.for_each_client</span>, and aggregating the client updates using weighted averaging to update the server model in Figure <a href="#S3.F6" title="Figure 6 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. This code implements the <span id="S3.p4.2.2" class="ltx_text ltx_font_smallcaps">FederatedAveraging</span> procedure of federated averaging from Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3 Example ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><svg id="S3.F6.pic1" class="ltx_picture" height="123.73" overflow="visible" version="1.1" width="600"><g transform="translate(0,123.73) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 117.82 C 0 121.08 2.64 123.73 5.91 123.73 L 594.09 123.73 C 597.36 123.73 600 121.08 600 117.82 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 117.82 C 1.97 119.99 3.73 121.76 5.91 121.76 L 594.09 121.76 C 596.27 121.76 598.03 119.99 598.03 117.82 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject class="ltx_minipage" width="402.3pt" height="96.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><pre id="S3.F6.pic1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter">
# 100 rounds of federated training
for _ in range(100):
  clients = federated_data.clients()
  client_updates = []
  client_weights = []
  for client_id, client_update in for_each_client(server_params, clients):
    client_updates.append(client_update)
    client_weights.append(federated_data.client_size(client_id))
  # Weighted average of client updates
  server_update = (
    jnp.sum(client_updates * client_weights) /
    jnp.sum(client_weights))
  # Server learning rate of 0.01
  server_params = server_params - server_update * 0.01
</pre></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Server update and the federated learning algorithm.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmarks</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We benchmark the <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span> federated averaging implementation on the image recognition task for the federated EMNIST-62 dataset <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib9" title="" class="ltx_ref">2018b</a>)</cite> and the next word prediction task for Stack Overflow <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.4" class="ltx_p">The EMNIST-62 dataset consists of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="3400" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">3400</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">3400</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">3400</annotation></semantics></math> writers and their writing samples, which are one of <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="62" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">62</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="integer" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">62</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">62</annotation></semantics></math> classes (alphanumeric).
Following <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, we train a convolutional neural network for <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="1500" display="inline"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">1500</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn type="integer" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">1500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">1500</annotation></semantics></math> rounds with <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.4.m4.1a"><mn id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><cn type="integer" id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">10</annotation></semantics></math> clients per round using federated averaging. We run experiments on GPU (a single NVIDIA V100) and TPU (a single TensorCore on a Google TPU v2) and report the final test accuracy, overall execution time, average training round duration, and full evaluation time in Table <a href="#S4.T1" title="Table 1 ‣ 4 Benchmarks ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We note that with a singleTensorCore, training takes under five minutes.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.6" class="ltx_p">The Stack Overflow dataset consists of questions and answers from the Stack Overflow forum, grouped by username. This dataset consists of roughly <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="342" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">342</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">342</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">342</annotation></semantics></math>K users in the train split and <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="204" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">204</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="integer" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">204</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">204</annotation></semantics></math>K in the test split. Following <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, we train a single layer LSTM for <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="1500" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">1500</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">1500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">1500</annotation></semantics></math> rounds with <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.p3.4.m4.1a"><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><cn type="integer" id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">50</annotation></semantics></math> clients per round using federated averaging. We run experiments on GPU (a single NVIDIA V100), TPU (a single TensorCore on a Google TPU v2) using only <span id="S4.p3.6.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.jit</span>, and multi-core TPU (eight TensorCores on a Google TPU v2) using <span id="S4.p3.6.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.pmap</span> and report results in Table <a href="#S4.T2" title="Table 2 ‣ 4 Benchmarks ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Benchmarks show that with multiple TensorCores, recurrent language models can be trained in under an hour. Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Benchmarks ‣ FedJAX: Federated learning simulation with JAX" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> also shows the average training round duration as the number of clients per round increases. We note that training with multiple TensorCores is substantially faster as the number of clients per round increases.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Benchmark results on EMNIST with federated averaging.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Hardware</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Test accuracy</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Overall (s)</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Average training round (s)</th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Full evaluation (s)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">GPU</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">85.92%</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">418</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">7.21</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center">TPU</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">85.85%</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">258</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center">0.16</td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_center">4.06</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Benchmark results on Stack Overflow with federated averaging.</figcaption>
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">Hardware</th>
<th id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Test accuracy</th>
<th id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Overall (m)</th>
<th id="S4.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Average training round (s)</th>
<th id="S4.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Full evaluation (m)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<th id="S4.T2.2.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">GPU</th>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">24.74%</td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">127.2</td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">4.33</td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">17.32</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">TPU (<span id="S4.T2.1.1.1.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.jit</span>)</th>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center">24.44%</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center">106.8</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center">3.73</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center">11.97</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">TPU (<span id="S4.T2.2.2.1.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">jax.pmap</span>)</th>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center">24.67%</td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center">48.0</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center">1.26</td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center">11.97</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2108.02117/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="215" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Stack Overflow average training round duration as the number of clients per round increases.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We introduced <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">FedJAX</span>, a JAX-based open source library for federated learning simulations that emphasizes ease-of-use in research.
<span id="S5.p1.1.2" class="ltx_text ltx_font_smallcaps">FedJAX</span> provides simple primitives for federated learning along with a collection of canonical datasets, models, and algorithms to make developing and evaluating federated algorithms easier and faster. Implementing additional algorithms, datasets, and models remains an on-going effort.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">Authors thank Sai Praneeth Kamireddy for contributing to the library and various discussions early on in development.</p>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">Authors also thank Ehsan Amid, Theresa Breiner, Mingqing Chen, Fabio Costa, Roy Frostig, Zachary Garrett, Satyen Kale, Rajiv Mathews, Lara Mcconnaughey, Brendan McMahan, Mehryar Mohri, Krzysztof Ostrowski, Max Rabinovich, Michael Riley, Gary Sivek, Jane Shapiro, Luciana Toledo-Lopez, and Michael Wunder for helpful comments and contributions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et al. [2020]</span>
<span class="ltx_bibblock">
Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and
Heiko Ludwig.

</span>
<span class="ltx_bibblock">Mitigating bias in federated learning, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. [2018]</span>
<span class="ltx_bibblock">
Naman Agarwal, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Brendan
McMahan.

</span>
<span class="ltx_bibblock">cpSGD: Communication-efficient and differentially-private
distributed SGD.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, pages 7575–7586, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2017]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>, pages 1175–1191. ACM, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradbury et al. [2018]</span>
<span class="ltx_bibblock">
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,
Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang.

</span>
<span class="ltx_bibblock">JAX: composable transformations of Python+NumPy programs,
2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/google/jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/jax</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brisimi et al. [2018]</span>
<span class="ltx_bibblock">
Theodora S. Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch.
Paschalidis, and Wei Shi.

</span>
<span class="ltx_bibblock">Federated learning of predictive models from federated electronic
health records.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International journal of medical informatics</em>, 112:59–67, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budden et al. [2020a]</span>
<span class="ltx_bibblock">
David Budden, Matteo Hessel, Iurii Kemaev, Stephen Spencer, and Fabio Viola.

</span>
<span class="ltx_bibblock">Chex: Testing made fun, in jax!, 2020a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/deepmind/chex" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/deepmind/chex</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budden et al. [2020b]</span>
<span class="ltx_bibblock">
David Budden, Matteo Hessel, John Quan, Steven Kapturowski, Kate Baumli, Surya
Bhupatiraju, Aurelia Guy, and Michael King.

</span>
<span class="ltx_bibblock">RLax: Reinforcement Learning in JAX, 2020b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/deepmind/rlax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/deepmind/rlax</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. [2018a]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by reducing client resource
requirements.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.07210</em>, 2018a.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. [2018b]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečnỳ, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>, 2018b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. [2020]</span>
<span class="ltx_bibblock">
Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong.

</span>
<span class="ltx_bibblock">Fairness-aware agnostic federated learning, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Godwin* et al. [2020]</span>
<span class="ltx_bibblock">
Jonathan Godwin*, Thomas Keck*, Peter Battaglia, Victor Bapst, Thomas Kipf,
Yujia Li, Kimberly Stachenfeld, Petar Veličković, and Alvaro
Sanchez-Gonzalez.

</span>
<span class="ltx_bibblock">Jraph: A library for graph neural networks in jax., 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/deepmind/jraph" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/deepmind/jraph</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddadpour and Mahdavi [2019]</span>
<span class="ltx_bibblock">
Farzin Haddadpour and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">On the convergence of local descent methods in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.14425</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. [2018]</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heek et al. [2020]</span>
<span class="ltx_bibblock">
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand
Rondepierre, Andreas Steiner, and Marc van Zee.

</span>
<span class="ltx_bibblock">Flax: A neural network library and ecosystem for JAX, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/google/flax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/flax</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hennigan et al. [2020]</span>
<span class="ltx_bibblock">
Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin.

</span>
<span class="ltx_bibblock">Haiku: Sonnet for JAX, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/deepmind/dm-haiku" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/deepmind/dm-haiku</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hessel et al. [2020]</span>
<span class="ltx_bibblock">
Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom
Hennigan.

</span>
<span class="ltx_bibblock">Optax: composable gradient transformation and optimisation, in jax!,
2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/deepmind/optax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/deepmind/optax</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2020]</span>
<span class="ltx_bibblock">
Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang.

</span>
<span class="ltx_bibblock">Fairness and accuracy in federated learning, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. [2021]</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie
He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,
Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz
Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,
Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova,
Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U.
Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,
and Sen Zhao.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. [2019]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi,
Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for on-device federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.06378</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. [2020]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J
Reddi, Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Mime: Mimicking centralized stochastic algorithms in federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.03606</em>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaled et al. [2020]</span>
<span class="ltx_bibblock">
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik.

</span>
<span class="ltx_bibblock">Tighter theory for local SGD on indentical and heterogeneous data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of AISTATS</em>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016a]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter
Richtárik.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016a.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016b]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016b.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">On the convergence of federated optimization in heterogeneous
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019a]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07873</em>, 2019a.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=ByexElSYDr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=ByexElSYDr</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019b]</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.

</span>
<span class="ltx_bibblock">On the convergence of FedAvg on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.02189</em>, 2019b.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ludwig et al. [2020]</span>
<span class="ltx_bibblock">
Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank
Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn, et al.

</span>
<span class="ltx_bibblock">Ibm federated learning: an enterprise framework white paper v0. 1.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.10987</em>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of AISTATS</em>, pages 1273–1282, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al. [2019]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICML</em>, volume 97 of <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine
Learning Research</em>, pages 4615–4625. PMLR, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Objax Developers [2020]</span>
<span class="ltx_bibblock">
Objax Developers.

</span>
<span class="ltx_bibblock">Objax, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/google/objax" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/objax</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peterson et al. [2019]</span>
<span class="ltx_bibblock">
Daniel Peterson, Pallika Kanani, and Virendra J Marathe.

</span>
<span class="ltx_bibblock">Private federated learning with domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06733</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. [2020]</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and H. Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ro et al. [2021]</span>
<span class="ltx_bibblock">
Jae Ro, Mingqing Chen, Rajiv Mathews, Mehryar Mohri, and Ananda Theertha
Suresh.

</span>
<span class="ltx_bibblock">Communication-efficient agnostic federated averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.02748</em>, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. [2018]</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al. [2019]</span>
<span class="ltx_bibblock">
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek.

</span>
<span class="ltx_bibblock">Clustered federated learning: Model-agnostic distributed multi-task
optimization under privacy constraints.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01991</em>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suresh et al. [2017]</span>
<span class="ltx_bibblock">
Ananda Theertha Suresh, Felix X Yu, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Distributed mean estimation with limited communication.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em>, pages 3329–3337. JMLR. org, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TFF [2018]</span>
<span class="ltx_bibblock">
TFF.

</span>
<span class="ltx_bibblock">Tensorflow federated, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2020]</span>
<span class="ltx_bibblock">
Jinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng.

</span>
<span class="ltx_bibblock">Ternary compression for communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>,
2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2018]</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2019]</span>
<span class="ltx_bibblock">
Hao Yu, Sen Yang, and Shenghuo Zhu.

</span>
<span class="ltx_bibblock">Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 33, pages 5693–5700, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2108.02116" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2108.02117" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2108.02117">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2108.02117" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2108.02118" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 01:58:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
