<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.08119] The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1</title><meta property="og:description" content="Data-driven Machine Learning has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems.
Existing medical …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.08119">

<!--Generated on Fri Mar  1 18:39:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Future of Digital Health with Federated Learning<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Disclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of <span id="footnote1.1" class="ltx_text" style="color:#0000FF;">https://www.nature.com/articles/s41746-020-00323-1</span></span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicola Rieke
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonny Hancox
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenqi Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fausto Milletari
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Holger Roth
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shadi Albarqouni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Technical University of Munich (TUM), Munich, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">Computer Vision Lab (CVL), ETH Zürich, Switzerland
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Spyridon Bakas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Pennsylvania (UPenn) Philadelphia, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mathieu N. Galtier
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Owkin, Paris, France
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bennett Landman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Vanderbilt University Medical Center, Nashville, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Klaus Maier-Hein
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">German Cancer Research Center (DKFZ), Heidelberg, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastien Ourselin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">King’s College London (KCL), London, UK
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Micah Sheller
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Intel Corporation, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ronald M. Summers
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Clinical Center, National Institutes of Health (NIH), Bethesda, Maryland, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew Trask
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">OpenMined
</span>
<span class="ltx_contact ltx_role_affiliation">University of Oxford, Oxford, UK
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daguang Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Maximilian Baust
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">NVIDIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">M. Jorge Cardoso
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">King’s College London (KCL), London, UK
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_italic">Data-driven Machine Learning has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems.
Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how Federated Learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.
</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Research on artificial intelligence (AI) has enabled a variety of significant breakthroughs over the course of the last two decades.
In digital healthcare, the introduction of powerful Machine Learning-based and particularly Deep Learning-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has led to disruptive innovations in radiology, pathology, genomics and many other fields.
In order to capture the complexity of these applications, modern Deep Learning (DL) models feature a large number (e.g. millions) of parameters that are learned from and validated on medical datasets.
Sufficiently large corpora of curated data are thus required in order to obtain models that yield clinical-grade accuracy, whilst being safe, fair, equitable and generalising well to unseen data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For example, training an automatic tumour detector and diagnostic tool in a supervised way requires a large annotated database that encompasses the full spectrum of possible anatomies, pathological patterns and types of input data. Data like this is hard to obtain and curate.
One of the main difficulties is that unlike other data, which may be shared and copied rather freely, health data is highly sensitive, subject to regulation and cannot be used for research without appropriate patient consent and ethical approval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Even if data anonymisation is sometimes proposed as a way to bypass these limitations, it is now well-understood that removing metadata such as patient name or date of birth is often not enough to preserve privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Imaging data suffers from the same issue - it is possible to reconstruct a patient’s face from three-dimensional imaging data, such as computed tomography (CT) or magnetic resonance imaging (MRI). Also the human brain itself has been shown to be as unique as a fingerprint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where subject identity, age and gender can be predicted and revealed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Another reason why data sharing is not systematic in healthcare is that medical data are potentially highly valuable and costly to acquire. Collecting, curating and maintaining a quality dataset takes considerable time and effort. These datasets may have a significant business value and so are not given away lightly. In practice, openly sharing medical data is often restricted by data collectors themselves, who need fine-grained control over the access to the data they have gathered.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is a learning paradigm that seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the underlying datasets.
The approach was originally developed in a different domain, but it recently gained traction for healthcare applications because it neatly addresses the problems that usually exist when trying to aggregate medical data.
Applied to digital health this means that FL enables insights to be gained collaboratively across institutions, e.g. in the form of a global or consensus model, without sharing the patient data.
In particular, the strength of FL is that sensitive training data does not need to be moved beyond the firewalls of the institutions in which they reside.
Instead, the Machine Learning (ML) process occurs locally at each participating institution and only model characteristics (e.g. parameters, gradients etc.) are exchanged.
Once training has been completed, the trained consensus model benefits from the knowledge accumulated across all institutions.
Recent research has shown that this approach can achieve a performance that is comparable to a scenario where the data was co-located in a data lake and superior to the models that only see isolated single-institutional data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For this reason, we believe that a successful implementation of FL holds significant potential for enabling precision medicine at large scale.
The scalability with respect to patient numbers included for model training would facilitate models that yield unbiased decisions, optimally reflect an individual’s physiology, and are sensitive to rare diseases in a way that is respectful of governance and privacy concerns.
Whilst FL still requires rigorous technical consideration to ensure that the algorithm is proceeding optimally without compromising safety or patient privacy, it does have the potential to overcome the limitations of current approaches that require a single pool of centralised data.

<br class="ltx_break"></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The aim of this paper is to provide context and detail for the community regarding the benefits and impact of FL for medical applications (Section <a href="#S2" title="2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) as well as highlighting key considerations and challenges of implementing FL in the context of digital health (Section <a href="#S3" title="3 Technical Considerations ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
The medical FL use-case is inherently different from other domains, e.g. in terms of number of participants and data diversity, and while recent surveys investigate the research advances and open questions of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we focus on what it actually means for digital health and what is needed to enable it.
We envision a federated future for digital health and hope to inspire and raise awareness with this article for the community.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2003.08119/assets/figures/FLTopology.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="277" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Federated Learning Communication Architectures</span> - (a) Client-Server architecture via Hub and Spokes: A Parameter Server distributes the model and each node trains a local model for several iterations, after which the updated models are returned to the Parameter Server for aggregation. This consensus model is then redistributed for subsequent iterations. (b) Decentralised Architecture via peer-to-peer: Rather than using a Parameter Server, each node broadcasts its locally trained model to some or all of its peers and each node does its own aggregation. (c) Hybrid Architecture: Federations can be composed into a hierarchy of hubs and spokes, which might represent regions, health authorities or countries.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data-driven Medicine Requires Federated Efforts</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">ML and especially DL is becoming the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">de facto</span> knowledge discovery approach in many industries, but successfully implementing data-driven applications requires that models are trained and evaluated on sufficiently large and diverse datasets.
These medical datasets are difficult to curate (Section <a href="#S2.SS1" title="2.1 The Reliance on Data ‣ 2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). FL offers a way to counteract this data dilemma and its associated governance and privacy concerns by enabling collaborative learning without centralising the data (Section  <a href="#S2.SS2" title="2.2 The Promise of Federated Efforts ‣ 2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). This learning paradigm, however, requires consideration from and offers benefits to the various stakeholders of the healthcare environment (Section <a href="#S2.SS3" title="2.3 Impact on Stakeholders ‣ 2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>).
All these points will be discussed in this section.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Reliance on Data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Data-driven approaches rely on datasets that truly represent the underlying data distribution of the problem to be solved.
Whilst the importance of comprehensive and encompassing databases is a well-known requirement to ensure generalisability, state-of-the-art algorithms are usually evaluated on carefully curated datasets, often originating from a small number of sources - if not a single source. This implies major challenges: pockets of isolated data can introduce sample bias in which demographic (e.g. gender, age etc.) or technical imbalances (e.g. acquisition protocol, equipment manufacturer) skew the predictions, adversely affecting the accuracy of prediction for certain groups or sites.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The need for sufficiently large databases for AI training has spawned many initiatives seeking to pool data from multiple institutions.
Large initiatives have so far primarily focused on the idea of creating data lakes. These data lakes have been built with the aim of leveraging either the commercial value of the data, as exemplified by IBM’s Merge Healthcare acquisition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, or as a resource for economic growth and scientific progress, with examples such as NHS Scotland’s National Safe Haven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the French Health Data Hub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Health Data Research UK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Substantial, albeit smaller, initiatives have also made data available to the general community such as <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">The Human Connectome</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, UK Biobank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">The Cancer Imaging Archive</span> (TCIA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, NIH CXR8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, NIH DeepLesion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_italic">The Cancer Genome Atlas</span> (TCGA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the <span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_italic">Alzheimer’s disease neuroimaging initiative</span> (ADNI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, or as part of medical grand challenges<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://grand-challenge.org/</span></span></span> such as the CAMELYON challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, the multimodal brain tumor image segmentation benchmark (BRATS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or the Medical Segmentation Decathlon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Public data is usually task- or disease-specific and often released with varying degrees of license restrictions, sometimes limiting its exploitation.
Regardless of the approach, the availability of such data has the potential to catalyse scientific advances, stimulate technology start-ups and deliver improvements in healthcare.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Centralising or releasing data, however, poses not only regulatory and legal challenges related to ethics, privacy and data protection, but also technical ones - safely anonymising, controlling access, and transferring healthcare data is a non-trivial, and often impossible, task. As an example, anonymised data from the electronic health record can appear innocuous and GDPR/PHI compliant, but just a few data elements may allow for patient reidentification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The same applies to genomic data and medical images, with their high-dimensional nature making them as unique as one’s fingerprint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Therefore, unless the anonymisation process destroys the fidelity of the data, likely rendering it useless, patient reidentification or information leakage cannot be ruled out.
Gated access, in which only approved users may access specific subsets of data, is often proposed as a putative solution to this issue. However, not only does this severely limit data availability, it is only practical for cases in which the consent granted by the data owners or patients is unconditional, since recalling data from those who may have had access to the data is practically unenforceable.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>The Promise of Federated Efforts</h3>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2003.08119/assets/figures/FLComputePlan.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Compute Plan</span> a) Federated Training: Standard approach to federation in which each of the nodes train in parallel and submit their model updates for aggregation every few epochs. The aggregation may happen on one of the training nodes or a separate Parameter Server node, which would then redistribute the consensus model.
b) Peer to Peer Training: Nodes broadcast their model updates to one or more nodes in the federation and each does its own aggregation. Cyclic Training happens when model updates are passed to a single neighbour one or more times, round-robin style.
c) Hybrid Training: Federations, perhaps in remote geographies, can be composed into a hierarchy and use different communication/aggregation strategies at each tier. In the illustrated case, three federations of varying size periodically share their models using a peer to peer approach. The consensus model is then redistributed to each federation and each node therein.</figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The promise of FL is simple - to address privacy and governance challenges by allowing algorithms to learn from non co-located data. In a FL setting, each data controller not only defines their own governance processes and associated privacy considerations, but also, by not allowing data to move or to be copied, controls data access and the possibility to revoke it. So the potential of FL is to provide controlled, indirect access to large and comprehensive datasets needed for the development of ML algorithms, whilst respecting patient privacy and data governance. It should be noted that this includes both the training as well as the validation phase of the development. In this way, FL could create new opportunities, e.g. by allowing large-scale validation across the globe directly in the institutions, and enable novel research on, for example, rare diseases, where the incident rates are low and it is unlikely that a single institution has a dataset that is sufficient for ML approaches.
Moving the to-be-trained model to the data instead of collecting the data in a central location has another major advantage: the high-dimensional, storage-intense medical data does not have to be duplicated from local institutions in a centralised pool and duplicated again by every user that uses this data for local model training. In a FL setup, only the model is transferred to the local institutions and can scale naturally with a potentially growing global dataset without replicating the data or multiplying the data storage requirements.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Some of the promises of FL are implicit: a certain degree of privacy is provided since other FL participants never directly access the data from other institutions and only receive the resulting model parameters that are aggregated over several participants.
And in a Client-Server Architecture (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), in which a federated server manages the aggregation and distribution, the participating institutions can even remain unknown to each other. However, it has been shown that the models themselves can, under certain conditions, memorise information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Therefore the FL setup can be further enhanced by privacy protections using mechanisms such as differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> or learning from encrypted data (c.f. Sec. <a href="#S3" title="3 Technical Considerations ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). And FL techniques are still a very active area of research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">All in all, a successful implementation of FL will represent a paradigm shift from centralised data warehouses or lakes, with a significant impact on the various stakeholders in the healthcare domain.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Impact on Stakeholders</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">If FL is indeed the answer to the challenge of healthcare ML at scale, then it is important to understand who the various stakeholders are in a FL ecosystem and what they have to consider in order to benefit from it.</p>
</div>
<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Clinicians</h5>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px1.p1.1" class="ltx_p">are usually exposed to only a certain sub-group of the population based on the location and demographic environment of the hospital or practice they are working in.
Therefore, their decisions might be based on biased assumptions about the probability of certain diseases or their interconnection.
By using ML-based systems, e.g. as a second reader, they can augment their own expertise with expert knowledge from other institutions, ensuring a consistency of diagnosis not attainable today.
Whilst this promise is generally true for any ML-based system, systems trained in a federated fashion are potentially able to yield even less biased decisions and higher sensitivity to rare cases as they are likely to have seen a more complete picture of the data distribution.
In order to be an active part of or to benefit from the federation, however, demands some up-front effort such as compliance with agreements e.g. regarding the data structure, annotation and report protocol, which is necessary to ensure that the information is presented to collaborators in a commonly understood format.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Patients</h5>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p1.1" class="ltx_p">are usually relying on local hospitals and practices.
Establishing FL on a global scale could ensure higher quality of clinical decisions regardless of the location of the deployed system.
For example, patients who need medical attention in remote areas could benefit from the same high-quality ML-aided diagnosis that are available in hospitals with a large number of cases.
The same advantage applies to patients suffering from rare, or geographically uncommon, diseases, who are likely to have better outcomes if faster and more accurate diagnoses can be made.
FL may also lower the hurdle for becoming a data donor, since patients can be reassured that the data remains with the institution and data access can be revoked.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hospitals and Practices</h5>

<div id="S2.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px3.p1.1" class="ltx_p">can remain in full control and possession of their patient data with complete traceability of how the data is accessed.
They can precisely control the purpose for which a given data sample is going to be used, limiting the risk of misuse when they work with third parties.
However, participating in federated efforts will require investment in on-premise computing infrastructure or private-cloud service provision.
The amount of necessary compute capabilities depends of course on whether a site is only participating in evaluation and testing efforts or also in training efforts.
Even relatively small institutions can participate, since enough of them will generate a valuable corpus and they will still benefit from collective models generated.
One of the drawbacks is that FL strongly relies on the standardisation and homogenisation of the data formats so that predictive models can be trained and evaluated seamlessly.
This involves significant standardisation efforts from data managers.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Researchers and AI Developers</h5>

<div id="S2.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px4.p1.1" class="ltx_p">who want to develop and evaluate novel algorithms stand to benefit from access to a potentially vast collection of real-world data.
This will especially impact smaller research labs and start-ups, who would be able to directly develop their applications on healthcare data without the need to curate their own datasets.
By introducing federated efforts, precious resources can be directed towards solving clinical needs and associated technical problems rather than relying on the limited supply of open datasets.
At the same time, it will be necessary to conduct research on algorithmic strategies for federated training, e.g. how to combine models or updates efficiently, how to be robust to distribution shifts, etc., as highlighted in the technical survey papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
And a FL-based development implies that the researcher or AI developer cannot investigate or visualise all of the data on which the model is trained. It is for example not possible to look at an individual failure case to understand why the current model performs poorly on it.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Healthcare Providers</h5>

<div id="S2.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px5.p1.1" class="ltx_p">in many countries are affected by the ongoing paradigm shift from volume-based, i.e. fee-for-service-based, to value-based healthcare.
A value-based reimbursement structure is in turn strongly connected to the successful establishment of precision medicine.
This is not about promoting more expensive individualised therapies but instead about achieving better outcomes sooner through more focused treatment, thereby reducing the costs for providers. By way of example, with sufficient data, ML approaches can learn to recognise cancer-subtypes or genotypic traits from radiology images that could indicate certain therapies and discount others.
So, by providing exposure to large amounts of data, FL has the potential to increase the accuracy and robustness of healthcare AI, whilst reducing costs and improving patient outcomes, and is therefore vital to precision medicine.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Manufacturers</h5>

<div id="S2.SS3.SSS0.Px6.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px6.p1.1" class="ltx_p">of healthcare software and hardware could benefit from federated efforts and infrastructures for FL as well, since combining the learning from many devices and applications, without revealing anything patient-specific can facilitate the continuous improvement of ML-based systems. This potentially opens up a new source of data and revenue to manufacturers. However, hospitals may require significant upgrades to local compute, data storage, networking capabilities and associated software to enable such a use-case.
Note, however, that this change could be quite disruptive: FL could eventually impact the business models of providers, practices, hospitals and manufacturers affecting patient data ownership; and the regulatory frameworks surrounding continual and FL approaches are still under development.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Technical Considerations</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">FL is perhaps best-known from the work of Konečnỳ et al.  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, but various other definitions have been proposed in literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
These approaches can be realised via different communication architectures (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and respective compute plans (see Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 The Promise of Federated Efforts ‣ 2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
The main goal of FL, however, remains the same: to combine knowledge learned from non co-located data, that resides within the participating entities, into a global model.
Whereas the initial application field mostly comprised mobile devices, participating entities in the case of healthcare could be institutions storing the data, e.g. hospitals, or medical devices itself, e.g. a CT scanner or even low-powered devices that are able to run computations locally.
It is important to understand that this domain-shift to the medical field implies different conditions and requirements.
For example, in the case of the federated mobile device application, potentially millions of participants could contribute, but it would be impossible to have the same scale of consortium in terms of participating hospitals. On the other hand medical institutions may rely on more sophisticated and powerful compute infrastructure with stable connectivity.
Another aspect is that the variation in terms of data type and defined tasks as well as acquisition protocol and standardisation in healthcare is significantly higher than pictures and messages seen in other domains.
The participating entities have to agree on a collaboration protocol and the high-dimensional medical data, which is predominant in the field of digital health, poses challenges by requiring models with huge numbers of parameters. This may become an issue in scenarios where the available bandwidth for communication between participants is limited, since the model has to be transferred frequently.
And even though data is never shared during FL, considerations about the security of the connections between sites as well as mitigation of data leakage risks through model parameters are necessary.
In this section, we will discuss more in detail what FL is and how it differs from similar techniques as well as highlighting the key challenges and technical considerations that arise when applying FL in digital health.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Federated Learning Definition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">FL is a learning paradigm in which multiple parties train collaboratively without the need to exchange or centralise datasets.
Although various training strategies have been implemented to address specific tasks, a general formulation of FL can be formalised as follows:
Let <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{L}</annotation></semantics></math> denote a global loss function obtained via a weighted combination of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">K</annotation></semantics></math> local losses <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\{\mathcal{L}_{k}\}_{k=1}^{K}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msubsup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml">ℒ</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p1.3.m3.1.1.1.3.1" xref="S3.SS1.p1.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><set id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2">ℒ</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3">𝑘</ci></apply></set><apply id="S3.SS1.p1.3.m3.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3"><eq id="S3.SS1.p1.3.m3.1.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3.1"></eq><ci id="S3.SS1.p1.3.m3.1.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3.2">𝑘</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\{\mathcal{L}_{k}\}_{k=1}^{K}</annotation></semantics></math>, computed from private data <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="X_{k}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">X</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑋</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">X_{k}</annotation></semantics></math> residing at the individual involved parties:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\displaystyle\min_{\phi}\mathcal{L}(X;\phi)\quad\text{with}\quad\mathcal{L}(X;\phi)=\sum_{k=1}^{K}w_{k}\;\mathcal{L}_{k}(X_{k};\phi)," display="inline"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.2.2" xref="S3.E1.m1.7.7.1.1.2.3.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.2.cmml"><munder id="S3.E1.m1.7.7.1.1.1.1.1.2.1" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.2.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1.2.cmml">min</mi><mi id="S3.E1.m1.7.7.1.1.1.1.1.2.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1.3.cmml">ϕ</mi></munder><mo lspace="0.167em" id="S3.E1.m1.7.7.1.1.1.1.1.2a" xref="S3.E1.m1.7.7.1.1.1.1.1.2.cmml">⁡</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7.1.1.1.1.1.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.2.2.cmml">ℒ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.3.2" xref="S3.E1.m1.7.7.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.3.2.1" xref="S3.E1.m1.7.7.1.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">X</mi><mo id="S3.E1.m1.7.7.1.1.1.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.3.1.cmml">;</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">ϕ</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mspace width="1em" id="S3.E1.m1.7.7.1.1.2.2.3" xref="S3.E1.m1.7.7.1.1.2.3.cmml"></mspace><mtext id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6a.cmml">with</mtext><mspace width="1em" id="S3.E1.m1.7.7.1.1.2.2.4" xref="S3.E1.m1.7.7.1.1.2.3.cmml"></mspace><mrow id="S3.E1.m1.7.7.1.1.2.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7.1.1.2.2.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.2.2.2.1" xref="S3.E1.m1.7.7.1.1.2.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.2.2.2.3.2" xref="S3.E1.m1.7.7.1.1.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.2.2.2.3.2.1" xref="S3.E1.m1.7.7.1.1.2.2.2.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">X</mi><mo id="S3.E1.m1.7.7.1.1.2.2.2.3.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.3.1.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">ϕ</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.2.2.2.3.2.3" xref="S3.E1.m1.7.7.1.1.2.2.2.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.1.4" xref="S3.E1.m1.7.7.1.1.4.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E1.m1.7.7.1.1.3.2" xref="S3.E1.m1.7.7.1.1.3.2.cmml"><munderover id="S3.E1.m1.7.7.1.1.3.2a" xref="S3.E1.m1.7.7.1.1.3.2.cmml"><mo movablelimits="false" id="S3.E1.m1.7.7.1.1.3.2.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.7.7.1.1.3.2.2.3" xref="S3.E1.m1.7.7.1.1.3.2.2.3.cmml"><mi id="S3.E1.m1.7.7.1.1.3.2.2.3.2" xref="S3.E1.m1.7.7.1.1.3.2.2.3.2.cmml">k</mi><mo id="S3.E1.m1.7.7.1.1.3.2.2.3.1" xref="S3.E1.m1.7.7.1.1.3.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.7.7.1.1.3.2.2.3.3" xref="S3.E1.m1.7.7.1.1.3.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.7.7.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.3.2.3.cmml">K</mi></munderover></mstyle><mrow id="S3.E1.m1.7.7.1.1.3.1" xref="S3.E1.m1.7.7.1.1.3.1.cmml"><msub id="S3.E1.m1.7.7.1.1.3.1.3" xref="S3.E1.m1.7.7.1.1.3.1.3.cmml"><mi id="S3.E1.m1.7.7.1.1.3.1.3.2" xref="S3.E1.m1.7.7.1.1.3.1.3.2.cmml">w</mi><mi id="S3.E1.m1.7.7.1.1.3.1.3.3" xref="S3.E1.m1.7.7.1.1.3.1.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.3.1.2" xref="S3.E1.m1.7.7.1.1.3.1.2.cmml">​</mo><msub id="S3.E1.m1.7.7.1.1.3.1.4" xref="S3.E1.m1.7.7.1.1.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7.1.1.3.1.4.2" xref="S3.E1.m1.7.7.1.1.3.1.4.2.cmml">ℒ</mi><mi id="S3.E1.m1.7.7.1.1.3.1.4.3" xref="S3.E1.m1.7.7.1.1.3.1.4.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.3.1.2a" xref="S3.E1.m1.7.7.1.1.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.3.1.1.1" xref="S3.E1.m1.7.7.1.1.3.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.3.1.1.1.2" xref="S3.E1.m1.7.7.1.1.3.1.1.2.cmml">(</mo><msub id="S3.E1.m1.7.7.1.1.3.1.1.1.1" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.3.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1.2.cmml">X</mi><mi id="S3.E1.m1.7.7.1.1.3.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E1.m1.7.7.1.1.3.1.1.1.3" xref="S3.E1.m1.7.7.1.1.3.1.1.2.cmml">;</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">ϕ</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.3.1.1.1.4" xref="S3.E1.m1.7.7.1.1.3.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1"><eq id="S3.E1.m1.7.7.1.1.4.cmml" xref="S3.E1.m1.7.7.1.1.4"></eq><list id="S3.E1.m1.7.7.1.1.2.3.cmml" xref="S3.E1.m1.7.7.1.1.2.2"><apply id="S3.E1.m1.7.7.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1"></times><apply id="S3.E1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2"><apply id="S3.E1.m1.7.7.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1">subscript</csymbol><min id="S3.E1.m1.7.7.1.1.1.1.1.2.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1.2"></min><ci id="S3.E1.m1.7.7.1.1.1.1.1.2.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2.1.3">italic-ϕ</ci></apply><ci id="S3.E1.m1.7.7.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2.2">ℒ</ci></apply><list id="S3.E1.m1.7.7.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑋</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">italic-ϕ</ci></list></apply><ci id="S3.E1.m1.6.6a.cmml" xref="S3.E1.m1.6.6"><mtext id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">with</mtext></ci><apply id="S3.E1.m1.7.7.1.1.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2"><times id="S3.E1.m1.7.7.1.1.2.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.1"></times><ci id="S3.E1.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2">ℒ</ci><list id="S3.E1.m1.7.7.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑋</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">italic-ϕ</ci></list></apply></list><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><apply id="S3.E1.m1.7.7.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.3.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.2"></sum><apply id="S3.E1.m1.7.7.1.1.3.2.2.3.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.3"><eq id="S3.E1.m1.7.7.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.3.1"></eq><ci id="S3.E1.m1.7.7.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.3.2">𝑘</ci><cn type="integer" id="S3.E1.m1.7.7.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.7.7.1.1.3.2.3.cmml" xref="S3.E1.m1.7.7.1.1.3.2.3">𝐾</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1"><times id="S3.E1.m1.7.7.1.1.3.1.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.2"></times><apply id="S3.E1.m1.7.7.1.1.3.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1.3">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.3.2">𝑤</ci><ci id="S3.E1.m1.7.7.1.1.3.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.1.3.3">𝑘</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.1.4.cmml" xref="S3.E1.m1.7.7.1.1.3.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.4.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1.4">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.1.4.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.4.2">ℒ</ci><ci id="S3.E1.m1.7.7.1.1.3.1.4.3.cmml" xref="S3.E1.m1.7.7.1.1.3.1.4.3">𝑘</ci></apply><list id="S3.E1.m1.7.7.1.1.3.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.1.1"><apply id="S3.E1.m1.7.7.1.1.3.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1.2">𝑋</ci><ci id="S3.E1.m1.7.7.1.1.3.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3.1.1.1.1.3">𝑘</ci></apply><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">italic-ϕ</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\displaystyle\min_{\phi}\mathcal{L}(X;\phi)\quad\text{with}\quad\mathcal{L}(X;\phi)=\sum_{k=1}^{K}w_{k}\;\mathcal{L}_{k}(X_{k};\phi),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.6" class="ltx_p">where <math id="S3.SS1.p1.5.m1.1" class="ltx_Math" alttext="w_{k}&gt;0" display="inline"><semantics id="S3.SS1.p1.5.m1.1a"><mrow id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml"><msub id="S3.SS1.p1.5.m1.1.1.2" xref="S3.SS1.p1.5.m1.1.1.2.cmml"><mi id="S3.SS1.p1.5.m1.1.1.2.2" xref="S3.SS1.p1.5.m1.1.1.2.2.cmml">w</mi><mi id="S3.SS1.p1.5.m1.1.1.2.3" xref="S3.SS1.p1.5.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.SS1.p1.5.m1.1.1.1" xref="S3.SS1.p1.5.m1.1.1.1.cmml">&gt;</mo><mn id="S3.SS1.p1.5.m1.1.1.3" xref="S3.SS1.p1.5.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b"><apply id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1"><gt id="S3.SS1.p1.5.m1.1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1.1"></gt><apply id="S3.SS1.p1.5.m1.1.1.2.cmml" xref="S3.SS1.p1.5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m1.1.1.2.1.cmml" xref="S3.SS1.p1.5.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.5.m1.1.1.2.2.cmml" xref="S3.SS1.p1.5.m1.1.1.2.2">𝑤</ci><ci id="S3.SS1.p1.5.m1.1.1.2.3.cmml" xref="S3.SS1.p1.5.m1.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS1.p1.5.m1.1.1.3.cmml" xref="S3.SS1.p1.5.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">w_{k}&gt;0</annotation></semantics></math> denote the respective weight coefficients.
It is important to note that the data <math id="S3.SS1.p1.6.m2.1" class="ltx_Math" alttext="X_{k}" display="inline"><semantics id="S3.SS1.p1.6.m2.1a"><msub id="S3.SS1.p1.6.m2.1.1" xref="S3.SS1.p1.6.m2.1.1.cmml"><mi id="S3.SS1.p1.6.m2.1.1.2" xref="S3.SS1.p1.6.m2.1.1.2.cmml">X</mi><mi id="S3.SS1.p1.6.m2.1.1.3" xref="S3.SS1.p1.6.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m2.1b"><apply id="S3.SS1.p1.6.m2.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m2.1.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m2.1.1.2.cmml" xref="S3.SS1.p1.6.m2.1.1.2">𝑋</ci><ci id="S3.SS1.p1.6.m2.1.1.3.cmml" xref="S3.SS1.p1.6.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m2.1c">X_{k}</annotation></semantics></math> is never shared among parties and remains private throughout learning.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In practice, each participant typically obtains and refines the global consensus model by running a few rounds of optimisation on their local data and then shares the updated parameters with its peers, either directly or via a parameter server.
The more rounds of local training are performed without sharing updates or synchronisation, the less it is guaranteed that the actual procedure is minimising the equation (<a href="#S3.E1" title="In 3.1 Federated Learning Definition ‣ 3 Technical Considerations ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The actual process used for aggregating parameters commonly depends on the FL network topology, as FL nodes might be segregated into sub-networks due to geographical or legal constraints (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Aggregation strategies can rely on a single aggregating node (hub and spokes models), or on multiple nodes without any centralisation. An example of this is peer-to-peer FL, where connections exist between all or a subset of the participants and model updates are shared only between directly-connected sites <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
An example of centralised FL aggregation with a client-server architecture is given in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.1 Federated Learning Definition ‣ 3 Technical Considerations ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Note that aggregation strategies do not necessarily require information about the full model update; clients might choose to share only a subset of the model parameters for the sake of reducing communication overhead of redundant information, ensure better privacy preservation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> or to produce multi-task learning algorithms having only part of their parameters learned in a federated manner.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">A unifying framework enabling various training schemes may disentangle compute resources (data and servers) from the <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">compute plan</span>, as depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 The Promise of Federated Efforts ‣ 2 Data-driven Medicine Requires Federated Efforts ‣ The Future of Digital Health with Federated Learningfootnote footnoteDisclaimer: The opinions expressed herein are those of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g. the U.S. Department of Health and Human Services or the National Institutes of Health. This is a pre-print version of https://www.nature.com/articles/s41746-020-00323-1" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The latter defines the trajectory of a model across several partners, to be trained and evaluated on specific datasets.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">For more details regarding state-of-the-art of FL techniques, such as aggregation methods, optimisation or model compression, we refer the reader to the overview by Kairouz et al.  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.12.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Example of a FL algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> in a client-server architecture with aggregation via <span id="alg1.13.2" class="ltx_text ltx_font_typewriter">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. </figcaption>
<div id="alg1.14" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text" style="font-size:90%;">num_federated_rounds </span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l1.m1.1a"><mi mathsize="90%" id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">T</annotation></semantics></math><span id="alg1.l1.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold" style="font-size:90%;">procedure</span><span id="alg1.l2.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l2.4" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Aggregating</span><span id="alg1.l2.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text" style="font-size:90%;">    Initialise global model: </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="W^{(0)}" display="inline"><semantics id="alg1.l3.m1.1a"><msup id="alg1.l3.m1.1.2" xref="alg1.l3.m1.1.2.cmml"><mi mathsize="90%" id="alg1.l3.m1.1.2.2" xref="alg1.l3.m1.1.2.2.cmml">W</mi><mrow id="alg1.l3.m1.1.1.1.3" xref="alg1.l3.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l3.m1.1.1.1.3.1" xref="alg1.l3.m1.1.2.cmml">(</mo><mn mathsize="90%" id="alg1.l3.m1.1.1.1.1" xref="alg1.l3.m1.1.1.1.1.cmml">0</mn><mo maxsize="90%" minsize="90%" id="alg1.l3.m1.1.1.1.3.2" xref="alg1.l3.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.2.cmml" xref="alg1.l3.m1.1.2"><csymbol cd="ambiguous" id="alg1.l3.m1.1.2.1.cmml" xref="alg1.l3.m1.1.2">superscript</csymbol><ci id="alg1.l3.m1.1.2.2.cmml" xref="alg1.l3.m1.1.2.2">𝑊</ci><cn type="integer" id="alg1.l3.m1.1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1.1">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">W^{(0)}</annotation></semantics></math><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l4.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="t\leftarrow 1\cdots T" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">t</mi><mo mathsize="90%" stretchy="false" id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">←</mo><mrow id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml"><mn mathsize="90%" id="alg1.l4.m1.1.1.3.2" xref="alg1.l4.m1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="alg1.l4.m1.1.1.3.3" xref="alg1.l4.m1.1.1.3.3.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1a" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="alg1.l4.m1.1.1.3.4" xref="alg1.l4.m1.1.1.3.4.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><ci id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1">←</ci><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝑡</ci><apply id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3"><times id="alg1.l4.m1.1.1.3.1.cmml" xref="alg1.l4.m1.1.1.3.1"></times><cn type="integer" id="alg1.l4.m1.1.1.3.2.cmml" xref="alg1.l4.m1.1.1.3.2">1</cn><ci id="alg1.l4.m1.1.1.3.3.cmml" xref="alg1.l4.m1.1.1.3.3">⋯</ci><ci id="alg1.l4.m1.1.1.3.4.cmml" xref="alg1.l4.m1.1.1.3.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">t\leftarrow 1\cdots T</annotation></semantics></math><span id="alg1.l4.5" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l4.6" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l4.7" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.2.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.3" class="ltx_text" style="font-size:90%;">         </span><span id="alg1.l5.4" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l5.5" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="client\ k\leftarrow 1\cdots K" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mrow id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l5.m1.1.1.2.2" xref="alg1.l5.m1.1.1.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.2.1" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.3" xref="alg1.l5.m1.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.2.1a" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.4" xref="alg1.l5.m1.1.1.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.2.1b" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.5" xref="alg1.l5.m1.1.1.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.2.1c" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.6" xref="alg1.l5.m1.1.1.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.2.1d" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.7" xref="alg1.l5.m1.1.1.2.7.cmml">t</mi><mo lspace="0.450em" rspace="0em" id="alg1.l5.m1.1.1.2.1e" xref="alg1.l5.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.2.8" xref="alg1.l5.m1.1.1.2.8.cmml">k</mi></mrow><mo mathsize="90%" stretchy="false" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">←</mo><mrow id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"><mn mathsize="90%" id="alg1.l5.m1.1.1.3.2" xref="alg1.l5.m1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="alg1.l5.m1.1.1.3.3" xref="alg1.l5.m1.1.1.3.3.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1a" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.3.4" xref="alg1.l5.m1.1.1.3.4.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">←</ci><apply id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2"><times id="alg1.l5.m1.1.1.2.1.cmml" xref="alg1.l5.m1.1.1.2.1"></times><ci id="alg1.l5.m1.1.1.2.2.cmml" xref="alg1.l5.m1.1.1.2.2">𝑐</ci><ci id="alg1.l5.m1.1.1.2.3.cmml" xref="alg1.l5.m1.1.1.2.3">𝑙</ci><ci id="alg1.l5.m1.1.1.2.4.cmml" xref="alg1.l5.m1.1.1.2.4">𝑖</ci><ci id="alg1.l5.m1.1.1.2.5.cmml" xref="alg1.l5.m1.1.1.2.5">𝑒</ci><ci id="alg1.l5.m1.1.1.2.6.cmml" xref="alg1.l5.m1.1.1.2.6">𝑛</ci><ci id="alg1.l5.m1.1.1.2.7.cmml" xref="alg1.l5.m1.1.1.2.7">𝑡</ci><ci id="alg1.l5.m1.1.1.2.8.cmml" xref="alg1.l5.m1.1.1.2.8">𝑘</ci></apply><apply id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3"><times id="alg1.l5.m1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.3.1"></times><cn type="integer" id="alg1.l5.m1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.3.2">1</cn><ci id="alg1.l5.m1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.3.3">⋯</ci><ci id="alg1.l5.m1.1.1.3.4.cmml" xref="alg1.l5.m1.1.1.3.4">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">client\ k\leftarrow 1\cdots K</annotation></semantics></math><span id="alg1.l5.6" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l5.7" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l5.8" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l5.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg1.l5.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l5.1.m1.1a"><mo id="alg1.l5.1.m1.1.1" xref="alg1.l5.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l5.1.m1.1b"><ci id="alg1.l5.1.m1.1.1.cmml" xref="alg1.l5.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.1.m1.1c">\triangleright</annotation></semantics></math>  <span id="alg1.l5.1.1" class="ltx_text ltx_font_italic">Run in parallel</span>
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:90%;">             Send </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="W^{(t-1)}" display="inline"><semantics id="alg1.l6.m1.1a"><msup id="alg1.l6.m1.1.2" xref="alg1.l6.m1.1.2.cmml"><mi mathsize="90%" id="alg1.l6.m1.1.2.2" xref="alg1.l6.m1.1.2.2.cmml">W</mi><mrow id="alg1.l6.m1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l6.m1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m1.1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l6.m1.1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l6.m1.1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.2.cmml" xref="alg1.l6.m1.1.2"><csymbol cd="ambiguous" id="alg1.l6.m1.1.2.1.cmml" xref="alg1.l6.m1.1.2">superscript</csymbol><ci id="alg1.l6.m1.1.2.2.cmml" xref="alg1.l6.m1.1.2.2">𝑊</ci><apply id="alg1.l6.m1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1"><minus id="alg1.l6.m1.1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1.1.1"></minus><ci id="alg1.l6.m1.1.1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l6.m1.1.1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">W^{(t-1)}</annotation></semantics></math><span id="alg1.l6.3" class="ltx_text" style="font-size:90%;"> to client </span><math id="alg1.l6.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l6.m2.1a"><mi mathsize="90%" id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">k</annotation></semantics></math><span id="alg1.l6.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">             Receive model updates and number of local training iterations </span><math id="alg1.l7.m1.3" class="ltx_Math" alttext="(\Delta W_{k}^{(t-1)},N_{k})" display="inline"><semantics id="alg1.l7.m1.3a"><mrow id="alg1.l7.m1.3.3.2" xref="alg1.l7.m1.3.3.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.3.3.2.3" xref="alg1.l7.m1.3.3.3.cmml">(</mo><mrow id="alg1.l7.m1.2.2.1.1" xref="alg1.l7.m1.2.2.1.1.cmml"><mi mathsize="90%" mathvariant="normal" id="alg1.l7.m1.2.2.1.1.2" xref="alg1.l7.m1.2.2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.2.2.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.cmml">​</mo><msubsup id="alg1.l7.m1.2.2.1.1.3" xref="alg1.l7.m1.2.2.1.1.3.cmml"><mi mathsize="90%" id="alg1.l7.m1.2.2.1.1.3.2.2" xref="alg1.l7.m1.2.2.1.1.3.2.2.cmml">W</mi><mi mathsize="90%" id="alg1.l7.m1.2.2.1.1.3.2.3" xref="alg1.l7.m1.2.2.1.1.3.2.3.cmml">k</mi><mrow id="alg1.l7.m1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l7.m1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup></mrow><mo mathsize="90%" id="alg1.l7.m1.3.3.2.4" xref="alg1.l7.m1.3.3.3.cmml">,</mo><msub id="alg1.l7.m1.3.3.2.2" xref="alg1.l7.m1.3.3.2.2.cmml"><mi mathsize="90%" id="alg1.l7.m1.3.3.2.2.2" xref="alg1.l7.m1.3.3.2.2.2.cmml">N</mi><mi mathsize="90%" id="alg1.l7.m1.3.3.2.2.3" xref="alg1.l7.m1.3.3.2.2.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.3.3.2.5" xref="alg1.l7.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.3b"><interval closure="open" id="alg1.l7.m1.3.3.3.cmml" xref="alg1.l7.m1.3.3.2"><apply id="alg1.l7.m1.2.2.1.1.cmml" xref="alg1.l7.m1.2.2.1.1"><times id="alg1.l7.m1.2.2.1.1.1.cmml" xref="alg1.l7.m1.2.2.1.1.1"></times><ci id="alg1.l7.m1.2.2.1.1.2.cmml" xref="alg1.l7.m1.2.2.1.1.2">Δ</ci><apply id="alg1.l7.m1.2.2.1.1.3.cmml" xref="alg1.l7.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.1.1.3.1.cmml" xref="alg1.l7.m1.2.2.1.1.3">superscript</csymbol><apply id="alg1.l7.m1.2.2.1.1.3.2.cmml" xref="alg1.l7.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.1.1.3.2.1.cmml" xref="alg1.l7.m1.2.2.1.1.3">subscript</csymbol><ci id="alg1.l7.m1.2.2.1.1.3.2.2.cmml" xref="alg1.l7.m1.2.2.1.1.3.2.2">𝑊</ci><ci id="alg1.l7.m1.2.2.1.1.3.2.3.cmml" xref="alg1.l7.m1.2.2.1.1.3.2.3">𝑘</ci></apply><apply id="alg1.l7.m1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1"><minus id="alg1.l7.m1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1"></minus><ci id="alg1.l7.m1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l7.m1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="alg1.l7.m1.3.3.2.2.cmml" xref="alg1.l7.m1.3.3.2.2"><csymbol cd="ambiguous" id="alg1.l7.m1.3.3.2.2.1.cmml" xref="alg1.l7.m1.3.3.2.2">subscript</csymbol><ci id="alg1.l7.m1.3.3.2.2.2.cmml" xref="alg1.l7.m1.3.3.2.2.2">𝑁</ci><ci id="alg1.l7.m1.3.3.2.2.3.cmml" xref="alg1.l7.m1.3.3.2.2.3">𝑘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.3c">(\Delta W_{k}^{(t-1)},N_{k})</annotation></semantics></math><span id="alg1.l7.3" class="ltx_text" style="font-size:90%;"> from client’s local training with </span><math id="alg1.l7.m2.3" class="ltx_Math" alttext="\mathcal{L}_{k}(X_{k};W^{(t-1)})" display="inline"><semantics id="alg1.l7.m2.3a"><mrow id="alg1.l7.m2.3.3" xref="alg1.l7.m2.3.3.cmml"><msub id="alg1.l7.m2.3.3.4" xref="alg1.l7.m2.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l7.m2.3.3.4.2" xref="alg1.l7.m2.3.3.4.2.cmml">ℒ</mi><mi mathsize="90%" id="alg1.l7.m2.3.3.4.3" xref="alg1.l7.m2.3.3.4.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="alg1.l7.m2.3.3.3" xref="alg1.l7.m2.3.3.3.cmml">​</mo><mrow id="alg1.l7.m2.3.3.2.2" xref="alg1.l7.m2.3.3.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m2.3.3.2.2.3" xref="alg1.l7.m2.3.3.2.3.cmml">(</mo><msub id="alg1.l7.m2.2.2.1.1.1" xref="alg1.l7.m2.2.2.1.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m2.2.2.1.1.1.2" xref="alg1.l7.m2.2.2.1.1.1.2.cmml">X</mi><mi mathsize="90%" id="alg1.l7.m2.2.2.1.1.1.3" xref="alg1.l7.m2.2.2.1.1.1.3.cmml">k</mi></msub><mo mathsize="90%" id="alg1.l7.m2.3.3.2.2.4" xref="alg1.l7.m2.3.3.2.3.cmml">;</mo><msup id="alg1.l7.m2.3.3.2.2.2" xref="alg1.l7.m2.3.3.2.2.2.cmml"><mi mathsize="90%" id="alg1.l7.m2.3.3.2.2.2.2" xref="alg1.l7.m2.3.3.2.2.2.2.cmml">W</mi><mrow id="alg1.l7.m2.1.1.1.1" xref="alg1.l7.m2.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m2.1.1.1.1.2" xref="alg1.l7.m2.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l7.m2.1.1.1.1.1" xref="alg1.l7.m2.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m2.1.1.1.1.1.2" xref="alg1.l7.m2.1.1.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l7.m2.1.1.1.1.1.1" xref="alg1.l7.m2.1.1.1.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l7.m2.1.1.1.1.1.3" xref="alg1.l7.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l7.m2.1.1.1.1.3" xref="alg1.l7.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><mo maxsize="90%" minsize="90%" id="alg1.l7.m2.3.3.2.2.5" xref="alg1.l7.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.3b"><apply id="alg1.l7.m2.3.3.cmml" xref="alg1.l7.m2.3.3"><times id="alg1.l7.m2.3.3.3.cmml" xref="alg1.l7.m2.3.3.3"></times><apply id="alg1.l7.m2.3.3.4.cmml" xref="alg1.l7.m2.3.3.4"><csymbol cd="ambiguous" id="alg1.l7.m2.3.3.4.1.cmml" xref="alg1.l7.m2.3.3.4">subscript</csymbol><ci id="alg1.l7.m2.3.3.4.2.cmml" xref="alg1.l7.m2.3.3.4.2">ℒ</ci><ci id="alg1.l7.m2.3.3.4.3.cmml" xref="alg1.l7.m2.3.3.4.3">𝑘</ci></apply><list id="alg1.l7.m2.3.3.2.3.cmml" xref="alg1.l7.m2.3.3.2.2"><apply id="alg1.l7.m2.2.2.1.1.1.cmml" xref="alg1.l7.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m2.2.2.1.1.1.1.cmml" xref="alg1.l7.m2.2.2.1.1.1">subscript</csymbol><ci id="alg1.l7.m2.2.2.1.1.1.2.cmml" xref="alg1.l7.m2.2.2.1.1.1.2">𝑋</ci><ci id="alg1.l7.m2.2.2.1.1.1.3.cmml" xref="alg1.l7.m2.2.2.1.1.1.3">𝑘</ci></apply><apply id="alg1.l7.m2.3.3.2.2.2.cmml" xref="alg1.l7.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="alg1.l7.m2.3.3.2.2.2.1.cmml" xref="alg1.l7.m2.3.3.2.2.2">superscript</csymbol><ci id="alg1.l7.m2.3.3.2.2.2.2.cmml" xref="alg1.l7.m2.3.3.2.2.2.2">𝑊</ci><apply id="alg1.l7.m2.1.1.1.1.1.cmml" xref="alg1.l7.m2.1.1.1.1"><minus id="alg1.l7.m2.1.1.1.1.1.1.cmml" xref="alg1.l7.m2.1.1.1.1.1.1"></minus><ci id="alg1.l7.m2.1.1.1.1.1.2.cmml" xref="alg1.l7.m2.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l7.m2.1.1.1.1.1.3.cmml" xref="alg1.l7.m2.1.1.1.1.1.3">1</cn></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.3c">\mathcal{L}_{k}(X_{k};W^{(t-1)})</annotation></semantics></math><span id="alg1.l7.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text" style="font-size:90%;">         </span><span id="alg1.l8.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l8.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l8.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text" style="font-size:90%;">         </span><math id="alg1.l9.m1.4" class="ltx_Math" alttext="W^{(t)}\leftarrow W^{(t-1)}+\frac{1}{\sum_{k}{N_{k}}}\sum_{k}{(N_{k}\cdot\Delta W_{k}^{(t-1)})}" display="inline"><semantics id="alg1.l9.m1.4a"><mrow id="alg1.l9.m1.4.4" xref="alg1.l9.m1.4.4.cmml"><msup id="alg1.l9.m1.4.4.3" xref="alg1.l9.m1.4.4.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.4.4.3.2" xref="alg1.l9.m1.4.4.3.2.cmml">W</mi><mrow id="alg1.l9.m1.1.1.1.3" xref="alg1.l9.m1.4.4.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.1.1.1.3.1" xref="alg1.l9.m1.4.4.3.cmml">(</mo><mi mathsize="90%" id="alg1.l9.m1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.1.1.1.3.2" xref="alg1.l9.m1.4.4.3.cmml">)</mo></mrow></msup><mo mathsize="90%" stretchy="false" id="alg1.l9.m1.4.4.2" xref="alg1.l9.m1.4.4.2.cmml">←</mo><mrow id="alg1.l9.m1.4.4.1" xref="alg1.l9.m1.4.4.1.cmml"><msup id="alg1.l9.m1.4.4.1.3" xref="alg1.l9.m1.4.4.1.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.4.4.1.3.2" xref="alg1.l9.m1.4.4.1.3.2.cmml">W</mi><mrow id="alg1.l9.m1.2.2.1.1" xref="alg1.l9.m1.2.2.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.2.2.1.1.2" xref="alg1.l9.m1.2.2.1.1.1.cmml">(</mo><mrow id="alg1.l9.m1.2.2.1.1.1" xref="alg1.l9.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.2.2.1.1.1.2" xref="alg1.l9.m1.2.2.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l9.m1.2.2.1.1.1.1" xref="alg1.l9.m1.2.2.1.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l9.m1.2.2.1.1.1.3" xref="alg1.l9.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.2.2.1.1.3" xref="alg1.l9.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo mathsize="90%" id="alg1.l9.m1.4.4.1.2" xref="alg1.l9.m1.4.4.1.2.cmml">+</mo><mrow id="alg1.l9.m1.4.4.1.1" xref="alg1.l9.m1.4.4.1.1.cmml"><mfrac id="alg1.l9.m1.4.4.1.1.3" xref="alg1.l9.m1.4.4.1.1.3.cmml"><mn mathsize="90%" id="alg1.l9.m1.4.4.1.1.3.2" xref="alg1.l9.m1.4.4.1.1.3.2.cmml">1</mn><mrow id="alg1.l9.m1.4.4.1.1.3.3" xref="alg1.l9.m1.4.4.1.1.3.3.cmml"><mstyle displaystyle="false" id="alg1.l9.m1.4.4.1.1.3.3.1" xref="alg1.l9.m1.4.4.1.1.3.3.1.cmml"><msub id="alg1.l9.m1.4.4.1.1.3.3.1a" xref="alg1.l9.m1.4.4.1.1.3.3.1.cmml"><mo maxsize="63%" minsize="63%" stretchy="true" id="alg1.l9.m1.4.4.1.1.3.3.1.2" xref="alg1.l9.m1.4.4.1.1.3.3.1.2.cmml">∑</mo><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.3.3.1.3" xref="alg1.l9.m1.4.4.1.1.3.3.1.3.cmml">k</mi></msub></mstyle><msub id="alg1.l9.m1.4.4.1.1.3.3.2" xref="alg1.l9.m1.4.4.1.1.3.3.2.cmml"><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.3.3.2.2" xref="alg1.l9.m1.4.4.1.1.3.3.2.2.cmml">N</mi><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.3.3.2.3" xref="alg1.l9.m1.4.4.1.1.3.3.2.3.cmml">k</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em" id="alg1.l9.m1.4.4.1.1.2" xref="alg1.l9.m1.4.4.1.1.2.cmml">​</mo><mrow id="alg1.l9.m1.4.4.1.1.1" xref="alg1.l9.m1.4.4.1.1.1.cmml"><msub id="alg1.l9.m1.4.4.1.1.1.2" xref="alg1.l9.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" rspace="0em" stretchy="true" id="alg1.l9.m1.4.4.1.1.1.2.2" xref="alg1.l9.m1.4.4.1.1.1.2.2.cmml">∑</mo><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.1.2.3" xref="alg1.l9.m1.4.4.1.1.1.2.3.cmml">k</mi></msub><mrow id="alg1.l9.m1.4.4.1.1.1.1.1" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.2" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l9.m1.4.4.1.1.1.1.1.1" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="alg1.l9.m1.4.4.1.1.1.1.1.1.2" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.cmml"><msub id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.cmml"><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.2" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">N</mi><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.3" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.1" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.1.cmml">⋅</mo><mi mathsize="90%" mathvariant="normal" id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.3" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.3.cmml">Δ</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l9.m1.4.4.1.1.1.1.1.1.1" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="alg1.l9.m1.4.4.1.1.1.1.1.1.3" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.2" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.2.cmml">W</mi><mi mathsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.3" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.3.cmml">k</mi><mrow id="alg1.l9.m1.3.3.1.1" xref="alg1.l9.m1.3.3.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.3.3.1.1.2" xref="alg1.l9.m1.3.3.1.1.1.cmml">(</mo><mrow id="alg1.l9.m1.3.3.1.1.1" xref="alg1.l9.m1.3.3.1.1.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.3.3.1.1.1.2" xref="alg1.l9.m1.3.3.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l9.m1.3.3.1.1.1.1" xref="alg1.l9.m1.3.3.1.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l9.m1.3.3.1.1.1.3" xref="alg1.l9.m1.3.3.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.3.3.1.1.3" xref="alg1.l9.m1.3.3.1.1.1.cmml">)</mo></mrow></msubsup></mrow><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.4.4.1.1.1.1.1.3" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.4b"><apply id="alg1.l9.m1.4.4.cmml" xref="alg1.l9.m1.4.4"><ci id="alg1.l9.m1.4.4.2.cmml" xref="alg1.l9.m1.4.4.2">←</ci><apply id="alg1.l9.m1.4.4.3.cmml" xref="alg1.l9.m1.4.4.3"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.3.1.cmml" xref="alg1.l9.m1.4.4.3">superscript</csymbol><ci id="alg1.l9.m1.4.4.3.2.cmml" xref="alg1.l9.m1.4.4.3.2">𝑊</ci><ci id="alg1.l9.m1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1">𝑡</ci></apply><apply id="alg1.l9.m1.4.4.1.cmml" xref="alg1.l9.m1.4.4.1"><plus id="alg1.l9.m1.4.4.1.2.cmml" xref="alg1.l9.m1.4.4.1.2"></plus><apply id="alg1.l9.m1.4.4.1.3.cmml" xref="alg1.l9.m1.4.4.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.3.1.cmml" xref="alg1.l9.m1.4.4.1.3">superscript</csymbol><ci id="alg1.l9.m1.4.4.1.3.2.cmml" xref="alg1.l9.m1.4.4.1.3.2">𝑊</ci><apply id="alg1.l9.m1.2.2.1.1.1.cmml" xref="alg1.l9.m1.2.2.1.1"><minus id="alg1.l9.m1.2.2.1.1.1.1.cmml" xref="alg1.l9.m1.2.2.1.1.1.1"></minus><ci id="alg1.l9.m1.2.2.1.1.1.2.cmml" xref="alg1.l9.m1.2.2.1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l9.m1.2.2.1.1.1.3.cmml" xref="alg1.l9.m1.2.2.1.1.1.3">1</cn></apply></apply><apply id="alg1.l9.m1.4.4.1.1.cmml" xref="alg1.l9.m1.4.4.1.1"><times id="alg1.l9.m1.4.4.1.1.2.cmml" xref="alg1.l9.m1.4.4.1.1.2"></times><apply id="alg1.l9.m1.4.4.1.1.3.cmml" xref="alg1.l9.m1.4.4.1.1.3"><divide id="alg1.l9.m1.4.4.1.1.3.1.cmml" xref="alg1.l9.m1.4.4.1.1.3"></divide><cn type="integer" id="alg1.l9.m1.4.4.1.1.3.2.cmml" xref="alg1.l9.m1.4.4.1.1.3.2">1</cn><apply id="alg1.l9.m1.4.4.1.1.3.3.cmml" xref="alg1.l9.m1.4.4.1.1.3.3"><apply id="alg1.l9.m1.4.4.1.1.3.3.1.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.1"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.3.3.1.1.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.1">subscript</csymbol><sum id="alg1.l9.m1.4.4.1.1.3.3.1.2.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.1.2"></sum><ci id="alg1.l9.m1.4.4.1.1.3.3.1.3.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.1.3">𝑘</ci></apply><apply id="alg1.l9.m1.4.4.1.1.3.3.2.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.2"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.3.3.2.1.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.2">subscript</csymbol><ci id="alg1.l9.m1.4.4.1.1.3.3.2.2.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.2.2">𝑁</ci><ci id="alg1.l9.m1.4.4.1.1.3.3.2.3.cmml" xref="alg1.l9.m1.4.4.1.1.3.3.2.3">𝑘</ci></apply></apply></apply><apply id="alg1.l9.m1.4.4.1.1.1.cmml" xref="alg1.l9.m1.4.4.1.1.1"><apply id="alg1.l9.m1.4.4.1.1.1.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.1.2.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.2">subscript</csymbol><sum id="alg1.l9.m1.4.4.1.1.1.2.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.2.2"></sum><ci id="alg1.l9.m1.4.4.1.1.1.2.3.cmml" xref="alg1.l9.m1.4.4.1.1.1.2.3">𝑘</ci></apply><apply id="alg1.l9.m1.4.4.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1"><times id="alg1.l9.m1.4.4.1.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.1"></times><apply id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2"><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.1">⋅</ci><apply id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.2">𝑁</ci><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.3.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.2.3">𝑘</ci></apply><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.2.3">Δ</ci></apply><apply id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3">superscript</csymbol><apply id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.1.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.2.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.2">𝑊</ci><ci id="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.3.cmml" xref="alg1.l9.m1.4.4.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="alg1.l9.m1.3.3.1.1.1.cmml" xref="alg1.l9.m1.3.3.1.1"><minus id="alg1.l9.m1.3.3.1.1.1.1.cmml" xref="alg1.l9.m1.3.3.1.1.1.1"></minus><ci id="alg1.l9.m1.3.3.1.1.1.2.cmml" xref="alg1.l9.m1.3.3.1.1.1.2">𝑡</ci><cn type="integer" id="alg1.l9.m1.3.3.1.1.1.3.cmml" xref="alg1.l9.m1.3.3.1.1.1.3">1</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.4c">W^{(t)}\leftarrow W^{(t-1)}+\frac{1}{\sum_{k}{N_{k}}}\sum_{k}{(N_{k}\cdot\Delta W_{k}^{(t-1)})}</annotation></semantics></math><span id="alg1.l9.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l10.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l10.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l10.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg1.l11.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l11.3" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="alg1.l11.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l11.m1.1" class="ltx_Math" alttext="W^{(t)}" display="inline"><semantics id="alg1.l11.m1.1a"><msup id="alg1.l11.m1.1.2" xref="alg1.l11.m1.1.2.cmml"><mi mathsize="90%" id="alg1.l11.m1.1.2.2" xref="alg1.l11.m1.1.2.2.cmml">W</mi><mrow id="alg1.l11.m1.1.1.1.3" xref="alg1.l11.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l11.m1.1.1.1.3.1" xref="alg1.l11.m1.1.2.cmml">(</mo><mi mathsize="90%" id="alg1.l11.m1.1.1.1.1" xref="alg1.l11.m1.1.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l11.m1.1.1.1.3.2" xref="alg1.l11.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.2.cmml" xref="alg1.l11.m1.1.2"><csymbol cd="ambiguous" id="alg1.l11.m1.1.2.1.cmml" xref="alg1.l11.m1.1.2">superscript</csymbol><ci id="alg1.l11.m1.1.2.2.cmml" xref="alg1.l11.m1.1.2.2">𝑊</ci><ci id="alg1.l11.m1.1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">W^{(t)}</annotation></semantics></math><span id="alg1.l11.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg1.l12.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l12.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l12.4" class="ltx_text ltx_font_bold" style="font-size:90%;">procedure</span>
</div>
</div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Relation to Similar Strategies</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">FL is rooted in older forms of collaborative learning where models are shared or compute is distributed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Transfer Learning, for example, is a well-established approach of model-sharing that makes it possible to tackle problems with deep neural networks that have millions of parameters, despite the lack of extensive, local datasets that are required for training from scratch:
a model is first trained on a large dataset and then further optimised on the actual target data. The dataset used for the initial training does not necessarily come from the same domain or even the same type of data source as the target dataset. This type of transfer learning has shown better performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> when compared to strategies where the model had been trained from scratch on the target data only, especially when the target dataset is comparably small.
It should be noted that similar to a FL setup, the data is not necessarily co-located in this approach. For Transfer Learning, however, the models are usually shared acyclically, e.g. using a pre-trained model to fine-tune it on another task, without contributing to a collective knowledge-gain. And, unfortunately, deep learning models tend to ”forget” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Therefore after a few training iterations on the target dataset the initial information contained in the model is lost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
To adopt this approach into a form of collaborative learning in a FL setup with continuous learning from different institutions, the participants can share their model with a peer-to-peer architecture in a ”round-robin” or parallel fashion and train in turn on their local data.
This yields better results when the goal is to learn from diverse datasets. A client-server architecture in this scenario enables learning on multi-party data at the same time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, possibly even without forgetting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">There are also other collaborative learning strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> such as ensembling, a statistical strategy of combining multiple independently trained models or predictions into a consensus, or multi-task learning, a strategy to leverage shared representations for related tasks.
These strategies are independent of the concept of FL, and can be used in combination with it.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The second characteristic of FL - to distribute the compute - has been well studied in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
Nowadays, the training of the large-scale models is often executed on multiple devices and even multiple nodes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. In this way, the task can be parallelised and enables fast training, such as training a neural network on the extensive dataset of the ImageNet project in 1 hour <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> or even in less than 80 seconds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
It should be noted that in these scenarios, the training is realised in a cluster environment, with centralised data and fast network communication.
So, distributing the compute for training on several nodes is feasible and FL may benefit from the advances in this area.
Compared to these approaches, however, FL comes with a significant communication and synchronisation cost.
In the FL setup, the compute resources are not as closely connected as in a cluster and every exchange may introduce a significant latency. Therefore, it may not be suitable to synchronise after every batch, but to continue local training for several iterations before aggregation.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">We refer the reader to the survey by Xu <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for an overview of the evolution of Federated Learning and the different concepts in the broader sense.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Challenges and Considerations</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Despite the advantages of FL, there are challenges that need to be taken into account when establishing federated training efforts.
In this section, we discuss five key aspects of FL that are of particular interest in the context of its application to digital health.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Privacy and Security</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">In healthcare, we work with highly sensitive data that must be protected accordingly. Therefore, some of the key considerations are the trade-offs, strategies and remaining risks regarding the privacy-preserving potential of FL.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p"><span id="S3.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Privacy Vs. Performance.</span> Although one of the main purposes of FL is to protect privacy by sharing model updates rather than data, FL does not solve all potential privacy issues and - similar to ML algorithms in general - will always carry some risks.
Strict regulations and data governance policies make any leakage, or perceived risk of leakage, of private information unacceptable. These regulations may even differ between federations and a catch-all solution will likely never exist. Consequently, it is important that potential adopters of FL are aware of potential risks and state-of-the-art options for mitigating them.
Privacy-preserving techniques for FL offer levels of protection that exceed today’s current commercially available ML models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
However, there is a trade-off in terms of performance and these techniques may affect for example the accuracy of the final model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Furthermore future techniques and/or ancillary data could be used to compromise a model previously considered to be low-risk.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p"><span id="S3.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Level of Trust</span>. Broadly speaking, participating parties can enter two types of FL collaboration:</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p"><span id="S3.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_italic">Trusted</span> - for FL consortia in which all parties are considered trustworthy and are bound by an enforceable collaboration agreement, we can eliminate many of the more nefarious motivations, such as deliberate attempts to extract sensitive information or to intentionally corrupt the model. This reduces the need for sophisticated counter-measures, falling back to the principles of standard collaborative research.</p>
</div>
<div id="S3.SS3.SSS1.p5" class="ltx_para">
<p id="S3.SS3.SSS1.p5.1" class="ltx_p"><span id="S3.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_italic">Non-trusted</span> - in FL systems that operate on larger scales, it is impractical to establish an enforceable collaborative agreement that can guarantee that all of the parties are acting benignly. Some may deliberately try to degrade performance, bring the system down or extract information from other parties.
In such an environment, security strategies will be required to mitigate these risks such as, encryption of model submissions, secure authentication of all parties, traceability of actions, differential privacy, verification systems, execution integrity, model confidentiality and protections against adversarial attacks.</p>
</div>
<div id="S3.SS3.SSS1.p6" class="ltx_para">
<p id="S3.SS3.SSS1.p6.1" class="ltx_p"><span id="S3.SS3.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Information leakage.</span>
By definition, FL systems sidestep the need to share healthcare data among participating institutions.
However, the shared information may still indirectly expose private data used for local training, for example by model inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> of the model updates, the gradients themselves <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> or adversarial attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. FL is different from traditional training insofar as the training process is exposed to multiple parties.
As a result, the risk of leakage via reverse-engineering increases if adversaries can observe model changes over time, observe specific model updates (i.e. a single institution’s update), or manipulate the model (e.g. induce additional memorisation by others through gradient-ascent-style attacks). Countermeasures, such as limiting the granularity of the shared model updates and to add specific noise to ensure differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> may be needed and is still an active area of research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Data heterogeneity</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Medical data is particularly diverse - not only in terms of type, dimensionality and characteristics of medical data in general but also within a defined medical task, due to factors like acquisition protocol, brand of the medical device or local demographics. This poses a challenge for FL algorithms and strategies: one of the core assumptions of many current approaches is that the data is independent and identically distributed (IID) across the participants.
Initial results indicate that FL training on medical non-IID data is possible, even if the data is not uniformly distributed across the institutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
In general however, strategies such as <span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> are prone to fail under these conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, in part defeating the very purpose of collaborative learning strategies. Research addressing this problem includes for example <em id="S3.SS3.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">FedProx</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and part-data-sharing strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
Another challenge is that data heterogeneity may lead to a situation in which the global solution may not be the optimal final local solution.
The definition of model training optimality should therefore be agreed by all participants before training.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Traceability and accountability</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">As per all safety-critical applications, the reproducibility of a system is important for FL in healthcare. In contrast to training on centralised data, FL involves running multi-party computations in environments that exhibit complexities in terms of hardware, software and networks.
The traceability requirement should be fulfilled to ensure that system events, data access history and training configuration changes, such as hyperparameter tuning, can be traced during the training processes.
Traceability can also be used to log the training history of a model and, in particular, to avoid the training dataset overlapping with the test dataset.
In particular in non-trusted federations, traceability and accountability processes running in require execution integrity.
After the training process reaches the mutually agreed model optimality criteria, it may also be helpful to measure the amount of contribution from each participant, such as computational resources consumed, quality of the local training data used for local training etc. The measurements could then be used to determine relevant compensation and establish a revenue model among the participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">One implication of FL is that researchers are not able to investigate images upon which models are being trained.
So, although each site will have access to its own raw data, federations may decide to provide some sort of secure intra-node viewing facility to cater for this need or perhaps even some utility for explainability and interpretability of the global model. However, the issue of interpretability within DL is still an open research question.</p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>System architecture</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p">Unlike running large-scale FL amongst consumer devices, healthcare institutional participants are often equipped with better computational resources and
reliable and higher throughput networks. These enable for example training of larger models with larger numbers of local training steps and sharing more model information between nodes.
This unique characteristic of FL in healthcare consequently brings opportunities as well as challenges such as (1) how to ensure data integrity when communicating (e.g. creating redundant nodes); (2) how to design secure encryption methods to take advantage of the computational resources; (3) how to design appropriate node schedulers and make use of the distributed computational devices to reduce idle time.</p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p id="S3.SS3.SSS4.p2.1" class="ltx_p">The administration of such a federation can be realised in different ways, each of which come with advantages and disadvantages. In high-trust situations, training may operate via some sort of ’honest broker’ system, in which a trusted third party acts as the intermediary and facilitates access to data.
This setup requires an independent entity to control the overall system which may not always be desirable, since it could involve an additional cost and procedural viscosity, but does have the advantage that the precise internal mechanisms can be abstracted away from the clients, making the system more agile and simpler to update.
In a peer-to-peer system each site interacts directly with some or all of the other participants. In other words, there is no gatekeeper function, all protocols must be agreed up-front, which requires significant agreement efforts, and changes must be made in a synchronised fashion by all parties to avoid problems.
And in a trustless-based architecture the platform operator may be cryptographically locked into being honest which creates significant computation overheads whilst securing the protocol.</p>
</div>
</section>
<section id="S3.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5 </span>Initiatives and consortia</h4>

<div id="S3.SS3.SSS5.p1" class="ltx_para">
<p id="S3.SS3.SSS5.p1.1" class="ltx_p">Future efforts to apply artificial intelligence to healthcare tasks may strongly depend on collaborative strategies between multiple institutions rather than large centralised databases belonging to only one hospital or research laboratory. The ability to leverage FL to capture and integrate knowledge acquired and maintained by different institutions provides an opportunity to capture larger data variability and analyse patients across different demographics. Moreover, FL is an opportunity to incorporate multi-expert annotation and multi-centre data acquired with different instruments and techniques. This collaborative effort requires, however, various agreements including definitions of scope, aim and technology which, since it is still novel, may incorporate several unknowns. In this context, large-scale initiatives such as the <em id="S3.SS3.SSS5.p1.1.1" class="ltx_emph ltx_font_italic">MELLODDY</em> project <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://www.imi.europa.eu/projects-results/project-factsheets/melloddy</span></span></span>, the HealthChain project <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.substra.ai/en/healthchain-project</span></span></span>, the Trustworthy Federated Data Analytics (TFDA) and the German Cancer Consortium’s Joint Imaging Platform (JIP) <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.dkfz.de/en/datascience/platforms-initiatives.html</span></span></span> represent pioneering efforts to set the standards for safe, fair and innovative collaboration in healthcare research.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">ML, and particularly DL, has led to a wide range of innovations in the area of digital healthcare.
As all ML methods benefit greatly from the ability to access data that approximates the true global distribution, FL is a promising approach to obtain powerful, accurate, safe, robust and unbiased models. By enabling multiple parties to train collaboratively without the need to exchange or centralise datasets, FL neatly addresses issues related to egress of sensitive medical data.
As a consequence, it may open novel research and business avenues and has the potential to improve patient care globally.
In this article, we have discussed the benefits and the considerations pertinent to FL within the healthcare field.
Not all technical questions have been answered yet and FL will certainly be an active research area throughout the next decade <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Despite this, we truly believe that its potential impact on precision medicine and ultimately improving medical care is very promising.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgement</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work was supported by the UK Research and Innovation London Medical Imaging &amp; Artificial Intelligence Centre for Value-Based Healthcare, by the Wellcome/EPSRC Centre for Medical Engineering (WT203148/Z/16/Z), by the Wellcome Flagship Programme (WT213038/Z/18/Z), by the Intramural Research Programme of the National Institutes of Health (NIH) Clinical Center, by the National Cancer Institute of the NIH under award number U01CA242871, by the National Institute of Neurological Disorders and Stroke of the NIH under award number R01NS042645, as well as by the Helmholtz Initiative and Networking Fund (project “Trustworthy Federated Data Analytics”) and the PRIME programme of the German Academic Exchange Service (DAAD) with funds from the German Federal Ministry of Education and Research (BMBF). The content and opinions expressed in this publication is solely the responsibility of the authors and do not necessarily represent those of the institutions they are affiliated with, e.g., the U.S. Department of Health and Human Services or the National Institutes of Health.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Competing Interests / Financial Disclosure: R.M.S. receives royalties from iCAD, ScanMed, Philips, Translation Holdings and Ping An. His lab has received research support from Ping An and NVIDIA. S.B. is supported by the National Institutes of Health (NIH). M.N.G. is supported by the HealthChain (BPIFrance) and Melloddy (IMI2) projects. A.T. is an employee of Google’s DeepMind. S.O. and M.J.C. are founders and shareholders of Brainminer, llc. The other authors declare no competing interests.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">nature</em>, vol. 521,
no. 7553, p. 436, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. Chartrand, P. M. Cheng, E. Vorontsov, M. Drozdzal, S. Turcotte, C. J. Pal,
S. Kadoury, and A. Tang, “Deep learning: a primer for radiologists,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Radiographics</em>, vol. 37, no. 7, pp. 2113–2131, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev,
S. Blackwell, H. Askham, X. Glorot, B. O’Donoghue, D. Visentin
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Clinically applicable deep learning for diagnosis and
referral in retinal disease,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Nature medicine</em>, vol. 24, no. 9, p.
1342, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting unreasonable
effectiveness of data in deep learning era,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE international conference on computer vision</em>, 2017, pp. 843–852.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. G. Van Panhuis, P. Paul, C. Emerson, J. Grefenstette, R. Wilder, A. J.
Herbst, D. Heymann, and D. S. Burke, “A systematic review of barriers to
data sharing in public health,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">BMC public health</em>, vol. 14, no. 1, p.
1144, 2014.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. Rocher, J. M. Hendrickx, and Y.-A. De Montjoye, “Estimating the success of
re-identifications in incomplete datasets using generative models,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Nature communications</em>, vol. 10, no. 1, pp. 1–9, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F.-C. Yeh, J. M. Vettel, A. Singh, B. Poczos, S. T. Grafton, K. I. Erickson,
W.-Y. I. Tseng, and T. D. Verstynen, “Quantifying differences and
similarities in whole-brain white matter architecture using local connectome
fingerprints,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">PLoS computational biology</em>, vol. 12, no. 11, p.
e1005203, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. Wachinger, P. Golland, W. Kremen, B. Fischl, M. Reuter, A. D. N. Initiative
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Brainprint: A discriminative characterization of brain
morphology,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">NeuroImage</em>, vol. 109, pp. 232–248, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, 2017, pp.
1273–1282.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1908.07873</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, p. 12, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Li, F. Milletarì, D. Xu, N. Rieke, J. Hancox, W. Zhu, M. Baust,
Y. Cheng, S. Ourselin, M. J. Cardoso <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Privacy-preserving
federated brain tumour segmentation,” in <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">International Workshop on
Machine Learning in Medical Imaging</em>.   Springer, 2019, pp. 133–141.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas,
“Multi-institutional deep learning modeling without sharing patient data: A
feasibility study on brain tumor segmentation,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International
MICCAI Brainlesion Workshop</em>.   Springer, 2018, pp. 92–104.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1912.04977</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Xu and F. Wang, “Federated learning for healthcare informatics,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.06270</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
“Ibm’s merge healthcare acquisition,”
https://www.reuters.com/article/us-merge-healthcare-m-a-ibm/ibm-to-buy-merge-healthcare-in-1-billion-deal-idUSKCN0QB1ML20150806,
2015 (accessed February 10, 2020).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
“Nhs scotland’s national safe haven,”
https://www.gov.scot/publications/charter-safe-havens-scotland-handling-unconsented-data-national-health-service-patient-records-support-research-statistics/pages/4/,
2015 (accessed February 10, 2020).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Cuggia and S. Combes, “The french health data hub and the german medical
informatics initiatives: Two national projects to promote data sharing in
healthcare,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Yearbook of medical informatics</em>, vol. 28, no. 01, pp.
195–202, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
“Health data research uk,” https://www.hdruk.ac.uk/, accessed February 10,
2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O. Sporns, G. Tononi, and R. Kötter, “The human connectome: a structural
description of the human brain,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">PLoS computational biology</em>, vol. 1,
no. 4, 2005.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. Sudlow, J. Gallacher, N. Allen, V. Beral, P. Burton, J. Danesh, P. Downey,
P. Elliott, J. Green, M. Landray <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Uk biobank: an open access
resource for identifying the causes of a wide range of complex diseases of
middle and old age,” <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">PLoS medicine</em>, vol. 12, no. 3, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore,
S. Phillips, D. Maffitt, M. Pringle <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The cancer imaging
archive (tcia): maintaining and operating a public information repository,”
<em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">Journal of digital imaging</em>, vol. 26, no. 6, pp. 1045–1057, 2013.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
Hospital-scale chest x-ray database and benchmarks on weakly-supervised
classification and localization of common thorax diseases,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 2097–2106.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Yan, X. Wang, L. Lu, and R. M. Summers, “Deeplesion: automated mining of
large-scale lesion annotations and universal lesion detection with deep
learning,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of medical imaging</em>, vol. 5, no. 3, p. 036501,
2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. Tomczak, P. Czerwińska, and M. Wiznerowicz, “The cancer genome atlas
(tcga): an immeasurable source of knowledge,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Contemporary oncology</em>,
vol. 19, no. 1A, p. A68, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander,
D. Harvey, B. Borowski, P. J. Britson, J. L. Whitwell, C. Ward <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“The alzheimer’s disease neuroimaging initiative (adni): Mri methods,”
<em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">Journal of Magnetic Resonance Imaging: An Official Journal of the
International Society for Magnetic Resonance in Medicine</em>, vol. 27, no. 4,
pp. 685–691, 2008.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink, M. Balkenhol,
P. Bult, A. Halilovic, M. Hermsen, R. van de Loo, R. Vogels <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“1399 h&amp;e-stained sentinel lymph node sections of breast cancer patients:
the camelyon dataset,” <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">GigaScience</em>, vol. 7, no. 6, p. giy065, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
Y. Burren, N. Porz, J. Slotboom, R. Wiest <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The multimodal
brain tumor image segmentation benchmark (brats),” <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">IEEE transactions
on medical imaging</em>, vol. 34, no. 10, pp. 1993–2024, 2014.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani,
B. Van Ginneken, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze
<em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A large annotated medical image dataset for the development
and evaluation of segmentation algorithms,” <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1902.09063</em>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on
Security and Privacy (SP)</em>.   IEEE,
2017, pp. 3–18.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Sablayrolles, M. Douze, Y. Ollivier, C. Schmid, and H. Jégou,
“White-box vs black-box: Bayes optimal strategies for membership
inference,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.11229</em>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep
learning requires rethinking generalization,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1611.03530</em>, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The secret
sharer: Evaluating and testing unintended memorization in neural networks,”
in <em id="bib.bib33.4.4" class="ltx_emph ltx_font_italic">28th <math id="bib.bib33.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib33.1.1.m1.1a"><mo stretchy="false" id="bib.bib33.1.1.m1.1.1" xref="bib.bib33.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib33.1.1.m1.1b"><ci id="bib.bib33.1.1.m1.1.1.cmml" xref="bib.bib33.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib33.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib33.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib33.2.2.m2.1a"><mo stretchy="false" id="bib.bib33.2.2.m2.1.1" xref="bib.bib33.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib33.2.2.m2.1b"><ci id="bib.bib33.2.2.m2.1.1.cmml" xref="bib.bib33.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib33.2.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib33.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib33.3.3.m3.1a"><mo stretchy="false" id="bib.bib33.3.3.m3.1.1" xref="bib.bib33.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib33.3.3.m3.1b"><ci id="bib.bib33.3.3.m3.1.1.cmml" xref="bib.bib33.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib33.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib33.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib33.4.4.m4.1a"><mo stretchy="false" id="bib.bib33.4.4.m4.1.1" xref="bib.bib33.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib33.4.4.m4.1b"><ci id="bib.bib33.4.4.m4.1.1.cmml" xref="bib.bib33.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib33.4.4.m4.1c">\}</annotation></semantics></math> Security
19)</em>, 2019, pp. 267–284.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications
Security</em>.   ACM, 2016, pp. 308–318.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</em>.   ACM, 2015,
pp. 1310–1321.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik,
“Federated optimization: Distributed machine learning for on-device
intelligence,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, and C. Wachinger,
“Braintorrent: A peer-to-peer environment for decentralized federated
learning,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.06731</em>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Lalitha, O. C. Kilinc, T. Javidi, and F. Koushanfar, “Peer-to-peer
federated learning on graphs,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.11173</em>, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
private recurrent language models,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on
Learning Representations (ICLR)</em>, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Chang, N. Balachandar, C. Lam, D. Yi, J. Brown, A. Beers, B. Rosen, D. L.
Rubin, and J. Kalpathy-Cramer, “Distributed deep learning networks among
institutions for medical imaging,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical
Informatics Association</em>, vol. 25, no. 8, pp. 945–954, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
and R. M. Summers, “Deep convolutional neural networks for computer-aided
detection: Cnn architectures, dataset characteristics and transfer
learning,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical imaging</em>, vol. 35, no. 5, pp.
1285–1298, 2016.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
Gotway, and J. Liang, “Convolutional neural networks for medical image
analysis: Full training or fine tuning?” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical
imaging</em>, vol. 35, no. 5, pp. 1299–1312, 2016.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. McCloskey and N. J. Cohen, “Catastrophic interference in connectionist
networks: The sequential learning problem,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Psychology of learning
and motivation</em>.   Elsevier, 1989,
vol. 24, pp. 109–165.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, “An
empirical investigation of catastrophic forgetting in gradient-based neural
networks,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.6211</em>, 2013.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Z. Li and D. Hoiem, “Learning without forgetting,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on
pattern analysis and machine intelligence</em>, vol. 40, no. 12, pp. 2935–2947,
2017.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
N. Shoham, T. Avidor, A. Keren, N. Israel, D. Benditkis, L. Mor-Yosef, and
I. Zeitak, “Overcoming forgetting in federated learning on non-iid data,”
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.07796</em>, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
G. Song and W. Chai, “Collaborative learning for deep neural networks,” in
<em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2018, pp.
1832–1841.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. Ruder, “An overview of multi-task learning in deep neural networks,”
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05098</em>, 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
P. H. Jin, Q. Yuan, F. Iandola, and K. Keutzer, “How to scale distributed deep
learning?” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.04581</em>, 2016.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training
imagenet in 1 hour,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.02677</em>, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep
learning: An in-depth concurrency analysis,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys
(CSUR)</em>, vol. 52, no. 4, pp. 1–43, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
M. Yamazaki, A. Kasagi, A. Tabuchi, T. Honda, M. Miwa, N. Fukumoto, T. Tabaru,
A. Ike, and K. Nakashima, “Yet another accelerated sgd: Resnet-50 training
on imagenet in 74.7 seconds,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.12650</em>, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
B. Wu, S. Zhao, G. Sun, X. Zhang, Z. Su, C. Zeng, and Z. Liu,
“P3sgd: Patient Privacy Preserving SGD for
Regularizing Deep CNNs in Pathological Image Classification,”
<em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv:1905.12883 [cs]</em>, May 2019, arXiv:
1905.12883. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1905.12883" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1905.12883</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances
in Neural Information Processing Systems</em>, 2019, pp. 14 747–14 756.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi, “Beyond inferring
class representatives: User-level privacy leakage from federated learning,”
in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2019-IEEE Conference on Computer Communications</em>.   IEEE, 2019, pp. 2512–2520.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
B. Hitaj, G. Ateniese, and F. Perez-Cruz, “Deep models under the gan:
Information leakage from collaborative deep learning,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security</em>,
ser. CCS ’17.   New York, NY, USA:
Association for Computing Machinery, 2017, p. 603–618. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1145/3133956.3134012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3133956.3134012</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
X. Li, Y. Gu, N. Dvornek, L. Staib, P. Ventola, and J. S. Duncan, “Multi-site
fmri analysis using privacy-preserving federated learning and domain
adaptation: Abide results,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.05647</em>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” 2018.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Communication-efficient learning of deep networks from decentralized
data,” <em id="bib.bib59.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>, 2016.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning
with non-iid data,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/1806.00582, 2018.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for machine
learning,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.02868</em>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.08118" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.08119" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.08119">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.08119" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.08120" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 18:39:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
