<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.10832] Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations</title><meta property="og:description" content="In the growing world of artificial intelligence, federated learning is a distributed learning framework enhanced to preserve the privacy of individuals’ data. Federated learning lays the groundwork for collaborative re…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.10832">

<!--Generated on Tue Feb 27 18:24:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">[1]<span id="p1.1.1" class="ltx_ERROR undefined">\fnm</span>Elaheh <span id="p1.1.2" class="ltx_ERROR undefined">\sur</span>Jafarigol
[1]<span id="p1.1.3" class="ltx_ERROR undefined">\orgdiv</span>Data Science and Analytics Institute, <span id="p1.1.4" class="ltx_ERROR undefined">\orgname</span>University of Oklahoma, <span id="p1.1.5" class="ltx_ERROR undefined">\orgaddress</span><span id="p1.1.6" class="ltx_ERROR undefined">\street</span>202 W. Boyd St., Room 409, <span id="p1.1.7" class="ltx_ERROR undefined">\city</span>Norman, <span id="p1.1.8" class="ltx_ERROR undefined">\postcode</span>73019, <span id="p1.1.9" class="ltx_ERROR undefined">\state</span>Ok, <span id="p1.1.10" class="ltx_ERROR undefined">\country</span>USA
2]<span id="p1.1.11" class="ltx_ERROR undefined">\orgdiv</span>School of Industrial and Systems Engineering, <span id="p1.1.12" class="ltx_ERROR undefined">\orgname</span>University of Oklahoma, <span id="p1.1.13" class="ltx_ERROR undefined">\orgaddress</span><span id="p1.1.14" class="ltx_ERROR undefined">\street</span>202 W Boyd St., Room 124, <span id="p1.1.15" class="ltx_ERROR undefined">\city</span>Norman, <span id="p1.1.16" class="ltx_ERROR undefined">\postcode</span>73019, <span id="p1.1.17" class="ltx_ERROR undefined">\state</span>OK, <span id="p1.1.18" class="ltx_ERROR undefined">\country</span>USA</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">3]<span id="p2.1.1" class="ltx_ERROR undefined">\orgdiv</span>Department of Industrial Engineering, <span id="p2.1.2" class="ltx_ERROR undefined">\orgname</span>Isfahan University of Technology, <span id="p2.1.3" class="ltx_ERROR undefined">\orgaddress</span><span id="p2.1.4" class="ltx_ERROR undefined">\city</span>Isfahan, <span id="p2.1.5" class="ltx_ERROR undefined">\country</span>Iran</p>
</div>
<h1 class="ltx_title ltx_title_document">Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:elaheh.jafarigol@ou.edu">elaheh.jafarigol@ou.edu</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Theodore B. <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Trafalis
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Talayeh <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Razzaghi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Mona<span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Zamankhani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">*
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">In the growing world of artificial intelligence, federated learning is a distributed learning framework enhanced to preserve the privacy of individuals’ data. Federated learning lays the groundwork for collaborative research in areas where the data is sensitive. Federated learning has several implications for real-world problems. In times of crisis, when real-time decision-making is critical, federated learning allows multiple entities to work collectively without sharing sensitive data. This distributed approach enables us to leverage information from multiple sources and gain more diverse insights.
This paper is a systematic review of the literature on privacy-preserving machine learning in the last few years based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Specifically, we have presented an extensive review of supervised/unsupervised machine learning algorithms, ensemble methods, meta-heuristic approaches, blockchain technology, and reinforcement learning used in the framework of federated learning, in addition to an overview of federated learning applications. This paper reviews the literature on the components of federated learning and its applications in the last few years. The main purpose of this work is to provide researchers and practitioners with a comprehensive overview of federated learning from the machine learning point of view. A discussion of some open problems and future research directions in federated learning is also provided.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Federated Learning, Privacy-preserving Machine Learning, Distributed Learning, Supervised/Unsupervised Learning, Artificial Intelligence
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Privacy is the individuals’ right to control their personal information. With the advances in data-driven technologies, ensuring privacy protection has become more challenging. Privacy and federated learning are intertwined concepts in machine learning. Privacy and federated learning are intertwined concepts in machine learning. Federated learning offers a promising solution by enabling collaborative model training without exposing sensitive data. This decentralized approach preserves privacy by design, allowing organizations and individuals to collaborate while minimizing the risk of data breaches or unauthorized access. With privacy at its core, federated learning has become a possible solution for scenarios where the data is sensitive and individual privacy is a concern. An algorithm is considered private if the outcome of the analysis is the same, whether any arbitrary individual data is part of the dataset or not <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This definition is the basis for privacy protection mechanisms in federated learning.
Federated learning is a broad term that includes different aspects of data collection, storage, analysis, and communication in a decentralized information system where data centers can not disclose data for learning purposes. The term federated learning was introduced in the paper published based on the results of a research project carried out at Google in 2016 for text input prediction on mobile devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The authors designed a collaborative environment for a group of devices referred to as clients, coordinated by a central server, also known as a service provider. They also conducted an empirical study on the model using different model structures on benchmark datasets for image classification and language processing applications.
<br class="ltx_break">The ideas behind federated learning have been around for decades, but thanks to the abundant data that is available to us and advances in computation power, we are able to efficiently train machine learning models on a network of decentralized data sources. Advances in the field of machine learning, especially deep learning, allow us to build powerful models that can accurately analyze different data types and provide valuable insights. Without the need to collect data in one place, we can leverage multiple sources of data, knowing that individuals’ privacy is preserved while reducing the costs of data storage and achieving high-quality results. Distributed machine learning algorithms create an environment where data storage and training happen on the group of distributed machines. However, the main difference between distributed learning and federated learning is the notion of privacy, and the advances in data analysis capacities made over the last decade allow us to develop and deploy privacy-preserving algorithms at scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.

<br class="ltx_break">Federated learning is the process of training data locally and improving the global model. In the federated learning framework, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the data is stored in local data centers, and limited information required for the learning task is privately communicated with the central server.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.10832/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The Federated Learning Framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.9" class="ltx_p">This architecture is called a client-server design. If the data centers are both responsible for the task of storage and aggregation, the architecture is called peer-to-peer. Suppose <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">n</annotation></semantics></math> is the total number of sample points. In that case, <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">K</annotation></semantics></math> is the total number of clients, <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S1.p2.3.m3.1a"><msub id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml"><mi id="S1.p2.3.m3.1.1.2" xref="S1.p2.3.m3.1.1.2.cmml">n</mi><mi id="S1.p2.3.m3.1.1.3" xref="S1.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><apply id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S1.p2.3.m3.1.1.1.cmml" xref="S1.p2.3.m3.1.1">subscript</csymbol><ci id="S1.p2.3.m3.1.1.2.cmml" xref="S1.p2.3.m3.1.1.2">𝑛</ci><ci id="S1.p2.3.m3.1.1.3.cmml" xref="S1.p2.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">n_{k}</annotation></semantics></math> is the total number of sample points on client <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p2.4.m4.1a"><mi id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">k</annotation></semantics></math>, and <math id="S1.p2.5.m5.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S1.p2.5.m5.1a"><mi id="S1.p2.5.m5.1.1" xref="S1.p2.5.m5.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S1.p2.5.m5.1b"><ci id="S1.p2.5.m5.1.1.cmml" xref="S1.p2.5.m5.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.5.m5.1c">\eta</annotation></semantics></math> is the learning rate. The goal of federated learning is to minimize the objective function <math id="S1.p2.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S1.p2.6.m6.1a"><mi id="S1.p2.6.m6.1.1" xref="S1.p2.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.p2.6.m6.1b"><ci id="S1.p2.6.m6.1.1.cmml" xref="S1.p2.6.m6.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.6.m6.1c">f</annotation></semantics></math>, also known as the loss function, where <math id="S1.p2.7.m7.1" class="ltx_Math" alttext="f_{k}(\delta)" display="inline"><semantics id="S1.p2.7.m7.1a"><mrow id="S1.p2.7.m7.1.2" xref="S1.p2.7.m7.1.2.cmml"><msub id="S1.p2.7.m7.1.2.2" xref="S1.p2.7.m7.1.2.2.cmml"><mi id="S1.p2.7.m7.1.2.2.2" xref="S1.p2.7.m7.1.2.2.2.cmml">f</mi><mi id="S1.p2.7.m7.1.2.2.3" xref="S1.p2.7.m7.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S1.p2.7.m7.1.2.1" xref="S1.p2.7.m7.1.2.1.cmml">​</mo><mrow id="S1.p2.7.m7.1.2.3.2" xref="S1.p2.7.m7.1.2.cmml"><mo stretchy="false" id="S1.p2.7.m7.1.2.3.2.1" xref="S1.p2.7.m7.1.2.cmml">(</mo><mi id="S1.p2.7.m7.1.1" xref="S1.p2.7.m7.1.1.cmml">δ</mi><mo stretchy="false" id="S1.p2.7.m7.1.2.3.2.2" xref="S1.p2.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.7.m7.1b"><apply id="S1.p2.7.m7.1.2.cmml" xref="S1.p2.7.m7.1.2"><times id="S1.p2.7.m7.1.2.1.cmml" xref="S1.p2.7.m7.1.2.1"></times><apply id="S1.p2.7.m7.1.2.2.cmml" xref="S1.p2.7.m7.1.2.2"><csymbol cd="ambiguous" id="S1.p2.7.m7.1.2.2.1.cmml" xref="S1.p2.7.m7.1.2.2">subscript</csymbol><ci id="S1.p2.7.m7.1.2.2.2.cmml" xref="S1.p2.7.m7.1.2.2.2">𝑓</ci><ci id="S1.p2.7.m7.1.2.2.3.cmml" xref="S1.p2.7.m7.1.2.2.3">𝑘</ci></apply><ci id="S1.p2.7.m7.1.1.cmml" xref="S1.p2.7.m7.1.1">𝛿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.7.m7.1c">f_{k}(\delta)</annotation></semantics></math> is the loss function, and <math id="S1.p2.8.m8.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S1.p2.8.m8.1a"><mi id="S1.p2.8.m8.1.1" xref="S1.p2.8.m8.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S1.p2.8.m8.1b"><ci id="S1.p2.8.m8.1.1.cmml" xref="S1.p2.8.m8.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.8.m8.1c">\delta</annotation></semantics></math> is the evaluation value for the <span id="S1.p2.9.1" class="ltx_text ltx_markedasmath ltx_font_italic">k-th</span> client:</p>
<table id="S1.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S1.E1.m1.2" class="ltx_Math" alttext="f(\delta):=\displaystyle\sum_{k=1}^{K}\frac{n_{k}}{n}f_{k}(\delta)" display="block"><semantics id="S1.E1.m1.2a"><mrow id="S1.E1.m1.2.3" xref="S1.E1.m1.2.3.cmml"><mrow id="S1.E1.m1.2.3.2" xref="S1.E1.m1.2.3.2.cmml"><mi id="S1.E1.m1.2.3.2.2" xref="S1.E1.m1.2.3.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S1.E1.m1.2.3.2.1" xref="S1.E1.m1.2.3.2.1.cmml">​</mo><mrow id="S1.E1.m1.2.3.2.3.2" xref="S1.E1.m1.2.3.2.cmml"><mo stretchy="false" id="S1.E1.m1.2.3.2.3.2.1" xref="S1.E1.m1.2.3.2.cmml">(</mo><mi id="S1.E1.m1.1.1" xref="S1.E1.m1.1.1.cmml">δ</mi><mo rspace="0.278em" stretchy="false" id="S1.E1.m1.2.3.2.3.2.2" xref="S1.E1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S1.E1.m1.2.3.1" xref="S1.E1.m1.2.3.1.cmml">:=</mo><mrow id="S1.E1.m1.2.3.3" xref="S1.E1.m1.2.3.3.cmml"><munderover id="S1.E1.m1.2.3.3.1" xref="S1.E1.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S1.E1.m1.2.3.3.1.2.2" xref="S1.E1.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S1.E1.m1.2.3.3.1.2.3" xref="S1.E1.m1.2.3.3.1.2.3.cmml"><mi id="S1.E1.m1.2.3.3.1.2.3.2" xref="S1.E1.m1.2.3.3.1.2.3.2.cmml">k</mi><mo id="S1.E1.m1.2.3.3.1.2.3.1" xref="S1.E1.m1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S1.E1.m1.2.3.3.1.2.3.3" xref="S1.E1.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S1.E1.m1.2.3.3.1.3" xref="S1.E1.m1.2.3.3.1.3.cmml">K</mi></munderover><mrow id="S1.E1.m1.2.3.3.2" xref="S1.E1.m1.2.3.3.2.cmml"><mfrac id="S1.E1.m1.2.3.3.2.2" xref="S1.E1.m1.2.3.3.2.2.cmml"><msub id="S1.E1.m1.2.3.3.2.2.2" xref="S1.E1.m1.2.3.3.2.2.2.cmml"><mi id="S1.E1.m1.2.3.3.2.2.2.2" xref="S1.E1.m1.2.3.3.2.2.2.2.cmml">n</mi><mi id="S1.E1.m1.2.3.3.2.2.2.3" xref="S1.E1.m1.2.3.3.2.2.2.3.cmml">k</mi></msub><mi id="S1.E1.m1.2.3.3.2.2.3" xref="S1.E1.m1.2.3.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S1.E1.m1.2.3.3.2.1" xref="S1.E1.m1.2.3.3.2.1.cmml">​</mo><msub id="S1.E1.m1.2.3.3.2.3" xref="S1.E1.m1.2.3.3.2.3.cmml"><mi id="S1.E1.m1.2.3.3.2.3.2" xref="S1.E1.m1.2.3.3.2.3.2.cmml">f</mi><mi id="S1.E1.m1.2.3.3.2.3.3" xref="S1.E1.m1.2.3.3.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S1.E1.m1.2.3.3.2.1a" xref="S1.E1.m1.2.3.3.2.1.cmml">​</mo><mrow id="S1.E1.m1.2.3.3.2.4.2" xref="S1.E1.m1.2.3.3.2.cmml"><mo stretchy="false" id="S1.E1.m1.2.3.3.2.4.2.1" xref="S1.E1.m1.2.3.3.2.cmml">(</mo><mi id="S1.E1.m1.2.2" xref="S1.E1.m1.2.2.cmml">δ</mi><mo stretchy="false" id="S1.E1.m1.2.3.3.2.4.2.2" xref="S1.E1.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.E1.m1.2b"><apply id="S1.E1.m1.2.3.cmml" xref="S1.E1.m1.2.3"><csymbol cd="latexml" id="S1.E1.m1.2.3.1.cmml" xref="S1.E1.m1.2.3.1">assign</csymbol><apply id="S1.E1.m1.2.3.2.cmml" xref="S1.E1.m1.2.3.2"><times id="S1.E1.m1.2.3.2.1.cmml" xref="S1.E1.m1.2.3.2.1"></times><ci id="S1.E1.m1.2.3.2.2.cmml" xref="S1.E1.m1.2.3.2.2">𝑓</ci><ci id="S1.E1.m1.1.1.cmml" xref="S1.E1.m1.1.1">𝛿</ci></apply><apply id="S1.E1.m1.2.3.3.cmml" xref="S1.E1.m1.2.3.3"><apply id="S1.E1.m1.2.3.3.1.cmml" xref="S1.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S1.E1.m1.2.3.3.1.1.cmml" xref="S1.E1.m1.2.3.3.1">superscript</csymbol><apply id="S1.E1.m1.2.3.3.1.2.cmml" xref="S1.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S1.E1.m1.2.3.3.1.2.1.cmml" xref="S1.E1.m1.2.3.3.1">subscript</csymbol><sum id="S1.E1.m1.2.3.3.1.2.2.cmml" xref="S1.E1.m1.2.3.3.1.2.2"></sum><apply id="S1.E1.m1.2.3.3.1.2.3.cmml" xref="S1.E1.m1.2.3.3.1.2.3"><eq id="S1.E1.m1.2.3.3.1.2.3.1.cmml" xref="S1.E1.m1.2.3.3.1.2.3.1"></eq><ci id="S1.E1.m1.2.3.3.1.2.3.2.cmml" xref="S1.E1.m1.2.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S1.E1.m1.2.3.3.1.2.3.3.cmml" xref="S1.E1.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S1.E1.m1.2.3.3.1.3.cmml" xref="S1.E1.m1.2.3.3.1.3">𝐾</ci></apply><apply id="S1.E1.m1.2.3.3.2.cmml" xref="S1.E1.m1.2.3.3.2"><times id="S1.E1.m1.2.3.3.2.1.cmml" xref="S1.E1.m1.2.3.3.2.1"></times><apply id="S1.E1.m1.2.3.3.2.2.cmml" xref="S1.E1.m1.2.3.3.2.2"><divide id="S1.E1.m1.2.3.3.2.2.1.cmml" xref="S1.E1.m1.2.3.3.2.2"></divide><apply id="S1.E1.m1.2.3.3.2.2.2.cmml" xref="S1.E1.m1.2.3.3.2.2.2"><csymbol cd="ambiguous" id="S1.E1.m1.2.3.3.2.2.2.1.cmml" xref="S1.E1.m1.2.3.3.2.2.2">subscript</csymbol><ci id="S1.E1.m1.2.3.3.2.2.2.2.cmml" xref="S1.E1.m1.2.3.3.2.2.2.2">𝑛</ci><ci id="S1.E1.m1.2.3.3.2.2.2.3.cmml" xref="S1.E1.m1.2.3.3.2.2.2.3">𝑘</ci></apply><ci id="S1.E1.m1.2.3.3.2.2.3.cmml" xref="S1.E1.m1.2.3.3.2.2.3">𝑛</ci></apply><apply id="S1.E1.m1.2.3.3.2.3.cmml" xref="S1.E1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.2.3.3.2.3.1.cmml" xref="S1.E1.m1.2.3.3.2.3">subscript</csymbol><ci id="S1.E1.m1.2.3.3.2.3.2.cmml" xref="S1.E1.m1.2.3.3.2.3.2">𝑓</ci><ci id="S1.E1.m1.2.3.3.2.3.3.cmml" xref="S1.E1.m1.2.3.3.2.3.3">𝑘</ci></apply><ci id="S1.E1.m1.2.2.cmml" xref="S1.E1.m1.2.2">𝛿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E1.m1.2c">f(\delta):=\displaystyle\sum_{k=1}^{K}\frac{n_{k}}{n}f_{k}(\delta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S1.p2.11" class="ltx_p">In this equation, <math id="S1.p2.10.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S1.p2.10.m1.1a"><mi id="S1.p2.10.m1.1.1" xref="S1.p2.10.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S1.p2.10.m1.1b"><ci id="S1.p2.10.m1.1.1.cmml" xref="S1.p2.10.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.10.m1.1c">\delta</annotation></semantics></math> is updated after each iteration until we reach the optimal solution or the number of iterations set as the stopping criterion is satisfied. This optimization problem is solved using a federated Stochastic Gradient Descent (SGD) method, which is described in algorithm <a href="#alg1" title="Algorithm 1 ‣ 1 Introduction ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This algorithm shows that the gradient steps are taken by each client, and the model parameters are calculated as <math id="S1.p2.11.m2.1" class="ltx_Math" alttext="\delta_{t+1}\leftarrow\delta_{t}-\eta g_{k}" display="inline"><semantics id="S1.p2.11.m2.1a"><mrow id="S1.p2.11.m2.1.1" xref="S1.p2.11.m2.1.1.cmml"><msub id="S1.p2.11.m2.1.1.2" xref="S1.p2.11.m2.1.1.2.cmml"><mi id="S1.p2.11.m2.1.1.2.2" xref="S1.p2.11.m2.1.1.2.2.cmml">δ</mi><mrow id="S1.p2.11.m2.1.1.2.3" xref="S1.p2.11.m2.1.1.2.3.cmml"><mi id="S1.p2.11.m2.1.1.2.3.2" xref="S1.p2.11.m2.1.1.2.3.2.cmml">t</mi><mo id="S1.p2.11.m2.1.1.2.3.1" xref="S1.p2.11.m2.1.1.2.3.1.cmml">+</mo><mn id="S1.p2.11.m2.1.1.2.3.3" xref="S1.p2.11.m2.1.1.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S1.p2.11.m2.1.1.1" xref="S1.p2.11.m2.1.1.1.cmml">←</mo><mrow id="S1.p2.11.m2.1.1.3" xref="S1.p2.11.m2.1.1.3.cmml"><msub id="S1.p2.11.m2.1.1.3.2" xref="S1.p2.11.m2.1.1.3.2.cmml"><mi id="S1.p2.11.m2.1.1.3.2.2" xref="S1.p2.11.m2.1.1.3.2.2.cmml">δ</mi><mi id="S1.p2.11.m2.1.1.3.2.3" xref="S1.p2.11.m2.1.1.3.2.3.cmml">t</mi></msub><mo id="S1.p2.11.m2.1.1.3.1" xref="S1.p2.11.m2.1.1.3.1.cmml">−</mo><mrow id="S1.p2.11.m2.1.1.3.3" xref="S1.p2.11.m2.1.1.3.3.cmml"><mi id="S1.p2.11.m2.1.1.3.3.2" xref="S1.p2.11.m2.1.1.3.3.2.cmml">η</mi><mo lspace="0em" rspace="0em" id="S1.p2.11.m2.1.1.3.3.1" xref="S1.p2.11.m2.1.1.3.3.1.cmml">​</mo><msub id="S1.p2.11.m2.1.1.3.3.3" xref="S1.p2.11.m2.1.1.3.3.3.cmml"><mi id="S1.p2.11.m2.1.1.3.3.3.2" xref="S1.p2.11.m2.1.1.3.3.3.2.cmml">g</mi><mi id="S1.p2.11.m2.1.1.3.3.3.3" xref="S1.p2.11.m2.1.1.3.3.3.3.cmml">k</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.11.m2.1b"><apply id="S1.p2.11.m2.1.1.cmml" xref="S1.p2.11.m2.1.1"><ci id="S1.p2.11.m2.1.1.1.cmml" xref="S1.p2.11.m2.1.1.1">←</ci><apply id="S1.p2.11.m2.1.1.2.cmml" xref="S1.p2.11.m2.1.1.2"><csymbol cd="ambiguous" id="S1.p2.11.m2.1.1.2.1.cmml" xref="S1.p2.11.m2.1.1.2">subscript</csymbol><ci id="S1.p2.11.m2.1.1.2.2.cmml" xref="S1.p2.11.m2.1.1.2.2">𝛿</ci><apply id="S1.p2.11.m2.1.1.2.3.cmml" xref="S1.p2.11.m2.1.1.2.3"><plus id="S1.p2.11.m2.1.1.2.3.1.cmml" xref="S1.p2.11.m2.1.1.2.3.1"></plus><ci id="S1.p2.11.m2.1.1.2.3.2.cmml" xref="S1.p2.11.m2.1.1.2.3.2">𝑡</ci><cn type="integer" id="S1.p2.11.m2.1.1.2.3.3.cmml" xref="S1.p2.11.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S1.p2.11.m2.1.1.3.cmml" xref="S1.p2.11.m2.1.1.3"><minus id="S1.p2.11.m2.1.1.3.1.cmml" xref="S1.p2.11.m2.1.1.3.1"></minus><apply id="S1.p2.11.m2.1.1.3.2.cmml" xref="S1.p2.11.m2.1.1.3.2"><csymbol cd="ambiguous" id="S1.p2.11.m2.1.1.3.2.1.cmml" xref="S1.p2.11.m2.1.1.3.2">subscript</csymbol><ci id="S1.p2.11.m2.1.1.3.2.2.cmml" xref="S1.p2.11.m2.1.1.3.2.2">𝛿</ci><ci id="S1.p2.11.m2.1.1.3.2.3.cmml" xref="S1.p2.11.m2.1.1.3.2.3">𝑡</ci></apply><apply id="S1.p2.11.m2.1.1.3.3.cmml" xref="S1.p2.11.m2.1.1.3.3"><times id="S1.p2.11.m2.1.1.3.3.1.cmml" xref="S1.p2.11.m2.1.1.3.3.1"></times><ci id="S1.p2.11.m2.1.1.3.3.2.cmml" xref="S1.p2.11.m2.1.1.3.3.2">𝜂</ci><apply id="S1.p2.11.m2.1.1.3.3.3.cmml" xref="S1.p2.11.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S1.p2.11.m2.1.1.3.3.3.1.cmml" xref="S1.p2.11.m2.1.1.3.3.3">subscript</csymbol><ci id="S1.p2.11.m2.1.1.3.3.3.2.cmml" xref="S1.p2.11.m2.1.1.3.3.3.2">𝑔</ci><ci id="S1.p2.11.m2.1.1.3.3.3.3.cmml" xref="S1.p2.11.m2.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.11.m2.1c">\delta_{t+1}\leftarrow\delta_{t}-\eta g_{k}</annotation></semantics></math>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated Stochastic Gradient Descent</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<math id="alg1.l1.m1.5" class="ltx_Math" alttext="\delta_{t},\nabla f(\delta_{t}),n,n_{k},\eta" display="inline"><semantics id="alg1.l1.m1.5a"><mrow id="alg1.l1.m1.5.5.3" xref="alg1.l1.m1.5.5.4.cmml"><msub id="alg1.l1.m1.3.3.1.1" xref="alg1.l1.m1.3.3.1.1.cmml"><mi id="alg1.l1.m1.3.3.1.1.2" xref="alg1.l1.m1.3.3.1.1.2.cmml">δ</mi><mi id="alg1.l1.m1.3.3.1.1.3" xref="alg1.l1.m1.3.3.1.1.3.cmml">t</mi></msub><mo id="alg1.l1.m1.5.5.3.4" xref="alg1.l1.m1.5.5.4.cmml">,</mo><mrow id="alg1.l1.m1.4.4.2.2" xref="alg1.l1.m1.4.4.2.2.cmml"><mrow id="alg1.l1.m1.4.4.2.2.3" xref="alg1.l1.m1.4.4.2.2.3.cmml"><mo rspace="0.167em" id="alg1.l1.m1.4.4.2.2.3.1" xref="alg1.l1.m1.4.4.2.2.3.1.cmml">∇</mo><mi id="alg1.l1.m1.4.4.2.2.3.2" xref="alg1.l1.m1.4.4.2.2.3.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l1.m1.4.4.2.2.2" xref="alg1.l1.m1.4.4.2.2.2.cmml">​</mo><mrow id="alg1.l1.m1.4.4.2.2.1.1" xref="alg1.l1.m1.4.4.2.2.1.1.1.cmml"><mo stretchy="false" id="alg1.l1.m1.4.4.2.2.1.1.2" xref="alg1.l1.m1.4.4.2.2.1.1.1.cmml">(</mo><msub id="alg1.l1.m1.4.4.2.2.1.1.1" xref="alg1.l1.m1.4.4.2.2.1.1.1.cmml"><mi id="alg1.l1.m1.4.4.2.2.1.1.1.2" xref="alg1.l1.m1.4.4.2.2.1.1.1.2.cmml">δ</mi><mi id="alg1.l1.m1.4.4.2.2.1.1.1.3" xref="alg1.l1.m1.4.4.2.2.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l1.m1.4.4.2.2.1.1.3" xref="alg1.l1.m1.4.4.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="alg1.l1.m1.5.5.3.5" xref="alg1.l1.m1.5.5.4.cmml">,</mo><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">n</mi><mo id="alg1.l1.m1.5.5.3.6" xref="alg1.l1.m1.5.5.4.cmml">,</mo><msub id="alg1.l1.m1.5.5.3.3" xref="alg1.l1.m1.5.5.3.3.cmml"><mi id="alg1.l1.m1.5.5.3.3.2" xref="alg1.l1.m1.5.5.3.3.2.cmml">n</mi><mi id="alg1.l1.m1.5.5.3.3.3" xref="alg1.l1.m1.5.5.3.3.3.cmml">k</mi></msub><mo id="alg1.l1.m1.5.5.3.7" xref="alg1.l1.m1.5.5.4.cmml">,</mo><mi id="alg1.l1.m1.2.2" xref="alg1.l1.m1.2.2.cmml">η</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.5b"><list id="alg1.l1.m1.5.5.4.cmml" xref="alg1.l1.m1.5.5.3"><apply id="alg1.l1.m1.3.3.1.1.cmml" xref="alg1.l1.m1.3.3.1.1"><csymbol cd="ambiguous" id="alg1.l1.m1.3.3.1.1.1.cmml" xref="alg1.l1.m1.3.3.1.1">subscript</csymbol><ci id="alg1.l1.m1.3.3.1.1.2.cmml" xref="alg1.l1.m1.3.3.1.1.2">𝛿</ci><ci id="alg1.l1.m1.3.3.1.1.3.cmml" xref="alg1.l1.m1.3.3.1.1.3">𝑡</ci></apply><apply id="alg1.l1.m1.4.4.2.2.cmml" xref="alg1.l1.m1.4.4.2.2"><times id="alg1.l1.m1.4.4.2.2.2.cmml" xref="alg1.l1.m1.4.4.2.2.2"></times><apply id="alg1.l1.m1.4.4.2.2.3.cmml" xref="alg1.l1.m1.4.4.2.2.3"><ci id="alg1.l1.m1.4.4.2.2.3.1.cmml" xref="alg1.l1.m1.4.4.2.2.3.1">∇</ci><ci id="alg1.l1.m1.4.4.2.2.3.2.cmml" xref="alg1.l1.m1.4.4.2.2.3.2">𝑓</ci></apply><apply id="alg1.l1.m1.4.4.2.2.1.1.1.cmml" xref="alg1.l1.m1.4.4.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m1.4.4.2.2.1.1.1.1.cmml" xref="alg1.l1.m1.4.4.2.2.1.1">subscript</csymbol><ci id="alg1.l1.m1.4.4.2.2.1.1.1.2.cmml" xref="alg1.l1.m1.4.4.2.2.1.1.1.2">𝛿</ci><ci id="alg1.l1.m1.4.4.2.2.1.1.1.3.cmml" xref="alg1.l1.m1.4.4.2.2.1.1.1.3">𝑡</ci></apply></apply><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑛</ci><apply id="alg1.l1.m1.5.5.3.3.cmml" xref="alg1.l1.m1.5.5.3.3"><csymbol cd="ambiguous" id="alg1.l1.m1.5.5.3.3.1.cmml" xref="alg1.l1.m1.5.5.3.3">subscript</csymbol><ci id="alg1.l1.m1.5.5.3.3.2.cmml" xref="alg1.l1.m1.5.5.3.3.2">𝑛</ci><ci id="alg1.l1.m1.5.5.3.3.3.cmml" xref="alg1.l1.m1.5.5.3.3.3">𝑘</ci></apply><ci id="alg1.l1.m1.2.2.cmml" xref="alg1.l1.m1.2.2">𝜂</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.5c">\delta_{t},\nabla f(\delta_{t}),n,n_{k},\eta</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<math id="alg1.l2.m1.1" class="ltx_Math" alttext="C&lt;1" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">C</mi><mo id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">&lt;</mo><mn id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><lt id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1"></lt><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝐶</ci><cn type="integer" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">C&lt;1</annotation></semantics></math>

<br class="ltx_break"><span id="alg1.l2.1" class="ltx_text" style="float:right;"><math id="alg1.l2.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l2.1.m1.1a"><mo id="alg1.l2.1.m1.1.1" xref="alg1.l2.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l2.1.m1.1b"><ci id="alg1.l2.1.m1.1.1.cmml" xref="alg1.l2.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.1.m1.1c">\triangleright</annotation></semantics></math> a subset of clients is selected at each round
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<math id="alg1.l3.m1.1" class="ltx_Math" alttext="\delta_{t}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">δ</mi><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝛿</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\delta_{t}</annotation></semantics></math>:= the current state of the evaluation value

</div>
<div id="alg1.l4" class="ltx_listingline">
<span id="alg1.l4.1" class="ltx_text ltx_font_bold">while</span> <math id="alg1.l4.m1.1" class="ltx_Math" alttext="f(\delta_{t})" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">​</mo><mrow id="alg1.l4.m1.1.1.1.1" xref="alg1.l4.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l4.m1.1.1.1.1.2" xref="alg1.l4.m1.1.1.1.1.1.cmml">(</mo><msub id="alg1.l4.m1.1.1.1.1.1" xref="alg1.l4.m1.1.1.1.1.1.cmml"><mi id="alg1.l4.m1.1.1.1.1.1.2" xref="alg1.l4.m1.1.1.1.1.1.2.cmml">δ</mi><mi id="alg1.l4.m1.1.1.1.1.1.3" xref="alg1.l4.m1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l4.m1.1.1.1.1.3" xref="alg1.l4.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><times id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2"></times><ci id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">𝑓</ci><apply id="alg1.l4.m1.1.1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l4.m1.1.1.1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1.1">subscript</csymbol><ci id="alg1.l4.m1.1.1.1.1.1.2.cmml" xref="alg1.l4.m1.1.1.1.1.1.2">𝛿</ci><ci id="alg1.l4.m1.1.1.1.1.1.3.cmml" xref="alg1.l4.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">f(\delta_{t})</annotation></semantics></math> <math id="alg1.l4.m2.1" class="ltx_Math" alttext="\neq" display="inline"><semantics id="alg1.l4.m2.1a"><mo id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">≠</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b"><neq id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1"></neq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.1c">\neq</annotation></semantics></math> optimal <span id="alg1.l4.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">     <math id="alg1.l5.m1.1" class="ltx_Math" alttext="g_{k}=\nabla f_{k}(\delta_{t})" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><msub id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"><mi id="alg1.l5.m1.1.1.3.2" xref="alg1.l5.m1.1.1.3.2.cmml">g</mi><mi id="alg1.l5.m1.1.1.3.3" xref="alg1.l5.m1.1.1.3.3.cmml">k</mi></msub><mo id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">=</mo><mrow id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml"><mrow id="alg1.l5.m1.1.1.1.3" xref="alg1.l5.m1.1.1.1.3.cmml"><mo rspace="0.167em" id="alg1.l5.m1.1.1.1.3.1" xref="alg1.l5.m1.1.1.1.3.1.cmml">∇</mo><msub id="alg1.l5.m1.1.1.1.3.2" xref="alg1.l5.m1.1.1.1.3.2.cmml"><mi id="alg1.l5.m1.1.1.1.3.2.2" xref="alg1.l5.m1.1.1.1.3.2.2.cmml">f</mi><mi id="alg1.l5.m1.1.1.1.3.2.3" xref="alg1.l5.m1.1.1.1.3.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.1.2" xref="alg1.l5.m1.1.1.1.2.cmml">​</mo><mrow id="alg1.l5.m1.1.1.1.1.1" xref="alg1.l5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l5.m1.1.1.1.1.1.2" xref="alg1.l5.m1.1.1.1.1.1.1.cmml">(</mo><msub id="alg1.l5.m1.1.1.1.1.1.1" xref="alg1.l5.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l5.m1.1.1.1.1.1.1.2" xref="alg1.l5.m1.1.1.1.1.1.1.2.cmml">δ</mi><mi id="alg1.l5.m1.1.1.1.1.1.1.3" xref="alg1.l5.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l5.m1.1.1.1.1.1.3" xref="alg1.l5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><eq id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2"></eq><apply id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.3">subscript</csymbol><ci id="alg1.l5.m1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.3.2">𝑔</ci><ci id="alg1.l5.m1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.3.3">𝑘</ci></apply><apply id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"><times id="alg1.l5.m1.1.1.1.2.cmml" xref="alg1.l5.m1.1.1.1.2"></times><apply id="alg1.l5.m1.1.1.1.3.cmml" xref="alg1.l5.m1.1.1.1.3"><ci id="alg1.l5.m1.1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.1.3.1">∇</ci><apply id="alg1.l5.m1.1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.1.3.2.1.cmml" xref="alg1.l5.m1.1.1.1.3.2">subscript</csymbol><ci id="alg1.l5.m1.1.1.1.3.2.2.cmml" xref="alg1.l5.m1.1.1.1.3.2.2">𝑓</ci><ci id="alg1.l5.m1.1.1.1.3.2.3.cmml" xref="alg1.l5.m1.1.1.1.3.2.3">𝑘</ci></apply></apply><apply id="alg1.l5.m1.1.1.1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l5.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l5.m1.1.1.1.1.1.1.2">𝛿</ci><ci id="alg1.l5.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l5.m1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">g_{k}=\nabla f_{k}(\delta_{t})</annotation></semantics></math>

<br class="ltx_break"><span id="alg1.l5.2" class="ltx_text" style="float:right;"><math id="alg1.l5.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l5.1.m1.1a"><mo id="alg1.l5.1.m1.1.1" xref="alg1.l5.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l5.1.m1.1b"><ci id="alg1.l5.1.m1.1.1.cmml" xref="alg1.l5.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.1.m1.1c">\triangleright</annotation></semantics></math> <math id="alg1.l5.2.m2.1" class="ltx_Math" alttext="g_{k}" display="inline"><semantics id="alg1.l5.2.m2.1a"><msub id="alg1.l5.2.m2.1.1" xref="alg1.l5.2.m2.1.1.cmml"><mi id="alg1.l5.2.m2.1.1.2" xref="alg1.l5.2.m2.1.1.2.cmml">g</mi><mi id="alg1.l5.2.m2.1.1.3" xref="alg1.l5.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.2.m2.1b"><apply id="alg1.l5.2.m2.1.1.cmml" xref="alg1.l5.2.m2.1.1"><csymbol cd="ambiguous" id="alg1.l5.2.m2.1.1.1.cmml" xref="alg1.l5.2.m2.1.1">subscript</csymbol><ci id="alg1.l5.2.m2.1.1.2.cmml" xref="alg1.l5.2.m2.1.1.2">𝑔</ci><ci id="alg1.l5.2.m2.1.1.3.cmml" xref="alg1.l5.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.2.m2.1c">g_{k}</annotation></semantics></math> := the gradient of client k
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">     <math id="alg1.l6.m1.1" class="ltx_Math" alttext="\delta_{t+l}\leftarrow\delta_{t}-\eta\nabla f(\delta_{t})=\delta_{t}-\sum_{k=1}^{K}\frac{n_{k}}{n}g_{k}" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><msub id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">δ</mi><mrow id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml"><mi id="alg1.l6.m1.1.1.3.3.2" xref="alg1.l6.m1.1.1.3.3.2.cmml">t</mi><mo id="alg1.l6.m1.1.1.3.3.1" xref="alg1.l6.m1.1.1.3.3.1.cmml">+</mo><mi id="alg1.l6.m1.1.1.3.3.3" xref="alg1.l6.m1.1.1.3.3.3.cmml">l</mi></mrow></msub><mo stretchy="false" id="alg1.l6.m1.1.1.4" xref="alg1.l6.m1.1.1.4.cmml">←</mo><mrow id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml"><msub id="alg1.l6.m1.1.1.1.3" xref="alg1.l6.m1.1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.1.3.2" xref="alg1.l6.m1.1.1.1.3.2.cmml">δ</mi><mi id="alg1.l6.m1.1.1.1.3.3" xref="alg1.l6.m1.1.1.1.3.3.cmml">t</mi></msub><mo id="alg1.l6.m1.1.1.1.2" xref="alg1.l6.m1.1.1.1.2.cmml">−</mo><mrow id="alg1.l6.m1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.cmml"><mi id="alg1.l6.m1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.3.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg1.l6.m1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.2.cmml">​</mo><mrow id="alg1.l6.m1.1.1.1.1.4" xref="alg1.l6.m1.1.1.1.1.4.cmml"><mo rspace="0.167em" id="alg1.l6.m1.1.1.1.1.4.1" xref="alg1.l6.m1.1.1.1.1.4.1.cmml">∇</mo><mi id="alg1.l6.m1.1.1.1.1.4.2" xref="alg1.l6.m1.1.1.1.1.4.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.1.1.2a" xref="alg1.l6.m1.1.1.1.1.2.cmml">​</mo><mrow id="alg1.l6.m1.1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l6.m1.1.1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="alg1.l6.m1.1.1.1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.1.1.1.cmml"><mi id="alg1.l6.m1.1.1.1.1.1.1.1.2" xref="alg1.l6.m1.1.1.1.1.1.1.1.2.cmml">δ</mi><mi id="alg1.l6.m1.1.1.1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l6.m1.1.1.1.1.1.1.3" xref="alg1.l6.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="alg1.l6.m1.1.1.5" xref="alg1.l6.m1.1.1.5.cmml">=</mo><mrow id="alg1.l6.m1.1.1.6" xref="alg1.l6.m1.1.1.6.cmml"><msub id="alg1.l6.m1.1.1.6.2" xref="alg1.l6.m1.1.1.6.2.cmml"><mi id="alg1.l6.m1.1.1.6.2.2" xref="alg1.l6.m1.1.1.6.2.2.cmml">δ</mi><mi id="alg1.l6.m1.1.1.6.2.3" xref="alg1.l6.m1.1.1.6.2.3.cmml">t</mi></msub><mo rspace="0.055em" id="alg1.l6.m1.1.1.6.1" xref="alg1.l6.m1.1.1.6.1.cmml">−</mo><mrow id="alg1.l6.m1.1.1.6.3" xref="alg1.l6.m1.1.1.6.3.cmml"><msubsup id="alg1.l6.m1.1.1.6.3.1" xref="alg1.l6.m1.1.1.6.3.1.cmml"><mo id="alg1.l6.m1.1.1.6.3.1.2.2" xref="alg1.l6.m1.1.1.6.3.1.2.2.cmml">∑</mo><mrow id="alg1.l6.m1.1.1.6.3.1.2.3" xref="alg1.l6.m1.1.1.6.3.1.2.3.cmml"><mi id="alg1.l6.m1.1.1.6.3.1.2.3.2" xref="alg1.l6.m1.1.1.6.3.1.2.3.2.cmml">k</mi><mo id="alg1.l6.m1.1.1.6.3.1.2.3.1" xref="alg1.l6.m1.1.1.6.3.1.2.3.1.cmml">=</mo><mn id="alg1.l6.m1.1.1.6.3.1.2.3.3" xref="alg1.l6.m1.1.1.6.3.1.2.3.3.cmml">1</mn></mrow><mi id="alg1.l6.m1.1.1.6.3.1.3" xref="alg1.l6.m1.1.1.6.3.1.3.cmml">K</mi></msubsup><mrow id="alg1.l6.m1.1.1.6.3.2" xref="alg1.l6.m1.1.1.6.3.2.cmml"><mfrac id="alg1.l6.m1.1.1.6.3.2.2" xref="alg1.l6.m1.1.1.6.3.2.2.cmml"><msub id="alg1.l6.m1.1.1.6.3.2.2.2" xref="alg1.l6.m1.1.1.6.3.2.2.2.cmml"><mi id="alg1.l6.m1.1.1.6.3.2.2.2.2" xref="alg1.l6.m1.1.1.6.3.2.2.2.2.cmml">n</mi><mi id="alg1.l6.m1.1.1.6.3.2.2.2.3" xref="alg1.l6.m1.1.1.6.3.2.2.2.3.cmml">k</mi></msub><mi id="alg1.l6.m1.1.1.6.3.2.2.3" xref="alg1.l6.m1.1.1.6.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.6.3.2.1" xref="alg1.l6.m1.1.1.6.3.2.1.cmml">​</mo><msub id="alg1.l6.m1.1.1.6.3.2.3" xref="alg1.l6.m1.1.1.6.3.2.3.cmml"><mi id="alg1.l6.m1.1.1.6.3.2.3.2" xref="alg1.l6.m1.1.1.6.3.2.3.2.cmml">g</mi><mi id="alg1.l6.m1.1.1.6.3.2.3.3" xref="alg1.l6.m1.1.1.6.3.2.3.3.cmml">k</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><and id="alg1.l6.m1.1.1a.cmml" xref="alg1.l6.m1.1.1"></and><apply id="alg1.l6.m1.1.1b.cmml" xref="alg1.l6.m1.1.1"><ci id="alg1.l6.m1.1.1.4.cmml" xref="alg1.l6.m1.1.1.4">←</ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3">subscript</csymbol><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝛿</ci><apply id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3"><plus id="alg1.l6.m1.1.1.3.3.1.cmml" xref="alg1.l6.m1.1.1.3.3.1"></plus><ci id="alg1.l6.m1.1.1.3.3.2.cmml" xref="alg1.l6.m1.1.1.3.3.2">𝑡</ci><ci id="alg1.l6.m1.1.1.3.3.3.cmml" xref="alg1.l6.m1.1.1.3.3.3">𝑙</ci></apply></apply><apply id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"><minus id="alg1.l6.m1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.2"></minus><apply id="alg1.l6.m1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.1.3">subscript</csymbol><ci id="alg1.l6.m1.1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.1.3.2">𝛿</ci><ci id="alg1.l6.m1.1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.1.3.3">𝑡</ci></apply><apply id="alg1.l6.m1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1"><times id="alg1.l6.m1.1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.1.2"></times><ci id="alg1.l6.m1.1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.1.3">𝜂</ci><apply id="alg1.l6.m1.1.1.1.1.4.cmml" xref="alg1.l6.m1.1.1.1.1.4"><ci id="alg1.l6.m1.1.1.1.1.4.1.cmml" xref="alg1.l6.m1.1.1.1.1.4.1">∇</ci><ci id="alg1.l6.m1.1.1.1.1.4.2.cmml" xref="alg1.l6.m1.1.1.1.1.4.2">𝑓</ci></apply><apply id="alg1.l6.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l6.m1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l6.m1.1.1.1.1.1.1.1.2">𝛿</ci><ci id="alg1.l6.m1.1.1.1.1.1.1.1.3.cmml" xref="alg1.l6.m1.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply><apply id="alg1.l6.m1.1.1c.cmml" xref="alg1.l6.m1.1.1"><eq id="alg1.l6.m1.1.1.5.cmml" xref="alg1.l6.m1.1.1.5"></eq><share href="#alg1.l6.m1.1.1.1.cmml" id="alg1.l6.m1.1.1d.cmml" xref="alg1.l6.m1.1.1"></share><apply id="alg1.l6.m1.1.1.6.cmml" xref="alg1.l6.m1.1.1.6"><minus id="alg1.l6.m1.1.1.6.1.cmml" xref="alg1.l6.m1.1.1.6.1"></minus><apply id="alg1.l6.m1.1.1.6.2.cmml" xref="alg1.l6.m1.1.1.6.2"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.6.2.1.cmml" xref="alg1.l6.m1.1.1.6.2">subscript</csymbol><ci id="alg1.l6.m1.1.1.6.2.2.cmml" xref="alg1.l6.m1.1.1.6.2.2">𝛿</ci><ci id="alg1.l6.m1.1.1.6.2.3.cmml" xref="alg1.l6.m1.1.1.6.2.3">𝑡</ci></apply><apply id="alg1.l6.m1.1.1.6.3.cmml" xref="alg1.l6.m1.1.1.6.3"><apply id="alg1.l6.m1.1.1.6.3.1.cmml" xref="alg1.l6.m1.1.1.6.3.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.6.3.1.1.cmml" xref="alg1.l6.m1.1.1.6.3.1">superscript</csymbol><apply id="alg1.l6.m1.1.1.6.3.1.2.cmml" xref="alg1.l6.m1.1.1.6.3.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.6.3.1.2.1.cmml" xref="alg1.l6.m1.1.1.6.3.1">subscript</csymbol><sum id="alg1.l6.m1.1.1.6.3.1.2.2.cmml" xref="alg1.l6.m1.1.1.6.3.1.2.2"></sum><apply id="alg1.l6.m1.1.1.6.3.1.2.3.cmml" xref="alg1.l6.m1.1.1.6.3.1.2.3"><eq id="alg1.l6.m1.1.1.6.3.1.2.3.1.cmml" xref="alg1.l6.m1.1.1.6.3.1.2.3.1"></eq><ci id="alg1.l6.m1.1.1.6.3.1.2.3.2.cmml" xref="alg1.l6.m1.1.1.6.3.1.2.3.2">𝑘</ci><cn type="integer" id="alg1.l6.m1.1.1.6.3.1.2.3.3.cmml" xref="alg1.l6.m1.1.1.6.3.1.2.3.3">1</cn></apply></apply><ci id="alg1.l6.m1.1.1.6.3.1.3.cmml" xref="alg1.l6.m1.1.1.6.3.1.3">𝐾</ci></apply><apply id="alg1.l6.m1.1.1.6.3.2.cmml" xref="alg1.l6.m1.1.1.6.3.2"><times id="alg1.l6.m1.1.1.6.3.2.1.cmml" xref="alg1.l6.m1.1.1.6.3.2.1"></times><apply id="alg1.l6.m1.1.1.6.3.2.2.cmml" xref="alg1.l6.m1.1.1.6.3.2.2"><divide id="alg1.l6.m1.1.1.6.3.2.2.1.cmml" xref="alg1.l6.m1.1.1.6.3.2.2"></divide><apply id="alg1.l6.m1.1.1.6.3.2.2.2.cmml" xref="alg1.l6.m1.1.1.6.3.2.2.2"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.6.3.2.2.2.1.cmml" xref="alg1.l6.m1.1.1.6.3.2.2.2">subscript</csymbol><ci id="alg1.l6.m1.1.1.6.3.2.2.2.2.cmml" xref="alg1.l6.m1.1.1.6.3.2.2.2.2">𝑛</ci><ci id="alg1.l6.m1.1.1.6.3.2.2.2.3.cmml" xref="alg1.l6.m1.1.1.6.3.2.2.2.3">𝑘</ci></apply><ci id="alg1.l6.m1.1.1.6.3.2.2.3.cmml" xref="alg1.l6.m1.1.1.6.3.2.2.3">𝑛</ci></apply><apply id="alg1.l6.m1.1.1.6.3.2.3.cmml" xref="alg1.l6.m1.1.1.6.3.2.3"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.6.3.2.3.1.cmml" xref="alg1.l6.m1.1.1.6.3.2.3">subscript</csymbol><ci id="alg1.l6.m1.1.1.6.3.2.3.2.cmml" xref="alg1.l6.m1.1.1.6.3.2.3.2">𝑔</ci><ci id="alg1.l6.m1.1.1.6.3.2.3.3.cmml" xref="alg1.l6.m1.1.1.6.3.2.3.3">𝑘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\delta_{t+l}\leftarrow\delta_{t}-\eta\nabla f(\delta_{t})=\delta_{t}-\sum_{k=1}^{K}\frac{n_{k}}{n}g_{k}</annotation></semantics></math> 
<br class="ltx_break"><span id="alg1.l6.1" class="ltx_text" style="float:right;"><math id="alg1.l6.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l6.1.m1.1a"><mo id="alg1.l6.1.m1.1.1" xref="alg1.l6.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.1.m1.1b"><ci id="alg1.l6.1.m1.1.1.cmml" xref="alg1.l6.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.1.m1.1c">\triangleright</annotation></semantics></math> Aggregation of client gradients to create a new model at the central server
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span id="alg1.l7.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l7.2" class="ltx_text ltx_font_bold">while</span>
</div>
</div>
</figure>
<div id="S1.p3" class="ltx_para">
<br class="ltx_break">
<br class="ltx_break">
<p id="S1.p3.1" class="ltx_p">Federated learning affects the modeling step of the Cross-Industry Standard Process for Data Mining (CRISP-DM), which starts from local data storage in data centers to communicate with the central server to iteratively aggregate the model parameters and update the global model to achieve the desired learning accuracy in data centers.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2311.10832/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data Mining Process in Federated Learning Based on CRISP-DM Model</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the Federated learning life cycle embedded in CRISP-DM.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Applications</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated learning has been applied to various fields when the data is sensitive and scattered across multiple servers or devices. In particular, federated learning is a compelling approach in healthcare, IoT, and crisis management, where information limitation is an issue.</p>
</div>
<section id="S2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">IoT</h3>

<div id="S2.SSx1.p1" class="ltx_para">
<p id="S2.SSx1.p1.1" class="ltx_p">Industry 4.0 is the era of interconnected physical and digital technologies. Through the fourth industrial revolution, smart operations evolved, and the demand for informed and data-driven solutions increased. In the digital world, data is constantly generated in texts, images, and measurements from thousands of sensors and devices, which require powerful systems to perform extensive computations for data processing. Smart cities, cloud-based technologies, edge computing, and IoT need reliable, secure, real-time analysis tools. This has increased the demand for systems that can address scalability, interoperability, resource limitations, and privacy issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SSx1.p2" class="ltx_para">
<p id="S2.SSx1.p2.1" class="ltx_p">Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> surveyed the application of federated learning to leverage the data on IoT devices for smart cities and industries, leading to advances in healthcare, transportation, and Unmanned Aerial Vehicles. Khan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> surveyed different aspects of federated learning for IoT applications. The authors compared and evaluated the methods from robustness, quantization, sparsification, scalability, security, and privacy perspectives. Varlamis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> used federated learning to find energy-saving solutions based on sensor data.
Federated learning offers several benefits when applied to Internet of Things (IoT) applications such as:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Privacy preservation: This is the primary advantage of federated learning. IoT devices often collect sensitive personal information, such as health data or home automation preferences, and federated learning allows these devices to train machine learning models without exposing individuals to possible harm and misuse of data.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Reduced data transfer: IoT devices are often resource-constrained, making it inefficient and costly to transmit large volumes of data to a central server. By keeping data on the edge, federated learning reduces the need for extensive data transfer and lowers communication overhead.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Edge computing: Federated learning fits well with edge computing, where data processing occurs closer to the source (IoT devices) rather than in a data center. Therefore, minimizing latency and improving real-time decision-making which is crucial for applications like autonomous vehicles and industrial automation.</p>
</div>
</li>
</ul>
<p id="S2.SSx1.p2.2" class="ltx_p">Some of the challenges and limitations of federated learning in this domain are:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Heterogeneity: IoT devices come in various shapes, sizes, and capabilities, making them challenging to harmonize for federated learning. Ensuring that models can be trained effectively across diverse IoT ecosystems is a significant challenge. In federated learning, some devices may also have more data or contribute more frequently to model updates than others. Managing the data across IoT devices can affect the fairness and accuracy of the learned models.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Communication overhead: Aggregating model updates from numerous IoT devices can be challenging, especially when dealing with intermittent connectivity, device failures, or adversarial behavior. Robust aggregation methods are needed to handle these scenarios.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">Computational overhead: Federated learning can be computationally intensive, which may be problematic for resource-constrained IoT devices. Balancing model training with energy efficiency and processing power is a limitation that needs to be addressed.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p">Scalability: As the number of IoT devices increases, managing federated learning across the network becomes more challenging.</p>
</div>
</li>
</ul>
<p id="S2.SSx1.p2.3" class="ltx_p">Federated learning holds great promise for IoT applications. However, addressing challenges and overcoming limitations is essential to fully harness the potential of federated learning in the rapidly expanding IoT ecosystem.</p>
</div>
</section>
<section id="S2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Healthcare</h3>

<div id="S2.SSx2.p1" class="ltx_para">
<p id="S2.SSx2.p1.1" class="ltx_p">Federated learning is a promising approach for learning from healthcare data, which is highly regulated and cannot be openly shared with the public. Therefore, if privacy is ensured, it can significantly benefit from utilizing artificial intelligence and machine learning to move towards personalized healthcare and computer-aided diagnosis. Federated learning creates a global model of decentralized data, such as the data from hospitals, labs, and clinical trials without direct access to the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. For instance, Feki et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> utilized a federated learning approach to build a powerful model to classify COVID-19 X-rays based on data collected from multiple institutes. Rieke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> explored the existing literature on federated learning for healthcare with the challenges and open problems in digital healthcare.
Some of the benefits of federated learning in healthcare applications are:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">Privacy preservation: Healthcare data is highly sensitive and subject to strict privacy regulations. Federated learning enables healthcare institutions to collaborate on model training without sharing raw patient data, preserving privacy and compliance with regulations like HIPAA.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">Large-scale data utilization: Without privacy concerns, healthcare organizations can tap into a vast pool of data from various sources, including hospitals, clinics, wearable devices, and electronic health records, to advance machine learning and computer-aided diagnosis.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p">Personalized medicine: by leveraging patient-specific data, personalized treatment plans become more viable, leading to more effective and tailored healthcare interventions.</p>
</div>
</li>
</ul>
<p id="S2.SSx2.p1.2" class="ltx_p">However, some of the challenges and limitations are:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p">Heterogeneity: Health data comes from various hospitals, clinics, personal health monitoring devices, and more. Differences in data formats, quality, completeness, and availability pose challenges for integrating the data and achieving accurate and reliable results.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p">Regulatory compliance: Healthcare is heavily regulated, with different regions and countries having their own sets of rules and standards. Navigating regulatory compliance while implementing federated learning can be complex and time-consuming.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p">Bias and fairness: Federated learning may inherit biases present in the data from participating institutions, potentially leading to biased or unfair model outcomes.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p id="S2.I4.i4.p1.1" class="ltx_p">Model quality control: Ensuring the quality and consistency of models across different institutions can be challenging. Mechanisms for model monitoring, validation, and quality control are essential to maintain high standards of care.</p>
</div>
</li>
</ul>
<p id="S2.SSx2.p1.3" class="ltx_p">Federated learning is a promising approach for healthcare applications, and addressing the challenges can have significant impacts on healthcare operations.</p>
</div>
</section>
<section id="S2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Crisis management</h3>

<div id="S2.SSx3.p1" class="ltx_para">
<p id="S2.SSx3.p1.1" class="ltx_p">There is a growing interest in machine learning algorithms for weather applications and natural disasters. Leveraging large data sets from multiple resources facilitates collaborative learning from data collected at different geographic regions, allowing for more powerful and precise models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Bypassing the risks of data sharing encourages meteorological institutions across the countries to harness the power of machine learning while ensuring privacy and efficient utilization of resources, resulting in more accurate and timely predictions. On a larger scale, when dealing with natural disasters and emergencies, federated learning allows government agencies, international organizations, local support groups, and communities to collaborate effectively without privacy restrictions caused by traditional centralized learning. Federated learning provides the framework for secure communication of knowledge, resulting in early detection, risk assessment, and effective emergency response strategies using more accurate and robust models.
The benefits of federated learning in crisis management are:</p>
<ul id="S2.I5" class="ltx_itemize">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p">Privacy Preservation: In crisis management scenarios, sensitive and critical data may be involved, such as location data, medical records, or disaster response plans. Federated learning enables multiple entities to collaborate on model training without sharing raw data, preserving privacy and security.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p">Real-time updates: Crisis management requires quick decision-making based on the latest information. Federated learning enables real-time model updates as new data becomes available, ensuring that decision support systems remain current and effective during rapidly evolving situations.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i3.p1" class="ltx_para">
<p id="S2.I5.i3.p1.1" class="ltx_p">Resource efficiency: Crisis response often involves distributed teams and resources. Federated learning leverages the computing power of edge devices and distributed data sources, minimizing the need for centralized data storage and processing resources.</p>
</div>
</li>
<li id="S2.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i4.p1" class="ltx_para">
<p id="S2.I5.i4.p1.1" class="ltx_p">Customization for local conditions: Different regions may have unique characteristics and needs during a crisis. Federated learning allows for localized model customization, ensuring that solutions are tailored to specific conditions and requirements.</p>
</div>
</li>
</ul>
<p id="S2.SSx3.p1.2" class="ltx_p">Despite its benefits, the challenges and limitations of this approach are:</p>
<ul id="S2.I6" class="ltx_itemize">
<li id="S2.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i1.p1" class="ltx_para">
<p id="S2.I6.i1.p1.1" class="ltx_p">Data availability: Federated learning relies on the availability of data. During a crisis when data may be sparse, incomplete, or unreliable due to infrastructure damage or connectivity issues, training reliable models becomes challenging.</p>
</div>
</li>
<li id="S2.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i2.p1" class="ltx_para">
<p id="S2.I6.i2.p1.1" class="ltx_p">Heterogeneity: In addition to data availability, collecting data from multiple sources such as governmental and private organization databases, social media, and on-site observations results in inconsistencies in data formats that need special attention during model training and interpretation of results.</p>
</div>
</li>
<li id="S2.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i3.p1" class="ltx_para">
<p id="S2.I6.i3.p1.1" class="ltx_p">Model drift: In dynamic crisis situations, data distributions can change rapidly, causing model drift which requires the model to be continuously updated and retrained.</p>
</div>
</li>
<li id="S2.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i4.p1" class="ltx_para">
<p id="S2.I6.i4.p1.1" class="ltx_p">Communication overhead: Connectivity issues and limitations on bandwidth are possible issues during a crisis. Allocating adequate resources and careful planning prior to emergency scenarios prevents disruptions in critical operations due to disconnections in the network.</p>
</div>
</li>
</ul>
<p id="S2.SSx3.p1.3" class="ltx_p">Overall, federated learning offers significant potential for enhancing crisis management applications and improving the effectiveness of disaster mitigation and recovery efforts.

<br class="ltx_break">Figure <a href="#S2.F3" title="Figure 3 ‣ Crisis management ‣ 2 Applications ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows some of the applications of federated learning in the industry based on the papers reviewed in this work.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2311.10832/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Applications of Federated Learning in Industry</figcaption>
</figure>
</section>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Contributions</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There is a rich body of research on different aspects of federated learning, such as data processing, learning models, aggregation methods, specifications of data centers and the central server, communication security, and efficiency among the elements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and the multiple software and libraries used for implementing federated learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In this work, we survey the existing research on federated learning. What distinguishes this work from other surveys is the focus on the model selection aspect of the federated learning process, complementing other recent surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. We explore the different machine learning models used in federated learning to tackle problems in different domains. In this survey, we have investigated the papers published between 2016-2022 in accredited peer-reviewed journals and conferences and classified them based on the machine learning methods used for learning. We limited the search to the keywords federated learning, privacy-preserving machine learning, distributed learning, supervised/unsupervised learning, and artificial intelligence. The PRISMA diagram presented in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.1 Contributions ‣ 2 Applications ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates the searching strategy in this survey.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2311.10832/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>PRISMA Flow Diagram Summarizing the Search Strategy </figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Components of Federated Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the following sections, we will provide a detailed explanation of the federated learning framework’s components: storage, privacy, communication, federated aggregation, and privacy-preserving machine learning, respectively. We dive into a detailed discussion of different machine learning models used for training decentralized data, their use cases and applications, and some technical details useful for implementation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Storage</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Federated learning is a cross-organizational framework. Therefore, the features and observations may vary between different data centers. Depending on the architecture of the data centers and how the data are partitioned, three scenarios of horizontal partitioning, vertical partitioning, and transfer learning have been discussed.</p>
</div>
<section id="S3.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Horizontal partitioning</h4>

<div id="S3.SS1.SSSx1.p1" class="ltx_para">
<p id="S3.SS1.SSSx1.p1.1" class="ltx_p">Horizontal federated learning, also known as sample-based federated learning, is the scenario in which the data centers have the same features but different sample spaces that require modifications to the training model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. For example, a network of local banks that individually collect a certain list of information from their clients. In this example, the clients are different. Therefore, the sample space varies between the banks. Horizontal federated learning allows entities to build a generalized global model based on a larger data pool without compromising privacy.

<br class="ltx_break"></p>
</div>
</section>
<section id="S3.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Vertical partitioning</h4>

<div id="S3.SS1.SSSx2.p1" class="ltx_para">
<p id="S3.SS1.SSSx2.p1.1" class="ltx_p">Vertical federated learning, also known as feature-based federated learning, is when local datasets have the same sample space but differ in their features. Numerous researchers have explored training models specified for vertically partitioned data. For example, a local bank and an insurance company share the same clients but collect different types of information. If the two entities were to build a local model collaboratively, the two datasets would have common sample space but very different features. The common aggregating methods aren’t effective in vertically partitioned data. Therefore, the difference between the feature spaces causes different challenges and creates opportunities for further research to address the issue of fault diagnosis.

<br class="ltx_break"></p>
</div>
</section>
<section id="S3.SS1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Transfer Learning</h4>

<div id="S3.SS1.SSSx3.p1" class="ltx_para">
<p id="S3.SS1.SSSx3.p1.1" class="ltx_p">Transfer Learning is the learning structure in which the local datasets differ both in the sample and feature space. Thus, knowledge is derived from various sources to achieve a global model. Despite all the challenges, transfer learning has been applied to a wide variety of problems in different domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and it has great potential for further improvement.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Privacy</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Preserving privacy is an essential constraint when learning from sensitive data that requires protection against data leakage and adversarial attacks.
In cybersecurity, the adversary is defined as a person or a group that performs malicious actions to disrupt or corrupt a cyber system. Mothukuri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> identified the different types of adversaries in the federated learning framework and highlighted the solutions to improve systems vulnerability.
Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Privacy ‣ 3 Components of Federated Learning ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> describes the two types of adversaries caused by adversaries.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Types of Adversaries in Cybersecurity</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;">Adversaries</span>
</span>
</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;">Action</span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:148.0pt;">Aim</span>
</span>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.4.1.1" class="ltx_p" style="width:51.2pt;">Protocol</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p" style="width:56.9pt;">Semi-honest</span>
</span>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:28.5pt;">Passive</span>
</span>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.3.1.1" class="ltx_p" style="width:148.0pt;">Learning about the system</span>
</span>
</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.4.1.1" class="ltx_p" style="width:51.2pt;">✓</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p" style="width:56.9pt;">Malicious</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:28.5pt;">Active</span>
</span>
</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.3.1.1" class="ltx_p" style="width:148.0pt;">Manipulation, corruption, control the system</span>
</span>
</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.4.1.1" class="ltx_p" style="width:51.2pt;">✗</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<br class="ltx_break">
<p id="S3.SS2.p2.1" class="ltx_p">To address the problem of adversaries, Secure Multiparty Computation(SMC) is developed. SMC is a framework that enables multiple parties to securely process data while ensuring that no valuable information is leaked. This scheme allows machine learning models to train sensitive data when most of the parties involved are honest. The components are input parties, computation parties, and result parties. For example, SMC is used as a privacy-preserving method in studies on genome data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, where the biobanks act as the input parties that hold gene data and medical diagnoses information. The neural hosts and other biobanks are chosen as computation parties, and designated recipients are selected as result parties. This method allows us to combine datasets from genome data collected from different individuals without compromising their privacy. There are different protocols for implementing an SMC that differ in size and number of supported input parties. In highly regulated industries such as healthcare, more advanced privacy-preserving methods are needed to facilitate the use of effective machine learning models in the Federated learning framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The privacy-preserving methods are categorized into the two following approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>:</p>
</div>
<section id="S3.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Perturbation Approach</h4>

<div id="S3.SS2.SSSx1.p1" class="ltx_para">
<p id="S3.SS2.SSSx1.p1.5" class="ltx_p">This is a privacy-preserving approach based on adding noise to the data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> such as Differential Privacy, which is a perturbation mechanism that involves adding noise to the data to obscure sensitive data and ensuring security in the federated learning framework. Perturbation methods are effective in securing the data, yet decrease the learning accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Differential privacy is based on the concept of semantic security, which means the encryption systems prevent enclosing any amount of information through the learning process. In Differential Privacy, the outcome is assumed to be the same regardless of the data if the model is implemented on two neighboring databases. Two datasets of D and D’ are considered neighboring if they differ at most in one random variable, and they are written as D<math id="S3.SS2.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="\backsim" display="inline"><semantics id="S3.SS2.SSSx1.p1.1.m1.1a"><mo id="S3.SS2.SSSx1.p1.1.m1.1.1" xref="S3.SS2.SSSx1.p1.1.m1.1.1.cmml">∽</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.1.m1.1b"><ci id="S3.SS2.SSSx1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSSx1.p1.1.m1.1.1">∽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.1.m1.1c">\backsim</annotation></semantics></math>D’. This ensures that the algorithm’s output does not reveal any information about data sharing and analysis. This feature is also known as <math id="S3.SS2.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.SSSx1.p1.2.m2.1a"><mi id="S3.SS2.SSSx1.p1.2.m2.1.1" xref="S3.SS2.SSSx1.p1.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.2.m2.1b"><ci id="S3.SS2.SSSx1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSSx1.p1.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.2.m2.1c">\epsilon</annotation></semantics></math>-<math id="S3.SS2.SSSx1.p1.3.m3.1" class="ltx_Math" alttext="DP" display="inline"><semantics id="S3.SS2.SSSx1.p1.3.m3.1a"><mrow id="S3.SS2.SSSx1.p1.3.m3.1.1" xref="S3.SS2.SSSx1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSSx1.p1.3.m3.1.1.2" xref="S3.SS2.SSSx1.p1.3.m3.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSSx1.p1.3.m3.1.1.1" xref="S3.SS2.SSSx1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.SSSx1.p1.3.m3.1.1.3" xref="S3.SS2.SSSx1.p1.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.3.m3.1b"><apply id="S3.SS2.SSSx1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSSx1.p1.3.m3.1.1"><times id="S3.SS2.SSSx1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSSx1.p1.3.m3.1.1.1"></times><ci id="S3.SS2.SSSx1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSSx1.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS2.SSSx1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSSx1.p1.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.3.m3.1c">DP</annotation></semantics></math>. To satisfy Differential Privacy, the Laplace or Gaussian mechanism is used for data with integer or real-valued outputs, and noise is sampled from a Laplace or Gaussian distribution to ensure <math id="S3.SS2.SSSx1.p1.4.m4.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.SSSx1.p1.4.m4.1a"><mi id="S3.SS2.SSSx1.p1.4.m4.1.1" xref="S3.SS2.SSSx1.p1.4.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.4.m4.1b"><ci id="S3.SS2.SSSx1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSSx1.p1.4.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.4.m4.1c">\epsilon</annotation></semantics></math>-DP for noisy data. The exponential mechanism is used for categorical data, in which each output is associated with a non-zero value for the probability of being selected based on a utility function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. First, we compute the sensitivity of the utility function and then compute the quality score of each output in the database. The output is selected probabilistically. Tuning <math id="S3.SS2.SSSx1.p1.5.m5.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.SSSx1.p1.5.m5.1a"><mi id="S3.SS2.SSSx1.p1.5.m5.1.1" xref="S3.SS2.SSSx1.p1.5.m5.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p1.5.m5.1b"><ci id="S3.SS2.SSSx1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSSx1.p1.5.m5.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p1.5.m5.1c">\epsilon</annotation></semantics></math> in the Differentially Private mechanisms to ensure semantic privacy is one of the challenges of using Differential Privacy for data privacy. Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> demonstrate the trade-off between model convergence and privacy level. They use a client scheduling strategy to improve model convergence while maintaining privacy.
Accumulation of noise can jeopardize the accuracy of results if it exceeds a threshold on several operations, and the threshold is on the depth of the operations rather than the number of operations performed to infuse noise to encrypt the data. The depth of the operations is the maximum degree of the evaluated polynomial. The operation depth is determined by the privacy protection scheme as well as the level of speed and security.</p>
</div>
</section>
<section id="S3.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Cryptography Approach</h4>

<div id="S3.SS2.SSSx2.p1" class="ltx_para">
<p id="S3.SS2.SSSx2.p1.1" class="ltx_p">This approach preserves data privacy using cryptographic primitives and includes homomorphic encryption, garbled circuits, secure processors, and order-preserving encryption.
In this section, we will look closely at Homomorphic Encryption, which preserves privacy and accuracy for the cost of higher running time. The components of Homomorphic Encryption are key generation, encryption, decryption, and evaluation algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Homomorphic Encryption is a cryptosystem that involves ciphering data using a public or private key and sharing the key among peers to decipher the ciphertext. Data is ciphered by mathematically transforming the data using addition and multiplication operators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Variations of Homomorphic Encryption ensure data security among data centers and the central server. Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> use Homomorphic Encryption for cloud-based privacy-preserving image processing, including feature detection, digital watermarking, and content-based image search.
Limitations of Homomorphic Encryption are that the target space is limited to 0,1 binary values, which is not a feasible representative in practice. There are solutions to address this issue that expand the message space to integers. However, in statistical learning, the values are not limited to binary and integer values. Also, The encrypted ciphertext increases drastically in size, sometimes by several orders of magnitude. This requires additional storage and computation power since the learning procedure is more computationally complex. Current Homomorphic Encryption schemes use only addition and multiplication operators. Therefore, comparison tasks are not supported. Improving encryption schemes to support subtraction and comparison operations, such as inequalities, is an open research question in Homomorphic Encryption.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Communication</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Communication between local data centers and the central server requires sufficient connectivity and bandwidth to ensure secure and private communication between the entities and the entire system. While communication efficiency is highly dependent on the existing infrastructure, reducing the number of interactions between the data centers and the central server can improve the efficiency of the federated learning model. For example, Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> built a blockchain-based model for secure data sharing and training using privacy-preserving Support Vector Machines. Their proposed model requires only two interactions in each iteration, which provides higher performance accuracy and data privacy with less computation cost. In addition, the issue of fairness between the data centers and the central server arises with the federated learning framework. The limitations posed by communication efficiency and security are an ongoing challenge in implementing Federated learning at scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Federated Aggregation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Secure aggregation is the function that receives model parameters from local data centers and outputs the aggregated model parameters to update the training model. Numerous studies explore aggregation methods to improve learning accuracy in encrypted data. Lia and Togan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> implemented federated learning with secure aggregation in Python. The authors ensure privacy by using SMC.
Federated Averaging(FedAvg) is the baseline aggregating method in federated learning. In this scheme, an initial global model is used to locally train the datasets located on a network of distributed data centers. The encrypted model parameters are uploaded to the central server, and the average updates of local models are used to update and improve the global model. The model parameters provided by the clients are aggregated at the central server using Equation <a href="#S3.E2" title="In 3.4 Federated Aggregation ‣ 3 Components of Federated Learning ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\delta_{t+1}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}\delta^{k}_{t+1}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">δ</mi><mrow id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.2.3.2" xref="S3.E2.m1.1.1.2.3.2.cmml">t</mi><mo id="S3.E2.m1.1.1.2.3.1" xref="S3.E2.m1.1.1.2.3.1.cmml">+</mo><mn id="S3.E2.m1.1.1.2.3.3" xref="S3.E2.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo rspace="0.111em" stretchy="false" id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">←</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><munderover id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.3.1.2.2" xref="S3.E2.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.3.1.2.3" xref="S3.E2.m1.1.1.3.1.2.3.cmml"><mi id="S3.E2.m1.1.1.3.1.2.3.2" xref="S3.E2.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S3.E2.m1.1.1.3.1.2.3.1" xref="S3.E2.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.3.1.2.3.3" xref="S3.E2.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.3.1.3" xref="S3.E2.m1.1.1.3.1.3.cmml">K</mi></munderover><mrow id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mfrac id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml"><msub id="S3.E2.m1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.1.1.3.2.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S3.E2.m1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><msubsup id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.2.2" xref="S3.E2.m1.1.1.3.2.3.2.2.cmml">δ</mi><mrow id="S3.E2.m1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.3.2.3.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.3.2" xref="S3.E2.m1.1.1.3.2.3.3.2.cmml">t</mi><mo id="S3.E2.m1.1.1.3.2.3.3.1" xref="S3.E2.m1.1.1.3.2.3.3.1.cmml">+</mo><mn id="S3.E2.m1.1.1.3.2.3.3.3" xref="S3.E2.m1.1.1.3.2.3.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.3.2.3.2.3" xref="S3.E2.m1.1.1.3.2.3.2.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">←</ci><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">𝛿</ci><apply id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"><plus id="S3.E2.m1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.2.3.1"></plus><ci id="S3.E2.m1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><apply id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.1.cmml" xref="S3.E2.m1.1.1.3.1">superscript</csymbol><apply id="S3.E2.m1.1.1.3.1.2.cmml" xref="S3.E2.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.2.1.cmml" xref="S3.E2.m1.1.1.3.1">subscript</csymbol><sum id="S3.E2.m1.1.1.3.1.2.2.cmml" xref="S3.E2.m1.1.1.3.1.2.2"></sum><apply id="S3.E2.m1.1.1.3.1.2.3.cmml" xref="S3.E2.m1.1.1.3.1.2.3"><eq id="S3.E2.m1.1.1.3.1.2.3.1.cmml" xref="S3.E2.m1.1.1.3.1.2.3.1"></eq><ci id="S3.E2.m1.1.1.3.1.2.3.2.cmml" xref="S3.E2.m1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E2.m1.1.1.3.1.2.3.3.cmml" xref="S3.E2.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.3.1.3.cmml" xref="S3.E2.m1.1.1.3.1.3">𝐾</ci></apply><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><times id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.1"></times><apply id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2"><divide id="S3.E2.m1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2"></divide><apply id="S3.E2.m1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2">𝑛</ci><ci id="S3.E2.m1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="S3.E2.m1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.3">𝑛</ci></apply><apply id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3">subscript</csymbol><apply id="S3.E2.m1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.3">superscript</csymbol><ci id="S3.E2.m1.1.1.3.2.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.3.2.2">𝛿</ci><ci id="S3.E2.m1.1.1.3.2.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3.2.3">𝑘</ci></apply><apply id="S3.E2.m1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3"><plus id="S3.E2.m1.1.1.3.2.3.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3.3.1"></plus><ci id="S3.E2.m1.1.1.3.2.3.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.1.1.3.2.3.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\delta_{t+1}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}\delta^{k}_{t+1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.2" class="ltx_p">Then, the new model is sent back to the clients. This iterative process continues until the model parameters converge to a specific performance level or the task is completed. FedAvg is a practical approach since it does not require data centers to disclose their data; thus, the models can be trained locally. However, the communication cost can be high. To address this issue, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> explore an adaptive communication frequency aggregation method that helps the algorithm converge faster and have a smaller loss. They also used a gradient sparse approach to reduce communication costs by decreasing the parameters that need to be updated. Another variation of FedAvg is a weighted FedAvg method, which has been proposed and has shown promising performance in experiments on fault diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Also, Hong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> used weighted FedAvg for non-IID imbalanced data and evaluated the model on CIFAR-10 and SVHN benchmark datasets.
Co-operative aggregation is another aggregation method (CO-OP) in which the local models are merged into the global model by using a weighted scheme based on the local models’ age to anticipate the time difference between them and how they have improved at each iteration.
In recent work, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> utilized a discrepancy-based weighted federated averaging method to address the inconsistencies in the contributions of data centers in the global model for federated averaging. The experiments show the effectiveness of federated transfer learning with the proposed averaging method for fault diagnosis.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Privacy-preserving Machine Learning</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In federated learning, a learning model is built and tuned collaboratively between the central server and data centers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Chandiramani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> compared the efficiency of numerous machine learning models in the federated learning framework on the benchmark fashion-MNIST data. Learning from distributed data poses different issues and challenges. Therefore, different machine learning models have been explored to compare the performance and efficiency of the learning models. For example, to increase the security in Android devices, Galvez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> built a federated learning malware classification model using K-Nearest Neighbor, Logistic Regression, Random Forest, and Support Vector Machines as machine learning models.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2311.10832/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Federated Machine Learning Algorithms</figcaption>
</figure>
<div id="S3.SS5.p2" class="ltx_para">
<br class="ltx_break">
<p id="S3.SS5.p2.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.5 Privacy-preserving Machine Learning ‣ 3 Components of Federated Learning ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a quick overview of machine learning models used in federated learning literature.

<br class="ltx_break">In this section, we have provided a detailed survey of the traditional machine learning algorithms and more recent learning schemes in this domain.</p>
</div>
<section id="S3.SS5.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Regression Models</h4>

<div id="S3.SS5.SSSx1.p1" class="ltx_para">
<p id="S3.SS5.SSSx1.p1.1" class="ltx_p">Regression is a predictive modeling approach for identifying the linear and nonlinear relationships between independent variables and the target. Logistic regression has been used in the framework of federated learning for different applications. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> explore a logistic regression model on clients’ credit card and healthcare data. Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> use a logistic regression model to classify illness/health in the cloud environment named POMP. A preprocessing technique and a Bloom filter are also used to reduce the computational complexity in the pre-diagnosis process. The model is implemented using Java and JPBC library. Dunner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> implement a ridge regression model on five benchmark datasets to compare the performance of two on Spark and Open MPI, two distributed machine learning frameworks, and suggest recommendations on improving the models implemented in Spark. The results from this paper show that fine-tuning the parameters in a distributed machine learning model to adapt the system’s specifications and offloading the Spark language-dependent overheads using C++ can improve computation efficiency. The model is trained for when at least one of the data centers is honest or honest but curious using SPINDLE, an operational system for generalized linear models in distributed learning.
<br class="ltx_break">Regression models offer numerous benefits, such as privacy preservation, continuous learning for real-time data analysis, and resource efficiency. However, they deal with the challenges related to heterogeneous data, model aggregation, and privacy concerns, common in many machine learning problems.</p>
</div>
</section>
<section id="S3.SS5.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Support Vector Machines</h4>

<div id="S3.SS5.SSSx2.p1" class="ltx_para">
<p id="S3.SS5.SSSx2.p1.1" class="ltx_p">Support Vector Machines are widely used in medical diagnosis, spam detection, facial recognition, and analyzing financial data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Bost et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> use Support Vector Machines to build efficient privacy-preserving algorithms and evaluate the results on breast cancer diagnosis, credit card approval, audiology, and nursery data. Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> implement linear and nonlinear Support Vector Machines on horizontally partitioned data using the MapReduce framework to preserve privacy. Saerom et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> propose a Homomorphic Encryption-friendly least-squares Support Vector Machines to train toy and real-world datasets that outperformed the logistic regression model. Senekane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> proposed a privacy-preserving Support Vector Machines framework for image classification. Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> focus on an outsourcing scheme for Support Vector Machines classification with an efficient cryptographic primitive named order-preserving encryption. Chanyaswad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> proposed a multi-kernel method using the lossy-encoding scheme to protect the privacy of the data. The training models are Support Vector Machines with an RBF kernel with multiple gamma values and a Signal-to-Noise Ratio-based Support Vector Machines, which is a Signal-to-Noise Ratio for kernel weight design that uses different kernels. The kernel functions are linear, polynomial, Radial Basic Function(RBF), Laplacian, and sigmoid. In terms of privacy, compressing single kernels to form a multi-kernel provides effective results and maximizes utility. The method based on the Signal-to-Noise Ratio method improves the performance compared to uniform and alignment-based methods. Hsu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> design a privacy-preserving system for malware software detection using SGD-based Support Vector Machines and SMC techniques. The paper by Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> proposes a solution for the problem of human motion recognition in multimedia interaction scenarios in a virtual reality environment using Support Vector Machines. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> focus on the issue of computation efficiency and low latency of edge computing for augmented reality applications. Hartmann et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> proposed a Support Vector Machines model in a privacy-preserving setting, called secret Support Vector Machines, for predicting user gender from tweets based on the online and offline evaluation. Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> propose a privacy-preserving feature selection using a multi-class Support Vector Machines named PPM2C, which is evaluated with PAN-SVM and LIB-SVM. The results from this study show that multi-class Support Vector Machines (PPM2C) reduce the chances of overfitting compared to regular Support Vector Machines. Despite being an effective learning method, implementing privacy-preserving Support Vector Machines on data with missing values poses numerous challenges that must be addressed. Omer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> built a distributed Support Vector Machines model with multiple imputations by chained equations on vertically partitioned data. In this work, the privacy of the data is ensured with the Paillier cryptosystem. The evaluation of the proposed scheme shows higher accuracy and lower computation time compared to the centralized model on imputed data.
Medical diagnosis systems can significantly benefit from advances in federated learning. Machine learning methods were used to design a secure framework to prevent severe health conditions by diagnosing patients based on their symptoms and the data collected from wearable monitoring devices that monitor heart rate, temperature, oxygen saturation, and other vital signs, and voice-controlled devices such as Google Assistant, Amazon Echo, and Apple Siri <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Privacy-preserving Support Vector Machines models have been particularly successful in healthcare applications. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> focus on outsourced Support Vector Machines and EPoSVM (Efficient and Privacy-preserving Outsourced Support Vector Machines) for data classification in the Internet of Medical Things, which results in an improvement in learning accuracy and security compared to Support Vector Machines. Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> explored an efficient and privacy-preserving online medical pre-diagnosis framework (eDiag) using nonlinear kernel Support Vector Machines. Ahmed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> developed ”mLung”, a cloud-based privacy-preserving service to detect chronic pulmonary issues from lung sounds such as cough. The analysis is performed on a personal mobile phone to ensure privacy. Medical components of the drugs must be kept private by pharmaceutical companies. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> explored an encrypted kernel Support Vector Machines using Homomorphic Encryption. They have built a sub-image to position different sections of the image so that a face can appear. The authors trained and tested their model on the BioID Face Database. 
<br class="ltx_break">One of the key benefits of Support Vector Machines in federated learning is their ability to generalize well from limited data, making them suitable for scenarios where each participant has a relatively small dataset. However, some of the challenges and limitations of Support Vector Machines are:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Complexity of kernel functions: Support Vector Machines rely on kernel functions to handle the data in nonlinear space. However, selecting appropriate kernel functions in a framework with diverse data sources can be challenging, as different participants may require different kernel types.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Communication overhead: Support Vector Machines are computationally expensive due to dealing with large datasets and complex kernel functions, making them unsuitable for edge devices or environments with limited computational power.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Hyperparameter tuning: Support Vector Machines have multiple hyperparameters, such as the regularization parameter (C) and the kernel parameters. Hyperparameter tuning across multiple participants in a federated setting can be complex and time-consuming.</p>
</div>
</li>
</ul>
<p id="S3.SS5.SSSx2.p1.2" class="ltx_p">While Support Vector Machines can be computationally expensive, they offer strong generalizations that can lead to robust models even with limited local data.</p>
</div>
</section>
<section id="S3.SS5.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Tree Models</h4>

<div id="S3.SS5.SSSx3.p1" class="ltx_para">
<p id="S3.SS5.SSSx3.p1.1" class="ltx_p">Decision Tree and Random Forest are a group of effective and widely used models for data classification and regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Khodaparast et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> presented a Decision Tree algorithm equipped with a federated learning framework for horizontal and vertically partitioned data. Yadav et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> presented a new Decision Tree-based model in a privacy-preserving manner in which the data are partitioned vertically into multiple parties, and the parameters are sent to the central server. Badsha et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> present a privacy-preserving Decision Tree framework to build and learn the tree-based model without requiring the parties to disclose private information. The authors use Homomorphic Encryption to maintain privacy, while the parties are assumed to be honest but curious. The Gini Index is used to measure the classification capability of the model. Canillas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> explored a privacy-preserving Decision Tree framework for private fraud detection systems at SiS ID, a French business platform. The model is used to classify transactions into four risk classes. The model’s accuracy depends on the configuration of the encryption key and the number of nodes for the Decision Tree. The proposed model utilizes a Decision Tree algorithm that helps improve the diagnosis pace and accuracy based on the patient’s symptoms without disclosing the patient’s private data. Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> propose a consent-based privacy-preserving Decision Tree model for the evaluation scheme. The additive Homomorphic Encryption method and a secure comparison model are used. Also, Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> propose a privacy-preserving Decision Tree for classification using additive Homomorphic Encryption, which provides lower computation and communication overhead.
Hou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> explore Random Forest with a Decision Tree for data classification and study the impact of tree depths in Decision Tree on privacy and classification. A privacy budget is allocated for nodes at different depths in the Decision Tree. Guan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> explore a budget allocation mechanism for Decision Tree construction for balancing the excessive noise introduced at leaf nodes. The iterative process speeds up the selective aggregation process. The tree is constructed based on the C4.5 method. Lv et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> proposed a hybrid Decision Tree algorithm for constructing a Random Forest to balance privacy and classification accuracy. In the paper by Xin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, the authors propose a new differentially private greedy Decision Tree algorithm called (DPGDF) which is a combination of greedy trees and parallel combination theory. Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> explore a tree-based data mining model for regression and binary classification tasks. In this work, a privacy-preserving Gradient Boosting Decision Tree (GBDT) model is aggregated into an ensemble. Random Forest is also used for feature engineering. Fritchman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> propose a tree ensemble approach to learn from the data collected in healthcare institutes securely. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> has built a secure Parkinson’s diagnosis framework using non-speech body sounds such as breathing and coughing.
<br class="ltx_break">The main advantages of tree-based models, including Decision Trees and Random Forests, are their interpretability, ensemble learning for robustness, and feature importance analysis.</p>
</div>
</section>
<section id="S3.SS5.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Naïve Bayesian Algorithms</h4>

<div id="S3.SS5.SSSx4.p1" class="ltx_para">
<p id="S3.SS5.SSSx4.p1.1" class="ltx_p">Naïve Bayesian algorithms is a supervised learning algorithm centered around the Bayes theorem, which is based on the assumption that there is conditional independence among each pair of features if the class is known. In smart environments such as smart cities, data privacy is crucial. Amma and Dhanaseelan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> explore a Naïve Bayesian classification framework for privacy-preserving machine learning on the cloud using smart city data. The authors validated the results using Viz road traffic, pollution, and parking data collected from the City Pulse Smart City dataset. Part et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> introduced a novel federated learning architecture that consists of three layers. In the edge layer, the data is processed, the machine learning models are trained in the fog layer, and the results are aggregated in the centralized cloud layer. Data partitioning poses specific challenges when learning from the data in a distributed setting. Vaidya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> investigate a privacy-preserving Naïve Bayesian classification model and compare the results on horizontal and vertically partitioned data. Yurochkin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> propose a Bayesian non-parametric federated learning framework with neural networks. The model is evaluated on two benchmark data sets for image classification. The paper by García-Recuero <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> addresses the issue of detecting and discouraging abusive behavior in online social networking applications such as Twitter by limiting the accessibility to user-sensitive information. This work uses feature engineering on relative importance calculated from the Random Forest learning algorithm. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> use Naïve Bayesian and a hyperplane-based decision model for classification. Furthering this work, Chai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> propose an outsourced encryption protocol to improve the security vulnerability of Li’s model. The classification models are trained using Scikit-learn. Paillier cryptosystems are used in this work due to light operations. In a paper by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, the authors propose a communication-efficient privacy-preserving framework based on the Naïve Bayesian method to predict the disease risk for e-health applications.
Sharkala et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> introduce a privacy-preserving machine learning algorithm for horizontally and vertically partitioned data based on a tree augmented Naïve Bayesian classifier. A third party conducts the operations, and the data is encrypted. Teo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> propose a privacy-preserving algorithm using kernel regression and Naïve Bayesian classifier for multiparty computation, and the Paillier cryptosystem is used for encryption. Medical diagnosis systems are the main application of privacy-preserving machine learning. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> build a secure diagnosis scheme using a Naïve Bayesian classifier. Furthermore, malware detection systems protect the user’s identity by identifying malware API call fragments. Lin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> propose a privacy-preserving Naïve Bayesian model for malware detection. Talbi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> compare their classification algorithms of Naïve Bayesian, Decision Tree, and logistic regression on encrypted data. 
<br class="ltx_break">Naive Bayesian algorithms provide probabilistic predictions and can quantify uncertainty. In federated settings, this is a significant advantage for risk assessment and decision-making, particularly in applications such as crisis management and disaster response scenarios where uncertainty plays a significant role.</p>
</div>
</section>
<section id="S3.SS5.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Deep Learning</h4>

<div id="S3.SS5.SSSx5.p1" class="ltx_para">
<p id="S3.SS5.SSSx5.p1.1" class="ltx_p">Deep learning has been the dominant learning method from structured and unstructured data in recent years. Deep learning is the burgeoning powerful technique in the field of machine learning. Deep architectures are useful for learning complicated patterns in large-scale data, attracting much attention in academia and industry. Different topologies and architectures with real-world applications exist. These models have been used in different areas such as Computer Vision, Natural Language Processing, and speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. The improvements in implementing a secure Neural Network model on the cloud create a platform for scalable Neural Networks to be used as a service <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. A Recurrent Neural Network is a variation of a forwarding propagation Neural Network in which the neurons in the hidden layers receive the input value with a delay in time and access information from previous iterations in the current layer. Recurrent Neural Network is useful in Natural Language Processing, where knowledge about the previous words in a sentence is necessary for predicting the next word. Text mining and Natural Language Processing, which is the process of extracting knowledge from text documents, are used for learning from data collected from highly sensitive resources such as homeland security for crime fight and detecting terrorism activities. Therefore, building secure federated learning text analysis methods is necessary to ensure no sensitive information is disclosed. To this end, Costantino et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> propose using an out-of-bag classification method to detect terrorist activities on Twitter. Convolutional Neural Network is a deep learning architecture that has gained popularity in Computer Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. This architecture uses one or more convolutional layers to extract high-level features. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> investigate a privacy-preserving Natural Language Model framework to compute word representations using deep learning. Abadi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> explore training a Neural Network model with a non-convex objective function, implemented using TensorFlow in Python, with differentially private Stochastic Gradient Descent method, the moments’ accountant, and hyperparameter tuning. The security protocol is a semi-honest security model that is a secure computation method based on additive secret-sharing techniques (secure addition, subtraction, and secure multiplication). Xia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> designed a Graph Convolutional Network model for predicting traffic flow quickly and efficiently.
Deep learning methods can also be combined with other classifiers, such as linear Support Vector Machines, for the classification of images. Niu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> explore a deep learning framework for mobile sensing systems. Lin et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> propose a Recurrent Neural Network framework called the Predictive Clinical Decision (PCD) scheme, which is used for e-health applications.
Eye-tracking devices are the main technology in virtual reality and augmented reality that can improve efficiency through gaze-based optimization methods. The eye-wear and eye-tracking devices used in the auto industry can pose privacy issues for the driver and bystanders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. Therefore, Steil et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> explore a privacy-preserving method for a first-person video dataset of daily life recordings. The authors propose the PrivacEye method that combines Computer Vision with eye movement analysis techniques. Another privacy concern with Computer Vision technologies in facial recognition systems such as Google Street View is when personal images of individuals are shared via different data centers. 
<br class="ltx_break">Deep learning models have demonstrated great potential for highly accurate and competitive results when dealing with diverse and large datasets. While increasing model complexity in neural network architectures helps improve generalization, excess complexity results in overfitting and computation overhead. Adaptation, optimization, and the integration of privacy-preserving techniques are essential to harness the strengths of deep learning while mitigating the specific challenges and limitations present in federated settings.</p>
</div>
</section>
<section id="S3.SS5.SSSx6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Unsupervised Machine Learning</h4>

<div id="S3.SS5.SSSx6.p1" class="ltx_para">
<p id="S3.SS5.SSSx6.p1.1" class="ltx_p">Current Federated learning machine learning-based models are constructed based on supervised learning. However, in most applications, no or little labeled data exists. Thus, it is appropriate to use unsupervised learning methods. While there has been considerable progress on federated transfer learning to cope with data with few labels, applying unsupervised learning in a federated setting remains a bottleneck for many applications. Clustering techniques have been employed to deal with the challenges of unlabeled data. While K-means clustering is widely used for pattern recognition in gene detection and image segmentation, a modified framework is required when the data is sensitive. Zhu and Li <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> proposed a secure aggregation and division protocol based on Homomorphic Encryption to build a secure clustering algorithm. Al-Saeidi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> proposed a clustering analysis for improving the communication cost in federated learning using the human activity recognition dataset. A secure weighted average protocol and secure number comparison protocol are used for privacy-preserving. Five different classification algorithms were explored: multi-layer perceptron, K Nearest Neighbor, Sequential Minimal Optimization, Naïve Bayesian, and J48 (an implementation of Decision Tree classification in WEKA). Anikin and Gazimov <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> proposed a clustering algorithm named Density-Based Spatial Clustering of Applications with NOISE (DBSCAN) for vertically partitioned data. Romsaiyud et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> investigate a privacy-preserving K Nearest Neighbor model for pattern recognition, with automated hyperparameter tuning to improve model accuracy and a cryptographic hash function to ensure data security. 
<br class="ltx_break">The benefits of unsupervised learning techniques in federated learning are:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Privacy-preserving clustering: Clustering algorithms can perform data analysis without the need for explicit labels or excess information sharing, which is beneficial when dealing with sensitive data.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Data exploration and anomaly detection: Unsupervised models excel at data exploration, allowing practitioners to identify the underlying patterns, anomalies, and outliers within local datasets. This exploratory capability is valuable for uncovering insights without exposing private data.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Reduced labeling effort: Limited labeled data is a well-known issue in machine learning. Unsupervised learning models can reduce the labeling effort by enabling semi-supervised or self-supervised learning approaches without exposing the data.</p>
</div>
</li>
</ul>
<p id="S3.SS5.SSSx6.p1.2" class="ltx_p">With appropriate evaluation strategies, we can effectively leverage the strengths of unsupervised learning while mitigating the specific challenges and limitations of federated learning.</p>
</div>
</section>
<section id="S3.SS5.SSSx7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Ensemble Learning</h4>

<div id="S3.SS5.SSSx7.p1" class="ltx_para">
<p id="S3.SS5.SSSx7.p1.1" class="ltx_p">Ensemble learning is a general approach that seeks to improve learning performance by aggregating the results from multiple classifiers. The paper published by Attota et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> used an ensemble multi-view federated learning model to identify intrusion in IoT devices to improve model efficiency against different attacks. Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> apply edge computing methods for medical diagnosis using the XGBoost model. They use a lightweight, adaptive boosting classification method (AdaBoost) for facial recognition on FERET, a standard face recognition evaluation database. The data is encrypted when sent to two servers for distributed learning. 
<br class="ltx_break">Ensemble learning is an effective approach in the machine learning domain and has yet to be extensively explored in the federated learning framework. There is a very limited number of papers published in peer-reviewed journals using this approach. However, they demonstrate the potential of ensemble learning in federated learning.</p>
</div>
</section>
<section id="S3.SS5.SSSx8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Meta-heuristic Approaches</h4>

<div id="S3.SS5.SSSx8.p1" class="ltx_para">
<p id="S3.SS5.SSSx8.p1.1" class="ltx_p">Apart from the more commonly used machine learning algorithms mentioned above, meta-heuristic approaches have also been introduced to the federated learning domain. Polap and Wozniak <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> used a novel approach based on parallelism to improve classification models’ efficiency in federated learning. The authors demonstrate the effectiveness of their approach when the sample size is relatively small. In another paper, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>, they explore using a meta-heuristic federated learning framework for image classification in the presence of poisoning attacks. With application in IoT and smart city services, Qolomany et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> investigate using Particle Swarm Optimization for efficiently tuning the hyperparameters in the machine learning model. Utilizing meta-heuristic approaches can be further explored as a novel approach to improve the efficiency of the training model in federated learning. 
<br class="ltx_break">Metaheuristic algorithms are versatile and adaptable to various problem domains. They can be customized to suit the specific requirements and constraints of federated learning scenarios.</p>
</div>
</section>
<section id="S3.SS5.SSSx9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Blockchain Technology</h4>

<div id="S3.SS5.SSSx9.p1" class="ltx_para">
<p id="S3.SS5.SSSx9.p1.1" class="ltx_p">Despite improvements in Homomorphic Encryption and Differential Privacy in preserving privacy, there is always a trade-off between learning accuracy and privacy. To overcome such issues, federated learning can be equipped with blockchain technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>. Utilizing blockchain technology in federated learning is an emerging field in federated learning and decentralized data storage and processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> survey the advances and challenges of federated learning with blockchain technology. In edge computing and learning from IoT data, blockchain federated learning is a solution to issues in data storage, communication cost, and privacy of sensitive data. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> use a blockchain-distributed setting as the groundwork for federated learning to ensure additional privacy protection from servers and prevent malicious attach on user-sensitive data. Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> proposed a federated learning framework based on a blockchain mechanism and introduced reputation as a metric to identify reliable data and propose a reliable framework to learn from the data on mobile networks. Later, Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> used multiple blockchains to design a cross-chain framework to improve the scalability and communication efficiency of federated learning for training the data on IoT devices. An example of other applications is a classification of COVID-19 cases from multiple resources using blockchain-based federated learning. Comparing the federated learning framework to centralized models shows improvement in diagnosing COVID-19 patients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>. 
<br class="ltx_break">Leveraging blockchain technology within the context of federated learning introduces many benefits, such as:</p>
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">Data privacy and security: Blockchain technology has inherent privacy protection capabilities allowing participants to maintain control over their data while securely contributing to the global model.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">Transparent and trustworthy transactions: The decentralized nature of blockchain ensures that all transactions and updates are transparent and traceable. This feature mitigates concerns of data tampering and adversarial attacks.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">Smart contracts for governance: Smart contracts are programmable scripts executed on the blockchain that can be employed for governing federated learning agreements and model updates. This automation streamlines the process and enforces predefined rules and policies, reducing the risk of malfunction and misuse.</p>
</div>
</li>
</ul>
<p id="S3.SS5.SSSx9.p1.2" class="ltx_p">Despite its benefits, implementing a blockchain-based federated learning system is complex and requires expertise in both blockchain technology and machine learning. Developing and maintaining such a system is challenging as some blockchain networks consume a significant amount of energy. This environmental impact may not align with sustainability goals in federated learning.</p>
</div>
</section>
<section id="S3.SS5.SSSx10" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Reinforcement Learning</h4>

<div id="S3.SS5.SSSx10.p1" class="ltx_para">
<p id="S3.SS5.SSSx10.p1.1" class="ltx_p">Incorporating other learning approaches into federated learning has shown promising results in different applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> built a reinforcement learning framework, which is a learning system through trial-and-error interactions between agents and environments combined with cloud computing and IoT technology to create a dynamic system for cancer patient treatment regimes. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> proposed a reinforcement learning mechanism to introduce a rewards system that optimizes accuracy and communication efficiency. A reinforcement learning approach is also used for evaluating node contributions and improving the pricing strategy in federated learning for IoT devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>. Krouka et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> investigate different aggregation schemes in a reinforcement learning-federated learning framework to improve communication costs.
<br class="ltx_break">The benefits of combining reinforcement learning and federated learning are :</p>
<ul id="S3.I4" class="ltx_itemize">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p">Dynamic model adaptation: Reinforcement Learning models can adapt dynamically to changing data distributions and evolving environments. In federated learning, where data sources may drift or have different characteristics, Reinforcement Learning can facilitate model adjustments for improved performance.</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p">Sequential decision-making: Reinforcement Learning is well-suited for sequential decision-making tasks. In federated learning scenarios, this capability can be valuable for applications that involve sequential interactions or recommendations.</p>
</div>
</li>
</ul>
<p id="S3.SS5.SSSx10.p1.2" class="ltx_p">Reinforcement Learning models often require extensive training and exploration of different policies, which can be computationally expensive and time-consuming. Reinforcement Learning must efficiently find the optimal policy by balancing the exploration of new actions and the exploitation of known policies. Managing the trade-off between exploration and exploitation is necessary to avoid excessive data sharing or overfitting.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Open Problems</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Federated learning is at the interface of several research areas, such as optimization, distributed learning, cryptography, and communication theory. In the last few years, many algorithmic developments have been accomplished with a focus on theory and applications. However, there are still several challenges and open problems in federated learning. In a recent paper, some advances and open problems are discussed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>.
In addition, central to the future of federated learning are the data-driven challenges outlined in Table <a href="#S4.T2" title="Table 2 ‣ 4 Challenges and Open Problems ‣ Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Data-driven Issues in Federated Learning</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p" style="width:71.1pt;">Issues</span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p" style="width:213.4pt;">  Challenges</span>
</span>
</th>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.2.1.1.1" class="ltx_p" style="width:71.1pt;">Unlabeled Data</span>
</span>
</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.2.3.1.1" class="ltx_p" style="width:213.4pt;">Labeling the data require considerable time and resources.</span>
</span>
</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt"></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.3.1.1.1" class="ltx_p" style="width:71.1pt;">Non-IID Data</span>
</span>
</td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.3.3.1.1" class="ltx_p" style="width:213.4pt;">Different distributions in data centers negatively affect learning performance and efficiency.</span>
</span>
</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<td id="S4.T2.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.T2.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.4.1.1.1" class="ltx_p" style="width:71.1pt;">Imbalanced Data</span>
</span>
</td>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_top ltx_border_b ltx_border_t"></td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.T2.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.4.3.1.1" class="ltx_p" style="width:213.4pt;">imbalanced Data result in reduced learning performance in the minority class.</span>
</span>
</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p2" class="ltx_para">
<br class="ltx_break">
<p id="S4.p2.1" class="ltx_p">The data-driven issues pose various challenges for efficient and accurate training of federated learning models.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Unlabeled data: The lack of annotated data with good quality is one of the limitations in this setting. When the data is unlabeled, and labeling the data is either impossible or too cost-inefficient, it is important to modify the machine learning algorithms to be able to learn from partially annotated data efficiently.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Non-Independent and Identically Distributed (Non-IID) Data: Non-IID data in federated learning refer to differences in the distribution of the available data over the data centers. It is also possible that the data become locally non-IID over time, which requires modifying existing algorithms or developing new ones. To address this issue, Sattler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> proposed a compression network to improve the communication efficiency and robustness of Federated learning on non-IID data. Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> and Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> investigated the recent advances in solving non-IID data in federated learning and the future trends in research on this issue. They also compared different model architectures of deep learning used in the literature on federated learning.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Imbalanced data: The data is imbalanced when the number of instances in one class is significantly larger than in the other. This issue can arise in the taxonomy of the data centers or the overall data. The challenges posed by data-driven issues require experiments on achieving highly efficient and accurate learning systems.</p>
</div>
</li>
</ul>
<p id="S4.p2.2" class="ltx_p">Other challenges that need to be overcome are:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Fairness: The issue of fairness in federated learning occurs at different levels, such as fairness in communication capacities, machine learning models, and aggregation results. To this end, fairness metrics must be defined to evaluate the model from privacy, accuracy, and fairness perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>. Also, Lyu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> propose the concept of collaborative federated learning, which ensures fairness in how the clients impact the global model in the aggregation process. Despite the improvements, there is a lack of an integrated approach that ensures fairness in different aspects of federated learning.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Scalability: To implement a federated learning protocol at scale, it must avoid the curse of dimensionality when data is large. To tackle the challenge of dimensionality, Principal Component Analysis has been employed in unsupervised settings such as the work by Al-Rubaie <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, which developed a privacy-preserving Principal Component Analysis to reduce the dimensionality of the horizontally partitioned data. In addition, other methods, such as Discriminant Component Analysis, can be explored for efficient feature engineering and dimensionality reduction to improve accuracy and computation cost and uphold privacy.</p>
</div>
</li>
</ul>
<p id="S4.p2.3" class="ltx_p">Possible future research directions are:</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Game theory: Recent connections between game theory and control can provide new insights into federated learning and new algorithmic developments. An attempt along these lines is the paper of Mehrjou<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>, which connects federated learning with mean field games by presenting federated learning as a differential game and discussing the properties of the equilibrium of this game.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Quantum optimization: Another future research area is quantum optimization applied to federated learning. Distributed learning across several quantum computers could significantly improve the training time and potentially improve data privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>. Connections with multi-objective optimization, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>, can benefit algorithmic developments.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Multiple Kernel Learning: Exploring the connections between federated learning and multiple kernel learning also holds potential for advancing algorithmic solutions.</p>
</div>
</li>
</ul>
<p id="S4.p2.4" class="ltx_p">Furthermore, federated learning must navigate complex challenges, including exploring more advanced ways of combining local networks, using different machine learning and ensemble models in addition to preprocessing techniques, performing experiments on larger datasets and further enhancement efficiency, and designing accurate and reliable machine learning models suitable for GPU implementation.
In summary, federated learning stands as a field where innovation and collaboration are essential. By collectively addressing these challenges and pursuing interdisciplinary connections, The research community can integrate federated learning into a future where it serves as a cornerstone for secure, efficient, and privacy-conscious machine learning applications.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have conducted an extensive literature review of federated learning from the machine learning point of view, complementing other recent literature reviews. This review will be useful for researchers in academia and industry and possibly a useful tool for graduate students who want to work in this area. Federated learning was proposed as a solution to the issue of data leakage and loss of privacy in machine learning. With a large amount of data at hand, there is a burgeoning demand for federated learning as potentially being the solution to private and environmental-friendly machine learning at scale. Decentralized learning strategy and privacy protection mechanisms in federated learning grant us access to otherwise unavailable data. Hence, we can expand machine learning in domains such as IoT and healthcare and crisis management in natural and human-caused disasters that require privacy preservation. In recent years, there has been an increasing number of papers published in this domain, and the goal of this study is to provide an overview of federated learning and the existing privacy-preserving machine learning algorithms used in this framework, in addition to their potential and limitations in various applications. Despite our effort to thoroughly search the literature on federated learning, we limited our search to published papers in peer-reviewed journals in English. Therefore, other novel approaches might be found in the papers not included in this survey.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Declarations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text ltx_font_bold">Conflict of interest</span> The authors declare no competing interests</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Cynthia Dwork and Kobbi Nissim.

</span>
<span class="ltx_bibblock">Privacy-preserving datamining on vertically partitioned databases.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Annual International Cryptology Conference</span>, pages 528–544.
Springer, 2004.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ping Li, Jin Li, Zhengan Huang, Chong-Zhi Gao, Wen-Bin Chen, and Kai Chen.

</span>
<span class="ltx_bibblock">Privacy-preserving outsourced classification in cloud computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Cluster Computing</span>, 21(1):277–286, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
ADP Team et al.

</span>
<span class="ltx_bibblock">Learning with privacy at scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Apple Mach. Learn. J</span>, 1(8):1–25, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.

</span>
<span class="ltx_bibblock">Towards personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
pages 1–17, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys and Tutorials</span>, 22(3):2031–2063,
2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Latif U. Khan, Shashi Raj Pandey, Nguyen H. Tran, Walid Saad, Zhu Han, Minh
N. H. Nguyen, and Choong Seon Hong.

</span>
<span class="ltx_bibblock">Federated learning for edge networks: Resource optimization and
incentive mechanism.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Communications Magazine</span>, 58(10):88–93, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dinh C. Nguyen, Ming Ding, Pubudu N. Pathirana, Aruna Seneviratne, Jun Li, and
H. Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys and Tutorials</span>, 23(3):1622–1658,
2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Latif U. Khan, Walid Saad, Zhu Han, Ekram Hossain, and Choong Seon Hong.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: Recent advances, taxonomy,
and open challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys and Tutorials</span>, 23(3):1759–1799,
2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Iraklis Varlamis, Christos Sardianos, Christos Chronis, George Dimitrakopoulos,
Yassine Himeur, Abdullah Alsalemi, Faycal Bensaali, and Abbes Amira.

</span>
<span class="ltx_bibblock">Using big data and federated learning for generating energy
efficiency recommendations.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Journal of Data Science and Analytics</span>, pages
1–17, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei
Wang.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Journal of Healthcare Informatics Research</span>, 5(1):1–19, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ines Feki, Sourour Ammar, Yousri Kessentini, and Khan Muhammad.

</span>
<span class="ltx_bibblock">Federated learning for covid-19 screening from chest x-ray images.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Applied Soft Computing</span>, 106:107330, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi
Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus
Maier-Hein, et al.

</span>
<span class="ltx_bibblock">The future of digital health with federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">NPJ digital medicine</span>, 3(1):1–7, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Elaheh Jafarigol and Theodore Trafalis.

</span>
<span class="ltx_bibblock">Imbalanced learning with parametric linear programming support vector
machine for weather data application.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">SN Computer Science</span>, 1:1–11, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Nuria Rodríguez-Barroso, Goran Stipcich, Daniel Jiménez-López,
José Antonio Ruiz-Millán, Eugenio Martínez-Cámara, Gerardo
González-Seco, M Victoria Luzón, Miguel Angel Veganzones, and
Francisco Herrera.

</span>
<span class="ltx_bibblock">Federated learning and differential privacy: Software tools analysis,
the sherpa. ai fl framework and methodological guidelines for preserving data
privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Information Fusion</span>, 64:270–292, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Mohammed Aledhari, Rehma Razzak, Reza M. Parizi, and Fahad Saeed.

</span>
<span class="ltx_bibblock">Federated learning: A survey on enabling technologies, protocols, and
applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:140699–140725, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Roseline Oluwaseun Ogundokun, Sanjay Misra, Rytis Maskeliunas, and Robertas
Damasevicius.

</span>
<span class="ltx_bibblock">A review on federated learning and machine learning approaches:
Categorization, application areas, and blockchain technology.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Information</span>, 13(5):263, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Maoguo Gong, Yu Xie, Ke Pan, Kaiyuan Feng, and Alex Kai Qin.

</span>
<span class="ltx_bibblock">A survey on differentially private machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE computational intelligence magazine</span>, 15(2):49–64, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Latif U Khan, Walid Saad, Zhu Han, Ekram Hossain, and Choong Seon Hong.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: Recent advances, taxonomy,
and open challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys and Tutorials</span>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Rodolfo Stoffel Antunes, Cristiano André da Costa, Arne Küderle,
Imrana Abdullahi Yari, and Björn Eskofier.

</span>
<span class="ltx_bibblock">Federated learning for healthcare: Systematic review and architecture
proposal.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
13(4):1–23, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Murat Kantarcıoglu, Jaideep Vaidya, and C Clifton.

</span>
<span class="ltx_bibblock">Privacy preserving naive bayes classifier for horizontally
partitioned data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE ICDM workshop on privacy preserving data mining</span>, pages
3–9, 2003.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Huajie Chen, Ali Burak Ünal, Mete Akgün, and Nico Pfeifer.

</span>
<span class="ltx_bibblock">Privacy-preserving svm on outsourced genomic data via secure
multi-party computation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the Sixth International Workshop on Security
and Privacy Analytics</span>, pages 61–69, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
Dehghantanha, and Gautam Srivastava.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 115:619–640, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Dan Bogdanov, Liina Kamm, Sven Laur, and Ville Sokk.

</span>
<span class="ltx_bibblock">Implementation and evaluation of an algorithm for cryptographically
private principal component analysis on genomic data.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE/ACM transactions on computational biology and
bioinformatics</span>, 15(5):1427–1432, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jemal H Abawajy and Mohammad Mehedi Hassan.

</span>
<span class="ltx_bibblock">Federated internet of things and cloud computing pervasive patient
health monitoring system.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Communications Magazine</span>, 55(1):48–53, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Nikita Lisin and Sergey Zapechnikov.

</span>
<span class="ltx_bibblock">Order-preserving encryption as a tool for privacy-preserving machine
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2020 IEEE Conference of Russian Young Researchers in
Electrical and Electronic Engineering (EIConRus)</span>, pages 2090–2092, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Sara Salim, Nour Moustafa, Benjamin Turnbull, and Imran Razzak.

</span>
<span class="ltx_bibblock">Perturbation-enabled deep federated learning for preserving internet
of things-based social networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM)</span>, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Mehmet Emre Gursoy, Ali Inan, Mehmet Ercan Nergiz, and Yucel Saygin.

</span>
<span class="ltx_bibblock">Privacy-preserving learning analytics: challenges and techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Learning technologies</span>, 10(1):68–81, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Stacey Truex, Ling Liu, Mehmet Emre Gursoy, and Lei Yu.

</span>
<span class="ltx_bibblock">Privacy-preserving inductive learning with decision trees.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Congress on Big Data (BigData
Congress)</span>, pages 57–64, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin,
Tony Q. S. Quek, and H. Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454–3469, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Nathan Dowlin, Ran Gilad-Bachrach, Kim Laine, Kristin Lauter, Michael Naehrig,
and John Wernsing.

</span>
<span class="ltx_bibblock">Manual for using homomorphic encryption for bioinformatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 105(3):552–567, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Paulo Martins, Leonel Sousa, and Artur Mariano.

</span>
<span class="ltx_bibblock">A survey on fully homomorphic encryption: An engineering perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 50(6):1–33, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zhan Qin, Jian Weng, Yong Cui, and Kui Ren.

</span>
<span class="ltx_bibblock">Privacy-preserving image processing in the cloud.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE Cloud Computing</span>, 5(2):48–57, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Meng Shen, Xiangyun Tang, Liehuang Zhu, Xiaojiang Du, and Mohsen Guizani.

</span>
<span class="ltx_bibblock">Privacy-preserving support vector machine training over
blockchain-based encrypted iot data in smart cities.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 6(5):7702–7712, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Sicong Zhou, Huawei Huang, Wuhui Chen, Pan Zhou, Zibin Zheng, and Song Guo.

</span>
<span class="ltx_bibblock">Pirate: A blockchain-based secure framework of distributed machine
learning in 5g networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, 34(6):84–91, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Dragos Lia and Mihai Togan.

</span>
<span class="ltx_bibblock">Privacy-preserving machine learning using federated learning and
secure aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">2020 12th International Conference on Electronics, Computers
and Artificial Intelligence (ECAI)</span>, pages 1–6, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yanbin Li, Ziming He, Xingjian Gu, Huanliang Xu, and Shougang Ren.

</span>
<span class="ltx_bibblock">Afedavg: communication-efficient federated learning aggregation with
adaptive communication frequency and gradient sparse.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Journal of Experimental &amp; Theoretical Artificial Intelligence</span>,
pages 1–23, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Junbin Chen, Jipu Li, Ruyi Huang, Ke Yue, Zhuyun Chen, and Weihua Li.

</span>
<span class="ltx_bibblock">Federated transfer learning for bearing fault diagnosis with
discrepancy-based weighted federated averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</span>, 71:1–11,
2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Mannsoo Hong, Seok-Kyu Kang, and Jee-Hyong Lee.

</span>
<span class="ltx_bibblock">Weighted averaging federated learning based on example forgetting
events in label imbalanced non-iid.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 12(12):5806, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Junbin Chen, Jipu Li, Ruyi Huang, Ke Yue, Zhuyun Chen, and Weihua Li.

</span>
<span class="ltx_bibblock">Federated transfer learning for bearing fault diagnosis with
discrepancy-based weighted federated averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</span>, 71:1–11,
2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Qing He, Ning Li, Wen-Juan Luo, and ZZ Shi.

</span>
<span class="ltx_bibblock">A survey of machine learning algorithms for big data.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Pattern recognition and artificial intelligence</span>,
27(4):327–336, 2014.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Kunal Chandiramani, Dhruv Garg, and N Maheswari.

</span>
<span class="ltx_bibblock">Performance analysis of distributed and federated learning models on
private data.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Procedia Computer Science</span>, 165:349–355, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Rafa Gálvez, Veelasha Moonsamy, and Claudia Diaz.

</span>
<span class="ltx_bibblock">Less is more: A privacy-respecting android malware classifier using
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proceedings on Privacy Enhancing Technologies</span>, 4:96–116, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yi Yang, Shuai Huang, Wei Huang, and Xiangyu Chang.

</span>
<span class="ltx_bibblock">Privacy-preserving cost-sensitive learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
32(5):2105–2116, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wei Guo, Jun Shao, Rongxing Lu, Yining Liu, and Ali A Ghorbani.

</span>
<span class="ltx_bibblock">A privacy-preserving online medical prediagnosis scheme for cloud
environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 6:48946–48957, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Celestine Dünner, Thomas Parnell, Kubilay Atasu, Manolis Sifalakis, and
Haralampos Pozidis.

</span>
<span class="ltx_bibblock">Understanding and optimizing the performance of distributed machine
learning applications on apache spark.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Big Data (Big Data)</span>,
pages 331–338, 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Takahiro Maekawa, Ayana Kawamura, Yuma Kinoshita, and Hitoshi Kiya.

</span>
<span class="ltx_bibblock">Privacy-preserving svm computing in the encrypted domain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">2018 Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference (APSIPA ASC)</span>, pages 897–902, 2018.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Raphael Bost, Raluca Ada Popa, Stephen Tu, and Shafi Goldwasser.

</span>
<span class="ltx_bibblock">Machine learning classification over encrypted data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">NDSS</span>, volume 4324, page 4325, 2015.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Kaihe Xu, Hao Yue, Linke Guo, Yuanxiong Guo, and Yuguang Fang.

</span>
<span class="ltx_bibblock">Privacy-preserving machine learning algorithms for big data systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">2015 IEEE 35th international conference on distributed
computing systems</span>, pages 318–327, 2015.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Saerom Park, Junyoung Byun, Joohee Lee, Jung Hee Cheon, and Jaewook Lee.

</span>
<span class="ltx_bibblock">He-friendly algorithm for privacy-preserving svm training.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:57414–57425, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Makhamisa Senekane.

</span>
<span class="ltx_bibblock">Differentially private image classification using support vector
machine and differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Machine Learning and Knowledge Extraction</span>, 1(1):483–491, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Jinwen Liang, Zheng Qin, Jianbing Ni, Xiaodong Lin, and Xuemin Shen.

</span>
<span class="ltx_bibblock">Efficient and privacy-preserving outsourced svm classification in
public cloud.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">ICC 2019-2019 IEEE International Conference on Communications
(ICC)</span>, pages 1–6, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Thee Chanyaswad, J Morris Chang, and Sun-Yuan Kung.

</span>
<span class="ltx_bibblock">A compressive multi-kernel method for privacy-preserving machine
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">2017 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 4079–4086, 2017.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Ruei-Hau Hsu, Yi-Cheng Wang, Chun-I Fan, Bo Sun, Tao Ban, Takeshi Takahashi,
Ting-Wei Wu, and Shang-Wei Kao.

</span>
<span class="ltx_bibblock">A privacy-preserving federated learning system for android malware
detection based on edge computing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">2020 15th Asia Joint Conference on Information Security
(AsiaJCIS)</span>, pages 128–136, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Fuquan Zhang, Tsu-Yang Wu, Jeng-Shyang Pan, Gangyi Ding, and Zuoyong Li.

</span>
<span class="ltx_bibblock">Human motion recognition based on svm in vr art media interaction
environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Human-centric Computing and Information Sciences</span>, 9(1):1–15,
2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Dawei Chen, Linda Jiang Xie, BaekGyu Kim, Li Wang, Choong Seon Hong, Li-Chun
Wang, and Zhu Han.

</span>
<span class="ltx_bibblock">Federated learning based mobile edge computing for augmented reality
applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">2020 international conference on computing, networking and
communications (ICNC)</span>, pages 767–773, 2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Valentin Hartmann, Konark Modi, Josep M Pujol, and Robert West.

</span>
<span class="ltx_bibblock">Privacy-preserving classification with secret vector machines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th ACM International Conference on
Information &amp; Knowledge Management</span>, pages 475–484, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Yunmei Lu, Mingyuan Yan, Meng Han, Qingliang Zhang, and Yanqing Zhang.

</span>
<span class="ltx_bibblock">Privacy preserving multiclass classification for horizontally
distributed data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th Annual SIG Conference on Information
Technology Education</span>, SIGITE ’18, page 165, New York, NY, USA, 2018.
Association for Computing Machinery.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Mohammed Z Omer, Hui Gao, and Nadir Mustafa.

</span>
<span class="ltx_bibblock">Privacy-preserving of svm over vertically partitioned with imputing
missing data.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Distributed and Parallel Databases</span>, 35(3):363–382, 2017.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Ranya Aloufi, Hamed Haddadi, and David Boyle.

</span>
<span class="ltx_bibblock">Privacy-preserving voice analysis via disentangled representations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 ACM SIGSAC Conference on Cloud
Computing Security Workshop</span>, pages 1–14, 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Jing Wang, Libing Wu, Huaqun Wang, Kim-Kwang Raymond Choo, and Debiao He.

</span>
<span class="ltx_bibblock">An efficient and privacy-preserving outsourced support vector machine
training for internet of medical things.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(1):458–473, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Hui Zhu, Xiaoxia Liu, Rongxing Lu, and Hui Li.

</span>
<span class="ltx_bibblock">Efficient and privacy-preserving online medical prediagnosis
framework using nonlinear svm.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health informatics</span>,
21(3):838–850, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Mohsin Y Ahmed, Md Mahbubur Rahman, Viswam Nathan, Ebrahim Nemati, Korosh
Vatanparvar, and Jilong Kuang.

</span>
<span class="ltx_bibblock">mlung: Privacy-preserving naturally windowed lung activity detection
for pulmonary patients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">2019 IEEE 16th International Conference on Wearable and
Implantable Body Sensor Networks (BSN)</span>, pages 1–4, 2019.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Qian Wang, Minxin Du, Xiuying Chen, Yanjiao Chen, Pan Zhou, Xiaofeng Chen, and
Xinyi Huang.

</span>
<span class="ltx_bibblock">Privacy-preserving collaborative model learning: The case of word
vector training.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
30(12):2381–2393, 2018.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Zekun Li and Shuyu Li.

</span>
<span class="ltx_bibblock">Random forest algorithm under differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">2017 IEEE 17th International Conference on Communication
Technology (ICCT)</span>, pages 1901–1905, 2017.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Fatemeh Khodaparast, Mina Sheikhalishahi, Hassan Haghighi, and Fabio
Martinelli.

</span>
<span class="ltx_bibblock">Privacy preserving random decision tree classification over
horizontally and vertically partitioned data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure
Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl
Conf on Big Data Intelligence and Computing and Cyber Science and Technology
Congress (DASC/PiCom/DataCom/CyberSciTech)</span>, pages 600–607, 2018.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sonal Yadav, Vivek Tiwari, and Basant Tiwari.

</span>
<span class="ltx_bibblock">Privacy preserving data mining with abridge time using vertical
partition decision tree.

</span>
<span class="ltx_bibblock">In <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Symposium on Women in Research 2016</span>,
pages 158–164, 2016.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Shahriar Badsha, Iman Vakilinia, and Shamik Sengupta.

</span>
<span class="ltx_bibblock">Privacy preserving cyber threat information sharing and learning for
cyber defense.

</span>
<span class="ltx_bibblock">In <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">2019 IEEE 9th Annual Computing and Communication Workshop and
Conference (CCWC)</span>, pages 0708–0714, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Rémi Canillas, Rania Talbi, Sara Bouchenak, Omar Hasan, Lionel Brunie, and
Laurent Sarrat.

</span>
<span class="ltx_bibblock">Exploratory study of privacy preserving fraud detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th International Middleware Conference
Industry</span>, pages 25–31, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Liang Xue, Dongxiao Liu, Jianbing Ni, Xiaodong Lin, and Xuemin Shen.

</span>
<span class="ltx_bibblock">Consent-based privacy-preserving decision tree evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">ICC 2020-2020 IEEE International Conference on Communications
(ICC)</span>, pages 1–6, 2020.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Liang Xue, Dongxiao Liu, Cheng Huang, Xiaodong Lin, and Xuemin Sherman Shen.

</span>
<span class="ltx_bibblock">Secure and privacy-preserving decision tree classification with lower
complexity.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Journal of Communications and Information Networks</span>,
5(1):16–25, 2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Jun Hou, Qianmu Li, Shunmei Meng, Zhen Ni, Yini Chen, and Yaozong Liu.

</span>
<span class="ltx_bibblock">Dprf: a differential privacy protection random forest.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 7:130707–130720, 2019.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Zhitao Guan, Xianwen Sun, Lingyun Shi, Longfei Wu, and Xiaojiang Du.

</span>
<span class="ltx_bibblock">A differentially private greedy decision forest classification
algorithm with high utility.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Computers &amp; Security</span>, 96:101930, 2020.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Chaoxian Lv, Qianmu Li, Huaqiu Long, Yumei Ren, and Fei Ling.

</span>
<span class="ltx_bibblock">A differential privacy random forest method of privacy protection in
cloud.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Computational Science
and Engineering (CSE) and IEEE International Conference on Embedded and
Ubiquitous Computing (EUC)</span>, pages 470–475, 2019.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Bangzhou Xin, Wei Yang, Shaowei Wang, and Liusheng Huang.

</span>
<span class="ltx_bibblock">Differentially private greedy decision forest.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 2672–2676, 2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Lingchen Zhao, Lihao Ni, Shengshan Hu, Yaniiao Chen, Pan Zhou, Fu Xiao, and
Libing Wu.

</span>
<span class="ltx_bibblock">Inprivate digging: Enabling tree-based distributed data mining with
differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2018-IEEE Conference on Computer
Communications</span>, pages 2087–2095, 2018.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Kyle Fritchman, Keerthanaa Saminathan, Rafael Dowsley, Tyler Hughes, Martine
De Cock, Anderson Nascimento, and Ankur Teredesai.

</span>
<span class="ltx_bibblock">Privacy-preserving scoring of tree ensembles: A novel framework for
ai in healthcare.

</span>
<span class="ltx_bibblock">In <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Big Data (Big Data)</span>,
pages 2413–2422, 2018.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Hanbin Zhang, Chen Song, Aosen Wang, Chenhan Xu, Dongmei Li, and Wenyao Xu.

</span>
<span class="ltx_bibblock">Pdvocal: Towards privacy-preserving parkinson’s disease detection
using non-speech body sounds.

</span>
<span class="ltx_bibblock">In <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">The 25th Annual International Conference on Mobile Computing
and Networking</span>, pages 1–16, 2019.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
NG Nageswari Amma and F Ramesh Dhanaseelan.

</span>
<span class="ltx_bibblock">Privacy preserving data mining classifier for smart city
applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">2018 3rd International Conference on Communication and
Electronics Systems (ICCES)</span>, pages 645–648, 2018.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Mahmoud Parto, Christopher Saldana, and Thomas Kurfess.

</span>
<span class="ltx_bibblock">A novel three-layer iot architecture for shared, private, scalable,
and real-time machine learning from ubiquitous cyber-physical systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Procedia manufacturing</span>, 48:959–967, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Jaideep Vaidya, Murat Kantarcıoğlu, and Chris Clifton.

</span>
<span class="ltx_bibblock">Privacy-preserving naive bayes classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">The VLDB Journal</span>, 17(4):879–898, 2008.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia
Hoang, and Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Bayesian nonparametric federated learning of neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
7252–7261. PMLR, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Álvaro García-Recuero.

</span>
<span class="ltx_bibblock">Discouraging abusive behavior in privacy-preserving online social
networking applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the 25th International Conference Companion on
World Wide Web</span>, pages 305–309, 2016.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Tong Li, Zhengan Huang, Ping Li, Zheli Liu, and Chunfu Jia.

</span>
<span class="ltx_bibblock">Outsourced privacy-preserving classification service over encrypted
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Journal of Network and Computer Applications</span>, 106:100–110,
2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Yanting Chai, Yu Zhan, Baocang Wang, Yuan Ping, and Zhili Zhang.

</span>
<span class="ltx_bibblock">Improvement on a privacy-preserving outsourced classification
protocol over encrypted data.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">Wireless Networks</span>, 26(6):4363–4374, 2020.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Xue Yang, Rongxing Lu, Jun Shao, Xiaohu Tang, and Haomiao Yang.

</span>
<span class="ltx_bibblock">An efficient and privacy-preserving disease risk prediction scheme
for e-healthcare.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 6(2):3284–3297, 2018.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Maria E Skarkala, Manolis Maragoudakis, Stefanos Gritzalis, and Lilian Mitrou.

</span>
<span class="ltx_bibblock">Pp-tan: a privacy preserving multi-party tree augmented naive bayes
classifier.

</span>
<span class="ltx_bibblock">In <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">2020 5th South-East Europe Design Automation, Computer
Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM)</span>,
pages 1–8, 2020.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Sin G Teo, Jianneng Cao, and Vincent CS Lee.

</span>
<span class="ltx_bibblock">Dag: A general model for privacy-preserving data mining.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
32(1):40–53, 2018.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Xiaoxia Liu, Hui Zhu, Rongxing Lu, and Hui Li.

</span>
<span class="ltx_bibblock">Efficient privacy-preserving online medical primary diagnosis scheme
on naive bayesian classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Peer-to-Peer Networking and Applications</span>, 11(2):334–347, 2018.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Zhaowen Lin, Fei Xiao, Yi Sun, Yan Ma, Cong-Cong Xing, and Jun Huang.

</span>
<span class="ltx_bibblock">A secure encryption-based malware detection system.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">KSII Transactions on Internet and Information Systems (TIIS)</span>,
12(4):1799–1818, 2018.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Rania Talbi, Sara Bouchenak, and Lydia Y Chen.

</span>
<span class="ltx_bibblock">Towards dynamic end-to-end privacy preserving data classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">2018 48th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks Workshops (DSN-W)</span>, pages 73–74, 2018.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Xiaoyu Zhang, Xiaofeng Chen, Jianfeng Wang, Zhihui Zhan, and Jin Li.

</span>
<span class="ltx_bibblock">Verifiable privacy-preserving single-layer perceptron training scheme
in cloud computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">Soft Computing</span>, 22(23):7719–7732, 2018.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Qijian He, Wei Yang, Bingren Chen, Yangyang Geng, and Liusheng Huang.

</span>
<span class="ltx_bibblock">Transnet: Training privacy-preserving neural network over transformed
layer.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">Proceedings of the VLDB Endowment</span>, 13(12):1849–1862, 2020.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Catherine Jones.

</span>
<span class="ltx_bibblock">Privacy-preserving machine learning in cloud.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 on cloud computing security
workshop</span>, pages 39–43, 2017.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Rebecca N Wright.

</span>
<span class="ltx_bibblock">Privacy-preserving machine learning as a service.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">Proc. Priv. Enhancing Technol.</span>, 2018(3):123–142, 2018.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Gianpiero Costantino, Antonio La Marra, Fabio Martinelli, Andrea Saracino, and
Mina Sheikhalishahi.

</span>
<span class="ltx_bibblock">Privacy-preserving text mining as a service.

</span>
<span class="ltx_bibblock">In <span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on Computers and Communications (ISCC)</span>,
pages 890–897, 2017.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Nasir Rahim, Jamil Ahmad, Khan Muhammad, Arun Kumar Sangaiah, and Sung Wook
Baik.

</span>
<span class="ltx_bibblock">Privacy-preserving image retrieval for mobile devices with deep
features on the cloud.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">Computer Communications</span>, 127:75–85, 2018.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Li Wang, Jun Jie Shi, Chen Chen, and Sheng Zhong.

</span>
<span class="ltx_bibblock">Privacy-preserving face detection based on linear and nonlinear
kernels.

</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, 77(6):7261–7281, 2018.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM SIGSAC conference on computer and
communications security</span>, pages 308–318, 2016.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Mengran Xia, Dawei Jin, and Jingyu Chen.

</span>
<span class="ltx_bibblock">Short-term traffic flow prediction based on graph convolutional
networks and federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, pages
1–13, 2022.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Xiaoguang Niu, Qiongzan Ye, Yihao Zhang, and Dengpan Ye.

</span>
<span class="ltx_bibblock">A privacy-preserving identification mechanism for mobile sensing
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 6:15457–15467, 2018.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Jiaping Lin, Jianwei Niu, and Hui Li.

</span>
<span class="ltx_bibblock">Pcd: A privacy-preserving predictive clinical decision scheme with
e-health big data based on rnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Communications Workshops
(INFOCOM WKSHPS)</span>, pages 808–813, 2017.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Efe Bozkir, David Geisler, and Enkelejda Kasneci.

</span>
<span class="ltx_bibblock">Person independent, privacy preserving, and real time assessment of
cognitive load using eye tracking in a virtual reality setup.

</span>
<span class="ltx_bibblock">In <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">2019 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR)</span>, pages 1834–1837, 2019.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Julian Steil, Marion Koelle, Wilko Heuten, Susanne Boll, and Andreas Bulling.

</span>
<span class="ltx_bibblock">Privaceye: privacy-preserving head-mounted eye tracking using
egocentric scene image and eye movement features.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Proceedings of the 11th ACM Symposium on Eye Tracking
Research &amp; Applications</span>, pages 1–10, 2019.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Youwen Zhu and Xingxin Li.

</span>
<span class="ltx_bibblock">Privacy-preserving k-means clustering with local synchronization in
peer-to-peer networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Peer-to-Peer Networking and Applications</span>, 13(6):2272–2284,
2020.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Ahmed A. Al-Saedi, Veselka Boeva, and Emiliano Casalicchio.

</span>
<span class="ltx_bibblock">Reducing communication overhead of federated learning through
clustering analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">2021 IEEE Symposium on Computers and Communications (ISCC)</span>,
pages 1–7, 2021.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Igor V Anikin and Rinat M Gazimov.

</span>
<span class="ltx_bibblock">Privacy preserving dbscan clustering algorithm for vertically
partitioned data in distributed systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">2017 International Siberian Conference on Control and
Communications (SIBCON)</span>, pages 1–4, 2017.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Walisa Romsaiyud, Henning Schnoor, and Wilhelm Hasselbring.

</span>
<span class="ltx_bibblock">Improving k-nearest neighbor pattern recognition models for
privacy-preserving data analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Big Data (Big Data)</span>,
pages 5804–5813, 2019.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Dinesh Chowdary Attota, Viraaji Mothukuri, Reza M. Parizi, and Seyedamin
Pouriyeh.

</span>
<span class="ltx_bibblock">An ensemble multi-view federated learning intrusion detection for
iot.

</span>
<span class="ltx_bibblock"><span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 9:117734–117745, 2021.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Zhuo Ma, Yang Liu, Ximeng Liu, Jianfeng Ma, and Kui Ren.

</span>
<span class="ltx_bibblock">Lightweight privacy-preserving ensemble classification for face
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 6(3):5778–5790, 2019.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Dawid Połap and Marcin Woźniak.

</span>
<span class="ltx_bibblock">A hybridization of distributed policy and heuristic augmentation for
improving federated learning approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 146:130–140, 2022.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Dawid Połap and Marcin Woźniak.

</span>
<span class="ltx_bibblock">Meta-heuristic as manager in federated learning approaches for image
processing purposes.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">Applied Soft Computing</span>, 113:107872, 2021.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Basheer Qolomany, Kashif Ahmad, Ala Al-Fuqaha, and Junaid Qadir.

</span>
<span class="ltx_bibblock">Particle swarm optimized federated learning for industrial iot and
smart city services.

</span>
<span class="ltx_bibblock">In <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">GLOBECOM 2020-2020 IEEE Global Communications Conference</span>,
pages 1–6. IEEE, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Biwen Chen, Honghong Zeng, Tao Xiang, Shangwei Guo, Tianwei Zhang, and Yang
Liu.

</span>
<span class="ltx_bibblock">Esb-fl: Efficient and secure blockchain-based federated learning with
fair payment.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Big Data</span>, pages 1–1, 2022.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Mansoor Ali, Hadis Karimipour, and Muhammad Tariq.

</span>
<span class="ltx_bibblock">Integration of blockchain and federated learning for internet of
things: Recent advances and future challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">Computers &amp; Security</span>, 108:102355, 2021.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.

</span>
<span class="ltx_bibblock">Blockchain and federated learning for privacy-preserved data sharing
in industrial iot.

</span>
<span class="ltx_bibblock"><span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 16(6):4177–4186,
2020.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Shiva Raj Pokhrel and Jinho Choi.

</span>
<span class="ltx_bibblock">Federated learning with blockchain for autonomous vehicles: Analysis
and design challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Communications</span>, 68(8):4734–4746, 2020.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Dinh C. Nguyen, Ming Ding, Quoc-Viet Pham, Pubudu N. Pathirana, Long Bao Le,
Aruna Seneviratne, Jun Li, Dusit Niyato, and H. Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning meets blockchain in edge computing: Opportunities
and challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(16):12806–12825, 2021.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Naiyu Wang, Wenti Yang, Xiaodong Wang, Longfei Wu, Zhitao Guan, Xiaojiang Du,
and Mohsen Guizani.

</span>
<span class="ltx_bibblock">A blockchain based privacy-preserving federated learning scheme for
internet of vehicles.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">Digital Communications and Networks</span>, 2022.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Naiyu Wang, Wenti Yang, Zhitao Guan, Xiaojiang Du, and Mohsen Guizani.

</span>
<span class="ltx_bibblock">Bpfl: A blockchain based privacy-preserving federated learning
scheme.

</span>
<span class="ltx_bibblock">In <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">2021 IEEE Global Communications Conference (GLOBECOM)</span>, pages
1–6, 2021.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and Mohsen
Guizani.

</span>
<span class="ltx_bibblock">Reliable federated learning for mobile networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 27(2):72–80, 2020.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Jiawen Kang, Xuandi Li, Jiangtian Nie, Yi Liu, Minrui Xu, Zehui Xiong, Dusit
Niyato, and Qiang Yan.

</span>
<span class="ltx_bibblock">Communication-efficient and cross-chain empowered federated learning
for artificial intelligence of things.

</span>
<span class="ltx_bibblock"><span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Network Science and Engineering</span>, pages
1–1, 2022.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Rajesh Kumar, Abdullah Aman Khan, Jay Kumar, Zakria, Noorbakhsh Amiri Golilarz,
Simin Zhang, Yang Ting, Chengyu Zheng, and Wenyong Wang.

</span>
<span class="ltx_bibblock">Blockchain-federated-learning and deep learning models for covid-19
detection using ct imaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">IEEE Sensors Journal</span>, 21(14):16301–16314, 2021.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">Blockchained on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">IEEE Communications Letters</span>, 24(6):1279–1283, 2020.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Zhuo Liu, Chenhui Yao, Hang Yu, and Taihua Wu.

</span>
<span class="ltx_bibblock">Deep reinforcement learning with its application for lung cancer
detection in medical internet of things.

</span>
<span class="ltx_bibblock"><span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 97:1–9, 2019.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li.

</span>
<span class="ltx_bibblock">Optimizing federated learning on non-iid data with reinforcement
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2020 - IEEE Conference on Computer
Communications</span>, pages 1698–1707, 2020.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Yufeng Zhan, Peng Li, Zhihao Qu, Deze Zeng, and Song Guo.

</span>
<span class="ltx_bibblock">A learning-based incentive mechanism for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 7(7):6360–6368, 2020.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Mounssif Krouka, Anis Elgabli, Chaouki Ben Issaid, and Mehdi Bennis.

</span>
<span class="ltx_bibblock">Communication-efficient and federated multi-agent reinforcement
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Cognitive Communications and Networking</span>,
8(1):311–320, 2022.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek.

</span>
<span class="ltx_bibblock">Robust and communication-efficient federated learning from non-i.i.d.
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
31(9):3400–3413, 2020.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Xiaodong Ma, Jia Zhu, Zhihao Lin, Shanxuan Chen, and Yangjie Qin.

</span>
<span class="ltx_bibblock">A state-of-the-art survey on solving non-iid data in federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 135:244–258, 2022.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Hangyu Zhu, Haoyu Zhang, and Yaochu Jin.

</span>
<span class="ltx_bibblock">From federated learning to federated neural architecture search: a
survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">Complex &amp; Intelligent Systems</span>, 7(2):639–657, 2021.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit
Niyato, and Qiang Yang.

</span>
<span class="ltx_bibblock">A fairness-aware incentive scheme for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society</span>, pages 393–399, 2020.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">Federated Learning</span>, pages 189–204. Springer, 2020.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Mohammad Al-Rubaie, Pei-yuan Wu, J Morris Chang, and Sun-Yuan Kung.

</span>
<span class="ltx_bibblock">Privacy-preserving pca on horizontally-partitioned data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Dependable and Secure Computing</span>,
pages 280–287, 2017.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Arash Mehrjou.

</span>
<span class="ltx_bibblock">Federated learning as a mean-field game.

</span>
<span class="ltx_bibblock"><span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2107.03770, 2021.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Samuel Yen-Chi Chen and Shinjae Yoo.

</span>
<span class="ltx_bibblock">Federated quantum machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">Entropy</span>, 23(4):460, 2021.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Jialin Zhong, Yahui Wu, Wubin Ma, Su Deng, and Haohao Zhou.

</span>
<span class="ltx_bibblock">Optimizing multi-objective federated learning on non-iid data with
improved nsga-iii and hierarchical clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">Symmetry</span>, 14(5):1070, 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Name"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.10831" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.10832" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.10832">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.10832" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.10833" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 18:24:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
