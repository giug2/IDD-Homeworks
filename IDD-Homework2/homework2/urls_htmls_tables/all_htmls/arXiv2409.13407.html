<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model</title>
<!--Generated on Fri Sep 20 11:06:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13407v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx1" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx2" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.SSx1" title="In Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.SSx2" title="In Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Design of Unified SegCap Data Format</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx4" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Data Annotation Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx4.SSx1" title="In Data Annotation Pipeline ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Object Labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx4.SSx2" title="In Data Annotation Pipeline ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Mask Tree Building</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx4.SSx3" title="In Data Annotation Pipeline ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Dense Context Organization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.SSx1" title="In Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Experimental Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.SSx2" title="In Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx6" title="In Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_title">Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Instruction-guided Multi-Granularity Segmentation and Captioning 
<br class="ltx_break"/>with Large Multimodal Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Li Zhou<sup class="ltx_sup" id="id1.1.id1">1<span class="ltx_ERROR undefined" id="id1.1.id1.1">\equalcontrib</span></sup>,
Xu Yuan<sup class="ltx_sup" id="id2.2.id2">2<span class="ltx_ERROR undefined" id="id2.2.id2.1">\equalcontrib</span></sup>,
Zenghui Sun<sup class="ltx_sup" id="id3.3.id3">1</sup>,
Zikun Zhou<sup class="ltx_sup" id="id4.4.id4">3</sup>,
Jinsong Lan<sup class="ltx_sup" id="id5.5.id5">1</sup>
</span><span class="ltx_author_notes">Corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Large Multimodal Models (LMMs) have achieved significant progress by extending large language models.
Building on this progress, the latest developments in LMMs demonstrate the ability to generate dense pixel-wise segmentation through the integration of segmentation models.
Despite the innovations, the textual responses and segmentation masks of existing works remain at the instance level, showing limited ability to perform fine-grained understanding and segmentation even provided with detailed textual cues.
To overcome this limitation, we introduce a Multi-Granularity Large Multimodal Model (MGLMM), which is capable of seamlessly adjusting the granularity of Segmentation and Captioning (SegCap) following user instructions, from panoptic SegCap to fine-grained SegCap. We name such a new task Multi-Granularity Segmentation and Captioning (MGSC).
Observing the lack of a benchmark for model training and evaluation over the MGSC task, we establish a benchmark with aligned masks and captions in multi-granularity using our customized automated annotation pipeline.
This benchmark comprises 10K images and more than 30K image-question pairs.
We will release our dataset along with the implementation of our automated dataset annotation pipeline for further research.
Besides, we propose a novel unified SegCap data format to unify heterogeneous segmentation datasets; it effectively facilitates learning to associate object concepts with visual features during multi-task training.
Extensive experiments demonstrate that our MGLMM excels at tackling more than eight downstream tasks and achieves state-of-the-art performance in MGSC, GCG, image captioning, referring segmentation, multiple and empty segmentation, and reasoning segmentation tasks.
The great performance and versatility of MGLMM underscore its potential impact on advancing multimodal research.
Code and dataset will be released at https://github.com/lizhou-cs/mglmm.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.3">{strip}</span>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="p1.2" style="width:505.9pt;">
<div class="ltx_para" id="p1.2.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="538" id="p1.1.g1" src="x1.png" width="969"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>MGLMM is a versatile and sophisticated LMM, which can handle various tasks involving textual and pixel-level mask responses. We show its visualization results in the following scenarios: multi-granularity segmentation and captioning, referring segmentation, multiple/empty segmentation, panoptic segmentation, reasoning segmentation, image-level captioning, and conversation.</figcaption>
</figure>
</div>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Leveraging the commonsense reasoning and understanding abilities of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib9" title="">2023</a>; Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib46" title="">2023</a>)</cite>, Large Multimodal Models (LMMs) <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib63" title="">2023</a>; Alayrac et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib2" title="">2022</a>; Bai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib4" title="">2023</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib31" title="">2024a</a>)</cite>
have notably advanced cross-modality understanding and vision-language alignment.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Recently, several studies  <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>; Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite> have explored the instruction-based LMMs capable of producing pixel-level segmentation masks as responses to user queries.
More recent researches <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>; Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>)</cite> concentrated on Grounded Conversation Generation (GCG) which aims to ground the main objects appearing in the conversations.
Although these methods <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>; Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>; Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>; Ren et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib42" title="">2024</a>)</cite> integrate a powerful segmentation model capable of panoptic segmentation, they still have difficulty generating mask-text-aligned responses for all the instances in the image, resulting in limited panoptic segmentation performance. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx1.F2" title="Figure 2 ‣ Introduction ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">2</span></a> (a) shows such a case where GLaMM overlooks the tennis racket, tennis ball and microphone in both mask and text responses.
Besides, these models only possess the ability to describe the image at the instance level and produce corresponding instance masks aligned with the output texts.
Hence, these models can hardly perceive the fine-grained objects, such as the hat, wristband, and skirt of the player in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx1.F2" title="Figure 2 ‣ Introduction ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">2</span></a> (b), even provided with detailed textual cues.
The missing of the above abilities would limit the universality and comprehension of the LMMs.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">To overcome these limitations, we introduce the Multi-Granularity LMM (MGLMM), which is capable of seamlessly adjusting the granularity of Segmentation and Captioning (SegCap) following user instructions, from panoptic SegCap to fine-grained SegCap. To be specific, for the query requiring describing the overall contents of an image, MGLMM outputs the precise panoptic segmentation masks with captions, offering a coarse-grained understanding of the entire image. For the instruction demanding to describe a certain object in the image, MGLMM can produce a detailed response including segmentation masks of the sub-parts of the object as well as corresponding descriptions, which reveal the components of the target object. We name such a task Multi-Granularity SegCap (MGSC), which assesses the ability of progressive cognition from coarse-grained to fine-grained. Overall, MGLMM excels at tackling more than eight downstream tasks such as panoptic SegCap, fine-grained SegCap, GCG, and multiple and empty segmentation, as presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#S0.F1" title="Figure 1 ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx1.T1" title="Table 1 ‣ Introduction ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Observing the lack of a benchmark for training and evaluating LMMs for the MGSC task in the community, we establish a new benchmark, dubbed MGSCData, with aligned masks and captions in multi-granularity using the customized automated annotation pipeline.
It consists of 10K images and over 30K image-question pairs, encompassing both panoptic and fine-grained segmentation. To be more specific, the dataset includes more than 300K segmentation masks, each annotated with a semantic label and an accompanying detailed description.
MGSCData effectively facilitates the training and assessment of the ability to associate object concepts and visual features in multi-granularity.
We will release MGSCData and expect it to benefit academia.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Besides the benchmark, another key challenge in unifying segmentation tasks across granularities lies in the significant variation in both the format and semantic level of the queries and outputs. Typically, existing studies directly incorporate the heterogeneous data of different tasks into model training, overlooking the task discrepancies and complicating multimodal alignment further. To handle this issue, we propose a Unified SegCap Data Format (USCDF) to explicitly guide the model in learning the alignment relationships between object concepts and segmentation masks in different granularities during training. Specifically, USCDF unifies the output formats of different segmentation tasks, bridging the gap between them and reducing the difficulty of multi-task learning for the model. The right part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F3" title="Figure 3 ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the instantiation of the unified data format on tasks including multi-referring reasoning, panoptic SegCap, and fine-grained SegCap. Experimental results demonstrate that USCDF benefits multi-task learning and vision-language learning. We also evaluate MGLMM across a variety of benchmarks. The experiments demonstrate that it achieves state-of-the-art results on six benchmarks.</p>
</div>
<figure class="ltx_figure" id="Sx1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="587" id="Sx1.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Qualitative comparison of GLaMM and our MGLMM. Please refer to <span class="ltx_text ltx_font_bold" id="Sx1.F2.2.1">Appendix. A</span> for more details.</figcaption>
</figure>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">In conclusion, our work has four main contributions:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">We propose MGLMM, the first model capable of seamlessly switching between multi-granularity segmentation and captioning, especially including panoptic and fine-grained segmentation and captioning.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">We introduce a novel benchmark MGSCData to train and evaluate the ability of multi-granularity segmentation and captioning for LMMs, which comprises over 30K high-quality image-question pairs.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">We propose a unified data format, which facilitates learning the alignment relationships between object concepts and segmentation masks in multiple granularities.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">We achieve state-of-the-art performance across various tasks, including MGSC, GCG, image captioning, various segmentation tasks, etc.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="Sx1.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx1.T1.1" style="width:411.9pt;height:106.9pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-177.2pt,45.8pt) scale(0.537591546194645,0.537591546194645) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx1.T1.1.1">
<tr class="ltx_tr" id="Sx1.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx1.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="Sx1.T1.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="Sx1.T1.1.1.1.2">Textual Response</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Sx1.T1.1.1.1.3">Mask Response</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="Sx1.T1.1.1.1.4">Textual &amp; Mask Response</td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.2">
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.1">Caption</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.2.2">Conversation</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.3">Referring Seg</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.4">Generic Seg</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.2.5">Multiple/Empty Seg</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.6">Reasoning Seg</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.7">GCG</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.2.8">MGSC</td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx1.T1.1.1.3.1">LISA <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx1.T1.1.1.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx1.T1.1.1.3.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx1.T1.1.1.3.4">✓</td>
<td class="ltx_td ltx_border_t" id="Sx1.T1.1.1.3.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="Sx1.T1.1.1.3.6"></td>
<td class="ltx_td ltx_border_t" id="Sx1.T1.1.1.3.7"></td>
<td class="ltx_td ltx_border_t" id="Sx1.T1.1.1.3.8"></td>
<td class="ltx_td ltx_border_t" id="Sx1.T1.1.1.3.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.4.1">PixelLM <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib42" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.4.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.4.3">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.4.4">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.4.6">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.4.7">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.4.8"></td>
<td class="ltx_td" id="Sx1.T1.1.1.4.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.5.1">GSVA <cite class="ltx_cite ltx_citemacro_citep">(Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.5.3">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.5.4">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.5.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.5.6">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.5.7"></td>
<td class="ltx_td" id="Sx1.T1.1.1.5.8"></td>
<td class="ltx_td" id="Sx1.T1.1.1.5.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.6.1">Osprey <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib58" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.6.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.6.3">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.6.4"></td>
<td class="ltx_td" id="Sx1.T1.1.1.6.5"></td>
<td class="ltx_td ltx_border_r" id="Sx1.T1.1.1.6.6"></td>
<td class="ltx_td" id="Sx1.T1.1.1.6.7"></td>
<td class="ltx_td" id="Sx1.T1.1.1.6.8"></td>
<td class="ltx_td" id="Sx1.T1.1.1.6.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.7.1">LaSagnA <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib51" title="">2024</a>)</cite>
</td>
<td class="ltx_td" id="Sx1.T1.1.1.7.2"></td>
<td class="ltx_td ltx_border_r" id="Sx1.T1.1.1.7.3"></td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.7.4">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.7.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.7.6">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.7.7"></td>
<td class="ltx_td" id="Sx1.T1.1.1.7.8"></td>
<td class="ltx_td" id="Sx1.T1.1.1.7.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.8.1">PSALM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib61" title="">2024b</a>)</cite>
</td>
<td class="ltx_td" id="Sx1.T1.1.1.8.2"></td>
<td class="ltx_td ltx_border_r" id="Sx1.T1.1.1.8.3"></td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.8.4">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.8.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.8.6">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.8.7"></td>
<td class="ltx_td" id="Sx1.T1.1.1.8.8"></td>
<td class="ltx_td" id="Sx1.T1.1.1.8.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.9.1">OMG-LLaVa <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.9.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.9.3">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.9.4">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.9.5">✓</td>
<td class="ltx_td ltx_border_r" id="Sx1.T1.1.1.9.6"></td>
<td class="ltx_td" id="Sx1.T1.1.1.9.7"></td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.9.8">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.9.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx1.T1.1.1.10.1">GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.10.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx1.T1.1.1.10.3">✓</td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.10.4">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.10.5"></td>
<td class="ltx_td ltx_border_r" id="Sx1.T1.1.1.10.6"></td>
<td class="ltx_td" id="Sx1.T1.1.1.10.7"></td>
<td class="ltx_td ltx_align_center" id="Sx1.T1.1.1.10.8">✓</td>
<td class="ltx_td" id="Sx1.T1.1.1.10.9"></td>
</tr>
<tr class="ltx_tr" id="Sx1.T1.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="Sx1.T1.1.1.11.1">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx1.T1.1.1.11.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx1.T1.1.1.11.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.7">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.8">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx1.T1.1.1.11.9">✓</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the capabilities of MGLMM with multiple representative methods. Here, “Generic Seg” comprises semantic segmentation, instance segmentation, and panoptic segmentation; “Reasoning Seg” requires the model to segment images based on queries involving complex reasoning and provide the corresponding textual interpretation.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Related Work</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Recently, there has been an increasing focus on fine-tuning pre-trained LLMs for visual instructions.
These approaches, including BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib27" title="">2023</a>)</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib14" title="">2023</a>)</cite>, LLaVA  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib32" title="">2024b</a>)</cite>, MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib63" title="">2023</a>)</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_citep">(Bai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib4" title="">2023</a>)</cite>, typically employ a pre-trained visual encoder to embedding visual input, utilize an LLM as the base model to comprehend user instructions and generate textual responses, and include an adapter to bridge the features of the vision encoder with those of the language model.
The integration of visual and linguistic modalities within LLMs aims to enhance their capacity to understand and respond to complex, visually guided tasks.
Although these methods have significantly facilitated the development of multimodal language models, their mechanisms fail to achieve pixel-level alignment and a comprehensive understanding of both images and language.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Furthermore, several works, including <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>; Ren et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib42" title="">2024</a>; Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>; Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>)</cite>, explore more complex tasks driven by instructions, involving segmentation or captioning as responses to achieve effective pixel-level alignment of images and text.
Although these methods perform well in various segmentation tasks, they are limited to learning only instance-level vision-language alignment, preventing them from perceiving fine-grained objects.
Furthermore, all these methods integrate a mask decoder capable of panoptic segmentation into their methods but fail to generate coherent mask-text-align responses, resulting in suboptimal performance.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">For the reasons mentioned above, our goal is to develop an LMM that can seamlessly perform panoptic and fine-grained segmentation and captioning based on user instructions.
Further, we establish a high-quality benchmark called MGSC that fills the gap for panoptic and fine-grained segmentation and captioning and introduce our automated annotation pipeline.
Last, we propose a unified data format that facilitates explicit learning of alignment relationships between object concepts and segmentation masks.
MGLMM achieves state-of-the-art performances on over six tasks and ablation results also prove the effectiveness of our methods.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Method</h2>
<figure class="ltx_figure" id="Sx3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="Sx3.F3.g1" src="x3.png" width="731"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="Sx3.F3.3.1">Left</span>: The model architecture of MGLMM. <span class="ltx_text ltx_font_bold" id="Sx3.F3.4.2">Right</span>: The proposed unified data format for multi-task learning.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">In this section, we introduce the model architecture of our MGLMM, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F3" title="Figure 3 ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>.
We then introduce the unified SegCap data format used during training.</p>
</div>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Model Architecture</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">To achieve multi-granularity segmentation and captioning, we utilize two foundational models to construct our model:
(1) an LMM for comprehending input images and user instructions and generating natural language responses, and (2) a segmentation model based on an encoder-decoder architecture for pixel-level visual understanding.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.p2.5"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p2.5.1">Large Multimodal Model.</span>
Considering the simplicity and consistency with previous works <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>; Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>, LLaVA emerges as our preferred choice.
Specifically, we employ the CLIP model as the vision encoder, denoted as <math alttext="\mathcal{F}_{v}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.1.m1.1"><semantics id="Sx3.SSx1.p2.1.m1.1a"><msub id="Sx3.SSx1.p2.1.m1.1.1" xref="Sx3.SSx1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p2.1.m1.1.1.2" xref="Sx3.SSx1.p2.1.m1.1.1.2.cmml">ℱ</mi><mi id="Sx3.SSx1.p2.1.m1.1.1.3" xref="Sx3.SSx1.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.1.m1.1b"><apply id="Sx3.SSx1.p2.1.m1.1.1.cmml" xref="Sx3.SSx1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.1.m1.1.1.1.cmml" xref="Sx3.SSx1.p2.1.m1.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.1.m1.1.1.2.cmml" xref="Sx3.SSx1.p2.1.m1.1.1.2">ℱ</ci><ci id="Sx3.SSx1.p2.1.m1.1.1.3.cmml" xref="Sx3.SSx1.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.1.m1.1c">\mathcal{F}_{v}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.1.m1.1d">caligraphic_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, in conjunction with the Vicuna-7B model as a decoder-based LLM, denoted as <math alttext="\mathcal{F}_{llm}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.2.m2.1"><semantics id="Sx3.SSx1.p2.2.m2.1a"><msub id="Sx3.SSx1.p2.2.m2.1.1" xref="Sx3.SSx1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p2.2.m2.1.1.2" xref="Sx3.SSx1.p2.2.m2.1.1.2.cmml">ℱ</mi><mrow id="Sx3.SSx1.p2.2.m2.1.1.3" xref="Sx3.SSx1.p2.2.m2.1.1.3.cmml"><mi id="Sx3.SSx1.p2.2.m2.1.1.3.2" xref="Sx3.SSx1.p2.2.m2.1.1.3.2.cmml">l</mi><mo id="Sx3.SSx1.p2.2.m2.1.1.3.1" xref="Sx3.SSx1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.2.m2.1.1.3.3" xref="Sx3.SSx1.p2.2.m2.1.1.3.3.cmml">l</mi><mo id="Sx3.SSx1.p2.2.m2.1.1.3.1a" xref="Sx3.SSx1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.2.m2.1.1.3.4" xref="Sx3.SSx1.p2.2.m2.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.2.m2.1b"><apply id="Sx3.SSx1.p2.2.m2.1.1.cmml" xref="Sx3.SSx1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.2.m2.1.1.1.cmml" xref="Sx3.SSx1.p2.2.m2.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.2.m2.1.1.2.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.2">ℱ</ci><apply id="Sx3.SSx1.p2.2.m2.1.1.3.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.3"><times id="Sx3.SSx1.p2.2.m2.1.1.3.1.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.3.1"></times><ci id="Sx3.SSx1.p2.2.m2.1.1.3.2.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.3.2">𝑙</ci><ci id="Sx3.SSx1.p2.2.m2.1.1.3.3.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.3.3">𝑙</ci><ci id="Sx3.SSx1.p2.2.m2.1.1.3.4.cmml" xref="Sx3.SSx1.p2.2.m2.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.2.m2.1c">\mathcal{F}_{llm}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.2.m2.1d">caligraphic_F start_POSTSUBSCRIPT italic_l italic_l italic_m end_POSTSUBSCRIPT</annotation></semantics></math>.
As illustrated in Figure. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F3" title="Figure 3 ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>, the vision encoder is responsible for extracting visual features from the input image <math alttext="x_{img}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.3.m3.1"><semantics id="Sx3.SSx1.p2.3.m3.1a"><msub id="Sx3.SSx1.p2.3.m3.1.1" xref="Sx3.SSx1.p2.3.m3.1.1.cmml"><mi id="Sx3.SSx1.p2.3.m3.1.1.2" xref="Sx3.SSx1.p2.3.m3.1.1.2.cmml">x</mi><mrow id="Sx3.SSx1.p2.3.m3.1.1.3" xref="Sx3.SSx1.p2.3.m3.1.1.3.cmml"><mi id="Sx3.SSx1.p2.3.m3.1.1.3.2" xref="Sx3.SSx1.p2.3.m3.1.1.3.2.cmml">i</mi><mo id="Sx3.SSx1.p2.3.m3.1.1.3.1" xref="Sx3.SSx1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.3.m3.1.1.3.3" xref="Sx3.SSx1.p2.3.m3.1.1.3.3.cmml">m</mi><mo id="Sx3.SSx1.p2.3.m3.1.1.3.1a" xref="Sx3.SSx1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.3.m3.1.1.3.4" xref="Sx3.SSx1.p2.3.m3.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.3.m3.1b"><apply id="Sx3.SSx1.p2.3.m3.1.1.cmml" xref="Sx3.SSx1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.3.m3.1.1.1.cmml" xref="Sx3.SSx1.p2.3.m3.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.3.m3.1.1.2.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.2">𝑥</ci><apply id="Sx3.SSx1.p2.3.m3.1.1.3.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.3"><times id="Sx3.SSx1.p2.3.m3.1.1.3.1.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.3.1"></times><ci id="Sx3.SSx1.p2.3.m3.1.1.3.2.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.3.2">𝑖</ci><ci id="Sx3.SSx1.p2.3.m3.1.1.3.3.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.3.3">𝑚</ci><ci id="Sx3.SSx1.p2.3.m3.1.1.3.4.cmml" xref="Sx3.SSx1.p2.3.m3.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.3.m3.1c">x_{img}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, after which a projector <math alttext="\phi" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.4.m4.1"><semantics id="Sx3.SSx1.p2.4.m4.1a"><mi id="Sx3.SSx1.p2.4.m4.1.1" xref="Sx3.SSx1.p2.4.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.4.m4.1b"><ci id="Sx3.SSx1.p2.4.m4.1.1.cmml" xref="Sx3.SSx1.p2.4.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.4.m4.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.4.m4.1d">italic_ϕ</annotation></semantics></math> is applied to map the extracted image features into the word embedding space of <math alttext="\mathcal{F}_{llm}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.5.m5.1"><semantics id="Sx3.SSx1.p2.5.m5.1a"><msub id="Sx3.SSx1.p2.5.m5.1.1" xref="Sx3.SSx1.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p2.5.m5.1.1.2" xref="Sx3.SSx1.p2.5.m5.1.1.2.cmml">ℱ</mi><mrow id="Sx3.SSx1.p2.5.m5.1.1.3" xref="Sx3.SSx1.p2.5.m5.1.1.3.cmml"><mi id="Sx3.SSx1.p2.5.m5.1.1.3.2" xref="Sx3.SSx1.p2.5.m5.1.1.3.2.cmml">l</mi><mo id="Sx3.SSx1.p2.5.m5.1.1.3.1" xref="Sx3.SSx1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.5.m5.1.1.3.3" xref="Sx3.SSx1.p2.5.m5.1.1.3.3.cmml">l</mi><mo id="Sx3.SSx1.p2.5.m5.1.1.3.1a" xref="Sx3.SSx1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.5.m5.1.1.3.4" xref="Sx3.SSx1.p2.5.m5.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.5.m5.1b"><apply id="Sx3.SSx1.p2.5.m5.1.1.cmml" xref="Sx3.SSx1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.5.m5.1.1.1.cmml" xref="Sx3.SSx1.p2.5.m5.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.5.m5.1.1.2.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.2">ℱ</ci><apply id="Sx3.SSx1.p2.5.m5.1.1.3.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.3"><times id="Sx3.SSx1.p2.5.m5.1.1.3.1.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.3.1"></times><ci id="Sx3.SSx1.p2.5.m5.1.1.3.2.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.3.2">𝑙</ci><ci id="Sx3.SSx1.p2.5.m5.1.1.3.3.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.3.3">𝑙</ci><ci id="Sx3.SSx1.p2.5.m5.1.1.3.4.cmml" xref="Sx3.SSx1.p2.5.m5.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.5.m5.1c">\mathcal{F}_{llm}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.5.m5.1d">caligraphic_F start_POSTSUBSCRIPT italic_l italic_l italic_m end_POSTSUBSCRIPT</annotation></semantics></math>.
Formally:</p>
<table class="ltx_equation ltx_eqn_table" id="Sx3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{img}=\phi(\mathcal{F}_{v}(x_{img}))." class="ltx_Math" display="block" id="Sx3.E1.m1.1"><semantics id="Sx3.E1.m1.1a"><mrow id="Sx3.E1.m1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.cmml"><mrow id="Sx3.E1.m1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.cmml"><msub id="Sx3.E1.m1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.3.cmml"><mi id="Sx3.E1.m1.1.1.1.1.3.2" xref="Sx3.E1.m1.1.1.1.1.3.2.cmml">z</mi><mrow id="Sx3.E1.m1.1.1.1.1.3.3" xref="Sx3.E1.m1.1.1.1.1.3.3.cmml"><mi id="Sx3.E1.m1.1.1.1.1.3.3.2" xref="Sx3.E1.m1.1.1.1.1.3.3.2.cmml">i</mi><mo id="Sx3.E1.m1.1.1.1.1.3.3.1" xref="Sx3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E1.m1.1.1.1.1.3.3.3" xref="Sx3.E1.m1.1.1.1.1.3.3.3.cmml">m</mi><mo id="Sx3.E1.m1.1.1.1.1.3.3.1a" xref="Sx3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E1.m1.1.1.1.1.3.3.4" xref="Sx3.E1.m1.1.1.1.1.3.3.4.cmml">g</mi></mrow></msub><mo id="Sx3.E1.m1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx3.E1.m1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.3.cmml">ϕ</mi><mo id="Sx3.E1.m1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Sx3.E1.m1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.E1.m1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">ℱ</mi><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">m</mi><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.4" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">g</mi></mrow></msub><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx3.E1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx3.E1.m1.1.1.1.2" lspace="0em" xref="Sx3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E1.m1.1b"><apply id="Sx3.E1.m1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1"><eq id="Sx3.E1.m1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.2"></eq><apply id="Sx3.E1.m1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E1.m1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E1.m1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.1.1.1.1.3.2">𝑧</ci><apply id="Sx3.E1.m1.1.1.1.1.3.3.cmml" xref="Sx3.E1.m1.1.1.1.1.3.3"><times id="Sx3.E1.m1.1.1.1.1.3.3.1.cmml" xref="Sx3.E1.m1.1.1.1.1.3.3.1"></times><ci id="Sx3.E1.m1.1.1.1.1.3.3.2.cmml" xref="Sx3.E1.m1.1.1.1.1.3.3.2">𝑖</ci><ci id="Sx3.E1.m1.1.1.1.1.3.3.3.cmml" xref="Sx3.E1.m1.1.1.1.1.3.3.3">𝑚</ci><ci id="Sx3.E1.m1.1.1.1.1.3.3.4.cmml" xref="Sx3.E1.m1.1.1.1.1.3.3.4">𝑔</ci></apply></apply><apply id="Sx3.E1.m1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1"><times id="Sx3.E1.m1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.2"></times><ci id="Sx3.E1.m1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.3">italic-ϕ</ci><apply id="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1"><times id="Sx3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.2"></times><apply id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.2">ℱ</ci><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.3">𝑣</ci></apply><apply id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑚</ci><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.4">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E1.m1.1c">z_{img}=\phi(\mathcal{F}_{v}(x_{img})).</annotation><annotation encoding="application/x-llamapun" id="Sx3.E1.m1.1d">italic_z start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT = italic_ϕ ( caligraphic_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx3.SSx1.p2.12">It is worth noting that the projector <math alttext="\phi" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.6.m1.1"><semantics id="Sx3.SSx1.p2.6.m1.1a"><mi id="Sx3.SSx1.p2.6.m1.1.1" xref="Sx3.SSx1.p2.6.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.6.m1.1b"><ci id="Sx3.SSx1.p2.6.m1.1.1.cmml" xref="Sx3.SSx1.p2.6.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.6.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.6.m1.1d">italic_ϕ</annotation></semantics></math> plays a crucial role in aligning image features with the linguistic modality.
Specifically, it consists of two linear layers with a GELU non-linearity and is initialized randomly.
Meanwhile, the text input is encoded into text tokens by the tokenizer <math alttext="T" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.7.m2.1"><semantics id="Sx3.SSx1.p2.7.m2.1a"><mi id="Sx3.SSx1.p2.7.m2.1.1" xref="Sx3.SSx1.p2.7.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.7.m2.1b"><ci id="Sx3.SSx1.p2.7.m2.1.1.cmml" xref="Sx3.SSx1.p2.7.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.7.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.7.m2.1d">italic_T</annotation></semantics></math> of <math alttext="\mathcal{F}_{llm}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.8.m3.1"><semantics id="Sx3.SSx1.p2.8.m3.1a"><msub id="Sx3.SSx1.p2.8.m3.1.1" xref="Sx3.SSx1.p2.8.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p2.8.m3.1.1.2" xref="Sx3.SSx1.p2.8.m3.1.1.2.cmml">ℱ</mi><mrow id="Sx3.SSx1.p2.8.m3.1.1.3" xref="Sx3.SSx1.p2.8.m3.1.1.3.cmml"><mi id="Sx3.SSx1.p2.8.m3.1.1.3.2" xref="Sx3.SSx1.p2.8.m3.1.1.3.2.cmml">l</mi><mo id="Sx3.SSx1.p2.8.m3.1.1.3.1" xref="Sx3.SSx1.p2.8.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.8.m3.1.1.3.3" xref="Sx3.SSx1.p2.8.m3.1.1.3.3.cmml">l</mi><mo id="Sx3.SSx1.p2.8.m3.1.1.3.1a" xref="Sx3.SSx1.p2.8.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.8.m3.1.1.3.4" xref="Sx3.SSx1.p2.8.m3.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.8.m3.1b"><apply id="Sx3.SSx1.p2.8.m3.1.1.cmml" xref="Sx3.SSx1.p2.8.m3.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.8.m3.1.1.1.cmml" xref="Sx3.SSx1.p2.8.m3.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.8.m3.1.1.2.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.2">ℱ</ci><apply id="Sx3.SSx1.p2.8.m3.1.1.3.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.3"><times id="Sx3.SSx1.p2.8.m3.1.1.3.1.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.3.1"></times><ci id="Sx3.SSx1.p2.8.m3.1.1.3.2.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.3.2">𝑙</ci><ci id="Sx3.SSx1.p2.8.m3.1.1.3.3.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.3.3">𝑙</ci><ci id="Sx3.SSx1.p2.8.m3.1.1.3.4.cmml" xref="Sx3.SSx1.p2.8.m3.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.8.m3.1c">\mathcal{F}_{llm}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.8.m3.1d">caligraphic_F start_POSTSUBSCRIPT italic_l italic_l italic_m end_POSTSUBSCRIPT</annotation></semantics></math>.
Subsequently, we integrate image tokens <math alttext="z_{img}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.9.m4.1"><semantics id="Sx3.SSx1.p2.9.m4.1a"><msub id="Sx3.SSx1.p2.9.m4.1.1" xref="Sx3.SSx1.p2.9.m4.1.1.cmml"><mi id="Sx3.SSx1.p2.9.m4.1.1.2" xref="Sx3.SSx1.p2.9.m4.1.1.2.cmml">z</mi><mrow id="Sx3.SSx1.p2.9.m4.1.1.3" xref="Sx3.SSx1.p2.9.m4.1.1.3.cmml"><mi id="Sx3.SSx1.p2.9.m4.1.1.3.2" xref="Sx3.SSx1.p2.9.m4.1.1.3.2.cmml">i</mi><mo id="Sx3.SSx1.p2.9.m4.1.1.3.1" xref="Sx3.SSx1.p2.9.m4.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.9.m4.1.1.3.3" xref="Sx3.SSx1.p2.9.m4.1.1.3.3.cmml">m</mi><mo id="Sx3.SSx1.p2.9.m4.1.1.3.1a" xref="Sx3.SSx1.p2.9.m4.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.9.m4.1.1.3.4" xref="Sx3.SSx1.p2.9.m4.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.9.m4.1b"><apply id="Sx3.SSx1.p2.9.m4.1.1.cmml" xref="Sx3.SSx1.p2.9.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.9.m4.1.1.1.cmml" xref="Sx3.SSx1.p2.9.m4.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.9.m4.1.1.2.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.2">𝑧</ci><apply id="Sx3.SSx1.p2.9.m4.1.1.3.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.3"><times id="Sx3.SSx1.p2.9.m4.1.1.3.1.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.3.1"></times><ci id="Sx3.SSx1.p2.9.m4.1.1.3.2.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.3.2">𝑖</ci><ci id="Sx3.SSx1.p2.9.m4.1.1.3.3.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.3.3">𝑚</ci><ci id="Sx3.SSx1.p2.9.m4.1.1.3.4.cmml" xref="Sx3.SSx1.p2.9.m4.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.9.m4.1c">z_{img}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.9.m4.1d">italic_z start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT</annotation></semantics></math> and text tokens <math alttext="z_{txt}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.10.m5.1"><semantics id="Sx3.SSx1.p2.10.m5.1a"><msub id="Sx3.SSx1.p2.10.m5.1.1" xref="Sx3.SSx1.p2.10.m5.1.1.cmml"><mi id="Sx3.SSx1.p2.10.m5.1.1.2" xref="Sx3.SSx1.p2.10.m5.1.1.2.cmml">z</mi><mrow id="Sx3.SSx1.p2.10.m5.1.1.3" xref="Sx3.SSx1.p2.10.m5.1.1.3.cmml"><mi id="Sx3.SSx1.p2.10.m5.1.1.3.2" xref="Sx3.SSx1.p2.10.m5.1.1.3.2.cmml">t</mi><mo id="Sx3.SSx1.p2.10.m5.1.1.3.1" xref="Sx3.SSx1.p2.10.m5.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.10.m5.1.1.3.3" xref="Sx3.SSx1.p2.10.m5.1.1.3.3.cmml">x</mi><mo id="Sx3.SSx1.p2.10.m5.1.1.3.1a" xref="Sx3.SSx1.p2.10.m5.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.10.m5.1.1.3.4" xref="Sx3.SSx1.p2.10.m5.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.10.m5.1b"><apply id="Sx3.SSx1.p2.10.m5.1.1.cmml" xref="Sx3.SSx1.p2.10.m5.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.10.m5.1.1.1.cmml" xref="Sx3.SSx1.p2.10.m5.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.10.m5.1.1.2.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.2">𝑧</ci><apply id="Sx3.SSx1.p2.10.m5.1.1.3.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.3"><times id="Sx3.SSx1.p2.10.m5.1.1.3.1.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.3.1"></times><ci id="Sx3.SSx1.p2.10.m5.1.1.3.2.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.3.2">𝑡</ci><ci id="Sx3.SSx1.p2.10.m5.1.1.3.3.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.3.3">𝑥</ci><ci id="Sx3.SSx1.p2.10.m5.1.1.3.4.cmml" xref="Sx3.SSx1.p2.10.m5.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.10.m5.1c">z_{txt}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.10.m5.1d">italic_z start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, which are then fed into the <math alttext="\mathcal{F}_{llm}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.11.m6.1"><semantics id="Sx3.SSx1.p2.11.m6.1a"><msub id="Sx3.SSx1.p2.11.m6.1.1" xref="Sx3.SSx1.p2.11.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p2.11.m6.1.1.2" xref="Sx3.SSx1.p2.11.m6.1.1.2.cmml">ℱ</mi><mrow id="Sx3.SSx1.p2.11.m6.1.1.3" xref="Sx3.SSx1.p2.11.m6.1.1.3.cmml"><mi id="Sx3.SSx1.p2.11.m6.1.1.3.2" xref="Sx3.SSx1.p2.11.m6.1.1.3.2.cmml">l</mi><mo id="Sx3.SSx1.p2.11.m6.1.1.3.1" xref="Sx3.SSx1.p2.11.m6.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.11.m6.1.1.3.3" xref="Sx3.SSx1.p2.11.m6.1.1.3.3.cmml">l</mi><mo id="Sx3.SSx1.p2.11.m6.1.1.3.1a" xref="Sx3.SSx1.p2.11.m6.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.11.m6.1.1.3.4" xref="Sx3.SSx1.p2.11.m6.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.11.m6.1b"><apply id="Sx3.SSx1.p2.11.m6.1.1.cmml" xref="Sx3.SSx1.p2.11.m6.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.11.m6.1.1.1.cmml" xref="Sx3.SSx1.p2.11.m6.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.11.m6.1.1.2.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.2">ℱ</ci><apply id="Sx3.SSx1.p2.11.m6.1.1.3.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.3"><times id="Sx3.SSx1.p2.11.m6.1.1.3.1.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.3.1"></times><ci id="Sx3.SSx1.p2.11.m6.1.1.3.2.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.3.2">𝑙</ci><ci id="Sx3.SSx1.p2.11.m6.1.1.3.3.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.3.3">𝑙</ci><ci id="Sx3.SSx1.p2.11.m6.1.1.3.4.cmml" xref="Sx3.SSx1.p2.11.m6.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.11.m6.1c">\mathcal{F}_{llm}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.11.m6.1d">caligraphic_F start_POSTSUBSCRIPT italic_l italic_l italic_m end_POSTSUBSCRIPT</annotation></semantics></math> to generate final textual output <math alttext="y_{txt}" class="ltx_Math" display="inline" id="Sx3.SSx1.p2.12.m7.1"><semantics id="Sx3.SSx1.p2.12.m7.1a"><msub id="Sx3.SSx1.p2.12.m7.1.1" xref="Sx3.SSx1.p2.12.m7.1.1.cmml"><mi id="Sx3.SSx1.p2.12.m7.1.1.2" xref="Sx3.SSx1.p2.12.m7.1.1.2.cmml">y</mi><mrow id="Sx3.SSx1.p2.12.m7.1.1.3" xref="Sx3.SSx1.p2.12.m7.1.1.3.cmml"><mi id="Sx3.SSx1.p2.12.m7.1.1.3.2" xref="Sx3.SSx1.p2.12.m7.1.1.3.2.cmml">t</mi><mo id="Sx3.SSx1.p2.12.m7.1.1.3.1" xref="Sx3.SSx1.p2.12.m7.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.12.m7.1.1.3.3" xref="Sx3.SSx1.p2.12.m7.1.1.3.3.cmml">x</mi><mo id="Sx3.SSx1.p2.12.m7.1.1.3.1a" xref="Sx3.SSx1.p2.12.m7.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p2.12.m7.1.1.3.4" xref="Sx3.SSx1.p2.12.m7.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p2.12.m7.1b"><apply id="Sx3.SSx1.p2.12.m7.1.1.cmml" xref="Sx3.SSx1.p2.12.m7.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p2.12.m7.1.1.1.cmml" xref="Sx3.SSx1.p2.12.m7.1.1">subscript</csymbol><ci id="Sx3.SSx1.p2.12.m7.1.1.2.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.2">𝑦</ci><apply id="Sx3.SSx1.p2.12.m7.1.1.3.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.3"><times id="Sx3.SSx1.p2.12.m7.1.1.3.1.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.3.1"></times><ci id="Sx3.SSx1.p2.12.m7.1.1.3.2.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.3.2">𝑡</ci><ci id="Sx3.SSx1.p2.12.m7.1.1.3.3.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.3.3">𝑥</ci><ci id="Sx3.SSx1.p2.12.m7.1.1.3.4.cmml" xref="Sx3.SSx1.p2.12.m7.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p2.12.m7.1c">y_{txt}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p2.12.m7.1d">italic_y start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="Sx3.SSx1.p2.12.1">i.e.</span>,</p>
<table class="ltx_equation ltx_eqn_table" id="Sx3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{y}_{txt}=\mathcal{F}_{llm}(z_{img}\|z_{txt})." class="ltx_Math" display="block" id="Sx3.E2.m1.1"><semantics id="Sx3.E2.m1.1a"><mrow id="Sx3.E2.m1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.cmml"><mrow id="Sx3.E2.m1.1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.cmml"><msub id="Sx3.E2.m1.1.1.1.1.3" xref="Sx3.E2.m1.1.1.1.1.3.cmml"><mover accent="true" id="Sx3.E2.m1.1.1.1.1.3.2" xref="Sx3.E2.m1.1.1.1.1.3.2.cmml"><mi id="Sx3.E2.m1.1.1.1.1.3.2.2" xref="Sx3.E2.m1.1.1.1.1.3.2.2.cmml">y</mi><mo id="Sx3.E2.m1.1.1.1.1.3.2.1" xref="Sx3.E2.m1.1.1.1.1.3.2.1.cmml">^</mo></mover><mrow id="Sx3.E2.m1.1.1.1.1.3.3" xref="Sx3.E2.m1.1.1.1.1.3.3.cmml"><mi id="Sx3.E2.m1.1.1.1.1.3.3.2" xref="Sx3.E2.m1.1.1.1.1.3.3.2.cmml">t</mi><mo id="Sx3.E2.m1.1.1.1.1.3.3.1" xref="Sx3.E2.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.3.3.3" xref="Sx3.E2.m1.1.1.1.1.3.3.3.cmml">x</mi><mo id="Sx3.E2.m1.1.1.1.1.3.3.1a" xref="Sx3.E2.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.3.3.4" xref="Sx3.E2.m1.1.1.1.1.3.3.4.cmml">t</mi></mrow></msub><mo id="Sx3.E2.m1.1.1.1.1.2" xref="Sx3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx3.E2.m1.1.1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.1.cmml"><msub id="Sx3.E2.m1.1.1.1.1.1.3" xref="Sx3.E2.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.E2.m1.1.1.1.1.1.3.2" xref="Sx3.E2.m1.1.1.1.1.1.3.2.cmml">ℱ</mi><mrow id="Sx3.E2.m1.1.1.1.1.1.3.3" xref="Sx3.E2.m1.1.1.1.1.1.3.3.cmml"><mi id="Sx3.E2.m1.1.1.1.1.1.3.3.2" xref="Sx3.E2.m1.1.1.1.1.1.3.3.2.cmml">l</mi><mo id="Sx3.E2.m1.1.1.1.1.1.3.3.1" xref="Sx3.E2.m1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.3.3.3" xref="Sx3.E2.m1.1.1.1.1.1.3.3.3.cmml">l</mi><mo id="Sx3.E2.m1.1.1.1.1.1.3.3.1a" xref="Sx3.E2.m1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.3.3.4" xref="Sx3.E2.m1.1.1.1.1.1.3.3.4.cmml">m</mi></mrow></msub><mo id="Sx3.E2.m1.1.1.1.1.1.2" xref="Sx3.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Sx3.E2.m1.1.1.1.1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.E2.m1.1.1.1.1.1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">z</mi><mrow id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.2" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.3" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">m</mi><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1a" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.4" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.1.1" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">∥</mo><msub id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">z</mi><mrow id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.2" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.3" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">x</mi><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1a" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.4" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">t</mi></mrow></msub></mrow><mo id="Sx3.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx3.E2.m1.1.1.1.2" lspace="0em" xref="Sx3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E2.m1.1b"><apply id="Sx3.E2.m1.1.1.1.1.cmml" xref="Sx3.E2.m1.1.1.1"><eq id="Sx3.E2.m1.1.1.1.1.2.cmml" xref="Sx3.E2.m1.1.1.1.1.2"></eq><apply id="Sx3.E2.m1.1.1.1.1.3.cmml" xref="Sx3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E2.m1.1.1.1.1.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.3">subscript</csymbol><apply id="Sx3.E2.m1.1.1.1.1.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.3.2"><ci id="Sx3.E2.m1.1.1.1.1.3.2.1.cmml" xref="Sx3.E2.m1.1.1.1.1.3.2.1">^</ci><ci id="Sx3.E2.m1.1.1.1.1.3.2.2.cmml" xref="Sx3.E2.m1.1.1.1.1.3.2.2">𝑦</ci></apply><apply id="Sx3.E2.m1.1.1.1.1.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.3.3"><times id="Sx3.E2.m1.1.1.1.1.3.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.3.3.1"></times><ci id="Sx3.E2.m1.1.1.1.1.3.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.3.3.2">𝑡</ci><ci id="Sx3.E2.m1.1.1.1.1.3.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.3.3.3">𝑥</ci><ci id="Sx3.E2.m1.1.1.1.1.3.3.4.cmml" xref="Sx3.E2.m1.1.1.1.1.3.3.4">𝑡</ci></apply></apply><apply id="Sx3.E2.m1.1.1.1.1.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1"><times id="Sx3.E2.m1.1.1.1.1.1.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.2"></times><apply id="Sx3.E2.m1.1.1.1.1.1.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E2.m1.1.1.1.1.1.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E2.m1.1.1.1.1.1.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.2">ℱ</ci><apply id="Sx3.E2.m1.1.1.1.1.1.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.3"><times id="Sx3.E2.m1.1.1.1.1.1.3.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.3.1"></times><ci id="Sx3.E2.m1.1.1.1.1.1.3.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.3.2">𝑙</ci><ci id="Sx3.E2.m1.1.1.1.1.1.3.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.3.3">𝑙</ci><ci id="Sx3.E2.m1.1.1.1.1.1.3.3.4.cmml" xref="Sx3.E2.m1.1.1.1.1.1.3.3.4">𝑚</ci></apply></apply><apply id="Sx3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.2">𝑧</ci><apply id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3"><times id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.3">𝑚</ci><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.4.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.2.3.4">𝑔</ci></apply></apply><apply id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.2">𝑧</ci><apply id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3"><times id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.3">𝑥</ci><ci id="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="Sx3.E2.m1.1.1.1.1.1.1.1.1.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E2.m1.1c">\hat{y}_{txt}=\mathcal{F}_{llm}(z_{img}\|z_{txt}).</annotation><annotation encoding="application/x-llamapun" id="Sx3.E2.m1.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT = caligraphic_F start_POSTSUBSCRIPT italic_l italic_l italic_m end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ∥ italic_z start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Sx3.SSx1.p3">
<p class="ltx_p" id="Sx3.SSx1.p3.1">Following LISA <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>, we adopt the embedding-as-mask paradigm to bridge these two modules.
In this paradigm, the vocabulary of the model is augmented with a specialized token ‘[SEG]’, designed to explicitly activate the segmentation behavior of the segmentation model.
When the LMM intends to generate a segmentation mask based on the user instruction, it inserts the ‘[SEG]’ token in the output sequence <math alttext="y_{txt}" class="ltx_Math" display="inline" id="Sx3.SSx1.p3.1.m1.1"><semantics id="Sx3.SSx1.p3.1.m1.1a"><msub id="Sx3.SSx1.p3.1.m1.1.1" xref="Sx3.SSx1.p3.1.m1.1.1.cmml"><mi id="Sx3.SSx1.p3.1.m1.1.1.2" xref="Sx3.SSx1.p3.1.m1.1.1.2.cmml">y</mi><mrow id="Sx3.SSx1.p3.1.m1.1.1.3" xref="Sx3.SSx1.p3.1.m1.1.1.3.cmml"><mi id="Sx3.SSx1.p3.1.m1.1.1.3.2" xref="Sx3.SSx1.p3.1.m1.1.1.3.2.cmml">t</mi><mo id="Sx3.SSx1.p3.1.m1.1.1.3.1" xref="Sx3.SSx1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p3.1.m1.1.1.3.3" xref="Sx3.SSx1.p3.1.m1.1.1.3.3.cmml">x</mi><mo id="Sx3.SSx1.p3.1.m1.1.1.3.1a" xref="Sx3.SSx1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p3.1.m1.1.1.3.4" xref="Sx3.SSx1.p3.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.1.m1.1b"><apply id="Sx3.SSx1.p3.1.m1.1.1.cmml" xref="Sx3.SSx1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.1.m1.1.1.1.cmml" xref="Sx3.SSx1.p3.1.m1.1.1">subscript</csymbol><ci id="Sx3.SSx1.p3.1.m1.1.1.2.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.2">𝑦</ci><apply id="Sx3.SSx1.p3.1.m1.1.1.3.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3"><times id="Sx3.SSx1.p3.1.m1.1.1.3.1.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3.1"></times><ci id="Sx3.SSx1.p3.1.m1.1.1.3.2.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3.2">𝑡</ci><ci id="Sx3.SSx1.p3.1.m1.1.1.3.3.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3.3">𝑥</ci><ci id="Sx3.SSx1.p3.1.m1.1.1.3.4.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.1.m1.1c">y_{txt}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p3.1.m1.1d">italic_y start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to indicate the presence of a target to segment.
For example:
</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p4">
<svg class="ltx_picture" height="43.44" id="Sx3.SSx1.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,43.44) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 3.95 L 0 39.5 C 0 41.68 1.77 43.44 3.95 43.44 L 596.05 43.44 C 598.23 43.44 600 41.68 600 39.5 L 600 3.95 C 600 1.77 598.23 0 596.05 0 L 3.95 0 C 1.77 0 0 1.77 0 3.95 Z" style="stroke:none"></path></g><g fill="#DEEBF7" fill-opacity="1.0"><path d="M 1.18 3.95 L 1.18 39.5 C 1.18 41.02 2.42 42.26 3.95 42.26 L 596.05 42.26 C 597.58 42.26 598.82 41.02 598.82 39.5 L 598.82 3.95 C 598.82 2.42 597.58 1.18 596.05 1.18 L 3.95 1.18 C 2.42 1.18 1.18 2.42 1.18 3.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 6.5 6.5)"><foreignobject color="#000000" height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="587">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="Sx3.SSx1.p4.pic1.1.1.1.1.1" style="width:424.2pt;">
<span class="ltx_p" id="Sx3.SSx1.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p4.pic1.1.1.1.1.1.1.1">User</span>: &lt;IMAGE&gt; Please segment the dog in this image.</span>
<span class="ltx_p" id="Sx3.SSx1.p4.pic1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p4.pic1.1.1.1.1.1.2.1">Assistant</span>: Sure, the segmentation result is dog [SEG].</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p5">
<p class="ltx_p" id="Sx3.SSx1.p5.7"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p5.7.1">Segmentation Model.</span>
This work employs SAM <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib25" title="">2023</a>)</cite> as our foundation segmentation architecture because of its promising pixel-level modeling capability.
As shown in Figure. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F3" title="Figure 3 ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>, the pixel encoder <math alttext="\mathcal{E}_{pixel}" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.1.m1.1"><semantics id="Sx3.SSx1.p5.1.m1.1a"><msub id="Sx3.SSx1.p5.1.m1.1.1" xref="Sx3.SSx1.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p5.1.m1.1.1.2" xref="Sx3.SSx1.p5.1.m1.1.1.2.cmml">ℰ</mi><mrow id="Sx3.SSx1.p5.1.m1.1.1.3" xref="Sx3.SSx1.p5.1.m1.1.1.3.cmml"><mi id="Sx3.SSx1.p5.1.m1.1.1.3.2" xref="Sx3.SSx1.p5.1.m1.1.1.3.2.cmml">p</mi><mo id="Sx3.SSx1.p5.1.m1.1.1.3.1" xref="Sx3.SSx1.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.1.m1.1.1.3.3" xref="Sx3.SSx1.p5.1.m1.1.1.3.3.cmml">i</mi><mo id="Sx3.SSx1.p5.1.m1.1.1.3.1a" xref="Sx3.SSx1.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.1.m1.1.1.3.4" xref="Sx3.SSx1.p5.1.m1.1.1.3.4.cmml">x</mi><mo id="Sx3.SSx1.p5.1.m1.1.1.3.1b" xref="Sx3.SSx1.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.1.m1.1.1.3.5" xref="Sx3.SSx1.p5.1.m1.1.1.3.5.cmml">e</mi><mo id="Sx3.SSx1.p5.1.m1.1.1.3.1c" xref="Sx3.SSx1.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.1.m1.1.1.3.6" xref="Sx3.SSx1.p5.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.1.m1.1b"><apply id="Sx3.SSx1.p5.1.m1.1.1.cmml" xref="Sx3.SSx1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p5.1.m1.1.1.1.cmml" xref="Sx3.SSx1.p5.1.m1.1.1">subscript</csymbol><ci id="Sx3.SSx1.p5.1.m1.1.1.2.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.2">ℰ</ci><apply id="Sx3.SSx1.p5.1.m1.1.1.3.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3"><times id="Sx3.SSx1.p5.1.m1.1.1.3.1.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.1"></times><ci id="Sx3.SSx1.p5.1.m1.1.1.3.2.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.2">𝑝</ci><ci id="Sx3.SSx1.p5.1.m1.1.1.3.3.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.3">𝑖</ci><ci id="Sx3.SSx1.p5.1.m1.1.1.3.4.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.4">𝑥</ci><ci id="Sx3.SSx1.p5.1.m1.1.1.3.5.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.5">𝑒</ci><ci id="Sx3.SSx1.p5.1.m1.1.1.3.6.cmml" xref="Sx3.SSx1.p5.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.1.m1.1c">\mathcal{E}_{pixel}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is instantiated using a frozen SAM encoder, while the pixel decoder <math alttext="\mathcal{D}_{pixel}" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.2.m2.1"><semantics id="Sx3.SSx1.p5.2.m2.1a"><msub id="Sx3.SSx1.p5.2.m2.1.1" xref="Sx3.SSx1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p5.2.m2.1.1.2" xref="Sx3.SSx1.p5.2.m2.1.1.2.cmml">𝒟</mi><mrow id="Sx3.SSx1.p5.2.m2.1.1.3" xref="Sx3.SSx1.p5.2.m2.1.1.3.cmml"><mi id="Sx3.SSx1.p5.2.m2.1.1.3.2" xref="Sx3.SSx1.p5.2.m2.1.1.3.2.cmml">p</mi><mo id="Sx3.SSx1.p5.2.m2.1.1.3.1" xref="Sx3.SSx1.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.2.m2.1.1.3.3" xref="Sx3.SSx1.p5.2.m2.1.1.3.3.cmml">i</mi><mo id="Sx3.SSx1.p5.2.m2.1.1.3.1a" xref="Sx3.SSx1.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.2.m2.1.1.3.4" xref="Sx3.SSx1.p5.2.m2.1.1.3.4.cmml">x</mi><mo id="Sx3.SSx1.p5.2.m2.1.1.3.1b" xref="Sx3.SSx1.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.2.m2.1.1.3.5" xref="Sx3.SSx1.p5.2.m2.1.1.3.5.cmml">e</mi><mo id="Sx3.SSx1.p5.2.m2.1.1.3.1c" xref="Sx3.SSx1.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.2.m2.1.1.3.6" xref="Sx3.SSx1.p5.2.m2.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.2.m2.1b"><apply id="Sx3.SSx1.p5.2.m2.1.1.cmml" xref="Sx3.SSx1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p5.2.m2.1.1.1.cmml" xref="Sx3.SSx1.p5.2.m2.1.1">subscript</csymbol><ci id="Sx3.SSx1.p5.2.m2.1.1.2.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.2">𝒟</ci><apply id="Sx3.SSx1.p5.2.m2.1.1.3.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3"><times id="Sx3.SSx1.p5.2.m2.1.1.3.1.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.1"></times><ci id="Sx3.SSx1.p5.2.m2.1.1.3.2.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.2">𝑝</ci><ci id="Sx3.SSx1.p5.2.m2.1.1.3.3.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.3">𝑖</ci><ci id="Sx3.SSx1.p5.2.m2.1.1.3.4.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.4">𝑥</ci><ci id="Sx3.SSx1.p5.2.m2.1.1.3.5.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.5">𝑒</ci><ci id="Sx3.SSx1.p5.2.m2.1.1.3.6.cmml" xref="Sx3.SSx1.p5.2.m2.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.2.m2.1c">\mathcal{D}_{pixel}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.2.m2.1d">caligraphic_D start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is initialized from the pre-trained SAM decoder.
The former takes the high-resolution image as input to extract fine-grained visual information, while the latter generates the desired segmentation masks prompted by the embedding of the ‘[SEG]’ token from the LLM.
Specifically, we select the output embedding <math alttext="\hat{z}_{seg}" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.3.m3.1"><semantics id="Sx3.SSx1.p5.3.m3.1a"><msub id="Sx3.SSx1.p5.3.m3.1.1" xref="Sx3.SSx1.p5.3.m3.1.1.cmml"><mover accent="true" id="Sx3.SSx1.p5.3.m3.1.1.2" xref="Sx3.SSx1.p5.3.m3.1.1.2.cmml"><mi id="Sx3.SSx1.p5.3.m3.1.1.2.2" xref="Sx3.SSx1.p5.3.m3.1.1.2.2.cmml">z</mi><mo id="Sx3.SSx1.p5.3.m3.1.1.2.1" xref="Sx3.SSx1.p5.3.m3.1.1.2.1.cmml">^</mo></mover><mrow id="Sx3.SSx1.p5.3.m3.1.1.3" xref="Sx3.SSx1.p5.3.m3.1.1.3.cmml"><mi id="Sx3.SSx1.p5.3.m3.1.1.3.2" xref="Sx3.SSx1.p5.3.m3.1.1.3.2.cmml">s</mi><mo id="Sx3.SSx1.p5.3.m3.1.1.3.1" xref="Sx3.SSx1.p5.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.3.m3.1.1.3.3" xref="Sx3.SSx1.p5.3.m3.1.1.3.3.cmml">e</mi><mo id="Sx3.SSx1.p5.3.m3.1.1.3.1a" xref="Sx3.SSx1.p5.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.3.m3.1.1.3.4" xref="Sx3.SSx1.p5.3.m3.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.3.m3.1b"><apply id="Sx3.SSx1.p5.3.m3.1.1.cmml" xref="Sx3.SSx1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p5.3.m3.1.1.1.cmml" xref="Sx3.SSx1.p5.3.m3.1.1">subscript</csymbol><apply id="Sx3.SSx1.p5.3.m3.1.1.2.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.2"><ci id="Sx3.SSx1.p5.3.m3.1.1.2.1.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.2.1">^</ci><ci id="Sx3.SSx1.p5.3.m3.1.1.2.2.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.2.2">𝑧</ci></apply><apply id="Sx3.SSx1.p5.3.m3.1.1.3.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.3"><times id="Sx3.SSx1.p5.3.m3.1.1.3.1.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.3.1"></times><ci id="Sx3.SSx1.p5.3.m3.1.1.3.2.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.3.2">𝑠</ci><ci id="Sx3.SSx1.p5.3.m3.1.1.3.3.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.3.3">𝑒</ci><ci id="Sx3.SSx1.p5.3.m3.1.1.3.4.cmml" xref="Sx3.SSx1.p5.3.m3.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.3.m3.1c">\hat{z}_{seg}</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.3.m3.1d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math> corresponding to the ‘[SEG]’ token <math alttext="\hat{y}_{txt}([SEG])" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.4.m4.1"><semantics id="Sx3.SSx1.p5.4.m4.1a"><mrow id="Sx3.SSx1.p5.4.m4.1.1" xref="Sx3.SSx1.p5.4.m4.1.1.cmml"><msub id="Sx3.SSx1.p5.4.m4.1.1.3" xref="Sx3.SSx1.p5.4.m4.1.1.3.cmml"><mover accent="true" id="Sx3.SSx1.p5.4.m4.1.1.3.2" xref="Sx3.SSx1.p5.4.m4.1.1.3.2.cmml"><mi id="Sx3.SSx1.p5.4.m4.1.1.3.2.2" xref="Sx3.SSx1.p5.4.m4.1.1.3.2.2.cmml">y</mi><mo id="Sx3.SSx1.p5.4.m4.1.1.3.2.1" xref="Sx3.SSx1.p5.4.m4.1.1.3.2.1.cmml">^</mo></mover><mrow id="Sx3.SSx1.p5.4.m4.1.1.3.3" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.cmml"><mi id="Sx3.SSx1.p5.4.m4.1.1.3.3.2" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.2.cmml">t</mi><mo id="Sx3.SSx1.p5.4.m4.1.1.3.3.1" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.4.m4.1.1.3.3.3" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.3.cmml">x</mi><mo id="Sx3.SSx1.p5.4.m4.1.1.3.3.1a" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.4.m4.1.1.3.3.4" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.4.cmml">t</mi></mrow></msub><mo id="Sx3.SSx1.p5.4.m4.1.1.2" xref="Sx3.SSx1.p5.4.m4.1.1.2.cmml">⁢</mo><mrow id="Sx3.SSx1.p5.4.m4.1.1.1.1" xref="Sx3.SSx1.p5.4.m4.1.1.cmml"><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.2" stretchy="false" xref="Sx3.SSx1.p5.4.m4.1.1.cmml">(</mo><mrow id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.2.cmml"><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.2" stretchy="false" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.2.1.cmml">[</mo><mrow id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.cmml"><mi id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.2" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.2.cmml">S</mi><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.3" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.3.cmml">E</mi><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1a" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.4" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.4.cmml">G</mi></mrow><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.3" stretchy="false" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.2.1.cmml">]</mo></mrow><mo id="Sx3.SSx1.p5.4.m4.1.1.1.1.3" stretchy="false" xref="Sx3.SSx1.p5.4.m4.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.4.m4.1b"><apply id="Sx3.SSx1.p5.4.m4.1.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1"><times id="Sx3.SSx1.p5.4.m4.1.1.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.2"></times><apply id="Sx3.SSx1.p5.4.m4.1.1.3.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="Sx3.SSx1.p5.4.m4.1.1.3.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3">subscript</csymbol><apply id="Sx3.SSx1.p5.4.m4.1.1.3.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.2"><ci id="Sx3.SSx1.p5.4.m4.1.1.3.2.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.2.1">^</ci><ci id="Sx3.SSx1.p5.4.m4.1.1.3.2.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.2.2">𝑦</ci></apply><apply id="Sx3.SSx1.p5.4.m4.1.1.3.3.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.3"><times id="Sx3.SSx1.p5.4.m4.1.1.3.3.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.1"></times><ci id="Sx3.SSx1.p5.4.m4.1.1.3.3.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.2">𝑡</ci><ci id="Sx3.SSx1.p5.4.m4.1.1.3.3.3.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.3">𝑥</ci><ci id="Sx3.SSx1.p5.4.m4.1.1.3.3.4.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.3.3.4">𝑡</ci></apply></apply><apply id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.2.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1"><times id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.1"></times><ci id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.2.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.2">𝑆</ci><ci id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.3.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.3">𝐸</ci><ci id="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.4.cmml" xref="Sx3.SSx1.p5.4.m4.1.1.1.1.1.1.1.4">𝐺</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.4.m4.1c">\hat{y}_{txt}([SEG])</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.4.m4.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t italic_x italic_t end_POSTSUBSCRIPT ( [ italic_S italic_E italic_G ] )</annotation></semantics></math> and transform it into the feature space of decoder using a projector <math alttext="\psi" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.5.m5.1"><semantics id="Sx3.SSx1.p5.5.m5.1a"><mi id="Sx3.SSx1.p5.5.m5.1.1" xref="Sx3.SSx1.p5.5.m5.1.1.cmml">ψ</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.5.m5.1b"><ci id="Sx3.SSx1.p5.5.m5.1.1.cmml" xref="Sx3.SSx1.p5.5.m5.1.1">𝜓</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.5.m5.1c">\psi</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.5.m5.1d">italic_ψ</annotation></semantics></math>.
Notably, the structure and initialization of projector <math alttext="\psi" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.6.m6.1"><semantics id="Sx3.SSx1.p5.6.m6.1a"><mi id="Sx3.SSx1.p5.6.m6.1.1" xref="Sx3.SSx1.p5.6.m6.1.1.cmml">ψ</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.6.m6.1b"><ci id="Sx3.SSx1.p5.6.m6.1.1.cmml" xref="Sx3.SSx1.p5.6.m6.1.1">𝜓</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.6.m6.1c">\psi</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.6.m6.1d">italic_ψ</annotation></semantics></math> are identical to those of projector <math alttext="\phi" class="ltx_Math" display="inline" id="Sx3.SSx1.p5.7.m7.1"><semantics id="Sx3.SSx1.p5.7.m7.1a"><mi id="Sx3.SSx1.p5.7.m7.1.1" xref="Sx3.SSx1.p5.7.m7.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.7.m7.1b"><ci id="Sx3.SSx1.p5.7.m7.1.1.cmml" xref="Sx3.SSx1.p5.7.m7.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.7.m7.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p5.7.m7.1d">italic_ϕ</annotation></semantics></math>.
The entire process can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="Sx3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{y}_{mask}=\mathcal{D}_{pixel}(\mathcal{E}_{pixel}(x_{img}),\psi(\hat{z}_{%
seg}))." class="ltx_Math" display="block" id="Sx3.E3.m1.1"><semantics id="Sx3.E3.m1.1a"><mrow id="Sx3.E3.m1.1.1.1" xref="Sx3.E3.m1.1.1.1.1.cmml"><mrow id="Sx3.E3.m1.1.1.1.1" xref="Sx3.E3.m1.1.1.1.1.cmml"><msub id="Sx3.E3.m1.1.1.1.1.4" xref="Sx3.E3.m1.1.1.1.1.4.cmml"><mover accent="true" id="Sx3.E3.m1.1.1.1.1.4.2" xref="Sx3.E3.m1.1.1.1.1.4.2.cmml"><mi id="Sx3.E3.m1.1.1.1.1.4.2.2" xref="Sx3.E3.m1.1.1.1.1.4.2.2.cmml">y</mi><mo id="Sx3.E3.m1.1.1.1.1.4.2.1" xref="Sx3.E3.m1.1.1.1.1.4.2.1.cmml">^</mo></mover><mrow id="Sx3.E3.m1.1.1.1.1.4.3" xref="Sx3.E3.m1.1.1.1.1.4.3.cmml"><mi id="Sx3.E3.m1.1.1.1.1.4.3.2" xref="Sx3.E3.m1.1.1.1.1.4.3.2.cmml">m</mi><mo id="Sx3.E3.m1.1.1.1.1.4.3.1" xref="Sx3.E3.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.4.3.3" xref="Sx3.E3.m1.1.1.1.1.4.3.3.cmml">a</mi><mo id="Sx3.E3.m1.1.1.1.1.4.3.1a" xref="Sx3.E3.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.4.3.4" xref="Sx3.E3.m1.1.1.1.1.4.3.4.cmml">s</mi><mo id="Sx3.E3.m1.1.1.1.1.4.3.1b" xref="Sx3.E3.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.4.3.5" xref="Sx3.E3.m1.1.1.1.1.4.3.5.cmml">k</mi></mrow></msub><mo id="Sx3.E3.m1.1.1.1.1.3" xref="Sx3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="Sx3.E3.m1.1.1.1.1.2" xref="Sx3.E3.m1.1.1.1.1.2.cmml"><msub id="Sx3.E3.m1.1.1.1.1.2.4" xref="Sx3.E3.m1.1.1.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.E3.m1.1.1.1.1.2.4.2" xref="Sx3.E3.m1.1.1.1.1.2.4.2.cmml">𝒟</mi><mrow id="Sx3.E3.m1.1.1.1.1.2.4.3" xref="Sx3.E3.m1.1.1.1.1.2.4.3.cmml"><mi id="Sx3.E3.m1.1.1.1.1.2.4.3.2" xref="Sx3.E3.m1.1.1.1.1.2.4.3.2.cmml">p</mi><mo id="Sx3.E3.m1.1.1.1.1.2.4.3.1" xref="Sx3.E3.m1.1.1.1.1.2.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.4.3.3" xref="Sx3.E3.m1.1.1.1.1.2.4.3.3.cmml">i</mi><mo id="Sx3.E3.m1.1.1.1.1.2.4.3.1a" xref="Sx3.E3.m1.1.1.1.1.2.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.4.3.4" xref="Sx3.E3.m1.1.1.1.1.2.4.3.4.cmml">x</mi><mo id="Sx3.E3.m1.1.1.1.1.2.4.3.1b" xref="Sx3.E3.m1.1.1.1.1.2.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.4.3.5" xref="Sx3.E3.m1.1.1.1.1.2.4.3.5.cmml">e</mi><mo id="Sx3.E3.m1.1.1.1.1.2.4.3.1c" xref="Sx3.E3.m1.1.1.1.1.2.4.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.4.3.6" xref="Sx3.E3.m1.1.1.1.1.2.4.3.6.cmml">l</mi></mrow></msub><mo id="Sx3.E3.m1.1.1.1.1.2.3" xref="Sx3.E3.m1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="Sx3.E3.m1.1.1.1.1.2.2.2" xref="Sx3.E3.m1.1.1.1.1.2.2.3.cmml"><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="Sx3.E3.m1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">ℰ</mi><mrow id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.2" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">p</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.3" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1a" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.4" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">x</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1b" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.5" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.5.cmml">e</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1c" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.6" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.6.cmml">l</mi></mrow></msub><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">m</mi><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.4" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">g</mi></mrow></msub><mo id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.4" xref="Sx3.E3.m1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="Sx3.E3.m1.1.1.1.1.2.2.2.2" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.cmml"><mi id="Sx3.E3.m1.1.1.1.1.2.2.2.2.3" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.3.cmml">ψ</mi><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.2" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.2.cmml">⁢</mo><mrow id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.cmml"><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.2" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.cmml">(</mo><msub id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.cmml"><mover accent="true" id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.cmml"><mi id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.2" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.2.cmml">z</mi><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.1" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.1.cmml">^</mo></mover><mrow id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.cmml"><mi id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.2" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.2.cmml">s</mi><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.3" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.3.cmml">e</mi><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1a" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.4" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.4.cmml">g</mi></mrow></msub><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.3" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx3.E3.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="Sx3.E3.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="Sx3.E3.m1.1.1.1.2" lspace="0em" xref="Sx3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E3.m1.1b"><apply id="Sx3.E3.m1.1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1"><eq id="Sx3.E3.m1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.1.1.1.1.3"></eq><apply id="Sx3.E3.m1.1.1.1.1.4.cmml" xref="Sx3.E3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="Sx3.E3.m1.1.1.1.1.4.1.cmml" xref="Sx3.E3.m1.1.1.1.1.4">subscript</csymbol><apply id="Sx3.E3.m1.1.1.1.1.4.2.cmml" xref="Sx3.E3.m1.1.1.1.1.4.2"><ci id="Sx3.E3.m1.1.1.1.1.4.2.1.cmml" xref="Sx3.E3.m1.1.1.1.1.4.2.1">^</ci><ci id="Sx3.E3.m1.1.1.1.1.4.2.2.cmml" xref="Sx3.E3.m1.1.1.1.1.4.2.2">𝑦</ci></apply><apply id="Sx3.E3.m1.1.1.1.1.4.3.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3"><times id="Sx3.E3.m1.1.1.1.1.4.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3.1"></times><ci id="Sx3.E3.m1.1.1.1.1.4.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3.2">𝑚</ci><ci id="Sx3.E3.m1.1.1.1.1.4.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3.3">𝑎</ci><ci id="Sx3.E3.m1.1.1.1.1.4.3.4.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3.4">𝑠</ci><ci id="Sx3.E3.m1.1.1.1.1.4.3.5.cmml" xref="Sx3.E3.m1.1.1.1.1.4.3.5">𝑘</ci></apply></apply><apply id="Sx3.E3.m1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2"><times id="Sx3.E3.m1.1.1.1.1.2.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.3"></times><apply id="Sx3.E3.m1.1.1.1.1.2.4.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="Sx3.E3.m1.1.1.1.1.2.4.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4">subscript</csymbol><ci id="Sx3.E3.m1.1.1.1.1.2.4.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.2">𝒟</ci><apply id="Sx3.E3.m1.1.1.1.1.2.4.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3"><times id="Sx3.E3.m1.1.1.1.1.2.4.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.1"></times><ci id="Sx3.E3.m1.1.1.1.1.2.4.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.2">𝑝</ci><ci id="Sx3.E3.m1.1.1.1.1.2.4.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.3">𝑖</ci><ci id="Sx3.E3.m1.1.1.1.1.2.4.3.4.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.4">𝑥</ci><ci id="Sx3.E3.m1.1.1.1.1.2.4.3.5.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.5">𝑒</ci><ci id="Sx3.E3.m1.1.1.1.1.2.4.3.6.cmml" xref="Sx3.E3.m1.1.1.1.1.2.4.3.6">𝑙</ci></apply></apply><interval closure="open" id="Sx3.E3.m1.1.1.1.1.2.2.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2"><apply id="Sx3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1"><times id="Sx3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.2"></times><apply id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.2">ℰ</ci><apply id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3"><times id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.2">𝑝</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.4">𝑥</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.5.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.5">𝑒</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.6.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.3.3.6">𝑙</ci></apply></apply><apply id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑚</ci><ci id="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="Sx3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.4">𝑔</ci></apply></apply></apply><apply id="Sx3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2"><times id="Sx3.E3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.2"></times><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.3">𝜓</ci><apply id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1">subscript</csymbol><apply id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2"><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.1">^</ci><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.2.2">𝑧</ci></apply><apply id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3"><times id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.1"></times><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.2.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.2">𝑠</ci><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.3.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.3">𝑒</ci><ci id="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.4.cmml" xref="Sx3.E3.m1.1.1.1.1.2.2.2.2.1.1.1.3.4">𝑔</ci></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E3.m1.1c">\hat{y}_{mask}=\mathcal{D}_{pixel}(\mathcal{E}_{pixel}(x_{img}),\psi(\hat{z}_{%
seg})).</annotation><annotation encoding="application/x-llamapun" id="Sx3.E3.m1.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT = caligraphic_D start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT ( caligraphic_E start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ) , italic_ψ ( over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="Sx3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="356" id="Sx3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The overview of our proposed data auto-annotated pipeline.
Due to space limitations, the detailed caption is not shown in the figure.
Please refer to the <span class="ltx_text ltx_font_bold" id="Sx3.F4.2.1">Appendix. B</span> for the detailed version.
Best viewed with zoom-in.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Design of Unified SegCap Data Format</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">Most existing studies primarily integrate various pixel-level segmentation capabilities into LMMs by directly extending corresponding task datasets.
For example, in referring segmentation, the query may be a phrase that requires the return of segmentation masks.
Conversely, in reasoning segmentation, the query can be a longer sentence or question in which the target may not be present, necessitating an answer along with segmentation masks.
In different segmentation tasks, the form and semantics of queries vary.
In this context, the model must adaptively align the semantic concepts of potential targets with visual features during training, which undoubtedly increases the burden on model learning.
Therefore, we propose a unified SegCap data format to leverage these data, explicitly guiding the model toward improved vision-language alignment.
In this manner, we unify the output formats of different segmentation tasks, bridging the gap between them and reducing the difficulty of multi-task learning for the model.
Specifically, apart from the ‘[SEG]’ token, we also introduce &lt;p&gt; and &lt;/p&gt; tokens to the vocabulary of the LMM to denote the start and end of the corresponding phrases of the segmentation mask, respectively.
The LLM is required to mark the corresponding description with &lt;p&gt; and &lt;/p&gt; while activating the segmentation behavior using ‘[SEG]’. The following is an example of data format for multi-referring segmentation:
</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p2">
<svg class="ltx_picture" height="60.05" id="Sx3.SSx2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,60.05) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 3.95 L 0 56.1 C 0 58.28 1.77 60.05 3.95 60.05 L 596.05 60.05 C 598.23 60.05 600 58.28 600 56.1 L 600 3.95 C 600 1.77 598.23 0 596.05 0 L 3.95 0 C 1.77 0 0 1.77 0 3.95 Z" style="stroke:none"></path></g><g fill="#DEEBF7" fill-opacity="1.0"><path d="M 1.18 3.95 L 1.18 56.1 C 1.18 57.63 2.42 58.87 3.95 58.87 L 596.05 58.87 C 597.58 58.87 598.82 57.63 598.82 56.1 L 598.82 3.95 C 598.82 2.42 597.58 1.18 596.05 1.18 L 3.95 1.18 C 2.42 1.18 1.18 2.42 1.18 3.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 6.5 6.5)"><foreignobject color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="587">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="Sx3.SSx2.p2.pic1.1.1.1.1.1" style="width:424.2pt;">
<span class="ltx_p" id="Sx3.SSx2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p2.pic1.1.1.1.1.1.1.1">User</span>: &lt;IMAGE&gt; Please segment the {obj-1}, {obj-2}, …, and {obj-n} in this image.</span>
<span class="ltx_p" id="Sx3.SSx2.p2.pic1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p2.pic1.1.1.1.1.1.2.1">Assistant</span>: Sure,
&lt;p&gt; {obj-1} &lt;/p&gt; [SEG],
&lt;p&gt; {obj-2} &lt;/p&gt; [SEG], …, and
&lt;p&gt; {obj-n} &lt;/p&gt; [SEG].</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p3">
<p class="ltx_p" id="Sx3.SSx2.p3.1">Here, &lt;IMAGE&gt; denotes the placeholder for image tokens. {obj-n} represents the semantic description of the corresponding segmentation targets.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p4">
<p class="ltx_p" id="Sx3.SSx2.p4.1">In contrast to previous work, such a unified data pattern enables the model to explicitly learn the alignment relationships between the object concepts and the segmentation masks during training.
Despite the fact that GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite> had adopted a similar format, it was only employed for the GCG task it presented.
In contrast, we utilize this unified schema for all tasks, which reduces the modeling burden by minimizing the differences in output formats across tasks.
In Figure. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F3" title="Figure 3 ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>, we demonstrate our unified data format on tasks such as reasoning and multi-granularity segmentation.
Notably, during the training phase, we convert the annotation format of some existing open-source datasets into the proposed unified data schema as they do not meet our requirements. For more details on this process, please refer to <span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p4.1.1">Appendix. C</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Data Annotation Pipeline</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">Most existing segmentation datasets focus on instance-level objects, and although the SAM dataset provides fine-grained segmentation mask annotations, it lacks corresponding text descriptions.
Therefore, to address the issue of insufficient benchmarks for evaluating models in multi-granular segmentation and captioning, we propose a novel task called Multi-Granularity SegCap.
To build up this benchmark, we came up with an automated annotation pipeline that allows us to leverage the capabilities of LMMs, specifically the GPT-4 and Qwen-VL series, for data labeling.
In the following section, we will introduce our automatic annotation pipeline, designed to seamlessly transform any segmentation dataset.
This pipeline consists of three main steps, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx3.F4" title="Figure 4 ‣ Model Architecture ‣ Method ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">4</span></a>.
The first step focuses on generating short captions and detailed captions for each masked target, known as object labeling.
Subsequently, the second step constructs tree relationships based on the segmentation masks.
The third step organizes various levels of granular information by utilizing the raw data from different levels of the subtree.
As a result, we achieve multi-granularity segmentation and captioning annotations that demonstrate high alignment between visual and textual concepts.
Since the SAM <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib25" title="">2023</a>)</cite> dataset provides hundreds of millions of high-quality images and fine-grain segmentation, we perform our automated pipeline on the SAM dataset.</p>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Object Labeling</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">In step 1, the key point is generating a short caption and detailed caption for each target in the images.
The short caption is used as a semantic representation of the target.
The detailed caption is a comprehensive and semantically rich textual representation of the target, which is primarily used to provide a reference representation to limit the divergence and randomness of LMMs.
In practice, we leverage the GPT-4o to create instruction-following data to generate the semantic label of each masked object.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Mask Tree Building</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">After obtaining the semantic labels of each target, we need to organize the hierarchical relationships between each target within the image.
We discover that the hierarchical relationships between the targets could be effectively reflected by the Intersection of Union (IoU) relationships among the masks.
Therefore, we denote the entire image as the root node and then extend the tree according to the inclusion relationship between masks.
Besides, in the SAM dataset, numerous mask annotations exist within a single image, many of which share the same semantics labels.
For example, in a building with many windows, each window is represented as an individual mask with the same short captions.
For such nodes that share the same parent node, we merge the nodes and their masks.
In this manner, we obtain a simple and hierarchical tree and significantly shorten the length of the resulting text annotations.</p>
</div>
<figure class="ltx_table" id="Sx4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T2.1" style="width:433.6pt;height:115pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-158.5pt,41.8pt) scale(0.577634854868778,0.577634854868778) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T2.1.1">
<tr class="ltx_tr" id="Sx4.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx4.T2.1.1.1.1" rowspan="3"><span class="ltx_text" id="Sx4.T2.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="Sx4.T2.1.1.1.2">Textual Response</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="Sx4.T2.1.1.1.3">Mask Response</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="Sx4.T2.1.1.1.4">Textual &amp; Mask Response</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.1">Flickr30k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T2.1.1.2.2">NoCap</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.3">refCOCO+</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.4">refCOCOg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.5">gRefCOCO</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T2.1.1.2.6">reasonSeg</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Sx4.T2.1.1.2.7">GCG</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Sx4.T2.1.1.2.8">MGSC</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.3">
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.1">CIDEr</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.3.2">CIDEr</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.3">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.4">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.5">cIoU</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.3.6">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.7">CIDEr</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.8">AP50</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.9">CIDEr</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.10">AP50</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T2.1.1.4.1">LISA <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T2.1.1.4.3">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.4">65.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.5">67.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.6">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T2.1.1.4.7">46.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.8">33.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.9">25.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.10">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.4.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.5.1">PixelLM <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib42" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.5.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.4">66.3</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.5">69.3</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.6">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.5.7">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.8">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.9">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.6.1">GSVA <cite class="ltx_cite ltx_citemacro_citep">(Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.6.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.4">65.9</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.5">72.7</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.6">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.6.7">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.8">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.9">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.7.1">LaSagnA <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib51" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.7.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.4">66.4</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.5">70.6</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.6">38.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.7.7">47.2</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.8">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.9">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.8.1">PSALM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib61" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.8.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.4">72.9</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.5">73.8</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.6">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.8.7">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.8">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.9">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.9.1">OMG-LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.9.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.4">69.1</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.5">72.9</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.6">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.9.7">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.8">41.2</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.9">29.9</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.9.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx4.T2.1.1.10.1">GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.2">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.10.3">106.8</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.4">72.6</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.5">74.2</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.6">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx4.T2.1.1.10.7">–</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.8">47.2</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.9">30.8</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.10">8.7</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.11">5.4</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="Sx4.T2.1.1.11.1">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.2"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.2.1">104.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="Sx4.T2.1.1.11.3"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.3.1">112.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.4"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.4.1">73.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.5"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.5.1">77.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.6"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.6.1">52.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="Sx4.T2.1.1.11.7"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.7.1">51.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.8"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.8.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.9"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.9.1">31.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.10"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.10.1">11.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.11.11"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.11.1">7.4</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The comprehensive comparison of MGLMM and other LMMs in terms of text description and pixel-level understanding capabilities. “–” indicates that the method does not handle this task.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Dense Context Organization</h3>
<div class="ltx_para" id="Sx4.SSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.p1.1">The generation of multi-granularity captions is based on the mask tree which provides semantic labels of each target and hierarchical relationships between them.
First, we utilize the semantic labels of child nodes of the root node to generate an ordered text input which mainly includes the instance-level objects in the image, which aims to create a coarse-grained caption for the entire picture.
Subsequently, we concatenate the well-designed prompt, the ordered text input, and the image to prompt GPT-4o and obtain an organized description in which each target is embedded in a natural and coherent sequence.
We apply the same process on each subtree under the root node.
In particular, we use all the descendant nodes of the subtree to build up a description aiming to obtain a fine-grained description of the specific target.
Through such a construction process, we obtain panoptic segmentation masks with aligned descriptions for each instance-level target, as well as fine-grained segmentation masks with aligned descriptions for the specific target in each image.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.p2">
<p class="ltx_p" id="Sx4.SSx3.p2.1">In this manner, we annotate 10K SAM images, which are inherently diverse and exhibit multi-granularity.
The resulting dataset comprises 30K conversations and contains over 45M tokens, totaling more than 300K segmentation masks, each accompanied by a short semantic label and a detailed caption.
For more details about the pipeline and dataset, please refer to the <span class="ltx_text ltx_font_bold" id="Sx4.SSx3.p2.1.1">Appendix. B</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Experiments</h2>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Experimental Settings</h3>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p1.1.1">Datasets.</span>
To achieve all the capabilities of MGLMM, our training dataset is composed of six parts:
(1) semantic segmentation: including ADE20K <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib62" title="">2019</a>)</cite>, COCO-Stuff <cite class="ltx_cite ltx_citemacro_citep">(Caesar, Uijlings, and Ferrari <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib5" title="">2018</a>)</cite>, Maplilary Vistas <cite class="ltx_cite ltx_citemacro_citep">(Neuhold et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib36" title="">2017</a>)</cite>, PACO-LVIS <cite class="ltx_cite ltx_citemacro_citep">(Ramanathan et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib40" title="">2023</a>)</cite>, and PASCAL-Part <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib8" title="">2014</a>)</cite> ;
(2) referring segmentation: including RefCLEF <cite class="ltx_cite ltx_citemacro_citep">(Jing et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib23" title="">2021</a>)</cite> the RefCOCO series <cite class="ltx_cite ltx_citemacro_citep">(Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib57" title="">2016</a>)</cite>;
(3) image-level caption: including COCO Caption <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib7" title="">2015</a>)</cite>;
(4) visual question answering: including LLaVA-150k <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib32" title="">2024b</a>)</cite>
(5) grounded conversation generation including GranDf. Additionally, we also use approximately 4M captioning and referring segmentation data from Grounding-anything Dataset (GranD) <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Although GranD contains 11M images, only 4M are available because the authors have yet to publicize all the data.</span></span></span> dataset published by GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>, which is annotated automatically on SAM <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib25" title="">2023</a>)</cite> images.
(6) multi-granularity SegCap, including MGSCData, which we proposed.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p2">
<p class="ltx_p" id="Sx5.SSx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p2.1.1">Implementation details.</span>
In our experiments, we use Vicuna-7B as a structure for LLM except for some ablations.
We train our model on 16 Tesla A100 GPUs (80GB) for 30,000 iterations with a batch size of 16 per device.
Unless otherwise specified, the model is trained with a joint training setting and without additional task-specific fine-tuning.
Following the previous works, we apply the CE loss for modeling text generation, and the BCE and DICE loss to supervise high-quality mask prediction.
Further implementation details, particularly regarding LORA fine-tuning, the optimizer, hyperparameter settings, and training objectives, can be found in the <span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p2.1.2">Appendix. D</span>.</p>
</div>
<figure class="ltx_table" id="Sx5.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="Sx5.T3.1" style="width:433.6pt;height:155.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(41.4pt,-14.9pt) scale(1.23631160127466,1.23631160127466) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx5.T3.1.1">
<tr class="ltx_tr" id="Sx5.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx5.T3.1.1.1.1" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Sx5.T3.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="5" id="Sx5.T3.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Multi-Granularity SepCap</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="Sx5.T3.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">GCG</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.2">
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">M</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">C</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">AP50</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">mIoU</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T3.1.1.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">MR</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">M</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">C</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.8" style="padding-left:3.0pt;padding-right:3.0pt;">AP50</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.9" style="padding-left:3.0pt;padding-right:3.0pt;">mIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.2.10" style="padding-left:3.0pt;padding-right:3.0pt;">MR</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T3.1.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">Kosmos-2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T3.1.1.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">16.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">27.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.9" style="padding-left:3.0pt;padding-right:3.0pt;">17.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.10" style="padding-left:3.0pt;padding-right:3.0pt;">55.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.1.1.3.11" style="padding-left:3.0pt;padding-right:3.0pt;">28.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T3.1.1.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">LISA</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T3.1.1.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">13.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">33.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.9" style="padding-left:3.0pt;padding-right:3.0pt;">25.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.10" style="padding-left:3.0pt;padding-right:3.0pt;">62.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.4.11" style="padding-left:3.0pt;padding-right:3.0pt;">36.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T3.1.1.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">OMG-LLaVA</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T3.1.1.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">14.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">41.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.9" style="padding-left:3.0pt;padding-right:3.0pt;">29.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.10" style="padding-left:3.0pt;padding-right:3.0pt;">65.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.5.11" style="padding-left:3.0pt;padding-right:3.0pt;">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T3.1.1.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">GLaMM</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">16.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">8.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">5.4</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">47.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T3.1.1.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">18.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">16.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.8" style="padding-left:3.0pt;padding-right:3.0pt;">47.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.9" style="padding-left:3.0pt;padding-right:3.0pt;">30.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.6.10.1">66.3</span></td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.1.1.6.11" style="padding-left:3.0pt;padding-right:3.0pt;">41.8</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="Sx5.T3.1.1.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.2.1">17.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.3.1">11.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.4.1">7.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.5.1">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="Sx5.T3.1.1.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.6.1">23.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.7.1">16.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.8.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.9" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.9.1">31.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.10.1">66.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx5.T3.1.1.7.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T3.1.1.7.11.1">45.2</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison on MGSC and GCG. Following the evaluation protocol of GCG, we report the metrics including METEOR (M), CIDEr (C), AP50, mIoU, and Mask Recall (MR).

</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p3">
<p class="ltx_p" id="Sx5.SSx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p3.1.1">Comparisons with State-of-the-Arts.</span>
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx4.T2" title="Table 2 ‣ Mask Tree Building ‣ Data Annotation Pipeline ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare our MGLMM with other representative methods on various kinds of tasks and outperform all tasks.
Then, we evaluate the effectiveness of our MGLMM on the following six benchmarks.
Additionally, we will provide more discussion of the experimental results in the Appendix. E.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p4">
<p class="ltx_p" id="Sx5.SSx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p4.1.1">Multi-Granularity SegCap.</span>
The MGSC aims to evaluate the ability to seamlessly adjust the granularity of segmentation and captioning.
Following the same settings, we finetune the GLaMM and our MGLMM on the training set of MGSCData and evaluate them on the same metric.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T3" title="Table 3 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>, we outperform GLaMM on every metric, demonstrating the impressive capabilities of our MGLMM in multi-granularity SegCap.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p5">
<p class="ltx_p" id="Sx5.SSx1.p5.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p5.1.1">Grounded Conversation Generation (GCG).</span>
Following GLaMM, we finetune our model on the GranDf dataset.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T3" title="Table 3 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">3</span></a>, our MGLMM outperforms other approaches in terms of both image description and pixel understanding capabilities.
It is worth noting that, despite more training data utilized by GLaMM in the pre-training phase compared to MGLMM, the latter still surpasses the former, particularly in terms of the CIDEr and Mask Recall scores.</p>
</div>
<figure class="ltx_table" id="Sx5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx5.T4.1" style="width:368.6pt;height:127.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-156.2pt,53.7pt) scale(0.541276181697711,0.541276181697711) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx5.T4.1.1">
<tr class="ltx_tr" id="Sx5.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Sx5.T4.1.1.1.1" rowspan="2"><span class="ltx_text" id="Sx5.T4.1.1.1.1.1">Type</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx5.T4.1.1.1.2" rowspan="2"><span class="ltx_text" id="Sx5.T4.1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Sx5.T4.1.1.1.3">refCOCO</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Sx5.T4.1.1.1.4">refCOCO+</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="Sx5.T4.1.1.1.5">refCOCOg</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Sx5.T4.1.1.1.6">ReasonSeg</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.2">
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.1">val</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.2">testA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.2.3">testB</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.4">val</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.5">testA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.2.6">testB</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.7">val</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.2.8">test</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.9">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.2.10">gIoU</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.3.1" rowspan="3"><span class="ltx_text" id="Sx5.T4.1.1.3.1.1"><span class="ltx_text" id="Sx5.T4.1.1.3.1.1.1"></span> <span class="ltx_text" id="Sx5.T4.1.1.3.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="Sx5.T4.1.1.3.1.1.2.1">
<span class="ltx_tr" id="Sx5.T4.1.1.3.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T4.1.1.3.1.1.2.1.1.1">Segmentation</span></span>
<span class="ltx_tr" id="Sx5.T4.1.1.3.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T4.1.1.3.1.1.2.1.2.1">Specialist</span></span>
</span></span> <span class="ltx_text" id="Sx5.T4.1.1.3.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T4.1.1.3.2">LAVT <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib54" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.3">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.4">75.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.3.5">68.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.6">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.7">68.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.3.8">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.9">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.3.10">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.11">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.3.12">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.4.1">ReLA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib30" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.2">73.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.3">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.4.4">70.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.5">66.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.6">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.4.7">57.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.8">65.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.4.9">66.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.4.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.5.1">PolyFormer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib33" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.2">74.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.3">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.5.4">71.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.5">67.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.6">72.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.5.7">59.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.8">67.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.5.9">69.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.5.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="Sx5.T4.1.1.6.1" rowspan="8"><span class="ltx_text" id="Sx5.T4.1.1.6.1.1"><span class="ltx_text" id="Sx5.T4.1.1.6.1.1.1"></span> <span class="ltx_text" id="Sx5.T4.1.1.6.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="Sx5.T4.1.1.6.1.1.2.1">
<span class="ltx_tr" id="Sx5.T4.1.1.6.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T4.1.1.6.1.1.2.1.1.1">LMM-based</span></span>
<span class="ltx_tr" id="Sx5.T4.1.1.6.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T4.1.1.6.1.1.2.1.2.1">Models</span></span>
</span></span> <span class="ltx_text" id="Sx5.T4.1.1.6.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T4.1.1.6.2">LISA <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.3">74.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.4">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.6.5">72.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.6">65.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.7">70.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.6.8">58.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.9">67.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.6.10">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.11">46.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.6.12">34.1</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.7.1">PixelLM <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib42" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.2">73.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.3">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.7.4">68.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.5">66.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.6">71.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.7.7">58.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.8">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.7.9">70.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.7.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.8.1">GSVA <cite class="ltx_cite ltx_citemacro_citep">(Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.2">77.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.3">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.8.4">73.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.5">65.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.6">69.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.8.7">59.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.8">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.8.9">73.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.8.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.9.1">LaSagnA <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib51" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.2">76.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.3">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.9.4">73.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.5">66.4</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.6">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.9.7">60.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.8">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.9.9">71.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.10">47.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.9.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.10.1">OMG-LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib60" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.2">78.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.3">80.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.10.4">74.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.5">69.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.6">73.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.10.7">63.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.8">72.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.10.9">72.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.10.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T4.1.1.11.1">GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.2">79.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.3">83.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.11.4">76.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.5">72.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.6">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.11.7">64.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.8">74.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T4.1.1.11.9">74.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.10">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T4.1.1.11.11">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T4.1.1.12.1">MGLMM (Ours)†</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.2">80.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.3">83.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.12.4">76.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.5">73.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.6">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.12.7">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.8">76.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T4.1.1.12.9"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.12.9.1">77.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.10"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.12.10.1">51.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.1.12.11"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.12.11.1">48.6</span></td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="Sx5.T4.1.1.13.1">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.2"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.2.1">81.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.3"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.3.1">83.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T4.1.1.13.4"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.4.1">77.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.5"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.5.1">73.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.6"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.6.1">79.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T4.1.1.13.7"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.7.1">67.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.8"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.13.8.1">77.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T4.1.1.13.9">77.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.10">–</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.1.13.11">–</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance on referring and reasoning segmentation benchmarks. The table only shows the cIoU values for referring segmentation. MGLMM† indicates that the referring segmentation dataset is used only in the pre-training phase.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p6">
<p class="ltx_p" id="Sx5.SSx1.p6.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p6.1.1">Referring Segmentation.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T4" title="Table 4 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">4</span></a> compares our MGLMM with current state-of-the-art models on three representative datasets.
We achieve significant lead performances over recent works like GLaMM, and OMG-LLaVG on the refCOCO/+/g validation and test sets in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T4" title="Table 4 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">4</span></a>.
Notably, even without any fine-tuning on the referring segmentation dataset (MGLMM† in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T4" title="Table 4 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">4</span></a>), our approach still surpasses GLaMM on the validation split of all benchmarks.</p>
</div>
<figure class="ltx_table" id="Sx5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx5.T5.1" style="width:433.6pt;height:188pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.6pt,5.0pt) scale(0.94930029084592,0.94930029084592) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx5.T5.1.1">
<tr class="ltx_tr" id="Sx5.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx5.T5.1.1.1.1" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Sx5.T5.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Sx5.T5.1.1.1.2" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Sx5.T5.1.1.1.2.1"><span class="ltx_text" id="Sx5.T5.1.1.1.2.1.1"></span> <span class="ltx_text" id="Sx5.T5.1.1.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="Sx5.T5.1.1.1.2.1.2.1">
<span class="ltx_tr" id="Sx5.T5.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T5.1.1.1.2.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">zero</span></span>
<span class="ltx_tr" id="Sx5.T5.1.1.1.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T5.1.1.1.2.1.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">shot</span></span>
</span></span> <span class="ltx_text" id="Sx5.T5.1.1.1.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="Sx5.T5.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Generalized Referring Segmentation</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.2">
<td class="ltx_td ltx_align_center" colspan="2" id="Sx5.T5.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">val</td>
<td class="ltx_td ltx_align_center" colspan="2" id="Sx5.T5.1.1.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">testA</td>
<td class="ltx_td ltx_align_center" colspan="2" id="Sx5.T5.1.1.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">testB</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.3">
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">gIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">gIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">cIoU</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">gIoU</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T5.1.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">ReLA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib30" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T5.1.1.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.4.2.1">\usym</span>2717</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">62.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">63.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">70.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">59.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.4.8" style="padding-left:4.0pt;padding-right:4.0pt;">61.0</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T5.1.1.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">LISA†<cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T5.1.1.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.5.2.1">\usym</span>2717</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">38.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">32.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">52.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">48.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.7" style="padding-left:4.0pt;padding-right:4.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.5.8" style="padding-left:4.0pt;padding-right:4.0pt;">39.7</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T5.1.1.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">LISA <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T5.1.1.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.6.2.1">\usym</span>2717</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">61.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">61.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">69.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">70.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">60.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">61.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T5.1.1.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">GSVA†<cite class="ltx_cite ltx_citemacro_citep">(Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T5.1.1.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.7.2.1">\usym</span>2717</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">61.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">63.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">69.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">70.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.7" style="padding-left:4.0pt;padding-right:4.0pt;">60.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.7.8" style="padding-left:4.0pt;padding-right:4.0pt;">61.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T5.1.1.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">GSVA <cite class="ltx_cite ltx_citemacro_citep">(Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib53" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T5.1.1.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.8.2.1">\usym</span>2717</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">63.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">66.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">69.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">71.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">60.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.8.8" style="padding-left:4.0pt;padding-right:4.0pt;">62.2</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T5.1.1.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">LaSagnA <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib51" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T5.1.1.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.9.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">38.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">32.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">50.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">47.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">42.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T5.1.1.9.8" style="padding-left:4.0pt;padding-right:4.0pt;">38.9</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T5.1.1.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">PSALM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib61" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T5.1.1.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.10.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">42.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">43.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">52.4</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">54.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">50.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T5.1.1.10.8" style="padding-left:4.0pt;padding-right:4.0pt;">52.5</td>
</tr>
<tr class="ltx_tr" id="Sx5.T5.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="Sx5.T5.1.1.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T5.1.1.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_ERROR undefined" id="Sx5.T5.1.1.11.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.3.1">52.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.4.1">50.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.5.1">61.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.6.1">58.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.7.1">56.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T5.1.1.11.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Sx5.T5.1.1.11.8.1">54.1</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison on generalized referring-expression segmentation with cIoU and gIoU metrics.
LISA† and GSVA† exclusively use the gRefCOCO dataset during the pre-training phase, while MGLLM performs zero-shot learning.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p7">
<p class="ltx_p" id="Sx5.SSx1.p7.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p7.1.1">Generalized Referring Segmentation and Reasoning Segmentation.</span>
The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T5" title="Table 5 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">5</span></a>.
Compared with PSLAM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib61" title="">2024b</a>)</cite>, the state-of-the-art method in the zero-shot setting, our MGLMM accomplishes average boosts of 6.0% and 6.5% in terms of cIoU and gIoU, respectively. Notably, MGLMM even outperforms LISA† in all cases, which incorporate gRefCOCO during the pre-training phase. For reasoning segmentation, we utilize the validation set of ReasonSeg dataset <cite class="ltx_cite ltx_citemacro_citep">(Lai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib26" title="">2024</a>)</cite> as the benchmark.
From the results reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T4" title="Table 4 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">4</span></a>, we can observe that the reasoning proficiency of MGLMM surpasses that of other methods.</p>
</div>
<figure class="ltx_table" id="Sx5.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx5.T6.1" style="width:433.6pt;height:172.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.8pt,3.9pt) scale(0.956806227591152,0.956806227591152) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx5.T6.1.1">
<tr class="ltx_tr" id="Sx5.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx5.T6.1.1.1.1" rowspan="2"><span class="ltx_text" id="Sx5.T6.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="Sx5.T6.1.1.1.2">Flickr30k</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Sx5.T6.1.1.1.3">NoCap</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.2">
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.2.1">CIDEr</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.2.2">SPICE</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.2.3">CIDEr</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.2.4">SPICE</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T6.1.1.3.1">LEMON <cite class="ltx_cite ltx_citemacro_citep">(Hu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib21" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.3.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T6.1.1.3.3">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.3.4">106.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.3.5">14.1</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T6.1.1.4.1">CoCa <cite class="ltx_cite ltx_citemacro_citep">(Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib55" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.4.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.4.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.4.4">120.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.4.5">15.5</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T6.1.1.5.1">BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib27" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.5.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.5.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.5.4">121.6</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.5.5"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.5.5.1">15.8</span></td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T6.1.1.6.1">InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib14" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.6.2">82.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.6.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.6.4"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.6.4.1">123.1</span></td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.6.5">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T6.1.1.7.1">Kosmos-1 <cite class="ltx_cite ltx_citemacro_citep">(Huang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib22" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.7.2">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T6.1.1.7.3">14.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.7.4">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.1.7.5">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T6.1.1.8.1">Kosmos-2 <cite class="ltx_cite ltx_citemacro_citep">(Peng et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib37" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.8.2">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.8.3">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.8.4">–</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.8.5">–</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T6.1.1.9.1">GLaMM <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib41" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.9.2">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T6.1.1.9.3">18.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.9.4">106.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.1.9.5"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.9.5.1">15.8</span></td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="Sx5.T6.1.1.10.1">MGLMM (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.1.10.2"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.10.2.1">104.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T6.1.1.10.3"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.10.3.1">22.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.1.10.4">112.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.1.10.5">15.2</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance comparison on image-level captioning.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="Sx5.SSx1.p8">
<p class="ltx_p" id="Sx5.SSx1.p8.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p8.1.1">Image-level Captioning.</span>
To investigate this capability, we finetune MGLMM on the Flickr-30K <cite class="ltx_cite ltx_citemacro_citep">(Plummer et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib38" title="">2015</a>)</cite> and evaluate Flickr-30K and NoCap <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#bib.bib1" title="">2019</a>)</cite>, where the latter can be considered as a <span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p8.1.2">zero-shot</span> scene. As reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T6" title="Table 6 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">6</span></a>,
MGLMM is superior to the counterpart model GLaMM on several metrics.</p>
</div>
<figure class="ltx_table" id="Sx5.T7">
<div class="ltx_inline-block ltx_transformed_outer" id="Sx5.T7.1" style="width:433.6pt;height:142.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(25.0pt,-8.2pt) scale(1.13052568370963,1.13052568370963) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx5.T7.1.1">
<tr class="ltx_tr" id="Sx5.T7.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="Sx5.T7.1.1.1.1" rowspan="2"><span class="ltx_text" id="Sx5.T7.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx5.T7.1.1.1.2" rowspan="2"><span class="ltx_text" id="Sx5.T7.1.1.1.2.1"><span class="ltx_text" id="Sx5.T7.1.1.1.2.1.1"></span> <span class="ltx_text" id="Sx5.T7.1.1.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="Sx5.T7.1.1.1.2.1.2.1">
<span class="ltx_tr" id="Sx5.T7.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T7.1.1.1.2.1.2.1.1.1">+ USCDF</span></span>
</span></span> <span class="ltx_text" id="Sx5.T7.1.1.1.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Sx5.T7.1.1.1.3" rowspan="2"><span class="ltx_text" id="Sx5.T7.1.1.1.3.1"><span class="ltx_text" id="Sx5.T7.1.1.1.3.1.1"></span> <span class="ltx_text" id="Sx5.T7.1.1.1.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="Sx5.T7.1.1.1.3.1.2.1">
<span class="ltx_tr" id="Sx5.T7.1.1.1.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T7.1.1.1.3.1.2.1.1.1">+ GranD</span></span>
<span class="ltx_tr" id="Sx5.T7.1.1.1.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="Sx5.T7.1.1.1.3.1.2.1.2.1">Dataset</span></span>
</span></span> <span class="ltx_text" id="Sx5.T7.1.1.1.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Sx5.T7.1.1.1.4">refCOCO+</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Sx5.T7.1.1.1.5">GCG</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.2">
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.2.1">val</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.2.2">testA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.2.3">testB</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.2.4">C</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.2.5">mIoU</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx5.T7.1.1.3.1">MGLMM-7B</td>
<td class="ltx_td ltx_border_t" id="Sx5.T7.1.1.3.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="Sx5.T7.1.1.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.1.3.4">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.1.3.5">74.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx5.T7.1.1.3.6">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.1.3.7">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.1.3.8">65.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T7.1.1.4.1">MGLMM-7B</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.4.2">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.4.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_border_r" id="Sx5.T7.1.1.4.3"></td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.4.4">69.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.4.5">76.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.4.6">62.5</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.4.7">46.3</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.4.8">65.6</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T7.1.1.5.1">MGLMM-7B</td>
<td class="ltx_td" id="Sx5.T7.1.1.5.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.5.3">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.5.3.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.5.4">71.4</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.5.5">76.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.5.6">64.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.5.7">48.0</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.5.8">66.2</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Sx5.T7.1.1.6.1">MGLMM-7B</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.6.2">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.6.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.6.3">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.6.3.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.6.4">73.2</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.6.5">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx5.T7.1.1.6.6">66.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.6.7">50.1</td>
<td class="ltx_td ltx_align_center" id="Sx5.T7.1.1.6.8">66.3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="Sx5.T7.1.1.7.1">MGLMM-13B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.1.7.2">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.7.2.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T7.1.1.7.3">
<span class="ltx_ERROR undefined" id="Sx5.T7.1.1.7.3.1">\usym</span>2713</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.1.7.4"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.7.4.1">73.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.1.7.5"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.7.5.1">79.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Sx5.T7.1.1.7.6"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.7.6.1">68.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.1.7.7"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.7.7.1">50.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.1.7.8"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.7.8.1">66.4</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation study results. For refCOCO+, we utilize cIoU as the metric. ‘C’ denotes the CIDEr score. We implement MGLMM-13B using Llama2-13B as the structure for LLM.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Ablation Studies</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1">To perform a thorough ablation study, we assess different variants of MGLMM using two representative benchmarks, <span class="ltx_text ltx_font_italic" id="Sx5.SSx2.p1.1.1">i.e.</span>, referring segmentation and GCG, which can demonstrate the models’ ability to understand pixel-level details and provide image descriptions. For more details, please refer to <span class="ltx_text ltx_font_bold" id="Sx5.SSx2.p1.1.2">Appendix. E</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx5.SSx2.p2">
<p class="ltx_p" id="Sx5.SSx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.p2.1.1">Effectiveness of USCDF.</span>
Compared to the 1st variant in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T7" title="Table 7 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">7</span></a>, MGLMM using USCDF obtains an improvement of more than 2% on challenging regCOCO+ benchmark.
The performance difference between the 3rd and 4th variants is significant, as GranD is four times larger than the other pre-training data, which further amplifies the gains of USCDF.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx5.SSx2.p3">
<p class="ltx_p" id="Sx5.SSx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.p3.1.1">Influence of GranD dataset.</span>
To investigate the impact of the extra GranD dataset on MGLMM, we experiment without 4M GranD samples.
Comparing the 2nd and 4th variants in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13407v1#Sx5.T7" title="Table 7 ‣ Experimental Settings ‣ Experiments ‣ Instruction-guided Multi-Granularity Segmentation and Captioning with Large Multimodal Model"><span class="ltx_text ltx_ref_tag">7</span></a>, we can find that the GranD dataset contributes a gain.
Despite not utilizing GranD, our MGLMM remains superior to models such as OMG-LLAVA in most cases, ranking second only to GLaMM, which employed over ten times training data during the pre-training phase.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">We propose MGLMM, the first model capable of seamlessly adjusting the granularity of segmentation and captioning following user instructions.
Realizing the lack of multi-granularity of segmentation and captioning dataset and benchmark, we introduce a novel benchmark MGSCData to train and evaluate the ability of multi-granularity segmentation and captioning for LMMs, which comprises over 30K high-quality image-question pairs.
To facilitate aligning object concepts with visual features during various segmentation tasks, we propose a unified data format.
Our model excels at tackling more than eight downstream
tasks and outperforms various benchmarks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2019)</span>
<span class="ltx_bibblock">
Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019.

</span>
<span class="ltx_bibblock">Nocaps: Novel object captioning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 8948–8957.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Alayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; et al. 2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</em>, 35: 23716–23736.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C. L.; and Parikh, D. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2015 IEEE International Conference on Computer Vision (ICCV)</em>, 2425–2433.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin, J.; Zhou, C.; and Zhou, J. 2023.

</span>
<span class="ltx_bibblock">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.

</span>
<span class="ltx_bibblock">arXiv:2308.12966.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caesar, Uijlings, and Ferrari (2018)</span>
<span class="ltx_bibblock">
Caesar, H.; Uijlings, J.; and Ferrari, V. 2018.

</span>
<span class="ltx_bibblock">Coco-stuff: Thing and stuff classes in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 1209–1218.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chen, K.; Zhang, Z.; Zeng, W.; Zhang, R.; Zhu, F.; and Zhao, R. 2023.

</span>
<span class="ltx_bibblock">Shikra: Unleashing multimodal llm’s referential dialogue magic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2306.15195</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Chen, X.; Fang, H.; Lin, T.-Y.; Vedantam, R.; Gupta, S.; Dollár, P.; and Zitnick, C. L. 2015.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:1504.00325</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2014)</span>
<span class="ltx_bibblock">
Chen, X.; Mottaghi, R.; Liu, X.; Fidler, S.; Urtasun, R.; and Yuille, A. 2014.

</span>
<span class="ltx_bibblock">Detect what you can: Detecting and representing objects using holistic models and body parts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 1971–1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clancey (1979)</span>
<span class="ltx_bibblock">
Clancey, W. J. 1979.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Transfer of Rule-Based Expertise through a Tutorial Dialogue</em>.

</span>
<span class="ltx_bibblock">Ph.D. diss., Dept. of Computer Science, Stanford Univ., Stanford, Calif.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clancey (1983)</span>
<span class="ltx_bibblock">
Clancey, W. J. 1983.

</span>
<span class="ltx_bibblock">Communication, Simulation, and Intelligent Agents: Implications of Personal Intelligent Machines for Medical Education.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI-83)</em>, 556–560. Menlo Park, Calif: IJCAI Organization.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clancey (1984)</span>
<span class="ltx_bibblock">
Clancey, W. J. 1984.

</span>
<span class="ltx_bibblock">Classification Problem Solving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Fourth National Conference on Artificial Intelligence</em>, 45–54. Menlo Park, Calif.: AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clancey (2021)</span>
<span class="ltx_bibblock">
Clancey, W. J. 2021.

</span>
<span class="ltx_bibblock">The Engineering of Qualitative Models.

</span>
<span class="ltx_bibblock">Forthcoming.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.; Li, B.; Fung, P.; and Hoi, S. 2023.

</span>
<span class="ltx_bibblock">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.

</span>
<span class="ltx_bibblock">arXiv:2305.06500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock">arXiv:1810.04805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2021)</span>
<span class="ltx_bibblock">
Ding, H.; Liu, C.; Wang, S.; and Jiang, X. 2021.

</span>
<span class="ltx_bibblock">Vision-language transformer and query generation for referring segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 16321–16330.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engelmore and Morgan (1986)</span>
<span class="ltx_bibblock">
Engelmore, R.; and Morgan, A., eds. 1986.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Blackboard Systems</em>.

</span>
<span class="ltx_bibblock">Reading, Mass.: Addison-Wesley.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasling, Clancey, and Rennels (1984)</span>
<span class="ltx_bibblock">
Hasling, D. W.; Clancey, W. J.; and Rennels, G. 1984.

</span>
<span class="ltx_bibblock">Strategic explanations for a diagnostic consultation system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Journal of Man-Machine Studies</em>, 20(1): 3–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasling et al. (1983)</span>
<span class="ltx_bibblock">
Hasling, D. W.; Clancey, W. J.; Rennels, G. R.; and Test, T. 1983.

</span>
<span class="ltx_bibblock">Strategic Explanations in Consultation—Duplicate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">The International Journal of Man-Machine Studies</em>, 20(1): 3–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2022.

</span>
<span class="ltx_bibblock">Scaling up vision-language pre-training for image captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 17980–17989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Huang, S.; Dong, L.; Wang, W.; Hao, Y.; Singhal, S.; Ma, S.; Lv, T.; Cui, L.; Mohammed, O. K.; Patra, B.; et al. 2024.

</span>
<span class="ltx_bibblock">Language is not all you need: Aligning perception with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jing et al. (2021)</span>
<span class="ltx_bibblock">
Jing, Y.; Kong, T.; Wang, W.; Wang, L.; Li, L.; and Tan, T. 2021.

</span>
<span class="ltx_bibblock">Locate then segment: A strong pipeline for referring image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9858–9867.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and Fei-Fei (2015)</span>
<span class="ltx_bibblock">
Karpathy, A.; and Fei-Fei, L. 2015.

</span>
<span class="ltx_bibblock">Deep Visual-Semantic Alignments for Generating Image Descriptions.

</span>
<span class="ltx_bibblock">arXiv:1412.2306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2023)</span>
<span class="ltx_bibblock">
Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-Y.; et al. 2023.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 4015–4026.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2024)</span>
<span class="ltx_bibblock">
Lai, X.; Tian, Z.; Chen, Y.; Li, Y.; Yuan, Y.; Liu, S.; and Jia, J. 2024.

</span>
<span class="ltx_bibblock">Lisa: Reasoning segmentation via large language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9579–9589.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2301.12597.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">International conference on machine learning</em>, 12888–12900. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Li, J.; Vo, D. M.; Sugimoto, A.; and Nakayama, H. 2024.

</span>
<span class="ltx_bibblock">EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 13733–13742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Liu, C.; Ding, H.; Jiang, X.; and Liu, C. 2023a.

</span>
<span class="ltx_bibblock">Gres: Generalized referring expression segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 23592–23601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024a.

</span>
<span class="ltx_bibblock">Improved Baselines with Visual Instruction Tuning.

</span>
<span class="ltx_bibblock">arXiv:2310.03744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2024b.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in neural information processing systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Liu, J.; Ding, H.; Cai, Z.; Zhang, Y.; Satzoda, R. K.; Mahadevan, V.; and Manmatha, R. 2023b.

</span>
<span class="ltx_bibblock">Polyformer: Referring image segmentation as sequential polygon generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 18653–18663.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2020)</span>
<span class="ltx_bibblock">
Luo, G.; Zhou, Y.; Sun, X.; Cao, L.; Wu, C.; Deng, C.; and Ji, R. 2020.

</span>
<span class="ltx_bibblock">Multi-task collaborative network for joint referring expression comprehension and segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</em>, 10034–10043.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NASA (2015)</span>
<span class="ltx_bibblock">
NASA. 2015.

</span>
<span class="ltx_bibblock">Pluto: The ’Other’ Red Planet.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://www.nasa.gov/nh/pluto-the-other-red-planet</span>.

</span>
<span class="ltx_bibblock">Accessed: 2018-12-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neuhold et al. (2017)</span>
<span class="ltx_bibblock">
Neuhold, G.; Ollmann, T.; Rota Bulo, S.; and Kontschieder, P. 2017.

</span>
<span class="ltx_bibblock">The mapillary vistas dataset for semantic understanding of street scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE international conference on computer vision</em>, 4990–4999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma, S.; and Wei, F. 2023.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding multimodal large language models to the world.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2306.14824</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2015)</span>
<span class="ltx_bibblock">
Plummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo, J. C.; Hockenmaier, J.; and Lazebnik, S. 2015.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2641–2649.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International conference on machine learning</em>, 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramanathan et al. (2023)</span>
<span class="ltx_bibblock">
Ramanathan, V.; Kalia, A.; Petrovic, V.; Wen, Y.; Zheng, B.; Guo, B.; Wang, R.; Marquez, A.; Kovvuri, R.; Kadian, A.; et al. 2023.

</span>
<span class="ltx_bibblock">Paco: Parts and attributes of common objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 7141–7151.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasheed et al. (2024)</span>
<span class="ltx_bibblock">
Rasheed, H.; Maaz, M.; Shaji, S.; Shaker, A.; Khan, S.; Cholakkal, H.; Anwer, R. M.; Xing, E.; Yang, M.-H.; and Khan, F. S. 2024.

</span>
<span class="ltx_bibblock">Glamm: Pixel grounding large multimodal model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 13009–13018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2024)</span>
<span class="ltx_bibblock">
Ren, Z.; Huang, Z.; Wei, Y.; Zhao, Y.; Fu, D.; Feng, J.; and Jin, X. 2024.

</span>
<span class="ltx_bibblock">Pixellm: Pixel reasoning with large multimodal model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 26374–26383.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rice (1986)</span>
<span class="ltx_bibblock">
Rice, J. 1986.

</span>
<span class="ltx_bibblock">Poligon: A System for Parallel Problem Solving.

</span>
<span class="ltx_bibblock">Technical Report KSL-86-19, Dept. of Computer Science, Stanford Univ.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson (1980a)</span>
<span class="ltx_bibblock">
Robinson, A. L. 1980a.

</span>
<span class="ltx_bibblock">New Ways to Make Microcircuits Smaller.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Science</em>, 208(4447): 1019–1022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson (1980b)</span>
<span class="ltx_bibblock">
Robinson, A. L. 1980b.

</span>
<span class="ltx_bibblock">New Ways to Make Microcircuits Smaller—Duplicate Entry.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Science</em>, 208: 1019–1026.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock">arXiv:1706.03762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vennerød, Kjærran, and Bugge (2021)</span>
<span class="ltx_bibblock">
Vennerød, C. B.; Kjærran, A.; and Bugge, E. S. 2021.

</span>
<span class="ltx_bibblock">Long Short-term Memory RNN.

</span>
<span class="ltx_bibblock">arXiv:2105.06756.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, Z.; Lu, Y.; Li, Q.; Tao, X.; Guo, Y.; Gong, M.; and Liu, T. 2022.

</span>
<span class="ltx_bibblock">Cris: Clip-driven referring image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 11686–11695.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021.

</span>
<span class="ltx_bibblock">Simvlm: Simple visual language model pretraining with weak supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2108.10904</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Wei, C.; Tan, H.; Zhong, Y.; Yang, Y.; and Ma, L. 2024.

</span>
<span class="ltx_bibblock">LaSagnA: Language-based Segmentation Assistant for Complex Queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2404.08506</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Wu, S.; Jin, S.; Zhang, W.; Xu, L.; Liu, W.; Li, W.; and Loy, C. C. 2024.

</span>
<span class="ltx_bibblock">F-LMM: Grounding Frozen Large Multimodal Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2406.05821</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2024)</span>
<span class="ltx_bibblock">
Xia, Z.; Han, D.; Han, Y.; Pan, X.; Song, S.; and Huang, G. 2024.

</span>
<span class="ltx_bibblock">Gsva: Generalized segmentation via multimodal large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 3858–3869.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Yang, Z.; Wang, J.; Tang, Y.; Chen, K.; Zhao, H.; and Torr, P. H. 2022.

</span>
<span class="ltx_bibblock">Lavt: Language-aware vision transformer for referring image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 18155–18165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2022)</span>
<span class="ltx_bibblock">
Yu, J.; Wang, Z.; Vasudevan, V.; Yeung, L.; Seyedhosseini, M.; and Wu, Y. 2022.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2205.01917</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2018)</span>
<span class="ltx_bibblock">
Yu, L.; Lin, Z.; Shen, X.; Yang, J.; Lu, X.; Bansal, M.; and Berg, T. L. 2018.

</span>
<span class="ltx_bibblock">Mattnet: Modular attention network for referring expression comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 1307–1315.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2016)</span>
<span class="ltx_bibblock">
Yu, L.; Poirson, P.; Yang, S.; Berg, A. C.; and Berg, T. L. 2016.

</span>
<span class="ltx_bibblock">Modeling context in referring expressions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</em>, 69–85. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2024)</span>
<span class="ltx_bibblock">
Yuan, Y.; Li, W.; Liu, J.; Tang, D.; Luo, X.; Qin, C.; Zhang, L.; and Zhu, J. 2024.

</span>
<span class="ltx_bibblock">Osprey: Pixel understanding with visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 28202–28211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 5579–5588.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Zhang, T.; Li, X.; Fei, H.; Yuan, H.; Wu, S.; Ji, S.; Loy, C. C.; and Yan, S. 2024a.

</span>
<span class="ltx_bibblock">Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2406.19389</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Zhang, Z.; Ma, Y.; Zhang, E.; and Bai, X. 2024b.

</span>
<span class="ltx_bibblock">PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2403.14598</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2019)</span>
<span class="ltx_bibblock">
Zhou, B.; Zhao, H.; Puig, X.; Xiao, T.; Fidler, S.; Barriuso, A.; and Torralba, A. 2019.

</span>
<span class="ltx_bibblock">Semantic understanding of scenes through the ade20k dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">International Journal of Computer Vision</em>, 127: 302–321.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023.

</span>
<span class="ltx_bibblock">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2304.10592.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 11:06:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
