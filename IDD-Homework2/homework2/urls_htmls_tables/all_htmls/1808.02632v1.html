<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1808.02632] Question-Guided Hybrid Convolution for Visual Question Answering</title><meta property="og:description" content="In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Question-Guided Hybrid Convolution for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Question-Guided Hybrid Convolution for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1808.02632">

<!--Generated on Fri Mar 15 19:39:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="VQA Dynamic Parameter Prediction Group Convolution">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{penggao,hsli,sli,plu,ykli,xgwang}@ee.cuhk.edu.hk</span></span></span> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>School of Information Systems, Singapore Management Univeristy
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>chhoi@smu.edu.sg</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Question-Guided Hybrid Convolution for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Gao
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pan Lu
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongsheng Li
</span><span class="ltx_author_notes">corresponding author11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuang Li
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yikang Li
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven C.H. Hoi
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaogang Wang
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.
To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting.
The hybrid convolution can generate discriminative multi-modal features with fewer parameters.
The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Experiments on VQA datasets validate the effectiveness of QGHC.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>VQA Dynamic Parameter Prediction Group Convolution
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/1808.02632/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="176" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of using multiple Question-guided Hybrid Convolution modules for VQA. Question-guided kernels are predicted by the input question and convoluted with visual features. Visualization of the question-guided convolution activations show they gradually focus on the regions corresponding to the correct answer.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Convolution Neural Networks (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Recurrent Neural Networks (RNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have shown great success in vision and language tasks.
Recently, CNN and RNN are jointly trained for learning feature representations for multi-modal tasks, including image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, text-to-image retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>,
and Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Among the vision-language tasks, VQA is one of the most challenging problems. Instead of embedding images and their textual descriptions into the same feature subspace as in the text-image matching problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, VQA requires algorithms to answer natural language questions about the visual contents. The methods are thus designed to understand both the questions and the image contents to reason the underlying truth.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To infer the answer based on the input image and question, it is important to fuse the information from both modalities to create joint representations. Answers could be predicted by learning classifiers on the joint features. Early VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> fuse textual and visual information by feature concatenation. State-of-the-art feature fusion methods, such as Multimodal Compact Bilinear pooling (MCB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, utilize bilinear pooling to learn multi-model features.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, the above type of methods have main limitations. The
multi-modal features are fused in the latter model stage and the spatial information from visual features gets lost before feature fusion.
The visual features are usually obtained by averaging the output of the last pooling layer and represented as 1-d vectors. But such operation abandons the spatial information of input images.
In addition, the textual and visual relationship is modeled only on the topmost layers and misses details from the low-level and mid-level layers.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.2" class="ltx_p">To solve these problems, we propose a feature fusion scheme that generates multi-modal features by applying question-guided convolutions on the visual features (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The mid-level visual features and language features are first learned independently using CNN and RNN. The visual features are designed to keep the spatial information. And then a series of kernels are generated based on the language features to convolve with the visual features.
Our model tightly couples the multi-modal features in an early stage to better capture the spatial information before feature fusion.
One problem induced by the question-guided kernels is that the large number of parameters make it hard to train the model. Directly predicting “full” convolutional filters requires estimating thousands of parameters (<span id="S1.p4.2.1" class="ltx_text ltx_font_italic">e.g.</span> <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S1.p4.1.m1.1a"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><cn type="integer" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">256</annotation></semantics></math> number of <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S1.p4.2.m2.1a"><mrow id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml"><mn id="S1.p4.2.m2.1.1.2" xref="S1.p4.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S1.p4.2.m2.1.1.1" xref="S1.p4.2.m2.1.1.1.cmml">×</mo><mn id="S1.p4.2.m2.1.1.3" xref="S1.p4.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"><times id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1.1"></times><cn type="integer" id="S1.p4.2.m2.1.1.2.cmml" xref="S1.p4.2.m2.1.1.2">3</cn><cn type="integer" id="S1.p4.2.m2.1.1.3.cmml" xref="S1.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">3\times 3</annotation></semantics></math> filters convolve with the 256-channel input feature map). This is memory-inefficient and time-consuming, and does not result in satisfactory performances (as shown in our experiments).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Motivated by the group convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we decompose large convolution kernels into group kernels, each of which works on a small number of input feature maps. In addition, only a portion of such group convolution kernels (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">question-dependent kernels</em>) are predicted by RNN and the remaining kernels (<em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">question-independent kernels</em>) are freely learned via back-propagation. Both question-dependent and question-independent
kernels are shown to be important, and we name the proposed operation as <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">Question-guided Hybrid Convolution (QGHC)</em>. The visual and language features are deeply fused to generate discriminative multi-modal features. The spatial relations between the input image and question could be well captured by the question-guided convolution. Our experiments on VQA datasets validate the effectiveness of our approach and show advantages of the proposed feature fusion over the state-of-the-arts.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized in threefold. 1) We propose a novel multi-modal feature fusion method based on question-guided convolution kernels. The relative visual regions have high response to the input question and spatial information could be well captured by encoding such connection in the QGHC model. The QGHC explores deep multi-modal relationships which benefits the visual question reasoning.
2) To achieve memory efficiency and robust performance in the question-guided convolution, we propose the group convolution to learn kernel parameters. The question-dependent kernels model the relationship of visual and textual information while the question-independent kernels reduce parameter size and alleviate over-fitting.
3) Extensive experiments and ablation studies on the public datasets show the effectiveness of the proposed QGHC and each individual component. Our approach outperforms the state-of-the-art methods using much fewer parameters.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Bilinear pooling for VQA.</span> Solving the VQA problem requires the algorithms to understand the relation between images and questions. It is important to obtain discriminative multi-modal features for accurate answer prediction. Early methods utilize feature concatenation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> for multi-modal feature fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Recently, bilinear pooling methods are introduced for VQA to capture high-level interactions between visual and textual features. Multimodal Compact Bilinear Pooling (MCB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> projects the language and visual features into a higher dimensional space and convolves them in the Fast Fourier Transform space. In Multimodal Low-rank Bilinear (MLB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the weighting tensor for bilinear pooling is approximated by three weight matrices, which enforces the rank of the weighting tensor to be low-rank. The multi-modal features are obtained as the Hadamard product of the linear-projected visual and language features. Ben-younes <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> propose the Multimodal Tucker Fusion (MUTAN), which unifies MCB and MLB into the same framework . The weights are decomposed according to the Tucker decomposition. MUTAN achieves better performance than MLB and MCB with fewer parameters.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Attention mechanisms in language and VQA tasks.</span> The attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> are originally proposed for solving language-related tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Xu <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> introduce an attention mechanism for image captioning, which shows that the attention maps could be adaptively generated for predicting captioning words. Based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Yang <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> propose to stack multiple attention layers so that each layer can focus on different regions adaptively. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a co-attention mechanism is proposed. The model generates question attention and spatial attention masks so that salient words and regions could be jointly selected for more effective feature fusion.
Similarly, Lu <span id="S2.p2.1.4" class="ltx_text ltx_font_italic">et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> employ a co-attention mechanism to simultaneously learn free-form and detection-based image regions related to the input question.
In MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, MLB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, attention mechanisms are adopted to partially recover the spatial information from the input image. Question-guided attention methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> are proposed to generate attention maps from the question.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Dynamic Network.</span> Network parameters could be dynamically predicted across different modalities.
Our approach is mostly related to methods in this direction. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, language are used to predict parameters of a fully-connected (FC) layer for learning visual features. However, the predicted fully-connected layer cannot capture spatial information of the image. To avoid introducing too many parameters, they predict only a small portion of parameters using a hashing function. However, this strategy introduces redundancy because the FC parameters only contain a small amount of training parameters. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, language is used to modulate the mean and variance parameters of the Batch Normalization layers in the visual CNN. However, learning the interactions between two modalities by predicting the BN parameters has limited learning capacity.
We conduct comparisons with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Our proposed method shows favorable performance. We notice that <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> use language-guided convolution for object tracking. However, they predict all the parameters which is difficult to train.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Group convolution in deep neural networks.</span>
Recent research found that the combination of depth-wise convolution and channel shuffle with group convolution could reduce the number of parameters in CNN without hindering the final performance. Motivated by Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, ResNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and ShuffleNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, we decompose the visual CNN kernels into several groups. By shuffling parameters among different groups, our model can reduce the number of predicted parameters and improve the answering accuracy simultaneously. Note that for existing CNN methods with group convolution, the convolutional parameters are solely learned via back-propagation. In contrast, our QGHC consists of question-dependent kernels that are predicted based on language features and question-independent kernels that are freely updated.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Visual Question Answering with Question-guided Hybrid Convolution</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">ImageQA systems take an image and a question as inputs and output the predicted answer for the question. ImageQA algorithms mostly rely on deep learning models and design effective approaches to fuse the multi-modal features for answering questions. Instead of fusing the textual and visual information in high level layers, such as feature concatenation in the last layer, we propose a novel multi-modal feature fusion method, named Question-guided Hybrid Convolution (QGHC). Our approach couples the textual-visual features in early layers for better capturing textual-visual relationships. It learns question-guided convolution kernels and reserves the visual spatial information before feature fusion, and thus achieves accurate results.
The overview of our method is illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The network predicts convolution kernels based on the question features, and then convolve them with visual feature maps. We stack multiple question-guided hybrid convolution modules, an average pooling layer, and a classifier layer together. The output of the language-guided convolution is the fused textual-visual features maps which used for answering questions. To improve the memory efficiency and experimental accuracy, we utilize the group convolution to predict a portion of convolution kernels based on the question features.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Most state-of-the-art VQA methods rely on deep neural networks for learning discriminative features of the input image <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I</annotation></semantics></math> and question <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">q</annotation></semantics></math>. Usually, Convolutional Neural Networks (CNN) are adopted for learning visual features, while Recurrent Neural Networks (RNN) (<span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_italic">e.g.</span>, Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU)) encode the input question, <span id="S3.SS1.p1.2.2" class="ltx_text ltx_font_italic">i.e.</span>,</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle f_{v}" display="inline"><semantics id="S3.E1.m1.1a"><msub id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">f</mi><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑓</ci><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle f_{v}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.2" class="ltx_Math" alttext="\displaystyle=\textnormal{CNN}(I;\theta_{v})," display="inline"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.3" xref="S3.E1.m2.2.2.1.1.3.cmml"></mi><mo id="S3.E1.m2.2.2.1.1.2" xref="S3.E1.m2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m2.2.2.1.1.1" xref="S3.E1.m2.2.2.1.1.1.cmml"><mtext id="S3.E1.m2.2.2.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.3a.cmml">CNN</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">I</mi><mo id="S3.E1.m2.2.2.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.2.cmml">;</mo><msub id="S3.E1.m2.2.2.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.cmml">v</mi></msub><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.4" xref="S3.E1.m2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1"><eq id="S3.E1.m2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E1.m2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.3">absent</csymbol><apply id="S3.E1.m2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1"><times id="S3.E1.m2.2.2.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.2"></times><ci id="S3.E1.m2.2.2.1.1.1.3a.cmml" xref="S3.E1.m2.2.2.1.1.1.3"><mtext id="S3.E1.m2.2.2.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.3">CNN</mtext></ci><list id="S3.E1.m2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1"><ci id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">𝐼</ci><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3">𝑣</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle=\textnormal{CNN}(I;\theta_{v}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle f_{q}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">f</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑓</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle f_{q}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.2" class="ltx_Math" alttext="\displaystyle=\textnormal{RNN}(q;\theta_{q})," display="inline"><semantics id="S3.E2.m2.2a"><mrow id="S3.E2.m2.2.2.1" xref="S3.E2.m2.2.2.1.1.cmml"><mrow id="S3.E2.m2.2.2.1.1" xref="S3.E2.m2.2.2.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.3" xref="S3.E2.m2.2.2.1.1.3.cmml"></mi><mo id="S3.E2.m2.2.2.1.1.2" xref="S3.E2.m2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.2.2.1.1.1" xref="S3.E2.m2.2.2.1.1.1.cmml"><mtext id="S3.E2.m2.2.2.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.3a.cmml">RNN</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m2.2.2.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.2.2.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">q</mi><mo id="S3.E2.m2.2.2.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.2.cmml">;</mo><msub id="S3.E2.m2.2.2.1.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S3.E2.m2.2.2.1.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.4" xref="S3.E2.m2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m2.2.2.1.2" xref="S3.E2.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.2b"><apply id="S3.E2.m2.2.2.1.1.cmml" xref="S3.E2.m2.2.2.1"><eq id="S3.E2.m2.2.2.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.2.2.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.3">absent</csymbol><apply id="S3.E2.m2.2.2.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1"><times id="S3.E2.m2.2.2.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.2"></times><ci id="S3.E2.m2.2.2.1.1.1.3a.cmml" xref="S3.E2.m2.2.2.1.1.1.3"><mtext id="S3.E2.m2.2.2.1.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.1.3">RNN</mtext></ci><list id="S3.E2.m2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1"><ci id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">𝑞</ci><apply id="S3.E2.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E2.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.3">𝑞</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.2c">\displaystyle=\textnormal{RNN}(q;\theta_{q}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.4" class="ltx_p">where <math id="S3.SS1.p1.3.m1.1" class="ltx_Math" alttext="f_{v}" display="inline"><semantics id="S3.SS1.p1.3.m1.1a"><msub id="S3.SS1.p1.3.m1.1.1" xref="S3.SS1.p1.3.m1.1.1.cmml"><mi id="S3.SS1.p1.3.m1.1.1.2" xref="S3.SS1.p1.3.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.3.m1.1.1.3" xref="S3.SS1.p1.3.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m1.1b"><apply id="S3.SS1.p1.3.m1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m1.1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m1.1.1.2.cmml" xref="S3.SS1.p1.3.m1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.3.m1.1.1.3.cmml" xref="S3.SS1.p1.3.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m1.1c">f_{v}</annotation></semantics></math> and <math id="S3.SS1.p1.4.m2.1" class="ltx_Math" alttext="f_{q}" display="inline"><semantics id="S3.SS1.p1.4.m2.1a"><msub id="S3.SS1.p1.4.m2.1.1" xref="S3.SS1.p1.4.m2.1.1.cmml"><mi id="S3.SS1.p1.4.m2.1.1.2" xref="S3.SS1.p1.4.m2.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.4.m2.1.1.3" xref="S3.SS1.p1.4.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m2.1b"><apply id="S3.SS1.p1.4.m2.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m2.1.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m2.1.1.2.cmml" xref="S3.SS1.p1.4.m2.1.1.2">𝑓</ci><ci id="S3.SS1.p1.4.m2.1.1.3.cmml" xref="S3.SS1.p1.4.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m2.1c">f_{q}</annotation></semantics></math> represent visual features and question features respectively.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Conventional ImageQA systems focus on designing robust feature fusion functions to generate multi-modal image-question features for answer prediction.
Most state-of-the-art feature fusion methods fuse 1-d visual and language feature vectors in a symmetric way to generate the multi-modal representations. The 1-d visual features are usually generated by the deep neural networks (<span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, GoogleNet and ResNet) with a global average pooling layer. Such visual features <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="f_{v}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑓</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">f_{v}</annotation></semantics></math> and the later fused textual-visual features abandon spatial information of the input image and thus less robust to spatial variations.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Question-guided Hybrid Convolution (QGHC) for multi-modal feature fusion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">To fully utilize the spatial information of the input image, we propose Language-guided Hybrid Convolution for feature fusion. Unlike bilinear pooling methods that treat visual and textual features in a symmetric way, our approach performs the convolution on visual feature maps and the convolution kernels are predicted based on the question features which can be formulated as:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle f_{v+q}" display="inline"><semantics id="S3.E3.m1.1a"><msub id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">f</mi><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">v</mi><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝑓</ci><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑣</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle f_{v+q}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.2" class="ltx_Math" alttext="\displaystyle=\textnormal{CNN}_{p}(I;\tilde{\theta}_{v}(f_{q}))," display="inline"><semantics id="S3.E3.m2.2a"><mrow id="S3.E3.m2.2.2.1" xref="S3.E3.m2.2.2.1.1.cmml"><mrow id="S3.E3.m2.2.2.1.1" xref="S3.E3.m2.2.2.1.1.cmml"><mi id="S3.E3.m2.2.2.1.1.3" xref="S3.E3.m2.2.2.1.1.3.cmml"></mi><mo id="S3.E3.m2.2.2.1.1.2" xref="S3.E3.m2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m2.2.2.1.1.1" xref="S3.E3.m2.2.2.1.1.1.cmml"><msub id="S3.E3.m2.2.2.1.1.1.3" xref="S3.E3.m2.2.2.1.1.1.3.cmml"><mtext id="S3.E3.m2.2.2.1.1.1.3.2" xref="S3.E3.m2.2.2.1.1.1.3.2a.cmml">CNN</mtext><mi id="S3.E3.m2.2.2.1.1.1.3.3" xref="S3.E3.m2.2.2.1.1.1.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.2.2.1.1.1.2" xref="S3.E3.m2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.2.2.1.1.1.1.1" xref="S3.E3.m2.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m2.2.2.1.1.1.1.1.2" xref="S3.E3.m2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml">I</mi><mo id="S3.E3.m2.2.2.1.1.1.1.1.3" xref="S3.E3.m2.2.2.1.1.1.1.2.cmml">;</mo><mrow id="S3.E3.m2.2.2.1.1.1.1.1.1" xref="S3.E3.m2.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E3.m2.2.2.1.1.1.1.1.1.3" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.2.cmml">θ</mi><mo id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.E3.m2.2.2.1.1.1.1.1.1.3.3" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.2.2.1.1.1.1.1.1.2" xref="S3.E3.m2.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m2.2.2.1.1.1.1.1.4" xref="S3.E3.m2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m2.2.2.1.2" xref="S3.E3.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.2b"><apply id="S3.E3.m2.2.2.1.1.cmml" xref="S3.E3.m2.2.2.1"><eq id="S3.E3.m2.2.2.1.1.2.cmml" xref="S3.E3.m2.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E3.m2.2.2.1.1.3.cmml" xref="S3.E3.m2.2.2.1.1.3">absent</csymbol><apply id="S3.E3.m2.2.2.1.1.1.cmml" xref="S3.E3.m2.2.2.1.1.1"><times id="S3.E3.m2.2.2.1.1.1.2.cmml" xref="S3.E3.m2.2.2.1.1.1.2"></times><apply id="S3.E3.m2.2.2.1.1.1.3.cmml" xref="S3.E3.m2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.2.2.1.1.1.3.1.cmml" xref="S3.E3.m2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E3.m2.2.2.1.1.1.3.2a.cmml" xref="S3.E3.m2.2.2.1.1.1.3.2"><mtext id="S3.E3.m2.2.2.1.1.1.3.2.cmml" xref="S3.E3.m2.2.2.1.1.1.3.2">CNN</mtext></ci><ci id="S3.E3.m2.2.2.1.1.1.3.3.cmml" xref="S3.E3.m2.2.2.1.1.1.3.3">𝑝</ci></apply><list id="S3.E3.m2.2.2.1.1.1.1.2.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1"><ci id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1">𝐼</ci><apply id="S3.E3.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1"><times id="S3.E3.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E3.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2"><ci id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.1">~</ci><ci id="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.2.2">𝜃</ci></apply><ci id="S3.E3.m2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.3.3">𝑣</ci></apply><apply id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.2.2.1.1.1.1.1.1.1.1.1.3">𝑞</ci></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.2c">\displaystyle=\textnormal{CNN}_{p}(I;\tilde{\theta}_{v}(f_{q})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.5" class="ltx_p">where <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\textnormal{CNN}_{p}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mtext id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2a.cmml">CNN</mtext><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2a.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><mtext id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">CNN</mtext></ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\textnormal{CNN}_{p}</annotation></semantics></math> is the output before the last pooling layer, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\tilde{\theta}_{v}(f_{q})" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mover accent="true" id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2.2" xref="S3.SS2.p1.2.m2.1.1.3.2.2.cmml">θ</mi><mo id="S3.SS2.p1.2.m2.1.1.3.2.1" xref="S3.SS2.p1.2.m2.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"></times><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">subscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2"><ci id="S3.SS2.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2.1">~</ci><ci id="S3.SS2.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2.2">𝜃</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝑣</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\tilde{\theta}_{v}(f_{q})</annotation></semantics></math> denotes the convolutional kernels predicted based on the question feature <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="f_{q}\in\mathbb{R}^{d}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><msub id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p1.3.m3.1.1.2.3" xref="S3.SS2.p1.3.m3.1.1.2.3.cmml">q</mi></msub><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><in id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></in><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">𝑓</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3">𝑞</ci></apply><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">f_{q}\in\mathbb{R}^{d}</annotation></semantics></math>, and the convolution on visual feature maps with the predicted kernels <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\tilde{\theta}_{v}(q)" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.2" xref="S3.SS2.p1.4.m4.1.2.cmml"><msub id="S3.SS2.p1.4.m4.1.2.2" xref="S3.SS2.p1.4.m4.1.2.2.cmml"><mover accent="true" id="S3.SS2.p1.4.m4.1.2.2.2" xref="S3.SS2.p1.4.m4.1.2.2.2.cmml"><mi id="S3.SS2.p1.4.m4.1.2.2.2.2" xref="S3.SS2.p1.4.m4.1.2.2.2.2.cmml">θ</mi><mo id="S3.SS2.p1.4.m4.1.2.2.2.1" xref="S3.SS2.p1.4.m4.1.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.4.m4.1.2.2.3" xref="S3.SS2.p1.4.m4.1.2.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.2.1" xref="S3.SS2.p1.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.4.m4.1.2.3.2" xref="S3.SS2.p1.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.4.m4.1.2.3.2.1" xref="S3.SS2.p1.4.m4.1.2.cmml">(</mo><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">q</mi><mo stretchy="false" id="S3.SS2.p1.4.m4.1.2.3.2.2" xref="S3.SS2.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.2.cmml" xref="S3.SS2.p1.4.m4.1.2"><times id="S3.SS2.p1.4.m4.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2.1"></times><apply id="S3.SS2.p1.4.m4.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.2.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2.2">subscript</csymbol><apply id="S3.SS2.p1.4.m4.1.2.2.2.cmml" xref="S3.SS2.p1.4.m4.1.2.2.2"><ci id="S3.SS2.p1.4.m4.1.2.2.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2.2.2.1">~</ci><ci id="S3.SS2.p1.4.m4.1.2.2.2.2.cmml" xref="S3.SS2.p1.4.m4.1.2.2.2.2">𝜃</ci></apply><ci id="S3.SS2.p1.4.m4.1.2.2.3.cmml" xref="S3.SS2.p1.4.m4.1.2.2.3">𝑣</ci></apply><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\tilde{\theta}_{v}(q)</annotation></semantics></math> results in the multi-modal feature maps <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="f_{v+q}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">f</mi><mrow id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">v</mi><mo id="S3.SS2.p1.5.m5.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">+</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑓</ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><plus id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.1"></plus><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">𝑣</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">f_{v+q}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">However, the naive solution of directly predicting “full” convolutional kernels is memory-inefficient and time-consuming. Mapping the question features to generate full CNN kernels contains a huge number of learnable parameters. In our model, we use the fully-connected layer to learn the question-guided convolutional kernels. To predict a commonly used <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="3\times 3\times 256\times 256" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1a" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.4" xref="S3.SS2.p2.1.m1.1.1.4.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1b" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.5" xref="S3.SS2.p2.1.m1.1.1.5.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">3</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.4.cmml" xref="S3.SS2.p2.1.m1.1.1.4">256</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.5.cmml" xref="S3.SS2.p2.1.m1.1.1.5">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">3\times 3\times 256\times 256</annotation></semantics></math> kernel from a 2000-d question feature vector, the FC layer for learning the mapping generates 117 million parameters, which is hard to learn and causes over-fitting on existing VQA datasets. In our experiments, we validate that the performance of the naive solution is even worse than the simple feature concatenation.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To mitigate the problem, we propose to predict parameters of group convolution kernels. The group convolution divides the input feature maps into several groups along the channel dimension, and thus each group has a reduced number of channels for convolution. Outputs of convolution with each group are then concatenated in the channel dimension to produce the output feature maps. In addition, we classify the convolution kernels into dynamically-predicted kernels and freely-updated kernels. The dynamic kernels are question-dependent, which are predicted based on the question feature vector <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="f_{q}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑓</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">f_{q}</annotation></semantics></math>. The freely-updated kernels are question-independent. They are trained as conventional convolution kernels via back-propagation. The dynamically-predicted kernels fuse the textual and visual information in early model stage which better capture the multi-model relationships. The freely-updated kernels reduce the parameter size and ensure the model can be trained efficiently. By shuffling parameters among these two kinds of kernels, our model can achieve both the accuracy and efficiency.
During the testing phase, the dynamic kernels are decided by the questions while the freely updated kernels are fixed for all input image-question pairs.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.6" class="ltx_p">Formally, we substitute Eqn. (<a href="#S3.E3" title="In 3.2 Question-guided Hybrid Convolution (QGHC) for multi-modal feature fusion ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) with the proposed QGHC for VQA,</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle f_{v+q}" display="inline"><semantics id="S3.E4.m1.1a"><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">f</mi><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">v</mi><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">𝑓</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑣</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle f_{v+q}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.2" class="ltx_Math" alttext="\displaystyle=\textnormal{CNN}_{g}\left(I;\tilde{\theta}_{v}(f_{q}),\theta_{v}\right)," display="inline"><semantics id="S3.E4.m2.2a"><mrow id="S3.E4.m2.2.2.1" xref="S3.E4.m2.2.2.1.1.cmml"><mrow id="S3.E4.m2.2.2.1.1" xref="S3.E4.m2.2.2.1.1.cmml"><mi id="S3.E4.m2.2.2.1.1.4" xref="S3.E4.m2.2.2.1.1.4.cmml"></mi><mo id="S3.E4.m2.2.2.1.1.3" xref="S3.E4.m2.2.2.1.1.3.cmml">=</mo><mrow id="S3.E4.m2.2.2.1.1.2" xref="S3.E4.m2.2.2.1.1.2.cmml"><msub id="S3.E4.m2.2.2.1.1.2.4" xref="S3.E4.m2.2.2.1.1.2.4.cmml"><mtext id="S3.E4.m2.2.2.1.1.2.4.2" xref="S3.E4.m2.2.2.1.1.2.4.2a.cmml">CNN</mtext><mi id="S3.E4.m2.2.2.1.1.2.4.3" xref="S3.E4.m2.2.2.1.1.2.4.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.2.2.1.1.2.3" xref="S3.E4.m2.2.2.1.1.2.3.cmml">​</mo><mrow id="S3.E4.m2.2.2.1.1.2.2.2" xref="S3.E4.m2.2.2.1.1.2.2.3.cmml"><mo id="S3.E4.m2.2.2.1.1.2.2.2.3" xref="S3.E4.m2.2.2.1.1.2.2.3.cmml">(</mo><mi id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml">I</mi><mo id="S3.E4.m2.2.2.1.1.2.2.2.4" xref="S3.E4.m2.2.2.1.1.2.2.3.cmml">;</mo><mrow id="S3.E4.m2.2.2.1.1.1.1.1.1" xref="S3.E4.m2.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E4.m2.2.2.1.1.1.1.1.1.3" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.2.cmml">θ</mi><mo id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.E4.m2.2.2.1.1.1.1.1.1.3.3" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.2.2.1.1.1.1.1.1.2" xref="S3.E4.m2.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m2.2.2.1.1.2.2.2.5" xref="S3.E4.m2.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E4.m2.2.2.1.1.2.2.2.2" xref="S3.E4.m2.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.1.1.2.2.2.2.2" xref="S3.E4.m2.2.2.1.1.2.2.2.2.2.cmml">θ</mi><mi id="S3.E4.m2.2.2.1.1.2.2.2.2.3" xref="S3.E4.m2.2.2.1.1.2.2.2.2.3.cmml">v</mi></msub><mo id="S3.E4.m2.2.2.1.1.2.2.2.6" xref="S3.E4.m2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m2.2.2.1.2" xref="S3.E4.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.2b"><apply id="S3.E4.m2.2.2.1.1.cmml" xref="S3.E4.m2.2.2.1"><eq id="S3.E4.m2.2.2.1.1.3.cmml" xref="S3.E4.m2.2.2.1.1.3"></eq><csymbol cd="latexml" id="S3.E4.m2.2.2.1.1.4.cmml" xref="S3.E4.m2.2.2.1.1.4">absent</csymbol><apply id="S3.E4.m2.2.2.1.1.2.cmml" xref="S3.E4.m2.2.2.1.1.2"><times id="S3.E4.m2.2.2.1.1.2.3.cmml" xref="S3.E4.m2.2.2.1.1.2.3"></times><apply id="S3.E4.m2.2.2.1.1.2.4.cmml" xref="S3.E4.m2.2.2.1.1.2.4"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.1.1.2.4.1.cmml" xref="S3.E4.m2.2.2.1.1.2.4">subscript</csymbol><ci id="S3.E4.m2.2.2.1.1.2.4.2a.cmml" xref="S3.E4.m2.2.2.1.1.2.4.2"><mtext id="S3.E4.m2.2.2.1.1.2.4.2.cmml" xref="S3.E4.m2.2.2.1.1.2.4.2">CNN</mtext></ci><ci id="S3.E4.m2.2.2.1.1.2.4.3.cmml" xref="S3.E4.m2.2.2.1.1.2.4.3">𝑔</ci></apply><list id="S3.E4.m2.2.2.1.1.2.2.3.cmml" xref="S3.E4.m2.2.2.1.1.2.2.2"><ci id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1">𝐼</ci><apply id="S3.E4.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1"><times id="S3.E4.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E4.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2"><ci id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.1">~</ci><ci id="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.2.2">𝜃</ci></apply><ci id="S3.E4.m2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.3.3">𝑣</ci></apply><apply id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.2.2.1.1.1.1.1.1.1.1.1.3">𝑞</ci></apply></apply><apply id="S3.E4.m2.2.2.1.1.2.2.2.2.cmml" xref="S3.E4.m2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.1.1.2.2.2.2.2">𝜃</ci><ci id="S3.E4.m2.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.1.1.2.2.2.2.3">𝑣</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.2c">\displaystyle=\textnormal{CNN}_{g}\left(I;\tilde{\theta}_{v}(f_{q}),\theta_{v}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle\widehat{a}" display="inline"><semantics id="S3.E5.m1.1a"><mover accent="true" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">a</mi><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><ci id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1">^</ci><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle\widehat{a}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.1" class="ltx_Math" alttext="\displaystyle=\textnormal{MLP}(f_{v+q})," display="inline"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mrow id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.3.cmml"></mi><mo id="S3.E5.m2.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m2.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.cmml"><mtext id="S3.E5.m2.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m2.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S3.E5.m2.1.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.2.cmml">v</mi><mo id="S3.E5.m2.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.3.cmml">q</mi></mrow></msub><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m2.1.1.1.2" xref="S3.E5.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1"><eq id="S3.E5.m2.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.3">absent</csymbol><apply id="S3.E5.m2.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1"><times id="S3.E5.m2.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.2"></times><ci id="S3.E5.m2.1.1.1.1.1.3a.cmml" xref="S3.E5.m2.1.1.1.1.1.3"><mtext id="S3.E5.m2.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.3">MLP</mtext></ci><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.2">𝑓</ci><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3"><plus id="S3.E5.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.1"></plus><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.2">𝑣</ci><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle=\textnormal{MLP}(f_{v+q}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.5" class="ltx_p">where CNN<sub id="S3.SS2.p4.5.1" class="ltx_sub"><span id="S3.SS2.p4.5.1.1" class="ltx_text ltx_font_italic">g</span></sub> denotes a group convolution network with dynamically-predicted kernels <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\tilde{\theta}_{v}(f_{q})" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><msub id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml"><mover accent="true" id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml"><mi id="S3.SS2.p4.2.m2.1.1.3.2.2" xref="S3.SS2.p4.2.m2.1.1.3.2.2.cmml">θ</mi><mo id="S3.SS2.p4.2.m2.1.1.3.2.1" xref="S3.SS2.p4.2.m2.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS2.p4.2.m2.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.2.m2.1.1.1.1.2" xref="S3.SS2.p4.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p4.2.m2.1.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.2" xref="S3.SS2.p4.2.m2.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.3" xref="S3.SS2.p4.2.m2.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS2.p4.2.m2.1.1.1.1.3" xref="S3.SS2.p4.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><times id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2"></times><apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3">subscript</csymbol><apply id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2"><ci id="S3.SS2.p4.2.m2.1.1.3.2.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2.1">~</ci><ci id="S3.SS2.p4.2.m2.1.1.3.2.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2.2">𝜃</ci></apply><ci id="S3.SS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3">𝑣</ci></apply><apply id="S3.SS2.p4.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\tilde{\theta}_{v}(f_{q})</annotation></semantics></math> and freely-updated kernels <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\theta_{v}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">θ</mi><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">𝜃</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\theta_{v}</annotation></semantics></math>. The output of the CNN <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="f_{v+q}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">f</mi><mrow id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS2.p4.4.m4.1.1.3.2" xref="S3.SS2.p4.4.m4.1.1.3.2.cmml">v</mi><mo id="S3.SS2.p4.4.m4.1.1.3.1" xref="S3.SS2.p4.4.m4.1.1.3.1.cmml">+</mo><mi id="S3.SS2.p4.4.m4.1.1.3.3" xref="S3.SS2.p4.4.m4.1.1.3.3.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">𝑓</ci><apply id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3"><plus id="S3.SS2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.3.1"></plus><ci id="S3.SS2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.3.2">𝑣</ci><ci id="S3.SS2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">f_{v+q}</annotation></semantics></math> fuses the textual and visual information and infers the final answers. MLP is a multi-layer perception module and <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="\widehat{a}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mover accent="true" id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">a</mi><mo id="S3.SS2.p4.5.m5.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><ci id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1.1">^</ci><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">\widehat{a}</annotation></semantics></math> is the predicted answers.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">The freely-updated kernels can capture pre-trained image patterns and we fix them during the testing stage. The dynamically-predicted kernels are dependent on the input questions and capture the question-image relationships. Our model fuses the textual and visual information in early model stage by the convolution operation. The spatial information between two modalities is well preserved which leads to more accurate results than previous feature concatenation strategies. The combination of the dynamic and freely-updated kernels is crucial important in keeping both the accuracy and efficiency and shows promising results in our experiments.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>QGHC module</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1808.02632/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Network structure of our QGHC module with <math id="S3.F2.6.m1.1" class="ltx_Math" alttext="N=8" display="inline"><semantics id="S3.F2.6.m1.1b"><mrow id="S3.F2.6.m1.1.1" xref="S3.F2.6.m1.1.1.cmml"><mi id="S3.F2.6.m1.1.1.2" xref="S3.F2.6.m1.1.1.2.cmml">N</mi><mo id="S3.F2.6.m1.1.1.1" xref="S3.F2.6.m1.1.1.1.cmml">=</mo><mn id="S3.F2.6.m1.1.1.3" xref="S3.F2.6.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.6.m1.1c"><apply id="S3.F2.6.m1.1.1.cmml" xref="S3.F2.6.m1.1.1"><eq id="S3.F2.6.m1.1.1.1.cmml" xref="S3.F2.6.m1.1.1.1"></eq><ci id="S3.F2.6.m1.1.1.2.cmml" xref="S3.F2.6.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.F2.6.m1.1.1.3.cmml" xref="S3.F2.6.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m1.1d">N=8</annotation></semantics></math> and <math id="S3.F2.7.m2.1" class="ltx_Math" alttext="C_{i}=C_{o}=512" display="inline"><semantics id="S3.F2.7.m2.1b"><mrow id="S3.F2.7.m2.1.1" xref="S3.F2.7.m2.1.1.cmml"><msub id="S3.F2.7.m2.1.1.2" xref="S3.F2.7.m2.1.1.2.cmml"><mi id="S3.F2.7.m2.1.1.2.2" xref="S3.F2.7.m2.1.1.2.2.cmml">C</mi><mi id="S3.F2.7.m2.1.1.2.3" xref="S3.F2.7.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.F2.7.m2.1.1.3" xref="S3.F2.7.m2.1.1.3.cmml">=</mo><msub id="S3.F2.7.m2.1.1.4" xref="S3.F2.7.m2.1.1.4.cmml"><mi id="S3.F2.7.m2.1.1.4.2" xref="S3.F2.7.m2.1.1.4.2.cmml">C</mi><mi id="S3.F2.7.m2.1.1.4.3" xref="S3.F2.7.m2.1.1.4.3.cmml">o</mi></msub><mo id="S3.F2.7.m2.1.1.5" xref="S3.F2.7.m2.1.1.5.cmml">=</mo><mn id="S3.F2.7.m2.1.1.6" xref="S3.F2.7.m2.1.1.6.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.7.m2.1c"><apply id="S3.F2.7.m2.1.1.cmml" xref="S3.F2.7.m2.1.1"><and id="S3.F2.7.m2.1.1a.cmml" xref="S3.F2.7.m2.1.1"></and><apply id="S3.F2.7.m2.1.1b.cmml" xref="S3.F2.7.m2.1.1"><eq id="S3.F2.7.m2.1.1.3.cmml" xref="S3.F2.7.m2.1.1.3"></eq><apply id="S3.F2.7.m2.1.1.2.cmml" xref="S3.F2.7.m2.1.1.2"><csymbol cd="ambiguous" id="S3.F2.7.m2.1.1.2.1.cmml" xref="S3.F2.7.m2.1.1.2">subscript</csymbol><ci id="S3.F2.7.m2.1.1.2.2.cmml" xref="S3.F2.7.m2.1.1.2.2">𝐶</ci><ci id="S3.F2.7.m2.1.1.2.3.cmml" xref="S3.F2.7.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.F2.7.m2.1.1.4.cmml" xref="S3.F2.7.m2.1.1.4"><csymbol cd="ambiguous" id="S3.F2.7.m2.1.1.4.1.cmml" xref="S3.F2.7.m2.1.1.4">subscript</csymbol><ci id="S3.F2.7.m2.1.1.4.2.cmml" xref="S3.F2.7.m2.1.1.4.2">𝐶</ci><ci id="S3.F2.7.m2.1.1.4.3.cmml" xref="S3.F2.7.m2.1.1.4.3">𝑜</ci></apply></apply><apply id="S3.F2.7.m2.1.1c.cmml" xref="S3.F2.7.m2.1.1"><eq id="S3.F2.7.m2.1.1.5.cmml" xref="S3.F2.7.m2.1.1.5"></eq><share href="#S3.F2.7.m2.1.1.4.cmml" id="S3.F2.7.m2.1.1d.cmml" xref="S3.F2.7.m2.1.1"></share><cn type="integer" id="S3.F2.7.m2.1.1.6.cmml" xref="S3.F2.7.m2.1.1.6">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m2.1d">C_{i}=C_{o}=512</annotation></semantics></math>. The question features are used to learn <math id="S3.F2.8.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.F2.8.m3.1b"><mi id="S3.F2.8.m3.1.1" xref="S3.F2.8.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.m3.1c"><ci id="S3.F2.8.m3.1.1.cmml" xref="S3.F2.8.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m3.1d">n</annotation></semantics></math> convolution groups in the <math id="S3.F2.9.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.F2.9.m4.1b"><mrow id="S3.F2.9.m4.1.1" xref="S3.F2.9.m4.1.1.cmml"><mn id="S3.F2.9.m4.1.1.2" xref="S3.F2.9.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.9.m4.1.1.1" xref="S3.F2.9.m4.1.1.1.cmml">×</mo><mn id="S3.F2.9.m4.1.1.3" xref="S3.F2.9.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.9.m4.1c"><apply id="S3.F2.9.m4.1.1.cmml" xref="S3.F2.9.m4.1.1"><times id="S3.F2.9.m4.1.1.1.cmml" xref="S3.F2.9.m4.1.1.1"></times><cn type="integer" id="S3.F2.9.m4.1.1.2.cmml" xref="S3.F2.9.m4.1.1.2">3</cn><cn type="integer" id="S3.F2.9.m4.1.1.3.cmml" xref="S3.F2.9.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.m4.1d">3\times 3</annotation></semantics></math> convolution layer (the yellow block). A group shuffling layer is utilized to share the textual information from question-guided kernels to the whole network.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">We stack multiple QGHC modules to better capture the interactions between the input image and question. Inspired by ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and ResNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, our QGHC module consists of a series of <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">1\times 1</annotation></semantics></math>, <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">3\times 3</annotation></semantics></math>, and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">1</cn><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1\times 1</annotation></semantics></math> convolutions.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.15" class="ltx_p">As shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 QGHC module ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the module is designed similarly to the ShffuleNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> module with group convolution and identity shortcuts. The <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">C</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝐶</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">C_{i}</annotation></semantics></math>-channel input feature maps are first equally divided into <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">N</annotation></semantics></math> groups (paths). Each of the <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">N</annotation></semantics></math> groups then goes through 3 stages of convolutions and outputs <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="C_{o}/N" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><msub id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml">C</mi><mi id="S3.SS3.p2.4.m4.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.2.3.cmml">o</mi></msub><mo id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">/</mo><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><divide id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"></divide><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2">𝐶</ci><ci id="S3.SS3.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.2.3">𝑜</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">C_{o}/N</annotation></semantics></math>-d feature maps. For each group, the first convolution is a <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mn id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.5.m5.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><times id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1"></times><cn type="integer" id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">1</cn><cn type="integer" id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">1\times 1</annotation></semantics></math> convolution that outputs <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="C_{i}/2N" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mrow id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml"><msub id="S3.SS3.p2.6.m6.1.1.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.2.cmml">C</mi><mi id="S3.SS3.p2.6.m6.1.1.2.2.3" xref="S3.SS3.p2.6.m6.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS3.p2.6.m6.1.1.2.1" xref="S3.SS3.p2.6.m6.1.1.2.1.cmml">/</mo><mn id="S3.SS3.p2.6.m6.1.1.2.3" xref="S3.SS3.p2.6.m6.1.1.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><times id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></times><apply id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2"><divide id="S3.SS3.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2.1"></divide><apply id="S3.SS3.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.2.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2.2">𝐶</ci><ci id="S3.SS3.p2.6.m6.1.1.2.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS3.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.2.3">2</cn></apply><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">C_{i}/2N</annotation></semantics></math>-channel feature maps. The second <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mrow id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mn id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.7.m7.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><times id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1"></times><cn type="integer" id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">3\times 3</annotation></semantics></math> convolution outputs <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="C_{i}/2N" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><mrow id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><mrow id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml"><msub id="S3.SS3.p2.8.m8.1.1.2.2" xref="S3.SS3.p2.8.m8.1.1.2.2.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2.2.2" xref="S3.SS3.p2.8.m8.1.1.2.2.2.cmml">C</mi><mi id="S3.SS3.p2.8.m8.1.1.2.2.3" xref="S3.SS3.p2.8.m8.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS3.p2.8.m8.1.1.2.1" xref="S3.SS3.p2.8.m8.1.1.2.1.cmml">/</mo><mn id="S3.SS3.p2.8.m8.1.1.2.3" xref="S3.SS3.p2.8.m8.1.1.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p2.8.m8.1.1.1" xref="S3.SS3.p2.8.m8.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><times id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1.1"></times><apply id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2"><divide id="S3.SS3.p2.8.m8.1.1.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.2.1"></divide><apply id="S3.SS3.p2.8.m8.1.1.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.2.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.8.m8.1.1.2.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2.2">𝐶</ci><ci id="S3.SS3.p2.8.m8.1.1.2.2.3.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS3.p2.8.m8.1.1.2.3.cmml" xref="S3.SS3.p2.8.m8.1.1.2.3">2</cn></apply><ci id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">C_{i}/2N</annotation></semantics></math>-channel feature maps, and the final <math id="S3.SS3.p2.9.m9.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p2.9.m9.1a"><mrow id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml"><mn id="S3.SS3.p2.9.m9.1.1.2" xref="S3.SS3.p2.9.m9.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.9.m9.1.1.1" xref="S3.SS3.p2.9.m9.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.9.m9.1.1.3" xref="S3.SS3.p2.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><apply id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1"><times id="S3.SS3.p2.9.m9.1.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1.1"></times><cn type="integer" id="S3.SS3.p2.9.m9.1.1.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2">1</cn><cn type="integer" id="S3.SS3.p2.9.m9.1.1.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">1\times 1</annotation></semantics></math> convolution outputs <math id="S3.SS3.p2.10.m10.1" class="ltx_Math" alttext="C_{o}/N" display="inline"><semantics id="S3.SS3.p2.10.m10.1a"><mrow id="S3.SS3.p2.10.m10.1.1" xref="S3.SS3.p2.10.m10.1.1.cmml"><msub id="S3.SS3.p2.10.m10.1.1.2" xref="S3.SS3.p2.10.m10.1.1.2.cmml"><mi id="S3.SS3.p2.10.m10.1.1.2.2" xref="S3.SS3.p2.10.m10.1.1.2.2.cmml">C</mi><mi id="S3.SS3.p2.10.m10.1.1.2.3" xref="S3.SS3.p2.10.m10.1.1.2.3.cmml">o</mi></msub><mo id="S3.SS3.p2.10.m10.1.1.1" xref="S3.SS3.p2.10.m10.1.1.1.cmml">/</mo><mi id="S3.SS3.p2.10.m10.1.1.3" xref="S3.SS3.p2.10.m10.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m10.1b"><apply id="S3.SS3.p2.10.m10.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1"><divide id="S3.SS3.p2.10.m10.1.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1.1"></divide><apply id="S3.SS3.p2.10.m10.1.1.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m10.1.1.2.1.cmml" xref="S3.SS3.p2.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.10.m10.1.1.2.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2.2">𝐶</ci><ci id="S3.SS3.p2.10.m10.1.1.2.3.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3">𝑜</ci></apply><ci id="S3.SS3.p2.10.m10.1.1.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m10.1c">C_{o}/N</annotation></semantics></math>-channel feature maps. We add a group shuffling layer after the <math id="S3.SS3.p2.11.m11.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p2.11.m11.1a"><mrow id="S3.SS3.p2.11.m11.1.1" xref="S3.SS3.p2.11.m11.1.1.cmml"><mn id="S3.SS3.p2.11.m11.1.1.2" xref="S3.SS3.p2.11.m11.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.11.m11.1.1.1" xref="S3.SS3.p2.11.m11.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.11.m11.1.1.3" xref="S3.SS3.p2.11.m11.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m11.1b"><apply id="S3.SS3.p2.11.m11.1.1.cmml" xref="S3.SS3.p2.11.m11.1.1"><times id="S3.SS3.p2.11.m11.1.1.1.cmml" xref="S3.SS3.p2.11.m11.1.1.1"></times><cn type="integer" id="S3.SS3.p2.11.m11.1.1.2.cmml" xref="S3.SS3.p2.11.m11.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.11.m11.1.1.3.cmml" xref="S3.SS3.p2.11.m11.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m11.1c">3\times 3</annotation></semantics></math> convolution layer to make features between different groups interact with each other and keep the advantages of both the dynamically-predicted kernels and freely-updated kernels. The output of <math id="S3.SS3.p2.12.m12.1" class="ltx_Math" alttext="C_{o}/N" display="inline"><semantics id="S3.SS3.p2.12.m12.1a"><mrow id="S3.SS3.p2.12.m12.1.1" xref="S3.SS3.p2.12.m12.1.1.cmml"><msub id="S3.SS3.p2.12.m12.1.1.2" xref="S3.SS3.p2.12.m12.1.1.2.cmml"><mi id="S3.SS3.p2.12.m12.1.1.2.2" xref="S3.SS3.p2.12.m12.1.1.2.2.cmml">C</mi><mi id="S3.SS3.p2.12.m12.1.1.2.3" xref="S3.SS3.p2.12.m12.1.1.2.3.cmml">o</mi></msub><mo id="S3.SS3.p2.12.m12.1.1.1" xref="S3.SS3.p2.12.m12.1.1.1.cmml">/</mo><mi id="S3.SS3.p2.12.m12.1.1.3" xref="S3.SS3.p2.12.m12.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m12.1b"><apply id="S3.SS3.p2.12.m12.1.1.cmml" xref="S3.SS3.p2.12.m12.1.1"><divide id="S3.SS3.p2.12.m12.1.1.1.cmml" xref="S3.SS3.p2.12.m12.1.1.1"></divide><apply id="S3.SS3.p2.12.m12.1.1.2.cmml" xref="S3.SS3.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.12.m12.1.1.2.1.cmml" xref="S3.SS3.p2.12.m12.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.12.m12.1.1.2.2.cmml" xref="S3.SS3.p2.12.m12.1.1.2.2">𝐶</ci><ci id="S3.SS3.p2.12.m12.1.1.2.3.cmml" xref="S3.SS3.p2.12.m12.1.1.2.3">𝑜</ci></apply><ci id="S3.SS3.p2.12.m12.1.1.3.cmml" xref="S3.SS3.p2.12.m12.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m12.1c">C_{o}/N</annotation></semantics></math>-channel feature maps for the <math id="S3.SS3.p2.13.m13.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.13.m13.1a"><mi id="S3.SS3.p2.13.m13.1.1" xref="S3.SS3.p2.13.m13.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.13.m13.1b"><ci id="S3.SS3.p2.13.m13.1.1.cmml" xref="S3.SS3.p2.13.m13.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.13.m13.1c">N</annotation></semantics></math> groups are then concatenated together along the channel dimension. For the shortcut connection, a <math id="S3.SS3.p2.14.m14.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p2.14.m14.1a"><mrow id="S3.SS3.p2.14.m14.1.1" xref="S3.SS3.p2.14.m14.1.1.cmml"><mn id="S3.SS3.p2.14.m14.1.1.2" xref="S3.SS3.p2.14.m14.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.14.m14.1.1.1" xref="S3.SS3.p2.14.m14.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.14.m14.1.1.3" xref="S3.SS3.p2.14.m14.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.14.m14.1b"><apply id="S3.SS3.p2.14.m14.1.1.cmml" xref="S3.SS3.p2.14.m14.1.1"><times id="S3.SS3.p2.14.m14.1.1.1.cmml" xref="S3.SS3.p2.14.m14.1.1.1"></times><cn type="integer" id="S3.SS3.p2.14.m14.1.1.2.cmml" xref="S3.SS3.p2.14.m14.1.1.2">1</cn><cn type="integer" id="S3.SS3.p2.14.m14.1.1.3.cmml" xref="S3.SS3.p2.14.m14.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.14.m14.1c">1\times 1</annotation></semantics></math> convolution transforms the input feature maps to <math id="S3.SS3.p2.15.m15.1" class="ltx_Math" alttext="C_{o}" display="inline"><semantics id="S3.SS3.p2.15.m15.1a"><msub id="S3.SS3.p2.15.m15.1.1" xref="S3.SS3.p2.15.m15.1.1.cmml"><mi id="S3.SS3.p2.15.m15.1.1.2" xref="S3.SS3.p2.15.m15.1.1.2.cmml">C</mi><mi id="S3.SS3.p2.15.m15.1.1.3" xref="S3.SS3.p2.15.m15.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.15.m15.1b"><apply id="S3.SS3.p2.15.m15.1.1.cmml" xref="S3.SS3.p2.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.15.m15.1.1.1.cmml" xref="S3.SS3.p2.15.m15.1.1">subscript</csymbol><ci id="S3.SS3.p2.15.m15.1.1.2.cmml" xref="S3.SS3.p2.15.m15.1.1.2">𝐶</ci><ci id="S3.SS3.p2.15.m15.1.1.3.cmml" xref="S3.SS3.p2.15.m15.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.15.m15.1c">C_{o}</annotation></semantics></math>-d features, which are added with the output feature maps. Batch Normalization and ReLU are performed after each convolutional operation except for the last one, where ReLU is performed after the addition with the shortcut.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">The <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">3\times 3</annotation></semantics></math> group convolution is guided by the input questions. We randomly select <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">n</annotation></semantics></math> group kernels. Their parameters are predicted based on the question features.
Those kernel weights are question-dependent and are used to capture location-sensitive question-image interactions.
The remaining <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="N-n" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">−</mo><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><minus id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"></minus><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑁</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">N-n</annotation></semantics></math> group kernels have freely-updated kernels. They are updated via back-propagation in the training stage and are fixed for all images during testing. These kernels capture the pre-trained image patterns or image-question patterns. They are constant to the input questions and images.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1808.02632/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The proposed QGHC network with three stacked QGHC modules for VQA. Question-guided kernels are learned based on the input question and convoluted with visual feature maps to generate multi-modal features for the answer prediction.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>QGHC network for visual question answering</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The network structure for our QGHC network is illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 QGHC module ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is first pre-trained on the ImageNet to extract mid-level visual features. The question features are generated by a language RNN model.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">The visual feature maps are then send to three QGHC modules with <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="N=8" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><eq id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></eq><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">N=8</annotation></semantics></math> groups and <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="C_{o}=512" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><msub id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2.2" xref="S3.SS4.p2.2.m2.1.1.2.2.cmml">C</mi><mi id="S3.SS4.p2.2.m2.1.1.2.3" xref="S3.SS4.p2.2.m2.1.1.2.3.cmml">o</mi></msub><mo id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><eq id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></eq><apply id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.2.1.cmml" xref="S3.SS4.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2.2">𝐶</ci><ci id="S3.SS4.p2.2.m2.1.1.2.3.cmml" xref="S3.SS4.p2.2.m2.1.1.2.3">𝑜</ci></apply><cn type="integer" id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">C_{o}=512</annotation></semantics></math>. The output of the QGHC modules <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="f_{v+q}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">f</mi><mrow id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml"><mi id="S3.SS4.p2.3.m3.1.1.3.2" xref="S3.SS4.p2.3.m3.1.1.3.2.cmml">v</mi><mo id="S3.SS4.p2.3.m3.1.1.3.1" xref="S3.SS4.p2.3.m3.1.1.3.1.cmml">+</mo><mi id="S3.SS4.p2.3.m3.1.1.3.3" xref="S3.SS4.p2.3.m3.1.1.3.3.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">𝑓</ci><apply id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><plus id="S3.SS4.p2.3.m3.1.1.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3.1"></plus><ci id="S3.SS4.p2.3.m3.1.1.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2">𝑣</ci><ci id="S3.SS4.p2.3.m3.1.1.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">f_{v+q}</annotation></semantics></math> has the same spatial sizes with the input feature maps. A global average pooling is applied to the final feature maps to generate the final multi-modal feature representation for predicting the most likely answer <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\widehat{a}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mover accent="true" id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">a</mi><mo id="S3.SS4.p2.4.m4.1.1.1" xref="S3.SS4.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><ci id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1.1">^</ci><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\widehat{a}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.5" class="ltx_p">To learn the dynamic convolution kernels in the QGHC modules, the question feature <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="f_{q}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑓</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">f_{q}</annotation></semantics></math> is transformed by two FC layers with a ReLU activation in between. The two FC layers first project the question to a <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="9216" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mn id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">9216</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><cn type="integer" id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">9216</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">9216</annotation></semantics></math>-d vector. The <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mrow id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mn id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.3.m3.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><times id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1"></times><cn type="integer" id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">3</cn><cn type="integer" id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">3\times 3</annotation></semantics></math> question-dependent kernel weights of the three QGHC modules are obtained by reshaping the learned parameters into <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="3\times 3\times 32\times 32" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mrow id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml"><mn id="S3.SS4.p3.4.m4.1.1.2" xref="S3.SS4.p3.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.4.m4.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS4.p3.4.m4.1.1.3" xref="S3.SS4.p3.4.m4.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.4.m4.1.1.1a" xref="S3.SS4.p3.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS4.p3.4.m4.1.1.4" xref="S3.SS4.p3.4.m4.1.1.4.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.4.m4.1.1.1b" xref="S3.SS4.p3.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS4.p3.4.m4.1.1.5" xref="S3.SS4.p3.4.m4.1.1.5.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1"><times id="S3.SS4.p3.4.m4.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1"></times><cn type="integer" id="S3.SS4.p3.4.m4.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2">3</cn><cn type="integer" id="S3.SS4.p3.4.m4.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.3">3</cn><cn type="integer" id="S3.SS4.p3.4.m4.1.1.4.cmml" xref="S3.SS4.p3.4.m4.1.1.4">32</cn><cn type="integer" id="S3.SS4.p3.4.m4.1.1.5.cmml" xref="S3.SS4.p3.4.m4.1.1.5">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">3\times 3\times 32\times 32</annotation></semantics></math>. However, directly training the proposed network with both dynamically-predicted kernels and freely-updated kernels is non-trivial. The dynamic kernel parameters are the output of the ReLU non-linear function with different magnitudes compared with the freely-updated kernel parameters. We adopt the Weight Normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to balance the weights between the two types of <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mrow id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml"><mn id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.5.m5.1.1.1" xref="S3.SS4.p3.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1"><times id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1.1"></times><cn type="integer" id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">3</cn><cn type="integer" id="S3.SS4.p3.5.m5.1.1.3.cmml" xref="S3.SS4.p3.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">3\times 3</annotation></semantics></math> kernels, which stabilizes the training of the network.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>QGHC network with bilinear pooling and attention</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Our proposed QGHC network is also complementary with the existing bilinear pooling fusion methods and the attention mechanism.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">To combine with the MLB fusion scheme <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the multi-modal features extracted from the global average pooling layer could be fused with the RNN question features again using a MLB. The fused features could be used to predict the final answers. The second stage fusion of textual and visual features brings a further improvement on the answering accuracy in our experiments.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">We also apply an attention model to better capture the spatial information. The original global average pooling layer is thus replaced by the the attention map.
To weight more on locations of interest, a weighting map is learned by attention mechanism. A <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mn id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><times id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">1\times 1</annotation></semantics></math> convolution following a spatial Softmax function generates the attention weighting map. The final multi-modal features is the weighted summation of features at all the locations. The output feature maps from the last QGHC module are added with the linearly transformed question features. The attention mechanism is shown as the green rectangles in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 QGHC module ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We test our proposed approach and compare it with the state-of-the-arts on two public datasets, the CLEVR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center" colspan="2"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Parameter size</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:70%;">val</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.1.1" class="ltx_text" style="font-size:70%;">QD Weights</span></td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">QI Weights</span></td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.3.1" class="ltx_text" style="font-size:70%;">All</span></td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.3.3.1.1" class="ltx_text" style="font-size:70%;">QGHC</span></th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.2.1" class="ltx_text" style="font-size:70%;">5.4M</span></td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.3.1" class="ltx_text" style="font-size:70%;">0.9M</span></td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.4.1" class="ltx_text" style="font-size:70%;">59.24</span></td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.4.4.1.1" class="ltx_text" style="font-size:70%;">QGHC-1</span></th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.4.4.2.1" class="ltx_text" style="font-size:70%;">1.8M</span></td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.4.4.3.1" class="ltx_text" style="font-size:70%;">0.3M</span></td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.4.4.4.1" class="ltx_text" style="font-size:70%;">58.88</span></td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.5.5.1.1" class="ltx_text" style="font-size:70%;">QGHC-2</span></th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.5.2.1" class="ltx_text" style="font-size:70%;">3.6M</span></td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.5.3.1" class="ltx_text" style="font-size:70%;">0.6M</span></td>
<td id="S4.T1.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.5.4.1" class="ltx_text" style="font-size:70%;">59.04</span></td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.6.6.1.1" class="ltx_text" style="font-size:70%;">QGHC-4</span></th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.2.1" class="ltx_text" style="font-size:70%;">7.2M</span></td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.3.1" class="ltx_text" style="font-size:70%;">1.2M</span></td>
<td id="S4.T1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.4.1" class="ltx_text" style="font-size:70%;">59.13</span></td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.7.7.1.1" class="ltx_text" style="font-size:70%;">QGHC-1/2</span></th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.2.1" class="ltx_text" style="font-size:70%;">1.3M</span></td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.3.1" class="ltx_text" style="font-size:70%;">0.7M</span></td>
<td id="S4.T1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.4.1" class="ltx_text" style="font-size:70%;">58.78</span></td>
</tr>
<tr id="S4.T1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.8.8.1.1" class="ltx_text" style="font-size:70%;">QGHC-group 4</span></th>
<td id="S4.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.8.8.2.1" class="ltx_text" style="font-size:70%;">8.7M</span></td>
<td id="S4.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.8.8.3.1" class="ltx_text" style="font-size:70%;">2.1M</span></td>
<td id="S4.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.8.8.4.1" class="ltx_text" style="font-size:70%;">59.01</span></td>
</tr>
<tr id="S4.T1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.9.9.1.1" class="ltx_text" style="font-size:70%;">QGHC-group 16</span></th>
<td id="S4.T1.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.9.9.2.1" class="ltx_text" style="font-size:70%;">1.3 M</span></td>
<td id="S4.T1.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.9.9.3.1" class="ltx_text" style="font-size:70%;">0.15M</span></td>
<td id="S4.T1.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.9.9.4.1" class="ltx_text" style="font-size:70%;">58.22</span></td>
</tr>
<tr id="S4.T1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.10.10.1.1" class="ltx_text" style="font-size:70%;">QGHC-w/o shuffle</span></th>
<td id="S4.T1.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.10.2.1" class="ltx_text" style="font-size:70%;">5.4M</span></td>
<td id="S4.T1.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.10.3.1" class="ltx_text" style="font-size:70%;">0.9M</span></td>
<td id="S4.T1.1.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.10.4.1" class="ltx_text" style="font-size:70%;">58.92</span></td>
</tr>
<tr id="S4.T1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.11.11.1.1" class="ltx_text" style="font-size:70%;">QGHC-1-naive</span></th>
<td id="S4.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.11.11.2.1" class="ltx_text" style="font-size:70%;">471M</span></td>
<td id="S4.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.11.11.3.1" class="ltx_text" style="font-size:70%;">0M</span></td>
<td id="S4.T1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.11.11.4.1" class="ltx_text" style="font-size:70%;">55.32</span></td>
</tr>
<tr id="S4.T1.1.12.12" class="ltx_tr">
<th id="S4.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.12.12.1.1" class="ltx_text" style="font-size:70%;">QGHC-1-full</span></th>
<td id="S4.T1.1.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.12.2.1" class="ltx_text" style="font-size:70%;">117M</span></td>
<td id="S4.T1.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.12.3.1" class="ltx_text" style="font-size:70%;">0.2M</span></td>
<td id="S4.T1.1.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.12.4.1" class="ltx_text" style="font-size:70%;">57.01</span></td>
</tr>
<tr id="S4.T1.1.13.13" class="ltx_tr">
<th id="S4.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.13.13.1.1" class="ltx_text" style="font-size:70%;">QGHC-1-group</span></th>
<td id="S4.T1.1.13.13.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.13.13.2.1" class="ltx_text" style="font-size:70%;">14M</span></td>
<td id="S4.T1.1.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.13.13.3.1" class="ltx_text" style="font-size:70%;">0.03M</span></td>
<td id="S4.T1.1.13.13.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.13.13.4.1" class="ltx_text" style="font-size:70%;">58.41</span></td>
</tr>
<tr id="S4.T1.1.14.14" class="ltx_tr">
<th id="S4.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.14.14.1.1" class="ltx_text" style="font-size:70%;">QGHC+concat</span></th>
<td id="S4.T1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.14.14.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.14.14.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.14.14.4.1" class="ltx_text" style="font-size:70%;">59.80</span></td>
</tr>
<tr id="S4.T1.1.15.15" class="ltx_tr">
<th id="S4.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.15.15.1.1" class="ltx_text" style="font-size:70%;">QGHC+MUTAN</span></th>
<td id="S4.T1.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.15.15.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.15.15.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.15.15.4.1" class="ltx_text" style="font-size:70%;">60.13</span></td>
</tr>
<tr id="S4.T1.1.16.16" class="ltx_tr">
<th id="S4.T1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.1.16.16.1.1" class="ltx_text" style="font-size:70%;">QGHC+att.</span></th>
<td id="S4.T1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.16.16.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.16.16.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.16.16.4.1" class="ltx_text" style="font-size:70%;">60.64</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ablation studies of our proposed QGHC network on the VQA dataset. QD and QI stands for question-dependent and -independent kernels.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>VQA Dataset</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Data and experimental setup.</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The VQA dataset is built from 204,721 MS-COCO images with human annotated questions and answers. On average, each image has 3 questions and 10 answers for each question. The dataset is divided into three splits: training (82,783 images), validation (40,504 images) and testing (81,434 images).
A testing subset named <em id="S4.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">test-dev</em> with 25% samples can be evaluated multiple times a day. We follow the setup of previous methods and perform ablation studies on the testing subset.
Our experiments focus on the open-ended task, which predict the correct answer in the free-form language expressions.
If the predicted answer appears more than 3 times in the ground truth answers, the predicted answer would be considered as correct.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.3" class="ltx_p">Our models have the same setting when comparing with the state-of-the-art methods. The compared methods follow their original setup. For the proposed approach, images are resized to <math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="448\times 448" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mrow id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.SSS1.p2.1.m1.1.1.2" xref="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml">448</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.p2.1.m1.1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p2.1.m1.1.1.3" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><apply id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1"><times id="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.2">448</cn><cn type="integer" id="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">448\times 448</annotation></semantics></math>. The <math id="S4.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="14\times 14\times 2048" display="inline"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mrow id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.SSS1.p2.2.m2.1.1.2" xref="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.p2.2.m2.1.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p2.2.m2.1.1.3" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.p2.2.m2.1.1.1a" xref="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p2.2.m2.1.1.4" xref="S4.SS1.SSS1.p2.2.m2.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><apply id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1"><times id="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.2">14</cn><cn type="integer" id="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.3">14</cn><cn type="integer" id="S4.SS1.SSS1.p2.2.m2.1.1.4.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">14\times 14\times 2048</annotation></semantics></math> visual features are learned by an ImageNet pre-trained ResNet-152, and the question is encoded to a 2400-d feature vector by the skip-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> using GRU. The candidate questions are selected as the most frequent 2,000 answers in the training and validation sets. The model is trained using the ADAM optimizer with an initial learning rate of <math id="S4.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><msup id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS1.p2.3.m3.1.1.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml"><mo id="S4.SS1.SSS1.p2.3.m3.1.1.3a" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS1.p2.3.m3.1.1.3.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">10</cn><apply id="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3"><minus id="S4.SS1.SSS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS1.SSS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">10^{-4}</annotation></semantics></math>. For results on the validation set, only the training set is used for training. For results on test-dev, we follow the setup of previous methods, both the training and validation data are used for training.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Ablation studies on the VQA dataset.</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.3" class="ltx_p">We conduct ablation studies to investigate factors that influence the final performance of our proposed QGHC network. The results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our default QGHC network (denoted as <em id="S4.SS1.SSS2.p1.3.1" class="ltx_emph ltx_font_italic">QGHC</em>) has a visual ResNet-152 followed by three consecutive QGHC modules. Each QGHC module has a <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mrow id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.1.m1.1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><times id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">1\times 1</annotation></semantics></math> stage-1 convolution with freely-updated kernels, a <math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.2.m2.1.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><times id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">3\times 3</annotation></semantics></math> stage-2 convolution with both dynamically-predicted kernels and freely-updated kernels, and another <math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mrow id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mn id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.3.m3.1.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><times id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">1</cn><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">1\times 1</annotation></semantics></math> convolution stage with freely-updated kernels (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 QGHC module ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Each of these three stage convolutions has 8 groups. They have 32, 32, and 64 output channels respectively.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">We first investigate the influence of the number of QGHC modules and the number of convolution channels. We list the results of different number of QGHC modules in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. <span id="S4.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">QGHC-1</span>, <span id="S4.SS1.SSS2.p2.1.2" class="ltx_text ltx_font_italic">QGHC-2</span>, <span id="S4.SS1.SSS2.p2.1.3" class="ltx_text ltx_font_italic">QGHC-4</span> represent 1, 2, and 4 QGHC modules respectively. As shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the parameter size improves as the number of QGHC increases but there is no further improvement when stacking more than 3 QGHC modules. We therefore keep 3 QGHC modules in our model.
We also test halving the numbers of output channels of the three group convolutions to 16, 16, and 32 (denoted as <em id="S4.SS1.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">QGHC-1/2</em>). The results show that halving the number of channels only slightly decreases the final accuracy.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">We then test different group numbers. We change the group number from 8 to 4 (<em id="S4.SS1.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">QGHC-group 4</em>) and 16 (<em id="S4.SS1.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">QGHC-group 16</em>). Our proposed method is not sensitive to the group number of the convolutions and the model with 8 groups achieves the best performance.
We also investigate the influence of the group shuffling layer.
Removing the group shuffling layer (denoted as <em id="S4.SS1.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">QGHC-w/o shuffle</em>) decreases the accuracy by 0.32% compared with our model. The shuffling layer makes features between different groups interact with each other and is helpful to the final results.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.6" class="ltx_p">For different QGHC module structures, we first test a naive solution. The QGHC module is implemented as a single <math id="S4.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS1.SSS2.p4.1.m1.1a"><mrow id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml"><mn id="S4.SS1.SSS2.p4.1.m1.1.1.2" xref="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.1.m1.1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.1.m1.1.1.3" xref="S4.SS1.SSS2.p4.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.1b"><apply id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1"><times id="S4.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.2">3</cn><cn type="integer" id="S4.SS1.SSS2.p4.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.1c">3\times 3</annotation></semantics></math> “full” convolution without groups. Its parameters are all dynamically predicted by question features (denoted as <em id="S4.SS1.SSS2.p4.6.1" class="ltx_emph ltx_font_italic">QGHC-1-naive</em>). We then convert the single <math id="S4.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS1.SSS2.p4.2.m2.1a"><mrow id="S4.SS1.SSS2.p4.2.m2.1.1" xref="S4.SS1.SSS2.p4.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p4.2.m2.1.1.2" xref="S4.SS1.SSS2.p4.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.2.m2.1.1.1" xref="S4.SS1.SSS2.p4.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.2.m2.1.1.3" xref="S4.SS1.SSS2.p4.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.2.m2.1b"><apply id="S4.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1"><times id="S4.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.2">3</cn><cn type="integer" id="S4.SS1.SSS2.p4.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.2.m2.1c">3\times 3</annotation></semantics></math> full convolution to a series of <math id="S4.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS1.SSS2.p4.3.m3.1a"><mrow id="S4.SS1.SSS2.p4.3.m3.1.1" xref="S4.SS1.SSS2.p4.3.m3.1.1.cmml"><mn id="S4.SS1.SSS2.p4.3.m3.1.1.2" xref="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.3.m3.1.1.1" xref="S4.SS1.SSS2.p4.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.3.m3.1.1.3" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.3.m3.1b"><apply id="S4.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1"><times id="S4.SS1.SSS2.p4.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.2">1</cn><cn type="integer" id="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.3.m3.1c">1\times 1</annotation></semantics></math>, <math id="S4.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS1.SSS2.p4.4.m4.1a"><mrow id="S4.SS1.SSS2.p4.4.m4.1.1" xref="S4.SS1.SSS2.p4.4.m4.1.1.cmml"><mn id="S4.SS1.SSS2.p4.4.m4.1.1.2" xref="S4.SS1.SSS2.p4.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.4.m4.1.1.1" xref="S4.SS1.SSS2.p4.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.4.m4.1.1.3" xref="S4.SS1.SSS2.p4.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.4.m4.1b"><apply id="S4.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1"><times id="S4.SS1.SSS2.p4.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.2">3</cn><cn type="integer" id="S4.SS1.SSS2.p4.4.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.4.m4.1c">3\times 3</annotation></semantics></math>, <math id="S4.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS1.SSS2.p4.5.m5.1a"><mrow id="S4.SS1.SSS2.p4.5.m5.1.1" xref="S4.SS1.SSS2.p4.5.m5.1.1.cmml"><mn id="S4.SS1.SSS2.p4.5.m5.1.1.2" xref="S4.SS1.SSS2.p4.5.m5.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.5.m5.1.1.1" xref="S4.SS1.SSS2.p4.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.5.m5.1.1.3" xref="S4.SS1.SSS2.p4.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.5.m5.1b"><apply id="S4.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1"><times id="S4.SS1.SSS2.p4.5.m5.1.1.1.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.5.m5.1.1.2.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.2">1</cn><cn type="integer" id="S4.SS1.SSS2.p4.5.m5.1.1.3.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.5.m5.1c">1\times 1</annotation></semantics></math> full convolutions with residual connection between the input and output feature maps (denoted as <em id="S4.SS1.SSS2.p4.6.2" class="ltx_emph ltx_font_italic">QGHC-1-full</em>), where the <math id="S4.SS1.SSS2.p4.6.m6.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS1.SSS2.p4.6.m6.1a"><mrow id="S4.SS1.SSS2.p4.6.m6.1.1" xref="S4.SS1.SSS2.p4.6.m6.1.1.cmml"><mn id="S4.SS1.SSS2.p4.6.m6.1.1.2" xref="S4.SS1.SSS2.p4.6.m6.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p4.6.m6.1.1.1" xref="S4.SS1.SSS2.p4.6.m6.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p4.6.m6.1.1.3" xref="S4.SS1.SSS2.p4.6.m6.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.6.m6.1b"><apply id="S4.SS1.SSS2.p4.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p4.6.m6.1.1"><times id="S4.SS1.SSS2.p4.6.m6.1.1.1.cmml" xref="S4.SS1.SSS2.p4.6.m6.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p4.6.m6.1.1.2.cmml" xref="S4.SS1.SSS2.p4.6.m6.1.1.2">3</cn><cn type="integer" id="S4.SS1.SSS2.p4.6.m6.1.1.3.cmml" xref="S4.SS1.SSS2.p4.6.m6.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.6.m6.1c">3\times 3</annotation></semantics></math> convolution kernels are all dynamically predicted by the question features. The improvement of QGHC-1-full over QGHC-1-naive demonstrates the advantages of the residual structure. Based on QGHC-1-full, we convert all the full convolutions to group convolutions with 8 groups (denoted as <em id="S4.SS1.SSS2.p4.6.3" class="ltx_emph ltx_font_italic">QGHC-1-group</em>). The results outperforms QGHC-1-full, which show the effectiveness of the group convolution.
However, the accuracy is still inferior to our proposed QGHC-1 with hybrid convolution. The results demonstrate that the question-guided kernels can help better fuse the textual and visual features and achieve robust answering performance.</p>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p">Finally, we test the combination of our method with different additional components. 1) The multi-modal features are concatenated with the question features, and then fed into the FC layer for answer prediction. (denoted as <em id="S4.SS1.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">QGHC+concat</em>). It results in a marginal improvement in the final accuracy. 2) We use MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to fuse our QGHC-generated multi-modal features with question features again for answer prediction (denoted as <em id="S4.SS1.SSS2.p5.1.2" class="ltx_emph ltx_font_italic">QGHC+MUTAN</em>). It has better results than QGHC+concat. 3) The attention is also added to QGHC following the descriptions in Section <a href="#S3.SS5" title="3.5 QGHC network with bilinear pooling and attention ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a> (denoted as <em id="S4.SS1.SSS2.p5.1.3" class="ltx_emph ltx_font_italic">QGHC+att.</em>).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">#parameters</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T2.1.1.1.3.1" class="ltx_text" style="font-size:70%;">test-dev</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text" style="font-size:70%;">val</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.1.1" class="ltx_text" style="font-size:70%;">Y/N</span></td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.2.1" class="ltx_text" style="font-size:70%;">Number</span></td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Other</span></td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.4.1" class="ltx_text" style="font-size:70%;">All</span></td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.5.1" class="ltx_text" style="font-size:70%;">All</span></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.1.3.3.1.1" class="ltx_text" style="font-size:70%;">Concat </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.3.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S4.T2.1.3.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.3.1" class="ltx_text" style="font-size:70%;">79.25</span></td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.4.1" class="ltx_text" style="font-size:70%;">36.18</span></td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.5.1" class="ltx_text" style="font-size:70%;">46.69</span></td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.6.1" class="ltx_text" style="font-size:70%;">58.91</span></td>
<td id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.7.1" class="ltx_text" style="font-size:70%;">56.92</span></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.4.4.1.1" class="ltx_text" style="font-size:70%;">MCB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.4.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.1.4.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.2.1" class="ltx_text" style="font-size:70%;">32M</span></td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.3.1" class="ltx_text" style="font-size:70%;">80.81</span></td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.4.1" class="ltx_text" style="font-size:70%;">35.91</span></td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.5.1" class="ltx_text" style="font-size:70%;">46.43</span></td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.6.1" class="ltx_text" style="font-size:70%;">59.40</span></td>
<td id="S4.T2.1.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.7.1" class="ltx_text" style="font-size:70%;">57.39</span></td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.5.5.1.1" class="ltx_text" style="font-size:70%;">MLB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.5.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.5.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.2.1" class="ltx_text" style="font-size:70%;">7.7M</span></td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.3.1" class="ltx_text" style="font-size:70%;">82.02</span></td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.4.1" class="ltx_text" style="font-size:70%;">36.61</span></td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.5.1" class="ltx_text" style="font-size:70%;">46.65</span></td>
<td id="S4.T2.1.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.6.1" class="ltx_text" style="font-size:70%;">60.08</span></td>
<td id="S4.T2.1.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.7.1" class="ltx_text" style="font-size:70%;">57.91</span></td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.6.6.1.1" class="ltx_text" style="font-size:70%;">MUTAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.6.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T2.1.6.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.2.1" class="ltx_text" style="font-size:70%;">4.9M</span></td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.3.1" class="ltx_text" style="font-size:70%;">81.45</span></td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.4.1" class="ltx_text" style="font-size:70%;">37.32</span></td>
<td id="S4.T2.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.5.1" class="ltx_text" style="font-size:70%;">47.17</span></td>
<td id="S4.T2.1.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.6.1" class="ltx_text" style="font-size:70%;">60.17</span></td>
<td id="S4.T2.1.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.7.1" class="ltx_text" style="font-size:70%;">58.16</span></td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.7.7.1.1" class="ltx_text" style="font-size:70%;">MUTAN+MLB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.7.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T2.1.7.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.2.1" class="ltx_text" style="font-size:70%;">17.5M</span></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.3.1" class="ltx_text" style="font-size:70%;">82.29</span></td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.4.1" class="ltx_text" style="font-size:70%;">37.27</span></td>
<td id="S4.T2.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.5.1" class="ltx_text" style="font-size:70%;">48.23</span></td>
<td id="S4.T2.1.7.7.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.6.1" class="ltx_text" style="font-size:70%;">61.02</span></td>
<td id="S4.T2.1.7.7.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.7.1" class="ltx_text" style="font-size:70%;">58.76</span></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.8.8.1.1" class="ltx_text" style="font-size:70%;">MFB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.8.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T2.1.8.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.3.1" class="ltx_text" style="font-size:70%;">81.80</span></td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.4.1" class="ltx_text" style="font-size:70%;">36.70</span></td>
<td id="S4.T2.1.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.5.1" class="ltx_text" style="font-size:70%;">51.20</span></td>
<td id="S4.T2.1.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.6.1" class="ltx_text" style="font-size:70%;">62.20</span></td>
<td id="S4.T2.1.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.9.9.1.1" class="ltx_text" style="font-size:70%;">DPPNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.9.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.T2.1.9.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.3.1" class="ltx_text" style="font-size:70%;">80.71</span></td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.4.1" class="ltx_text" style="font-size:70%;">37.23</span></td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.5.1" class="ltx_text" style="font-size:70%;">41.69</span></td>
<td id="S4.T2.1.9.9.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.6.1" class="ltx_text" style="font-size:70%;">57.22</span></td>
<td id="S4.T2.1.9.9.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.1.10.10.1.1" class="ltx_text" style="font-size:70%;">QGHC-1</span></th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.2.1" class="ltx_text" style="font-size:70%;">2.1M</span></td>
<td id="S4.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.7.1" class="ltx_text" style="font-size:70%;">58.88</span></td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.11.11.1.1" class="ltx_text" style="font-size:70%;">QGHC</span></th>
<td id="S4.T2.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.2.1" class="ltx_text" style="font-size:70%;">5.4M</span></td>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.3.1" class="ltx_text" style="font-size:70%;">82.39</span></td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">37.36</span></td>
<td id="S4.T2.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.5.1" class="ltx_text" style="font-size:70%;">53.24</span></td>
<td id="S4.T2.1.11.11.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.6.1" class="ltx_text" style="font-size:70%;">63.48</span></td>
<td id="S4.T2.1.11.11.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.7.1" class="ltx_text" style="font-size:70%;">59.24</span></td>
</tr>
<tr id="S4.T2.1.12.12" class="ltx_tr">
<th id="S4.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.12.12.1.1" class="ltx_text" style="font-size:70%;">QGHC+concat</span></th>
<td id="S4.T2.1.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.3.1" class="ltx_text" style="font-size:70%;">82.54</span></td>
<td id="S4.T2.1.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.4.1" class="ltx_text" style="font-size:70%;">36.94</span></td>
<td id="S4.T2.1.12.12.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">54.00</span></td>
<td id="S4.T2.1.12.12.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.6.1" class="ltx_text" style="font-size:70%;">63.86</span></td>
<td id="S4.T2.1.12.12.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.7.1" class="ltx_text" style="font-size:70%;">59.80</span></td>
</tr>
<tr id="S4.T2.1.13.13" class="ltx_tr">
<th id="S4.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.1.13.13.1.1" class="ltx_text" style="font-size:70%;">QGHC+MUTAN</span></th>
<td id="S4.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">82.96</span></td>
<td id="S4.T2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.4.1" class="ltx_text" style="font-size:70%;">37.16</span></td>
<td id="S4.T2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.5.1" class="ltx_text" style="font-size:70%;">53.88</span></td>
<td id="S4.T2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">64.00</span></td>
<td id="S4.T2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">60.13</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparisons of question answering accuracy of the proposed approach and the state-of-the-art methods on the VQA dataset without using the attention mechanism.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Comparison with state-of-the-art methods.</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">QGHC fuses multi-modal features in an efficient way. The output feature maps of our QGHC module utilize the textual information to guide the learning of visual features and outperform state-of-the-art feature fusion methods. In this section, we compare our proposed approach (without using the attention module) with state-of-the-arts. The results on the VQA dataset are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.1.2 Ablation studies on the VQA dataset. ‣ 4.1 VQA Dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We compare our proposed approach with multi-modal feature concatenation methods including MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, MLB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Our feature fusion is performed before the spatial pooling and can better capture the spatial information than previous methods.
Since MUTAN can be combined with MLB (denoted as MUTAN+MLB) to further improve the overall performance.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">test-dev</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">test-std</span></th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.2.2.1.1" class="ltx_text" style="font-size:70%;">Y/N</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.2.2.2.1" class="ltx_text" style="font-size:70%;">Number</span></th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Other</span></th>
<th id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.2.2.4.1" class="ltx_text" style="font-size:70%;">All</span></th>
<th id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.2.2.5.1" class="ltx_text" style="font-size:70%;">All</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.3.1.1.1" class="ltx_text" style="font-size:70%;">SMem </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S4.T3.1.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.2.1" class="ltx_text" style="font-size:70%;">80.90</span></td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.3.1" class="ltx_text" style="font-size:70%;">37.30</span></td>
<td id="S4.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.4.1" class="ltx_text" style="font-size:70%;">43.10</span></td>
<td id="S4.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.5.1" class="ltx_text" style="font-size:70%;">58.00</span></td>
<td id="S4.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.6.1" class="ltx_text" style="font-size:70%;">58.20</span></td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.4.2.1.1" class="ltx_text" style="font-size:70%;">NMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.4.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T3.1.4.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.2.2.1" class="ltx_text" style="font-size:70%;">81.20</span></td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.2.3.1" class="ltx_text" style="font-size:70%;">38.00</span></td>
<td id="S4.T3.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.2.4.1" class="ltx_text" style="font-size:70%;">44.00</span></td>
<td id="S4.T3.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.2.5.1" class="ltx_text" style="font-size:70%;">58.60</span></td>
<td id="S4.T3.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.2.6.1" class="ltx_text" style="font-size:70%;">58.70</span></td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.5.3.1.1" class="ltx_text" style="font-size:70%;">SAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.5.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.T3.1.5.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.3.2.1" class="ltx_text" style="font-size:70%;">79.30</span></td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.3.3.1" class="ltx_text" style="font-size:70%;">36.60</span></td>
<td id="S4.T3.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.3.4.1" class="ltx_text" style="font-size:70%;">46.10</span></td>
<td id="S4.T3.1.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.3.5.1" class="ltx_text" style="font-size:70%;">58.70</span></td>
<td id="S4.T3.1.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.3.6.1" class="ltx_text" style="font-size:70%;">58.90</span></td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<th id="S4.T3.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.6.4.1.1" class="ltx_text" style="font-size:70%;">MRN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.6.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.1.6.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.4.2.1" class="ltx_text" style="font-size:70%;">80.81</span></td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.4.3.1" class="ltx_text" style="font-size:70%;">35.91</span></td>
<td id="S4.T3.1.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.4.4.1" class="ltx_text" style="font-size:70%;">46.43</span></td>
<td id="S4.T3.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.4.5.1" class="ltx_text" style="font-size:70%;">59.40</span></td>
<td id="S4.T3.1.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.4.6.1" class="ltx_text" style="font-size:70%;">57.39</span></td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<th id="S4.T3.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.7.5.1.1" class="ltx_text" style="font-size:70%;">DNMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.7.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T3.1.7.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.5.2.1" class="ltx_text" style="font-size:70%;">81.10</span></td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.5.3.1" class="ltx_text" style="font-size:70%;">38.60</span></td>
<td id="S4.T3.1.7.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.5.4.1" class="ltx_text" style="font-size:70%;">45.40</span></td>
<td id="S4.T3.1.7.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.5.5.1" class="ltx_text" style="font-size:70%;">59.40</span></td>
<td id="S4.T3.1.7.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.5.6.1" class="ltx_text" style="font-size:70%;">59.40</span></td>
</tr>
<tr id="S4.T3.1.8.6" class="ltx_tr">
<th id="S4.T3.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.8.6.1.1" class="ltx_text" style="font-size:70%;">MHieCoAtt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.8.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S4.T3.1.8.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.8.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.6.2.1" class="ltx_text" style="font-size:70%;">79.70</span></td>
<td id="S4.T3.1.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.6.3.1" class="ltx_text" style="font-size:70%;">38.70</span></td>
<td id="S4.T3.1.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.6.4.1" class="ltx_text" style="font-size:70%;">51.70</span></td>
<td id="S4.T3.1.8.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.6.5.1" class="ltx_text" style="font-size:70%;">61.80</span></td>
<td id="S4.T3.1.8.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.6.6.1" class="ltx_text" style="font-size:70%;">62.10</span></td>
</tr>
<tr id="S4.T3.1.9.7" class="ltx_tr">
<th id="S4.T3.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.9.7.1.1" class="ltx_text" style="font-size:70%;">MODERN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.9.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.T3.1.9.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.9.7.2.1" class="ltx_text" style="font-size:70%;">81.38</span></td>
<td id="S4.T3.1.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.9.7.3.1" class="ltx_text" style="font-size:70%;">36.06</span></td>
<td id="S4.T3.1.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.9.7.4.1" class="ltx_text" style="font-size:70%;">51.64</span></td>
<td id="S4.T3.1.9.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.9.7.5.1" class="ltx_text" style="font-size:70%;">62.16</span></td>
<td id="S4.T3.1.9.7.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.9.7.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.1.10.8" class="ltx_tr">
<th id="S4.T3.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.10.8.1.1" class="ltx_text" style="font-size:70%;">RAU </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.10.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S4.T3.1.10.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.10.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.8.2.1" class="ltx_text" style="font-size:70%;">81.90</span></td>
<td id="S4.T3.1.10.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.8.3.1" class="ltx_text" style="font-size:70%;">39.00</span></td>
<td id="S4.T3.1.10.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.8.4.1" class="ltx_text" style="font-size:70%;">53.00</span></td>
<td id="S4.T3.1.10.8.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.8.5.1" class="ltx_text" style="font-size:70%;">63.30</span></td>
<td id="S4.T3.1.10.8.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.8.6.1" class="ltx_text" style="font-size:70%;">63.20</span></td>
</tr>
<tr id="S4.T3.1.11.9" class="ltx_tr">
<th id="S4.T3.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.11.9.1.1" class="ltx_text" style="font-size:70%;">MCB+Att </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.11.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T3.1.11.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.11.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.9.2.1" class="ltx_text" style="font-size:70%;">82.20</span></td>
<td id="S4.T3.1.11.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.9.3.1" class="ltx_text" style="font-size:70%;">37.70</span></td>
<td id="S4.T3.1.11.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.9.4.1" class="ltx_text" style="font-size:70%;">54.80</span></td>
<td id="S4.T3.1.11.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.9.5.1" class="ltx_text" style="font-size:70%;">64.20</span></td>
<td id="S4.T3.1.11.9.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.9.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.1.12.10" class="ltx_tr">
<th id="S4.T3.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.12.10.1.1" class="ltx_text" style="font-size:70%;">DAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.12.10.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S4.T3.1.12.10.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.12.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.2.1" class="ltx_text" style="font-size:70%;">83.00</span></td>
<td id="S4.T3.1.12.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.3.1" class="ltx_text" style="font-size:70%;">39.10</span></td>
<td id="S4.T3.1.12.10.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.4.1" class="ltx_text" style="font-size:70%;">53.90</span></td>
<td id="S4.T3.1.12.10.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.5.1" class="ltx_text" style="font-size:70%;">64.30</span></td>
<td id="S4.T3.1.12.10.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.6.1" class="ltx_text" style="font-size:70%;">64.20</span></td>
</tr>
<tr id="S4.T3.1.13.11" class="ltx_tr">
<th id="S4.T3.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.13.11.1.1" class="ltx_text" style="font-size:70%;">MFB+Att </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.13.11.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T3.1.13.11.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.13.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.2.1" class="ltx_text" style="font-size:70%;">82.50</span></td>
<td id="S4.T3.1.13.11.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.3.1" class="ltx_text" style="font-size:70%;">38.30</span></td>
<td id="S4.T3.1.13.11.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.4.1" class="ltx_text" style="font-size:70%;">55.20</span></td>
<td id="S4.T3.1.13.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.5.1" class="ltx_text" style="font-size:70%;">64.60</span></td>
<td id="S4.T3.1.13.11.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.1.14.12" class="ltx_tr">
<th id="S4.T3.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.14.12.1.1" class="ltx_text" style="font-size:70%;">EENMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.14.12.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S4.T3.1.14.12.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.14.12.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.14.12.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T3.1.14.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.14.12.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T3.1.14.12.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.14.12.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T3.1.14.12.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.14.12.5.1" class="ltx_text" style="font-size:70%;">64.90</span></td>
<td id="S4.T3.1.14.12.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.14.12.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.1.15.13" class="ltx_tr">
<th id="S4.T3.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.15.13.1.1" class="ltx_text" style="font-size:70%;">MLB+Att </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.15.13.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T3.1.15.13.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.15.13.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.13.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">84.02</span></td>
<td id="S4.T3.1.15.13.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.13.3.1" class="ltx_text" style="font-size:70%;">37.90</span></td>
<td id="S4.T3.1.15.13.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.13.4.1" class="ltx_text" style="font-size:70%;">54.77</span></td>
<td id="S4.T3.1.15.13.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.13.5.1" class="ltx_text" style="font-size:70%;">65.08</span></td>
<td id="S4.T3.1.15.13.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.13.6.1" class="ltx_text" style="font-size:70%;">65.07</span></td>
</tr>
<tr id="S4.T3.1.16.14" class="ltx_tr">
<th id="S4.T3.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.16.14.1.1" class="ltx_text" style="font-size:70%;">MFB+CoAtt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.16.14.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T3.1.16.14.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T3.1.16.14.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.14.2.1" class="ltx_text" style="font-size:70%;">83.20</span></td>
<td id="S4.T3.1.16.14.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.14.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">38.80</span></td>
<td id="S4.T3.1.16.14.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.14.4.1" class="ltx_text" style="font-size:70%;">55.50</span></td>
<td id="S4.T3.1.16.14.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.14.5.1" class="ltx_text" style="font-size:70%;">65.10</span></td>
<td id="S4.T3.1.16.14.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.14.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.1.17.15" class="ltx_tr">
<th id="S4.T3.1.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.1.1" class="ltx_text" style="font-size:70%;">QGHC+Att+Concat</span></th>
<td id="S4.T3.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.2.1" class="ltx_text" style="font-size:70%;">83.54</span></td>
<td id="S4.T3.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.3.1" class="ltx_text" style="font-size:70%;">38.06</span></td>
<td id="S4.T3.1.17.15.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">57.10</span></td>
<td id="S4.T3.1.17.15.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">65.89</span></td>
<td id="S4.T3.1.17.15.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.17.15.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">65.90</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparisons of question answering accuracy of the proposed approach and the state-of-the-art methods on the VQA dataset with the attention mechanism.</figcaption>
</figure>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Attention mechanism is widely utilized in VQA algorithms for associating words with image regions.
Our method can be combined with attention models for predicting more accurate answers. In Section <a href="#S3.SS5" title="3.5 QGHC network with bilinear pooling and attention ‣ 3 Visual Question Answering with Question-guided Hybrid Convolution ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>, we adopt a simple attention implementation. More complex attention mechanisms, such as hierachical attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and stacked attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> can also be combined with our approach.
The results in Table <a href="#S4.T3" title="Table 3 ‣ 4.1.3 Comparison with state-of-the-art methods. ‣ 4.1 VQA Dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> list the answering accuracies on the VQA dataset of different state-of-the-art methods with attention mechanism.</p>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">We also compare our method with dynamic parameter prediction methods. DPPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> (Table <a href="#S4.T2" title="Table 2 ‣ 4.1.2 Ablation studies on the VQA dataset. ‣ 4.1 VQA Dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and MODERN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (Table <a href="#S4.T3" title="Table 3 ‣ 4.1.3 Comparison with state-of-the-art methods. ‣ 4.1 VQA Dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) are two state-of-the-art dynamic learning methods.
Compared with DPPNet(VGG) and MODERN(ResNet-152), QGHC improves the performance by 6.78% and 3.73% respectively on the test-dev subset, which demonstrates the effectiveness of our QGHC model.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>CLEVR dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The CLEVR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is proposed to test the reasoning ability of VQA tasks, such as counting, comparing, and logical reasoning. Questions and images from CLEVR are generated by a simulation engine that randomly combines 3D objects. This dataset contains 699,989 training questions, 149,991 validation questions, and 149,988 test questions.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Experimental setting.</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.2" class="ltx_p">In our proposed model, the image is resized to <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><times id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">224\times 224</annotation></semantics></math>. The question is first embedded to a 300-d vector through a FC layer followed by a ReLU non-linear function, and then input into a 2-layer LSTM with 256 hidden states to generate textual features. Our QGHC network contains three QGHC modules for fusing multi-modal information. All parameters are learned from scratch and trained in an end-to-end manner. The network is trained using the ADAM optimizer with the learning rate <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mrow id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS1.p1.2.m2.1.1.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.p1.2.m2.1.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS1.p1.2.m2.1.1.3" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mn id="S4.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.SSS1.p1.2.m2.1.1.3.3a" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS1.p1.2.m2.1.1.3.3.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><apply id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1"><times id="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.2">5</cn><apply id="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3"><minus id="S4.SS2.SSS1.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">5\times 10^{-4}</annotation></semantics></math> and batch size 64. All the results are reported on the validation subset.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<table id="S4.F4.8" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F4.8.8" class="ltx_tr">
<td id="S4.F4.8.8.9" class="ltx_td"></td>
<td id="S4.F4.8.8.8" class="ltx_td ltx_align_center">
<table id="S4.F4.8.8.8.8" class="ltx_tabular ltx_align_middle">
<tr id="S4.F4.4.4.4.4.4" class="ltx_tr">
<td id="S4.F4.1.1.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1808.02632/assets/images/CLEVR_val_000085.png" id="S4.F4.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></td>
<td id="S4.F4.2.2.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.2.2.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.2.2.2.2.2.2.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAMyellow.jpg" id="S4.F4.2.2.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F4.3.3.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.3.3.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.3.3.3.3.3.3.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAMpurple.jpg" id="S4.F4.3.3.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F4.4.4.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.4.4.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.4.4.4.4.4.4.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAMgreen.jpg" id="S4.F4.4.4.4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F4.8.8.8.8.9" class="ltx_tr">
<td id="S4.F4.8.8.8.8.9.1" class="ltx_td"></td>
<td id="S4.F4.8.8.8.8.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.9.2.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.9.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.9.2.1.1.2" class="ltx_text" style="font-size:80%;">: What shape is the </span><span id="S4.F4.8.8.8.8.9.2.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.9.2.1.1.3.1" class="ltx_emph ltx_font_italic">yellow</em></span><span id="S4.F4.8.8.8.8.9.2.1.1.4" class="ltx_text" style="font-size:80%;"> thing?</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.9.3.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.9.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.9.3.1.1.2" class="ltx_text" style="font-size:80%;">: What shape is the </span><span id="S4.F4.8.8.8.8.9.3.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.9.3.1.1.3.1" class="ltx_emph ltx_font_italic">purple</em></span><span id="S4.F4.8.8.8.8.9.3.1.1.4" class="ltx_text" style="font-size:80%;"> thing?</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.9.4.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.9.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.9.4.1.1.2" class="ltx_text" style="font-size:80%;">: What shape is the </span><span id="S4.F4.8.8.8.8.9.4.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.9.4.1.1.3.1" class="ltx_emph ltx_font_italic">green</em></span><span id="S4.F4.8.8.8.8.9.4.1.1.4" class="ltx_text" style="font-size:80%;"> thing?</span></span>
</span>
</td>
</tr>
<tr id="S4.F4.8.8.8.8.10" class="ltx_tr">
<td id="S4.F4.8.8.8.8.10.1" class="ltx_td"></td>
<td id="S4.F4.8.8.8.8.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.10.2.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.10.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.10.2.1.1.2" class="ltx_text" style="font-size:80%;">: cube</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.10.3.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.10.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.10.3.1.1.2" class="ltx_text" style="font-size:80%;">: sphere</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.10.4.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.10.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.10.4.1.1.2" class="ltx_text" style="font-size:80%;">: cube</span></span>
</span>
</td>
</tr>
<tr id="S4.F4.8.8.8.8.8" class="ltx_tr">
<td id="S4.F4.5.5.5.5.5.1" class="ltx_td ltx_align_center"><img src="/html/1808.02632/assets/images/CLEVR_val_010001.png" id="S4.F4.5.5.5.5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></td>
<td id="S4.F4.6.6.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.6.6.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.6.6.6.6.6.2.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAM1.jpg" id="S4.F4.6.6.6.6.6.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F4.7.7.7.7.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.7.7.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.7.7.7.7.7.3.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAM2.jpg" id="S4.F4.7.7.7.7.7.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.8.4.1.1" class="ltx_p" style="width:83.1pt;"><img src="/html/1808.02632/assets/images/CAM3.jpg" id="S4.F4.8.8.8.8.8.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="115" height="77" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F4.8.8.8.8.11" class="ltx_tr">
<td id="S4.F4.8.8.8.8.11.1" class="ltx_td"></td>
<td id="S4.F4.8.8.8.8.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.11.2.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.11.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.11.2.1.1.2" class="ltx_text" style="font-size:80%;">: What </span><span id="S4.F4.8.8.8.8.11.2.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.11.2.1.1.3.1" class="ltx_emph ltx_font_italic">number</em></span><span id="S4.F4.8.8.8.8.11.2.1.1.4" class="ltx_text" style="font-size:80%;"> of things are rubber in front of the tiny matte cylinder or
big purple things</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.11.3.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.11.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.11.3.1.1.2" class="ltx_text" style="font-size:80%;">: The large cylinder that is the same
material as the purple is what </span><span id="S4.F4.8.8.8.8.11.3.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.11.3.1.1.3.1" class="ltx_emph ltx_font_italic">color</em></span><span id="S4.F4.8.8.8.8.11.3.1.1.4" class="ltx_text" style="font-size:80%;">?</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.11.4.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.11.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q</span><span id="S4.F4.8.8.8.8.11.4.1.1.2" class="ltx_text" style="font-size:80%;">: How many </span><span id="S4.F4.8.8.8.8.11.4.1.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;"><em id="S4.F4.8.8.8.8.11.4.1.1.3.1" class="ltx_emph ltx_font_italic">green</em></span><span id="S4.F4.8.8.8.8.11.4.1.1.4" class="ltx_text" style="font-size:80%;"> things?</span></span>
</span>
</td>
</tr>
<tr id="S4.F4.8.8.8.8.12" class="ltx_tr">
<td id="S4.F4.8.8.8.8.12.1" class="ltx_td"></td>
<td id="S4.F4.8.8.8.8.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.12.2.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.12.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.12.2.1.1.2" class="ltx_text" style="font-size:80%;">: 3</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.12.3.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.12.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.12.3.1.1.2" class="ltx_text" style="font-size:80%;">: Red</span></span>
</span>
</td>
<td id="S4.F4.8.8.8.8.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F4.8.8.8.8.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.8.12.4.1.1" class="ltx_p" style="width:83.1pt;"><span id="S4.F4.8.8.8.8.12.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A</span><span id="S4.F4.8.8.8.8.12.4.1.1.2" class="ltx_text" style="font-size:80%;">: 2</span></span>
</span>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of answer activation maps generate by the QGHC.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Comparison with state-of-the-arts.</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We compare our model with the following methods. <em id="S4.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">CNN-LSTM</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> encodes images and questions using CNN and LSTM respectively. The encoded image features and question features are concatenated and then passed through a MLP to predict the final answers. <em id="S4.SS2.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">Multimodal Compact Bilinear Pooling (MCB)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> fuses textual and visual feature by compact bilinear pooling which captures the high level interaction between images and questions.
<em id="S4.SS2.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">Stacked Attention (SA)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> adopts multiple attention models to refine the fusion results and utilizes linear transformations to obtain the attention maps. MCB and SA could be combined with the above CNN-LSTM method.
<em id="S4.SS2.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">Neural Module Network (NMN)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> propose a sentence parsing method and a dynamic neural network. However, sentence parsing might fail in practice and lead to bad network structure.
<em id="S4.SS2.SSS2.p1.1.5" class="ltx_emph ltx_font_italic">End-to-end Neural Module Network (N2NMN)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> learns to parse the question and predicts the answer distribution using dynamic network structure.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The results of different methods on the CLEVR dataset are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.2.2 Comparison with state-of-the-arts. ‣ 4.2 CLEVR dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The multi-modal concatenation (CNN-LSTM) does not perform well, since it cannot model the complex interactions between images and questions. Stacked Attention (+SA) can improve the results since it utilizes the spatial information from input images. Our QGHC model still outperforms +SA by 17.40%.
For the N2NMN, it parses the input question to dynamically predict the network structure. Our proposed method outperforms it by 2.20%.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:42.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.2pt,68.7pt) scale(0.725061453378036,0.237106238471137) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T4.1.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Compare integers</span></th>
<th id="S4.T4.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T4.1.1.1.1.6.1" class="ltx_text" style="font-size:80%;">Query attribute</span></th>
<th id="S4.T4.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T4.1.1.1.1.7.1" class="ltx_text" style="font-size:80%;">Compare attribute</span></th>
</tr>
<tr id="S4.T4.1.1.2.2" class="ltx_tr">
<th id="S4.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T4.1.1.2.2.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.1.2.2.2.1" class="ltx_text" style="font-size:80%;">Overall</span></th>
<th id="S4.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.1.2.2.3.1" class="ltx_text" style="font-size:80%;">Exist</span></th>
<th id="S4.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.1.2.2.4.1" class="ltx_text" style="font-size:80%;">Count</span></th>
<th id="S4.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.5.1" class="ltx_text" style="font-size:80%;">equal</span></th>
<th id="S4.T4.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.6.1" class="ltx_text" style="font-size:80%;">less</span></th>
<th id="S4.T4.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.7.1" class="ltx_text" style="font-size:80%;">more</span></th>
<th id="S4.T4.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.8.1" class="ltx_text" style="font-size:80%;">size</span></th>
<th id="S4.T4.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.9.1" class="ltx_text" style="font-size:80%;">color</span></th>
<th id="S4.T4.1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.10.1" class="ltx_text" style="font-size:80%;">material</span></th>
<th id="S4.T4.1.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.11.1" class="ltx_text" style="font-size:80%;">shape</span></th>
<th id="S4.T4.1.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.12.1" class="ltx_text" style="font-size:80%;">size</span></th>
<th id="S4.T4.1.1.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.13.1" class="ltx_text" style="font-size:80%;">color</span></th>
<th id="S4.T4.1.1.2.2.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.14.1" class="ltx_text" style="font-size:80%;">material</span></th>
<th id="S4.T4.1.1.2.2.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.2.2.15.1" class="ltx_text" style="font-size:80%;">shape</span></th>
</tr>
<tr id="S4.T4.1.1.3.3" class="ltx_tr">
<th id="S4.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T4.1.1.3.3.1.1" class="ltx_text" style="font-size:80%;">Human </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.3.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S4.T4.1.1.3.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S4.T4.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.2.1" class="ltx_text" style="font-size:80%;">92.60</span></th>
<th id="S4.T4.1.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.3.1" class="ltx_text" style="font-size:80%;">96.60</span></th>
<th id="S4.T4.1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.4.1" class="ltx_text" style="font-size:80%;">86.70</span></th>
<th id="S4.T4.1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.5.1" class="ltx_text" style="font-size:80%;">79.00</span></th>
<th id="S4.T4.1.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.6.1" class="ltx_text" style="font-size:80%;">87.00</span></th>
<th id="S4.T4.1.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.7.1" class="ltx_text" style="font-size:80%;">91.00</span></th>
<th id="S4.T4.1.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.8.1" class="ltx_text" style="font-size:80%;">97.00</span></th>
<th id="S4.T4.1.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.9.1" class="ltx_text" style="font-size:80%;">95.00</span></th>
<th id="S4.T4.1.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.10.1" class="ltx_text" style="font-size:80%;">94.00</span></th>
<th id="S4.T4.1.1.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.11.1" class="ltx_text" style="font-size:80%;">94.00</span></th>
<th id="S4.T4.1.1.3.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.12.1" class="ltx_text" style="font-size:80%;">94.00</span></th>
<th id="S4.T4.1.1.3.3.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.13.1" class="ltx_text" style="font-size:80%;">98.00</span></th>
<th id="S4.T4.1.1.3.3.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.14.1" class="ltx_text" style="font-size:80%;">96.00</span></th>
<th id="S4.T4.1.1.3.3.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.1.3.3.15.1" class="ltx_text" style="font-size:80%;">96.00</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.4.1" class="ltx_tr">
<th id="S4.T4.1.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T4.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">CNN-LSTM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.4.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S4.T4.1.1.4.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.2.1" class="ltx_text" style="font-size:80%;">52.30</span></td>
<td id="S4.T4.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.3.1" class="ltx_text" style="font-size:80%;">65.20</span></td>
<td id="S4.T4.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.4.1" class="ltx_text" style="font-size:80%;">43.70</span></td>
<td id="S4.T4.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.5.1" class="ltx_text" style="font-size:80%;">57.00</span></td>
<td id="S4.T4.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.6.1" class="ltx_text" style="font-size:80%;">72.00</span></td>
<td id="S4.T4.1.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.7.1" class="ltx_text" style="font-size:80%;">69.00</span></td>
<td id="S4.T4.1.1.4.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.8.1" class="ltx_text" style="font-size:80%;">59.00</span></td>
<td id="S4.T4.1.1.4.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.9.1" class="ltx_text" style="font-size:80%;">32.00</span></td>
<td id="S4.T4.1.1.4.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.10.1" class="ltx_text" style="font-size:80%;">58.00</span></td>
<td id="S4.T4.1.1.4.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.11.1" class="ltx_text" style="font-size:80%;">48.00</span></td>
<td id="S4.T4.1.1.4.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.12.1" class="ltx_text" style="font-size:80%;">54.00</span></td>
<td id="S4.T4.1.1.4.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.13.1" class="ltx_text" style="font-size:80%;">54.00</span></td>
<td id="S4.T4.1.1.4.1.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.14.1" class="ltx_text" style="font-size:80%;">51.00</span></td>
<td id="S4.T4.1.1.4.1.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.4.1.15.1" class="ltx_text" style="font-size:80%;">53.00</span></td>
</tr>
<tr id="S4.T4.1.1.5.2" class="ltx_tr">
<th id="S4.T4.1.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T4.1.1.5.2.1.1" class="ltx_text" style="font-size:80%;">+MCB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.5.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T4.1.1.5.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.5.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.2.1" class="ltx_text" style="font-size:80%;">51.40</span></td>
<td id="S4.T4.1.1.5.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.3.1" class="ltx_text" style="font-size:80%;">63.40</span></td>
<td id="S4.T4.1.1.5.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.4.1" class="ltx_text" style="font-size:80%;">42.10</span></td>
<td id="S4.T4.1.1.5.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.5.1" class="ltx_text" style="font-size:80%;">57.00</span></td>
<td id="S4.T4.1.1.5.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.6.1" class="ltx_text" style="font-size:80%;">71.00</span></td>
<td id="S4.T4.1.1.5.2.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.7.1" class="ltx_text" style="font-size:80%;">68.00</span></td>
<td id="S4.T4.1.1.5.2.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.8.1" class="ltx_text" style="font-size:80%;">59.00</span></td>
<td id="S4.T4.1.1.5.2.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.9.1" class="ltx_text" style="font-size:80%;">32.00</span></td>
<td id="S4.T4.1.1.5.2.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.10.1" class="ltx_text" style="font-size:80%;">57.00</span></td>
<td id="S4.T4.1.1.5.2.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.11.1" class="ltx_text" style="font-size:80%;">48.00</span></td>
<td id="S4.T4.1.1.5.2.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.12.1" class="ltx_text" style="font-size:80%;">51.00</span></td>
<td id="S4.T4.1.1.5.2.13" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.13.1" class="ltx_text" style="font-size:80%;">52.00</span></td>
<td id="S4.T4.1.1.5.2.14" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.14.1" class="ltx_text" style="font-size:80%;">50.00</span></td>
<td id="S4.T4.1.1.5.2.15" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.2.15.1" class="ltx_text" style="font-size:80%;">51.00</span></td>
</tr>
<tr id="S4.T4.1.1.6.3" class="ltx_tr">
<th id="S4.T4.1.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T4.1.1.6.3.1.1" class="ltx_text" style="font-size:80%;">+SA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.6.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.T4.1.1.6.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.6.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.2.1" class="ltx_text" style="font-size:80%;">68.50</span></td>
<td id="S4.T4.1.1.6.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.3.1" class="ltx_text" style="font-size:80%;">71.10</span></td>
<td id="S4.T4.1.1.6.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.4.1" class="ltx_text" style="font-size:80%;">52.2</span></td>
<td id="S4.T4.1.1.6.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.5.1" class="ltx_text" style="font-size:80%;">60.00</span></td>
<td id="S4.T4.1.1.6.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.6.1" class="ltx_text" style="font-size:80%;">82.00</span></td>
<td id="S4.T4.1.1.6.3.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.7.1" class="ltx_text" style="font-size:80%;">74.00</span></td>
<td id="S4.T4.1.1.6.3.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.8.1" class="ltx_text" style="font-size:80%;">87.00</span></td>
<td id="S4.T4.1.1.6.3.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.9.1" class="ltx_text" style="font-size:80%;">81.00</span></td>
<td id="S4.T4.1.1.6.3.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.10.1" class="ltx_text" style="font-size:80%;">88.00</span></td>
<td id="S4.T4.1.1.6.3.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.11.1" class="ltx_text" style="font-size:80%;">85.00</span></td>
<td id="S4.T4.1.1.6.3.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.12.1" class="ltx_text" style="font-size:80%;">52.00</span></td>
<td id="S4.T4.1.1.6.3.13" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.13.1" class="ltx_text" style="font-size:80%;">55.00</span></td>
<td id="S4.T4.1.1.6.3.14" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.14.1" class="ltx_text" style="font-size:80%;">51.00</span></td>
<td id="S4.T4.1.1.6.3.15" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.3.15.1" class="ltx_text" style="font-size:80%;">51.00</span></td>
</tr>
<tr id="S4.T4.1.1.7.4" class="ltx_tr">
<th id="S4.T4.1.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T4.1.1.7.4.1.1" class="ltx_text" style="font-size:80%;">NMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.7.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T4.1.1.7.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.7.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.2.1" class="ltx_text" style="font-size:80%;">72.10</span></td>
<td id="S4.T4.1.1.7.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.3.1" class="ltx_text" style="font-size:80%;">79.30</span></td>
<td id="S4.T4.1.1.7.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.4.1" class="ltx_text" style="font-size:80%;">52.50</span></td>
<td id="S4.T4.1.1.7.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.5.1" class="ltx_text" style="font-size:80%;">61.20</span></td>
<td id="S4.T4.1.1.7.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.6.1" class="ltx_text" style="font-size:80%;">77.90</span></td>
<td id="S4.T4.1.1.7.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.7.1" class="ltx_text" style="font-size:80%;">75.20</span></td>
<td id="S4.T4.1.1.7.4.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.8.1" class="ltx_text" style="font-size:80%;">84.20</span></td>
<td id="S4.T4.1.1.7.4.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.9.1" class="ltx_text" style="font-size:80%;">68.90</span></td>
<td id="S4.T4.1.1.7.4.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.10.1" class="ltx_text" style="font-size:80%;">82.60</span></td>
<td id="S4.T4.1.1.7.4.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.11.1" class="ltx_text" style="font-size:80%;">80.20</span></td>
<td id="S4.T4.1.1.7.4.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.12.1" class="ltx_text" style="font-size:80%;">80.70</span></td>
<td id="S4.T4.1.1.7.4.13" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.13.1" class="ltx_text" style="font-size:80%;">74.40</span></td>
<td id="S4.T4.1.1.7.4.14" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.14.1" class="ltx_text" style="font-size:80%;">77.60</span></td>
<td id="S4.T4.1.1.7.4.15" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.7.4.15.1" class="ltx_text" style="font-size:80%;">79.30</span></td>
</tr>
<tr id="S4.T4.1.1.8.5" class="ltx_tr">
<th id="S4.T4.1.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T4.1.1.8.5.1.1" class="ltx_text" style="font-size:80%;">N2NMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.8.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S4.T4.1.1.8.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.8.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.2.1" class="ltx_text" style="font-size:80%;">83.70</span></td>
<td id="S4.T4.1.1.8.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.3.1" class="ltx_text" style="font-size:80%;">85.70</span></td>
<td id="S4.T4.1.1.8.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.4.1" class="ltx_text" style="font-size:80%;">68.50</span></td>
<td id="S4.T4.1.1.8.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.5.1" class="ltx_text" style="font-size:80%;">73.80</span></td>
<td id="S4.T4.1.1.8.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.6.1" class="ltx_text" style="font-size:80%;">89.70</span></td>
<td id="S4.T4.1.1.8.5.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.7.1" class="ltx_text" style="font-size:80%;">87.70</span></td>
<td id="S4.T4.1.1.8.5.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.8.1" class="ltx_text" style="font-size:80%;">93.10</span></td>
<td id="S4.T4.1.1.8.5.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.9.1" class="ltx_text" style="font-size:80%;">84.50</span></td>
<td id="S4.T4.1.1.8.5.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.10.1" class="ltx_text" style="font-size:80%;">91.50</span></td>
<td id="S4.T4.1.1.8.5.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.11.1" class="ltx_text" style="font-size:80%;">90.60</span></td>
<td id="S4.T4.1.1.8.5.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.12.1" class="ltx_text" style="font-size:80%;">92.60</span></td>
<td id="S4.T4.1.1.8.5.13" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.13.1" class="ltx_text" style="font-size:80%;">82.80</span></td>
<td id="S4.T4.1.1.8.5.14" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.14.1" class="ltx_text" style="font-size:80%;">89.60</span></td>
<td id="S4.T4.1.1.8.5.15" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.5.15.1" class="ltx_text" style="font-size:80%;">90.00</span></td>
</tr>
<tr id="S4.T4.1.1.9.6" class="ltx_tr">
<th id="S4.T4.1.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T4.1.1.9.6.1.1" class="ltx_text" style="font-size:80%;">FiLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.9.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S4.T4.1.1.9.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T4.1.1.9.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.6.2.1" class="ltx_text" style="font-size:80%;">97.7</span></td>
<td id="S4.T4.1.1.9.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.6.3.1" class="ltx_text" style="font-size:80%;">99.1</span></td>
<td id="S4.T4.1.1.9.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.6.4.1" class="ltx_text" style="font-size:80%;">94.3</span></td>
<td id="S4.T4.1.1.9.6.5" class="ltx_td ltx_align_center" colspan="3"><span id="S4.T4.1.1.9.6.5.1" class="ltx_text" style="font-size:80%;">96.8</span></td>
<td id="S4.T4.1.1.9.6.6" class="ltx_td ltx_align_center" colspan="4"><span id="S4.T4.1.1.9.6.6.1" class="ltx_text" style="font-size:80%;">99.1</span></td>
<td id="S4.T4.1.1.9.6.7" class="ltx_td ltx_align_center" colspan="4"><span id="S4.T4.1.1.9.6.7.1" class="ltx_text" style="font-size:80%;">99.1</span></td>
</tr>
<tr id="S4.T4.1.1.10.7" class="ltx_tr">
<th id="S4.T4.1.1.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.1.1" class="ltx_text" style="font-size:80%;">QGHC(ours)</span></th>
<td id="S4.T4.1.1.10.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.2.1" class="ltx_text" style="font-size:80%;">86.30</span></td>
<td id="S4.T4.1.1.10.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.3.1" class="ltx_text" style="font-size:80%;">78.10</span></td>
<td id="S4.T4.1.1.10.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.4.1" class="ltx_text" style="font-size:80%;">91.17</span></td>
<td id="S4.T4.1.1.10.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.5.1" class="ltx_text" style="font-size:80%;">67.30</span></td>
<td id="S4.T4.1.1.10.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.6.1" class="ltx_text" style="font-size:80%;">87.14</span></td>
<td id="S4.T4.1.1.10.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.7.1" class="ltx_text" style="font-size:80%;">83.28</span></td>
<td id="S4.T4.1.1.10.7.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.8.1" class="ltx_text" style="font-size:80%;">93.65</span></td>
<td id="S4.T4.1.1.10.7.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.9.1" class="ltx_text" style="font-size:80%;">87.86</span></td>
<td id="S4.T4.1.1.10.7.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.10.1" class="ltx_text" style="font-size:80%;">86.75</span></td>
<td id="S4.T4.1.1.10.7.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.11.1" class="ltx_text" style="font-size:80%;">90.70</span></td>
<td id="S4.T4.1.1.10.7.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.12.1" class="ltx_text" style="font-size:80%;">86.24</span></td>
<td id="S4.T4.1.1.10.7.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.13.1" class="ltx_text" style="font-size:80%;">87.24</span></td>
<td id="S4.T4.1.1.10.7.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.14.1" class="ltx_text" style="font-size:80%;">86.75</span></td>
<td id="S4.T4.1.1.10.7.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.7.15.1" class="ltx_text" style="font-size:80%;">86.93</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparisons of question answering accuracy of the proposed approach and the state-of-the-art methods on the CLVER dataset.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Visualization of question-guided convolution</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Motivated by the class activation mapping (CAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we visualize the activation maps of the output feature maps generated by the QGHC modules. The weighted summation of the topmost feature maps can localize answer regions.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Convolution activation maps for our last QGHC module are shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2.1 Experimental setting. ‣ 4.2 CLEVR dataset ‣ 4 Experiments ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We can observe that the activation regions relate to the questions and the answers are predicted correctly for different types of questions, including shape, color, and number. In addition, we also visualize the activation maps of different QGHC modules by training an answer prediction FC layer for each of them.
As examples shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Question-Guided Hybrid Convolution for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the QGHC gradually focus on the correct regions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose a question-guided hybrid convolution for learning discriminative multi-modal feature representations. Our approach fully utilizes the spatial information and is able to capture complex relations between the image and question. By introducing the question-guided group convolution kernels with both dynamically-predicted and freely-updated kernels, the proposed QGHC network shows strong capability on solving the visual question answering problem. The proposed approach is complementary with existing feature fusion methods and attention mechanisms. Extensive experiments demonstrate the effectiveness of our QGHC network and its individual components.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work is supported by SenseTime Group Limited, the General Research Fund sponsored by the Research Grants Council of Hong Kong (Nos. CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015, CUHK14239816, CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217), the Hong Kong Innovation and Technology Support Program (No.ITS/121/15FX), the National Research Foundation, Prime Minister’s Office, Singapore under its International Research Centres in Singapore Funding Initiative.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.:

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2012)
1097–1105

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Sutskever, I., Vinyals, O., Le, Q.V.:

</span>
<span class="ltx_bibblock">Sequence to sequence learning with neural networks.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2014)
3104–3112

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Li, Y., Ouyang, W., Zhou, B., Wang, K., Wang, X.:

</span>
<span class="ltx_bibblock">Scene graph generation from objects, phrases and region captions.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2017) 1261–1270

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,
Bengio, Y.:

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In: International Conference on Machine Learning. (2015) 2048–2057

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.:

</span>
<span class="ltx_bibblock">Natural language object retrieval.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 4555–4564

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C.,
Parikh, D.:

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision. (2015) 2425–2433

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T.,
et al.:

</span>
<span class="ltx_bibblock">Devise: A deep visual-semantic embedding model.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2013)
2121–2129

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Reed, S., Akata, Z., Lee, H., Schiele, B.:

</span>
<span class="ltx_bibblock">Learning deep representations of fine-grained visual descriptions.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 49–58

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Zhou, B., Tian, Y., Sukhbaatar, S., Szlam, A., Fergus, R.:

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1512.02167 (2015)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.:

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1606.01847 (2016)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kim, J.H., On, K.W., Kim, J., Ha, J.W., Zhang, B.T.:

</span>
<span class="ltx_bibblock">Hadamard product for low-rank bilinear pooling.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1610.04325 (2016)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ben-younes, H., Cadene, R., Cord, M., Thome, N.:

</span>
<span class="ltx_bibblock">Mutan: Multimodal tucker fusion for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1705.06676 (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chollet, F.:

</span>
<span class="ltx_bibblock">Xception: Deep learning with depthwise separable convolutions.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1610.02357 (2016)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.:

</span>
<span class="ltx_bibblock">Aggregated residual transformations for deep neural networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1611.05431 (2016)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lin, T.Y., RoyChowdhury, A., Maji, S.:

</span>
<span class="ltx_bibblock">Bilinear cnn models for fine-grained visual recognition.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision. (2015) 1449–1457

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Bahdanau, D., Cho, K., Bengio, Y.:

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1409.0473 (2014)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Xu, H., Saenko, K.:

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision, Springer (2016) 451–466

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yang, Z., He, X., Gao, J., Deng, L., Smola, A.:

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 21–29

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., Parikh, D.:

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In: Advances In Neural Information Processing Systems. (2016)
289–297

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lu, P., Li, H., Zhang, W., Wang, J., Wang, X.:

</span>
<span class="ltx_bibblock">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering.

</span>
<span class="ltx_bibblock">In: Proceedings of AAAI. (2018) 7218-7225

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Chen, K., Wang, J., Chen, L.C., Gao, H., Xu, W., Nevatia, R.:

</span>
<span class="ltx_bibblock">Abc-cnn: An attention based convolutional neural network for visual
question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05960 (2015)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Noh, H., Hongsuck Seo, P., Han, B.:

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with
dynamic parameter prediction.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 30–38

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
de Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., Courville, A.:

</span>
<span class="ltx_bibblock">Modulating early visual processing by language.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1707.00683 (2017)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Li, Z., Tao, R., Gavves, E., Snoek, C.G., Smeulders, A., et al.:

</span>
<span class="ltx_bibblock">Tracking by natural language specification.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2017) 6495–6503

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhang, X., Zhou, X., Lin, M., Sun, J.:

</span>
<span class="ltx_bibblock">Shufflenet: An extremely efficient convolutional neural network for
mobile devices.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1707.01083 (2017)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Li Shuang, Xiao Tong, Li Hongsheng, Yang Wei, and Wang Xiaogang:

</span>
<span class="ltx_bibblock">Identity-aware textual-visual matching with latent co-attention.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Computer Vision. (2017)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.:

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE conference on computer vision and pattern
recognition. (2016) 770–778

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Salimans, T., Kingma, D.P.:

</span>
<span class="ltx_bibblock">Weight normalization: A simple reparameterization to accelerate
training of deep neural networks.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems. (2016)
901–909

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L.,
Girshick, R.:

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1612.06890 (2016)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kiros, R., Zhu, Y., Salakhutdinov, R.R., Zemel, R., Urtasun, R., Torralba, A.,
Fidler, S.:

</span>
<span class="ltx_bibblock">Skip-thought vectors.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2015)
3294–3302

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.:

</span>
<span class="ltx_bibblock">Learning deep features for discriminative localization.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 2921–2929

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Fan, J., Tao, D.:

</span>
<span class="ltx_bibblock">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Li Shuang, Xiao Tong, Li Hongsheng, Zhou Bolei, Yue Dayu, and Wang Xiaogang:

</span>
<span class="ltx_bibblock">Person search with natural language description.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition. (2017)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M., Darrell, T., Klein, D.:

</span>
<span class="ltx_bibblock">Neural module networks.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 39–48

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kim, J.H., Lee, S.W., Kwak, D., Heo, M.O., Kim, J., Ha, J.W., Zhang, B.T.:

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual qa.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems. (2016)
361–369

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M., Darrell, T., Klein, D.:

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">In: Proceedings of NAACL-HLT. (2016) 1545–1554

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Noh, H., Han, B.:

</span>
<span class="ltx_bibblock">Training recurrent answering units with joint loss minimization for
vqa.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1606.03647 (2016)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lu, P., Ji, L., Zhang, W., Duan, N., Zhou, M., Wang, J.:

</span>
<span class="ltx_bibblock">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering.

</span>
<span class="ltx_bibblock">In: Proceedings of SIGKDD. (2018)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Li Shuang, Bak Slawomir, Carr Peter, and Wang Xiaogang:

</span>
<span class="ltx_bibblock">Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition. (2018)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nam, H., Ha, J.W., Kim, J.:

</span>
<span class="ltx_bibblock">Dual attention networks for multimodal reasoning and matching.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1611.00471 (2016)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hu, R., Andreas, J., Rohrbach, M., Darrell, T., Saenko, K.:

</span>
<span class="ltx_bibblock">Learning to reason: End-to-end module networks for visual question
answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1704.05526 (2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Johnson, J., Hariharan, B., van der Maaten, L., Hoffman, J., Fei-Fei, L.,
Zitnick, C.L., Girshick, R.:

</span>
<span class="ltx_bibblock">Inferring and executing programs for visual reasoning.

</span>
<span class="ltx_bibblock">In: ICCV. (2017)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron:

</span>
<span class="ltx_bibblock">Film: Visual reasoning with a general conditioning layer.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1709.07871 (2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1808.02631" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1808.02632" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1808.02632">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1808.02632" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1808.02633" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar 15 19:39:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
