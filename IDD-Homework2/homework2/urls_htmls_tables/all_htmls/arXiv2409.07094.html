<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deep intra-operative illumination calibration of hyperspectral cameras</title>
<!--Generated on Wed Sep 11 07:18:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="deep learning,  hyperspectral imaging,  intra-operative imaging,  illumination calibration,  surgical data science,  semantic organ segmentation" lang="en" name="keywords"/>
<base href="/html/2409.07094v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1" title="In Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2" title="In Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2.SS1" title="In 2 Materials and Methods â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2.SS2" title="In 2 Materials and Methods â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Physics-based illumination simulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2.SS3" title="In 2 Materials and Methods â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Neural network implementation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3" title="In Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments and Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S4" title="In Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S4.SS0.SSS1" title="In 4 Discussion â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Acknowledgments.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S4.SS0.SSS2" title="In 4 Discussion â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span>Disclosure of Interests.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1a" title="In Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Supplementary material</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
Siemens AG, Munich, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and university medical center Heidelberg </span></span></span><span class="ltx_note ltx_role_institutetext" id="id6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">institutetext: </span>Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">institutetext: </span>Medical Faculty, Heidelberg University, Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_note_type">institutetext: </span>Department of Urology and Urosurgery, University Medical Center Mannheim, Medical Faculty of the University of Heidelberg, Mannheim, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_note_type">institutetext: </span>German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Systems and Robotics in Urology (ISRU), Heidelberg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">institutetext: </span>DKFZ Hector Cancer Institute at the University Medical Center Mannheim, Mannheim, Germany
</span></span></span>
<h1 class="ltx_title ltx_title_document">Deep intra-operative illumination calibration of hyperspectral cameras</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Alexander Baumann
</span><span class="ltx_author_notes">112277</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leonardo Ayala
</span><span class="ltx_author_notes">2277</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Studier-Fischer
</span><span class="ltx_author_notes">6688991010</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Sellner
</span><span class="ltx_author_notes">22334455</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Berkin Ã–zdemir
</span><span class="ltx_author_notes">66991010</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karl-Friedrich Kowalewski
</span><span class="ltx_author_notes">88991010</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Slobodan Ilic
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Silvia Seidlitz
</span><span class="ltx_author_notes">Silvia Seidlitz and Lena Maier-Hein contributed equally to this work.22334455</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lena Maier-Hein
</span><span class="ltx_author_notes">2233445577â‹†â‹†</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Hyperspectral imaging (HSI) is emerging as a promising novel imaging modality with various potential surgical applications. Currently available cameras, however, suffer from poor integration into the clinical workflow because they require the lights to be switched off, or the camera to be manually recalibrated as soon as lighting conditions change. Given this critical bottleneck, the contribution of this paper is threefold: (1) We demonstrate that dynamically changing lighting conditions in the operating room dramatically affect the performance of HSI applications, namely physiological parameter estimation, and surgical scene segmentation. (2) We propose the first learning-based approach to automatically recalibrating hyperspectral images during surgery and show that it is sufficiently accurate to replace the tedious process of white reference-based recalibration.
(3) Based on a total of 742 HSI cubes from a phantom, porcine models, and rats we show that our recalibration method not only outperforms previously proposed methods, but also generalizes across species, lighting conditions, and image processing tasks. Due to its simple workflow integration as well as high accuracy, speed, and generalization capabilities, our method could evolve as a central component in clinical surgical HSI.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>deep learning, hyperspectral imaging, intra-operative imaging, illumination calibration, surgical data science, semantic organ segmentation
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Hyperspectral imaging (HSI) emerges as a promising medical imaging modality that offers distinct advantages over conventional RGB imaging. In particular, HSI captures spectral information across numerous contiguous bands, thereby enriching the representation of the underlying sample. Recent works have demonstrated the resulting enhancement in tissue classification
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib11" title="">11</a>]</cite>,
and the capability of estimating physiological tissue parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib22" title="">22</a>]</cite>.
However, in open surgery, spectral data is affected by changes in illumination and must be correctly calibrated whenever lighting conditions vary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib8" title="">8</a>]</cite>.
The gold standard to achieve this is to switch off all light sources before acquisition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib15" title="">15</a>]</cite>.
As this protocol severely disrupts the clinical workflow, it is presumed not to be consistently applied â€“ leading to unreliable data acquisition and severe failures in downstream tasks, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">1</span></a>. This may be one reason why spectral imaging has not yet found widespread use in clinical practice.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="362" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Motivation: Current hyperspectral cameras, which require known lighting conditions, fail in real-world scenarios with dynamically changing lighting conditions.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Conventional HSI calibration is conducted with physical white reference measurements, capturing the surrounding illumination. However, they pose challenges in terms of time consumption and sterility, rendering them impractical in the operating room (OR) context. The proposed use of white OR rulers as a sterile alternative <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib4" title="">4</a>]</cite> still presents considerable challenges in the shape of additional workload and their small size.
A number of automatic calibration algorithms originally devised for RGB imaging, such as Gray-world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib5" title="">5</a>]</cite> and Max-RGB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib16" title="">16</a>]</cite>, recover a global illuminant of the scene based on intensity statistics. An alternative calibration strategy tailored specifically to HSI leverages specular highlights for illuminant estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib2" title="">2</a>]</cite>. However, these methods rely on unrealistic assumptions, such as a homogeneous illumination across the entire surgical scene, and may thus fail in practice. Multi-illuminant color constancy models devised for RGB imaging have shown potential in overcoming this issue, as they predict pixel-wise illuminants for calibration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib7" title="">7</a>]</cite>.
Notably, convolutional neural networks (CNNs) have shown superior performance compared to non-learning-based methods
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib7" title="">7</a>]</cite>.
Spectral imaging has so far seen the development of one deep learning approach for multi-illuminant calibration, factorizing reflectance and illumination through an unrolling network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib17" title="">17</a>]</cite>.
However, this research was conducted on outdoor scenes and does not generalize well to the illumination setup in an OR.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Overall, the methods proposed in the literature either remain untested for surgical HSI and/or are conceptually not suitable for spatially resolved calibration. Given this bottleneck, the mission of our work was to develop a new workflow-optimized calibration approach that enables widespread clinical spectral imaging. Our specific contribution is threefold: (1) We are the first to experimentally demonstrate that previously proposed calibration methods fail in in vivo surgical settings. (2) We present the first learning-based approach to performing spatially resolved light recalibration of surgical hyperspectral images. Specifically, we propose to replace conventional physical white reference measurements with a data-driven prediction of the corresponding white tile measurement. This enables a seamless and sterile recalibration process during surgery. (3) Based on the downstream tasks of semantic segmentation and physiological parameter estimation, we show that our recalibration method not only outperforms previous methods, but also generalizes across species, lighting conditions, and image processing tasks.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="265" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S1.F2.2.1">The proposed approach replaces tedious manual calibration with a dynamic fully-automatic approach.</span>
The core of our data-centric method is a 3D-convolutional neural network, trained on in vivo data with artificial light manipulations. At inference time, it takes a raw hyperspectral image as input and generates the corresponding white reference image. The prediction of the white tile image can be used for subsequent calibration of the input image.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The main issue in data-driven calibration is the generalization to unseen settings. To make our method conceptually robust to domain shifts, we propose estimating the white tile measurement that we would obtain for a given scene rather than directly predicting the recalibrated image. Our approach is based on the hypothesis that capturing a representative set of tissue configurations, including illumination conditions, as a training set is infeasible. We therefore disentangle the space of possible illuminations from the space of possible tissue configurations, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">2</span></a>. More specifically, we employ a two-dataset training paradigm for the neural network. The first dataset, the illumination dataset, comprises real and simulated white reference images encompassing a wide range of illumination conditions encountered within the OR. The second dataset consists of accurately calibrated HSI cubes of clinically relevant samples. To simulate uncalibrated HSI cubes, each image in the sample dataset is augmented by multiplication with the associated white reference image. Subsequently, the neural network is trained to retrieve the white reference image from the simulated uncalibrated HSI cube. During inference, an uncalibrated HSI cube, possibly acquired with stray light, is fed into the neural network to predict the white reference image needed for illumination calibration.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S2.F3.2.1">Testing concept based on data from three species.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Training and validation was performed exclusively on porcine data, while testing was performed for unseen stray light scenarios on unseen porcine individuals, a phantom and rats, as summarized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2.F3" title="Figure 3 â€£ 2.1 Datasets â€£ 2 Materials and Methods â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">3</span></a>.
While the phantom colorchecker board dataset was acquired with the Tivita<sup class="ltx_sup" id="S2.SS1.p1.1.1">Â®</sup> 2.0 Surgery (Diaspective Vision GmbH, Am Salzhaff, Germany) featuring light-emitting diode (LED) illumination, the others were captured with the halogen-based Tivita<sup class="ltx_sup" id="S2.SS1.p1.1.2">Â®</sup> Tissue. As these light sources exhibit different behavior when interfering with the main stray light source, namely LED-based OR lights, we validated on both systems. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.3">Measured illumination dataset</span>
The purpose of this dataset was to capture a variety of representative OR lighting conditions for algorithm training.
To this end, we acquired white reference images in an OR to capture camera light and additional stray light sources such as surgical lights, ceiling lights, or daylight.
The surgical lights used in our study were manufactured by Dr. Mach and are composed of LEDs (Model: LED 8 MC).
Diverse stray light scenarios were achieved by varying the angle, distance, and number of surgical lights as well as adjusting blinds or ceiling light, resulting in a wide range of illumination spectra.
As the two HSI systems used in this study differ in the light sources, we acquired one illumination dataset with each camera: <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.4">ds_dev_ill_led</span> (LED) and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.5">ds_dev_ill_hal</span> (halogen).
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.6">In vivo porcine development dataset:</span>
For model development, we curated a subset of the publicly available HSI dataset HeiPorSPECTRAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib24" title="">24</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.7">ds_test_pig</span>, consisting of accurately calibrated hyperspectral images of surgical scenes semantically annotated with 18 organ classes.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.8">Test datasets:</span>
Comprehensive validation of our methodology was performed based on the three datasets <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.9">ds_test_pig</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.10">ds_test_cc</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.11">ds_test_rat</span> summarized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S2.F3" title="Figure 3 â€£ 2.1 Datasets â€£ 2 Materials and Methods â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">3</span></a>.
In-domain testing of calibration quality was performed with data resembling the training data. To simulate stray light in <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.12">ds_test_pig</span>, we acquired a white reference test set <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.13">ds_test_ill_hal</span> with four stray light scenarios (left).
Colorchecker boards imaged under various lighting conditions were used for the assessment of recalibration performance based on highly reliable reference data (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.14">ds_test_cc</span>).
The effect of the recalibration method on surgical image analysis was assessed by means of the downstream tasks organ segmentation and physiological parameter analysis using in-domain and out of domain in vivo hyperspectral imaging data from porcine models (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.15">ds_test_pig</span>) and rats (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.16">ds_test_rat</span>).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Physics-based illumination simulation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.12">To implement the data-centric recalibration concept, we focused on the model-based generation of plausible white tile data.
To overcome the resource-intensive white reference acquisitions, we enhanced <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.12.1">ds_dev_ill_led</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.12.2">ds_dev_ill_hal</span> by synthesizing white tile images based on real L1-normalized white tile images. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.12.3">LED simulations:</span>
Surgical lights are the main source of stray light in the OR. For our LED-based surgical lights, interference with an LED-based HSI system is approximately constructive and local extrema in the spectrum of the camera light source are preserved. This behavior can be modeled by linear inter- and extrapolations of white reference images. To avoid the generation of duplicates, we first conduct clustering, before combining images of distinct clusters. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.12.4">Halogen simulations:</span>
In contrast to LED spectra, halogen spectra differ in terms of width and the number of local extrema. Consequently, the interference with LED light does not preserve local extrema. To obtain powerful simulations of stray light-affected halogen spectra, we propose to mathematically model the curves. Inspired by Planckâ€™s radiation law, we empirically observed that the following parametric function describes stray light-affected halogen spectra:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{a,b,c,d}(\lambda)=\frac{(a\lambda-b)^{3}}{\exp(c\lambda-d)-1}" class="ltx_Math" display="block" id="S2.Ex1.m1.8"><semantics id="S2.Ex1.m1.8a"><mrow id="S2.Ex1.m1.8.9" xref="S2.Ex1.m1.8.9.cmml"><mrow id="S2.Ex1.m1.8.9.2" xref="S2.Ex1.m1.8.9.2.cmml"><msub id="S2.Ex1.m1.8.9.2.2" xref="S2.Ex1.m1.8.9.2.2.cmml"><mi id="S2.Ex1.m1.8.9.2.2.2" xref="S2.Ex1.m1.8.9.2.2.2.cmml">f</mi><mrow id="S2.Ex1.m1.4.4.4.6" xref="S2.Ex1.m1.4.4.4.5.cmml"><mi id="S2.Ex1.m1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.cmml">a</mi><mo id="S2.Ex1.m1.4.4.4.6.1" xref="S2.Ex1.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex1.m1.2.2.2.2" xref="S2.Ex1.m1.2.2.2.2.cmml">b</mi><mo id="S2.Ex1.m1.4.4.4.6.2" xref="S2.Ex1.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex1.m1.3.3.3.3" xref="S2.Ex1.m1.3.3.3.3.cmml">c</mi><mo id="S2.Ex1.m1.4.4.4.6.3" xref="S2.Ex1.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex1.m1.4.4.4.4" xref="S2.Ex1.m1.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.Ex1.m1.8.9.2.1" xref="S2.Ex1.m1.8.9.2.1.cmml">â¢</mo><mrow id="S2.Ex1.m1.8.9.2.3.2" xref="S2.Ex1.m1.8.9.2.cmml"><mo id="S2.Ex1.m1.8.9.2.3.2.1" stretchy="false" xref="S2.Ex1.m1.8.9.2.cmml">(</mo><mi id="S2.Ex1.m1.8.8" xref="S2.Ex1.m1.8.8.cmml">Î»</mi><mo id="S2.Ex1.m1.8.9.2.3.2.2" stretchy="false" xref="S2.Ex1.m1.8.9.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.8.9.1" xref="S2.Ex1.m1.8.9.1.cmml">=</mo><mfrac id="S2.Ex1.m1.7.7" xref="S2.Ex1.m1.7.7.cmml"><msup id="S2.Ex1.m1.5.5.1" xref="S2.Ex1.m1.5.5.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml"><mo id="S2.Ex1.m1.5.5.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.2.2.cmml">a</mi><mo id="S2.Ex1.m1.5.5.1.1.1.1.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.2.1.cmml">â¢</mo><mi id="S2.Ex1.m1.5.5.1.1.1.1.2.3" xref="S2.Ex1.m1.5.5.1.1.1.1.2.3.cmml">Î»</mi></mrow><mo id="S2.Ex1.m1.5.5.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.cmml">âˆ’</mo><mi id="S2.Ex1.m1.5.5.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.1.3.cmml">b</mi></mrow><mo id="S2.Ex1.m1.5.5.1.1.1.3" stretchy="false" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mn id="S2.Ex1.m1.5.5.1.3" xref="S2.Ex1.m1.5.5.1.3.cmml">3</mn></msup><mrow id="S2.Ex1.m1.7.7.3" xref="S2.Ex1.m1.7.7.3.cmml"><mrow id="S2.Ex1.m1.7.7.3.2.1" xref="S2.Ex1.m1.7.7.3.2.2.cmml"><mi id="S2.Ex1.m1.6.6.2.1" xref="S2.Ex1.m1.6.6.2.1.cmml">exp</mi><mo id="S2.Ex1.m1.7.7.3.2.1a" xref="S2.Ex1.m1.7.7.3.2.2.cmml">â¡</mo><mrow id="S2.Ex1.m1.7.7.3.2.1.1" xref="S2.Ex1.m1.7.7.3.2.2.cmml"><mo id="S2.Ex1.m1.7.7.3.2.1.1.2" stretchy="false" xref="S2.Ex1.m1.7.7.3.2.2.cmml">(</mo><mrow id="S2.Ex1.m1.7.7.3.2.1.1.1" xref="S2.Ex1.m1.7.7.3.2.1.1.1.cmml"><mrow id="S2.Ex1.m1.7.7.3.2.1.1.1.2" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.cmml"><mi id="S2.Ex1.m1.7.7.3.2.1.1.1.2.2" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.2.cmml">c</mi><mo id="S2.Ex1.m1.7.7.3.2.1.1.1.2.1" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.1.cmml">â¢</mo><mi id="S2.Ex1.m1.7.7.3.2.1.1.1.2.3" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.3.cmml">Î»</mi></mrow><mo id="S2.Ex1.m1.7.7.3.2.1.1.1.1" xref="S2.Ex1.m1.7.7.3.2.1.1.1.1.cmml">âˆ’</mo><mi id="S2.Ex1.m1.7.7.3.2.1.1.1.3" xref="S2.Ex1.m1.7.7.3.2.1.1.1.3.cmml">d</mi></mrow><mo id="S2.Ex1.m1.7.7.3.2.1.1.3" stretchy="false" xref="S2.Ex1.m1.7.7.3.2.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.7.7.3.3" xref="S2.Ex1.m1.7.7.3.3.cmml">âˆ’</mo><mn id="S2.Ex1.m1.7.7.3.4" xref="S2.Ex1.m1.7.7.3.4.cmml">1</mn></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.8b"><apply id="S2.Ex1.m1.8.9.cmml" xref="S2.Ex1.m1.8.9"><eq id="S2.Ex1.m1.8.9.1.cmml" xref="S2.Ex1.m1.8.9.1"></eq><apply id="S2.Ex1.m1.8.9.2.cmml" xref="S2.Ex1.m1.8.9.2"><times id="S2.Ex1.m1.8.9.2.1.cmml" xref="S2.Ex1.m1.8.9.2.1"></times><apply id="S2.Ex1.m1.8.9.2.2.cmml" xref="S2.Ex1.m1.8.9.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.8.9.2.2.1.cmml" xref="S2.Ex1.m1.8.9.2.2">subscript</csymbol><ci id="S2.Ex1.m1.8.9.2.2.2.cmml" xref="S2.Ex1.m1.8.9.2.2.2">ğ‘“</ci><list id="S2.Ex1.m1.4.4.4.5.cmml" xref="S2.Ex1.m1.4.4.4.6"><ci id="S2.Ex1.m1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1">ğ‘</ci><ci id="S2.Ex1.m1.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2.2">ğ‘</ci><ci id="S2.Ex1.m1.3.3.3.3.cmml" xref="S2.Ex1.m1.3.3.3.3">ğ‘</ci><ci id="S2.Ex1.m1.4.4.4.4.cmml" xref="S2.Ex1.m1.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.Ex1.m1.8.8.cmml" xref="S2.Ex1.m1.8.8">ğœ†</ci></apply><apply id="S2.Ex1.m1.7.7.cmml" xref="S2.Ex1.m1.7.7"><divide id="S2.Ex1.m1.7.7.4.cmml" xref="S2.Ex1.m1.7.7"></divide><apply id="S2.Ex1.m1.5.5.1.cmml" xref="S2.Ex1.m1.5.5.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.2.cmml" xref="S2.Ex1.m1.5.5.1">superscript</csymbol><apply id="S2.Ex1.m1.5.5.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1"><minus id="S2.Ex1.m1.5.5.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1"></minus><apply id="S2.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2"><times id="S2.Ex1.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2.1"></times><ci id="S2.Ex1.m1.5.5.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2.2">ğ‘</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2.3">ğœ†</ci></apply><ci id="S2.Ex1.m1.5.5.1.1.1.1.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.3">ğ‘</ci></apply><cn id="S2.Ex1.m1.5.5.1.3.cmml" type="integer" xref="S2.Ex1.m1.5.5.1.3">3</cn></apply><apply id="S2.Ex1.m1.7.7.3.cmml" xref="S2.Ex1.m1.7.7.3"><minus id="S2.Ex1.m1.7.7.3.3.cmml" xref="S2.Ex1.m1.7.7.3.3"></minus><apply id="S2.Ex1.m1.7.7.3.2.2.cmml" xref="S2.Ex1.m1.7.7.3.2.1"><exp id="S2.Ex1.m1.6.6.2.1.cmml" xref="S2.Ex1.m1.6.6.2.1"></exp><apply id="S2.Ex1.m1.7.7.3.2.1.1.1.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1"><minus id="S2.Ex1.m1.7.7.3.2.1.1.1.1.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.1"></minus><apply id="S2.Ex1.m1.7.7.3.2.1.1.1.2.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2"><times id="S2.Ex1.m1.7.7.3.2.1.1.1.2.1.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.1"></times><ci id="S2.Ex1.m1.7.7.3.2.1.1.1.2.2.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.2">ğ‘</ci><ci id="S2.Ex1.m1.7.7.3.2.1.1.1.2.3.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.2.3">ğœ†</ci></apply><ci id="S2.Ex1.m1.7.7.3.2.1.1.1.3.cmml" xref="S2.Ex1.m1.7.7.3.2.1.1.1.3">ğ‘‘</ci></apply></apply><cn id="S2.Ex1.m1.7.7.3.4.cmml" type="integer" xref="S2.Ex1.m1.7.7.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.8c">f_{a,b,c,d}(\lambda)=\frac{(a\lambda-b)^{3}}{\exp(c\lambda-d)-1}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.8d">italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» ) = divide start_ARG ( italic_a italic_Î» - italic_b ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG start_ARG roman_exp ( italic_c italic_Î» - italic_d ) - 1 end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p1.11">where <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_f</annotation></semantics></math> denotes the intensity, <math alttext="\lambda" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_Î»</annotation></semantics></math> the wavelength and <math alttext="a,b,c,d" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.4"><semantics id="S2.SS2.p1.3.m3.4a"><mrow id="S2.SS2.p1.3.m3.4.5.2" xref="S2.SS2.p1.3.m3.4.5.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">a</mi><mo id="S2.SS2.p1.3.m3.4.5.2.1" xref="S2.SS2.p1.3.m3.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.3.m3.2.2" xref="S2.SS2.p1.3.m3.2.2.cmml">b</mi><mo id="S2.SS2.p1.3.m3.4.5.2.2" xref="S2.SS2.p1.3.m3.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.3.m3.3.3" xref="S2.SS2.p1.3.m3.3.3.cmml">c</mi><mo id="S2.SS2.p1.3.m3.4.5.2.3" xref="S2.SS2.p1.3.m3.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.3.m3.4.4" xref="S2.SS2.p1.3.m3.4.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.4b"><list id="S2.SS2.p1.3.m3.4.5.1.cmml" xref="S2.SS2.p1.3.m3.4.5.2"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ‘</ci><ci id="S2.SS2.p1.3.m3.2.2.cmml" xref="S2.SS2.p1.3.m3.2.2">ğ‘</ci><ci id="S2.SS2.p1.3.m3.3.3.cmml" xref="S2.SS2.p1.3.m3.3.3">ğ‘</ci><ci id="S2.SS2.p1.3.m3.4.4.cmml" xref="S2.SS2.p1.3.m3.4.4">ğ‘‘</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.4c">a,b,c,d</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.4d">italic_a , italic_b , italic_c , italic_d</annotation></semantics></math> the parameters. Least-square optimization to the mean spectra of <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.11.1">ds_dev_ill_hal</span> yielded parameter ranges for a,b,c,d so that <math alttext="f_{a,b,c,d}(\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.5"><semantics id="S2.SS2.p1.4.m4.5a"><mrow id="S2.SS2.p1.4.m4.5.6" xref="S2.SS2.p1.4.m4.5.6.cmml"><msub id="S2.SS2.p1.4.m4.5.6.2" xref="S2.SS2.p1.4.m4.5.6.2.cmml"><mi id="S2.SS2.p1.4.m4.5.6.2.2" xref="S2.SS2.p1.4.m4.5.6.2.2.cmml">f</mi><mrow id="S2.SS2.p1.4.m4.4.4.4.6" xref="S2.SS2.p1.4.m4.4.4.4.5.cmml"><mi id="S2.SS2.p1.4.m4.1.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.1.cmml">a</mi><mo id="S2.SS2.p1.4.m4.4.4.4.6.1" xref="S2.SS2.p1.4.m4.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.4.m4.2.2.2.2" xref="S2.SS2.p1.4.m4.2.2.2.2.cmml">b</mi><mo id="S2.SS2.p1.4.m4.4.4.4.6.2" xref="S2.SS2.p1.4.m4.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.4.m4.3.3.3.3" xref="S2.SS2.p1.4.m4.3.3.3.3.cmml">c</mi><mo id="S2.SS2.p1.4.m4.4.4.4.6.3" xref="S2.SS2.p1.4.m4.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.4.m4.4.4.4.4" xref="S2.SS2.p1.4.m4.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.SS2.p1.4.m4.5.6.1" xref="S2.SS2.p1.4.m4.5.6.1.cmml">â¢</mo><mrow id="S2.SS2.p1.4.m4.5.6.3.2" xref="S2.SS2.p1.4.m4.5.6.cmml"><mo id="S2.SS2.p1.4.m4.5.6.3.2.1" stretchy="false" xref="S2.SS2.p1.4.m4.5.6.cmml">(</mo><mi id="S2.SS2.p1.4.m4.5.5" xref="S2.SS2.p1.4.m4.5.5.cmml">Î»</mi><mo id="S2.SS2.p1.4.m4.5.6.3.2.2" stretchy="false" xref="S2.SS2.p1.4.m4.5.6.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.5b"><apply id="S2.SS2.p1.4.m4.5.6.cmml" xref="S2.SS2.p1.4.m4.5.6"><times id="S2.SS2.p1.4.m4.5.6.1.cmml" xref="S2.SS2.p1.4.m4.5.6.1"></times><apply id="S2.SS2.p1.4.m4.5.6.2.cmml" xref="S2.SS2.p1.4.m4.5.6.2"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.5.6.2.1.cmml" xref="S2.SS2.p1.4.m4.5.6.2">subscript</csymbol><ci id="S2.SS2.p1.4.m4.5.6.2.2.cmml" xref="S2.SS2.p1.4.m4.5.6.2.2">ğ‘“</ci><list id="S2.SS2.p1.4.m4.4.4.4.5.cmml" xref="S2.SS2.p1.4.m4.4.4.4.6"><ci id="S2.SS2.p1.4.m4.1.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1.1">ğ‘</ci><ci id="S2.SS2.p1.4.m4.2.2.2.2.cmml" xref="S2.SS2.p1.4.m4.2.2.2.2">ğ‘</ci><ci id="S2.SS2.p1.4.m4.3.3.3.3.cmml" xref="S2.SS2.p1.4.m4.3.3.3.3">ğ‘</ci><ci id="S2.SS2.p1.4.m4.4.4.4.4.cmml" xref="S2.SS2.p1.4.m4.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.SS2.p1.4.m4.5.5.cmml" xref="S2.SS2.p1.4.m4.5.5">ğœ†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.5c">f_{a,b,c,d}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.5d">italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» )</annotation></semantics></math> adequately models the illumination conditions captured in <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.11.2">ds_dev_ill_hal</span>. Increasing the upper bounds of these ranges led to halogen spectra with higher levels of stray light.
To synthesize a hyperspectral image that features realistic spatial variations of intensity from the simulated light spectrum <math alttext="f_{a,b,c,d}(\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.5"><semantics id="S2.SS2.p1.5.m5.5a"><mrow id="S2.SS2.p1.5.m5.5.6" xref="S2.SS2.p1.5.m5.5.6.cmml"><msub id="S2.SS2.p1.5.m5.5.6.2" xref="S2.SS2.p1.5.m5.5.6.2.cmml"><mi id="S2.SS2.p1.5.m5.5.6.2.2" xref="S2.SS2.p1.5.m5.5.6.2.2.cmml">f</mi><mrow id="S2.SS2.p1.5.m5.4.4.4.6" xref="S2.SS2.p1.5.m5.4.4.4.5.cmml"><mi id="S2.SS2.p1.5.m5.1.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.1.cmml">a</mi><mo id="S2.SS2.p1.5.m5.4.4.4.6.1" xref="S2.SS2.p1.5.m5.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.5.m5.2.2.2.2" xref="S2.SS2.p1.5.m5.2.2.2.2.cmml">b</mi><mo id="S2.SS2.p1.5.m5.4.4.4.6.2" xref="S2.SS2.p1.5.m5.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.5.m5.3.3.3.3" xref="S2.SS2.p1.5.m5.3.3.3.3.cmml">c</mi><mo id="S2.SS2.p1.5.m5.4.4.4.6.3" xref="S2.SS2.p1.5.m5.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.5.m5.4.4.4.4" xref="S2.SS2.p1.5.m5.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.SS2.p1.5.m5.5.6.1" xref="S2.SS2.p1.5.m5.5.6.1.cmml">â¢</mo><mrow id="S2.SS2.p1.5.m5.5.6.3.2" xref="S2.SS2.p1.5.m5.5.6.cmml"><mo id="S2.SS2.p1.5.m5.5.6.3.2.1" stretchy="false" xref="S2.SS2.p1.5.m5.5.6.cmml">(</mo><mi id="S2.SS2.p1.5.m5.5.5" xref="S2.SS2.p1.5.m5.5.5.cmml">Î»</mi><mo id="S2.SS2.p1.5.m5.5.6.3.2.2" stretchy="false" xref="S2.SS2.p1.5.m5.5.6.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.5b"><apply id="S2.SS2.p1.5.m5.5.6.cmml" xref="S2.SS2.p1.5.m5.5.6"><times id="S2.SS2.p1.5.m5.5.6.1.cmml" xref="S2.SS2.p1.5.m5.5.6.1"></times><apply id="S2.SS2.p1.5.m5.5.6.2.cmml" xref="S2.SS2.p1.5.m5.5.6.2"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.5.6.2.1.cmml" xref="S2.SS2.p1.5.m5.5.6.2">subscript</csymbol><ci id="S2.SS2.p1.5.m5.5.6.2.2.cmml" xref="S2.SS2.p1.5.m5.5.6.2.2">ğ‘“</ci><list id="S2.SS2.p1.5.m5.4.4.4.5.cmml" xref="S2.SS2.p1.5.m5.4.4.4.6"><ci id="S2.SS2.p1.5.m5.1.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1.1">ğ‘</ci><ci id="S2.SS2.p1.5.m5.2.2.2.2.cmml" xref="S2.SS2.p1.5.m5.2.2.2.2">ğ‘</ci><ci id="S2.SS2.p1.5.m5.3.3.3.3.cmml" xref="S2.SS2.p1.5.m5.3.3.3.3">ğ‘</ci><ci id="S2.SS2.p1.5.m5.4.4.4.4.cmml" xref="S2.SS2.p1.5.m5.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.SS2.p1.5.m5.5.5.cmml" xref="S2.SS2.p1.5.m5.5.5">ğœ†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.5c">f_{a,b,c,d}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.5d">italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» )</annotation></semantics></math>, we leveraged the acquired images.
More concretely, a hyperspectral image <math alttext="I(i,j,\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.3"><semantics id="S2.SS2.p1.6.m6.3a"><mrow id="S2.SS2.p1.6.m6.3.4" xref="S2.SS2.p1.6.m6.3.4.cmml"><mi id="S2.SS2.p1.6.m6.3.4.2" xref="S2.SS2.p1.6.m6.3.4.2.cmml">I</mi><mo id="S2.SS2.p1.6.m6.3.4.1" xref="S2.SS2.p1.6.m6.3.4.1.cmml">â¢</mo><mrow id="S2.SS2.p1.6.m6.3.4.3.2" xref="S2.SS2.p1.6.m6.3.4.3.1.cmml"><mo id="S2.SS2.p1.6.m6.3.4.3.2.1" stretchy="false" xref="S2.SS2.p1.6.m6.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">i</mi><mo id="S2.SS2.p1.6.m6.3.4.3.2.2" xref="S2.SS2.p1.6.m6.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p1.6.m6.2.2" xref="S2.SS2.p1.6.m6.2.2.cmml">j</mi><mo id="S2.SS2.p1.6.m6.3.4.3.2.3" xref="S2.SS2.p1.6.m6.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p1.6.m6.3.3" xref="S2.SS2.p1.6.m6.3.3.cmml">Î»</mi><mo id="S2.SS2.p1.6.m6.3.4.3.2.4" stretchy="false" xref="S2.SS2.p1.6.m6.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.3b"><apply id="S2.SS2.p1.6.m6.3.4.cmml" xref="S2.SS2.p1.6.m6.3.4"><times id="S2.SS2.p1.6.m6.3.4.1.cmml" xref="S2.SS2.p1.6.m6.3.4.1"></times><ci id="S2.SS2.p1.6.m6.3.4.2.cmml" xref="S2.SS2.p1.6.m6.3.4.2">ğ¼</ci><vector id="S2.SS2.p1.6.m6.3.4.3.1.cmml" xref="S2.SS2.p1.6.m6.3.4.3.2"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">ğ‘–</ci><ci id="S2.SS2.p1.6.m6.2.2.cmml" xref="S2.SS2.p1.6.m6.2.2">ğ‘—</ci><ci id="S2.SS2.p1.6.m6.3.3.cmml" xref="S2.SS2.p1.6.m6.3.3">ğœ†</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.3c">I(i,j,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.3d">italic_I ( italic_i , italic_j , italic_Î» )</annotation></semantics></math> is randomly selected from <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.11.3">ds_dev_ill_hal</span> and divided by its spatially averaged spectrum <math alttext="\overline{I}(\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.1"><semantics id="S2.SS2.p1.7.m7.1a"><mrow id="S2.SS2.p1.7.m7.1.2" xref="S2.SS2.p1.7.m7.1.2.cmml"><mover accent="true" id="S2.SS2.p1.7.m7.1.2.2" xref="S2.SS2.p1.7.m7.1.2.2.cmml"><mi id="S2.SS2.p1.7.m7.1.2.2.2" xref="S2.SS2.p1.7.m7.1.2.2.2.cmml">I</mi><mo id="S2.SS2.p1.7.m7.1.2.2.1" xref="S2.SS2.p1.7.m7.1.2.2.1.cmml">Â¯</mo></mover><mo id="S2.SS2.p1.7.m7.1.2.1" xref="S2.SS2.p1.7.m7.1.2.1.cmml">â¢</mo><mrow id="S2.SS2.p1.7.m7.1.2.3.2" xref="S2.SS2.p1.7.m7.1.2.cmml"><mo id="S2.SS2.p1.7.m7.1.2.3.2.1" stretchy="false" xref="S2.SS2.p1.7.m7.1.2.cmml">(</mo><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">Î»</mi><mo id="S2.SS2.p1.7.m7.1.2.3.2.2" stretchy="false" xref="S2.SS2.p1.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.2.cmml" xref="S2.SS2.p1.7.m7.1.2"><times id="S2.SS2.p1.7.m7.1.2.1.cmml" xref="S2.SS2.p1.7.m7.1.2.1"></times><apply id="S2.SS2.p1.7.m7.1.2.2.cmml" xref="S2.SS2.p1.7.m7.1.2.2"><ci id="S2.SS2.p1.7.m7.1.2.2.1.cmml" xref="S2.SS2.p1.7.m7.1.2.2.1">Â¯</ci><ci id="S2.SS2.p1.7.m7.1.2.2.2.cmml" xref="S2.SS2.p1.7.m7.1.2.2.2">ğ¼</ci></apply><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">ğœ†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">\overline{I}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.1d">overÂ¯ start_ARG italic_I end_ARG ( italic_Î» )</annotation></semantics></math>.
By multiplication with the simulated spectrum <math alttext="f_{a,b,c,d}(\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m8.5"><semantics id="S2.SS2.p1.8.m8.5a"><mrow id="S2.SS2.p1.8.m8.5.6" xref="S2.SS2.p1.8.m8.5.6.cmml"><msub id="S2.SS2.p1.8.m8.5.6.2" xref="S2.SS2.p1.8.m8.5.6.2.cmml"><mi id="S2.SS2.p1.8.m8.5.6.2.2" xref="S2.SS2.p1.8.m8.5.6.2.2.cmml">f</mi><mrow id="S2.SS2.p1.8.m8.4.4.4.6" xref="S2.SS2.p1.8.m8.4.4.4.5.cmml"><mi id="S2.SS2.p1.8.m8.1.1.1.1" xref="S2.SS2.p1.8.m8.1.1.1.1.cmml">a</mi><mo id="S2.SS2.p1.8.m8.4.4.4.6.1" xref="S2.SS2.p1.8.m8.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.8.m8.2.2.2.2" xref="S2.SS2.p1.8.m8.2.2.2.2.cmml">b</mi><mo id="S2.SS2.p1.8.m8.4.4.4.6.2" xref="S2.SS2.p1.8.m8.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.8.m8.3.3.3.3" xref="S2.SS2.p1.8.m8.3.3.3.3.cmml">c</mi><mo id="S2.SS2.p1.8.m8.4.4.4.6.3" xref="S2.SS2.p1.8.m8.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.8.m8.4.4.4.4" xref="S2.SS2.p1.8.m8.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.SS2.p1.8.m8.5.6.1" xref="S2.SS2.p1.8.m8.5.6.1.cmml">â¢</mo><mrow id="S2.SS2.p1.8.m8.5.6.3.2" xref="S2.SS2.p1.8.m8.5.6.cmml"><mo id="S2.SS2.p1.8.m8.5.6.3.2.1" stretchy="false" xref="S2.SS2.p1.8.m8.5.6.cmml">(</mo><mi id="S2.SS2.p1.8.m8.5.5" xref="S2.SS2.p1.8.m8.5.5.cmml">Î»</mi><mo id="S2.SS2.p1.8.m8.5.6.3.2.2" stretchy="false" xref="S2.SS2.p1.8.m8.5.6.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.5b"><apply id="S2.SS2.p1.8.m8.5.6.cmml" xref="S2.SS2.p1.8.m8.5.6"><times id="S2.SS2.p1.8.m8.5.6.1.cmml" xref="S2.SS2.p1.8.m8.5.6.1"></times><apply id="S2.SS2.p1.8.m8.5.6.2.cmml" xref="S2.SS2.p1.8.m8.5.6.2"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.5.6.2.1.cmml" xref="S2.SS2.p1.8.m8.5.6.2">subscript</csymbol><ci id="S2.SS2.p1.8.m8.5.6.2.2.cmml" xref="S2.SS2.p1.8.m8.5.6.2.2">ğ‘“</ci><list id="S2.SS2.p1.8.m8.4.4.4.5.cmml" xref="S2.SS2.p1.8.m8.4.4.4.6"><ci id="S2.SS2.p1.8.m8.1.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1.1.1">ğ‘</ci><ci id="S2.SS2.p1.8.m8.2.2.2.2.cmml" xref="S2.SS2.p1.8.m8.2.2.2.2">ğ‘</ci><ci id="S2.SS2.p1.8.m8.3.3.3.3.cmml" xref="S2.SS2.p1.8.m8.3.3.3.3">ğ‘</ci><ci id="S2.SS2.p1.8.m8.4.4.4.4.cmml" xref="S2.SS2.p1.8.m8.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.SS2.p1.8.m8.5.5.cmml" xref="S2.SS2.p1.8.m8.5.5">ğœ†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.5c">f_{a,b,c,d}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m8.5d">italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» )</annotation></semantics></math>, we obtain a simulated white reference image <math alttext="I_{s}(i,j,\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m9.3"><semantics id="S2.SS2.p1.9.m9.3a"><mrow id="S2.SS2.p1.9.m9.3.4" xref="S2.SS2.p1.9.m9.3.4.cmml"><msub id="S2.SS2.p1.9.m9.3.4.2" xref="S2.SS2.p1.9.m9.3.4.2.cmml"><mi id="S2.SS2.p1.9.m9.3.4.2.2" xref="S2.SS2.p1.9.m9.3.4.2.2.cmml">I</mi><mi id="S2.SS2.p1.9.m9.3.4.2.3" xref="S2.SS2.p1.9.m9.3.4.2.3.cmml">s</mi></msub><mo id="S2.SS2.p1.9.m9.3.4.1" xref="S2.SS2.p1.9.m9.3.4.1.cmml">â¢</mo><mrow id="S2.SS2.p1.9.m9.3.4.3.2" xref="S2.SS2.p1.9.m9.3.4.3.1.cmml"><mo id="S2.SS2.p1.9.m9.3.4.3.2.1" stretchy="false" xref="S2.SS2.p1.9.m9.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p1.9.m9.1.1" xref="S2.SS2.p1.9.m9.1.1.cmml">i</mi><mo id="S2.SS2.p1.9.m9.3.4.3.2.2" xref="S2.SS2.p1.9.m9.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p1.9.m9.2.2" xref="S2.SS2.p1.9.m9.2.2.cmml">j</mi><mo id="S2.SS2.p1.9.m9.3.4.3.2.3" xref="S2.SS2.p1.9.m9.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p1.9.m9.3.3" xref="S2.SS2.p1.9.m9.3.3.cmml">Î»</mi><mo id="S2.SS2.p1.9.m9.3.4.3.2.4" stretchy="false" xref="S2.SS2.p1.9.m9.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.3b"><apply id="S2.SS2.p1.9.m9.3.4.cmml" xref="S2.SS2.p1.9.m9.3.4"><times id="S2.SS2.p1.9.m9.3.4.1.cmml" xref="S2.SS2.p1.9.m9.3.4.1"></times><apply id="S2.SS2.p1.9.m9.3.4.2.cmml" xref="S2.SS2.p1.9.m9.3.4.2"><csymbol cd="ambiguous" id="S2.SS2.p1.9.m9.3.4.2.1.cmml" xref="S2.SS2.p1.9.m9.3.4.2">subscript</csymbol><ci id="S2.SS2.p1.9.m9.3.4.2.2.cmml" xref="S2.SS2.p1.9.m9.3.4.2.2">ğ¼</ci><ci id="S2.SS2.p1.9.m9.3.4.2.3.cmml" xref="S2.SS2.p1.9.m9.3.4.2.3">ğ‘ </ci></apply><vector id="S2.SS2.p1.9.m9.3.4.3.1.cmml" xref="S2.SS2.p1.9.m9.3.4.3.2"><ci id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1">ğ‘–</ci><ci id="S2.SS2.p1.9.m9.2.2.cmml" xref="S2.SS2.p1.9.m9.2.2">ğ‘—</ci><ci id="S2.SS2.p1.9.m9.3.3.cmml" xref="S2.SS2.p1.9.m9.3.3">ğœ†</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.3c">I_{s}(i,j,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.9.m9.3d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_i , italic_j , italic_Î» )</annotation></semantics></math> with the simulated spectrum <math alttext="f_{a,b,c,d}(\lambda)" class="ltx_Math" display="inline" id="S2.SS2.p1.10.m10.5"><semantics id="S2.SS2.p1.10.m10.5a"><mrow id="S2.SS2.p1.10.m10.5.6" xref="S2.SS2.p1.10.m10.5.6.cmml"><msub id="S2.SS2.p1.10.m10.5.6.2" xref="S2.SS2.p1.10.m10.5.6.2.cmml"><mi id="S2.SS2.p1.10.m10.5.6.2.2" xref="S2.SS2.p1.10.m10.5.6.2.2.cmml">f</mi><mrow id="S2.SS2.p1.10.m10.4.4.4.6" xref="S2.SS2.p1.10.m10.4.4.4.5.cmml"><mi id="S2.SS2.p1.10.m10.1.1.1.1" xref="S2.SS2.p1.10.m10.1.1.1.1.cmml">a</mi><mo id="S2.SS2.p1.10.m10.4.4.4.6.1" xref="S2.SS2.p1.10.m10.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.10.m10.2.2.2.2" xref="S2.SS2.p1.10.m10.2.2.2.2.cmml">b</mi><mo id="S2.SS2.p1.10.m10.4.4.4.6.2" xref="S2.SS2.p1.10.m10.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.10.m10.3.3.3.3" xref="S2.SS2.p1.10.m10.3.3.3.3.cmml">c</mi><mo id="S2.SS2.p1.10.m10.4.4.4.6.3" xref="S2.SS2.p1.10.m10.4.4.4.5.cmml">,</mo><mi id="S2.SS2.p1.10.m10.4.4.4.4" xref="S2.SS2.p1.10.m10.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.SS2.p1.10.m10.5.6.1" xref="S2.SS2.p1.10.m10.5.6.1.cmml">â¢</mo><mrow id="S2.SS2.p1.10.m10.5.6.3.2" xref="S2.SS2.p1.10.m10.5.6.cmml"><mo id="S2.SS2.p1.10.m10.5.6.3.2.1" stretchy="false" xref="S2.SS2.p1.10.m10.5.6.cmml">(</mo><mi id="S2.SS2.p1.10.m10.5.5" xref="S2.SS2.p1.10.m10.5.5.cmml">Î»</mi><mo id="S2.SS2.p1.10.m10.5.6.3.2.2" stretchy="false" xref="S2.SS2.p1.10.m10.5.6.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m10.5b"><apply id="S2.SS2.p1.10.m10.5.6.cmml" xref="S2.SS2.p1.10.m10.5.6"><times id="S2.SS2.p1.10.m10.5.6.1.cmml" xref="S2.SS2.p1.10.m10.5.6.1"></times><apply id="S2.SS2.p1.10.m10.5.6.2.cmml" xref="S2.SS2.p1.10.m10.5.6.2"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m10.5.6.2.1.cmml" xref="S2.SS2.p1.10.m10.5.6.2">subscript</csymbol><ci id="S2.SS2.p1.10.m10.5.6.2.2.cmml" xref="S2.SS2.p1.10.m10.5.6.2.2">ğ‘“</ci><list id="S2.SS2.p1.10.m10.4.4.4.5.cmml" xref="S2.SS2.p1.10.m10.4.4.4.6"><ci id="S2.SS2.p1.10.m10.1.1.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1.1.1">ğ‘</ci><ci id="S2.SS2.p1.10.m10.2.2.2.2.cmml" xref="S2.SS2.p1.10.m10.2.2.2.2">ğ‘</ci><ci id="S2.SS2.p1.10.m10.3.3.3.3.cmml" xref="S2.SS2.p1.10.m10.3.3.3.3">ğ‘</ci><ci id="S2.SS2.p1.10.m10.4.4.4.4.cmml" xref="S2.SS2.p1.10.m10.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.SS2.p1.10.m10.5.5.cmml" xref="S2.SS2.p1.10.m10.5.5">ğœ†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m10.5c">f_{a,b,c,d}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.10.m10.5d">italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» )</annotation></semantics></math> as mean spectrum and the spatial variations from <math alttext="I" class="ltx_Math" display="inline" id="S2.SS2.p1.11.m11.1"><semantics id="S2.SS2.p1.11.m11.1a"><mi id="S2.SS2.p1.11.m11.1.1" xref="S2.SS2.p1.11.m11.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.11.m11.1b"><ci id="S2.SS2.p1.11.m11.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.11.m11.1c">I</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.11.m11.1d">italic_I</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{s}(i,j,\lambda)=f_{a,b,c,d}(\lambda)\odot I(i,j,\lambda)\oslash\overline{I}%
(\lambda)" class="ltx_Math" display="block" id="S2.Ex2.m1.12"><semantics id="S2.Ex2.m1.12a"><mrow id="S2.Ex2.m1.12.13" xref="S2.Ex2.m1.12.13.cmml"><mrow id="S2.Ex2.m1.12.13.2" xref="S2.Ex2.m1.12.13.2.cmml"><msub id="S2.Ex2.m1.12.13.2.2" xref="S2.Ex2.m1.12.13.2.2.cmml"><mi id="S2.Ex2.m1.12.13.2.2.2" xref="S2.Ex2.m1.12.13.2.2.2.cmml">I</mi><mi id="S2.Ex2.m1.12.13.2.2.3" xref="S2.Ex2.m1.12.13.2.2.3.cmml">s</mi></msub><mo id="S2.Ex2.m1.12.13.2.1" xref="S2.Ex2.m1.12.13.2.1.cmml">â¢</mo><mrow id="S2.Ex2.m1.12.13.2.3.2" xref="S2.Ex2.m1.12.13.2.3.1.cmml"><mo id="S2.Ex2.m1.12.13.2.3.2.1" stretchy="false" xref="S2.Ex2.m1.12.13.2.3.1.cmml">(</mo><mi id="S2.Ex2.m1.5.5" xref="S2.Ex2.m1.5.5.cmml">i</mi><mo id="S2.Ex2.m1.12.13.2.3.2.2" xref="S2.Ex2.m1.12.13.2.3.1.cmml">,</mo><mi id="S2.Ex2.m1.6.6" xref="S2.Ex2.m1.6.6.cmml">j</mi><mo id="S2.Ex2.m1.12.13.2.3.2.3" xref="S2.Ex2.m1.12.13.2.3.1.cmml">,</mo><mi id="S2.Ex2.m1.7.7" xref="S2.Ex2.m1.7.7.cmml">Î»</mi><mo id="S2.Ex2.m1.12.13.2.3.2.4" stretchy="false" xref="S2.Ex2.m1.12.13.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.12.13.1" xref="S2.Ex2.m1.12.13.1.cmml">=</mo><mrow id="S2.Ex2.m1.12.13.3" xref="S2.Ex2.m1.12.13.3.cmml"><mrow id="S2.Ex2.m1.12.13.3.2" xref="S2.Ex2.m1.12.13.3.2.cmml"><mrow id="S2.Ex2.m1.12.13.3.2.2" xref="S2.Ex2.m1.12.13.3.2.2.cmml"><mrow id="S2.Ex2.m1.12.13.3.2.2.2" xref="S2.Ex2.m1.12.13.3.2.2.2.cmml"><mrow id="S2.Ex2.m1.12.13.3.2.2.2.2" xref="S2.Ex2.m1.12.13.3.2.2.2.2.cmml"><msub id="S2.Ex2.m1.12.13.3.2.2.2.2.2" xref="S2.Ex2.m1.12.13.3.2.2.2.2.2.cmml"><mi id="S2.Ex2.m1.12.13.3.2.2.2.2.2.2" xref="S2.Ex2.m1.12.13.3.2.2.2.2.2.2.cmml">f</mi><mrow id="S2.Ex2.m1.4.4.4.6" xref="S2.Ex2.m1.4.4.4.5.cmml"><mi id="S2.Ex2.m1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml">a</mi><mo id="S2.Ex2.m1.4.4.4.6.1" xref="S2.Ex2.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex2.m1.2.2.2.2" xref="S2.Ex2.m1.2.2.2.2.cmml">b</mi><mo id="S2.Ex2.m1.4.4.4.6.2" xref="S2.Ex2.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex2.m1.3.3.3.3" xref="S2.Ex2.m1.3.3.3.3.cmml">c</mi><mo id="S2.Ex2.m1.4.4.4.6.3" xref="S2.Ex2.m1.4.4.4.5.cmml">,</mo><mi id="S2.Ex2.m1.4.4.4.4" xref="S2.Ex2.m1.4.4.4.4.cmml">d</mi></mrow></msub><mo id="S2.Ex2.m1.12.13.3.2.2.2.2.1" xref="S2.Ex2.m1.12.13.3.2.2.2.2.1.cmml">â¢</mo><mrow id="S2.Ex2.m1.12.13.3.2.2.2.2.3.2" xref="S2.Ex2.m1.12.13.3.2.2.2.2.cmml"><mo id="S2.Ex2.m1.12.13.3.2.2.2.2.3.2.1" stretchy="false" xref="S2.Ex2.m1.12.13.3.2.2.2.2.cmml">(</mo><mi id="S2.Ex2.m1.8.8" xref="S2.Ex2.m1.8.8.cmml">Î»</mi><mo id="S2.Ex2.m1.12.13.3.2.2.2.2.3.2.2" rspace="0.055em" stretchy="false" xref="S2.Ex2.m1.12.13.3.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.12.13.3.2.2.2.1" rspace="0.222em" xref="S2.Ex2.m1.12.13.3.2.2.2.1.cmml">âŠ™</mo><mi id="S2.Ex2.m1.12.13.3.2.2.2.3" xref="S2.Ex2.m1.12.13.3.2.2.2.3.cmml">I</mi></mrow><mo id="S2.Ex2.m1.12.13.3.2.2.1" xref="S2.Ex2.m1.12.13.3.2.2.1.cmml">â¢</mo><mrow id="S2.Ex2.m1.12.13.3.2.2.3.2" xref="S2.Ex2.m1.12.13.3.2.2.3.1.cmml"><mo id="S2.Ex2.m1.12.13.3.2.2.3.2.1" stretchy="false" xref="S2.Ex2.m1.12.13.3.2.2.3.1.cmml">(</mo><mi id="S2.Ex2.m1.9.9" xref="S2.Ex2.m1.9.9.cmml">i</mi><mo id="S2.Ex2.m1.12.13.3.2.2.3.2.2" xref="S2.Ex2.m1.12.13.3.2.2.3.1.cmml">,</mo><mi id="S2.Ex2.m1.10.10" xref="S2.Ex2.m1.10.10.cmml">j</mi><mo id="S2.Ex2.m1.12.13.3.2.2.3.2.3" xref="S2.Ex2.m1.12.13.3.2.2.3.1.cmml">,</mo><mi id="S2.Ex2.m1.11.11" xref="S2.Ex2.m1.11.11.cmml">Î»</mi><mo id="S2.Ex2.m1.12.13.3.2.2.3.2.4" stretchy="false" xref="S2.Ex2.m1.12.13.3.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.12.13.3.2.1" xref="S2.Ex2.m1.12.13.3.2.1.cmml">âŠ˜</mo><mover accent="true" id="S2.Ex2.m1.12.13.3.2.3" xref="S2.Ex2.m1.12.13.3.2.3.cmml"><mi id="S2.Ex2.m1.12.13.3.2.3.2" xref="S2.Ex2.m1.12.13.3.2.3.2.cmml">I</mi><mo id="S2.Ex2.m1.12.13.3.2.3.1" xref="S2.Ex2.m1.12.13.3.2.3.1.cmml">Â¯</mo></mover></mrow><mo id="S2.Ex2.m1.12.13.3.1" xref="S2.Ex2.m1.12.13.3.1.cmml">â¢</mo><mrow id="S2.Ex2.m1.12.13.3.3.2" xref="S2.Ex2.m1.12.13.3.cmml"><mo id="S2.Ex2.m1.12.13.3.3.2.1" stretchy="false" xref="S2.Ex2.m1.12.13.3.cmml">(</mo><mi id="S2.Ex2.m1.12.12" xref="S2.Ex2.m1.12.12.cmml">Î»</mi><mo id="S2.Ex2.m1.12.13.3.3.2.2" stretchy="false" xref="S2.Ex2.m1.12.13.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.12b"><apply id="S2.Ex2.m1.12.13.cmml" xref="S2.Ex2.m1.12.13"><eq id="S2.Ex2.m1.12.13.1.cmml" xref="S2.Ex2.m1.12.13.1"></eq><apply id="S2.Ex2.m1.12.13.2.cmml" xref="S2.Ex2.m1.12.13.2"><times id="S2.Ex2.m1.12.13.2.1.cmml" xref="S2.Ex2.m1.12.13.2.1"></times><apply id="S2.Ex2.m1.12.13.2.2.cmml" xref="S2.Ex2.m1.12.13.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.12.13.2.2.1.cmml" xref="S2.Ex2.m1.12.13.2.2">subscript</csymbol><ci id="S2.Ex2.m1.12.13.2.2.2.cmml" xref="S2.Ex2.m1.12.13.2.2.2">ğ¼</ci><ci id="S2.Ex2.m1.12.13.2.2.3.cmml" xref="S2.Ex2.m1.12.13.2.2.3">ğ‘ </ci></apply><vector id="S2.Ex2.m1.12.13.2.3.1.cmml" xref="S2.Ex2.m1.12.13.2.3.2"><ci id="S2.Ex2.m1.5.5.cmml" xref="S2.Ex2.m1.5.5">ğ‘–</ci><ci id="S2.Ex2.m1.6.6.cmml" xref="S2.Ex2.m1.6.6">ğ‘—</ci><ci id="S2.Ex2.m1.7.7.cmml" xref="S2.Ex2.m1.7.7">ğœ†</ci></vector></apply><apply id="S2.Ex2.m1.12.13.3.cmml" xref="S2.Ex2.m1.12.13.3"><times id="S2.Ex2.m1.12.13.3.1.cmml" xref="S2.Ex2.m1.12.13.3.1"></times><apply id="S2.Ex2.m1.12.13.3.2.cmml" xref="S2.Ex2.m1.12.13.3.2"><ci id="S2.Ex2.m1.12.13.3.2.1.cmml" xref="S2.Ex2.m1.12.13.3.2.1">âŠ˜</ci><apply id="S2.Ex2.m1.12.13.3.2.2.cmml" xref="S2.Ex2.m1.12.13.3.2.2"><times id="S2.Ex2.m1.12.13.3.2.2.1.cmml" xref="S2.Ex2.m1.12.13.3.2.2.1"></times><apply id="S2.Ex2.m1.12.13.3.2.2.2.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2"><csymbol cd="latexml" id="S2.Ex2.m1.12.13.3.2.2.2.1.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.1">direct-product</csymbol><apply id="S2.Ex2.m1.12.13.3.2.2.2.2.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.2"><times id="S2.Ex2.m1.12.13.3.2.2.2.2.1.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.2.1"></times><apply id="S2.Ex2.m1.12.13.3.2.2.2.2.2.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.12.13.3.2.2.2.2.2.1.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.2.2">subscript</csymbol><ci id="S2.Ex2.m1.12.13.3.2.2.2.2.2.2.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.2.2.2">ğ‘“</ci><list id="S2.Ex2.m1.4.4.4.5.cmml" xref="S2.Ex2.m1.4.4.4.6"><ci id="S2.Ex2.m1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1">ğ‘</ci><ci id="S2.Ex2.m1.2.2.2.2.cmml" xref="S2.Ex2.m1.2.2.2.2">ğ‘</ci><ci id="S2.Ex2.m1.3.3.3.3.cmml" xref="S2.Ex2.m1.3.3.3.3">ğ‘</ci><ci id="S2.Ex2.m1.4.4.4.4.cmml" xref="S2.Ex2.m1.4.4.4.4">ğ‘‘</ci></list></apply><ci id="S2.Ex2.m1.8.8.cmml" xref="S2.Ex2.m1.8.8">ğœ†</ci></apply><ci id="S2.Ex2.m1.12.13.3.2.2.2.3.cmml" xref="S2.Ex2.m1.12.13.3.2.2.2.3">ğ¼</ci></apply><vector id="S2.Ex2.m1.12.13.3.2.2.3.1.cmml" xref="S2.Ex2.m1.12.13.3.2.2.3.2"><ci id="S2.Ex2.m1.9.9.cmml" xref="S2.Ex2.m1.9.9">ğ‘–</ci><ci id="S2.Ex2.m1.10.10.cmml" xref="S2.Ex2.m1.10.10">ğ‘—</ci><ci id="S2.Ex2.m1.11.11.cmml" xref="S2.Ex2.m1.11.11">ğœ†</ci></vector></apply><apply id="S2.Ex2.m1.12.13.3.2.3.cmml" xref="S2.Ex2.m1.12.13.3.2.3"><ci id="S2.Ex2.m1.12.13.3.2.3.1.cmml" xref="S2.Ex2.m1.12.13.3.2.3.1">Â¯</ci><ci id="S2.Ex2.m1.12.13.3.2.3.2.cmml" xref="S2.Ex2.m1.12.13.3.2.3.2">ğ¼</ci></apply></apply><ci id="S2.Ex2.m1.12.12.cmml" xref="S2.Ex2.m1.12.12">ğœ†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.12c">I_{s}(i,j,\lambda)=f_{a,b,c,d}(\lambda)\odot I(i,j,\lambda)\oslash\overline{I}%
(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.12d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_i , italic_j , italic_Î» ) = italic_f start_POSTSUBSCRIPT italic_a , italic_b , italic_c , italic_d end_POSTSUBSCRIPT ( italic_Î» ) âŠ™ italic_I ( italic_i , italic_j , italic_Î» ) âŠ˜ overÂ¯ start_ARG italic_I end_ARG ( italic_Î» )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p1.13">To further enhance the coverage of illumination conditions, inter- and extrapolation was performed as for the LED simulations.
Overall, this process yielded a set of about 200 different illumination conditions for each light source.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Neural network implementation details</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We feed the HSI cubes into a 3D CNN that employs an autoencoder architecture, utilizing ResNet blocks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib12" title="">12</a>]</cite> in both the encoder and decoder.
Two design decisions were particularly important for our methodâ€™s success:
During training, we only optimize the predicted white reference image and not the resulting calibrated sample image.
Furthermore, we omit skip connections between the encoder and decoder. Both design choices aim to prevent the model from relying heavily on the content of the sample images, instead focusing on learning the illumination information.
As loss function, we employ the MSE-reconstruction loss between the predicted and original white reference image. Further implementation details are provided in Suppl. Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.T1" title="Table 1 â€£ A Supplementary material â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We investigated the following research questions (RQs):</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(<span class="ltx_text ltx_font_bold" id="S3.I1.i1.1.1.1">RQ1)</span></span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">How do dynamically changing lighting conditions in the OR affect the performance of hyperspectral image analysis algorithms?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(<span class="ltx_text ltx_font_bold" id="S3.I1.i2.1.1.1">RQ2)</span></span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Are neural networks capable of replacing white tile recalibration of hyperspectral cameras in the OR?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(<span class="ltx_text ltx_font_bold" id="S3.I1.i3.1.1.1">RQ3)</span></span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">To what extent can neural network-based recalibration mitigate the performance drop of hyperspectral image analysis algorithms under varying lighting conditions?</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.p1.2">For all experiments involving our method, we used the same model trained exclusively on porcine data (<span class="ltx_text ltx_font_italic" id="S3.p1.2.1">ds_dev_pig</span>). The generalization to unseen domains (here: colorchecker boards and rats) was investigated on untouched test sets. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_indent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Experiment RQ1:</span>
We used the traditional approaches Gray-world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib5" title="">5</a>]</cite>, Max-RGB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib16" title="">16</a>]</cite>, and a method based on specular highlights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib2" title="">2</a>]</cite>, as baselines. Additionally, we integrated a learning-based method by adapting the RGB-calibration framework AngularGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib23" title="">23</a>]</cite> to hyperspectral imaging. To this end, we trained AngularGAN on <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">ds_dev_pig</span> augmented by the originally acquired white reference images. As downstream tasks, we conducted semantic organ segmentation and physiological parameter estimation on in vivo data.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">For the semantic organ segmentation task, we leveraged the accurately calibrated <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">ds_test_pig</span> and illumination test set <span class="ltx_text ltx_font_italic" id="S3.p3.1.2">ds_test_ill_hal</span> to obtain four stray light-affected versions of each image in <span class="ltx_text ltx_font_italic" id="S3.p3.1.3">ds_test_pig</span>. Subsequently, the resulting 664 images were recalibrated by one of the methods, followed by the inference of segmentation masks using a public segmentation model trained on calibrated pig organ images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib21" title="">21</a>]</cite>. As segmentation metrics, the Dice similarity coefficient (DSC) and the normalized surface distance (NSD) were used, as recommended by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib18" title="">18</a>]</cite>.
For physiological parameter estimation, recalibration procedures were applied to <span class="ltx_text ltx_font_italic" id="S3.p3.1.4">ds_test_rat</span>, followed by computation of the oxygen saturation, perfusion, hemoglobin, and water index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#bib.bib15" title="">15</a>]</cite>. To gauge calibration performance, mean absolute errors were calculated between the stray light-affected parameters and reference values derived from images devoid of stray light interference.
For both downstream tasks, the hierarchical structure of the data was respected during aggregation.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3.F4" title="Figure 4 â€£ 3 Experiments and Results â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">4</span></a> and Suppl. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F6" title="Figure 6 â€£ A Supplementary material â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">6</span></a>
show that existing HSI calibration techniques lack adequate accuracy. Even the best performing methods (Specular highlights and AngularGAN) come with a decrease of the DSC of more than <math alttext="15\text{\,}\mathrm{\char 37\relax}" class="ltx_Math" display="inline" id="S3.p4.1.m1.3"><semantics id="S3.p4.1.m1.3a"><mrow id="S3.p4.1.m1.3.3" xref="S3.p4.1.m1.3.3.cmml"><mn id="S3.p4.1.m1.1.1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.1.1.cmml">15</mn><mtext id="S3.p4.1.m1.2.2.2.2.2.2" xref="S3.p4.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S3.p4.1.m1.3.3.3.3.3.3" mathvariant="normal" xref="S3.p4.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.3b"><apply id="S3.p4.1.m1.3.3.cmml" xref="S3.p4.1.m1.3.3"><csymbol cd="latexml" id="S3.p4.1.m1.2.2.2.2.2.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S3.p4.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.p4.1.m1.1.1.1.1.1.1">15</cn><csymbol cd="latexml" id="S3.p4.1.m1.3.3.3.3.3.3.cmml" xref="S3.p4.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.3c">15\text{\,}\mathrm{\char 37\relax}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.3d">start_ARG 15 end_ARG start_ARG times end_ARG start_ARG % end_ARG</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.4.1">State-of-the-art methods fail under dynamically changing light conditions. Our approach addresses this issue.</span> (Left) Results on colorchecker dataset <span class="ltx_text ltx_font_italic" id="S3.F4.5.2">ds_test_cc</span>. The boxplots show the cosine similarity between recalibrated and reference spectra, averaged across colors. Red line: Gold standard of manual white tile calibration. (Right) Results on semantic segmentation dataset <span class="ltx_text ltx_font_italic" id="S3.F4.6.3">ds_test_pig</span>. Red line: Mean DSC in the absence of stray light. Points: Different stray light scenarios.</figcaption>
</figure>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Similarly (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3.F5" title="Figure 5 â€£ 3 Experiments and Results â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">5</span></a>), the tissue parameter maps change substantially under varying lighting conditions, even when manual white tile recalibration was performed (cf. Suppl. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F7" title="Figure 7 â€£ A Supplementary material â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">7</span></a>). 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_indent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Experiment RQ2:</span>
To measure the calibration accuracy in an OOD setting with a highly reliable reference, we applied our model trained on porcine data to recalibrate <span class="ltx_text ltx_font_italic" id="S3.p6.1.2">ds_test_cc</span>. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3.F4" title="Figure 4 â€£ 3 Experiments and Results â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">4</span></a>, our method demonstrates the highest average cosine similarity.
They are almost on par with the gold standard of manually acquiring white tile measurements. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_indent" id="S3.p7">
<p class="ltx_p" id="S3.p7.4"><span class="ltx_text ltx_font_bold" id="S3.p7.4.1">Experiment RQ3:</span>
To assess the capability of our method to boost the downstream task performance, we performed Experiment RQ1 on our recalibration approach.
As shown in Figs. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3.F4" title="Figure 4 â€£ 3 Experiments and Results â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S3.F5" title="Figure 5 â€£ 3 Experiments and Results â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">5</span></a>, our method outperforms previous methods by a large margin. For semantic segmentation, relative improvements of the DSC from <math alttext="14\text{\,}\mathrm{\char 37\relax}" class="ltx_Math" display="inline" id="S3.p7.1.m1.3"><semantics id="S3.p7.1.m1.3a"><mrow id="S3.p7.1.m1.3.3" xref="S3.p7.1.m1.3.3.cmml"><mn id="S3.p7.1.m1.1.1.1.1.1.1" xref="S3.p7.1.m1.1.1.1.1.1.1.cmml">14</mn><mtext id="S3.p7.1.m1.2.2.2.2.2.2" xref="S3.p7.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S3.p7.1.m1.3.3.3.3.3.3" mathvariant="normal" xref="S3.p7.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.3b"><apply id="S3.p7.1.m1.3.3.cmml" xref="S3.p7.1.m1.3.3"><csymbol cd="latexml" id="S3.p7.1.m1.2.2.2.2.2.2.cmml" xref="S3.p7.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S3.p7.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.p7.1.m1.1.1.1.1.1.1">14</cn><csymbol cd="latexml" id="S3.p7.1.m1.3.3.3.3.3.3.cmml" xref="S3.p7.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.3c">14\text{\,}\mathrm{\char 37\relax}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.1.m1.3d">start_ARG 14 end_ARG start_ARG times end_ARG start_ARG % end_ARG</annotation></semantics></math> to <math alttext="191\text{\,}\mathrm{\char 37\relax}" class="ltx_Math" display="inline" id="S3.p7.2.m2.3"><semantics id="S3.p7.2.m2.3a"><mrow id="S3.p7.2.m2.3.3" xref="S3.p7.2.m2.3.3.cmml"><mn id="S3.p7.2.m2.1.1.1.1.1.1" xref="S3.p7.2.m2.1.1.1.1.1.1.cmml">191</mn><mtext id="S3.p7.2.m2.2.2.2.2.2.2" xref="S3.p7.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S3.p7.2.m2.3.3.3.3.3.3" mathvariant="normal" xref="S3.p7.2.m2.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.3b"><apply id="S3.p7.2.m2.3.3.cmml" xref="S3.p7.2.m2.3.3"><csymbol cd="latexml" id="S3.p7.2.m2.2.2.2.2.2.2.cmml" xref="S3.p7.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.p7.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S3.p7.2.m2.1.1.1.1.1.1">191</cn><csymbol cd="latexml" id="S3.p7.2.m2.3.3.3.3.3.3.cmml" xref="S3.p7.2.m2.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.3c">191\text{\,}\mathrm{\char 37\relax}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.2.m2.3d">start_ARG 191 end_ARG start_ARG times end_ARG start_ARG % end_ARG</annotation></semantics></math> were obtained. For oxygen saturation estimation, the error could be reduced by <math alttext="50\text{\,}\mathrm{\char 37\relax}" class="ltx_Math" display="inline" id="S3.p7.3.m3.3"><semantics id="S3.p7.3.m3.3a"><mrow id="S3.p7.3.m3.3.3" xref="S3.p7.3.m3.3.3.cmml"><mn id="S3.p7.3.m3.1.1.1.1.1.1" xref="S3.p7.3.m3.1.1.1.1.1.1.cmml">50</mn><mtext id="S3.p7.3.m3.2.2.2.2.2.2" xref="S3.p7.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S3.p7.3.m3.3.3.3.3.3.3" mathvariant="normal" xref="S3.p7.3.m3.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.3.m3.3b"><apply id="S3.p7.3.m3.3.3.cmml" xref="S3.p7.3.m3.3.3"><csymbol cd="latexml" id="S3.p7.3.m3.2.2.2.2.2.2.cmml" xref="S3.p7.3.m3.2.2.2.2.2.2">times</csymbol><cn id="S3.p7.3.m3.1.1.1.1.1.1.cmml" type="integer" xref="S3.p7.3.m3.1.1.1.1.1.1">50</cn><csymbol cd="latexml" id="S3.p7.3.m3.3.3.3.3.3.3.cmml" xref="S3.p7.3.m3.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.3.m3.3c">50\text{\,}\mathrm{\char 37\relax}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.3.m3.3d">start_ARG 50 end_ARG start_ARG times end_ARG start_ARG % end_ARG</annotation></semantics></math> to <math alttext="69\text{\,}\mathrm{\char 37\relax}" class="ltx_Math" display="inline" id="S3.p7.4.m4.3"><semantics id="S3.p7.4.m4.3a"><mrow id="S3.p7.4.m4.3.3" xref="S3.p7.4.m4.3.3.cmml"><mn id="S3.p7.4.m4.1.1.1.1.1.1" xref="S3.p7.4.m4.1.1.1.1.1.1.cmml">69</mn><mtext id="S3.p7.4.m4.2.2.2.2.2.2" xref="S3.p7.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S3.p7.4.m4.3.3.3.3.3.3" mathvariant="normal" xref="S3.p7.4.m4.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.4.m4.3b"><apply id="S3.p7.4.m4.3.3.cmml" xref="S3.p7.4.m4.3.3"><csymbol cd="latexml" id="S3.p7.4.m4.2.2.2.2.2.2.cmml" xref="S3.p7.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S3.p7.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S3.p7.4.m4.1.1.1.1.1.1">69</cn><csymbol cd="latexml" id="S3.p7.4.m4.3.3.3.3.3.3.cmml" xref="S3.p7.4.m4.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.4.m4.3c">69\text{\,}\mathrm{\char 37\relax}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.4.m4.3d">start_ARG 69 end_ARG start_ARG times end_ARG start_ARG % end_ARG</annotation></semantics></math>. Similar performance gains were obtained for other tissue parameters (cf. Suppl. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F7" title="Figure 7 â€£ A Supplementary material â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">7</span></a>). A qualitative assessment of the high fidelity of our recalibrated tissue spectra is available in Suppl. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07094v1#S1.F8" title="Figure 8 â€£ A Supplementary material â€£ Deep intra-operative illumination calibration of hyperspectral cameras"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S3.F5.2.1">In contrast to related methods, our approach generalizes across species.</span>
(Left) Organ-specific absolute oxygen saturation errors between calibrated rat images without stray light and corresponding stray light images that are recalibrated by one of the methods. Red line: Mean performance of the gold standard (manual white tile calibration). (Right) Our method yields precise hemoglobin index estimates under dynamically changing lighting conditions.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We were the first to provide in vivo evidence that dynamically changing lighting conditions in the OR can cause dramatic failures in HSI analysis. This is a finding of high clinical relevance because the manual white tile-based recalibration of cameras during surgery severely disrupts the clinical workflow and may currently hinder widespread clinical adoption of HSI cameras.
According to our comprehensive study, our method represents the only calibration model that is capable of maintaining high accuracy independently of the downstream task and domain, indicating high applicability for clinical use cases. It also features several major conceptual advantages: White reference measurements are not only impractical as they suffer from sterilization and workflow issues, but are also prone to oversaturation. This explains the suboptimal performance on the rat data (see Fig. 6).
Specular highlights and Max-RGB exhibit high calibration accuracy on the colorchecker board, as both methods calibrate the images with the white color field by design, but fail to generalize to in vivo scenarios. Note that these methods recover a global illumination estimate, which is not sufficient in the case of spatially heterogeneous illumination encountered in the OR. In fact, we also saw drops in performance when reducing the estimations of white tile calibration (classic and data-driven) to a global estimate.
Overall, the core strength of our approach is its generalizability. Notably, it outperforms the competing neural network method AngularGAN even when trained on the exact same data as our method. We attribute this to the inherent domain shift of auto-encoded images.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">A limitation of our work could be seen in the fact that we did not cover all possible illumination settings that can occur in practice. However, as we focused on the most important light sources (surgical lights and ceiling light) and conducted our validation on highly diverse datasets, we are confident that our conclusions will hold in diverse settings.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">In conclusion, our work presents the first learning-based light calibration method for hyperspectral imaging. The proposed methodology not only outperforms previously proposed approaches in various settings but can be seamlessly incorporated into hyperspectral imaging systems for ORs. Our work could therefore pave the way for clinical workflow-optimized and robust HSI in surgery.

<span class="ltx_text" id="S4.p3.1.1" style="font-size:90%;"></span></p>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Acknowledgments.</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1"><span class="ltx_text" id="S4.SS0.SSS1.p1.1.1" style="font-size:90%;">This project was supported by the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (NEURAL SPICING, 101002198), the National Center for Tumor Diseases (NCT), Heidelbergâ€™s Surgical Oncology Program, the German Cancer Research Center (DKFZ), and the Helmholtz Association under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for Health). We also acknowledge the support through state funds for the Innovation Campus Health + Life Science Alliance Heidelberg Mannheim from the structured postdoc program for Alexander Studier-Fischer: Artificial Intelligence in Health (AIH).</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span>Disclosure of Interests.</h4>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1"><span class="ltx_text" id="S4.SS0.SSS2.p1.1.1" style="font-size:90%;">The authors have no competing interests to declare that are relevant to the content of this article.
</span><span class="ltx_text" id="S4.SS0.SSS2.p1.1.2"></span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ayala, L., Adler, T.J., Seidlitz, S., Wirkert, S., Engels, C., Seitel, A., Sellner, J., Aksenov, A., Bodenbach, M., Bader, P., etÂ al.: Spectral imaging enables contrast agentâ€“free real-time ischemia monitoring in laparoscopic surgery. Science advances <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">9</span>(10), eadd6778 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ayala, L., Seidlitz, S., Vemuri, A., Wirkert, S.J., Kirchner, T., Adler, T.J., Engels, C., Teber, D., Maier-Hein, L.: Light source calibration for multispectral imaging in surgery. International Journal of Computer Assisted Radiology and Surgery <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">15</span>, 1117â€“1125 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ayala, L.A., Wirkert, S.J., GrÃ¶hl, J., Herrera, M.A., Hernandez-Aguilera, A., Vemuri, A., Santos, E., Maier-Hein, L.: Live monitoring of haemodynamic changes with multispectral image analysis. In: OR 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging: Second International Workshop, OR 2.0 2019, and Second International Workshop, MLCN 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings 2. pp. 38â€“46. Springer (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bahl, A., Horgan, C.C., Janatka, M., MacCormac, O.J., Noonan, P., Xie, Y., Qiu, J., Cavalcanti, N., FÃ¼rnstahl, P., Ebner, M., etÂ al.: Synthetic white balancing for intra-operative hyperspectral imaging. Journal of Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">10</span>(4), 046001â€“046001 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Buchsbaum, G.: A spatial processor model for object colour perception. Journal of the Franklin institute <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">310</span>(1), 1â€“26 (1980)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Clancy, N.T., Jones, G., Maier-Hein, L., Elson, D.S., Stoyanov, D.: Surgical spectral imaging. Medical image analysis <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">63</span>, 101699 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Das, P., Liu, Y., Karaoglu, S., Gevers, T.: Generative models for multi-illumination color constancy. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1194â€“1203 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ebner, M., Nabavi, E., Shapey, J., Xie, Y., Liebmann, F., Spirig, J.M., Hoch, A., Farshad, M., Saeed, S.R., Bradford, R., etÂ al.: Intraoperative hyperspectral label-free imaging: from system design to first-in-patient translation. Journal of Physics D: Applied Physics <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">54</span>(29), 294003 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gao, S.B., Ren, Y.Z., Zhang, M., Li, Y.J.: Combining bottom-up and top-down visual mechanisms for color constancy under varying illumination. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">28</span>(9), 4387â€“4400 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Halicek, M., Fabelo, H., Ortega, S., Callico, G.M., Fei, B.: In-vivo and ex-vivo tissue analysis through hyperspectral imaging techniques: revealing the invisible features of cancer. Cancers <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">11</span>(6), Â 756 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Halicek, M., Lu, G., Little, J.V., Wang, X., Patel, M., Griffith, C.C., El-Deiry, M.W., Chen, A.Y., Fei, B.: Deep convolutional neural networks for classifying head and neck cancer using hyperspectral imaging. Journal of biomedical optics <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">22</span>(6), 060503â€“060503 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770â€“778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Holmer, A., Tetschke, F., Marotz, J., Malberg, H., Markgraf, W., Thiele, C., Kulcke, A.: Oxygenation and perfusion monitoring with a hyperspectral camera system for chemical based tissue analysis of skin and organs. Physiological measurement <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">37</span>(11), Â 2064 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hussain, M.A., Akbari, A.S.: Color constancy algorithm for mixed-illuminant scene images. IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">6</span>, 8964â€“8976 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kulcke, A., Holmer, A., Wahl, P., Siemers, F., Wild, T., Daeschlein, G.: A compact hyperspectral camera for measurement of perfusion parameters in medicine. Biomedical Engineering/Biomedizinische Technik <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">63</span>(5), 519â€“527 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Land, E.H.: The retinex theory of color vision. Scientific american <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">237</span>(6), 108â€“129 (1977)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Li, Y., Fu, Q., Heidrich, W.: Multispectral illumination estimation using deep unrolling network. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2672â€“2681 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Maier-Hein, L., Reinke, A., Godau, P., Tizabi, M.D., Buettner, F., Christodoulou, E., Glocker, B., Isensee, F., Kleesiek, J., Kozubek, M., etÂ al.: Metrics reloaded: recommendations for image analysis validation. Nature methods pp. 1â€“18 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Mutimbu, L., Robles-Kelly, A.: Multiple illuminant color estimation via statistical inference on factor graphs. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">25</span>(11), 5383â€“5396 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Seidlitz, S., Sellner, J., Odenthal, J., Ã–zdemir, B., Studier-Fischer, A., KnÃ¶dler, S., Ayala, L., Adler, T.J., Kenngott, H.G., Tizabi, M., etÂ al.: Robust deep learning-based semantic organ segmentation in hyperspectral images. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">80</span>, 102488 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sellner, J., Seidlitz, S., Studier-Fischer, A., Motta, A., Ã–zdemir, B., MÃ¼ller-Stich, B.P., Nickel, F., Maier-Hein, L.: Semantic segmentation of surgical hyperspectral images under geometric domain shifts. arXiv preprint arXiv:2303.10972 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Shapey, J., Xie, Y., Nabavi, E., Bradford, R., Saeed, S.R., Ourselin, S., Vercauteren, T.: Intraoperative multispectral and hyperspectral label-free imaging: A systematic review of in vivo clinical studies. Journal of biophotonics <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">12</span>(9), e201800455 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sidorov, O.: Conditional gans for multi-illuminant color constancy: Revolution or yet another approach? In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp.Â 0â€“0 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Studier-Fischer, A., Seidlitz, S., Sellner, J., Bressan, M., Ã–zdemir, B., Ayala, L., Odenthal, J., Knoedler, S., Kowalewski, K.F., Haney, C.M., etÂ al.: Heiporspectral-the heidelberg porcine hyperspectral imaging dataset of 20 physiological organs. Scientific Data <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">10</span>(1), Â 414 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Trajanovski, S., Shan, C., Weijtmans, P.J., deÂ Koning, S.G.B., Ruers, T.J.: Tongue tumor detection in hyperspectral images using deep learning semantic segmentation. IEEE transactions on biomedical engineering <span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">68</span>(4), 1330â€“1340 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Wirkert, S.J., Vemuri, A.S., Kenngott, H.G., Moccia, S., GÃ¶tz, M., Mayer, B.F., Maier-Hein, K.H., Elson, D.S., Maier-Hein, L.: Physiological parameter estimation from multispectral images unleashed. In: Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20. pp. 134â€“141. Springer (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Supplementary material</h2>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.2.3.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.3.1.1.1">
<span class="ltx_p" id="S1.T1.2.3.1.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.3.1.1.1.1.1">Optimization algorithm</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.2.3.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.3.1.2.1">
<span class="ltx_p" id="S1.T1.2.3.1.2.1.1" style="width:170.7pt;">Adam Optimizer</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.2.3.1">
<span class="ltx_p" id="S1.T1.2.2.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.2.3.1.1.1">Learning rate</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.2.2.2">
<span class="ltx_p" id="S1.T1.2.2.2.2.2" style="width:170.7pt;">Decay from <math alttext="10^{-4}" class="ltx_Math" display="inline" id="S1.T1.1.1.1.1.1.m1.1"><semantics id="S1.T1.1.1.1.1.1.m1.1a"><msup id="S1.T1.1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.1.m1.1.1.cmml"><mn id="S1.T1.1.1.1.1.1.m1.1.1.2" xref="S1.T1.1.1.1.1.1.m1.1.1.2.cmml">10</mn><mrow id="S1.T1.1.1.1.1.1.m1.1.1.3" xref="S1.T1.1.1.1.1.1.m1.1.1.3.cmml"><mo id="S1.T1.1.1.1.1.1.m1.1.1.3a" xref="S1.T1.1.1.1.1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S1.T1.1.1.1.1.1.m1.1.1.3.2" xref="S1.T1.1.1.1.1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.1.m1.1b"><apply id="S1.T1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S1.T1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S1.T1.1.1.1.1.1.m1.1.1.2">10</cn><apply id="S1.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1.3"><minus id="S1.T1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1.3"></minus><cn id="S1.T1.1.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S1.T1.1.1.1.1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.1.m1.1c">10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.1.1.1.1.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="10^{-5}" class="ltx_Math" display="inline" id="S1.T1.2.2.2.2.2.m2.1"><semantics id="S1.T1.2.2.2.2.2.m2.1a"><msup id="S1.T1.2.2.2.2.2.m2.1.1" xref="S1.T1.2.2.2.2.2.m2.1.1.cmml"><mn id="S1.T1.2.2.2.2.2.m2.1.1.2" xref="S1.T1.2.2.2.2.2.m2.1.1.2.cmml">10</mn><mrow id="S1.T1.2.2.2.2.2.m2.1.1.3" xref="S1.T1.2.2.2.2.2.m2.1.1.3.cmml"><mo id="S1.T1.2.2.2.2.2.m2.1.1.3a" xref="S1.T1.2.2.2.2.2.m2.1.1.3.cmml">âˆ’</mo><mn id="S1.T1.2.2.2.2.2.m2.1.1.3.2" xref="S1.T1.2.2.2.2.2.m2.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.2.2.m2.1b"><apply id="S1.T1.2.2.2.2.2.m2.1.1.cmml" xref="S1.T1.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S1.T1.2.2.2.2.2.m2.1.1.1.cmml" xref="S1.T1.2.2.2.2.2.m2.1.1">superscript</csymbol><cn id="S1.T1.2.2.2.2.2.m2.1.1.2.cmml" type="integer" xref="S1.T1.2.2.2.2.2.m2.1.1.2">10</cn><apply id="S1.T1.2.2.2.2.2.m2.1.1.3.cmml" xref="S1.T1.2.2.2.2.2.m2.1.1.3"><minus id="S1.T1.2.2.2.2.2.m2.1.1.3.1.cmml" xref="S1.T1.2.2.2.2.2.m2.1.1.3"></minus><cn id="S1.T1.2.2.2.2.2.m2.1.1.3.2.cmml" type="integer" xref="S1.T1.2.2.2.2.2.m2.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.2.2.m2.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.2.2.2.2.2.m2.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.4.1.1.1">
<span class="ltx_p" id="S1.T1.2.4.1.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.4.1.1.1.1.1">Augmentations during training</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.4.1.2.1">
<span class="ltx_p" id="S1.T1.2.4.1.2.1.1" style="width:170.7pt;">Rotating, flipping</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.5.2.1.1">
<span class="ltx_p" id="S1.T1.2.5.2.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.5.2.1.1.1.1">Number of parameters</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.5.2.2.1">
<span class="ltx_p" id="S1.T1.2.5.2.2.1.1" style="width:170.7pt;">7.7 million</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.6.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.6.3.1.1">
<span class="ltx_p" id="S1.T1.2.6.3.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.6.3.1.1.1.1">Number of channels in backbone</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.6.3.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.6.3.2.1">
<span class="ltx_p" id="S1.T1.2.6.3.2.1.1" style="width:170.7pt;">512</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.7.4.1.1">
<span class="ltx_p" id="S1.T1.2.7.4.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.7.4.1.1.1.1">Batch size during training</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.7.4.2.1">
<span class="ltx_p" id="S1.T1.2.7.4.2.1.1" style="width:170.7pt;">5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.8.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.8.5.1.1">
<span class="ltx_p" id="S1.T1.2.8.5.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.8.5.1.1.1.1">Training time</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.8.5.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.8.5.2.1">
<span class="ltx_p" id="S1.T1.2.8.5.2.1.1" style="width:170.7pt;">4 hours</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.9.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.9.6.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.9.6.1.1">
<span class="ltx_p" id="S1.T1.2.9.6.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.9.6.1.1.1.1">Software</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.2.9.6.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.9.6.2.1">
<span class="ltx_p" id="S1.T1.2.9.6.2.1.1" style="width:170.7pt;">PyTorch 2.0.1</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.10.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.2.10.7.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.10.7.1.1">
<span class="ltx_p" id="S1.T1.2.10.7.1.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.2.10.7.1.1.1.1">Hardware</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.2.10.7.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.2.10.7.2.1">
<span class="ltx_p" id="S1.T1.2.10.7.2.1.1" style="width:170.7pt;">NVIDIA GeForce RTX 3090 GPU</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Training details</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="468" id="S1.F6.g1" src="x6.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Results on the semantic segmentation dataset. Red line: Mean normalized surface dice (NSD) in the absence of straylight. Points: Different stray light scenarios.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S1.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Organ-specific absolute perfusion (left), hemoglobin (middle) and water (right) index errors between calibrated rat images without stray light and corresponding stray light images that are recalibrated by one of the methods. Red line: Mean performance of the gold standard (manual white tile calibration).</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="529" id="S1.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>While tissue spectra in the presence of stray light (brown) significantly deviate from reference spectra in the absence of straylight (magenta), our recalibration approach accurately restores the tissue spectra (blue). Solid lines denote the mean L-1 normalized reflectance. Shaded areas depict the standard deviation interval.
</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 11 07:18:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
