<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging</title>
<!--Generated on Thu Sep 12 17:13:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Brain tumor segmentation MRI
Deep learning Pediatric brain tumors Meningioma Metastases.
" lang="en" name="keywords"/>
<base href="/html/2409.08232v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S1" title="In Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2" title="In Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS1" title="In 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS2" title="In 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep Learning Models and Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS2.SSS1" title="In 2.2 Deep Learning Models and Experimental Setup ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>nnU-Net</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS2.SSS2" title="In 2.2 Deep Learning Models and Experimental Setup ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Swin UNETR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS3" title="In 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model Ensembling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.SS4" title="In 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Post-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S3" title="In Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S4" title="In Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S5" title="In Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S5.SS0.SSS1" title="In 5 Conclusion ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Hospital, Washington, DC 20010, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Biomedical Image Technologies, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid 28040, Spain and CIBER-BBN, ISCIII, Madrid, Spain </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Princeton University, Princeton, NJ 08544, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>School of Medicine and Health Sciences, George Washington University, Washington, DC 20052, USA</span></span></span>
<h1 class="ltx_title ltx_title_document">Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Capellán-Martín
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zhifan Jiang
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Abhijeet Parida
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xinyang Liu
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Van Lam
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hareem Nisar
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Austin Tapp
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Sarah Elsharkawi
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> María J. Ledesma-Carbayo
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Syed Muhammad Anwar and Marius George Linguraru
</span><span class="ltx_author_notes">11441144</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Segmenting brain tumors in multi-parametric magnetic resonance imaging enables performing quantitative analysis in support of clinical trials and personalized patient care. This analysis provides the potential to impact clinical decision-making processes, including diagnosis and prognosis. In 2023, the well-established Brain Tumor Segmentation (BraTS) challenge presented a substantial expansion with eight tasks and 4,500 brain tumor cases. In this paper, we present a deep learning-based ensemble strategy that is evaluated for newly included tumor cases in three tasks: pediatric brain tumors (PED), intracranial meningioma (MEN), and brain metastases (MET). In particular, we ensemble outputs from state-of-the-art nnU-Net and Swin UNETR models on a region-wise basis. Furthermore, we implemented a targeted post-processing strategy based on a cross-validated threshold search to improve the segmentation results for tumor sub-regions. The evaluation of our proposed method on unseen test cases for the three tasks resulted in lesion-wise Dice scores for PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; and MET: 0.555, 0.6, 0.58; for the enhancing tumor, tumor core, and whole tumor, respectively. Our method was ranked first for PED, third for MEN, and fourth for MET, respectively.</p>
<p class="ltx_p" id="id2.id2">* These authors contributed equally.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Brain tumor segmentation MRI
Deep learning Pediatric brain tumors Meningioma Metastases.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The brain tumor segmentation (BraTS) challenge held in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) conference, established in 2012, has generated a benchmark dataset for the segmentation of adult brain gliomas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib21" title="">21</a>]</cite>. The BraTS 2023 challenge has expanded to a cluster of challenges, encompassing a variety of tumor types alongside augmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib25" title="">25</a>]</cite>. Herein, we propose a segmentation technique for newly introduced tasks featuring smaller datasets or new types of tumors. Particularly, these tasks include pediatric brain tumors (PED) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib16" title="">16</a>]</cite>, intracranial meningioma (MEN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib18" title="">18</a>]</cite>, and brain metastasis (MET) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib22" title="">22</a>]</cite>. This paper presents the methodology primarily developed for PED tumor segmentation, which was also adapted for MEN and MET segmentation tasks.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Brain cancer has become the leading cause of cancer death among children in the United States <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib8" title="">8</a>]</cite>. Although rare, pediatric high-grade brain tumors can be aggressive. For example, the median overall survival was reported to be less than one year for pediatric diffuse mid-line gliomas (DMGs, which replaced the formerly known diffuse intrinsic pontine gliomas (DIPGs) to emphasize that the disease may affect areas other than the pons) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib24" title="">24</a>]</cite>. Multi-parametric magnetic resonance imaging (mpMRI) is essential in pediatric brain tumor diagnosis and monitoring of tumor progression. While adult and pediatric brain tumors share certain similarities, their locations in the brain and imaging characteristics may be different. For DMGs, necrosis is rare or unclear and the tumor may or may not present enhancement on post-gadolinium T1-weighted MRI. Hence, imaging tools specially designed to analyze pediatric brain tumors are necessary to improve clinical management. Automatic tumor segmentation is usually the first and most important step for the success of such analysis. Over the years, several methods have been presented for adult brain tumor segmentation using the BraTS dataset, however, efforts are needed to develop methods more suitable for pediatric brain tumors. The recently introduced PED task in BraTS 2023 provided an opportunity to develop such methods.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Although, a few previous works addressed pediatric brain tumors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib20" title="">20</a>]</cite>, their automatic segmentation remains challenging. This is due to a lack of training data and variation in heterogeneous histologic sub-regions including peritumoral edematous/invaded tissue, necrotic core, and enhancing tumor. BraTS-PED 2023 provides these types of data for the first benchmarking initiative on pediatric brain tumor segmentation. The task provides the largest annotated publicly-available retrospective cohort of high-grade gliomas including astrocytomas and DMG/DIPG in children.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we developed an ensemble approach involving two state-of-the-art deep learning models. Our approach to segmenting pediatric tumors is tested on two additional tasks at BraTS 2023: the segmentation of meningiomas (MEN) and brain metastases (MET). Meningioma is the most common primary intracranial tumor in adults and can result in significant morbidity and mortality for affected patients. Brain metastases are the most common form of central nervous system (CNS) malignancy in adults. Accurate detection of small metastatic lesions is essential for patient prognosis, as missing even one lesion can lead to repeated interventions and treatment delays. For each task, training was performed only on the dataset provided by each sub-challenge.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="282" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Proposed method: model ensembling and post-processing pipeline. Outputs are obtained from two state-of-the-art deep learning models. These outputs are subjected to nonlinear activation functions and ensembling strategies. Finally, the ensembled predictions are subjected to a specifically tailored post-processing step.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The model ensemble technique is a widely recognized strategy used with machine learning methods and is aimed at increasing the stability and accuracy of model predictions. To harness the benefits inherent to both convolutional neural networks and vision transformers, we adopted an ensemble approach (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F1" title="Figure 1 ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>) involving two state-of-the-art models: nnU-Net (“no new U-Net”, winner of BraTS 2020) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib13" title="">13</a>]</cite> and Swin UNETR (“Swin U-Net transformers”, top-performing model of BraTS 2021) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib27" title="">27</a>]</cite>. Given that all tasks address the segmentation of multiple tumor sub-regions, our analysis and ensemble approach were conducted in a label-wise manner.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data description</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">For all three sub-challenges, mpMRI data included pre- and post-gadolinium T1-weighted (T1 and T1CE), T2-weighted (T2), and T2-weighted fluid attenuated inversion recovery (T2-FLAIR) MRI. The mpMRI scans were pre-processed in a standardized fashion, including co-registration to the same anatomical template <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib25" title="">25</a>]</cite>, re-sampling to an isotropic resolution, and skull-stripping. Each task provided three manual segmentation labels with slightly different definitions. For example, the labels for PED included enhancing tumor (ET), non-enhancing component (NCR - a combination of non-enhancing tumor, cystic component, and necrosis), and peritumoral edematous area (ED). The labels of MEN and MET included enhancing tumor (ET), non-enhancing tumor core (NETC), and surrounding non-enhancing FLAIR hyperintensity (SNFH). Based on the three labels, three tumor sub-regions were defined to evaluate algorithm performance: enhancing tumor (ET), tumor coure (TC) and whole tumor (WT). The TC region included ET and NCR labels, and WT included all three labels. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F2" title="Figure 2 ‣ 2.1 Data description ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> shows samples of training examples for the three tasks.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="S2.F2.g1" src="extracted/5851248/src/methods/Data.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Training examples in the PED, MEN, and MET tasks (from top to bottom) with the following tumor subregions: enhancing tumor ET (blue), a combination of nonenhancing tumor, cystic component, and necrosis NCR (red), and peritumoral edematous area ED in PED or surrounding nonenhancing FLAIR hyperintensity (SNFH) in MEN and MET (green).</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Imaging data for PED are split into training (n=99), validation (n=45), and additional testing subsets. Imaging data for MEN are split into training (n=1,000), validation (n=141), and testing datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib18" title="">18</a>]</cite>. Imaging data for MET are split into training (n=165), validation (n=31), and testing datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deep Learning Models and Experimental Setup</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>nnU-Net</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The nnU-Net, which is based on the U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib26" title="">26</a>]</cite>, is a self-configuring deep learning framework for semantic segmentation. According to the specific imaging modality and unique attributes of each dataset, the framework autonomously adjusts its internal configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib13" title="">13</a>]</cite>. This results in an improved segmentation performance and generalization when compared to other state-of-the-art methods for biomedical image segmentation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">For each of the three tasks, we trained a full-resolution 3D nnU-Net (v2) model using a five-fold cross-validation approach. A preprocessing consisting of a zero mean unit variance normalization was applied to input images. Each input image was divided into patches of 128x128x128 voxels for PED and 128x160x112 for MEN and MET. The model output consisted on three channels corresponding to the three tumor sub-regions. Region-based training was employed and the patch size was determined by the GPU memory allocation, favoring larger patches while remaining within the GPU’s capacity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib13" title="">13</a>]</cite>. We used a class-weight loss function that combined Dice loss and cross entropy loss. To optimize the loss function, we used the stochastic gradient descent (SGD) optimizer with Nesterov momentum with the following parameters: initial learning rate of 0.01, momentum of 0.99, and weight decay of 3e-05. Each of the five folds was trained for 100 epochs on an NVIDIA A100 (40 GB) GPU. At inference time, images were predicted using a sliding window approach. The window size matched the patch size used during training. The nnU-Net implementation is available in an open-source repository: <a class="ltx_ref ltx_href" href="https://github.com/MIC-DKFZ/nnUNet" title="">https://github.com/MIC-DKFZ/nnUNet</a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Swin UNETR</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">The Swin UNETR framework employs a vision transformer (ViT)-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib9" title="">9</a>]</cite> hierarchical structure for localized self-attention using non-overlapping windows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib27" title="">27</a>]</cite>. Swin UNETR’s innovative local window self-attention outperforms traditional ViT, which is well suited to multiscale tasks. The framework includes a 3D Swin transformer encoder with window-shifting for extended receptive fields, and it connects to a multiscale residual U-Net-like decoder to perform tasks like 3D medical image segmentation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">For each of the three tasks, we trained a full-resolution 3D Swin UNETR model using a five-fold cross-validation approach. A preprocessing consisting of a zero mean unit variance normalization was applied to input images. Each input image was sampled four times using patches of 96x96x96 voxels to fully utilize the GPU’s memory. The model output was 4 channels corresponding to the three labels and background. We used a class-weight loss function that combined Dice loss and focal loss. To optimize the loss function, we used the AdamW optimizer with an initial learning rate of 0.0001, momentum of 0.99, and weight decay of 3e-05. Each of the folds were trained for 600 epochs on an NVIDIA A5000 (24 GB) GPU and NVIDIA A6000 (48GB) GPU. The Swin UNETR implementation is part of the PyTorch-based framework MONAI: <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://monai.io" title="">https://monai.io</a></span></span></span>. Hyper-parameter optimization was carried out using Optuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib2" title="">2</a>]</cite>: <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://optuna.readthedocs.io/" title="">optuna.readthedocs.io/</a></span></span></span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Ensembling</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To enhance the accuracy and robustness of the segmentation outcome, we propose a model ensembling strategy. This approach (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F1" title="Figure 1 ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>) involves harnessing the complementary strengths of the two models described, nnU-Net and Swin UNETR, to collectively address the task of pixel classification.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The ensembling strategy was optimized for each task based on each model’s performance. For the PED task, we trained an nnU-Net solely on the ET region and ensembled its predictions with a Swin UNETR, which was trained on all labels, for ET region prediction.
For TC and WT, we ensembled predictions from nnU-Net and Swin UNETR (as described previously), both trained on all labels. For MEN, predictions from both implementations, trained on all labels, were combined to generate ET, TC, and WT regions. Finally, for the MET task, only nnU-Net, trained on all labels, was used after experimental evaluation, as Swin UNETR showed inferior performance in this scenario.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">When using a combination of nnU-Net and Swin UNETR, we ensembled the outputs (after applying the corresponding nonlinear activation functions) from each fold obtained during cross-validation (<math alttext="k=5" class="ltx_Math" display="inline" id="S2.SS3.p3.1.m1.1"><semantics id="S2.SS3.p3.1.m1.1a"><mrow id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mi id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS3.p3.1.m1.1.1.1" xref="S2.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><eq id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1.1"></eq><ci id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2">𝑘</ci><cn id="S2.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S2.SS3.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.1.m1.1d">italic_k = 5</annotation></semantics></math>) training of both models. This allowed us to leverage the advantages of both approaches.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Post-processing</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="206" id="S2.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Post-processing strategy. The ensemble predictions were first cleaned of small disconnected regions. Then, for the PED task, ET and ED labels were redefined based on ET/WT and ED/WT thresholds, respectively.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">New performance metrics were introduced in this year’s BraTS challenge to assess segmentation models at a lesion-wise level rather than over the entire/multiple tumor region(s). We developed a post-processing strategy to adapt to the lesion-wise scores. This post-processing (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F3" title="Figure 3 ‣ 2.4 Post-processing ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">3</span></a>) was applied on the ensembled predictions and, first, removed disconnected regions (which contributed to undesired noise) smaller than 130 voxels for PED, 110 voxels for MEN, and 15 voxels for MET (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F4" title="Figure 4 ‣ 2.4 Post-processing ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">4</span></a>). These optimal threshold values were determined by experimenting with the cross-validation data.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="133" id="S2.F4.g1" src="extracted/5851248/src/results/threshold.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Threshold search on the cross-validation set for identifying small disconnected regions. LW refers to lesion-wise metrics.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Subsequently, given the nature of pediatric brain tumors, in the PED task, numerous cases had empty ground truth annotations for the ET and the ED labels. Therefore, redefining these labels in cases where they fell below a certain threshold with respect to the WT volume was particularly useful. For example, if the ET/WT ratio fell below the threshold, the ET label would be redefined to either NCR or ED, which corresponds to the TC region. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F5.sf1" title="In Figure 5 ‣ 2.4 Post-processing ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">5(a)</span></a> displays the lesion-wise Dice score <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.1">vs.</span> ET/WT threshold curve obtained from the optimal threshold search process performed on the cross-validation sets. For ET, we obtained an optimal threshold of 0.04. On the other hand, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S2.F5.sf2" title="In Figure 5 ‣ 2.4 Post-processing ‣ 2 Methods ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">5(b)</span></a> displays the lesion-wise Dice score vs ED/WT threshold curve obtained from the same process but applied to the ED label, which yielded an optimal threshold of 1.00. These optimal threshold values were also determined by experimenting with the cross-validation data.</p>
</div>
<figure class="ltx_figure" id="S2.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="569" id="S2.F5.sf1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F5.sf1.3.2" style="font-size:90%;">ET/WT threshold search</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="570" id="S2.F5.sf2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F5.sf2.3.2" style="font-size:90%;">ED/WT threshold search</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">Threshold search on the cross-validation set. Abbreviations: ED, Peritumoral edematous; ET, enhancing tumor; WT, whole tumor.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">By carrying out these post-processing steps, we aimed for a balance between maintaining the integrity of the original predictions and enhancing the final scores by mitigating potential misclassifications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Task</span></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.2.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">Model</span></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.2.1.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.5.1">LW Dice</span></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.2.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.7.1">LW HD95 (mm)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td" id="S3.T1.2.2.2.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.2.2.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.3.1">ET</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.4.1">TC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.5.1">WT</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.2.2.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.7.1">ET</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.8.1">TC</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.2.2.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.9.1">WT</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.1" rowspan="4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.1.1">PED</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.3.3.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.3.3">nnU-Net</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.5">0.462</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.6">0.731</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.7">0.798</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.3.3.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.9">224.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.3.10">30.41</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.3.3.11">26.08</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.4">
<td class="ltx_td" id="S3.T1.2.4.4.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.4.2">Swin UNETR</td>
<td class="ltx_td" id="S3.T1.2.4.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.4">0.362</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.5">0.641</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.6">0.716</td>
<td class="ltx_td" id="S3.T1.2.4.4.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.8">191.47</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.9">82.51</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.4.4.10">63.33</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.5">
<td class="ltx_td" id="S3.T1.2.5.5.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.5.2">Ensemble</td>
<td class="ltx_td" id="S3.T1.2.5.5.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5.4">0.466</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5.5">0.73</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5.6">0.797</td>
<td class="ltx_td" id="S3.T1.2.5.5.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5.8">158.89</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5.9">39.34</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.5.5.10">35.11</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.6">
<td class="ltx_td" id="S3.T1.2.6.6.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.6.2">Post-processing</td>
<td class="ltx_td" id="S3.T1.2.6.6.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.4.1">0.733</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.5.1">0.782</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.6.1">0.817</span></td>
<td class="ltx_td" id="S3.T1.2.6.6.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.8.1">75.93</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.9.1">25.54</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.6.6.10"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.10.1">24.18</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.1" rowspan="4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.7.7.1.1">MEN</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.7.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.7.7.3">nnU-Net</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.7.7.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.5">0.818</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.6">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.7">0.794</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.7.7.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.9">51.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.10">55.74</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.7.7.11">55.59</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8.8">
<td class="ltx_td" id="S3.T1.2.8.8.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.8.2">Swin UNETR</td>
<td class="ltx_td" id="S3.T1.2.8.8.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.4">0.64</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.5">0.644</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.6">0.59</td>
<td class="ltx_td" id="S3.T1.2.8.8.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.8">117.85</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.9">114.43</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.8.8.10">135.63</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9.9">
<td class="ltx_td" id="S3.T1.2.9.9.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.9.9.2">Ensemble</td>
<td class="ltx_td" id="S3.T1.2.9.9.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.4">0.833</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.5">0.832</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.6">0.804</td>
<td class="ltx_td" id="S3.T1.2.9.9.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.8">45.94</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.9">43.68</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.9.9.10">53.26</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.10.10">
<td class="ltx_td" id="S3.T1.2.10.10.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.10.10.2">Post-processing</td>
<td class="ltx_td" id="S3.T1.2.10.10.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.4.1">0.852</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.5.1">0.846</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.6.1">0.832</span></td>
<td class="ltx_td" id="S3.T1.2.10.10.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.8.1">37.98</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.9.1">38.68</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.10.10.10"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.10.10.1">42.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.11.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.2.11.11.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.11.11.1.1">MET</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.11.11.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.11.11.3">nnU-Net</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.11.11.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.11.11.5">0.565</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.11.11.6">0.614</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.11.11.7">0.545</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.11.11.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.11.11.9">117.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.11.11.10">110.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.11.11.11">115.5</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.12.12">
<td class="ltx_td" id="S3.T1.2.12.12.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.12.2">Swin UNETR</td>
<td class="ltx_td" id="S3.T1.2.12.12.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.12.4">0.341</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.12.5">0.378</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.12.6">0.343</td>
<td class="ltx_td" id="S3.T1.2.12.12.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.12.8">204.33</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.12.9">191.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.12.12.10">200.05</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.13.13">
<td class="ltx_td" id="S3.T1.2.13.13.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.13.13.2">Ensemble</td>
<td class="ltx_td" id="S3.T1.2.13.13.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.13.13.4">0.533</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.13.13.5">0.594</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.13.13.6">0.539</td>
<td class="ltx_td" id="S3.T1.2.13.13.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.13.13.8">123.18</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.13.13.9">110.28</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.13.13.10">115.38</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.14.14">
<td class="ltx_td" id="S3.T1.2.14.14.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.14.14.2">Post-processing</td>
<td class="ltx_td" id="S3.T1.2.14.14.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.14.14.4">0.559</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.14.14.5">0.604</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.14.14.6">0.565</td>
<td class="ltx_td" id="S3.T1.2.14.14.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.14.14.8">110.07</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.14.14.9">105.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.14.14.10">102.77</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.15.15">
<td class="ltx_td ltx_border_bb" id="S3.T1.2.15.15.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.15.15.2">Without Swin</td>
<td class="ltx_td ltx_border_bb" id="S3.T1.2.15.15.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.4.1">0.608</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.5.1">0.649</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.6.1">0.587</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.2.15.15.7"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.8.1">91.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.9.1">91.29</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.2.15.15.10"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.15.10.1">95.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Quantitative results on the validation datasets of PED, MEN and MET. Lesion-wise (LW) Dice coefficients and 95% Hausdorff distances (HD95) were computed for enhancing tumor (ET), tumor core (TC), and whole tumor (WT), respectively. Numbers in bold indicate the best results for each task.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S3.T1" title="Table 1 ‣ 3 Results ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">1</span></a> provides a comprehensive overview of the performance evaluation of our models across the validation datasets for each task. This performance evaluation was performed automatically by the challenge’s digital platform, with no access to the validation ground truth data. Additionally, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S3.F6" title="Figure 6 ‣ 3 Results ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">6</span></a> shows qualitative results on some cases of the validation datasets for PED, MEN, and MET tasks.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="405" id="S3.F6.g1" src="extracted/5851248/src/results/qual_results1.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Qualitative results on the validation datasets of PED, MEN, and MET. The selected cases show the median of the averaged lesion-wise Dice over three tumor regions for each task (NCR-red, ED-green, ET-blue).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#S3.T2" title="Table 2 ‣ 3 Results ‣ Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> provides quantitative evaluation of our proposed solution across the test datasets for each task.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.1.3.1">Statistic</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T2.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.1.5.1">LW Dice</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T2.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.1.7.1">LW HD95 (mm)</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.2">
<th class="ltx_td ltx_th ltx_th_column" id="S3.T2.2.2.2.1"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S3.T2.2.2.2.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.3.1">ET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.4.1">TC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.5.1">WT</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.7.1">ET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.8.1">TC</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.2.2.9"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.9.1">WT</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.1.1">PED</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.3.1.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.3.1">Mean</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.3.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.1.5">0.653</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.1.6">0.809</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.1.7">0.826</td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.3.1.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.1.9">43.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.1.10">21.82</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.2.3.1.11">20.86</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.4.2">
<td class="ltx_td" id="S3.T2.2.4.2.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.4.2.2.1">(Standard deviation)</span></td>
<td class="ltx_td" id="S3.T2.2.4.2.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.2.4">(0.32)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.2.5">(0.185)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.2.6">(0.183)</td>
<td class="ltx_td" id="S3.T2.2.4.2.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.2.8">(108.59)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.2.9">(75.15)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.4.2.10">(75.31)</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.5.3">
<td class="ltx_td ltx_align_left" id="S3.T2.2.5.3.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.5.3.1.1">1st rank</span></td>
<td class="ltx_td" id="S3.T2.2.5.3.2"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.5.3.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.5.3.3.1">25th quantile</span></td>
<td class="ltx_td" id="S3.T2.2.5.3.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.3.5">0.569</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.3.6">0.809</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.3.7">0.840</td>
<td class="ltx_td" id="S3.T2.2.5.3.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.3.9">1.41</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.3.10">2.96</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.5.3.11">2.96</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.6.4">
<td class="ltx_td" id="S3.T2.2.6.4.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.6.4.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.6.4.2.1">Median</span></td>
<td class="ltx_td" id="S3.T2.2.6.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.4.4">0.741</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.4.5">0.856</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.4.6">0.875</td>
<td class="ltx_td" id="S3.T2.2.6.4.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.4.8">3.67</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.4.9">5.26</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.6.4.10">4.24</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.7.5">
<td class="ltx_td" id="S3.T2.2.7.5.1"></td>
<td class="ltx_td" id="S3.T2.2.7.5.2"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.7.5.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.7.5.3.1">75th quantile</span></td>
<td class="ltx_td" id="S3.T2.2.7.5.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.5.5">0.916</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.5.6">0.888</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.5.7">0.895</td>
<td class="ltx_td" id="S3.T2.2.7.5.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.5.9">8.98</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.5.10">9.17</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.7.5.11">6.76</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.8.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.8.6.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.8.6.1.1">MEN</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.8.6.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.8.6.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.8.6.3.1">Mean</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.8.6.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.8.6.5">0.876</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.8.6.6">0.867</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.8.6.7">0.849</td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.8.6.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.8.6.9">30.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.8.6.10">31.69</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.2.8.6.11">35.17</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.9.7">
<td class="ltx_td" id="S3.T2.2.9.7.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.9.7.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.9.7.2.1">(Standard deviation)</span></td>
<td class="ltx_td" id="S3.T2.2.9.7.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.9.7.4">(0.217)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.9.7.5">(0.227)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.9.7.6">(0.231)</td>
<td class="ltx_td" id="S3.T2.2.9.7.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.9.7.8">(80.9)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.9.7.9">(83.53)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.9.7.10">(86.77)</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.10.8">
<td class="ltx_td ltx_align_left" id="S3.T2.2.10.8.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.10.8.1.1">3rd rank</span></td>
<td class="ltx_td" id="S3.T2.2.10.8.2"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.10.8.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.10.8.3.1">25th quantile</span></td>
<td class="ltx_td" id="S3.T2.2.10.8.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.10.8.5">0.907</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.10.8.6">0.885</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.10.8.7">0.863</td>
<td class="ltx_td" id="S3.T2.2.10.8.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.10.8.9">1.00</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.10.8.10">1.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.10.8.11">1.00</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.11.9">
<td class="ltx_td" id="S3.T2.2.11.9.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.11.9.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.11.9.2.1">Median</span></td>
<td class="ltx_td" id="S3.T2.2.11.9.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.11.9.4">0.968</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.11.9.5">0.968</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.11.9.6">0.953</td>
<td class="ltx_td" id="S3.T2.2.11.9.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.11.9.8">1.00</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.11.9.9">1.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.11.9.10">1.62</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.12.10">
<td class="ltx_td" id="S3.T2.2.12.10.1"></td>
<td class="ltx_td" id="S3.T2.2.12.10.2"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.12.10.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.12.10.3.1">75th quantile</span></td>
<td class="ltx_td" id="S3.T2.2.12.10.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.12.10.5">0.985</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.12.10.6">0.985</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.12.10.7">0.975</td>
<td class="ltx_td" id="S3.T2.2.12.10.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.12.10.9">2.91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.12.10.10">3.61</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.12.10.11">4.36</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.13.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.13.11.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.13.11.1.1">MET</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.13.11.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.13.11.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.13.11.3.1">Mean</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.13.11.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.13.11.5">0.555</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.13.11.6">0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.13.11.7">0.58</td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.13.11.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.13.11.9">113.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.13.11.10">112.84</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.2.13.11.11">108.17</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.14.12">
<td class="ltx_td" id="S3.T2.2.14.12.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.14.12.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.14.12.2.1">(Standard deviation)</span></td>
<td class="ltx_td" id="S3.T2.2.14.12.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.14.12.4">(0.279)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.14.12.5">(0.3)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.14.12.6">(0.289)</td>
<td class="ltx_td" id="S3.T2.2.14.12.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.14.12.8">(122.53)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.14.12.9">(121.6)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.14.12.10">(122.13)</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.15.13">
<td class="ltx_td ltx_align_left" id="S3.T2.2.15.13.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.15.13.1.1">4th rank</span></td>
<td class="ltx_td" id="S3.T2.2.15.13.2"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.15.13.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.15.13.3.1">25th quantile</span></td>
<td class="ltx_td" id="S3.T2.2.15.13.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.15.13.5">0.312</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.15.13.6">0.370</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.15.13.7">0.342</td>
<td class="ltx_td" id="S3.T2.2.15.13.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.15.13.9">1.66</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.15.13.10">1.29</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.15.13.11">2.45</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.16.14">
<td class="ltx_td" id="S3.T2.2.16.14.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.16.14.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.16.14.2.1">Median</span></td>
<td class="ltx_td" id="S3.T2.2.16.14.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.16.14.4">0.638</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.16.14.5">0.691</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.16.14.6">0.644</td>
<td class="ltx_td" id="S3.T2.2.16.14.7"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.16.14.8">75.78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.16.14.9">75.60</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.16.14.10">9.49</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.17.15">
<td class="ltx_td ltx_border_bb" id="S3.T2.2.17.15.1"></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.17.15.2"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.17.15.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.17.15.3.1">75th quantile</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.17.15.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.5">0.813</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.6">0.863</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.7">0.826</td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.17.15.8"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.9">202.39</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.10">189.43</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T2.2.17.15.11">189.03</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.4.2" style="font-size:90%;">Quantitative results on the testing datasets of PED, MEN and MET and ranks obtained in the competition. Lesion-wise (LW) Dice coefficients and 95% Hausdorff distances (HD95) were computed for enhancing tumor (ET), tumor core (TC), and whole tumor (WT), respectively.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Overall, the combination of ensemble models and post-processing procedures yielded clear improvements in the performance for the PED, MEN, and MET tasks. With our approach, we are able to synergize the capabilities of nnU-Net and Swin UNETR models based on the validation scores to effectively generate more accurate and robust segmentation. Our strategy minimized undesired smaller contributions, which would negatively impact lesion-wise performance.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Despite the consistent labeling of the three tumor sub-regions (ET, TC, WT) across all tasks, the lesions’ morphology and spatial location are variable across subjects and tasks. Among these three sub-regions, ET identification was the most challenging in the PED task, as evidenced by the lowest Dice coefficient for this region. For the MEN and MET tasks, a better balance between label segmentation was achieved. This confirms the importance of applying label-wise post-processing techniques tailored to each task. Consequently, binary metrics disproportionately penalize false positive predictions. Moreover, the novel metrics centered on lesions result in severe penalties for isolated volumes that do not correspond to actual lesions. All of this accentuates the necessity for post-processing strategies to address disconnected and small components.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Each task’s training process was restricted to the usage of task-specific data, adhering to the predefined guidelines of the BraTS 2023 challenge. Outside of the scope of this challenge, it would be beneficial to enhance the model’s capacity with additional pre-training to ensure it generalizes effectively.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Our intention was to tackle the diversity in the challenge data from both the perspective of the model and that of the data. Towards this, we argue that self-supervised learning strategies and the concept of foundation model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08232v1#bib.bib23" title="">23</a>]</cite> holds the potential to augment model’s generalizability, thus making it suitable for a more extensive range of clinical applications. The PED and MET tasks were conducted with limited available data, which is not surprising given the rarity of brain tumors. To overcome this limitation, the incorporation of generative artificial intelligence could also be a potential solution to augment the training set with synthetic data.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The segmentation of rare tumors from radiology images using deep learning techniques is challenging given the limited available data. We developed a model ensemble technique to address the segmentation of three types of brain tumors included in the BraTS 2023 challenge: i.e., pediatric, meningioma, and metastases. The ensemble prediction from nnU-net and Swin UNETR provided better segmentation accuracy for all tumor types. Furthermore, our targeted post-processing strategy based on cross-validated thresholding search improved the performance in all tasks, being more noticeable in pediatric tumor segmentation due to the label-wise targeted post-processing steps. Through this work, we explored ensembling techniques that leverage the generalizability of deep learning models and strategies across diverse data distributions and tasks. The success of our method is demonstrated by the challenge ranking positions on unseen test cases, being the winner in PED task, ranked third for MEN, and fourth for MET task.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S5.SS0.SSS1.p1">
<p class="ltx_p" id="S5.SS0.SSS1.p1.1">Partial support for this work was provided by the National Cancer Institute (UG3 CA236536) and by the Spanish Ministerio de Ciencia e Innovación, the Agencia Estatal de Investigación and NextGenerationEU funds, under grants PDC2022-133865-I00 and PID2022-141493OB-I00, and EUCAIM project co-funded by the European Union (Grant Agreement #101100633). The authors gratefully acknowledge the Universidad Politécnica de Madrid (www.upm.es) for providing computing resources on Magerit Supercomputer.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Adewole, M., Rudie, J.D., Gbadamosi, A., et al.: The Brain Tumor Segmentation
(BraTS) Challenge 2023: Glioma Segmentation in Sub-Saharan Africa Patient
Population (BraTS-Africa) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A
next-generation hyperparameter optimization framework. In: Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anwar, S.M., Parida, A., Atito, S., et al.: SS-CXR: Multitask representation
learning using self supervised pre-training from chest x-rays.
arXiv:2211.12944 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Baid, U., Ghodasara, S., Mohan, S., et al.: The RSNA-ASNR-MICCAI BraTS 2021
benchmark on brain tumor segmentation and radiogenomic classification. arXiv
preprint arXiv:2107.02314 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bakas, S., Akbari, H., Sotiras, A., et al.: Advancing The Cancer Genome
Atlas glioma MRI collections with expert segmentation labels and radiomic
features. Scientific Data <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">4</span>(1), 170117 (sep 2017).
https://doi.org/10.1038/sdata.2017.117, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/sdata.2017.117" title="">https://doi.org/10.1038/sdata.2017.117</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bakas, S., Akbari, H., Sotiras, A., et al.: Segmentation Labels and Radiomic
Features for the Pre-operative Scans of the TCGA-GBM collection. The Cancer
Imaging Archive (2017). https://doi.org/10.7937/K9/TCIA.2017.KLXWJJ1Q

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Bakas, S., Akbari, H., Sotiras, A., et al.: Segmentation Labels and Radiomic
Features for the Pre-operative Scans of the TCGA-LGG collection. The Cancer
Imaging Archive (2017). https://doi.org/10.7937/K9/TCIA.2017.GJQ7R0EF

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Curtin, S., Minino, A., Anderson, R.: Declines in cancer death rates among
children and adolescents in the united states, 1999-2014. National Center for
Health Statistics Data Brief (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al.: An image is worth 16x16
words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fathi Kazerooni, A., Arif, S., Madhogarhia, R., et al.: Automated tumor
segmentation and brain tissue extraction from multiparametric mri of
pediatric brain tumors: A multi-institutional study. Neuro-Oncology Advances
<span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">5</span>(1), vdad027 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Nath, V., Tang, Y., et al.: Swin UNETR: Swin transformers
for semantic segmentation of brain tumors in mri images. In: Crimi, A.,
Bakas, S. (eds.) Brainlesion: Glioma, Multiple Sclerosis, Stroke and
Traumatic Brain Injuries. pp. 272–284. Springer, Cham (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Tang, Y., Nath, V., et al.: UNETR: Transformers for 3D
Medical Image Segmentation. In: IEEE/CVF Winter Conference on Applications
of Computer Vision (WACV). pp. 574–584 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A., et al.: nnU-Net: a self-configuring
method for deep learning-based biomedical image segmentation. Nature methods
<span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">18</span>(2), 203–211 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jiang, Z., Parida, A., Anwar, S.M., Tang, Y., Roth, H.R., Fisher, M.J., Packer,
R.J., Avery, R.A., Linguraru, M.G.: Automatic visual acuity loss prediction
in children with optic pathway gliomas using magnetic resonance imaging. In:
2023 45th Annual International Conference of the IEEE Engineering in Medicine
&amp; Biology Society (EMBC). pp. 1–5 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Karargyris, A., Umeton, R., Sheller, M., et al.: Federated benchmarking of
medical artificial intelligence with medperf. Nat Mach Intell <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">5</span>,
799–810 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kazerooni, A.F., Khalili, N., Liu, X., et al.: The Brain Tumor Segmentation
(BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI
BraTS-PEDs) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kofler, F., Meissen, F., Steinbauer, F., et al.: The Brain Tumor Segmentation
(BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via
Inpainting (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
LaBella, D., Adewole, M., Alonso-Basanta, M., et al.: The ASNR-MICCAI Brain
Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Li, H.B., Conte, G.M., Anwar, S.M., et al.: The Brain Tumor Segmentation
(BraTS) Challenge 2023: Brain MR Image Synthesis for Tumor Segmentation
(BraSyn) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Liu, X., Bonner, E., Jiang, Z., et al.: Automatic segmentation of rare
pediatric brain tumors using knowledge transfer from adult data. 20th IEEE
International Symposium on Biomedical Imaging (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Menze, B.H., Jakab, A., Bauer, S., et al.: The Multimodal Brain Tumor Image
Segmentation Benchmark (BRATS). IEEE Transactions on Medical Imaging
<span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">34</span>(10), 1993–2024 (2015). https://doi.org/10.1109/TMI.2014.2377694

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Moawad, A.W., Janas, A., Baid, U., et al.: The Brain Tumor Segmentation
(BraTS-METS) Challenge 2023: Brain Metastasis Segmentation on Pre-treatment
MRI (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Parida, A., Capellan-Martin, D., Atito, S., et al.: DiCoM–Diverse Concept
Modeling towards Enhancing Generalizability in Chest X-Ray Studies.
arXiv:2402.15534 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Rashed, W.M., Maher, E., Adel, M., et al.: Pediatric diffuse intrinsic pontine
glioma: where do we stand? Cancer and Metastasis Reviews <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">38</span>(4),
759–770 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Rohlfing, T., Zahr, N.M., Sullivan, E.V., Pfefferbaum, A.: The SRI24
multichannel atlas of normal adult human brain structure. Human brain
mapping <span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">31</span>(5), 798–819 (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for
biomedical image segmentation. In: Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234–241.
Springer (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Tang, Y., Yang, D., Li, W., Roth, H.R., et al.: Self-Supervised Pre-Training
of Swin Transformers for 3D Medical Image Analysis. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
20730–20740 (2022)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 17:13:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
