<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.16255] UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception</title><meta property="og:description" content="Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in coâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.16255">

<!--Generated on Tue Feb 27 22:43:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Maxey<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>â€…<sup id="id9.9.id2" class="ltx_sup">1,2</sup>, Jaehoon Choi<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>â€…<sup id="id11.11.id4" class="ltx_sup">2</sup>, Hyungtae Lee<sup id="id12.12.id5" class="ltx_sup"><span id="id12.12.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Dinesh Manocha<sup id="id13.13.id6" class="ltx_sup"><span id="id13.13.id6.1" class="ltx_text ltx_font_italic">2</span></sup>, and Heesung Kwon<sup id="id14.14.id7" class="ltx_sup"><span id="id14.14.id7.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id15.15.id8" class="ltx_sup"><span id="id15.15.id8.1" class="ltx_text ltx_font_italic">1</span></sup>DEVCOM Army Research Laboratory <sup id="id16.16.id9" class="ltx_sup"><span id="id16.16.id9.1" class="ltx_text ltx_font_italic">2</span></sup>University of Maryland
</span><span class="ltx_author_notes">* These two authors contributed equallyCorrespondence to cmaxey@umd.edu, kevchoi@umd.edu</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in conjunction with perception models is prevalent to create synthetic data to augment the learning in the ground-based imaging domain. However, severe challenges in the austere UAV-based domain require distinctive solutions to image synthesis for data augmentation. In this work, we leverage recent advancements in neural rendering to improve static and dynamic novel-view UAV-based image synthesis, especially from high altitudes, capturing salient scene attributes. Finally, we demonstrate a considerable performance boost is achieved when a state-of-the-art detection model is optimized primarily on hybrid sets of real and synthetic data instead of the real or synthetic data separately.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Unmanned aerial vehicle (UAV)-based perception, such as recognizing objects of interest in real-time, is a core problem in numerous applications spanning various civil and military fields. Modern UAVs can be equipped with low power, mobile GPUs that can run efficient yet state-of-the-art (SOTA) neural networks, for instance, YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Utilizing the compact networks embedded on small UAVs enables important tasks, such as search and rescue for disaster relief <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, highlighting points of interest, such as people or vehicles in surveillance and reconnaissance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and assessing the conditions or recognizing an occurrence of specific activities, or events in a certain region of interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Due to the large degrees of freedom of the UAV-based imaging domain, procuring sufficient training data for UAV-based perception can be an onerous task. Various challenges, including flight restrictions, security and safety issues, and weather conditions, further compound the complications of UAV-based data collection. In particular, setting up adequate backgrounds or surroundings so that the scene can closely reflect a real-world scenario can be costly and time-consuming. Compounded by the data-hungry nature of UAV-based perception algorithms, acquiring diverse training data representing real-world scenes and backgrounds becomes a significant barrier against learning efficient UAV-based learning models.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.8.1" class="ltx_text ltx_font_bold">Constructing a training set of UAV-Sim.</span> On the left, camera poses are shown in spatial (<math id="S1.F1.4.m1.1" class="ltx_Math" alttext="s_{\text{1}}" display="inline"><semantics id="S1.F1.4.m1.1b"><msub id="S1.F1.4.m1.1.1" xref="S1.F1.4.m1.1.1.cmml"><mi id="S1.F1.4.m1.1.1.2" xref="S1.F1.4.m1.1.1.2.cmml">s</mi><mtext id="S1.F1.4.m1.1.1.3" xref="S1.F1.4.m1.1.1.3a.cmml">1</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.F1.4.m1.1c"><apply id="S1.F1.4.m1.1.1.cmml" xref="S1.F1.4.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.4.m1.1.1.1.cmml" xref="S1.F1.4.m1.1.1">subscript</csymbol><ci id="S1.F1.4.m1.1.1.2.cmml" xref="S1.F1.4.m1.1.1.2">ğ‘ </ci><ci id="S1.F1.4.m1.1.1.3a.cmml" xref="S1.F1.4.m1.1.1.3"><mtext mathsize="70%" id="S1.F1.4.m1.1.1.3.cmml" xref="S1.F1.4.m1.1.1.3">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m1.1d">s_{\text{1}}</annotation></semantics></math>, <math id="S1.F1.5.m2.1" class="ltx_Math" alttext="s_{\text{2}}" display="inline"><semantics id="S1.F1.5.m2.1b"><msub id="S1.F1.5.m2.1.1" xref="S1.F1.5.m2.1.1.cmml"><mi id="S1.F1.5.m2.1.1.2" xref="S1.F1.5.m2.1.1.2.cmml">s</mi><mtext id="S1.F1.5.m2.1.1.3" xref="S1.F1.5.m2.1.1.3a.cmml">2</mtext></msub><annotation-xml encoding="MathML-Content" id="S1.F1.5.m2.1c"><apply id="S1.F1.5.m2.1.1.cmml" xref="S1.F1.5.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.5.m2.1.1.1.cmml" xref="S1.F1.5.m2.1.1">subscript</csymbol><ci id="S1.F1.5.m2.1.1.2.cmml" xref="S1.F1.5.m2.1.1.2">ğ‘ </ci><ci id="S1.F1.5.m2.1.1.3a.cmml" xref="S1.F1.5.m2.1.1.3"><mtext mathsize="70%" id="S1.F1.5.m2.1.1.3.cmml" xref="S1.F1.5.m2.1.1.3">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m2.1d">s_{\text{2}}</annotation></semantics></math>) and temporal (<math id="S1.F1.6.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S1.F1.6.m3.1b"><mi id="S1.F1.6.m3.1.1" xref="S1.F1.6.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S1.F1.6.m3.1c"><ci id="S1.F1.6.m3.1.1.cmml" xref="S1.F1.6.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m3.1d">t</annotation></semantics></math>) coordinates. Images captured with the corresponding camera poses are shown on the right. UAV-Sim generates synthetic images (blue x mark) at spatial locations on the UAV trajectory using NeRF models trained on the limited original UAV-based images. To cope with the dynamic nature of UAV-based images, UAV-Sim also generates synthetic images (green x marks) at different times at a certain location on the trajectory. The training set for an object detector can be constructed with a hybrid set of original real images and novel-view synthetic images.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To help alleviate some of the burdens of learning data-hungry algorithms in the UAV-based domain, creating synthetic data has become an active area of research. A straightforward approach involves generating high-quality synthetic data using synthetic renderers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or generative models (GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, or diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>). However, they still suffer from an issue known as <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">domain gap</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> in which synthetic data in training and the test data differ in fine details of the scene, including appearance, texture, etc., resulting in performance decrease during testing. Recent advancements in scene synthesis, such as neural radiance fields (NeRFs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, have allowed for high-fidelity scene reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and novel-view image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, given a limited set of training images from the target scene. NeRFs can create high-quality novel-view imagery with enhanced realism that closely matches the target scene, effectively augmenting training data and thus enhancing model learning.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Main Results:</span> Our main goal is to learn NeRF-based models in the UAV-based imaging domain that can create novel-view images from previously unseen camera positions capturing salient attributes of the scene. As shown in Fig. 1, our approach is designed to augment datasets by generating synthetic images that are applicable to both static and dynamic scenes relevant to the object detection, as well as action and activity recognition. In this work, we show that using NeRFs is particularly beneficial in supplementing UAV-based data as most UAV-based benchmarks do not sufficiently represent huge variations in the UAV-based imaging domain.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We also explore the challenges of reconstructing novel-view dynamic scenes with fidelity on par with original scenes by enhancing an existing dynamic NeRF algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Our approach is able to better capture the details of dynamic scenes from the UAV-based data for human action recognition called the Okutama-Action dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In UAV-based data, it is not possible to use data-driven depth, optical flow, and motion segmentation masks which are typically used by previous dynamic NeRF methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
To the best of our knowledge, our work is one of the first attempts to augment UAV-based data captured with challenging conditions including relatively high altitudes and far range, steep view angles, and unstable imaging conditions. Some novel components of our work include:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Development of a model optimization pipeline for UAV-based perception using NeRF integrating unseen critical salient attributes into model fine-tuning with self-generated bounding box annotation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Extension of the optimization pipeline to dynamic UAV-based scene using extended dynamic NeRF to enhance recognition of objects in motion.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Demonstration of the overall benefit of leveraging NeRF for data augmentation in the UAV-based perception, showing a 55.85% improvement in mAP for static scenes and a 12.4% enhancement for dynamic scenes.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The most prevalent approach for generating high-quality synthetic data involves the use of simulators such as game engine.
Most simulators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> are capable of synthesizing high-quality data for training deep learning-based models. Nevertheless, most of them exhibit two limitations: a domain gap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and the need for skilled human graphics experts.
Recently, neural rendering methods that solely rely on image data and camera poses have demonstrated remarkable levels of photo-realistic rendering quality. In particular, NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> introduce the concept of the neural radiance field representation and apply differentiable volume rendering. Various methods have emerged for improving training efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and visual quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> primarily focus on enhancing NeRF for large-scale drone footage, emphasizing improvements at training efficiency and rendering quality. Our main focus, however, is on applying NeRF to generate synthetic data and to improve perception algorithms for UAV images.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">As can be pertinent to UAV footage, NeRF research also encompasses rendering dynamic scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. One common approach is to model a static canonical scene and a deformation network to adjust time dependent input rays to match the canonical scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. This effectively guides the NeRF on how the scene should look at a given time with respect to a pose at a fixed time. DynIBaR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> utilizes features extracted from the images and stacks said features along epipolar lines for static portions of the scene and along a learned deformable line for transient portions of the scene. Another approach relies on explicitly representing a scene via data structures that store feature vectors based on locations within a scene. This can be done with a full dimensional volume representation of a scene or by factoring a sceneâ€™s volume into planes, both static and dynamic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Time-aware voxels in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> utilize a full 3D volume to represent the scene but also includes a deformation network for rays projected into the volume. All of these methods focus on dynamic scenes that are from a ground perspective and typically have transient motions that are near to the camera. Dynamic imagery from distant cameras such as UAVs presents additional challenges.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Our Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to develop a new data augmentation method to improve performance for UAV-based object detection. In this paper, we propose using two Neural Radiance Fields (NeRFs) for either static or dynamic scenes can be incorporated into augmentation method. One NeRF is designed for static scenes, which are captured by UAVs and exclusively comprise static objects. The other is tailored for dynamic scenes that include moving people or pedestrians (SectionÂ <a href="#S3.SS1" title="III-A Novel-View Image Generation â€£ III Our Method â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>). To properly leverage generated images by NeRF models, we also designed a selection strategy of camera poses and time for novel-view images (SectionÂ <a href="#S3.SS2" title="III-B Novel Camera Pose Selection â€£ III Our Method â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.16255/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our training pipeline for static NeRF. Our method requires two NeRF models (<math id="S3.F2.5.m1.1" class="ltx_Math" alttext="f_{im}" display="inline"><semantics id="S3.F2.5.m1.1b"><msub id="S3.F2.5.m1.1.1" xref="S3.F2.5.m1.1.1.cmml"><mi id="S3.F2.5.m1.1.1.2" xref="S3.F2.5.m1.1.1.2.cmml">f</mi><mrow id="S3.F2.5.m1.1.1.3" xref="S3.F2.5.m1.1.1.3.cmml"><mi id="S3.F2.5.m1.1.1.3.2" xref="S3.F2.5.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F2.5.m1.1.1.3.1" xref="S3.F2.5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.5.m1.1.1.3.3" xref="S3.F2.5.m1.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F2.5.m1.1c"><apply id="S3.F2.5.m1.1.1.cmml" xref="S3.F2.5.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.5.m1.1.1.1.cmml" xref="S3.F2.5.m1.1.1">subscript</csymbol><ci id="S3.F2.5.m1.1.1.2.cmml" xref="S3.F2.5.m1.1.1.2">ğ‘“</ci><apply id="S3.F2.5.m1.1.1.3.cmml" xref="S3.F2.5.m1.1.1.3"><times id="S3.F2.5.m1.1.1.3.1.cmml" xref="S3.F2.5.m1.1.1.3.1"></times><ci id="S3.F2.5.m1.1.1.3.2.cmml" xref="S3.F2.5.m1.1.1.3.2">ğ‘–</ci><ci id="S3.F2.5.m1.1.1.3.3.cmml" xref="S3.F2.5.m1.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m1.1d">f_{im}</annotation></semantics></math> and <math id="S3.F2.6.m2.1" class="ltx_Math" alttext="f_{bbox}" display="inline"><semantics id="S3.F2.6.m2.1b"><msub id="S3.F2.6.m2.1.1" xref="S3.F2.6.m2.1.1.cmml"><mi id="S3.F2.6.m2.1.1.2" xref="S3.F2.6.m2.1.1.2.cmml">f</mi><mrow id="S3.F2.6.m2.1.1.3" xref="S3.F2.6.m2.1.1.3.cmml"><mi id="S3.F2.6.m2.1.1.3.2" xref="S3.F2.6.m2.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.F2.6.m2.1.1.3.1" xref="S3.F2.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.6.m2.1.1.3.3" xref="S3.F2.6.m2.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.F2.6.m2.1.1.3.1b" xref="S3.F2.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.6.m2.1.1.3.4" xref="S3.F2.6.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.F2.6.m2.1.1.3.1c" xref="S3.F2.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.6.m2.1.1.3.5" xref="S3.F2.6.m2.1.1.3.5.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F2.6.m2.1c"><apply id="S3.F2.6.m2.1.1.cmml" xref="S3.F2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.6.m2.1.1.1.cmml" xref="S3.F2.6.m2.1.1">subscript</csymbol><ci id="S3.F2.6.m2.1.1.2.cmml" xref="S3.F2.6.m2.1.1.2">ğ‘“</ci><apply id="S3.F2.6.m2.1.1.3.cmml" xref="S3.F2.6.m2.1.1.3"><times id="S3.F2.6.m2.1.1.3.1.cmml" xref="S3.F2.6.m2.1.1.3.1"></times><ci id="S3.F2.6.m2.1.1.3.2.cmml" xref="S3.F2.6.m2.1.1.3.2">ğ‘</ci><ci id="S3.F2.6.m2.1.1.3.3.cmml" xref="S3.F2.6.m2.1.1.3.3">ğ‘</ci><ci id="S3.F2.6.m2.1.1.3.4.cmml" xref="S3.F2.6.m2.1.1.3.4">ğ‘œ</ci><ci id="S3.F2.6.m2.1.1.3.5.cmml" xref="S3.F2.6.m2.1.1.3.5">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m2.1d">f_{bbox}</annotation></semantics></math>) trained using either original images or masked images. When provided with novel camera poses, <math id="S3.F2.7.m3.1" class="ltx_Math" alttext="f_{im}" display="inline"><semantics id="S3.F2.7.m3.1b"><msub id="S3.F2.7.m3.1.1" xref="S3.F2.7.m3.1.1.cmml"><mi id="S3.F2.7.m3.1.1.2" xref="S3.F2.7.m3.1.1.2.cmml">f</mi><mrow id="S3.F2.7.m3.1.1.3" xref="S3.F2.7.m3.1.1.3.cmml"><mi id="S3.F2.7.m3.1.1.3.2" xref="S3.F2.7.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F2.7.m3.1.1.3.1" xref="S3.F2.7.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.7.m3.1.1.3.3" xref="S3.F2.7.m3.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F2.7.m3.1c"><apply id="S3.F2.7.m3.1.1.cmml" xref="S3.F2.7.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.7.m3.1.1.1.cmml" xref="S3.F2.7.m3.1.1">subscript</csymbol><ci id="S3.F2.7.m3.1.1.2.cmml" xref="S3.F2.7.m3.1.1.2">ğ‘“</ci><apply id="S3.F2.7.m3.1.1.3.cmml" xref="S3.F2.7.m3.1.1.3"><times id="S3.F2.7.m3.1.1.3.1.cmml" xref="S3.F2.7.m3.1.1.3.1"></times><ci id="S3.F2.7.m3.1.1.3.2.cmml" xref="S3.F2.7.m3.1.1.3.2">ğ‘–</ci><ci id="S3.F2.7.m3.1.1.3.3.cmml" xref="S3.F2.7.m3.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m3.1d">f_{im}</annotation></semantics></math> generates novel augmented images, while <math id="S3.F2.8.m4.1" class="ltx_Math" alttext="f_{bbox}" display="inline"><semantics id="S3.F2.8.m4.1b"><msub id="S3.F2.8.m4.1.1" xref="S3.F2.8.m4.1.1.cmml"><mi id="S3.F2.8.m4.1.1.2" xref="S3.F2.8.m4.1.1.2.cmml">f</mi><mrow id="S3.F2.8.m4.1.1.3" xref="S3.F2.8.m4.1.1.3.cmml"><mi id="S3.F2.8.m4.1.1.3.2" xref="S3.F2.8.m4.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.F2.8.m4.1.1.3.1" xref="S3.F2.8.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.8.m4.1.1.3.3" xref="S3.F2.8.m4.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.F2.8.m4.1.1.3.1b" xref="S3.F2.8.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.8.m4.1.1.3.4" xref="S3.F2.8.m4.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.F2.8.m4.1.1.3.1c" xref="S3.F2.8.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.F2.8.m4.1.1.3.5" xref="S3.F2.8.m4.1.1.3.5.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F2.8.m4.1c"><apply id="S3.F2.8.m4.1.1.cmml" xref="S3.F2.8.m4.1.1"><csymbol cd="ambiguous" id="S3.F2.8.m4.1.1.1.cmml" xref="S3.F2.8.m4.1.1">subscript</csymbol><ci id="S3.F2.8.m4.1.1.2.cmml" xref="S3.F2.8.m4.1.1.2">ğ‘“</ci><apply id="S3.F2.8.m4.1.1.3.cmml" xref="S3.F2.8.m4.1.1.3"><times id="S3.F2.8.m4.1.1.3.1.cmml" xref="S3.F2.8.m4.1.1.3.1"></times><ci id="S3.F2.8.m4.1.1.3.2.cmml" xref="S3.F2.8.m4.1.1.3.2">ğ‘</ci><ci id="S3.F2.8.m4.1.1.3.3.cmml" xref="S3.F2.8.m4.1.1.3.3">ğ‘</ci><ci id="S3.F2.8.m4.1.1.3.4.cmml" xref="S3.F2.8.m4.1.1.3.4">ğ‘œ</ci><ci id="S3.F2.8.m4.1.1.3.5.cmml" xref="S3.F2.8.m4.1.1.3.5">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m4.1d">f_{bbox}</annotation></semantics></math> synthesizes novel masked images for the purpose of bounding box extraction. Then, we can acquire novel-view images along with their corresponding bounding boxes to train the object detector.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2310.16255/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of intermediate results of data augmentation pipeline. (a) Masked images. (b) Novel-view images generated by NeRF with bounding box masks. (c) Novel-view images with corresponding bounding boxes.
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Novel-View Image Generation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this task, we address both static and dynamic scene synthesis, given the inherent characteristics of UAV-based imaging in real-world scenarios. A UAV captures the scene while in flight with objects of interest on the ground, static or in motion. Therefore, general NeRFs using images taken from multiple perspectives at the same time require special consideration to deal with dynamic scenes such as our scenario. In this subsection, we first describe NeRF-based data augmentation for object detection in static scenes and then cover alternative methods used for dynamic scenes.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p"><span id="S3.SS1.p2.4.1" class="ltx_text ltx_font_bold">1) Static Scene Synthesis:</span> To generate a novel-view image, we employ a NeRF model, <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="f_{\text{im}}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">f</mi><mtext id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3a.cmml">im</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p2.1.m1.1.1.3a.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">im</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">f_{\text{im}}</annotation></semantics></math>, which is trained using original input images <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">I</annotation></semantics></math> and their corresponding camera pose <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">P</annotation></semantics></math>, coupled with classical volume rendering techniques as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In our approach, the camera pose <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">P</annotation></semantics></math> for a new-view image is calculated by Structure-from-motion (SfM)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Then, the novel-view images must be annotated by locating bounding boxes on objects of interest before training an object detector.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">To estimate the bounding boxes in the novel-view images, we use a separate NeRF model, <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="f_{\text{bbox}}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">f</mi><mtext id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3a.cmml">bbox</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p3.1.m1.1.1.3a.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">bbox</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">f_{\text{bbox}}</annotation></semantics></math>. <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="f_{\text{bbox}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">f</mi><mtext id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3a.cmml">bbox</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ‘“</ci><ci id="S3.SS1.p3.2.m2.1.1.3a.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">bbox</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">f_{\text{bbox}}</annotation></semantics></math> is trained to synthesize masked images corresponding to the object-of-interest regions in the original images from a novel viewpoint. The bounding box in the generated images can be acquired from the masked images without requiring human-in-the-loop annotation. For example, Fig. <a href="#S3.F3" title="Figure 3 â€£ III Our Method â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-(b) is the synthesized image from <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="f_{\text{bbox}}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">f</mi><mtext id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3a.cmml">bbox</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">ğ‘“</ci><ci id="S3.SS1.p3.3.m3.1.1.3a.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">bbox</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">f_{\text{bbox}}</annotation></semantics></math> with bounding box masks and we can obtain the bounding boxes for each color blob as shown in Fig. <a href="#S3.F3" title="Figure 3 â€£ III Our Method â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-(c). Although it is possible to transform bounding boxes using depth maps and camera parameters from the training images to align them with novel-view images, the accuracy of the depth maps generated by NeRF is insufficient to achieve perfect alignment with small human objects, due to their volumetric rendering properties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Following the generation of novel-view images along with their respective bounding boxes, we use this synthetic data for fine-tuning the state-of-the-art object detection model, <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">e.g.</span>, YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Our overall pipeline is shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ III Our Method â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.For the NeRF architecture, we adopt the Nerfacto model from Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> that combines various features from recent papers such as scene contraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, hash grid encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and proposal network sampler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Both NeRF models are represented by Multi-layer Pereceptron (MLP) networks, which takes a 3D point and viewing direction (or camera pose) as input and produce color and density as output.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.6" class="ltx_p"><span id="S3.SS1.p5.6.1" class="ltx_text ltx_font_bold">2) Dynamic Scene Synthesis: Considering Temporal FactorÂ </span> To cope with dynamic scenes, we employ the K-Planes dynamic NeRF algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This algorithm represents a scene explicitly by factoring the spatial and temporal dimensions of a 4D volume into 2D planes in a certain space and time. Specifically, the K-planes method uses six planes to represent dynamic scenes, where three planes are for the pairwise combinations of spatial dimensions and the rest are for each spatial dimension plus time, <span id="S3.SS1.p5.6.2" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="xy" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mrow id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.1.1.1" xref="S3.SS1.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><times id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1"></times><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">xy</annotation></semantics></math>, <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="xz" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><times id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1"></times><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">xz</annotation></semantics></math>, <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="yz" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mrow id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.3.m3.1.1.1" xref="S3.SS1.p5.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><times id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1.1"></times><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">ğ‘¦</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">yz</annotation></semantics></math>, <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="xt" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><mrow id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.4.m4.1.1.1" xref="S3.SS1.p5.4.m4.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><times id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1.1"></times><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">xt</annotation></semantics></math>, <math id="S3.SS1.p5.5.m5.1" class="ltx_Math" alttext="yt" display="inline"><semantics id="S3.SS1.p5.5.m5.1a"><mrow id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml"><mi id="S3.SS1.p5.5.m5.1.1.2" xref="S3.SS1.p5.5.m5.1.1.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.5.m5.1.1.1" xref="S3.SS1.p5.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.5.m5.1.1.3" xref="S3.SS1.p5.5.m5.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><apply id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1"><times id="S3.SS1.p5.5.m5.1.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1.1"></times><ci id="S3.SS1.p5.5.m5.1.1.2.cmml" xref="S3.SS1.p5.5.m5.1.1.2">ğ‘¦</ci><ci id="S3.SS1.p5.5.m5.1.1.3.cmml" xref="S3.SS1.p5.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">yt</annotation></semantics></math>, and <math id="S3.SS1.p5.6.m6.1" class="ltx_Math" alttext="zt" display="inline"><semantics id="S3.SS1.p5.6.m6.1a"><mrow id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml"><mi id="S3.SS1.p5.6.m6.1.1.2" xref="S3.SS1.p5.6.m6.1.1.2.cmml">z</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.6.m6.1.1.1" xref="S3.SS1.p5.6.m6.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p5.6.m6.1.1.3" xref="S3.SS1.p5.6.m6.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><apply id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1"><times id="S3.SS1.p5.6.m6.1.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1.1"></times><ci id="S3.SS1.p5.6.m6.1.1.2.cmml" xref="S3.SS1.p5.6.m6.1.1.2">ğ‘§</ci><ci id="S3.SS1.p5.6.m6.1.1.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">zt</annotation></semantics></math>. When rendering the novel view image at a certain point in time, a 4D point (three spatial dimensions plus time) is projected onto each plane, and feature vectors from each plane are interpolated and multiplied to get a final vector for a given 4D location, as follows:</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="f(\mathbf{q})=\prod_{c\in C}f(\mathbf{q})_{c}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><mrow id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.1" xref="S3.E1.m1.2.3.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.3.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.1" xref="S3.E1.m1.2.3.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğª</mi><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.2" xref="S3.E1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><munder id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.3.3.1.2" xref="S3.E1.m1.2.3.3.1.2.cmml">âˆ</mo><mrow id="S3.E1.m1.2.3.3.1.3" xref="S3.E1.m1.2.3.3.1.3.cmml"><mi id="S3.E1.m1.2.3.3.1.3.2" xref="S3.E1.m1.2.3.3.1.3.2.cmml">c</mi><mo id="S3.E1.m1.2.3.3.1.3.1" xref="S3.E1.m1.2.3.3.1.3.1.cmml">âˆˆ</mo><mi id="S3.E1.m1.2.3.3.1.3.3" xref="S3.E1.m1.2.3.3.1.3.3.cmml">C</mi></mrow></munder><mrow id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><mi id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.2.1" xref="S3.E1.m1.2.3.3.2.1.cmml">â€‹</mo><msub id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.2.3.cmml"><mrow id="S3.E1.m1.2.3.3.2.3.2.2" xref="S3.E1.m1.2.3.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.2.3.2.2.1" xref="S3.E1.m1.2.3.3.2.3.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">ğª</mi><mo stretchy="false" id="S3.E1.m1.2.3.3.2.3.2.2.2" xref="S3.E1.m1.2.3.3.2.3.cmml">)</mo></mrow><mi id="S3.E1.m1.2.3.3.2.3.3" xref="S3.E1.m1.2.3.3.2.3.3.cmml">c</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><times id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2.1"></times><ci id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2">ğ‘“</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğª</ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><apply id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.1">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.2.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.1.2">product</csymbol><apply id="S3.E1.m1.2.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.1.3"><in id="S3.E1.m1.2.3.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.1.3.1"></in><ci id="S3.E1.m1.2.3.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.1.3.2">ğ‘</ci><ci id="S3.E1.m1.2.3.3.1.3.3.cmml" xref="S3.E1.m1.2.3.3.1.3.3">ğ¶</ci></apply></apply><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><times id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2.1"></times><ci id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2">ğ‘“</ci><apply id="S3.E1.m1.2.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.3.1.cmml" xref="S3.E1.m1.2.3.3.2.3">subscript</csymbol><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğª</ci><ci id="S3.E1.m1.2.3.3.2.3.3.cmml" xref="S3.E1.m1.2.3.3.2.3.3">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">f(\mathbf{q})=\prod_{c\in C}f(\mathbf{q})_{c}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.6" class="ltx_p">wherein <math id="S3.SS1.p7.1.m1.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="S3.SS1.p7.1.m1.1a"><mrow id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">c</mi><mo id="S3.SS1.p7.1.m1.1.1.1" xref="S3.SS1.p7.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><in id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1.1"></in><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">c\in C</annotation></semantics></math> is each plane in the set of all planes, <math id="S3.SS1.p7.2.m2.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">\mathbf{q}</annotation></semantics></math> represents a 4D coordinate <math id="S3.SS1.p7.3.m3.4" class="ltx_Math" alttext="(i,j,k,\tau)" display="inline"><semantics id="S3.SS1.p7.3.m3.4a"><mrow id="S3.SS1.p7.3.m3.4.5.2" xref="S3.SS1.p7.3.m3.4.5.1.cmml"><mo stretchy="false" id="S3.SS1.p7.3.m3.4.5.2.1" xref="S3.SS1.p7.3.m3.4.5.1.cmml">(</mo><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">i</mi><mo id="S3.SS1.p7.3.m3.4.5.2.2" xref="S3.SS1.p7.3.m3.4.5.1.cmml">,</mo><mi id="S3.SS1.p7.3.m3.2.2" xref="S3.SS1.p7.3.m3.2.2.cmml">j</mi><mo id="S3.SS1.p7.3.m3.4.5.2.3" xref="S3.SS1.p7.3.m3.4.5.1.cmml">,</mo><mi id="S3.SS1.p7.3.m3.3.3" xref="S3.SS1.p7.3.m3.3.3.cmml">k</mi><mo id="S3.SS1.p7.3.m3.4.5.2.4" xref="S3.SS1.p7.3.m3.4.5.1.cmml">,</mo><mi id="S3.SS1.p7.3.m3.4.4" xref="S3.SS1.p7.3.m3.4.4.cmml">Ï„</mi><mo stretchy="false" id="S3.SS1.p7.3.m3.4.5.2.5" xref="S3.SS1.p7.3.m3.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.4b"><vector id="S3.SS1.p7.3.m3.4.5.1.cmml" xref="S3.SS1.p7.3.m3.4.5.2"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">ğ‘–</ci><ci id="S3.SS1.p7.3.m3.2.2.cmml" xref="S3.SS1.p7.3.m3.2.2">ğ‘—</ci><ci id="S3.SS1.p7.3.m3.3.3.cmml" xref="S3.SS1.p7.3.m3.3.3">ğ‘˜</ci><ci id="S3.SS1.p7.3.m3.4.4.cmml" xref="S3.SS1.p7.3.m3.4.4">ğœ</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.4c">(i,j,k,\tau)</annotation></semantics></math>, <math id="S3.SS1.p7.4.m4.1" class="ltx_Math" alttext="f(\mathbf{q})_{c}" display="inline"><semantics id="S3.SS1.p7.4.m4.1a"><mrow id="S3.SS1.p7.4.m4.1.2" xref="S3.SS1.p7.4.m4.1.2.cmml"><mi id="S3.SS1.p7.4.m4.1.2.2" xref="S3.SS1.p7.4.m4.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.4.m4.1.2.1" xref="S3.SS1.p7.4.m4.1.2.1.cmml">â€‹</mo><msub id="S3.SS1.p7.4.m4.1.2.3" xref="S3.SS1.p7.4.m4.1.2.3.cmml"><mrow id="S3.SS1.p7.4.m4.1.2.3.2.2" xref="S3.SS1.p7.4.m4.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.p7.4.m4.1.2.3.2.2.1" xref="S3.SS1.p7.4.m4.1.2.3.cmml">(</mo><mi id="S3.SS1.p7.4.m4.1.1" xref="S3.SS1.p7.4.m4.1.1.cmml">ğª</mi><mo stretchy="false" id="S3.SS1.p7.4.m4.1.2.3.2.2.2" xref="S3.SS1.p7.4.m4.1.2.3.cmml">)</mo></mrow><mi id="S3.SS1.p7.4.m4.1.2.3.3" xref="S3.SS1.p7.4.m4.1.2.3.3.cmml">c</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.4.m4.1b"><apply id="S3.SS1.p7.4.m4.1.2.cmml" xref="S3.SS1.p7.4.m4.1.2"><times id="S3.SS1.p7.4.m4.1.2.1.cmml" xref="S3.SS1.p7.4.m4.1.2.1"></times><ci id="S3.SS1.p7.4.m4.1.2.2.cmml" xref="S3.SS1.p7.4.m4.1.2.2">ğ‘“</ci><apply id="S3.SS1.p7.4.m4.1.2.3.cmml" xref="S3.SS1.p7.4.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p7.4.m4.1.2.3.1.cmml" xref="S3.SS1.p7.4.m4.1.2.3">subscript</csymbol><ci id="S3.SS1.p7.4.m4.1.1.cmml" xref="S3.SS1.p7.4.m4.1.1">ğª</ci><ci id="S3.SS1.p7.4.m4.1.2.3.3.cmml" xref="S3.SS1.p7.4.m4.1.2.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.4.m4.1c">f(\mathbf{q})_{c}</annotation></semantics></math> is the specific feature vector for a given plane based on the projection of <math id="S3.SS1.p7.5.m5.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS1.p7.5.m5.1a"><mi id="S3.SS1.p7.5.m5.1.1" xref="S3.SS1.p7.5.m5.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.5.m5.1b"><ci id="S3.SS1.p7.5.m5.1.1.cmml" xref="S3.SS1.p7.5.m5.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.5.m5.1c">\mathbf{q}</annotation></semantics></math> and subsequent interpolation between plane cells, and <math id="S3.SS1.p7.6.m6.1" class="ltx_Math" alttext="f(\mathbf{q})" display="inline"><semantics id="S3.SS1.p7.6.m6.1a"><mrow id="S3.SS1.p7.6.m6.1.2" xref="S3.SS1.p7.6.m6.1.2.cmml"><mi id="S3.SS1.p7.6.m6.1.2.2" xref="S3.SS1.p7.6.m6.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.6.m6.1.2.1" xref="S3.SS1.p7.6.m6.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p7.6.m6.1.2.3.2" xref="S3.SS1.p7.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS1.p7.6.m6.1.2.3.2.1" xref="S3.SS1.p7.6.m6.1.2.cmml">(</mo><mi id="S3.SS1.p7.6.m6.1.1" xref="S3.SS1.p7.6.m6.1.1.cmml">ğª</mi><mo stretchy="false" id="S3.SS1.p7.6.m6.1.2.3.2.2" xref="S3.SS1.p7.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.6.m6.1b"><apply id="S3.SS1.p7.6.m6.1.2.cmml" xref="S3.SS1.p7.6.m6.1.2"><times id="S3.SS1.p7.6.m6.1.2.1.cmml" xref="S3.SS1.p7.6.m6.1.2.1"></times><ci id="S3.SS1.p7.6.m6.1.2.2.cmml" xref="S3.SS1.p7.6.m6.1.2.2">ğ‘“</ci><ci id="S3.SS1.p7.6.m6.1.1.cmml" xref="S3.SS1.p7.6.m6.1.1">ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.6.m6.1c">f(\mathbf{q})</annotation></semantics></math> is the final feature vector passed to the NeRF models to output a location density and color. The factored planes reduce the memory requirements for representing a scene while maintaining equivalent performance to full 4D representations. K-Planes is among the state of the art for performance on datasets such as DNerf <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and DyNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and is chosen for its compact representation.</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.2" class="ltx_p"><span id="S3.SS1.p8.2.1" class="ltx_text ltx_font_bold">3) Dynamic Scene Synthesis: Extended K-PlanesÂ </span> To rectify the performance issues of stock K-Planes, we extend the algorithm in several ways. We empirically find out that factoring the 4D volume into a set of static spatial planes, <math id="S3.SS1.p8.1.m1.1" class="ltx_Math" alttext="C_{s}" display="inline"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">C</mi><mi id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">ğ¶</ci><ci id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">C_{s}</annotation></semantics></math>, and a set of dynamic spatial and temporal planes, <math id="S3.SS1.p8.2.m2.1" class="ltx_Math" alttext="C_{d}" display="inline"><semantics id="S3.SS1.p8.2.m2.1a"><msub id="S3.SS1.p8.2.m2.1.1" xref="S3.SS1.p8.2.m2.1.1.cmml"><mi id="S3.SS1.p8.2.m2.1.1.2" xref="S3.SS1.p8.2.m2.1.1.2.cmml">C</mi><mi id="S3.SS1.p8.2.m2.1.1.3" xref="S3.SS1.p8.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.2.m2.1b"><apply id="S3.SS1.p8.2.m2.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.2.m2.1.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p8.2.m2.1.1.2.cmml" xref="S3.SS1.p8.2.m2.1.1.2">ğ¶</ci><ci id="S3.SS1.p8.2.m2.1.1.3.cmml" xref="S3.SS1.p8.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.2.m2.1c">C_{d}</annotation></semantics></math>, helps to alleviate some of the issues with noisy camera poses and better separates the static and temporal elements of the scene. With this new factorization, we extract feature vectors as before, multiplying the three static plane feature vectors together and separately multiplying the six dynamic plane feature vectors together, as follows:</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="f_{s}(\mathbf{q})=\prod_{c\in C_{s}}f_{s}(\mathbf{q})_{c},\;\;\;\;f_{d}(\mathbf{q})=\prod_{c\in C_{d}}f_{d}(\mathbf{q})_{c}." display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1"><mrow id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.3.cmml"><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml"><msub id="S3.E2.m1.5.5.1.1.1.1.2.2" xref="S3.E2.m1.5.5.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.1.1.2.2.2.cmml">f</mi><mi id="S3.E2.m1.5.5.1.1.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.1.1.2.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.2.1" xref="S3.E2.m1.5.5.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.2.3.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.2.3.2.1" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">ğª</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.2.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml"><munder id="S3.E2.m1.5.5.1.1.1.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.5.5.1.1.1.1.3.1.2" xref="S3.E2.m1.5.5.1.1.1.1.3.1.2.cmml">âˆ</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.3.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.2.cmml">c</mi><mo id="S3.E2.m1.5.5.1.1.1.1.3.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.1.cmml">âˆˆ</mo><msub id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.2.cmml">C</mi><mi id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.3" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.3.cmml">s</mi></msub></mrow></munder><mrow id="S3.E2.m1.5.5.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.cmml"><msub id="S3.E2.m1.5.5.1.1.1.1.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3.2.2.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2.2.cmml">f</mi><mi id="S3.E2.m1.5.5.1.1.1.1.3.2.2.3" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.3.2.1" xref="S3.E2.m1.5.5.1.1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.E2.m1.5.5.1.1.1.1.3.2.3" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.cmml"><mrow id="S3.E2.m1.5.5.1.1.1.1.3.2.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.3.2.3.2.2.1" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">ğª</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.3.2.3.2.2.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.cmml">)</mo></mrow><mi id="S3.E2.m1.5.5.1.1.1.1.3.2.3.3" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.3.cmml">c</mi></msub></mrow></mrow></mrow><mo rspace="1.277em" id="S3.E2.m1.5.5.1.1.2.3" xref="S3.E2.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S3.E2.m1.5.5.1.1.2.2" xref="S3.E2.m1.5.5.1.1.2.2.cmml"><mrow id="S3.E2.m1.5.5.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml"><msub id="S3.E2.m1.5.5.1.1.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.cmml">f</mi><mi id="S3.E2.m1.5.5.1.1.2.2.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.2.2.2.1" xref="S3.E2.m1.5.5.1.1.2.2.2.1.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.2.3.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.2.2.3.2.1" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">ğª</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.2.2.3.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E2.m1.5.5.1.1.2.2.1" xref="S3.E2.m1.5.5.1.1.2.2.1.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.cmml"><munder id="S3.E2.m1.5.5.1.1.2.2.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.5.5.1.1.2.2.3.1.2" xref="S3.E2.m1.5.5.1.1.2.2.3.1.2.cmml">âˆ</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.3.1.3" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.1.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.2.cmml">c</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.1.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.1.cmml">âˆˆ</mo><msub id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.2.cmml">C</mi><mi id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.3.cmml">d</mi></msub></mrow></munder><mrow id="S3.E2.m1.5.5.1.1.2.2.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.cmml"><msub id="S3.E2.m1.5.5.1.1.2.2.3.2.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.2.cmml">f</mi><mi id="S3.E2.m1.5.5.1.1.2.2.3.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.2.2.3.2.1" xref="S3.E2.m1.5.5.1.1.2.2.3.2.1.cmml">â€‹</mo><msub id="S3.E2.m1.5.5.1.1.2.2.3.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml"><mrow id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.2.1" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml">(</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">ğª</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml">)</mo></mrow><mi id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml">c</mi></msub></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.5.5.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.3a.cmml" xref="S3.E2.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><eq id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"></eq><apply id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2"><times id="S3.E2.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2.1"></times><apply id="S3.E2.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2.2.2">ğ‘“</ci><ci id="S3.E2.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2.2.3">ğ‘ </ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğª</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3"><apply id="S3.E2.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1">subscript</csymbol><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.1.1.3.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.2">product</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.3.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3"><in id="S3.E2.m1.5.5.1.1.1.1.3.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.1"></in><ci id="S3.E2.m1.5.5.1.1.1.1.3.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.2">ğ‘</ci><apply id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.2">ğ¶</ci><ci id="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1.3.3.3">ğ‘ </ci></apply></apply></apply><apply id="S3.E2.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2"><times id="S3.E2.m1.5.5.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.1"></times><apply id="S3.E2.m1.5.5.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2.2">ğ‘“</ci><ci id="S3.E2.m1.5.5.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.2.3">ğ‘ </ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğª</ci><ci id="S3.E2.m1.5.5.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2.3.3">ğ‘</ci></apply></apply></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2"><eq id="S3.E2.m1.5.5.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.1"></eq><apply id="S3.E2.m1.5.5.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2"><times id="S3.E2.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.1"></times><apply id="S3.E2.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2">ğ‘“</ci><ci id="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3">ğ‘‘</ci></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğª</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><apply id="S3.E2.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.1.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1">subscript</csymbol><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.2.2.3.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.2">product</csymbol><apply id="S3.E2.m1.5.5.1.1.2.2.3.1.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3"><in id="S3.E2.m1.5.5.1.1.2.2.3.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.1"></in><ci id="S3.E2.m1.5.5.1.1.2.2.3.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.2">ğ‘</ci><apply id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.2">ğ¶</ci><ci id="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1.3.3.3">ğ‘‘</ci></apply></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2"><times id="S3.E2.m1.5.5.1.1.2.2.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.1"></times><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.2">ğ‘“</ci><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.3">ğ‘‘</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ğª</ci><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3">ğ‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">f_{s}(\mathbf{q})=\prod_{c\in C_{s}}f_{s}(\mathbf{q})_{c},\;\;\;\;f_{d}(\mathbf{q})=\prod_{c\in C_{d}}f_{d}(\mathbf{q})_{c}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p10" class="ltx_para">
<p id="S3.SS1.p10.6" class="ltx_p">Here, each temporal plane stores feature vectors of dimension <math id="S3.SS1.p10.1.m1.1" class="ltx_Math" alttext="D+1" display="inline"><semantics id="S3.SS1.p10.1.m1.1a"><mrow id="S3.SS1.p10.1.m1.1.1" xref="S3.SS1.p10.1.m1.1.1.cmml"><mi id="S3.SS1.p10.1.m1.1.1.2" xref="S3.SS1.p10.1.m1.1.1.2.cmml">D</mi><mo id="S3.SS1.p10.1.m1.1.1.1" xref="S3.SS1.p10.1.m1.1.1.1.cmml">+</mo><mn id="S3.SS1.p10.1.m1.1.1.3" xref="S3.SS1.p10.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.1.m1.1b"><apply id="S3.SS1.p10.1.m1.1.1.cmml" xref="S3.SS1.p10.1.m1.1.1"><plus id="S3.SS1.p10.1.m1.1.1.1.cmml" xref="S3.SS1.p10.1.m1.1.1.1"></plus><ci id="S3.SS1.p10.1.m1.1.1.2.cmml" xref="S3.SS1.p10.1.m1.1.1.2">ğ·</ci><cn type="integer" id="S3.SS1.p10.1.m1.1.1.3.cmml" xref="S3.SS1.p10.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.1.m1.1c">D+1</annotation></semantics></math>, where the extra element is used for mask generation corresponding to the ground truth bounding boxes. <math id="S3.SS1.p10.2.m2.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S3.SS1.p10.2.m2.1a"><msub id="S3.SS1.p10.2.m2.1.1" xref="S3.SS1.p10.2.m2.1.1.cmml"><mi id="S3.SS1.p10.2.m2.1.1.2" xref="S3.SS1.p10.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS1.p10.2.m2.1.1.3" xref="S3.SS1.p10.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.2.m2.1b"><apply id="S3.SS1.p10.2.m2.1.1.cmml" xref="S3.SS1.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p10.2.m2.1.1.1.cmml" xref="S3.SS1.p10.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p10.2.m2.1.1.2.cmml" xref="S3.SS1.p10.2.m2.1.1.2">ğ‘“</ci><ci id="S3.SS1.p10.2.m2.1.1.3.cmml" xref="S3.SS1.p10.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.2.m2.1c">f_{s}</annotation></semantics></math> and <math id="S3.SS1.p10.3.m3.1" class="ltx_Math" alttext="f_{d}" display="inline"><semantics id="S3.SS1.p10.3.m3.1a"><msub id="S3.SS1.p10.3.m3.1.1" xref="S3.SS1.p10.3.m3.1.1.cmml"><mi id="S3.SS1.p10.3.m3.1.1.2" xref="S3.SS1.p10.3.m3.1.1.2.cmml">f</mi><mi id="S3.SS1.p10.3.m3.1.1.3" xref="S3.SS1.p10.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.3.m3.1b"><apply id="S3.SS1.p10.3.m3.1.1.cmml" xref="S3.SS1.p10.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p10.3.m3.1.1.1.cmml" xref="S3.SS1.p10.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p10.3.m3.1.1.2.cmml" xref="S3.SS1.p10.3.m3.1.1.2">ğ‘“</ci><ci id="S3.SS1.p10.3.m3.1.1.3.cmml" xref="S3.SS1.p10.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.3.m3.1c">f_{d}</annotation></semantics></math> are passed through an MLP along with the learned mask values from the three temporal planes. The output of the MLP is a feature vector <math id="S3.SS1.p10.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS1.p10.4.m4.1a"><mi id="S3.SS1.p10.4.m4.1.1" xref="S3.SS1.p10.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.4.m4.1b"><ci id="S3.SS1.p10.4.m4.1.1.cmml" xref="S3.SS1.p10.4.m4.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.4.m4.1c">f</annotation></semantics></math> of dimension <math id="S3.SS1.p10.5.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p10.5.m5.1a"><mi id="S3.SS1.p10.5.m5.1.1" xref="S3.SS1.p10.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.5.m5.1b"><ci id="S3.SS1.p10.5.m5.1.1.cmml" xref="S3.SS1.p10.5.m5.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.5.m5.1c">D</annotation></semantics></math>. This vector is passed as input to the decoder MLPs to output a density and color at <math id="S3.SS1.p10.6.m6.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS1.p10.6.m6.1a"><mi id="S3.SS1.p10.6.m6.1.1" xref="S3.SS1.p10.6.m6.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.6.m6.1b"><ci id="S3.SS1.p10.6.m6.1.1.cmml" xref="S3.SS1.p10.6.m6.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.6.m6.1c">\mathbf{q}</annotation></semantics></math>, the same as in the stock algorithm.</p>
</div>
<div id="S3.SS1.p11" class="ltx_para">
<p id="S3.SS1.p11.1" class="ltx_p">We also introduced a cosine-similarity loss between the plane feature vectors for each location. The loss is applied between each pair of static and dynamic spatial planes, as follows:</p>
</div>
<div id="S3.SS1.p12" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\text{cos}}=\sum_{c\in C_{spatial}}|d_{\text{cos}}(f_{s}(\mathbf{q})_{c},~{}f_{d}(\mathbf{q})_{c})|," display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.3.3.1.1.3.2" xref="S3.E3.m1.3.3.1.1.3.2.cmml">â„’</mi><mtext id="S3.E3.m1.3.3.1.1.3.3" xref="S3.E3.m1.3.3.1.1.3.3a.cmml">cos</mtext></msub><mo rspace="0.111em" id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><munder id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.3.3.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2.3.2" xref="S3.E3.m1.3.3.1.1.1.2.3.2.cmml">c</mi><mo id="S3.E3.m1.3.3.1.1.1.2.3.1" xref="S3.E3.m1.3.3.1.1.1.2.3.1.cmml">âˆˆ</mo><msub id="S3.E3.m1.3.3.1.1.1.2.3.3" xref="S3.E3.m1.3.3.1.1.1.2.3.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.2" xref="S3.E3.m1.3.3.1.1.1.2.3.3.2.cmml">C</mi><mrow id="S3.E3.m1.3.3.1.1.1.2.3.3.3" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.2" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.3" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1a" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.4" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1b" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.5" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1c" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.6" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1d" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.7" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1e" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.3.3.1.1.1.2.3.3.3.8" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.8.cmml">l</mi></mrow></msub></mrow></munder><mrow id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.4" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.4.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.2.cmml">d</mi><mtext id="S3.E3.m1.3.3.1.1.1.1.1.1.4.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.3a.cmml">cos</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">ğª</mi><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">c</mi></msub></mrow><mo rspace="0.497em" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.4" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.2.cmml">f</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml">â€‹</mo><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.2.2.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">(</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">ğª</mi><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.2.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.3.cmml">c</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.5" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"></eq><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2">â„’</ci><ci id="S3.E3.m1.3.3.1.1.3.3a.cmml" xref="S3.E3.m1.3.3.1.1.3.3"><mtext mathsize="70%" id="S3.E3.m1.3.3.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3">cos</mtext></ci></apply><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><apply id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.3.3.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2.2"></sum><apply id="S3.E3.m1.3.3.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3"><in id="S3.E3.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.1"></in><ci id="S3.E3.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.2">ğ‘</ci><apply id="S3.E3.m1.3.3.1.1.1.2.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.2.3.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.2">ğ¶</ci><apply id="S3.E3.m1.3.3.1.1.1.2.3.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3"><times id="S3.E3.m1.3.3.1.1.1.2.3.3.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.1"></times><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.2">ğ‘ </ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.3">ğ‘</ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.4.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.4">ğ‘</ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.5.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.5">ğ‘¡</ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.6.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.6">ğ‘–</ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.7.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.7">ğ‘</ci><ci id="S3.E3.m1.3.3.1.1.1.2.3.3.3.8.cmml" xref="S3.E3.m1.3.3.1.1.1.2.3.3.3.8">ğ‘™</ci></apply></apply></apply></apply><apply id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><abs id="S3.E3.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.2"></abs><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3"></times><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.2">ğ‘‘</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.4.3a.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.3"><mtext mathsize="70%" id="S3.E3.m1.3.3.1.1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.3">cos</mtext></ci></apply><interval closure="open" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2"><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">ğ‘“</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.3">ğ‘ </ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğª</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">ğ‘</ci></apply></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.1"></times><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.2">ğ‘“</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.2.3">ğ‘‘</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğª</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.2.3.3">ğ‘</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\mathcal{L}_{\text{cos}}=\sum_{c\in C_{spatial}}|d_{\text{cos}}(f_{s}(\mathbf{q})_{c},~{}f_{d}(\mathbf{q})_{c})|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p13" class="ltx_para ltx_noindent">
<p id="S3.SS1.p13.2" class="ltx_p">where <math id="S3.SS1.p13.1.m1.2" class="ltx_Math" alttext="d_{\text{cos}}(\cdot,\cdot)" display="inline"><semantics id="S3.SS1.p13.1.m1.2a"><mrow id="S3.SS1.p13.1.m1.2.3" xref="S3.SS1.p13.1.m1.2.3.cmml"><msub id="S3.SS1.p13.1.m1.2.3.2" xref="S3.SS1.p13.1.m1.2.3.2.cmml"><mi id="S3.SS1.p13.1.m1.2.3.2.2" xref="S3.SS1.p13.1.m1.2.3.2.2.cmml">d</mi><mtext id="S3.SS1.p13.1.m1.2.3.2.3" xref="S3.SS1.p13.1.m1.2.3.2.3a.cmml">cos</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p13.1.m1.2.3.1" xref="S3.SS1.p13.1.m1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p13.1.m1.2.3.3.2" xref="S3.SS1.p13.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p13.1.m1.2.3.3.2.1" xref="S3.SS1.p13.1.m1.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p13.1.m1.1.1" xref="S3.SS1.p13.1.m1.1.1.cmml">â‹…</mo><mo rspace="0em" id="S3.SS1.p13.1.m1.2.3.3.2.2" xref="S3.SS1.p13.1.m1.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p13.1.m1.2.2" xref="S3.SS1.p13.1.m1.2.2.cmml">â‹…</mo><mo stretchy="false" id="S3.SS1.p13.1.m1.2.3.3.2.3" xref="S3.SS1.p13.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.1.m1.2b"><apply id="S3.SS1.p13.1.m1.2.3.cmml" xref="S3.SS1.p13.1.m1.2.3"><times id="S3.SS1.p13.1.m1.2.3.1.cmml" xref="S3.SS1.p13.1.m1.2.3.1"></times><apply id="S3.SS1.p13.1.m1.2.3.2.cmml" xref="S3.SS1.p13.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p13.1.m1.2.3.2.1.cmml" xref="S3.SS1.p13.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS1.p13.1.m1.2.3.2.2.cmml" xref="S3.SS1.p13.1.m1.2.3.2.2">ğ‘‘</ci><ci id="S3.SS1.p13.1.m1.2.3.2.3a.cmml" xref="S3.SS1.p13.1.m1.2.3.2.3"><mtext mathsize="70%" id="S3.SS1.p13.1.m1.2.3.2.3.cmml" xref="S3.SS1.p13.1.m1.2.3.2.3">cos</mtext></ci></apply><interval closure="open" id="S3.SS1.p13.1.m1.2.3.3.1.cmml" xref="S3.SS1.p13.1.m1.2.3.3.2"><ci id="S3.SS1.p13.1.m1.1.1.cmml" xref="S3.SS1.p13.1.m1.1.1">â‹…</ci><ci id="S3.SS1.p13.1.m1.2.2.cmml" xref="S3.SS1.p13.1.m1.2.2">â‹…</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.1.m1.2c">d_{\text{cos}}(\cdot,\cdot)</annotation></semantics></math> represents cosine similarity and <math id="S3.SS1.p13.2.m2.1" class="ltx_Math" alttext="C_{spatial}" display="inline"><semantics id="S3.SS1.p13.2.m2.1a"><msub id="S3.SS1.p13.2.m2.1.1" xref="S3.SS1.p13.2.m2.1.1.cmml"><mi id="S3.SS1.p13.2.m2.1.1.2" xref="S3.SS1.p13.2.m2.1.1.2.cmml">C</mi><mrow id="S3.SS1.p13.2.m2.1.1.3" xref="S3.SS1.p13.2.m2.1.1.3.cmml"><mi id="S3.SS1.p13.2.m2.1.1.3.2" xref="S3.SS1.p13.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.3" xref="S3.SS1.p13.2.m2.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1a" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.4" xref="S3.SS1.p13.2.m2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1b" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.5" xref="S3.SS1.p13.2.m2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1c" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.6" xref="S3.SS1.p13.2.m2.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1d" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.7" xref="S3.SS1.p13.2.m2.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p13.2.m2.1.1.3.1e" xref="S3.SS1.p13.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p13.2.m2.1.1.3.8" xref="S3.SS1.p13.2.m2.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p13.2.m2.1b"><apply id="S3.SS1.p13.2.m2.1.1.cmml" xref="S3.SS1.p13.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p13.2.m2.1.1.1.cmml" xref="S3.SS1.p13.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p13.2.m2.1.1.2.cmml" xref="S3.SS1.p13.2.m2.1.1.2">ğ¶</ci><apply id="S3.SS1.p13.2.m2.1.1.3.cmml" xref="S3.SS1.p13.2.m2.1.1.3"><times id="S3.SS1.p13.2.m2.1.1.3.1.cmml" xref="S3.SS1.p13.2.m2.1.1.3.1"></times><ci id="S3.SS1.p13.2.m2.1.1.3.2.cmml" xref="S3.SS1.p13.2.m2.1.1.3.2">ğ‘ </ci><ci id="S3.SS1.p13.2.m2.1.1.3.3.cmml" xref="S3.SS1.p13.2.m2.1.1.3.3">ğ‘</ci><ci id="S3.SS1.p13.2.m2.1.1.3.4.cmml" xref="S3.SS1.p13.2.m2.1.1.3.4">ğ‘</ci><ci id="S3.SS1.p13.2.m2.1.1.3.5.cmml" xref="S3.SS1.p13.2.m2.1.1.3.5">ğ‘¡</ci><ci id="S3.SS1.p13.2.m2.1.1.3.6.cmml" xref="S3.SS1.p13.2.m2.1.1.3.6">ğ‘–</ci><ci id="S3.SS1.p13.2.m2.1.1.3.7.cmml" xref="S3.SS1.p13.2.m2.1.1.3.7">ğ‘</ci><ci id="S3.SS1.p13.2.m2.1.1.3.8.cmml" xref="S3.SS1.p13.2.m2.1.1.3.8">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p13.2.m2.1c">C_{spatial}</annotation></semantics></math> are the spatial dimension combination of planes for both static and dynamic planes. The absolute value of the cosine similarity is minimized in order to separate the learned features for the static and dynamic planes. This loss helps to reduce cross learning of static features in the dynamic planes due to pose inaccuracies being interpreted as temporal movement between frames.</p>
</div>
<div id="S3.SS1.p14" class="ltx_para">
<p id="S3.SS1.p14.1" class="ltx_p">Finally, we take inspiration from the mask loss in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to implement a similar function. Ground truth bounding boxes are used as a segmentation mask for the dynamic regions of a scene. Given the separation of static and dynamic planes as mentioned above, we can apply this mask to the back propagation of the gradients for each set of planes such that only gradients from static pixels influence the static planes and only gradients from dynamic pixels influence the dynamic planes.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Novel Camera Pose Selection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To generate novel-view images, it is required to identify the camera pose <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">P</annotation></semantics></math> that defines the view. For static scens, we randomly sample new camera poses based on input poses for the NeRF. In order to diversify our dataset and ensure robust training, we incorporate various poses that encompass factors like altitude, camera viewing angle, and radius rotation of the camera circle. Subsequently, we interpolate these novel trajectories to render the novel-view images.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.8" class="ltx_p">When determining new camera poses for dynamic scenes, the novel pose cannot deviate significantly from the input poses to NeRF. Considering this limitation, we randomly selected <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">N</annotation></semantics></math> spatial locations on the camera trajectory of the training images. For each spatial location, we generated images at three different times for dynamic scenes. The three different times represent 1) the time when the camera is at the location, 2) the preceding time by <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\Delta\&gt;t" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">Î”</mi><mo lspace="0.220em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">Î”</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\Delta\&gt;t</annotation></semantics></math>, and 3) the succeeding time by <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\Delta\&gt;t" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">Î”</mi><mo lspace="0.220em" rspace="0em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">Î”</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\Delta\&gt;t</annotation></semantics></math> (<span id="S3.SS2.p2.8.1" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">t</annotation></semantics></math>, <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="t\pm\Delta\&gt;t" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">t</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">Â±</mo><mrow id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">Î”</mi><mo lspace="0.220em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.3.1" xref="S3.SS2.p2.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1">plus-or-minus</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ğ‘¡</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><times id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.1"></times><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">Î”</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">t\pm\Delta\&gt;t</annotation></semantics></math>). <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\Delta\&gt;t" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">Î”</mi><mo lspace="0.220em" rspace="0em" id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">Î”</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\Delta\&gt;t</annotation></semantics></math> is defined as the recipocal of a frame rate of UAV-based footage. As a result, using the selected camera poses and the trained NeRF models, we can generate <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">N</annotation></semantics></math> novel-view images for static scenes and 3<math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">N</annotation></semantics></math> novel-view images for dynamic scenes.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Implementation and Performance</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conducted experiments using two datasets, the Archangel and Okutama-Action datasets, both of which entail detecting peoplee in various poses standing in open areas. The Archangel dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is chosen for static scenes as it represents a fairly simple scenario of people maintaining poses in a field. This offers a strong baseline as both the background and target objects lack complicating details. The Okutama-Action dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is chosen for dynamic scenes as it represents a more realistic scenario for UAV drone footage. Background details are more complicated than Archangel and the moving people in the scene reflect the reality of most situations in which not every object is static.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Archangel Dataset</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The Archangel dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is a unique hybrid dataset captured using Unmanned Aerial Vehicles (UAVs), under similar imaging conditions in both real and synthetic domains. It provides metadata detailing camera positions, including UAV altitudes and the radii of rotation circles, for each individual image. We only use the Archangel-Real sub-dataset for our experiment. Archangel-Real, collected from a real-world environment, includes a group of real human subjects as targets, with each individual assuming one of three potential poses: standing, kneeling, and prone. Both the altitude and radius of the rotation circle were varied within the range of 15 to 50 meters, with 5-meter increments.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Okutama-Action Dataset</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The Okutama-Action dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> includes video from a UAV within a baseball stadium with multiple human â€œagentsâ€ in the scene performing a variety of single and multi-agent actions. There are up to 9 actors per scene and 12 different action categories on display. The drones range from 10 to 45 meters in altitude with a camera angle set to either 45 or 90 degrees. The dataset consists of two separate set of scenes, each one of which with its own unique pilot for the drone. Videos are captured at both morning and noon time frames. In light of all of this, Okutama-Action presents a challenging dataset for a variety of tasks, including scene reconstruction and novel view synthesis with NeRF algorithms.</p>
</div>
<figure id="S4.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span> AP Comparison on the different altitudes of YOLOv8 family</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T1.12.12" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:121.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-220.3pt,65.1pt) scale(0.483191543973876,0.483191543973876) ;">
<table id="S4.T1.12.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.12.12.12.13.1" class="ltx_tr">
<th id="S4.T1.12.12.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.12.12.12.13.1.1.1" class="ltx_text">Data</span></th>
<th id="S4.T1.12.12.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.12.12.12.13.1.2.1" class="ltx_text">Model</span></th>
<th id="S4.T1.12.12.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.12.12.12.13.1.3.1" class="ltx_text">Metric</span></th>
<th id="S4.T1.12.12.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">Altitude 35m</th>
<th id="S4.T1.12.12.12.13.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">Altitude 40m</th>
<th id="S4.T1.12.12.12.13.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">Altitude 45m</th>
<th id="S4.T1.12.12.12.13.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Altitude 50m</th>
</tr>
<tr id="S4.T1.12.12.12.14.2" class="ltx_tr">
<th id="S4.T1.12.12.12.14.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T1.12.12.12.14.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T1.12.12.12.14.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T1.12.12.12.14.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">All</th>
<th id="S4.T1.12.12.12.14.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T1.12.12.12.14.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T1.12.12.12.14.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T1.12.12.12.14.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">All</th>
<th id="S4.T1.12.12.12.14.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T1.12.12.12.14.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T1.12.12.12.14.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T1.12.12.12.14.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">All</th>
<th id="S4.T1.12.12.12.14.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T1.12.12.12.14.2.14" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T1.12.12.12.14.2.15" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T1.12.12.12.14.2.16" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Real</th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">YOLOv8n</th>
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T1.1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.2"><mtext id="S4.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">0.877</td>
<td id="S4.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.815</td>
<td id="S4.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">0.937</td>
<td id="S4.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.876</td>
<td id="S4.T1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">0.756</td>
<td id="S4.T1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">0.674</td>
<td id="S4.T1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t">0.914</td>
<td id="S4.T1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.782</td>
<td id="S4.T1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t">0.663</td>
<td id="S4.T1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_border_t">0.602</td>
<td id="S4.T1.1.1.1.1.14" class="ltx_td ltx_align_center ltx_border_t">0.893</td>
<td id="S4.T1.1.1.1.1.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.719</td>
<td id="S4.T1.1.1.1.1.16" class="ltx_td ltx_align_center ltx_border_t">0.491</td>
<td id="S4.T1.1.1.1.1.17" class="ltx_td ltx_align_center ltx_border_t">0.351</td>
<td id="S4.T1.1.1.1.1.18" class="ltx_td ltx_align_center ltx_border_t">0.748</td>
<td id="S4.T1.1.1.1.1.19" class="ltx_td ltx_align_center ltx_border_t">0.530</td>
</tr>
<tr id="S4.T1.2.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S4.T1.2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8n</th>
<th id="S4.T1.2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.2.2.2.2.1.m1.1a"><msub id="S4.T1.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.1.m1.1.1.cmml"><mtext id="S4.T1.2.2.2.2.1.m1.1.1.2" xref="S4.T1.2.2.2.2.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.2.2.2.2.1.m1.1.1.3" xref="S4.T1.2.2.2.2.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.1.m1.1b"><apply id="S4.T1.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.2.1.m1.1.1.2a.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1.2"><mtext id="S4.T1.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.2.2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.2.2.2.4.1" class="ltx_text ltx_font_bold">0.992</span></td>
<td id="S4.T1.2.2.2.2.5" class="ltx_td ltx_align_center">0.976</td>
<td id="S4.T1.2.2.2.2.6" class="ltx_td ltx_align_center">0.969</td>
<td id="S4.T1.2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">0.979</td>
<td id="S4.T1.2.2.2.2.8" class="ltx_td ltx_align_center">0.977</td>
<td id="S4.T1.2.2.2.2.9" class="ltx_td ltx_align_center">0.967</td>
<td id="S4.T1.2.2.2.2.10" class="ltx_td ltx_align_center">0.948</td>
<td id="S4.T1.2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r">0.964</td>
<td id="S4.T1.2.2.2.2.12" class="ltx_td ltx_align_center">0.977</td>
<td id="S4.T1.2.2.2.2.13" class="ltx_td ltx_align_center">0.905</td>
<td id="S4.T1.2.2.2.2.14" class="ltx_td ltx_align_center">0.898</td>
<td id="S4.T1.2.2.2.2.15" class="ltx_td ltx_align_center ltx_border_r">0.927</td>
<td id="S4.T1.2.2.2.2.16" class="ltx_td ltx_align_center">0.856</td>
<td id="S4.T1.2.2.2.2.17" class="ltx_td ltx_align_center">0.555</td>
<td id="S4.T1.2.2.2.2.18" class="ltx_td ltx_align_center">0.841</td>
<td id="S4.T1.2.2.2.2.19" class="ltx_td ltx_align_center">0.750</td>
</tr>
<tr id="S4.T1.3.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R + S</th>
<th id="S4.T1.3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8n</th>
<th id="S4.T1.3.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.3.3.3.3.1.m1.1a"><msub id="S4.T1.3.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.3.1.m1.1.1.cmml"><mtext id="S4.T1.3.3.3.3.1.m1.1.1.2" xref="S4.T1.3.3.3.3.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.3.3.3.3.1.m1.1.1.3" xref="S4.T1.3.3.3.3.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.1.m1.1b"><apply id="S4.T1.3.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.3.3.1.m1.1.1.2a.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1.2"><mtext id="S4.T1.3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.3.3.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.3.3.3.3.4" class="ltx_td ltx_align_center">0.978</td>
<td id="S4.T1.3.3.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.5.1" class="ltx_text ltx_font_bold">0.983</span></td>
<td id="S4.T1.3.3.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.6.1" class="ltx_text ltx_font_bold">0.993</span></td>
<td id="S4.T1.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.3.3.7.1" class="ltx_text ltx_font_bold">0.985</span></td>
<td id="S4.T1.3.3.3.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.8.1" class="ltx_text ltx_font_bold">0.980</span></td>
<td id="S4.T1.3.3.3.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.9.1" class="ltx_text ltx_font_bold">0.984</span></td>
<td id="S4.T1.3.3.3.3.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.10.1" class="ltx_text ltx_font_bold">0.975</span></td>
<td id="S4.T1.3.3.3.3.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.3.3.11.1" class="ltx_text ltx_font_bold">0.980</span></td>
<td id="S4.T1.3.3.3.3.12" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.12.1" class="ltx_text ltx_font_bold">0.977</span></td>
<td id="S4.T1.3.3.3.3.13" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.13.1" class="ltx_text ltx_font_bold">0.957</span></td>
<td id="S4.T1.3.3.3.3.14" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.14.1" class="ltx_text ltx_font_bold">0.944</span></td>
<td id="S4.T1.3.3.3.3.15" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.3.3.15.1" class="ltx_text ltx_font_bold">0.959</span></td>
<td id="S4.T1.3.3.3.3.16" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.16.1" class="ltx_text ltx_font_bold">0.864</span></td>
<td id="S4.T1.3.3.3.3.17" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.17.1" class="ltx_text ltx_font_bold">0.670</span></td>
<td id="S4.T1.3.3.3.3.18" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.18.1" class="ltx_text ltx_font_bold">0.942</span></td>
<td id="S4.T1.3.3.3.3.19" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.19.1" class="ltx_text ltx_font_bold">0.826</span></td>
</tr>
<tr id="S4.T1.4.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Real</th>
<th id="S4.T1.4.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">YOLOv8n</th>
<th id="S4.T1.4.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T1.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.4.4.4.4.1.m1.1a"><msub id="S4.T1.4.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.4.1.m1.1.1.cmml"><mtext id="S4.T1.4.4.4.4.1.m1.1.1.2" xref="S4.T1.4.4.4.4.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.4.4.4.4.1.m1.1.1.3" xref="S4.T1.4.4.4.4.1.m1.1.1.3.cmml"><mn id="S4.T1.4.4.4.4.1.m1.1.1.3.2" xref="S4.T1.4.4.4.4.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.4.4.4.4.1.m1.1.1.3.1" xref="S4.T1.4.4.4.4.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.4.4.4.4.1.m1.1.1.3.3" xref="S4.T1.4.4.4.4.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.1.m1.1b"><apply id="S4.T1.4.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.4.4.1.m1.1.1.2a.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.2"><mtext id="S4.T1.4.4.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.4.4.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.3"><ci id="S4.T1.4.4.4.4.1.m1.1.1.3.1.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.4.4.4.4.1.m1.1.1.3.2.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.4.4.4.4.1.m1.1.1.3.3.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.507</td>
<td id="S4.T1.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">0.372</td>
<td id="S4.T1.4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.4.6.1" class="ltx_text ltx_font_bold">0.447</span></td>
<td id="S4.T1.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.442</td>
<td id="S4.T1.4.4.4.4.8" class="ltx_td ltx_align_center ltx_border_t">0.377</td>
<td id="S4.T1.4.4.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.4.9.1" class="ltx_text ltx_font_bold">0.444</span></td>
<td id="S4.T1.4.4.4.4.10" class="ltx_td ltx_align_center ltx_border_t">0.312</td>
<td id="S4.T1.4.4.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.374</td>
<td id="S4.T1.4.4.4.4.12" class="ltx_td ltx_align_center ltx_border_t">0.387</td>
<td id="S4.T1.4.4.4.4.13" class="ltx_td ltx_align_center ltx_border_t">0.284</td>
<td id="S4.T1.4.4.4.4.14" class="ltx_td ltx_align_center ltx_border_t">0.293</td>
<td id="S4.T1.4.4.4.4.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.321</td>
<td id="S4.T1.4.4.4.4.16" class="ltx_td ltx_align_center ltx_border_t">0.183</td>
<td id="S4.T1.4.4.4.4.17" class="ltx_td ltx_align_center ltx_border_t">0.106</td>
<td id="S4.T1.4.4.4.4.18" class="ltx_td ltx_align_center ltx_border_t">0.257</td>
<td id="S4.T1.4.4.4.4.19" class="ltx_td ltx_align_center ltx_border_t">0.182</td>
</tr>
<tr id="S4.T1.5.5.5.5" class="ltx_tr">
<th id="S4.T1.5.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S4.T1.5.5.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8n</th>
<th id="S4.T1.5.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.5.5.5.5.1.m1.1a"><msub id="S4.T1.5.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.5.1.m1.1.1.cmml"><mtext id="S4.T1.5.5.5.5.1.m1.1.1.2" xref="S4.T1.5.5.5.5.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.5.5.5.5.1.m1.1.1.3" xref="S4.T1.5.5.5.5.1.m1.1.1.3.cmml"><mn id="S4.T1.5.5.5.5.1.m1.1.1.3.2" xref="S4.T1.5.5.5.5.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.5.5.5.5.1.m1.1.1.3.1" xref="S4.T1.5.5.5.5.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.5.5.5.5.1.m1.1.1.3.3" xref="S4.T1.5.5.5.5.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.1.m1.1b"><apply id="S4.T1.5.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.5.5.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1">subscript</csymbol><ci id="S4.T1.5.5.5.5.1.m1.1.1.2a.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.2"><mtext id="S4.T1.5.5.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.5.5.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.3"><ci id="S4.T1.5.5.5.5.1.m1.1.1.3.1.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.5.5.5.5.1.m1.1.1.3.2.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.5.5.5.5.1.m1.1.1.3.3.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.5.5.5.5.4" class="ltx_td ltx_align_center">0.455</td>
<td id="S4.T1.5.5.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.5.5.5.1" class="ltx_text ltx_font_bold">0.423</span></td>
<td id="S4.T1.5.5.5.5.6" class="ltx_td ltx_align_center">0.343</td>
<td id="S4.T1.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r">0.407</td>
<td id="S4.T1.5.5.5.5.8" class="ltx_td ltx_align_center">0.494</td>
<td id="S4.T1.5.5.5.5.9" class="ltx_td ltx_align_center">0.405</td>
<td id="S4.T1.5.5.5.5.10" class="ltx_td ltx_align_center">0.366</td>
<td id="S4.T1.5.5.5.5.11" class="ltx_td ltx_align_center ltx_border_r">0.422</td>
<td id="S4.T1.5.5.5.5.12" class="ltx_td ltx_align_center">0.475</td>
<td id="S4.T1.5.5.5.5.13" class="ltx_td ltx_align_center">0.329</td>
<td id="S4.T1.5.5.5.5.14" class="ltx_td ltx_align_center">0.279</td>
<td id="S4.T1.5.5.5.5.15" class="ltx_td ltx_align_center ltx_border_r">0.361</td>
<td id="S4.T1.5.5.5.5.16" class="ltx_td ltx_align_center">0.255</td>
<td id="S4.T1.5.5.5.5.17" class="ltx_td ltx_align_center">0.136</td>
<td id="S4.T1.5.5.5.5.18" class="ltx_td ltx_align_center">0.255</td>
<td id="S4.T1.5.5.5.5.19" class="ltx_td ltx_align_center">0.215</td>
</tr>
<tr id="S4.T1.6.6.6.6" class="ltx_tr">
<th id="S4.T1.6.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R + S</th>
<th id="S4.T1.6.6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8n</th>
<th id="S4.T1.6.6.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.6.6.6.6.1.m1.1a"><msub id="S4.T1.6.6.6.6.1.m1.1.1" xref="S4.T1.6.6.6.6.1.m1.1.1.cmml"><mtext id="S4.T1.6.6.6.6.1.m1.1.1.2" xref="S4.T1.6.6.6.6.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.6.6.6.6.1.m1.1.1.3" xref="S4.T1.6.6.6.6.1.m1.1.1.3.cmml"><mn id="S4.T1.6.6.6.6.1.m1.1.1.3.2" xref="S4.T1.6.6.6.6.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.6.6.6.6.1.m1.1.1.3.1" xref="S4.T1.6.6.6.6.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.6.6.6.6.1.m1.1.1.3.3" xref="S4.T1.6.6.6.6.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.1.m1.1b"><apply id="S4.T1.6.6.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.6.6.6.6.1.m1.1.1.1.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1">subscript</csymbol><ci id="S4.T1.6.6.6.6.1.m1.1.1.2a.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.2"><mtext id="S4.T1.6.6.6.6.1.m1.1.1.2.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.6.6.6.6.1.m1.1.1.3.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.3"><ci id="S4.T1.6.6.6.6.1.m1.1.1.3.1.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.6.6.6.6.1.m1.1.1.3.2.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.6.6.6.6.1.m1.1.1.3.3.cmml" xref="S4.T1.6.6.6.6.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.6.6.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.4.1" class="ltx_text ltx_font_bold">0.546</span></td>
<td id="S4.T1.6.6.6.6.5" class="ltx_td ltx_align_center">0.418</td>
<td id="S4.T1.6.6.6.6.6" class="ltx_td ltx_align_center">0.389</td>
<td id="S4.T1.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.6.6.6.6.7.1" class="ltx_text ltx_font_bold">0.451</span></td>
<td id="S4.T1.6.6.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.8.1" class="ltx_text ltx_font_bold">0.546</span></td>
<td id="S4.T1.6.6.6.6.9" class="ltx_td ltx_align_center">0.433</td>
<td id="S4.T1.6.6.6.6.10" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.10.1" class="ltx_text ltx_font_bold">0.384</span></td>
<td id="S4.T1.6.6.6.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.6.6.6.6.11.1" class="ltx_text ltx_font_bold">0.454</span></td>
<td id="S4.T1.6.6.6.6.12" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.12.1" class="ltx_text ltx_font_bold">0.535</span></td>
<td id="S4.T1.6.6.6.6.13" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.13.1" class="ltx_text ltx_font_bold">0.393</span></td>
<td id="S4.T1.6.6.6.6.14" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.14.1" class="ltx_text ltx_font_bold">0.327</span></td>
<td id="S4.T1.6.6.6.6.15" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.6.6.6.6.15.1" class="ltx_text ltx_font_bold">0.418</span></td>
<td id="S4.T1.6.6.6.6.16" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.16.1" class="ltx_text ltx_font_bold">0.269</span></td>
<td id="S4.T1.6.6.6.6.17" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.17.1" class="ltx_text ltx_font_bold">0.190</span></td>
<td id="S4.T1.6.6.6.6.18" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.18.1" class="ltx_text ltx_font_bold">0.345</span></td>
<td id="S4.T1.6.6.6.6.19" class="ltx_td ltx_align_center"><span id="S4.T1.6.6.6.6.19.1" class="ltx_text ltx_font_bold">0.268</span></td>
</tr>
<tr id="S4.T1.7.7.7.7" class="ltx_tr">
<th id="S4.T1.7.7.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Real</th>
<th id="S4.T1.7.7.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">YOLOv8s</th>
<th id="S4.T1.7.7.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><math id="S4.T1.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.7.7.7.7.1.m1.1a"><msub id="S4.T1.7.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.7.1.m1.1.1.cmml"><mtext id="S4.T1.7.7.7.7.1.m1.1.1.2" xref="S4.T1.7.7.7.7.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.7.7.7.7.1.m1.1.1.3" xref="S4.T1.7.7.7.7.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.1.m1.1b"><apply id="S4.T1.7.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.7.7.7.7.1.m1.1.1.1.cmml" xref="S4.T1.7.7.7.7.1.m1.1.1">subscript</csymbol><ci id="S4.T1.7.7.7.7.1.m1.1.1.2a.cmml" xref="S4.T1.7.7.7.7.1.m1.1.1.2"><mtext id="S4.T1.7.7.7.7.1.m1.1.1.2.cmml" xref="S4.T1.7.7.7.7.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.7.7.7.7.1.m1.1.1.3.cmml" xref="S4.T1.7.7.7.7.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.7.7.7.7.4" class="ltx_td ltx_align_center ltx_border_tt">0.879</td>
<td id="S4.T1.7.7.7.7.5" class="ltx_td ltx_align_center ltx_border_tt">0.817</td>
<td id="S4.T1.7.7.7.7.6" class="ltx_td ltx_align_center ltx_border_tt">0.988</td>
<td id="S4.T1.7.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.895</td>
<td id="S4.T1.7.7.7.7.8" class="ltx_td ltx_align_center ltx_border_tt">0.832</td>
<td id="S4.T1.7.7.7.7.9" class="ltx_td ltx_align_center ltx_border_tt">0.763</td>
<td id="S4.T1.7.7.7.7.10" class="ltx_td ltx_align_center ltx_border_tt">0.983</td>
<td id="S4.T1.7.7.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.859</td>
<td id="S4.T1.7.7.7.7.12" class="ltx_td ltx_align_center ltx_border_tt">0.767</td>
<td id="S4.T1.7.7.7.7.13" class="ltx_td ltx_align_center ltx_border_tt">0.707</td>
<td id="S4.T1.7.7.7.7.14" class="ltx_td ltx_align_center ltx_border_tt">0.959</td>
<td id="S4.T1.7.7.7.7.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.811</td>
<td id="S4.T1.7.7.7.7.16" class="ltx_td ltx_align_center ltx_border_tt">0.586</td>
<td id="S4.T1.7.7.7.7.17" class="ltx_td ltx_align_center ltx_border_tt">0.490</td>
<td id="S4.T1.7.7.7.7.18" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.7.7.7.7.18.1" class="ltx_text ltx_font_bold">0.940</span></td>
<td id="S4.T1.7.7.7.7.19" class="ltx_td ltx_align_center ltx_border_tt">0.672</td>
</tr>
<tr id="S4.T1.8.8.8.8" class="ltx_tr">
<th id="S4.T1.8.8.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S4.T1.8.8.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8s</th>
<th id="S4.T1.8.8.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.8.8.8.8.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.8.8.8.8.1.m1.1a"><msub id="S4.T1.8.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.8.1.m1.1.1.cmml"><mtext id="S4.T1.8.8.8.8.1.m1.1.1.2" xref="S4.T1.8.8.8.8.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.8.8.8.8.1.m1.1.1.3" xref="S4.T1.8.8.8.8.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.1.m1.1b"><apply id="S4.T1.8.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.8.8.8.8.1.m1.1.1.1.cmml" xref="S4.T1.8.8.8.8.1.m1.1.1">subscript</csymbol><ci id="S4.T1.8.8.8.8.1.m1.1.1.2a.cmml" xref="S4.T1.8.8.8.8.1.m1.1.1.2"><mtext id="S4.T1.8.8.8.8.1.m1.1.1.2.cmml" xref="S4.T1.8.8.8.8.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.8.8.8.8.1.m1.1.1.3.cmml" xref="S4.T1.8.8.8.8.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.8.8.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.4.1" class="ltx_text ltx_font_bold">0.994</span></td>
<td id="S4.T1.8.8.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.5.1" class="ltx_text ltx_font_bold">0.958</span></td>
<td id="S4.T1.8.8.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.6.1" class="ltx_text ltx_font_bold">0.973</span></td>
<td id="S4.T1.8.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r">0.975</td>
<td id="S4.T1.8.8.8.8.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.8.1" class="ltx_text ltx_font_bold">0.991</span></td>
<td id="S4.T1.8.8.8.8.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.9.1" class="ltx_text ltx_font_bold">0.977</span></td>
<td id="S4.T1.8.8.8.8.10" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.10.1" class="ltx_text ltx_font_bold">0.991</span></td>
<td id="S4.T1.8.8.8.8.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.8.8.11.1" class="ltx_text ltx_font_bold">0.987</span></td>
<td id="S4.T1.8.8.8.8.12" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.12.1" class="ltx_text ltx_font_bold">0.992</span></td>
<td id="S4.T1.8.8.8.8.13" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.13.1" class="ltx_text ltx_font_bold">0.950</span></td>
<td id="S4.T1.8.8.8.8.14" class="ltx_td ltx_align_center">0.928</td>
<td id="S4.T1.8.8.8.8.15" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.8.8.15.1" class="ltx_text ltx_font_bold">0.957</span></td>
<td id="S4.T1.8.8.8.8.16" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.16.1" class="ltx_text ltx_font_bold">0.927</span></td>
<td id="S4.T1.8.8.8.8.17" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.17.1" class="ltx_text ltx_font_bold">0.880</span></td>
<td id="S4.T1.8.8.8.8.18" class="ltx_td ltx_align_center">0.890</td>
<td id="S4.T1.8.8.8.8.19" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.19.1" class="ltx_text ltx_font_bold">0.899</span></td>
</tr>
<tr id="S4.T1.9.9.9.9" class="ltx_tr">
<th id="S4.T1.9.9.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R + S</th>
<th id="S4.T1.9.9.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8s</th>
<th id="S4.T1.9.9.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.9.9.9.9.1.m1.1a"><msub id="S4.T1.9.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.9.1.m1.1.1.cmml"><mtext id="S4.T1.9.9.9.9.1.m1.1.1.2" xref="S4.T1.9.9.9.9.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.9.9.9.9.1.m1.1.1.3" xref="S4.T1.9.9.9.9.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.9.1.m1.1b"><apply id="S4.T1.9.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.9.9.9.9.1.m1.1.1.1.cmml" xref="S4.T1.9.9.9.9.1.m1.1.1">subscript</csymbol><ci id="S4.T1.9.9.9.9.1.m1.1.1.2a.cmml" xref="S4.T1.9.9.9.9.1.m1.1.1.2"><mtext id="S4.T1.9.9.9.9.1.m1.1.1.2.cmml" xref="S4.T1.9.9.9.9.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.9.9.9.9.1.m1.1.1.3.cmml" xref="S4.T1.9.9.9.9.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.9.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<td id="S4.T1.9.9.9.9.4" class="ltx_td ltx_align_center">0.992</td>
<td id="S4.T1.9.9.9.9.5" class="ltx_td ltx_align_center">0.949</td>
<td id="S4.T1.9.9.9.9.6" class="ltx_td ltx_align_center">0.994</td>
<td id="S4.T1.9.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.9.9.9.9.7.1" class="ltx_text ltx_font_bold">0.978</span></td>
<td id="S4.T1.9.9.9.9.8" class="ltx_td ltx_align_center">0.965</td>
<td id="S4.T1.9.9.9.9.9" class="ltx_td ltx_align_center">0.955</td>
<td id="S4.T1.9.9.9.9.10" class="ltx_td ltx_align_center">0.977</td>
<td id="S4.T1.9.9.9.9.11" class="ltx_td ltx_align_center ltx_border_r">0.966</td>
<td id="S4.T1.9.9.9.9.12" class="ltx_td ltx_align_center">0.944</td>
<td id="S4.T1.9.9.9.9.13" class="ltx_td ltx_align_center">0.929</td>
<td id="S4.T1.9.9.9.9.14" class="ltx_td ltx_align_center"><span id="S4.T1.9.9.9.9.14.1" class="ltx_text ltx_font_bold">0.964</span></td>
<td id="S4.T1.9.9.9.9.15" class="ltx_td ltx_align_center ltx_border_r">0.946</td>
<td id="S4.T1.9.9.9.9.16" class="ltx_td ltx_align_center">0.843</td>
<td id="S4.T1.9.9.9.9.17" class="ltx_td ltx_align_center">0.825</td>
<td id="S4.T1.9.9.9.9.18" class="ltx_td ltx_align_center">0.906</td>
<td id="S4.T1.9.9.9.9.19" class="ltx_td ltx_align_center">0.858</td>
</tr>
<tr id="S4.T1.10.10.10.10" class="ltx_tr">
<th id="S4.T1.10.10.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Real</th>
<th id="S4.T1.10.10.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">YOLOv8s</th>
<th id="S4.T1.10.10.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T1.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.10.10.10.10.1.m1.1a"><msub id="S4.T1.10.10.10.10.1.m1.1.1" xref="S4.T1.10.10.10.10.1.m1.1.1.cmml"><mtext id="S4.T1.10.10.10.10.1.m1.1.1.2" xref="S4.T1.10.10.10.10.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.10.10.10.10.1.m1.1.1.3" xref="S4.T1.10.10.10.10.1.m1.1.1.3.cmml"><mn id="S4.T1.10.10.10.10.1.m1.1.1.3.2" xref="S4.T1.10.10.10.10.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.10.10.10.10.1.m1.1.1.3.1" xref="S4.T1.10.10.10.10.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.10.10.10.10.1.m1.1.1.3.3" xref="S4.T1.10.10.10.10.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.10.1.m1.1b"><apply id="S4.T1.10.10.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.10.10.10.10.1.m1.1.1.1.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1">subscript</csymbol><ci id="S4.T1.10.10.10.10.1.m1.1.1.2a.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.2"><mtext id="S4.T1.10.10.10.10.1.m1.1.1.2.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.10.10.10.10.1.m1.1.1.3.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.3"><ci id="S4.T1.10.10.10.10.1.m1.1.1.3.1.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.10.10.10.10.1.m1.1.1.3.2.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.10.10.10.10.1.m1.1.1.3.3.cmml" xref="S4.T1.10.10.10.10.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.10.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.10.10.10.10.4" class="ltx_td ltx_align_center ltx_border_t">0.537</td>
<td id="S4.T1.10.10.10.10.5" class="ltx_td ltx_align_center ltx_border_t">0.367</td>
<td id="S4.T1.10.10.10.10.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.10.10.10.10.6.1" class="ltx_text ltx_font_bold">0.471</span></td>
<td id="S4.T1.10.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.459</td>
<td id="S4.T1.10.10.10.10.8" class="ltx_td ltx_align_center ltx_border_t">0.497</td>
<td id="S4.T1.10.10.10.10.9" class="ltx_td ltx_align_center ltx_border_t">0.315</td>
<td id="S4.T1.10.10.10.10.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.10.10.10.10.10.1" class="ltx_text ltx_font_bold">0.429</span></td>
<td id="S4.T1.10.10.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.414</td>
<td id="S4.T1.10.10.10.10.12" class="ltx_td ltx_align_center ltx_border_t">0.454</td>
<td id="S4.T1.10.10.10.10.13" class="ltx_td ltx_align_center ltx_border_t">0.328</td>
<td id="S4.T1.10.10.10.10.14" class="ltx_td ltx_align_center ltx_border_t">0.368</td>
<td id="S4.T1.10.10.10.10.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.383</td>
<td id="S4.T1.10.10.10.10.16" class="ltx_td ltx_align_center ltx_border_t">0.229</td>
<td id="S4.T1.10.10.10.10.17" class="ltx_td ltx_align_center ltx_border_t">0.161</td>
<td id="S4.T1.10.10.10.10.18" class="ltx_td ltx_align_center ltx_border_t">0.330</td>
<td id="S4.T1.10.10.10.10.19" class="ltx_td ltx_align_center ltx_border_t">0.240</td>
</tr>
<tr id="S4.T1.11.11.11.11" class="ltx_tr">
<th id="S4.T1.11.11.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S4.T1.11.11.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOv8s</th>
<th id="S4.T1.11.11.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T1.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.11.11.11.11.1.m1.1a"><msub id="S4.T1.11.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.11.1.m1.1.1.cmml"><mtext id="S4.T1.11.11.11.11.1.m1.1.1.2" xref="S4.T1.11.11.11.11.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.11.11.11.11.1.m1.1.1.3" xref="S4.T1.11.11.11.11.1.m1.1.1.3.cmml"><mn id="S4.T1.11.11.11.11.1.m1.1.1.3.2" xref="S4.T1.11.11.11.11.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.11.11.11.11.1.m1.1.1.3.1" xref="S4.T1.11.11.11.11.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.11.11.11.11.1.m1.1.1.3.3" xref="S4.T1.11.11.11.11.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.11.1.m1.1b"><apply id="S4.T1.11.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.11.11.11.11.1.m1.1.1.1.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1">subscript</csymbol><ci id="S4.T1.11.11.11.11.1.m1.1.1.2a.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.2"><mtext id="S4.T1.11.11.11.11.1.m1.1.1.2.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.11.11.11.11.1.m1.1.1.3.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.3"><ci id="S4.T1.11.11.11.11.1.m1.1.1.3.1.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.11.11.11.11.1.m1.1.1.3.2.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.11.11.11.11.1.m1.1.1.3.3.cmml" xref="S4.T1.11.11.11.11.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.11.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.11.11.11.11.4" class="ltx_td ltx_align_center">0.551</td>
<td id="S4.T1.11.11.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T1.11.11.11.11.5.1" class="ltx_text ltx_font_bold">0.464</span></td>
<td id="S4.T1.11.11.11.11.6" class="ltx_td ltx_align_center">0.391</td>
<td id="S4.T1.11.11.11.11.7" class="ltx_td ltx_align_center ltx_border_r">0.468</td>
<td id="S4.T1.11.11.11.11.8" class="ltx_td ltx_align_center">0.549</td>
<td id="S4.T1.11.11.11.11.9" class="ltx_td ltx_align_center">0.428</td>
<td id="S4.T1.11.11.11.11.10" class="ltx_td ltx_align_center">0.390</td>
<td id="S4.T1.11.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r">0.455</td>
<td id="S4.T1.11.11.11.11.12" class="ltx_td ltx_align_center">0.578</td>
<td id="S4.T1.11.11.11.11.13" class="ltx_td ltx_align_center">0.403</td>
<td id="S4.T1.11.11.11.11.14" class="ltx_td ltx_align_center">0.306</td>
<td id="S4.T1.11.11.11.11.15" class="ltx_td ltx_align_center ltx_border_r">0.429</td>
<td id="S4.T1.11.11.11.11.16" class="ltx_td ltx_align_center">0.311</td>
<td id="S4.T1.11.11.11.11.17" class="ltx_td ltx_align_center">0.253</td>
<td id="S4.T1.11.11.11.11.18" class="ltx_td ltx_align_center">0.310</td>
<td id="S4.T1.11.11.11.11.19" class="ltx_td ltx_align_center">0.291</td>
</tr>
<tr id="S4.T1.12.12.12.12" class="ltx_tr">
<th id="S4.T1.12.12.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">R + S</th>
<th id="S4.T1.12.12.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">YOLOv8s</th>
<th id="S4.T1.12.12.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><math id="S4.T1.12.12.12.12.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.12.12.12.12.1.m1.1a"><msub id="S4.T1.12.12.12.12.1.m1.1.1" xref="S4.T1.12.12.12.12.1.m1.1.1.cmml"><mtext id="S4.T1.12.12.12.12.1.m1.1.1.2" xref="S4.T1.12.12.12.12.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.12.12.12.12.1.m1.1.1.3" xref="S4.T1.12.12.12.12.1.m1.1.1.3.cmml"><mn id="S4.T1.12.12.12.12.1.m1.1.1.3.2" xref="S4.T1.12.12.12.12.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.12.12.12.12.1.m1.1.1.3.1" xref="S4.T1.12.12.12.12.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.12.12.12.12.1.m1.1.1.3.3" xref="S4.T1.12.12.12.12.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.12.1.m1.1b"><apply id="S4.T1.12.12.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.12.12.12.12.1.m1.1.1.1.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1">subscript</csymbol><ci id="S4.T1.12.12.12.12.1.m1.1.1.2a.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.2"><mtext id="S4.T1.12.12.12.12.1.m1.1.1.2.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T1.12.12.12.12.1.m1.1.1.3.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.3"><ci id="S4.T1.12.12.12.12.1.m1.1.1.3.1.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.12.12.12.12.1.m1.1.1.3.2.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.12.12.12.12.1.m1.1.1.3.3.cmml" xref="S4.T1.12.12.12.12.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.12.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
<td id="S4.T1.12.12.12.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.4.1" class="ltx_text ltx_font_bold">0.589</span></td>
<td id="S4.T1.12.12.12.12.5" class="ltx_td ltx_align_center ltx_border_bb">0.434</td>
<td id="S4.T1.12.12.12.12.6" class="ltx_td ltx_align_center ltx_border_bb">0.452</td>
<td id="S4.T1.12.12.12.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.12.12.12.12.7.1" class="ltx_text ltx_font_bold">0.492</span></td>
<td id="S4.T1.12.12.12.12.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.8.1" class="ltx_text ltx_font_bold">0.578</span></td>
<td id="S4.T1.12.12.12.12.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.9.1" class="ltx_text ltx_font_bold">0.459</span></td>
<td id="S4.T1.12.12.12.12.10" class="ltx_td ltx_align_center ltx_border_bb">0.421</td>
<td id="S4.T1.12.12.12.12.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.12.12.12.12.11.1" class="ltx_text ltx_font_bold">0.486</span></td>
<td id="S4.T1.12.12.12.12.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.12.1" class="ltx_text ltx_font_bold">0.589</span></td>
<td id="S4.T1.12.12.12.12.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.13.1" class="ltx_text ltx_font_bold">0.416</span></td>
<td id="S4.T1.12.12.12.12.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.14.1" class="ltx_text ltx_font_bold">0.383</span></td>
<td id="S4.T1.12.12.12.12.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.12.12.12.12.15.1" class="ltx_text ltx_font_bold">0.463</span></td>
<td id="S4.T1.12.12.12.12.16" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.16.1" class="ltx_text ltx_font_bold">0.330</span></td>
<td id="S4.T1.12.12.12.12.17" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.17.1" class="ltx_text ltx_font_bold">0.263</span></td>
<td id="S4.T1.12.12.12.12.18" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.18.1" class="ltx_text ltx_font_bold">0.365</span></td>
<td id="S4.T1.12.12.12.12.19" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.12.12.12.12.19.1" class="ltx_text ltx_font_bold">0.319</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T1.14" class="ltx_p ltx_figure_panel">â€œRealâ€ denotes the detection performance achieved through training with Archangel-Real data collected at an altitude of 15m. â€œSyntheticâ€ represents the performance achieved through training with synthetic data generated by NeRF. â€œR+Sâ€ indicates the detection accuracy attained through training with a combined dataset that includes both Archangel-Real and NeRF-generated synthetic data. Compared to YOLOv8n trained with â€œRealâ€ dataset, we improve the <math id="S4.T1.13.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T1.13.m1.1a"><msub id="S4.T1.13.m1.1.1" xref="S4.T1.13.m1.1.1.cmml"><mtext id="S4.T1.13.m1.1.1.2" xref="S4.T1.13.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T1.13.m1.1.1.3" xref="S4.T1.13.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.13.m1.1b"><apply id="S4.T1.13.m1.1.1.cmml" xref="S4.T1.13.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.13.m1.1.1.1.cmml" xref="S4.T1.13.m1.1.1">subscript</csymbol><ci id="S4.T1.13.m1.1.1.2a.cmml" xref="S4.T1.13.m1.1.1.2"><mtext id="S4.T1.13.m1.1.1.2.cmml" xref="S4.T1.13.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T1.13.m1.1.1.3.cmml" xref="S4.T1.13.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.m1.1c">\text{mAP}_{50}</annotation></semantics></math> metric by 55.85 % and the <math id="S4.T1.14.m2.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T1.14.m2.1a"><msub id="S4.T1.14.m2.1.1" xref="S4.T1.14.m2.1.1.cmml"><mtext id="S4.T1.14.m2.1.1.2" xref="S4.T1.14.m2.1.1.2a.cmml">mAP</mtext><mrow id="S4.T1.14.m2.1.1.3" xref="S4.T1.14.m2.1.1.3.cmml"><mn id="S4.T1.14.m2.1.1.3.2" xref="S4.T1.14.m2.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.14.m2.1.1.3.1" xref="S4.T1.14.m2.1.1.3.1.cmml">:</mo><mn id="S4.T1.14.m2.1.1.3.3" xref="S4.T1.14.m2.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.14.m2.1b"><apply id="S4.T1.14.m2.1.1.cmml" xref="S4.T1.14.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.14.m2.1.1.1.cmml" xref="S4.T1.14.m2.1.1">subscript</csymbol><ci id="S4.T1.14.m2.1.1.2a.cmml" xref="S4.T1.14.m2.1.1.2"><mtext id="S4.T1.14.m2.1.1.2.cmml" xref="S4.T1.14.m2.1.1.2">mAP</mtext></ci><apply id="S4.T1.14.m2.1.1.3.cmml" xref="S4.T1.14.m2.1.1.3"><ci id="S4.T1.14.m2.1.1.3.1.cmml" xref="S4.T1.14.m2.1.1.3.1">:</ci><cn type="integer" id="S4.T1.14.m2.1.1.3.2.cmml" xref="S4.T1.14.m2.1.1.3.2">50</cn><cn type="integer" id="S4.T1.14.m2.1.1.3.3.cmml" xref="S4.T1.14.m2.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.m2.1c">\text{mAP}_{50:95}</annotation></semantics></math> metric by 47.25 % at 50m altitude.</p>
</div>
</div>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Implementation Details</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.6" class="ltx_p">For each experiment, we employ the Nerfacto and K-Planes within the Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
The Nerfacto model are trained for 30K iterations, requiring up to half an hour on an NVIDIA RTX 3090 GPU, while K-planes models are trained for 100K iterations, taking up to 3.5 hours on the same GPU.
Hyperparameters for losses are kept as their default values. The learning rate is lowered to 1e-3 for both network proposals and plane features. Three plane sizes are used with dimensions <math id="S4.SS2.p1.1.m1.8" class="ltx_Math" alttext="(x,y,z,t)=(128,128,64,77)" display="inline"><semantics id="S4.SS2.p1.1.m1.8a"><mrow id="S4.SS2.p1.1.m1.8.9" xref="S4.SS2.p1.1.m1.8.9.cmml"><mrow id="S4.SS2.p1.1.m1.8.9.2.2" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.8.9.2.2.1" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml">(</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p1.1.m1.8.9.2.2.2" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">y</mi><mo id="S4.SS2.p1.1.m1.8.9.2.2.3" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.3.3" xref="S4.SS2.p1.1.m1.3.3.cmml">z</mi><mo id="S4.SS2.p1.1.m1.8.9.2.2.4" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.4.4" xref="S4.SS2.p1.1.m1.4.4.cmml">t</mi><mo stretchy="false" id="S4.SS2.p1.1.m1.8.9.2.2.5" xref="S4.SS2.p1.1.m1.8.9.2.1.cmml">)</mo></mrow><mo id="S4.SS2.p1.1.m1.8.9.1" xref="S4.SS2.p1.1.m1.8.9.1.cmml">=</mo><mrow id="S4.SS2.p1.1.m1.8.9.3.2" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.8.9.3.2.1" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml">(</mo><mn id="S4.SS2.p1.1.m1.5.5" xref="S4.SS2.p1.1.m1.5.5.cmml">128</mn><mo id="S4.SS2.p1.1.m1.8.9.3.2.2" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.6.6" xref="S4.SS2.p1.1.m1.6.6.cmml">128</mn><mo id="S4.SS2.p1.1.m1.8.9.3.2.3" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.7.7" xref="S4.SS2.p1.1.m1.7.7.cmml">64</mn><mo id="S4.SS2.p1.1.m1.8.9.3.2.4" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.8.8" xref="S4.SS2.p1.1.m1.8.8.cmml">77</mn><mo stretchy="false" id="S4.SS2.p1.1.m1.8.9.3.2.5" xref="S4.SS2.p1.1.m1.8.9.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.8b"><apply id="S4.SS2.p1.1.m1.8.9.cmml" xref="S4.SS2.p1.1.m1.8.9"><eq id="S4.SS2.p1.1.m1.8.9.1.cmml" xref="S4.SS2.p1.1.m1.8.9.1"></eq><vector id="S4.SS2.p1.1.m1.8.9.2.1.cmml" xref="S4.SS2.p1.1.m1.8.9.2.2"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘¥</ci><ci id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">ğ‘¦</ci><ci id="S4.SS2.p1.1.m1.3.3.cmml" xref="S4.SS2.p1.1.m1.3.3">ğ‘§</ci><ci id="S4.SS2.p1.1.m1.4.4.cmml" xref="S4.SS2.p1.1.m1.4.4">ğ‘¡</ci></vector><vector id="S4.SS2.p1.1.m1.8.9.3.1.cmml" xref="S4.SS2.p1.1.m1.8.9.3.2"><cn type="integer" id="S4.SS2.p1.1.m1.5.5.cmml" xref="S4.SS2.p1.1.m1.5.5">128</cn><cn type="integer" id="S4.SS2.p1.1.m1.6.6.cmml" xref="S4.SS2.p1.1.m1.6.6">128</cn><cn type="integer" id="S4.SS2.p1.1.m1.7.7.cmml" xref="S4.SS2.p1.1.m1.7.7">64</cn><cn type="integer" id="S4.SS2.p1.1.m1.8.8.cmml" xref="S4.SS2.p1.1.m1.8.8">77</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.8c">(x,y,z,t)=(128,128,64,77)</annotation></semantics></math> at base and with subsequent planes having a spatial dimension multiplier of 2 and 4, respectively. The <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">z</annotation></semantics></math> dimension is halved compared to <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">x</annotation></semantics></math> and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">y</annotation></semantics></math> because the rendering volume contains the cameras at the top of the volume pointing in approximately along the negative <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mi id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><ci id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">z</annotation></semantics></math> axis. This saves on memory as the positive <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">z</annotation></semantics></math> axis of any particular plane would remained unused. The time dimension is set to half the number of total time steps in the scene. Feature vector dimensions within each plane is set to 32.
Additionally, we fine-tune the object detection model, YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, for 20 epochs with with a batch size of 16 and other settings follow the default settings in the official website <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Evaluation</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Static NeRF</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To validate our method, we conducted experiments using Archangel-Real captured at diverse UAV altitudes. UAV-based images captured at different altitudes exhibit distinct image characteristics, presenting a challenging problem to tackle. Especially, at higher altitudes, the majority of human targets appear smaller, making it impractical to directly utilize detectors pre-trained by standard datasets. Object detectors trained for specific altitudes often struggle to generalize to images captured at different altitudes. Our data augmentation technique allows us to generate novel-view training images along with corresponding bounding boxes at different altitudes with unique viewpoints. We conducted four different experimental settings in which we initially trained the model with a low-altitude scenario (15m) and then evaluated it in high-altitude scenarios (35m, 40m, 45m, or 50m). In each experimental setup, we compare three detection models trained with different types of the dataset. â€œRealâ€ refers to the original data from Archangel-Real, while â€œSyntheticâ€ represents training data generated by NeRF. In terms of the synthetic data, we generate the same number of training images as that of real data to ensure a fair comparison. The â€œR+Sâ€ model utilizes a hybrid dataset, combining both â€œRealâ€ and â€œSyntheticâ€ datasets. Regarding object detection models, the YOLOv8 family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> offers a range of five different levels of architectural complexity, all pretrained on MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We utilize YOLOv8 models with a lower number of model parameters, specifically YOLOv8n and YOLOv8s, suitable for operating on resource-constrained UAV platforms.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">In Table <a href="#S4.T1" title="TABLE I â€£ IV-A2 Okutama-Action Dataset â€£ IV-A Datasets â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we can observe that YOLOv8 models trained with synthetic data only mostly outperform those trained solely with real datasets, indicating the advantage of synthesizing novel-view images at high altitudes and the realism of the synthetic data on par with the real data. Futhermore, when we optimize YOLOv8 on a hybrid dataset, it shows the best performance among models trained with either Real only or synthetic data only. Notably, the performance gap is even more noticeable at an altitude of 50m, thanks to the inclusion of high-fidelity synthetic images captured at high-altitudes. These results validate that data augmentation with NeRF can significantly enhance detection performance for UAV-based images, particularly in more challenging scenarios. In Table <a href="#S4.T2" title="TABLE II â€£ IV-C1 Static NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we initially trained the model at a high altitude of 50 meters and subsequently evaluated it in a low-altitude scenario at 15 meters. We observed a performance improvement trend similar to our previous results.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">We present visualization results of our method in Figure <a href="#S4.F4" title="Figure 4 â€£ IV-C1 Static NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In Figure <a href="#S4.F4" title="Figure 4 â€£ IV-C1 Static NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(b), our approach demonstrates its ability to produce images with enhanced realism from novel viewpoints. Notably, we achieve this with only low-altitude UAV-based images (15m) used to train the NeRF model, enabling it to render realistic images captured at high altitudes. Additionally, as shown in Figure <a href="#S4.F4" title="Figure 4 â€£ IV-C1 Static NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(c), our pipeline can generate corresponding bounding boxes for these images.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span> AP Comparison on the different altitudes of YOLOv8n</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T2.2.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:38.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.8pt,25.9pt) scale(0.425422987395984,0.425422987395984) ;">
<table id="S4.T2.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.2.3.1" class="ltx_text">Method</span></th>
<th id="S4.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.2.4.1" class="ltx_text">Altitude</span></th>
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><math id="S4.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T2.1.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T2.1.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2"><mtext id="S4.T2.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T2.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></th>
<th id="S4.T2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><math id="S4.T2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T2.2.2.2.2.2.m1.1a"><msub id="S4.T2.2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T2.2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.2.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T2.2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.2.m1.1.1.3.cmml"><mn id="S4.T2.2.2.2.2.2.m1.1.1.3.2" xref="S4.T2.2.2.2.2.2.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T2.2.2.2.2.2.m1.1.1.3.1" xref="S4.T2.2.2.2.2.2.m1.1.1.3.1.cmml">:</mo><mn id="S4.T2.2.2.2.2.2.m1.1.1.3.3" xref="S4.T2.2.2.2.2.2.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.2"><mtext id="S4.T2.2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.2">mAP</mtext></ci><apply id="S4.T2.2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.3"><ci id="S4.T2.2.2.2.2.2.m1.1.1.3.1.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T2.2.2.2.2.2.m1.1.1.3.2.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T2.2.2.2.2.2.m1.1.1.3.3.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.2.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></th>
</tr>
<tr id="S4.T2.2.2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T2.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T2.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T2.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">All</th>
<th id="S4.T2.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standing</th>
<th id="S4.T2.2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kneeling</th>
<th id="S4.T2.2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Prone</th>
<th id="S4.T2.2.2.2.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.2.4.1" class="ltx_tr">
<th id="S4.T2.2.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Real</th>
<th id="S4.T2.2.2.2.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50mâ€…â†’â€…15m</th>
<td id="S4.T2.2.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">0.284</td>
<td id="S4.T2.2.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">0.553</td>
<td id="S4.T2.2.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">0.774</td>
<td id="S4.T2.2.2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.537</td>
<td id="S4.T2.2.2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">0.0623</td>
<td id="S4.T2.2.2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">0.122</td>
<td id="S4.T2.2.2.2.4.1.9" class="ltx_td ltx_align_center ltx_border_t">0.263</td>
<td id="S4.T2.2.2.2.4.1.10" class="ltx_td ltx_align_center ltx_border_t">0.149</td>
</tr>
<tr id="S4.T2.2.2.2.5.2" class="ltx_tr">
<th id="S4.T2.2.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S4.T2.2.2.2.5.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">50mâ€…â†’â€…15m</th>
<td id="S4.T2.2.2.2.5.2.3" class="ltx_td ltx_align_center">0.432</td>
<td id="S4.T2.2.2.2.5.2.4" class="ltx_td ltx_align_center">0.684</td>
<td id="S4.T2.2.2.2.5.2.5" class="ltx_td ltx_align_center">0.898</td>
<td id="S4.T2.2.2.2.5.2.6" class="ltx_td ltx_align_center ltx_border_r">0.671</td>
<td id="S4.T2.2.2.2.5.2.7" class="ltx_td ltx_align_center">0.131</td>
<td id="S4.T2.2.2.2.5.2.8" class="ltx_td ltx_align_center">0.276</td>
<td id="S4.T2.2.2.2.5.2.9" class="ltx_td ltx_align_center">0.328</td>
<td id="S4.T2.2.2.2.5.2.10" class="ltx_td ltx_align_center">0.245</td>
</tr>
<tr id="S4.T2.2.2.2.6.3" class="ltx_tr">
<th id="S4.T2.2.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">R + S</th>
<th id="S4.T2.2.2.2.6.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">50mâ€…â†’â€…15m</th>
<td id="S4.T2.2.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.3.1" class="ltx_text ltx_font_bold">0.484</span></td>
<td id="S4.T2.2.2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.4.1" class="ltx_text ltx_font_bold">0.727</span></td>
<td id="S4.T2.2.2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.5.1" class="ltx_text ltx_font_bold">0.929</span></td>
<td id="S4.T2.2.2.2.6.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.2.2.2.6.3.6.1" class="ltx_text ltx_font_bold">0.713</span></td>
<td id="S4.T2.2.2.2.6.3.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.7.1" class="ltx_text ltx_font_bold">0.152</span></td>
<td id="S4.T2.2.2.2.6.3.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.8.1" class="ltx_text ltx_font_bold">0.275</span></td>
<td id="S4.T2.2.2.2.6.3.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.9.1" class="ltx_text ltx_font_bold">0.362</span></td>
<td id="S4.T2.2.2.2.6.3.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.3.10.1" class="ltx_text ltx_font_bold">0.263</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T2.4" class="ltx_p ltx_figure_panel">Our method shows improvement of YOLOv8n in <math id="S4.T2.3.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T2.3.m1.1a"><msub id="S4.T2.3.m1.1.1" xref="S4.T2.3.m1.1.1.cmml"><mtext id="S4.T2.3.m1.1.1.2" xref="S4.T2.3.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T2.3.m1.1.1.3" xref="S4.T2.3.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.m1.1b"><apply id="S4.T2.3.m1.1.1.cmml" xref="S4.T2.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.m1.1.1.1.cmml" xref="S4.T2.3.m1.1.1">subscript</csymbol><ci id="S4.T2.3.m1.1.1.2a.cmml" xref="S4.T2.3.m1.1.1.2"><mtext id="S4.T2.3.m1.1.1.2.cmml" xref="S4.T2.3.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T2.3.m1.1.1.3.cmml" xref="S4.T2.3.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.m1.1c">\text{mAP}_{50}</annotation></semantics></math> metric by 32.77 % and <math id="S4.T2.4.m2.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T2.4.m2.1a"><msub id="S4.T2.4.m2.1.1" xref="S4.T2.4.m2.1.1.cmml"><mtext id="S4.T2.4.m2.1.1.2" xref="S4.T2.4.m2.1.1.2a.cmml">mAP</mtext><mrow id="S4.T2.4.m2.1.1.3" xref="S4.T2.4.m2.1.1.3.cmml"><mn id="S4.T2.4.m2.1.1.3.2" xref="S4.T2.4.m2.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T2.4.m2.1.1.3.1" xref="S4.T2.4.m2.1.1.3.1.cmml">:</mo><mn id="S4.T2.4.m2.1.1.3.3" xref="S4.T2.4.m2.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.4.m2.1b"><apply id="S4.T2.4.m2.1.1.cmml" xref="S4.T2.4.m2.1.1"><csymbol cd="ambiguous" id="S4.T2.4.m2.1.1.1.cmml" xref="S4.T2.4.m2.1.1">subscript</csymbol><ci id="S4.T2.4.m2.1.1.2a.cmml" xref="S4.T2.4.m2.1.1.2"><mtext id="S4.T2.4.m2.1.1.2.cmml" xref="S4.T2.4.m2.1.1.2">mAP</mtext></ci><apply id="S4.T2.4.m2.1.1.3.cmml" xref="S4.T2.4.m2.1.1.3"><ci id="S4.T2.4.m2.1.1.3.1.cmml" xref="S4.T2.4.m2.1.1.3.1">:</ci><cn type="integer" id="S4.T2.4.m2.1.1.3.2.cmml" xref="S4.T2.4.m2.1.1.3.2">50</cn><cn type="integer" id="S4.T2.4.m2.1.1.3.3.cmml" xref="S4.T2.4.m2.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m2.1c">\text{mAP}_{50:95}</annotation></semantics></math> metric by 76.51 % at 15m altitdue.</p>
</div>
</div>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.16255/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization Results: (a) Examples of original images at 15m altitude from the Archangel dataset. (b) Synthetic images generated by NeRF. (c) Synthetic images with corresponding object pose: stand (blue), kneel (red), and prone (green). Although our NeRF model is initially trained on UAV images captured at low altitudes, it has the capability to generate photorealistic images from novel viewpoints at high altitudes.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.16255/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="438" height="567" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Left images show novel views from a validation set rendered by the stock K-Planes algorithm trained on a subset of the Okutama-Action dataset. Right images show novel views from the same validation set but rendered with our extended K-Planes algorithm trained on the same subset of data and same hyperparameters.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2310.16255/assets/ICRA_figures/bbox_comp.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization Results: (a) Example of masked image from the Okutama-Action dataset. (b) Novel-view images generated by extended K-Planes NeRF, with bounding box masks. (c) Novel-view images with corresponding bounding boxes: each person bound with a unique color box.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Dynamic NeRF</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">For validation on dynamic scenes, we conducted experiments using a subset of Okutama-Action. Amongst the same challenges mentioned for Archangel, the irregular flight path and moving targets in Okutama make for an especially challenging dataset with respect to novel view synthesis. Specifically, utilizing powerful techniques such as Pix-SfM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to extract camera poses can still output noisy pose estimates. Our extended version of K-Planes helps to alleviate some of these issues.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">To further help minimize errors from pose estimates, we extract a subset of frames representing a smooth linear path from a single Okutama video, morning video from drone pilot 1, video 1.1.1, frames 230 to 535 for a total of 306 frames. We then use Pix-SfM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to get the estimated poses for each frame and annotate each frame with a timestamp from 0 to 1 in their chronological order within the video. These frames are separated into a train and validation set, taking every other frame for the training set. Given this subset and training/validation split, we compare the PSNR of the validation set for both stock K-Planes and our extended K-Planes in Table <a href="#S4.T3" title="TABLE III â€£ IV-C2 Dynamic NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Our K-Planes model increases PSNR by 3.85 points over stock K-Planes, a large improvement that can be seen qualitatively in Figure <a href="#S4.F5" title="Figure 5 â€£ IV-C1 Static NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Our K-Planes model does a better job at separating the static and temporal elements of the scene given noisy camera pose estimations, reducing the blur and artifacts seen from stock K-Planes.</p>
</div>
<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span> PSNR Comparison on Okutama-Action Validation Subset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:86.7pt;height:34.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.5pt,9.7pt) scale(0.638970754064272,0.638970754064272) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text">PSNR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Stock K-Planes</th>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">16.62</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Extended K-Planes</th>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.3.2.2.1" class="ltx_text ltx_font_bold">20.47</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T3.2" class="ltx_p ltx_figure_panel">PSNR from validation subset of Okutama-Action. Validation subset consistes of every other image from frames 230 to 535 from the video 1.1.1: morning video, drone pilot 1.</p>
</div>
</div>
</figure>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span> AP Comparison on the Okutama-Action of YOLOv8 family</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T4.2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T4.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Data</span></th>
<th id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T4.2.2.2.4.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:90%;"><math id="S4.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T4.1.1.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T4.1.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.1.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1.2"><mtext id="S4.T4.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T4.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\text{mAP}_{50}</annotation></semantics></math></span></th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;"><math id="S4.T4.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\text{mAP}_{50:95}" display="inline"><semantics id="S4.T4.2.2.2.2.1.m1.1a"><msub id="S4.T4.2.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.2.1.m1.1.1.cmml"><mtext id="S4.T4.2.2.2.2.1.m1.1.1.2" xref="S4.T4.2.2.2.2.1.m1.1.1.2a.cmml">mAP</mtext><mrow id="S4.T4.2.2.2.2.1.m1.1.1.3" xref="S4.T4.2.2.2.2.1.m1.1.1.3.cmml"><mn id="S4.T4.2.2.2.2.1.m1.1.1.3.2" xref="S4.T4.2.2.2.2.1.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T4.2.2.2.2.1.m1.1.1.3.1" xref="S4.T4.2.2.2.2.1.m1.1.1.3.1.cmml">:</mo><mn id="S4.T4.2.2.2.2.1.m1.1.1.3.3" xref="S4.T4.2.2.2.2.1.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T4.2.2.2.2.1.m1.1.1.2a.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.2"><mtext id="S4.T4.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.2">mAP</mtext></ci><apply id="S4.T4.2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.3"><ci id="S4.T4.2.2.2.2.1.m1.1.1.3.1.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T4.2.2.2.2.1.m1.1.1.3.2.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T4.2.2.2.2.1.m1.1.1.3.3.cmml" xref="S4.T4.2.2.2.2.1.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.1.m1.1c">\text{mAP}_{50:95}</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.3.1" class="ltx_tr">
<th id="S4.T4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">Real</span></th>
<td id="S4.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.3.1.2.1" class="ltx_text" style="font-size:90%;">YOLOv8n</span></td>
<td id="S4.T4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.3.1.3.1" class="ltx_text" style="font-size:90%;">0.234</span></td>
<td id="S4.T4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.3.1.4.1" class="ltx_text" style="font-size:90%;">0.072</span></td>
</tr>
<tr id="S4.T4.2.2.4.2" class="ltx_tr">
<th id="S4.T4.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.2.2.4.2.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></th>
<td id="S4.T4.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.4.2.2.1" class="ltx_text" style="font-size:90%;">YOLOv8n</span></td>
<td id="S4.T4.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.4.2.3.1" class="ltx_text" style="font-size:90%;">0.224</span></td>
<td id="S4.T4.2.2.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.4.1" class="ltx_text" style="font-size:90%;">0.074</span></td>
</tr>
<tr id="S4.T4.2.2.5.3" class="ltx_tr">
<th id="S4.T4.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.2.2.5.3.1.1" class="ltx_text" style="font-size:90%;">R + S</span></th>
<td id="S4.T4.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.5.3.2.1" class="ltx_text" style="font-size:90%;">YOLOv8n</span></td>
<td id="S4.T4.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.5.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.263</span></td>
<td id="S4.T4.2.2.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.5.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.075</span></td>
</tr>
<tr id="S4.T4.2.2.6.4" class="ltx_tr">
<th id="S4.T4.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.2.2.6.4.1.1" class="ltx_text" style="font-size:90%;">Real</span></th>
<td id="S4.T4.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.6.4.2.1" class="ltx_text" style="font-size:90%;">YOLOv8s</span></td>
<td id="S4.T4.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.6.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.256</span></td>
<td id="S4.T4.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.6.4.4.1" class="ltx_text" style="font-size:90%;">0.086</span></td>
</tr>
<tr id="S4.T4.2.2.7.5" class="ltx_tr">
<th id="S4.T4.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.2.2.7.5.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></th>
<td id="S4.T4.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.7.5.2.1" class="ltx_text" style="font-size:90%;">YOLOv8s</span></td>
<td id="S4.T4.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.2.7.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.254</span></td>
<td id="S4.T4.2.2.7.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.7.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.087</span></td>
</tr>
<tr id="S4.T4.2.2.8.6" class="ltx_tr">
<th id="S4.T4.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T4.2.2.8.6.1.1" class="ltx_text" style="font-size:90%;">R + S</span></th>
<td id="S4.T4.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.2.2.8.6.2.1" class="ltx_text" style="font-size:90%;">YOLOv8s</span></td>
<td id="S4.T4.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.2.2.8.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.255</span></td>
<td id="S4.T4.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.8.6.4.1" class="ltx_text" style="font-size:90%;">0.082</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T4.3" class="ltx_p ltx_figure_panel">â€œRealâ€ denotes the detection performance achieved through training with the subset of Okutama-Action data collected from video 1.1.1. â€œSyntheticâ€ represents the performance achieved through training with synthetic data generated by extended K-Planes NeRF. â€œR+Sâ€ indicates the detection accuracy attained through training with a combined dataset of both â€œrealâ€ and â€œsyntheticâ€. Compared to YOLOv8n trained with â€Realâ€ dataset, we improve the <math id="S4.T4.3.m1.1" class="ltx_Math" alttext="\text{mAP}_{50}" display="inline"><semantics id="S4.T4.3.m1.1a"><msub id="S4.T4.3.m1.1.1" xref="S4.T4.3.m1.1.1.cmml"><mtext id="S4.T4.3.m1.1.1.2" xref="S4.T4.3.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T4.3.m1.1.1.3" xref="S4.T4.3.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.3.m1.1b"><apply id="S4.T4.3.m1.1.1.cmml" xref="S4.T4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.m1.1.1.1.cmml" xref="S4.T4.3.m1.1.1">subscript</csymbol><ci id="S4.T4.3.m1.1.1.2a.cmml" xref="S4.T4.3.m1.1.1.2"><mtext id="S4.T4.3.m1.1.1.2.cmml" xref="S4.T4.3.m1.1.1.2">mAP</mtext></ci><cn type="integer" id="S4.T4.3.m1.1.1.3.cmml" xref="S4.T4.3.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.m1.1c">\text{mAP}_{50}</annotation></semantics></math> metric by 12.4 %.</p>
</div>
</div>
</figure>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">The synthetic dynamic data is assessed in a similar manner to archangel by training on a subset of the Okutama-Action dataset and testing on equivalent test sets. Three detection models trained with different dataset types, â€œRealâ€, â€œSyntheticâ€ and â€œReal + Syntheticâ€, are compared. The test set for each model is the subset of test videos provided by Okutama that include drone pilot 1 footage during the morning, videos 1.1.8 and 1.1.9.</p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">Table <a href="#S4.T4" title="TABLE IV â€£ IV-C2 Dynamic NeRF â€£ IV-C Evaluation â€£ IV Implementation and Performance â€£ UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> showcases the benefits we see with synthetic NeRF data when training YOLOv8 on a subset of Okutama-Action. â€œSyntheticâ€ and â€œrealâ€ trained models have equivalent performance, indicating that the â€œsyntheticâ€ data rendered by our k-planes algorithm is effectively equivalent to the â€œrealâ€ data from Okutama for training YOLOv8n/s. For YOLOv8n, the â€œR+Sâ€ model achieves a relative 12.4% improvement in mAP50 over â€œrealâ€ alone. However, mAP50:95 does not improve significantly. This may be due to errors in extracting bounding boxes from the NeRF model compared to manually labeled data. We also do not see any improvements for YOLOv8s between any version of the models. This is likely due to general variation in training for YOLOv8 as well as the small extrapolations from training data for the dynamic â€œsyntheticâ€ data. The Archangel â€œsyntheticâ€ data has larger variations in pose relative to its training data, so larger improvements are expected compared to the â€œsyntheticâ€ data from Okutama-Action. The â€œsyntheticâ€ data from Okutama-Action is very similar to that of the â€œrealâ€ training data so as to reduce errors and artifacts from extrapolating too far from the training data. This is likely causing the larger YOLOv8s model, which has approximately three times as many parameters as that of YOLOv8n, to underfit during training. From the perspective of UAV hardware and power limitations, the nano class models are more applicable and thus performance gains there are desirable over the other larger classes of models.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions, Limitations, and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have shown that NeRF algorithms are a valid method for bridging the domain gap, given a limited amount of real-world data, to supplement training for modern data-hungry algorithms, such as YOLOv8. We have also developed the model optimization pipeline where any SOTA detection model can be further optimized by unseen salient scene attributes captured with novel camera poses and self-generated bounding box annotation. Given the current state of the art in NeRF, ideal UAV scenarios include circular paths with fixed camera angles, as shown by the performance improvements on the Archangel dataset as compared to Okutama-Action. The challenges of UAV-based footage as well as dynamic scenes provide a limitation to the performance benefits with YOLOv8 on Okutama-Action; however, we do show that an increased PSNR can be achieved when compared to the stock K-Planes algorithm and a modest increase in mAP50 can be achieved for certain YOLOv8 models. Future work may focus on fully exploring the extent of extrapolating â€œsyntheticâ€ data from dynamic datasets. The benefits shown in Archangel highlight the potential for improvements on Okutama-Action, while the increased PSNR from our extended K-Planes gives direction for future research in minimizing camera pose noise and better reconstructing dynamic UAV scenes, leading to enhanced recognition of an occurrence of certain activities or events of interest.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements</span>
This work was supported in part by the Office of the Under Secretary of Defense for Research and Engineering (OUSD R&amp;E) through Army Cooperative Agreement W911NF2120076, and ARO Grants W911NF2110026, W911NF2310046, and W911NF2310352.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Erdelj, Milan, and Enrico Natalizio. â€œUAV-assisted disaster management: Applications and open issues.â€ In 2016 international conference on computing, networking and communications (ICNC), pp. 1-5. IEEE, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Chriki, Amira, Haifa Touati, Hichem Snoussi, and Farouk Kamoun. â€œUav-based surveillance system: an anomaly detection approach.â€ In 2020 IEEE Symposium on computers and communications (ISCC), pp. 1-6. IEEE, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Khan, Muhammad Arsalan, Wim Ectors, Tom Bellemans, Davy Janssens, and Geert Wets. â€œUnmanned aerial vehicle-based traffic analysis: A case study for shockwave identification and flow parameters estimation at signalized intersections.â€ Remote Sensing 10, no. 3 (2018): 458.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Yaqot, Mohammed, and Brenno C. Menezes. â€œUnmanned aerial vehicle (UAV) in precision agriculture: business information technology towards farming as a service.â€ In 2021 1st international conference on emerging smart technologies and applications (eSmarTA), pp. 1-7. IEEE, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> de Melo, Celso M., Antonio Torralba, Leonidas Guibas, James DiCarlo, Rama Chellappa, and Jessica Hodgins. â€Next-generation deep learning based on simulators and synthetic data.â€ Trends in cognitive sciences (2022).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Jocher, G., Chaurasia, A., and Qiu, J. (2023). YOLO by Ultralytics (Version 8.0.0) [Computer software]. https://github.com/ultralytics/ultralytics

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Plowman, Justin. 3D Game Design with Unreal Engine 4 and Blender. Packt Publishing Ltd, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> Shah, Shital, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. â€œAirsim: High-fidelity visual and physical simulation for autonomous vehicles.â€ In Field and Service Robotics: Results of the 11th International Conference, pp. 621-635. Springer International Publishing, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Alvey, Brendan, Derek T. Anderson, Andrew Buck, Matthew Deardorff, Grant Scott, and James M. Keller. â€œSimulated photorealistic deep learning framework and workflows to accelerate computer vision and unmanned aerial vehicle research.â€ In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3889-3898. 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. â€œGenerative adversarial networks.â€ Communications of the ACM 63, no. 11 (2020): 139-144.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. â€œHigh-resolution image synthesis with latent diffusion models.â€ In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695. 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> Kajiya, James T., and Brian P. Von Herzen. â€œRay tracing volume densities.â€ ACM SIGGRAPH computer graphics 18, no. 3 (1984): 165-174.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> Tancik, Matthew, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen et al. â€œNerfstudio: A modular framework for neural radiance field development.â€ In ACM SIGGRAPH 2023 Conference Proceedings, pp. 1-12. 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. â€œNerf: Representing scenes as neural radiance fields for view synthesis.â€ Communications of the ACM 65, no. 1 (2021): 99-106.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> Zhang, Kai, Gernot Riegler, Noah Snavely, and Vladlen Koltun. â€œNerf++: Analyzing and improving neural radiance fields.â€ arXiv preprint arXiv:2010.07492 (2020).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> MÃ¼ller, Thomas, Alex Evans, Christoph Schied, and Alexander Keller. â€œInstant neural graphics primitives with a multiresolution hash encoding.â€ ACM Transactions on Graphics (ToG) 41, no. 4 (2022): 1-15.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> Barron, Jonathan T., Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. â€œMip-nerf 360: Unbounded anti-aliased neural radiance fields.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5470-5479. 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> Fridovich-Keil, Sara, Giacomo Meanti, Frederik RahbÃ¦k Warburg, Benjamin Recht, and Angjoo Kanazawa. â€œK-planes: Explicit radiance fields in space, time, and appearance.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12479-12488. 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> Cao, Ang, and Justin Johnson. â€œHexplane: A fast representation for dynamic scenes.â€ Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. â€œNerf in the wild: Neural radiance fields for unconstrained photo collections.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7210-7219. 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> Park, Keunhong, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. â€œNerfies: Deformable neural radiance fields.â€ In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865-5874. 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> Li, Zhengqi, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. â€œDynibar: Neural dynamic image-based rendering.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4273-4284. 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> Fang, Jiemin, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias NieÃŸner, and Qi Tian. â€œFast dynamic radiance fields with time-aware neural voxels.â€ In SIGGRAPH Asia 2022 Conference Papers, pp. 1-9. 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> Pumarola, Albert, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. â€œD-nerf: Neural radiance fields for dynamic scenes.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10318-10327. 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> Li, Tianye, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt et al. â€œNeural 3d video synthesis from multi-view video.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5521-5531. 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> Turki, Haithem, Deva Ramanan, and Mahadev Satyanarayanan. â€œMega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs.â€ In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12922-12931. 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> Jia, Zhihao, Bing Wang, and Changhao Chen. â€œDrone-NeRF: Efficient NeRF Based 3D Scene Reconstruction for Large-Scale Drone Survey.â€ arXiv preprint arXiv:2308.15733 (2023).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> Shen, Yi-Ting, Yaesop Lee, Heesung Kwon, Damon M. Conover, Shuvra S. Bhattacharyya, Nikolas Vale, Joshua D. Gray, G. Jeremy Leong, Kenneth Evensen, and Frank Skirlo. â€œArchangel: A hybrid UAV-based human detection benchmark with position and pose metadata.â€ IEEE Access (2023).

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> Barekatain, Mohammadamin, Miquel MartÃ­, Hsueh-Fu Shih, Samuel Murray, Kotaro Nakayama, Yutaka Matsuo, and Helmut Prendinger. â€œOkutama-action: An aerial view video dataset for concurrent human action detection.â€ In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 28-35. 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. â€œMicrosoft coco: Common objects in context.â€ In Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740-755. Springer International Publishing, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> Lindenberger, Philipp, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. â€œPixel-perfect structure-from-motion with featuremetric refinement.â€ In Proceedings of the IEEE/CVF international conference on computer vision, pp. 5987-5997. 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.16254" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.16255" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.16255">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.16255" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.16256" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 22:43:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
