<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.04259] Unity Perception: Generate Synthetic Data for Computer Vision</title><meta property="og:description" content="We introduce the Unity Perception package which aims to simplify and accelerate the process of generating synthetic datasets for computer vision tasks by offering an easy-to-use and highly customizable toolset. This op…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Unity Perception: Generate Synthetic Data for Computer Vision">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Unity Perception: Generate Synthetic Data for Computer Vision">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.04259">

<!--Generated on Tue Mar 19 12:41:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Unity Perception: Generate Synthetic Data for Computer Vision</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, 
<br class="ltx_break">You-Cyuan Jhang, Mohsen Kamalzadeh, Bowen Li, Steven Leal, Pete Parisi, Cesar Romero, 
<br class="ltx_break">Wesley Smith, Alex Thaman, Samuel Warren, Nupur Yadav 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Unity Technologies
<br class="ltx_break"></span><span id="id2.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">computer-vision@unity.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">We introduce the Unity Perception package which aims to simplify and accelerate the process of generating synthetic datasets for computer vision tasks by offering an easy-to-use and highly customizable toolset. This open-source package extends the Unity Editor and engine components to generate perfectly annotated examples for several common computer vision tasks. Additionally, it offers an extensible Randomization framework that lets the user quickly construct and configure randomized simulation parameters in order to introduce variation into the generated datasets. We provide an overview of the provided tools and how they work, and demonstrate the value of the generated synthetic datasets by training a 2D object detection model. The model trained with mostly synthetic data outperforms the model trained using only real data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past decade, computer vision has evolved from heuristics to a set of deep-learning-based models that learn from large amounts of labeled data. These models have made significant progress in what are now considered more basic tasks like image classification, and the computer vision community has shown interest in more complex tasks such as object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, semantic segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and instance segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. These more complex tasks require increasingly complex models, datasets, and labels. However, these large and complex datasets come with challenges related to cost, bias, and privacy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As the focus shifts to more complex tasks, the cost of annotating each example increases from labeling frames to labeling objects and even pixels in the image. Furthermore, as human annotators label these examples, their workflows and tools become more complex as well. This, in turn, creates a need to review or audit annotations, leading to additional costs for each labeled example.
Furthermore, as tasks become complex and the range of possible variations to account for expands, the requirements of data collection become more challenging. Some scenarios may rarely occur in the real world, yet correctly handling these events is crucial. For example, misplaced obstacles on the road need to be detected by autonomous vehicles. Such rare events further increase the cost of collecting data and not accounting for them can affect the model’s performance, leading to uncertainty about how the model will perform on these rare events.
Privacy concerns surrounding machine learning models have also become increasingly important, further complicating data collection. Regulations such as the EU General Data Protection Regulation (GDPR)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and the California Consumer Privacy Act (CCPA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> enhance privacy rights and restrict the use of consumer data to train machine learning models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">By creating virtual environments, we can generate training examples in a controllable and customizable manner and avoid the challenges of collecting and annotating data in the real world. A rendering engine requires compute-time rather than human-time to generate examples. It has perfect information about the scenes it renders, making it possible to bypass the time and cost of human annotations and reviews. A rendering engine also makes it possible to generate rare examples, allowing control on the distribution of the training dataset. Moreover, such a process does not rely on any individual’s private data by design.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">An additional benefit of using synthetic environments to generate labeled data is that the environments are reusable. Once we create the environment with all its visual assets, we can introduce variation into the environment at simulation time by changing randomization parameters on the fly. This allows for faster iterations on the generated datasets and the computer vision model. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates how manually annotated datasets differ from synthetically generated ones in terms of workflows and iteration. By eliminating the need to review and audit datasets and avoiding time-consuming data collection and clean-up steps, synthetic data open the door to more rapid iteration.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2107.04259/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="418" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Comparison of the dataset iteration process with real-world and synthetic datasets. <span id="S1.F1.5.2.1" class="ltx_text ltx_font_bold">Top:</span> At a minimum, a real-world dataset requires the collect, cleanup, annotate, and finally review, and audit steps. These steps involve costly and time-consuming human effort. <span id="S1.F1.5.2.2" class="ltx_text ltx_font_bold">Bottom:</span> A synthetic dataset requires an environment built with 3D assets, setting or altering randomization parameters, and running the environment to generate new data. The datasets come with accurate annotations and are automatically validated, eliminating most of the time-consuming steps.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, we introduce the Unity Perception package<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/Unity-Technologies/com.unity.perception" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Unity-Technologies/com.unity.perception</a></span></span></span>, which offers a variety of convenient and customizable tools that help speed up and simplify the process of generating labeled synthetic datasets for computer vision problems. This open-source package extends the Unity editor and Unity’s world-class rendering engine<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with the capability to generate perfectly annotated examples for several common computer vision tasks (Section <a href="#S3.SS1" title="3.1 Ground truth generation ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). As described in Section <a href="#S3.SS2" title="3.2 Randomization tools ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the package comes with an extensible randomization framework for setting up and configuring randomized parameters that can introduce variation into the generated datasets (Section <a href="#S3.SS2" title="3.2 Randomization tools ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We also provide a companion python package to help consume and parse the generated datasets (Section <a href="#S3.SS4" title="3.4 Dataset Insights ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The computer vision community has invested great resources to create datasets such as PASCAL VOC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, NYU-Depth V2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, MS COCO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and SUN RGB-D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
While these have contributed to a significant boost in research on complex tasks such as semantic segmentation of indoor scenes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, they cannot cover all the scenarios researchers are interested in, or provide a path for others to create new datasets. Some researchers have identified the potential for synthetic data and have achieved great results in specific tasks and domains. Examples of this include object detection of groceries<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, controlling robotic arms to move blocks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and fine-grained manipulation of a Rubik’s cube<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>; however, these simulation environments are often not publicly available, which makes the research difficult to reproduce and impossible to extend.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Others have identified this challenge and created simulation environments such as SYNTHIA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, CARLA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, AI2-THOR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Habitat<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and iGibson<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. These simulators increase the number of examples researchers can use to train their computer vision models and facilitate reproducibility; however, they do not provide a general-purpose platform as they couple the simulator with specific domains and tasks. NVIDIA Isaac Sim<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> takes these efforts a step further and provides a platform built to enable a wide range of robotics simulations, instead of a specific domain or task. BlenderProc<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and NVISII<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> share similar goals of providing an API to generate training examples for a few computer vision tasks, but don’t focus on physics or simulation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The Unity Perception package builds on top of the Unity Editor to be a building block of such robotics and computer vision research projects. The first project to use this package was SynthDet, which is and end-to-end solution for detecting grocery objects using synthetic datasets and analyzing model performance under various combinations of real and synthetic data. This project will be discussed in section <a href="#S4" title="4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">NVIDIA Isaac Sim</th>
<th id="S2.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">NVISII</th>
<th id="S2.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">BlenderProc</th>
<th id="S2.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Unity Perception</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<th id="S2.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Semantic segmentation</th>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S2.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S2.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S2.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">yes</td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<th id="S2.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Instance segmentation</th>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.3.2.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.3.2.4" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.3.2.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr">
<th id="S2.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2D bounding-box</th>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.4.3.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.4.3.4" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.4.3.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T1.2.5.4" class="ltx_tr">
<th id="S2.T1.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3D bounding-box</th>
<td id="S2.T1.2.5.4.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.5.4.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.5.4.4" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.5.4.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T1.2.6.5" class="ltx_tr">
<th id="S2.T1.2.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Depth</th>
<td id="S2.T1.2.6.5.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.6.5.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.6.5.4" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.6.5.5" class="ltx_td ltx_align_center">no</td>
</tr>
<tr id="S2.T1.2.7.6" class="ltx_tr">
<th id="S2.T1.2.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Keypoints</th>
<td id="S2.T1.2.7.6.2" class="ltx_td ltx_align_center">no</td>
<td id="S2.T1.2.7.6.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.7.6.4" class="ltx_td ltx_align_center">no</td>
<td id="S2.T1.2.7.6.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T1.2.8.7" class="ltx_tr">
<th id="S2.T1.2.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Normals</th>
<td id="S2.T1.2.8.7.2" class="ltx_td ltx_align_center">–</td>
<td id="S2.T1.2.8.7.3" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.8.7.4" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T1.2.8.7.5" class="ltx_td ltx_align_center">no</td>
</tr>
<tr id="S2.T1.2.9.8" class="ltx_tr">
<th id="S2.T1.2.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Optical flow</th>
<td id="S2.T1.2.9.8.2" class="ltx_td ltx_align_center ltx_border_b">–</td>
<td id="S2.T1.2.9.8.3" class="ltx_td ltx_align_center ltx_border_b">yes</td>
<td id="S2.T1.2.9.8.4" class="ltx_td ltx_align_center ltx_border_b">no</td>
<td id="S2.T1.2.9.8.5" class="ltx_td ltx_align_center ltx_border_b">no</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">A comparison of different platforms and common computer vision tasks they support.</span></figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.2.1.1" class="ltx_tr">
<th id="S2.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S2.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">NVIDIA Isaac</th>
<th id="S2.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">NVISII</th>
<th id="S2.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">BlenderProc</th>
<th id="S2.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Unity Perception</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.2.2.1" class="ltx_tr">
<th id="S2.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Content</th>
<td id="S2.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S2.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S2.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S2.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">asset store</td>
</tr>
<tr id="S2.T2.2.3.2" class="ltx_tr">
<th id="S2.T2.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Domain Randomization</th>
<td id="S2.T2.2.3.2.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T2.2.3.2.3" class="ltx_td ltx_align_center">–</td>
<td id="S2.T2.2.3.2.4" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.3.2.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T2.2.4.3" class="ltx_tr">
<th id="S2.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Behavior simulation</th>
<td id="S2.T2.2.4.3.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T2.2.4.3.3" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.4.3.4" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.4.3.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T2.2.5.4" class="ltx_tr">
<th id="S2.T2.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Physics</th>
<td id="S2.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S2.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t">no</td>
<td id="S2.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S2.T2.2.5.4.5" class="ltx_td ltx_align_center ltx_border_t">yes</td>
</tr>
<tr id="S2.T2.2.6.5" class="ltx_tr">
<th id="S2.T2.2.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Robotics</th>
<td id="S2.T2.2.6.5.2" class="ltx_td ltx_align_center">yes</td>
<td id="S2.T2.2.6.5.3" class="ltx_td ltx_align_center">–</td>
<td id="S2.T2.2.6.5.4" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.6.5.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T2.2.7.6" class="ltx_tr">
<th id="S2.T2.2.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Managed cloud scaling</th>
<td id="S2.T2.2.7.6.2" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.7.6.3" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.7.6.4" class="ltx_td ltx_align_center">no</td>
<td id="S2.T2.2.7.6.5" class="ltx_td ltx_align_center">yes</td>
</tr>
<tr id="S2.T2.2.8.7" class="ltx_tr">
<th id="S2.T2.2.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Speed</th>
<td id="S2.T2.2.8.7.2" class="ltx_td ltx_align_center">++</td>
<td id="S2.T2.2.8.7.3" class="ltx_td ltx_align_center">++</td>
<td id="S2.T2.2.8.7.4" class="ltx_td ltx_align_center">+</td>
<td id="S2.T2.2.8.7.5" class="ltx_td ltx_align_center">++</td>
</tr>
<tr id="S2.T2.2.9.8" class="ltx_tr">
<th id="S2.T2.2.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Rendering</th>
<td id="S2.T2.2.9.8.2" class="ltx_td ltx_align_center">RT,R</td>
<td id="S2.T2.2.9.8.3" class="ltx_td ltx_align_center">RT</td>
<td id="S2.T2.2.9.8.4" class="ltx_td ltx_align_center">RT</td>
<td id="S2.T2.2.9.8.5" class="ltx_td ltx_align_center">RT(Windows only),R</td>
</tr>
<tr id="S2.T2.2.10.9" class="ltx_tr">
<th id="S2.T2.2.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Language</th>
<td id="S2.T2.2.10.9.2" class="ltx_td ltx_align_center">C++, python</td>
<td id="S2.T2.2.10.9.3" class="ltx_td ltx_align_center">python</td>
<td id="S2.T2.2.10.9.4" class="ltx_td ltx_align_center">python</td>
<td id="S2.T2.2.10.9.5" class="ltx_td ltx_align_center">C#</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.4.2" class="ltx_text" style="font-size:90%;">Summary of features provided by Unity Perception and other comparable platforms. ’–’ means unclear or partial support. RT means ray tracing and R means rasterization.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The Unity Perception Package</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The Unity Perception package extends the Unity Editor with tools for generating synthetic datasets that include ground truth annotations. In addition, the package supports domain randomization for introducing variety into the generated datasets. Out of the box, the package supports various computer vision tasks including 2D/3D object detection, semantic segmentation, instance segmentation, and keypoints (nodes and edges attached to 3D objects, useful for tasks such as human-pose estimation). Figure <a href="#S3.F2" title="Figure 2 ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts sample outputs for these tasks. In addition, users can extended the package using C
<span id="S3.p1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.2pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.1pt,0.1pt) scale(0.983997310546218,0.983997310546218) ;">
<span id="S3.p1.1.1.1" class="ltx_p"><span id="S3.p1.1.1.1.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:0.0pt;">#</span></span>
</span></span> to support new ground truth labeling methods and randomization techniques. Additionally, along with the capability to generate datasets locally, the Perception package has easy-to-use built-in support for running dataset generation jobs in the cloud using Unity Simulation, making it possible to generate millions of annotated images relatively quickly, and without the need for powerful local computing resources.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<table id="S3.F2.5" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F2.2.2" class="ltx_tr">
<td id="S3.F2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2107.04259/assets/figures/2D_BB_2.png" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="150" height="84" alt="Refer to caption"></td>
<td id="S3.F2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2107.04259/assets/figures/3D_BB_2.png" id="S3.F2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="150" height="84" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.5.6.1" class="ltx_tr">
<td id="S3.F2.5.6.1.1" class="ltx_td ltx_align_center">
(a) 2D Bounding Boxes</td>
<td id="S3.F2.5.6.1.2" class="ltx_td ltx_align_center">(b) 3D Bounding Boxes</td>
</tr>
<tr id="S3.F2.4.4" class="ltx_tr">
<td id="S3.F2.3.3.1" class="ltx_td ltx_align_center"><img src="/html/2107.04259/assets/figures/Instance_151.png" id="S3.F2.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="150" height="84" alt="Refer to caption"></td>
<td id="S3.F2.4.4.2" class="ltx_td ltx_align_center"><img src="/html/2107.04259/assets/figures/segmentation_151.png" id="S3.F2.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="150" height="84" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.5.7.2" class="ltx_tr">
<td id="S3.F2.5.7.2.1" class="ltx_td ltx_align_center">
(c) Instance Segmentation</td>
<td id="S3.F2.5.7.2.2" class="ltx_td ltx_align_center">(d) Semantic Segmentation</td>
</tr>
<tr id="S3.F2.5.5" class="ltx_tr">
<td id="S3.F2.5.5.1" class="ltx_td ltx_align_center" colspan="2"><img src="/html/2107.04259/assets/figures/Pose.png" id="S3.F2.5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="93" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.5.8.3" class="ltx_tr">
<td id="S3.F2.5.8.3.1" class="ltx_td ltx_align_center" colspan="2">(e) Keypoints (points attached to objects)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.7.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.8.2" class="ltx_text" style="font-size:90%;">Provided Labelers in the Perception package</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Ground truth generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The Perception package includes a set of Labelers which capture ground truth information along with each captured frame. The built-in Labelers support a variety of common computer vision tasks (see Table <a href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The package also includes extensible components for building new Labelers to support additional tasks using C
<span id="S3.SS1.p1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.2pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.1pt,0.1pt) scale(0.983997310546218,0.983997310546218) ;">
<span id="S3.SS1.p1.1.1.1" class="ltx_p"><span id="S3.SS1.p1.1.1.1.1" class="ltx_text ltx_font_bold" style="position:relative; bottom:0.0pt;">#</span></span>
</span></span>. Labelers derive ground truth data from labels specified on the 3D assets present in the scene. The user manually annotates the project’s library of assets with semantic labels such as “chair” and “motorcycle” to provide the Labelers with the data required to capture datasets that match the target task. The Labelers are configured with a mapping from the human readable semantic labels to the numeric canonical class ids used to train the target model. This mapping allows for labeled assets to be used across many dataset generators and labeling strategies. During simulation, these Labelers compute ground truth based on the state of the 3D scene and the rendering results, through a custom rendering pipeline. Appendix <a href="#A1" title="Appendix A Perception package diagrams ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> provides a high-level component diagram for the ground-truth generation system.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel">
<p id="S3.F3.sf1.1" class="ltx_p"><span id="S3.F3.sf1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:5.0pt;"><img src="/html/2107.04259/assets/x2.png" id="S3.F3.sf1.1.1.g1" class="ltx_graphics ltx_img_square" width="415" height="365" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.4.2" class="ltx_text" style="font-size:90%;">Captures</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel">
<p id="S3.F3.sf2.1" class="ltx_p"><span id="S3.F3.sf2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:5.0pt;"><img src="/html/2107.04259/assets/x3.png" id="S3.F3.sf2.1.1.g1" class="ltx_graphics ltx_img_square" width="415" height="365" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.4.2" class="ltx_text" style="font-size:90%;">Metrics</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Synthetic dataset entity relation diagram for captures and metrics. (a) The captures.json files contain one to many relationships between sensor outputs and Labeler outputs (annotations). Each capture and annotation are assigned a unique identifier that allows users to look up extra data in the metrics.json files. The annotation_definition.json file contains references to decode annotation records programmatically. Each mapping is assigned a unique identifier to distinguish between different annotation types. (b) The metrics.json files contain extra data to describe a particular capture or annotation. These extra data can be used to compute dataset statistics.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Randomization tools</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The Unity Perception package provides a randomization framework that simplifies introducing variation into synthetic environments, leading to varied data. An entity called the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Scenario</span> controls and coordinates all randomizations in the scene. This involves triggering a set of <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">Randomizers</span> in a predetermined order. Each Scenario’s execution is called an <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">Iteration</span> and each Iteration can run for a user-defined number of frames. Users can configure Randomizers to act at various timestamps during each Iteration, including at the start and end or per each frame. The Randomizers expose the environment parameters for randomization and utilize samplers to pick random values for these parameters. The combination of the Scenario and its Randomizers allow the user to define elaborate and deterministic schedules for randomizations to occur throughout a simulation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Most pieces of the randomization framework are customizable and extensible. Users can create new Randomizers by extending the base Randomizer class to control various parameters in their environments. The Perception package comes with several sample Randomizers to assist with common randomization tasks (e.g. random object placement, position, rotation, texture, and hue), and examples on extending and customizing Randomizers. Besides, users can extend the Scenario class to include custom scheduling behavior. Users can also control the sampling strategy for each individual randomized parameter by either selecting from a set of provided distributions (including normal and uniform), or providing custom distribution curves by graphically drawing them.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The randomization framework has built-in support for distributed data generation. Users can scale randomized simulations to the cloud by launching a Unity Simulation run directly from Unity Editor. The toolset uses deterministic random sampling and ordering of randomizations to ensure that data generated during distributed cloud execution is reproducible in Unity Editor for debugging purposes.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Appendix <a href="#A1" title="Appendix A Perception package diagrams ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> includes high-level component and sequence diagrams for Perception’s randomization framework.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Schema</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The synthetic datasets generated using the Unity Perception package typically include two types of data: simulated sensor outputs and Labeler outputs. Inspired by<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we used JSON files with lightweight and extensible schema design to connect sensor and Labeler output files. This lightweight design allows us to programmatically read data while maintaining flexibility to support various computer vision tasks. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Ground truth generation ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the dataset schema generated by the Unity Perception package. The package documentation<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/Unity-Technologies/com.unity.perception/blob/master/com.unity.perception/Documentation~/Schema/Synthetic_Dataset_Schema.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Unity-Technologies/com.unity.perception/blob/master/com.unity.perception/Documentation~/Schema/Synthetic_Dataset_Schema.md</a></span></span></span> includes the complete schema design.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2107.04259/assets/figures/total_object_counts.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="209" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2107.04259/assets/figures/object_counts_per_capture.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2107.04259/assets/figures/object_relative_size.png" id="S3.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="299" height="211" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.4.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.2.1" class="ltx_text" style="font-size:90%;">Dataset statistics from the SynthDet example project (described in Section <a href="#S4" title="4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), providing insight into the generated dataset. (a) Total object counts aggregated per label in the dataset. The counts are nearly identical for each label since we sample objects uniformly in the generation process. The chart here depicts only the first ten labels. (b) Distribution of the number of objects per capture (label agnostic). (c) Distribution of sizes of objects relative to the 2D images. Users can control the size of objects through Randomizers. Here, <math id="S3.F4.2.1.m1.1" class="ltx_Math" alttext="\text{relative size}=\sqrt{\frac{\text{object occupied pixels}}{\text{total image pixels}}}" display="inline"><semantics id="S3.F4.2.1.m1.1b"><mrow id="S3.F4.2.1.m1.1.1" xref="S3.F4.2.1.m1.1.1.cmml"><mtext id="S3.F4.2.1.m1.1.1.2" xref="S3.F4.2.1.m1.1.1.2a.cmml">relative size</mtext><mo id="S3.F4.2.1.m1.1.1.1" xref="S3.F4.2.1.m1.1.1.1.cmml">=</mo><msqrt id="S3.F4.2.1.m1.1.1.3" xref="S3.F4.2.1.m1.1.1.3.cmml"><mfrac id="S3.F4.2.1.m1.1.1.3.2" xref="S3.F4.2.1.m1.1.1.3.2.cmml"><mtext id="S3.F4.2.1.m1.1.1.3.2.2" xref="S3.F4.2.1.m1.1.1.3.2.2a.cmml">object occupied pixels</mtext><mtext id="S3.F4.2.1.m1.1.1.3.2.3" xref="S3.F4.2.1.m1.1.1.3.2.3a.cmml">total image pixels</mtext></mfrac></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.2.1.m1.1c"><apply id="S3.F4.2.1.m1.1.1.cmml" xref="S3.F4.2.1.m1.1.1"><eq id="S3.F4.2.1.m1.1.1.1.cmml" xref="S3.F4.2.1.m1.1.1.1"></eq><ci id="S3.F4.2.1.m1.1.1.2a.cmml" xref="S3.F4.2.1.m1.1.1.2"><mtext id="S3.F4.2.1.m1.1.1.2.cmml" xref="S3.F4.2.1.m1.1.1.2">relative size</mtext></ci><apply id="S3.F4.2.1.m1.1.1.3.cmml" xref="S3.F4.2.1.m1.1.1.3"><root id="S3.F4.2.1.m1.1.1.3a.cmml" xref="S3.F4.2.1.m1.1.1.3"></root><apply id="S3.F4.2.1.m1.1.1.3.2.cmml" xref="S3.F4.2.1.m1.1.1.3.2"><divide id="S3.F4.2.1.m1.1.1.3.2.1.cmml" xref="S3.F4.2.1.m1.1.1.3.2"></divide><ci id="S3.F4.2.1.m1.1.1.3.2.2a.cmml" xref="S3.F4.2.1.m1.1.1.3.2.2"><mtext mathsize="70%" id="S3.F4.2.1.m1.1.1.3.2.2.cmml" xref="S3.F4.2.1.m1.1.1.3.2.2">object occupied pixels</mtext></ci><ci id="S3.F4.2.1.m1.1.1.3.2.3a.cmml" xref="S3.F4.2.1.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.F4.2.1.m1.1.1.3.2.3.cmml" xref="S3.F4.2.1.m1.1.1.3.2.3">total image pixels</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.1.m1.1d">\text{relative size}=\sqrt{\frac{\text{object occupied pixels}}{\text{total image pixels}}}</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset Insights</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In addition to the perception package, we provide an accompanying python package named DatasetInsights<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/Unity-Technologies/datasetinsights" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Unity-Technologies/datasetinsights</a></span></span></span> to assist the user in working with datasets generated using the Perception package. This includes generating and visualizing dataset statistics and performing model training tasks. To support both use cases, we constructed dataset IO modules that allow users to parse, load, and transform datasets in memory.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The included statistics cover elements such as total and per frame object count, visible pixels per object, and frame by frame visualization of the captured ground truth. These, along with support for extending the toolset to include new statistics, make it possible to understand and verify the generated datasets before using them for model training. For instance, statistics can help users to decide whether a larger dataset or one with different domain randomizations applied is needed for the specific problem they are trying to solve. They also serve as a debugging tool to spot issues in the dataset. Statistics from the SynthDet project (Section <a href="#S4" title="4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) are shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 Schema ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Example Project - SynthDet</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To prove the viability of Unity’s Perception package, we built SynthDet. This project involves generating synthetic training data for a set of 63 common grocery objects, training Faster R-CNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> 2D object detection models using various combinations of synthetic and real data, and comparing and analyzing the performance of said models. This approach was inspired by previous research<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and entails generating large quantities of highly randomized images in which the grocery products are placed in-between two layers of distracting objects. An example frame is shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 Randomizations ‣ 4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To generate synthetic datasets, we built 3D models using 3D scans of the actual grocery objects, and imported the models into Unity Editor. In addition, we created a real-world dataset using the same products by taking numerous pictures of them in various formations and locations. In the results section, we will discuss our training approach and results using these datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Randomizations</h3>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.04259/assets/figures/synthdet_example_frame.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">RGB frame output</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.04259/assets/figures/synthdet_side_view.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Side view in Unity Scene</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">(a) An example frame generated for the SynthDet project. The grocery products are displayed against a backdrop of objects with randomized shape, rotation, hue, and texture, and behind a foreground of occluding objects. (b) The Unity Scene for the same frame as seen from its right side. The camera is placed to the left of this view.</span></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To achieve a randomized environment with the 3D models we used a set of Randomizers, with each undertaking a specific randomization task. In summary, the following aspects were randomized:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Grocery (foreground) objects:</span> A randomly selected subset of the 63 grocery objects is instantiated and randomly positioned in front of the camera per frame. The density of these objects is also randomized such that in some frames the objects are placed much closer to each other and there are significantly more of them compared to other frames. Furthermore, the scales of these objects are randomized on each frame, and the whole group of objects is assigned a unified random rotation.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Background objects:</span> A group of primitive 3D objects are randomly placed close to each other, creating a “wall” of intersecting shapes behind the grocery objects. These objects also have a random texture, chosen from a set of 530 varied images of fruits and vegetables<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> , applied to them on each frame. Additionally, the rotations and color hues of these objects are randomized per frame.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Occluding objects:</span> A set of foreground occluding objects are placed randomly at a distance closer to the camera. These are the same primitive objects used for the background, but placed farther apart. The same texture, hue, and rotation randomizations applied to the background are also applied to these occluders.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Lighting:</span> A combination of four directional lights illuminate the scene. All of the lights have randomized intensity and color, and one has randomized rotation as well. Three of the lights affects all objects, while one significantly brighter light only affects the background objects. This light is switched on with a small probability, resulting in the background becoming overexposed in some frames, leading to more visual separation between it and the grocery objects. This reflects the real test dataset in which some frames contain much less distraction that others.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Camera post processing:</span> The contrast and saturation of the output are randomized in small percentages. Additionally, in some frames, a small amount of blur is applied to the camera to simulate real test images.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Using the approach discussed above, we generated a randomized synthetic dataset containing 400,000 images and 2D bounding box annotations. We also collected and annotated a real-world dataset, named UnityGroceries-Real<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/Unity-Technologies/SynthDet/blob/master/docs/UnityGroceriesReal.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Unity-Technologies/SynthDet/blob/master/docs/UnityGroceriesReal.md</a></span></span></span>, which contains 1267 images of the 63 target grocery items. This dataset is primarily used to assess the model’s performance trained on different combinations of synthetic and real-world data. The dataset was annotated using VGG Image Annotator (VIA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> with the guidelines from the PASCAL VOC dataset. We randomly shuffled and split all annotated images into a training set of 760 (60%) images, a validation set of 253 (20%) images, and a testing set of 254 (20%) images. We also built a few randomly selected subsets of size 76 (10%), 380 (50%), and 760 (100%) from the training set for model fine-tuning using limited amounts of the training data.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.5" class="ltx_tr">
<th id="S4.T3.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.5.5.6.1" class="ltx_text" style="font-size:70%;">Training Data (number of training examples)</span></th>
<th id="S4.T3.5.5.7" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.5.7.1.1" class="ltx_p"><span id="S4.T3.5.5.7.1.1.1" class="ltx_text" style="font-size:70%;">mAP(error)</span></span>
</span>
</th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1" class="ltx_p"><math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAP}" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="S4.T3.1.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.T3.1.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.1.cmml">​</mo><mtext mathsize="70%" id="S4.T3.1.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.1.m1.1.1.3a.cmml">mAP</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.1"></times><ci id="S4.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="S4.T3.1.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3">mAP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\Delta\text{mAP}</annotation></semantics></math><span id="S4.T3.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.2.2.2.1.1" class="ltx_p"><math id="S4.T3.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><msup id="S4.T3.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"><mtext mathsize="70%" id="S4.T3.2.2.2.1.1.m1.1.1.2" xref="S4.T3.2.2.2.1.1.m1.1.1.2a.cmml">mAP</mtext><mtext mathsize="70%" id="S4.T3.2.2.2.1.1.m1.1.1.3" xref="S4.T3.2.2.2.1.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><apply id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1">superscript</csymbol><ci id="S4.T3.2.2.2.1.1.m1.1.1.2a.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.2"><mtext mathsize="70%" id="S4.T3.2.2.2.1.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.T3.2.2.2.1.1.m1.1.1.3a.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.3"><mtext mathsize="49%" id="S4.T3.2.2.2.1.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">\text{mAP}^{\text{IoU50}}</annotation></semantics></math><span id="S4.T3.2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">(error)</span></span>
</span>
</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.3.3.3.1.1" class="ltx_p"><math id="S4.T3.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="S4.T3.3.3.3.1.1.m1.1a"><mrow id="S4.T3.3.3.3.1.1.m1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="S4.T3.3.3.3.1.1.m1.1.1.2" xref="S4.T3.3.3.3.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.T3.3.3.3.1.1.m1.1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.1.cmml">​</mo><msup id="S4.T3.3.3.3.1.1.m1.1.1.3" xref="S4.T3.3.3.3.1.1.m1.1.1.3.cmml"><mtext mathsize="70%" id="S4.T3.3.3.3.1.1.m1.1.1.3.2" xref="S4.T3.3.3.3.1.1.m1.1.1.3.2a.cmml">mAP</mtext><mtext mathsize="70%" id="S4.T3.3.3.3.1.1.m1.1.1.3.3" xref="S4.T3.3.3.3.1.1.m1.1.1.3.3a.cmml">IoU50</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.1.m1.1b"><apply id="S4.T3.3.3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1"><times id="S4.T3.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.1"></times><ci id="S4.T3.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.2">Δ</ci><apply id="S4.T3.3.3.3.1.1.m1.1.1.3.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.3.3.3.1.1.m1.1.1.3.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3">superscript</csymbol><ci id="S4.T3.3.3.3.1.1.m1.1.1.3.2a.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3.2"><mtext mathsize="70%" id="S4.T3.3.3.3.1.1.m1.1.1.3.2.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3.2">mAP</mtext></ci><ci id="S4.T3.3.3.3.1.1.m1.1.1.3.3a.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3.3"><mtext mathsize="49%" id="S4.T3.3.3.3.1.1.m1.1.1.3.3.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3.3">IoU50</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.1.m1.1c">\Delta\text{mAP}^{\text{IoU50}}</annotation></semantics></math><span id="S4.T3.3.3.3.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.4.4.4.1.1" class="ltx_p"><math id="S4.T3.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="S4.T3.4.4.4.1.1.m1.1a"><msup id="S4.T3.4.4.4.1.1.m1.1.1" xref="S4.T3.4.4.4.1.1.m1.1.1.cmml"><mtext mathsize="70%" id="S4.T3.4.4.4.1.1.m1.1.1.2" xref="S4.T3.4.4.4.1.1.m1.1.1.2a.cmml">mAR</mtext><mtext mathsize="70%" id="S4.T3.4.4.4.1.1.m1.1.1.3" xref="S4.T3.4.4.4.1.1.m1.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.1.m1.1b"><apply id="S4.T3.4.4.4.1.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.1.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1">superscript</csymbol><ci id="S4.T3.4.4.4.1.1.m1.1.1.2a.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.2"><mtext mathsize="70%" id="S4.T3.4.4.4.1.1.m1.1.1.2.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.2">mAR</mtext></ci><ci id="S4.T3.4.4.4.1.1.m1.1.1.3a.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.3"><mtext mathsize="49%" id="S4.T3.4.4.4.1.1.m1.1.1.3.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.1.m1.1c">\text{mAR}^{\text{max=100}}</annotation></semantics></math><span id="S4.T3.4.4.4.1.1.1" class="ltx_text" style="font-size:70%;">(error)</span></span>
</span>
</th>
<th id="S4.T3.5.5.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.5.5.1.1" class="ltx_p"><math id="S4.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAR}^{\text{max=100}}" display="inline"><semantics id="S4.T3.5.5.5.1.1.m1.1a"><mrow id="S4.T3.5.5.5.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="S4.T3.5.5.5.1.1.m1.1.1.2" xref="S4.T3.5.5.5.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.T3.5.5.5.1.1.m1.1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.1.cmml">​</mo><msup id="S4.T3.5.5.5.1.1.m1.1.1.3" xref="S4.T3.5.5.5.1.1.m1.1.1.3.cmml"><mtext mathsize="70%" id="S4.T3.5.5.5.1.1.m1.1.1.3.2" xref="S4.T3.5.5.5.1.1.m1.1.1.3.2a.cmml">mAR</mtext><mtext mathsize="70%" id="S4.T3.5.5.5.1.1.m1.1.1.3.3" xref="S4.T3.5.5.5.1.1.m1.1.1.3.3a.cmml">max=100</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.m1.1b"><apply id="S4.T3.5.5.5.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1"><times id="S4.T3.5.5.5.1.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.1"></times><ci id="S4.T3.5.5.5.1.1.m1.1.1.2.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.2">Δ</ci><apply id="S4.T3.5.5.5.1.1.m1.1.1.3.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.5.5.5.1.1.m1.1.1.3.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3">superscript</csymbol><ci id="S4.T3.5.5.5.1.1.m1.1.1.3.2a.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3.2"><mtext mathsize="70%" id="S4.T3.5.5.5.1.1.m1.1.1.3.2.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3.2">mAR</mtext></ci><ci id="S4.T3.5.5.5.1.1.m1.1.1.3.3a.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3.3"><mtext mathsize="49%" id="S4.T3.5.5.5.1.1.m1.1.1.3.3.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3.3">max=100</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.m1.1c">\Delta\text{mAR}^{\text{max=100}}</annotation></semantics></math><span id="S4.T3.5.5.5.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.6.1" class="ltx_tr">
<th id="S4.T3.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.1.1" class="ltx_text" style="font-size:70%;">Real-World </span><em id="S4.T3.5.6.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">baseline</em><span id="S4.T3.5.6.1.1.3" class="ltx_text" style="font-size:70%;"> (760)</span>
</th>
<td id="S4.T3.5.6.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.2.1.1" class="ltx_p"><span id="S4.T3.5.6.1.2.1.1.1" class="ltx_text" style="font-size:70%;">0.450 (0.020)</span></span>
</span>
</td>
<td id="S4.T3.5.6.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.3.1.1" class="ltx_p"><span id="S4.T3.5.6.1.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="S4.T3.5.6.1.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.4.1.1" class="ltx_p"><span id="S4.T3.5.6.1.4.1.1.1" class="ltx_text" style="font-size:70%;">0.719 (0.020)</span></span>
</span>
</td>
<td id="S4.T3.5.6.1.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.5.1.1" class="ltx_p"><span id="S4.T3.5.6.1.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="S4.T3.5.6.1.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.6.1.1" class="ltx_p"><span id="S4.T3.5.6.1.6.1.1.1" class="ltx_text" style="font-size:70%;">0.570 (0.015)</span></span>
</span>
</td>
<td id="S4.T3.5.6.1.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T3.5.6.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.7.1.1" class="ltx_p"><span id="S4.T3.5.6.1.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.5.7.2" class="ltx_tr">
<th id="S4.T3.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.5.7.2.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000)</span></th>
<td id="S4.T3.5.7.2.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.2.1.1" class="ltx_p"><span id="S4.T3.5.7.2.2.1.1.1" class="ltx_text" style="font-size:70%;">0.381 (0.013)</span></span>
</span>
</td>
<td id="S4.T3.5.7.2.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.3.1.1" class="ltx_p"><span id="S4.T3.5.7.2.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.069 (2e-4)</span></span>
</span>
</td>
<td id="S4.T3.5.7.2.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.4.1.1" class="ltx_p"><span id="S4.T3.5.7.2.4.1.1.1" class="ltx_text" style="font-size:70%;">0.538 (0.019)</span></span>
</span>
</td>
<td id="S4.T3.5.7.2.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.5.1.1" class="ltx_p"><span id="S4.T3.5.7.2.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.182 (4e-2)</span></span>
</span>
</td>
<td id="S4.T3.5.7.2.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.6.1.1" class="ltx_p"><span id="S4.T3.5.7.2.6.1.1.1" class="ltx_text" style="font-size:70%;">0.487 (0.016)</span></span>
</span>
</td>
<td id="S4.T3.5.7.2.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.7.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.2.7.1.1" class="ltx_p"><span id="S4.T3.5.7.2.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.082 (3e-5)</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.5.8.3" class="ltx_tr">
<th id="S4.T3.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.5.8.3.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (76)</span></th>
<td id="S4.T3.5.8.3.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.2.1.1" class="ltx_p"><span id="S4.T3.5.8.3.2.1.1.1" class="ltx_text" style="font-size:70%;">0.528 (0.006)</span></span>
</span>
</td>
<td id="S4.T3.5.8.3.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.3.1.1" class="ltx_p"><span id="S4.T3.5.8.3.3.1.1.1" class="ltx_text" style="font-size:70%;">+0.079 (3e-5)</span></span>
</span>
</td>
<td id="S4.T3.5.8.3.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.4.1.1" class="ltx_p"><span id="S4.T3.5.8.3.4.1.1.1" class="ltx_text" style="font-size:70%;">0.705 (0.008)</span></span>
</span>
</td>
<td id="S4.T3.5.8.3.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.5.1.1" class="ltx_p"><span id="S4.T3.5.8.3.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.014 (1e-1)</span></span>
</span>
</td>
<td id="S4.T3.5.8.3.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.6.1.1" class="ltx_p"><span id="S4.T3.5.8.3.6.1.1.1" class="ltx_text" style="font-size:70%;">0.636 (0.005)</span></span>
</span>
</td>
<td id="S4.T3.5.8.3.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.8.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.3.7.1.1" class="ltx_p"><span id="S4.T3.5.8.3.7.1.1.1" class="ltx_text" style="font-size:70%;">+0.066 (1e-5)</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.5.9.4" class="ltx_tr">
<th id="S4.T3.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.5.9.4.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (380)</span></th>
<td id="S4.T3.5.9.4.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.2.1.1" class="ltx_p"><span id="S4.T3.5.9.4.2.1.1.1" class="ltx_text" style="font-size:70%;">0.644 (0.004)</span></span>
</span>
</td>
<td id="S4.T3.5.9.4.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.3.1.1" class="ltx_p"><span id="S4.T3.5.9.4.3.1.1.1" class="ltx_text" style="font-size:70%;">+0.194 (2e-8)</span></span>
</span>
</td>
<td id="S4.T3.5.9.4.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.4.1.1" class="ltx_p"><span id="S4.T3.5.9.4.4.1.1.1" class="ltx_text" style="font-size:70%;">0.815 (0.005)</span></span>
</span>
</td>
<td id="S4.T3.5.9.4.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.5.1.1" class="ltx_p"><span id="S4.T3.5.9.4.5.1.1.1" class="ltx_text" style="font-size:70%;">+0.095 (6e-6)</span></span>
</span>
</td>
<td id="S4.T3.5.9.4.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.6.1.1" class="ltx_p"><span id="S4.T3.5.9.4.6.1.1.1" class="ltx_text" style="font-size:70%;">0.732 (0.004)</span></span>
</span>
</td>
<td id="S4.T3.5.9.4.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S4.T3.5.9.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.9.4.7.1.1" class="ltx_p"><span id="S4.T3.5.9.4.7.1.1.1" class="ltx_text" style="font-size:70%;">+0.162 (1e-8)</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.5.10.5" class="ltx_tr">
<th id="S4.T3.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T3.5.10.5.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (760)</span></th>
<td id="S4.T3.5.10.5.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.2.1.1" class="ltx_p"><span id="S4.T3.5.10.5.2.1.1.1" class="ltx_text" style="font-size:70%;">0.684 (0.006)</span></span>
</span>
</td>
<td id="S4.T3.5.10.5.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.3.1.1" class="ltx_p"><span id="S4.T3.5.10.5.3.1.1.1" class="ltx_text" style="font-size:70%;">+0.234 (6e-9)</span></span>
</span>
</td>
<td id="S4.T3.5.10.5.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.4.1.1" class="ltx_p"><span id="S4.T3.5.10.5.4.1.1.1" class="ltx_text" style="font-size:70%;">0.854 (0.007)</span></span>
</span>
</td>
<td id="S4.T3.5.10.5.5" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.5.1.1" class="ltx_p"><span id="S4.T3.5.10.5.5.1.1.1" class="ltx_text" style="font-size:70%;">+0.135 (5e-7)</span></span>
</span>
</td>
<td id="S4.T3.5.10.5.6" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.6.1.1" class="ltx_p"><span id="S4.T3.5.10.5.6.1.1.1" class="ltx_text" style="font-size:70%;">0.757 (0.006)</span></span>
</span>
</td>
<td id="S4.T3.5.10.5.7" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="S4.T3.5.10.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.10.5.7.1.1" class="ltx_p"><span id="S4.T3.5.10.5.7.1.1.1" class="ltx_text" style="font-size:70%;">+0.187 (4e-9)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.11.3.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.9.2" class="ltx_text" style="font-size:90%;">Detection performance (mAP, <math id="S4.T3.8.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="S4.T3.8.1.m1.1b"><msup id="S4.T3.8.1.m1.1.1" xref="S4.T3.8.1.m1.1.1.cmml"><mtext id="S4.T3.8.1.m1.1.1.2" xref="S4.T3.8.1.m1.1.1.2a.cmml">mAP</mtext><mtext id="S4.T3.8.1.m1.1.1.3" xref="S4.T3.8.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.8.1.m1.1c"><apply id="S4.T3.8.1.m1.1.1.cmml" xref="S4.T3.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.8.1.m1.1.1.1.cmml" xref="S4.T3.8.1.m1.1.1">superscript</csymbol><ci id="S4.T3.8.1.m1.1.1.2a.cmml" xref="S4.T3.8.1.m1.1.1.2"><mtext id="S4.T3.8.1.m1.1.1.2.cmml" xref="S4.T3.8.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.T3.8.1.m1.1.1.3a.cmml" xref="S4.T3.8.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.8.1.m1.1.1.3.cmml" xref="S4.T3.8.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.1.m1.1d">\text{mAP}^{\text{IoU50}}</annotation></semantics></math>, <math id="S4.T3.9.2.m2.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="S4.T3.9.2.m2.1b"><msup id="S4.T3.9.2.m2.1.1" xref="S4.T3.9.2.m2.1.1.cmml"><mtext id="S4.T3.9.2.m2.1.1.2" xref="S4.T3.9.2.m2.1.1.2a.cmml">mAR</mtext><mtext id="S4.T3.9.2.m2.1.1.3" xref="S4.T3.9.2.m2.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T3.9.2.m2.1c"><apply id="S4.T3.9.2.m2.1.1.cmml" xref="S4.T3.9.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.9.2.m2.1.1.1.cmml" xref="S4.T3.9.2.m2.1.1">superscript</csymbol><ci id="S4.T3.9.2.m2.1.1.2a.cmml" xref="S4.T3.9.2.m2.1.1.2"><mtext id="S4.T3.9.2.m2.1.1.2.cmml" xref="S4.T3.9.2.m2.1.1.2">mAR</mtext></ci><ci id="S4.T3.9.2.m2.1.1.3a.cmml" xref="S4.T3.9.2.m2.1.1.3"><mtext mathsize="70%" id="S4.T3.9.2.m2.1.1.3.cmml" xref="S4.T3.9.2.m2.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.2.m2.1d">\text{mAR}^{\text{max=100}}</annotation></semantics></math>) evaluated on the testing set of the UnityGrocreies-Real dataset. The mean and standard deviation of these metrics over 5 repeated model training procedures under different mixtures of real-world and synthetic datasets are reported in this table. We also provide the mean differences between each training strategy and real-world (baseline) with the p-value from two-sided t-tests.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.4" class="ltx_p">We use the Faster R-CNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> model with the ResNet50<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> backbone pre-trained on the ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> dataset using the pytorch/torchvision<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> implementation. Three training strategies with different combinations of synthetic and real-world data are used in this project. In the first strategy, only the training set of the real-world dataset is used to update the model. These models are trained with a minibatch size of 8 on 1 NVIDIA-V100 GPU for 100 epochs. We selected the best model with the lowest multi-task loss<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> on the validation set of the real-world data. In the second strategy, we use only the 400,000 synthetic data for model training and validation. The dataset is split into 90% for training and 10% for validation. These models are trained with a minibatch size of 4 on 8 NVIDIA-V100 GPUs for 10 epochs. We selected the best model with the lowest multi-task loss on the validation set of the synthetic dataset. In the third strategy, we start from the model trained and selected in strategy two and fine-tune with various subsets of the real-world training set. These models are trained with a minibatch size of 8 on 1 NVIDIA-V100 GPU for 30 epochs. We selected the best model with the lowest multi-task loss on the validation set of the real-world data. We used the Adam method for optimization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, with <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><msub id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2.2" xref="S4.SS2.p2.1.m1.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p2.1.m1.1.1.2.3" xref="S4.SS2.p2.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><apply id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.p2.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><msub id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p2.2.m2.1.1.2.3" xref="S4.SS2.p2.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\beta_{2}=0.999</annotation></semantics></math> in all training strategies, and an initial learning rate of <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="2e^{-4}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml"><mo id="S4.SS2.p2.3.m3.1.1.3.3a" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p2.3.m3.1.1.3.3.2" xref="S4.SS2.p2.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">2</cn><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.2">𝑒</ci><apply id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3"><minus id="S4.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">2e^{-4}</annotation></semantics></math> in all strategies except the fine-tuning steps in strategy three, where we used a smaller learning rate of <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="2e^{-5}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml"><mi id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p2.4.m4.1.1.3.3" xref="S4.SS2.p2.4.m4.1.1.3.3.cmml"><mo id="S4.SS2.p2.4.m4.1.1.3.3a" xref="S4.SS2.p2.4.m4.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p2.4.m4.1.1.3.3.2" xref="S4.SS2.p2.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><times id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">2</cn><apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS2.p2.4.m4.1.1.3.2">𝑒</ci><apply id="S4.SS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3"><minus id="S4.SS2.p2.4.m4.1.1.3.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.4.m4.1.1.3.3.2.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">2e^{-5}</annotation></semantics></math>. We used a gradient accumulation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> of size 8 in all training strategies. All training procedures were repeated 5 times with different random weight initializations.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2107.04259/assets/figures/fine_tuning.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="219" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.6.3.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">Detection performance (mAP, <math id="S4.F6.3.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="S4.F6.3.1.m1.1b"><msup id="S4.F6.3.1.m1.1.1" xref="S4.F6.3.1.m1.1.1.cmml"><mtext id="S4.F6.3.1.m1.1.1.2" xref="S4.F6.3.1.m1.1.1.2a.cmml">mAP</mtext><mtext id="S4.F6.3.1.m1.1.1.3" xref="S4.F6.3.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.F6.3.1.m1.1c"><apply id="S4.F6.3.1.m1.1.1.cmml" xref="S4.F6.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F6.3.1.m1.1.1.1.cmml" xref="S4.F6.3.1.m1.1.1">superscript</csymbol><ci id="S4.F6.3.1.m1.1.1.2a.cmml" xref="S4.F6.3.1.m1.1.1.2"><mtext id="S4.F6.3.1.m1.1.1.2.cmml" xref="S4.F6.3.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.F6.3.1.m1.1.1.3a.cmml" xref="S4.F6.3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.F6.3.1.m1.1.1.3.cmml" xref="S4.F6.3.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.3.1.m1.1d">\text{mAP}^{\text{IoU50}}</annotation></semantics></math>, <math id="S4.F6.4.2.m2.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="S4.F6.4.2.m2.1b"><msup id="S4.F6.4.2.m2.1.1" xref="S4.F6.4.2.m2.1.1.cmml"><mtext id="S4.F6.4.2.m2.1.1.2" xref="S4.F6.4.2.m2.1.1.2a.cmml">mAR</mtext><mtext id="S4.F6.4.2.m2.1.1.3" xref="S4.F6.4.2.m2.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.F6.4.2.m2.1c"><apply id="S4.F6.4.2.m2.1.1.cmml" xref="S4.F6.4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.F6.4.2.m2.1.1.1.cmml" xref="S4.F6.4.2.m2.1.1">superscript</csymbol><ci id="S4.F6.4.2.m2.1.1.2a.cmml" xref="S4.F6.4.2.m2.1.1.2"><mtext id="S4.F6.4.2.m2.1.1.2.cmml" xref="S4.F6.4.2.m2.1.1.2">mAR</mtext></ci><ci id="S4.F6.4.2.m2.1.1.3a.cmml" xref="S4.F6.4.2.m2.1.1.3"><mtext mathsize="70%" id="S4.F6.4.2.m2.1.1.3.cmml" xref="S4.F6.4.2.m2.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.2.m2.1d">\text{mAR}^{\text{max=100}}</annotation></semantics></math>) under different combinations of synthetic (0, 40K, 100K, 400K) + real-world (0, 76, 380, 760) data. The x-axis represents synthetic dataset size. Different colors (blue, red, green, purple) represent different sized subsets of the real-world training sets (0, 76, 380, and 760 images).</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Table <a href="#A2.T4" title="Table 4 ‣ Appendix B SynthDet model performance ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents model performance on the testing set of real-world data using different training strategies. We report three evaluation metrics: 1) mean Average Precision averaged across IoU thresholds of [0.5:0.95] (<span id="S4.SS2.p3.2.1" class="ltx_text" style="font-size:90%;">mAP</span>), 2) mean Average Precision with a single IoU threshold of 0.5 (<math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><msup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mtext mathsize="90%" id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2a.cmml">mAP</mtext><mtext mathsize="90%" id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2a.cmml" xref="S4.SS2.p3.1.m1.1.1.2"><mtext mathsize="90%" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">mAP</mtext></ci><ci id="S4.SS2.p3.1.m1.1.1.3a.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><mtext mathsize="63%" id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\text{mAP}^{\text{IoU50}}</annotation></semantics></math>), and 3) mean Average Recall with a maximum detection of 100 (<math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><msup id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mtext mathsize="90%" id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2a.cmml">mAR</mtext><mtext mathsize="90%" id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2a.cmml" xref="S4.SS2.p3.2.m2.1.1.2"><mtext mathsize="90%" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">mAR</mtext></ci><ci id="S4.SS2.p3.2.m2.1.1.3a.cmml" xref="S4.SS2.p3.2.m2.1.1.3"><mtext mathsize="63%" id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\text{mAR}^{\text{max=100}}</annotation></semantics></math>). Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Results ‣ 4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows detection performance of models trained on different combinations of synthetic (0, 40K, 100K, 400K) + real-world (0, 76, 380, 760) data. Under each real-world data size, with more synthetic data, the model performance is improved. When using only 76 real-world images in training, the synthetic led to substantial improvements in model performance, as shown by the red lines in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Results ‣ 4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. This is while even with 760 real-world images, the addition of synthetic data helped improve model performance to a significant extent, as shown by the purple lines in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Results ‣ 4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The model trained on 400K synthetic data and 760 real-world data showed the best performance. Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 Results ‣ 4 Example Project - SynthDet ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> depicts some visual examples of model prediction under different training strategies. We find that the number of false-positives and false-negatives dropped significantly with synthetic + 760 real-world data. Additionally, we also see remarkable improvements in the bounding box localization. However, we still see that the model is struggling in situations with complex lighting. Detailed model performance results are provided in appendix <a href="#A2" title="Appendix B SynthDet model performance ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. Overall, our results clearly demonstrate that synthetic data can play a significant role in computer vision model training.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2107.04259/assets/x4.png" id="S4.F7.g1" class="ltx_graphics ltx_img_landscape" width="438" height="182" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Illustration of model prediction quality under two different training strategies. Green bounding boxes are correct predictions, and red bounding boxes are false-positive detections. Using synthetic data improves model prediction quality in situations where the objects have complex orientations, configurations, and lighting conditions.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">There are several directions in which we would like to see this package and its ecosystem progress in order to lower the barrier of entry to computer vision research, or perhaps even enable new research avenues.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">An extensible sensor framework would make it easier to add new passive and active sensor types to support environments that rely on radar or lidar sensors, such as robotics and autonomous vehicles. Moreover, it would make it easier to implement specific well-known sensors that would simulate the behavior of their real-world counterparts.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Continued improvement to our existing ground truth generators (Labelers) and the addition of new ones will be another area of future work. For instance, a number of actively researched computer vision tasks require data in the form of sequences of frames. An example of these is object tracking over time. Adding support for this type of output, as well as making it easy for the user to implement additional ground truth generators will be an important component of our road-map moving forward.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Research on new domains and tasks requires content which is not always readily available, as game or film assets are not necessarily readily usable in a simulation environment. We would like to formalize the definition of what it means for assets to be simulation-ready and create tools that will catalyze the process of validating assets for use in simulation. This will cover factors such as requirements on texture scales so they can be swapped at run-time, consistent pivot points which are semantically meaningful, semantic labels that can be mapped to a global taxonomy for randomized placement, and so on.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">To further support researchers, we plan to improve the Linux compatibility of Unity’s High Definition Render Pipeline (HDRP) and enable it to run in headless mode on Linux, the same way it currently does on Windows. The HDRP is a scriptable rendering pipeline that enables advanced global illumination algorithms that contribute to more realistic looking simulations.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Finally, we intend to make Perception simulations externally controllable from other processes potentially running on other nodes. This will allow us to support important workloads such as automated domain randomization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We are working on a protocol as well as a way to define and run such heterogeneous distributed compute graphs in order to significantly reduce the complexity of scaling out experiments over large pools of simulation nodes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look Once:
Unified, Real-Time Object Detection,” in </span><span id="bib.bib1.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, June 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“SSD: Single Shot MultiBox Detector,” in </span><span id="bib.bib2.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision
– ECCV 2016</span><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, pp. 21–37, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal Loss for
Dense Object Detection,” in </span><span id="bib.bib3.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
International Conference on Computer Vision (ICCV)</span><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
Real-Time Object Detection with Region Proposal Networks,”
</span><span id="bib.bib4.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">,
vol. 39, no. 6, pp. 1137–1149, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN,” in </span><span id="bib.bib5.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision
(ICCV)</span><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
M. Tan, R. Pang, and Q. V. Le, “EfficientDet: Scalable and Efficient
Object Detection,” in </span><span id="bib.bib6.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</span><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, pp. 10778–10787, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
J. Long, E. Shelhamer, and T. Darrell, “Fully Convolutional Networks for
Semantic Segmentation,” in </span><span id="bib.bib7.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, June 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
“DeepLab: Semantic Image Segmentation with Deep Convolutional
Nets, Atrous Convolution, and Fully Connected CRFs,” </span><span id="bib.bib8.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE
transactions on pattern analysis and machine intelligence</span><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, vol. 40, no. 4,
pp. 834–848, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks
for Biomedical Image Segmentation,” in </span><span id="bib.bib9.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Medical Image
Computing and Computer-Assisted Intervention – MICCAI 2015</span><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">
(N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds.), (Cham),
pp. 234–241, Springer International Publishing, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking Atrous
Convolution for Semantic Image Segmentation,” in </span><span id="bib.bib10.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
W. Kuo, A. Angelova, J. Malik, and T.-Y. Lin, “ShapeMask: Learning to
Segment Novel Objects by Refining Shape Priors,” in </span><span id="bib.bib11.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV)</span><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, “YOLACT: Real-time Instance
Segmentation,” in </span><span id="bib.bib12.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV)</span><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
P. Voigt and A. v. d. Bussche, </span><span id="bib.bib13.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The EU General Data Protection
Regulation (GDPR): A Practical Guide</span><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">Springer Publishing Company, Incorporated, 1st ed., 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
P. BUKATY, </span><span id="bib.bib14.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The California Consumer Privacy Act (CCPA): An
implementation guide</span><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">IT Governance Publishing, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Unity, “Unity Technologies,” 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The
Pascal Visual Object Classes (VOC) Challenge,” </span><span id="bib.bib16.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, vol. 88, pp. 303–338, June 2010.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor Segmentation and
Support Inference from RGBD Images,” in </span><span id="bib.bib17.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision –
ECCV 2012</span><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, (Berlin, Heidelberg), pp. 746–760, Springer Berlin Heidelberg,
2012.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona,
D. Ramanan, C. L. Zitnick, and P. Dollár, “Microsoft COCO: Common
Objects in Context,” in </span><span id="bib.bib18.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2014</span><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, (Cham),
pp. 740–755, Springer International Publishing, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene
understanding benchmark suite,” in </span><span id="bib.bib19.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">,
June 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
Recognition,” in </span><span id="bib.bib20.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, pp. 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep
Convolutional Encoder-Decoder Architecture for Image
Segmentation,” </span><span id="bib.bib21.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine
Intelligence</span><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, vol. 39, pp. 2481–2495, Dec. 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">Conference Name: IEEE Transactions on Pattern Analysis and Machine
Intelligence.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
S. Hinterstoisser, O. Pauly, H. Heibel, M. Marek, and M. Bokeloh, “An
Annotation Saved is an Annotation Earned: Using Fully Synthetic
Training for Object Instance Detection,” in </span><span id="bib.bib22.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV)
Workshops</span><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
J. Tobin, L. Biewald, R. Duan, M. Andrychowicz, A. Handa, V. Kumar, B. McGrew,
J. Schneider, P. Welinder, W. Zaremba, and P. Abbeel, “Domain
Randomization and Generative Models for Robotic Grasping,” in </span><span id="bib.bib23.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</span><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, pp. 3482–3489, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew,
A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider,
N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang,
“Solving Rubik’s Cube with a Robot Hand,” Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
SYNTHIA Dataset: A Large Collection of Synthetic Images for
Semantic Segmentation of Urban Scenes,” June 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An
Open Urban Driving Simulator,” in </span><span id="bib.bib26.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 1st
Annual Conference on Robot Learning</span><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;"> (S. Levine, V. Vanhoucke, and
K. Goldberg, eds.), vol. 78 of </span><span id="bib.bib26.4.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning
Research</span><span id="bib.bib26.5.5" class="ltx_text" style="font-size:90%;">, pp. 1–16, PMLR, Nov. 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon,
Y. Zhu, A. Gupta, and A. Farhadi, “AI2-THOR: An Interactive 3D
Environment for Visual AI,” </span><span id="bib.bib27.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1712.05474 [cs]</span><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, Mar. 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub,
J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, “Habitat: A
Platform for Embodied AI Research,” in </span><span id="bib.bib28.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV)</span><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">,
Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, S. Buch,
C. D’Arpino, S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio,
L. Fei-Fei, and S. Savarese, “iGibson, a Simulation Environment for
Interactive Tasks in Large Realistic Scenes,” </span><span id="bib.bib29.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2012.02924 [cs]</span><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, Dec. 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">arXiv: 2012.02924.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
“NVIDIA Isaac Sim,” Dec. 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
M. Denninger, M. Sundermeyer, D. Winkelbauer, Y. Zidan, D. Olefir,
M. Elbadrawy, A. Lodhi, and H. Katam, “BlenderProc,” </span><span id="bib.bib31.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1911.01911 [cs]</span><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">arXiv: 1911.01911.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
N. Morrical, J. Tremblay, Y. Lin, S. Tyree, S. Birchﬁeld, V. Pascucci, and
I. Wald, “NViSII: A Scriptable Tool for Photorealistic Image
Generation,” p. 9, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A Multimodal Dataset
for Autonomous Driving,” in </span><span id="bib.bib33.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib33.3.3" class="ltx_text" style="font-size:90%;">,
June 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
M. Klasson, C. Zhang, and H. Kjellström, “A Hierarchical Grocery Store
Image Dataset with Visual and Semantic Labels,” in </span><span id="bib.bib34.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE
Winter Conference on Applications of Computer Vision (WACV)</span><span id="bib.bib34.3.3" class="ltx_text" style="font-size:90%;">,
491-500, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
A. Dutta and A. Zisserman, “The VIA Annotation Software for Images,
Audio and Video,” in </span><span id="bib.bib35.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 27th ACM International
Conference on Multimedia</span><span id="bib.bib35.3.3" class="ltx_text" style="font-size:90%;">, (Nice France), pp. 2276–2279, ACM, Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,
“ImageNet Large Scale Visual Recognition Challenge,” </span><span id="bib.bib36.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib36.3.3" class="ltx_text" style="font-size:90%;">, vol. 115, pp. 211–252, Dec. 2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,
M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and
S. Chintala, “PyTorch: An Imperative Style, High-Performance
Deep Learning Library,” in </span><span id="bib.bib37.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems 32</span><span id="bib.bib37.3.3" class="ltx_text" style="font-size:90%;"> (H. Wallach, H. Larochelle, A. Beygelzimer, F. d.
Alché-Buc, E. Fox, and R. Garnett, eds.), pp. 8024–8035, Curran Associates,
Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
in </span><span id="bib.bib38.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib38.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, “Deep Gradient
Compression: Reducing the Communication Bandwidth for Distributed
Training,” in </span><span id="bib.bib39.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning
Representations</span><span id="bib.bib39.3.3" class="ltx_text" style="font-size:90%;">, p. 14, 2018.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Perception package diagrams</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Figures <a href="#A1.F8" title="Figure 8 ‣ Appendix A Perception package diagrams ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <a href="#A1.F9" title="Figure 9 ‣ Appendix A Perception package diagrams ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and <a href="#A1.F10" title="Figure 10 ‣ Appendix A Perception package diagrams ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> depict high-level component diagrams and a sequence diagram for the ground-truth generation and randomization systems within the Perception package. The level of detail in these diagrams are abstracted to a certain extent, to make them easier to digest here.</p>
</div>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2107.04259/assets/figures/labeling_uml.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A1.F8.3.2" class="ltx_text" style="font-size:90%;">Class diagram for the ground truth generation system of the Perception package. A set of Camera Labelers are added to the Perception Camera, each tasked with generating a specific type of ground truth. For instance, the Semantic Segmentation Labeler outputs segmentation images in which each labeled object is rendered in a unique user-definable color and non-labeled objects and the background are rendered black. The LabelConfig acts as a mapping between string labels and object classes (currently colors or integers), deciding which labels in the scene (and thus which objects) should be tracked by the Labeler, and what color (or integer id) they should have in the captured frames. The Perception package currently comes with Labelers for five computer vision tasks (Figure <a href="#S3.F2" title="Figure 2 ‣ 3 The Unity Perception Package ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and the user can implement more by extending the CameraLabeler class.</span></figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure"><img src="/html/2107.04259/assets/figures/randomization_uml.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A1.F9.3.2" class="ltx_text" style="font-size:90%;">Class diagram for the randomization framework included in the Perception package. The Scenario coordinates the life-cycle of the simulation, executing a number of randomization Iterations. In each iteration, a group of Randomizers are triggered, each of which is tasked with randomizing one or more aspects of the simulation and the objects present in the scene. To carry out this randomization, Randomizers can include one or more Parameter objects which internally use Samplers in order to generate random typed values. The Scenario, Randomizer, and Parameter classes are all extensible. Additionally, the user can implement the ISampler interface to achieve further customization in sampling behavior if the provided distributions are not sufficient.</span></figcaption>
</figure>
<figure id="A1.F10" class="ltx_figure"><img src="/html/2107.04259/assets/figures/scenario_seq_diag.png" id="A1.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="222" height="592" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A1.F10.3.2" class="ltx_text" style="font-size:90%;">Sequence diagram for the execution of a Scenario, depicting the various life-cycle events that are triggered as the Scenario progresses through its Iterations, calling on its Randomizers to perform their duties. At each point in the life-cycle, the Randomizers included in a Scenario are triggered in the order they are added to the Scenario by the user. All life-cycle events in the diagram above can be overridden in order to customize the behavior of the Scenario and its Randomizers. For simplicity, the diagram above portrays a Scenario with just one Randomizer.</span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>SynthDet model performance</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Table <a href="#A2.T4" title="Table 4 ‣ Appendix B SynthDet model performance ‣ Unity Perception: Generate Synthetic Data for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides detailed performance figures for the SynthDet model under a variety of combinations of real and synthetic training data.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<table id="A2.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T4.5.5" class="ltx_tr">
<th id="A2.T4.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A2.T4.5.5.6.1" class="ltx_text" style="font-size:70%;">Training Data (number of training examples)</span></th>
<th id="A2.T4.5.5.7" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.5.7.1.1" class="ltx_p"><span id="A2.T4.5.5.7.1.1.1" class="ltx_text" style="font-size:70%;">mAP(error)</span></span>
</span>
</th>
<th id="A2.T4.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.1.1.1.1.1" class="ltx_p"><math id="A2.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAP}" display="inline"><semantics id="A2.T4.1.1.1.1.1.m1.1a"><mrow id="A2.T4.1.1.1.1.1.m1.1.1" xref="A2.T4.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="A2.T4.1.1.1.1.1.m1.1.1.2" xref="A2.T4.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.T4.1.1.1.1.1.m1.1.1.1" xref="A2.T4.1.1.1.1.1.m1.1.1.1.cmml">​</mo><mtext mathsize="70%" id="A2.T4.1.1.1.1.1.m1.1.1.3" xref="A2.T4.1.1.1.1.1.m1.1.1.3a.cmml">mAP</mtext></mrow><annotation-xml encoding="MathML-Content" id="A2.T4.1.1.1.1.1.m1.1b"><apply id="A2.T4.1.1.1.1.1.m1.1.1.cmml" xref="A2.T4.1.1.1.1.1.m1.1.1"><times id="A2.T4.1.1.1.1.1.m1.1.1.1.cmml" xref="A2.T4.1.1.1.1.1.m1.1.1.1"></times><ci id="A2.T4.1.1.1.1.1.m1.1.1.2.cmml" xref="A2.T4.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="A2.T4.1.1.1.1.1.m1.1.1.3a.cmml" xref="A2.T4.1.1.1.1.1.m1.1.1.3"><mtext mathsize="70%" id="A2.T4.1.1.1.1.1.m1.1.1.3.cmml" xref="A2.T4.1.1.1.1.1.m1.1.1.3">mAP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.1.1.1.1.1.m1.1c">\Delta\text{mAP}</annotation></semantics></math><span id="A2.T4.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
<th id="A2.T4.2.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.2.2.2.1.1" class="ltx_p"><math id="A2.T4.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="A2.T4.2.2.2.1.1.m1.1a"><msup id="A2.T4.2.2.2.1.1.m1.1.1" xref="A2.T4.2.2.2.1.1.m1.1.1.cmml"><mtext mathsize="70%" id="A2.T4.2.2.2.1.1.m1.1.1.2" xref="A2.T4.2.2.2.1.1.m1.1.1.2a.cmml">mAP</mtext><mtext mathsize="70%" id="A2.T4.2.2.2.1.1.m1.1.1.3" xref="A2.T4.2.2.2.1.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T4.2.2.2.1.1.m1.1b"><apply id="A2.T4.2.2.2.1.1.m1.1.1.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T4.2.2.2.1.1.m1.1.1.1.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1">superscript</csymbol><ci id="A2.T4.2.2.2.1.1.m1.1.1.2a.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1.2"><mtext mathsize="70%" id="A2.T4.2.2.2.1.1.m1.1.1.2.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1.2">mAP</mtext></ci><ci id="A2.T4.2.2.2.1.1.m1.1.1.3a.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1.3"><mtext mathsize="49%" id="A2.T4.2.2.2.1.1.m1.1.1.3.cmml" xref="A2.T4.2.2.2.1.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.2.2.2.1.1.m1.1c">\text{mAP}^{\text{IoU50}}</annotation></semantics></math><span id="A2.T4.2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">(error)</span></span>
</span>
</th>
<th id="A2.T4.3.3.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.3.3.1.1" class="ltx_p"><math id="A2.T4.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="A2.T4.3.3.3.1.1.m1.1a"><mrow id="A2.T4.3.3.3.1.1.m1.1.1" xref="A2.T4.3.3.3.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="A2.T4.3.3.3.1.1.m1.1.1.2" xref="A2.T4.3.3.3.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.T4.3.3.3.1.1.m1.1.1.1" xref="A2.T4.3.3.3.1.1.m1.1.1.1.cmml">​</mo><msup id="A2.T4.3.3.3.1.1.m1.1.1.3" xref="A2.T4.3.3.3.1.1.m1.1.1.3.cmml"><mtext mathsize="70%" id="A2.T4.3.3.3.1.1.m1.1.1.3.2" xref="A2.T4.3.3.3.1.1.m1.1.1.3.2a.cmml">mAP</mtext><mtext mathsize="70%" id="A2.T4.3.3.3.1.1.m1.1.1.3.3" xref="A2.T4.3.3.3.1.1.m1.1.1.3.3a.cmml">IoU50</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T4.3.3.3.1.1.m1.1b"><apply id="A2.T4.3.3.3.1.1.m1.1.1.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1"><times id="A2.T4.3.3.3.1.1.m1.1.1.1.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.1"></times><ci id="A2.T4.3.3.3.1.1.m1.1.1.2.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.2">Δ</ci><apply id="A2.T4.3.3.3.1.1.m1.1.1.3.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T4.3.3.3.1.1.m1.1.1.3.1.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3">superscript</csymbol><ci id="A2.T4.3.3.3.1.1.m1.1.1.3.2a.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3.2"><mtext mathsize="70%" id="A2.T4.3.3.3.1.1.m1.1.1.3.2.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3.2">mAP</mtext></ci><ci id="A2.T4.3.3.3.1.1.m1.1.1.3.3a.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3.3"><mtext mathsize="49%" id="A2.T4.3.3.3.1.1.m1.1.1.3.3.cmml" xref="A2.T4.3.3.3.1.1.m1.1.1.3.3">IoU50</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.3.3.3.1.1.m1.1c">\Delta\text{mAP}^{\text{IoU50}}</annotation></semantics></math><span id="A2.T4.3.3.3.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
<th id="A2.T4.4.4.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.4.4.4.1.1" class="ltx_p"><math id="A2.T4.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="A2.T4.4.4.4.1.1.m1.1a"><msup id="A2.T4.4.4.4.1.1.m1.1.1" xref="A2.T4.4.4.4.1.1.m1.1.1.cmml"><mtext mathsize="70%" id="A2.T4.4.4.4.1.1.m1.1.1.2" xref="A2.T4.4.4.4.1.1.m1.1.1.2a.cmml">mAR</mtext><mtext mathsize="70%" id="A2.T4.4.4.4.1.1.m1.1.1.3" xref="A2.T4.4.4.4.1.1.m1.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T4.4.4.4.1.1.m1.1b"><apply id="A2.T4.4.4.4.1.1.m1.1.1.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T4.4.4.4.1.1.m1.1.1.1.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1">superscript</csymbol><ci id="A2.T4.4.4.4.1.1.m1.1.1.2a.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1.2"><mtext mathsize="70%" id="A2.T4.4.4.4.1.1.m1.1.1.2.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1.2">mAR</mtext></ci><ci id="A2.T4.4.4.4.1.1.m1.1.1.3a.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1.3"><mtext mathsize="49%" id="A2.T4.4.4.4.1.1.m1.1.1.3.cmml" xref="A2.T4.4.4.4.1.1.m1.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.4.4.4.1.1.m1.1c">\text{mAR}^{\text{max=100}}</annotation></semantics></math><span id="A2.T4.4.4.4.1.1.1" class="ltx_text" style="font-size:70%;">(error)</span></span>
</span>
</th>
<th id="A2.T4.5.5.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="A2.T4.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.5.5.1.1" class="ltx_p"><math id="A2.T4.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\Delta\text{mAR}^{\text{max=100}}" display="inline"><semantics id="A2.T4.5.5.5.1.1.m1.1a"><mrow id="A2.T4.5.5.5.1.1.m1.1.1" xref="A2.T4.5.5.5.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="A2.T4.5.5.5.1.1.m1.1.1.2" xref="A2.T4.5.5.5.1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.T4.5.5.5.1.1.m1.1.1.1" xref="A2.T4.5.5.5.1.1.m1.1.1.1.cmml">​</mo><msup id="A2.T4.5.5.5.1.1.m1.1.1.3" xref="A2.T4.5.5.5.1.1.m1.1.1.3.cmml"><mtext mathsize="70%" id="A2.T4.5.5.5.1.1.m1.1.1.3.2" xref="A2.T4.5.5.5.1.1.m1.1.1.3.2a.cmml">mAR</mtext><mtext mathsize="70%" id="A2.T4.5.5.5.1.1.m1.1.1.3.3" xref="A2.T4.5.5.5.1.1.m1.1.1.3.3a.cmml">max=100</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T4.5.5.5.1.1.m1.1b"><apply id="A2.T4.5.5.5.1.1.m1.1.1.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1"><times id="A2.T4.5.5.5.1.1.m1.1.1.1.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.1"></times><ci id="A2.T4.5.5.5.1.1.m1.1.1.2.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.2">Δ</ci><apply id="A2.T4.5.5.5.1.1.m1.1.1.3.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T4.5.5.5.1.1.m1.1.1.3.1.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3">superscript</csymbol><ci id="A2.T4.5.5.5.1.1.m1.1.1.3.2a.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3.2"><mtext mathsize="70%" id="A2.T4.5.5.5.1.1.m1.1.1.3.2.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3.2">mAR</mtext></ci><ci id="A2.T4.5.5.5.1.1.m1.1.1.3.3a.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3.3"><mtext mathsize="49%" id="A2.T4.5.5.5.1.1.m1.1.1.3.3.cmml" xref="A2.T4.5.5.5.1.1.m1.1.1.3.3">max=100</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.5.5.5.1.1.m1.1c">\Delta\text{mAR}^{\text{max=100}}</annotation></semantics></math><span id="A2.T4.5.5.5.1.1.1" class="ltx_text" style="font-size:70%;">(p-value)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T4.5.6.1" class="ltx_tr">
<th id="A2.T4.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.1.1" class="ltx_text" style="font-size:70%;">Real-World </span><em id="A2.T4.5.6.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">baseline</em><span id="A2.T4.5.6.1.1.3" class="ltx_text" style="font-size:70%;"> (760)</span>
</th>
<td id="A2.T4.5.6.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.2.1.1" class="ltx_p"><span id="A2.T4.5.6.1.2.1.1.1" class="ltx_text" style="font-size:70%;">0.450 (0.020)</span></span>
</span>
</td>
<td id="A2.T4.5.6.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.3.1.1" class="ltx_p"><span id="A2.T4.5.6.1.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="A2.T4.5.6.1.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.4.1.1" class="ltx_p"><span id="A2.T4.5.6.1.4.1.1.1" class="ltx_text" style="font-size:70%;">0.719 (0.020)</span></span>
</span>
</td>
<td id="A2.T4.5.6.1.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.5.1.1" class="ltx_p"><span id="A2.T4.5.6.1.5.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
<td id="A2.T4.5.6.1.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.6.1.1" class="ltx_p"><span id="A2.T4.5.6.1.6.1.1.1" class="ltx_text" style="font-size:70%;">0.570 (0.015)</span></span>
</span>
</td>
<td id="A2.T4.5.6.1.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="A2.T4.5.6.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.6.1.7.1.1" class="ltx_p"><span id="A2.T4.5.6.1.7.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.7.2" class="ltx_tr">
<th id="A2.T4.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.7.2.1.1" class="ltx_text" style="font-size:70%;">Real-World (76)</span></th>
<td id="A2.T4.5.7.2.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.2.1.1" class="ltx_p"><span id="A2.T4.5.7.2.2.1.1.1" class="ltx_text" style="font-size:70%;">0.001 (0.001)</span></span>
</span>
</td>
<td id="A2.T4.5.7.2.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.3.1.1" class="ltx_p"><span id="A2.T4.5.7.2.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.449 (3e-11)</span></span>
</span>
</td>
<td id="A2.T4.5.7.2.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.4.1.1" class="ltx_p"><span id="A2.T4.5.7.2.4.1.1.1" class="ltx_text" style="font-size:70%;">0.002 (0.003)</span></span>
</span>
</td>
<td id="A2.T4.5.7.2.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.5.1.1" class="ltx_p"><span id="A2.T4.5.7.2.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.717 (7e-13)</span></span>
</span>
</td>
<td id="A2.T4.5.7.2.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.6.1.1" class="ltx_p"><span id="A2.T4.5.7.2.6.1.1.1" class="ltx_text" style="font-size:70%;">0.014 (0.018)</span></span>
</span>
</td>
<td id="A2.T4.5.7.2.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.7.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.7.2.7.1.1" class="ltx_p"><span id="A2.T4.5.7.2.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.556 (2e-11)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.8.3" class="ltx_tr">
<th id="A2.T4.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.8.3.1.1" class="ltx_text" style="font-size:70%;">Real-World (380)</span></th>
<td id="A2.T4.5.8.3.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.2.1.1" class="ltx_p"><span id="A2.T4.5.8.3.2.1.1.1" class="ltx_text" style="font-size:70%;">0.244 (0.048)</span></span>
</span>
</td>
<td id="A2.T4.5.8.3.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.3.1.1" class="ltx_p"><span id="A2.T4.5.8.3.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.206 (2e-5)</span></span>
</span>
</td>
<td id="A2.T4.5.8.3.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.4.1.1" class="ltx_p"><span id="A2.T4.5.8.3.4.1.1.1" class="ltx_text" style="font-size:70%;">0.461 (0.080)</span></span>
</span>
</td>
<td id="A2.T4.5.8.3.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.5.1.1" class="ltx_p"><span id="A2.T4.5.8.3.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.258 (1e-4)</span></span>
</span>
</td>
<td id="A2.T4.5.8.3.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.6.1.1" class="ltx_p"><span id="A2.T4.5.8.3.6.1.1.1" class="ltx_text" style="font-size:70%;">0.431 (0.025)</span></span>
</span>
</td>
<td id="A2.T4.5.8.3.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.8.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.8.3.7.1.1" class="ltx_p"><span id="A2.T4.5.8.3.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.138 (5e-6)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.9.4" class="ltx_tr">
<th id="A2.T4.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.9.4.1.1" class="ltx_text" style="font-size:70%;">Synthetic (40,000)</span></th>
<td id="A2.T4.5.9.4.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.2.1.1" class="ltx_p"><span id="A2.T4.5.9.4.2.1.1.1" class="ltx_text" style="font-size:70%;">0.311 (0.023)</span></span>
</span>
</td>
<td id="A2.T4.5.9.4.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.3.1.1" class="ltx_p"><span id="A2.T4.5.9.4.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.139 (7e-6)</span></span>
</span>
</td>
<td id="A2.T4.5.9.4.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.4.1.1" class="ltx_p"><span id="A2.T4.5.9.4.4.1.1.1" class="ltx_text" style="font-size:70%;">0.498 (0.026)</span></span>
</span>
</td>
<td id="A2.T4.5.9.4.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.5.1.1" class="ltx_p"><span id="A2.T4.5.9.4.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.221 (4e-7)</span></span>
</span>
</td>
<td id="A2.T4.5.9.4.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.6.1.1" class="ltx_p"><span id="A2.T4.5.9.4.6.1.1.1" class="ltx_text" style="font-size:70%;">0.433 (0.023)</span></span>
</span>
</td>
<td id="A2.T4.5.9.4.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.9.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.9.4.7.1.1" class="ltx_p"><span id="A2.T4.5.9.4.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.137 (4e-6)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.10.5" class="ltx_tr">
<th id="A2.T4.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.10.5.1.1" class="ltx_text" style="font-size:70%;">Synthetic (100,000)</span></th>
<td id="A2.T4.5.10.5.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.2.1.1" class="ltx_p"><span id="A2.T4.5.10.5.2.1.1.1" class="ltx_text" style="font-size:70%;">0.364 (0.005)</span></span>
</span>
</td>
<td id="A2.T4.5.10.5.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.3.1.1" class="ltx_p"><span id="A2.T4.5.10.5.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.086 (1e-5)</span></span>
</span>
</td>
<td id="A2.T4.5.10.5.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.4.1.1" class="ltx_p"><span id="A2.T4.5.10.5.4.1.1.1" class="ltx_text" style="font-size:70%;">0.538 (0.007)</span></span>
</span>
</td>
<td id="A2.T4.5.10.5.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.5.1.1" class="ltx_p"><span id="A2.T4.5.10.5.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.181 (5e-8)</span></span>
</span>
</td>
<td id="A2.T4.5.10.5.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.6.1.1" class="ltx_p"><span id="A2.T4.5.10.5.6.1.1.1" class="ltx_text" style="font-size:70%;">0.477 (0.004)</span></span>
</span>
</td>
<td id="A2.T4.5.10.5.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.10.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.10.5.7.1.1" class="ltx_p"><span id="A2.T4.5.10.5.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.093 (7e-7)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.11.6" class="ltx_tr">
<th id="A2.T4.5.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.11.6.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000)</span></th>
<td id="A2.T4.5.11.6.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.2.1.1" class="ltx_p"><span id="A2.T4.5.11.6.2.1.1.1" class="ltx_text" style="font-size:70%;">0.381 (0.013)</span></span>
</span>
</td>
<td id="A2.T4.5.11.6.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.3.1.1" class="ltx_p"><span id="A2.T4.5.11.6.3.1.1.1" class="ltx_text" style="font-size:70%;">-0.069 (2e-4)</span></span>
</span>
</td>
<td id="A2.T4.5.11.6.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.4.1.1" class="ltx_p"><span id="A2.T4.5.11.6.4.1.1.1" class="ltx_text" style="font-size:70%;">0.538 (0.019)</span></span>
</span>
</td>
<td id="A2.T4.5.11.6.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.5.1.1" class="ltx_p"><span id="A2.T4.5.11.6.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.182 (5e-7)</span></span>
</span>
</td>
<td id="A2.T4.5.11.6.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.6.1.1" class="ltx_p"><span id="A2.T4.5.11.6.6.1.1.1" class="ltx_text" style="font-size:70%;">0.487 (0.004)</span></span>
</span>
</td>
<td id="A2.T4.5.11.6.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.11.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.11.6.7.1.1" class="ltx_p"><span id="A2.T4.5.11.6.7.1.1.1" class="ltx_text" style="font-size:70%;">-0.082 (3e-5)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.12.7" class="ltx_tr">
<th id="A2.T4.5.12.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.12.7.1.1" class="ltx_text" style="font-size:70%;">Synthetic (40,000) + Real-World (76)</span></th>
<td id="A2.T4.5.12.7.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.2.1.1" class="ltx_p"><span id="A2.T4.5.12.7.2.1.1.1" class="ltx_text" style="font-size:70%;">0.469 (0.008)</span></span>
</span>
</td>
<td id="A2.T4.5.12.7.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.3.1.1" class="ltx_p"><span id="A2.T4.5.12.7.3.1.1.1" class="ltx_text" style="font-size:70%;">0.019 (7e-2)</span></span>
</span>
</td>
<td id="A2.T4.5.12.7.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.4.1.1" class="ltx_p"><span id="A2.T4.5.12.7.4.1.1.1" class="ltx_text" style="font-size:70%;">0.688 (0.008)</span></span>
</span>
</td>
<td id="A2.T4.5.12.7.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.5.1.1" class="ltx_p"><span id="A2.T4.5.12.7.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.31 (1e-2)</span></span>
</span>
</td>
<td id="A2.T4.5.12.7.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.6.1.1" class="ltx_p"><span id="A2.T4.5.12.7.6.1.1.1" class="ltx_text" style="font-size:70%;">0.591 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.12.7.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.12.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.12.7.7.1.1" class="ltx_p"><span id="A2.T4.5.12.7.7.1.1.1" class="ltx_text" style="font-size:70%;">0.021 (2e-2)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.13.8" class="ltx_tr">
<th id="A2.T4.5.13.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.13.8.1.1" class="ltx_text" style="font-size:70%;">Synthetic (100,000) + Real-World (76)</span></th>
<td id="A2.T4.5.13.8.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.2.1.1" class="ltx_p"><span id="A2.T4.5.13.8.2.1.1.1" class="ltx_text" style="font-size:70%;">0.523 (0.008)</span></span>
</span>
</td>
<td id="A2.T4.5.13.8.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.3.1.1" class="ltx_p"><span id="A2.T4.5.13.8.3.1.1.1" class="ltx_text" style="font-size:70%;">0.073 (6e-5)</span></span>
</span>
</td>
<td id="A2.T4.5.13.8.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.4.1.1" class="ltx_p"><span id="A2.T4.5.13.8.4.1.1.1" class="ltx_text" style="font-size:70%;">0.727 (0.009)</span></span>
</span>
</td>
<td id="A2.T4.5.13.8.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.5.1.1" class="ltx_p"><span id="A2.T4.5.13.8.5.1.1.1" class="ltx_text" style="font-size:70%;">0.008 (4e-1)</span></span>
</span>
</td>
<td id="A2.T4.5.13.8.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.6.1.1" class="ltx_p"><span id="A2.T4.5.13.8.6.1.1.1" class="ltx_text" style="font-size:70%;">0.630 (0.007)</span></span>
</span>
</td>
<td id="A2.T4.5.13.8.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.13.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.13.8.7.1.1" class="ltx_p"><span id="A2.T4.5.13.8.7.1.1.1" class="ltx_text" style="font-size:70%;">0.060 (3e-5)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.14.9" class="ltx_tr">
<th id="A2.T4.5.14.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.14.9.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (76)</span></th>
<td id="A2.T4.5.14.9.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.2.1.1" class="ltx_p"><span id="A2.T4.5.14.9.2.1.1.1" class="ltx_text" style="font-size:70%;">0.528 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.14.9.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.3.1.1" class="ltx_p"><span id="A2.T4.5.14.9.3.1.1.1" class="ltx_text" style="font-size:70%;">0.078 (2e-8)</span></span>
</span>
</td>
<td id="A2.T4.5.14.9.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.4.1.1" class="ltx_p"><span id="A2.T4.5.14.9.4.1.1.1" class="ltx_text" style="font-size:70%;">0.705 (0.002)</span></span>
</span>
</td>
<td id="A2.T4.5.14.9.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.5.1.1" class="ltx_p"><span id="A2.T4.5.14.9.5.1.1.1" class="ltx_text" style="font-size:70%;">-0.014 (1e-1)</span></span>
</span>
</td>
<td id="A2.T4.5.14.9.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.6.1.1" class="ltx_p"><span id="A2.T4.5.14.9.6.1.1.1" class="ltx_text" style="font-size:70%;">0.636 (0.004)</span></span>
</span>
</td>
<td id="A2.T4.5.14.9.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.14.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.14.9.7.1.1" class="ltx_p"><span id="A2.T4.5.14.9.7.1.1.1" class="ltx_text" style="font-size:70%;">0.066 (1e-5)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.15.10" class="ltx_tr">
<th id="A2.T4.5.15.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.15.10.1.1" class="ltx_text" style="font-size:70%;">Synthetic (40,000) + Real-World (380)</span></th>
<td id="A2.T4.5.15.10.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.2.1.1" class="ltx_p"><span id="A2.T4.5.15.10.2.1.1.1" class="ltx_text" style="font-size:70%;">0.559 (0.011)</span></span>
</span>
</td>
<td id="A2.T4.5.15.10.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.3.1.1" class="ltx_p"><span id="A2.T4.5.15.10.3.1.1.1" class="ltx_text" style="font-size:70%;">0.109 (5e-6)</span></span>
</span>
</td>
<td id="A2.T4.5.15.10.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.4.1.1" class="ltx_p"><span id="A2.T4.5.15.10.4.1.1.1" class="ltx_text" style="font-size:70%;">0.781 (0.009)</span></span>
</span>
</td>
<td id="A2.T4.5.15.10.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.5.1.1" class="ltx_p"><span id="A2.T4.5.15.10.5.1.1.1" class="ltx_text" style="font-size:70%;">0.062 (2e-4)</span></span>
</span>
</td>
<td id="A2.T4.5.15.10.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.6.1.1" class="ltx_p"><span id="A2.T4.5.15.10.6.1.1.1" class="ltx_text" style="font-size:70%;">0.664 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.15.10.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.15.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.15.10.7.1.1" class="ltx_p"><span id="A2.T4.5.15.10.7.1.1.1" class="ltx_text" style="font-size:70%;">0.095 (9e-7)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.16.11" class="ltx_tr">
<th id="A2.T4.5.16.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.16.11.1.1" class="ltx_text" style="font-size:70%;">Synthetic (100,000) + Real-World (380)</span></th>
<td id="A2.T4.5.16.11.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.2.1.1" class="ltx_p"><span id="A2.T4.5.16.11.2.1.1.1" class="ltx_text" style="font-size:70%;">0.625 (0.048)</span></span>
</span>
</td>
<td id="A2.T4.5.16.11.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.3.1.1" class="ltx_p"><span id="A2.T4.5.16.11.3.1.1.1" class="ltx_text" style="font-size:70%;">0.175 (6e-8)</span></span>
</span>
</td>
<td id="A2.T4.5.16.11.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.4.1.1" class="ltx_p"><span id="A2.T4.5.16.11.4.1.1.1" class="ltx_text" style="font-size:70%;">0.823 (0.007)</span></span>
</span>
</td>
<td id="A2.T4.5.16.11.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.5.1.1" class="ltx_p"><span id="A2.T4.5.16.11.5.1.1.1" class="ltx_text" style="font-size:70%;">0.104 (4e-6)</span></span>
</span>
</td>
<td id="A2.T4.5.16.11.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.6.1.1" class="ltx_p"><span id="A2.T4.5.16.11.6.1.1.1" class="ltx_text" style="font-size:70%;">0.713 (0.003)</span></span>
</span>
</td>
<td id="A2.T4.5.16.11.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.16.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.16.11.7.1.1" class="ltx_p"><span id="A2.T4.5.16.11.7.1.1.1" class="ltx_text" style="font-size:70%;">0.143 (2e-8)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.17.12" class="ltx_tr">
<th id="A2.T4.5.17.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.17.12.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (380)</span></th>
<td id="A2.T4.5.17.12.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.2.1.1" class="ltx_p"><span id="A2.T4.5.17.12.2.1.1.1" class="ltx_text" style="font-size:70%;">0.644 (0.004)</span></span>
</span>
</td>
<td id="A2.T4.5.17.12.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.3.1.1" class="ltx_p"><span id="A2.T4.5.17.12.3.1.1.1" class="ltx_text" style="font-size:70%;">0.194 (2e-8)</span></span>
</span>
</td>
<td id="A2.T4.5.17.12.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.4.1.1" class="ltx_p"><span id="A2.T4.5.17.12.4.1.1.1" class="ltx_text" style="font-size:70%;">0.815 (0.005)</span></span>
</span>
</td>
<td id="A2.T4.5.17.12.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.5.1.1" class="ltx_p"><span id="A2.T4.5.17.12.5.1.1.1" class="ltx_text" style="font-size:70%;">0.095 (6e-6)</span></span>
</span>
</td>
<td id="A2.T4.5.17.12.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.6.1.1" class="ltx_p"><span id="A2.T4.5.17.12.6.1.1.1" class="ltx_text" style="font-size:70%;">0.732 (0.004)</span></span>
</span>
</td>
<td id="A2.T4.5.17.12.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.17.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.17.12.7.1.1" class="ltx_p"><span id="A2.T4.5.17.12.7.1.1.1" class="ltx_text" style="font-size:70%;">0.162 (1e-8)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.18.13" class="ltx_tr">
<th id="A2.T4.5.18.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.18.13.1.1" class="ltx_text" style="font-size:70%;">Synthetic (40,000) + Real-World (760)</span></th>
<td id="A2.T4.5.18.13.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.2.1.1" class="ltx_p"><span id="A2.T4.5.18.13.2.1.1.1" class="ltx_text" style="font-size:70%;">0.600 (0.010)</span></span>
</span>
</td>
<td id="A2.T4.5.18.13.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.3.1.1" class="ltx_p"><span id="A2.T4.5.18.13.3.1.1.1" class="ltx_text" style="font-size:70%;">0.150 (4e-7)</span></span>
</span>
</td>
<td id="A2.T4.5.18.13.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.4.1.1" class="ltx_p"><span id="A2.T4.5.18.13.4.1.1.1" class="ltx_text" style="font-size:70%;">0.825 (0.010)</span></span>
</span>
</td>
<td id="A2.T4.5.18.13.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.5.1.1" class="ltx_p"><span id="A2.T4.5.18.13.5.1.1.1" class="ltx_text" style="font-size:70%;">0.106 (5e-6)</span></span>
</span>
</td>
<td id="A2.T4.5.18.13.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.6.1.1" class="ltx_p"><span id="A2.T4.5.18.13.6.1.1.1" class="ltx_text" style="font-size:70%;">0.687 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.18.13.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.18.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.18.13.7.1.1" class="ltx_p"><span id="A2.T4.5.18.13.7.1.1.1" class="ltx_text" style="font-size:70%;">0.118 (1e-7)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.19.14" class="ltx_tr">
<th id="A2.T4.5.19.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="A2.T4.5.19.14.1.1" class="ltx_text" style="font-size:70%;">Synthetic (100,000) + Real-World (760)</span></th>
<td id="A2.T4.5.19.14.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.2.1.1" class="ltx_p"><span id="A2.T4.5.19.14.2.1.1.1" class="ltx_text" style="font-size:70%;">0.662 (0.008)</span></span>
</span>
</td>
<td id="A2.T4.5.19.14.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.3.1.1" class="ltx_p"><span id="A2.T4.5.19.14.3.1.1.1" class="ltx_text" style="font-size:70%;">0.212 (2e-8)</span></span>
</span>
</td>
<td id="A2.T4.5.19.14.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.4.1.1" class="ltx_p"><span id="A2.T4.5.19.14.4.1.1.1" class="ltx_text" style="font-size:70%;">0.860 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.19.14.5" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.5.1.1" class="ltx_p"><span id="A2.T4.5.19.14.5.1.1.1" class="ltx_text" style="font-size:70%;">0.141 (3e-7)</span></span>
</span>
</td>
<td id="A2.T4.5.19.14.6" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.6.1.1" class="ltx_p"><span id="A2.T4.5.19.14.6.1.1.1" class="ltx_text" style="font-size:70%;">0.734 (0.005)</span></span>
</span>
</td>
<td id="A2.T4.5.19.14.7" class="ltx_td ltx_align_justify ltx_border_r">
<span id="A2.T4.5.19.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.19.14.7.1.1" class="ltx_p"><span id="A2.T4.5.19.14.7.1.1.1" class="ltx_text" style="font-size:70%;">0.164 (1e-8)</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.5.20.15" class="ltx_tr">
<th id="A2.T4.5.20.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="A2.T4.5.20.15.1.1" class="ltx_text" style="font-size:70%;">Synthetic (400,000) + Real-World (760)</span></th>
<td id="A2.T4.5.20.15.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.2.1.1" class="ltx_p"><span id="A2.T4.5.20.15.2.1.1.1" class="ltx_text" style="font-size:70%;">0.684 (0.006)</span></span>
</span>
</td>
<td id="A2.T4.5.20.15.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.3.1.1" class="ltx_p"><span id="A2.T4.5.20.15.3.1.1.1" class="ltx_text" style="font-size:70%;">0.234 (6e-9)</span></span>
</span>
</td>
<td id="A2.T4.5.20.15.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.4.1.1" class="ltx_p"><span id="A2.T4.5.20.15.4.1.1.1" class="ltx_text" style="font-size:70%;">0.854 (0.007)</span></span>
</span>
</td>
<td id="A2.T4.5.20.15.5" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.5.1.1" class="ltx_p"><span id="A2.T4.5.20.15.5.1.1.1" class="ltx_text" style="font-size:70%;">0.135 (5e-7)</span></span>
</span>
</td>
<td id="A2.T4.5.20.15.6" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.6.1.1" class="ltx_p"><span id="A2.T4.5.20.15.6.1.1.1" class="ltx_text" style="font-size:70%;">0.757 (0.005)</span></span>
</span>
</td>
<td id="A2.T4.5.20.15.7" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r">
<span id="A2.T4.5.20.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.5.20.15.7.1.1" class="ltx_p"><span id="A2.T4.5.20.15.7.1.1.1" class="ltx_text" style="font-size:70%;">0.187 (4e-9)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T4.11.3.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A2.T4.9.2" class="ltx_text" style="font-size:90%;">Detection performance (mAP, <math id="A2.T4.8.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{\text{IoU50}}" display="inline"><semantics id="A2.T4.8.1.m1.1b"><msup id="A2.T4.8.1.m1.1.1" xref="A2.T4.8.1.m1.1.1.cmml"><mtext id="A2.T4.8.1.m1.1.1.2" xref="A2.T4.8.1.m1.1.1.2a.cmml">mAP</mtext><mtext id="A2.T4.8.1.m1.1.1.3" xref="A2.T4.8.1.m1.1.1.3a.cmml">IoU50</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T4.8.1.m1.1c"><apply id="A2.T4.8.1.m1.1.1.cmml" xref="A2.T4.8.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T4.8.1.m1.1.1.1.cmml" xref="A2.T4.8.1.m1.1.1">superscript</csymbol><ci id="A2.T4.8.1.m1.1.1.2a.cmml" xref="A2.T4.8.1.m1.1.1.2"><mtext id="A2.T4.8.1.m1.1.1.2.cmml" xref="A2.T4.8.1.m1.1.1.2">mAP</mtext></ci><ci id="A2.T4.8.1.m1.1.1.3a.cmml" xref="A2.T4.8.1.m1.1.1.3"><mtext mathsize="70%" id="A2.T4.8.1.m1.1.1.3.cmml" xref="A2.T4.8.1.m1.1.1.3">IoU50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.8.1.m1.1d">\text{mAP}^{\text{IoU50}}</annotation></semantics></math>, <math id="A2.T4.9.2.m2.1" class="ltx_Math" alttext="\text{mAR}^{\text{max=100}}" display="inline"><semantics id="A2.T4.9.2.m2.1b"><msup id="A2.T4.9.2.m2.1.1" xref="A2.T4.9.2.m2.1.1.cmml"><mtext id="A2.T4.9.2.m2.1.1.2" xref="A2.T4.9.2.m2.1.1.2a.cmml">mAR</mtext><mtext id="A2.T4.9.2.m2.1.1.3" xref="A2.T4.9.2.m2.1.1.3a.cmml">max=100</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T4.9.2.m2.1c"><apply id="A2.T4.9.2.m2.1.1.cmml" xref="A2.T4.9.2.m2.1.1"><csymbol cd="ambiguous" id="A2.T4.9.2.m2.1.1.1.cmml" xref="A2.T4.9.2.m2.1.1">superscript</csymbol><ci id="A2.T4.9.2.m2.1.1.2a.cmml" xref="A2.T4.9.2.m2.1.1.2"><mtext id="A2.T4.9.2.m2.1.1.2.cmml" xref="A2.T4.9.2.m2.1.1.2">mAR</mtext></ci><ci id="A2.T4.9.2.m2.1.1.3a.cmml" xref="A2.T4.9.2.m2.1.1.3"><mtext mathsize="70%" id="A2.T4.9.2.m2.1.1.3.cmml" xref="A2.T4.9.2.m2.1.1.3">max=100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.9.2.m2.1d">\text{mAR}^{\text{max=100}}</annotation></semantics></math>) evaluated on the testing set of the UnityGrocreies-Real dataset. This table reports the mean and standard deviation of all metrics over 5 repeated model training procedures, for each of the included combinations of real and synthetic data.</span></figcaption>
</figure>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="Sharelatex Example"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.04258" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.04259" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.04259">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.04259" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.04260" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 12:41:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
