<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1703.09684] An Analysis of Visual Question Answering Algorithms</title><meta property="og:description" content="In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Analysis of Visual Question Answering Algorithms">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Analysis of Visual Question Answering Algorithms">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1703.09684">

<!--Generated on Thu Mar  7 23:55:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An Analysis of Visual Question Answering Algorithms</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kushal Kafle   Christopher Kanan
<br class="ltx_break">Rochester Institute of Technology
<br class="ltx_break">Rochester, New York
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">kk6055,kanan@rit.edu</span>
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset called the Task Driven Image Understanding Challenge (TDIUC), which has over 1.6 million questions organized into 12 different categories (available for download at <a target="_blank" href="https://goo.gl/Ng9ix4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://goo.gl/Ng9ix4</a>). We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. VQA is an exciting computer vision problem that requires a system to be capable of many tasks. Truly solving VQA would be a milestone in artificial intelligence, and would significantly advance human computer interaction. However, VQA datasets must test a wide range of abilities for progress to be adequately measured.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA research began in earnest in late 2014 when the DAQUAR dataset was released <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved.
On the most popular dataset, ‘The VQA Dataset’ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the best algorithms are now approaching 70% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1703.09684/assets/images/overview.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="181" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">A good VQA benchmark tests a wide range of computer vision tasks in an unbiased manner. In this paper, we propose a new dataset with 12 distinct tasks and evaluation metrics that compensate for bias, so that the strengths and limitations of algorithms can be better measured.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Contributions</span>: Our paper has four major contributions aimed at better analyzing and comparing VQA algorithms: 1) We create a new VQA benchmark dataset where questions are divided into 12 different categories based on the task they solve; 2) We propose two new evaluation metrics that compensate for forms of dataset bias; 3) We balance the number of yes/no object presence detection questions to assess whether a balanced distribution can help algorithms learn better; and 4) We introduce absurd questions that force an algorithm to determine if a question is valid for a given image. We then use the new dataset to re-train and evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, ‘Is the generalization capacity of the algorithms hindered by the bias in the dataset?’, ‘Does the use of spatial attention help answer specific question-types?’, ‘How successful are the VQA algorithms in answering less-common questions?’, and ’Can the VQA algorithms differentiate between real and absurd questions?’</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Prior Natural Image VQA Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Six datasets for VQA with natural images have been released between 2014–2016: DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, The VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Synthetic Datasets that Fight Bias ‣ 2 Background ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows statistics for the other datasets. Following others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we refer to the portion of The VQA Dataset containing natural images as COCO-VQA. Detailed dataset reviews can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">All of the aforementioned VQA datasets are biased. DAQUAR and COCO-QA are small and have a limited variety of question-types. Visual Genome, Visual7W, and COCO-VQA are larger, but they suffer from several biases. Bias takes the form of both the kinds of questions asked and the answers that people give for them. For COCO-VQA, a system trained using only question features achieves 50% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. This suggests that some questions have predictable answers. Without a more nuanced analysis, it is challenging to determine what kinds of questions are more dependent on the image. For datasets made using Mechanical Turk, annotators often ask object recognition questions, e.g., ‘What is in the image?’ or ‘Is there an elephant in the image?’. Note that in the latter example, annotators rarely ask that kind of question unless the object is in the image. On COCO-VQA, 79% of questions beginning with ‘Is there a’ will have ‘yes’ as their ground truth answer.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In 2017, the VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> dataset was introduced. In VQA 2.0, the same question is asked for two different images and annotators are instructed to give opposite answers, which helped reduce language bias. However, in addition to language bias, these datasets are also biased in their distribution of different types of questions and the distribution of answers within each question-type. Existing VQA datasets use performance metrics that treat each test instance with equal value (e.g., simple accuracy). While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This exacerbates the issues with the bias because the question-types that are more likely to be biased are also more common. Questions beginning with ‘Why’ and ‘Where’ are rarely asked by annotators compared to those beginning with ‘Is’ and ’Are’. For example, on COCO-VQA, improving accuracy on ‘Is/Are’ questions by 15% will increase overall accuracy by over 5%, but answering <em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">all</em> ‘Why/Where’ questions correctly will increase accuracy by only 4.1% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean per-class accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Existing benchmarks do not require reporting accuracies across different question-types. Even when they are reported, the question-types can be too coarse to be useful, e.g., ‘yes/no’, ‘number’ and ‘other’ in COCO-VQA. To improve the analysis of the VQA algorithms, we categorize the questions into meaningful types, calculate the sub-scores, and incorporate them in our evaluation metrics.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Datasets that Fight Bias</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Previous works have studied bias in VQA and proposed countermeasures. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, the Yin and Yang dataset was created to study the effect of having an equal number of binary (yes/no) questions about cartoon images. They found that answering questions from a balanced dataset was harder. This work is significant, but it was limited to yes/no questions and their approach using cartoon imagery cannot be directly extended to real-world images.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">One of the goals of this paper is to determine what kinds of questions an algorithm can answer easily. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the SHAPES dataset was proposed, which has similar objectives. SHAPES is a small dataset, consisting of 64 images that are composed by arranging colored geometric shapes in different spatial orientations. Each image has the same 244 yes/no questions, resulting in 15,616 questions. Although SHAPES serves as an important adjunct evaluation, it alone cannot suffice for testing a VQA algorithm. The major limitation of SHAPES is that all of its images are of 2D shapes, which are not representative of real-world imagery. Along similar lines, Compositional Language and Elementary Visual Reasoning (CLEVR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> also proposes use of 3D rendered geometric objects to study reasoning capacities of a model. CLEVR is larger than SHAPES and makes use of 3D rendered geometric objects. In addition to shape and color, it adds material property to the objects. CLEVR has five types of questions: attribute query, attribute comparison, integer comparison, counting, and existence.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Both SHAPES and CLEVR were specifically tailored for compositional language approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and downplay the importance of visual reasoning. For instance, the CLEVR question, ‘What size is the cylinder that is left of the brown metal thing that is left of the big sphere?’ requires demanding language reasoning capabilities, but only limited visual understanding is needed to parse simple geometric objects. Unlike these three synthetic datasets, our dataset contains natural images and questions. To improve algorithm analysis and comparison, our dataset has more (12) explicitly defined question-types and new evaluation metrics.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.4.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S2.T1.5.2" class="ltx_text" style="font-size:113%;">Comparison of previous natural image VQA datasets with TDIUC. For COCO-VQA, the explicitly defined number of question-types is used, but a much finer granularity would be possible if they were individually classified. MC/OE refers to whether open-ended or multiple-choice evaluation is used.</span></figcaption>
<table id="S2.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.6.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S2.T1.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.6.1.1.2.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Images</span></td>
</tr>
</table>
</th>
<th id="S2.T1.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Questions</span></th>
<th id="S2.T1.6.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.6.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.6.1.1.4.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Annotation</span></td>
</tr>
<tr id="S2.T1.6.1.1.4.1.2" class="ltx_tr">
<td id="S2.T1.6.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Source</span></td>
</tr>
</table>
</th>
<th id="S2.T1.6.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.6.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.6.1.1.5.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question</span></td>
</tr>
<tr id="S2.T1.6.1.1.5.1.2" class="ltx_tr">
<td id="S2.T1.6.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Types</span></td>
</tr>
</table>
</th>
<th id="S2.T1.6.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.6.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.6.1.1.6.1.1" class="ltx_tr">
<td id="S2.T1.6.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Unique</span></td>
</tr>
<tr id="S2.T1.6.1.1.6.1.2" class="ltx_tr">
<td id="S2.T1.6.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.6.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Answers</span></td>
</tr>
</table>
</th>
<th id="S2.T1.6.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.6.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MC/OE</span></th>
</tr>
<tr id="S2.T1.6.2.2" class="ltx_tr">
<td id="S2.T1.6.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.6.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DAQUAR</span></td>
<td id="S2.T1.6.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.6.2.2.2.1" class="ltx_text" style="font-size:80%;">1,449</span></td>
<td id="S2.T1.6.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.6.2.2.3.1" class="ltx_text" style="font-size:80%;">16,590</span></td>
<td id="S2.T1.6.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.2.2.4.1" class="ltx_text" style="font-size:80%;">Both</span></td>
<td id="S2.T1.6.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.2.2.5.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S2.T1.6.2.2.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.6.2.2.6.1" class="ltx_text" style="font-size:80%;">968</span></td>
<td id="S2.T1.6.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.6.2.2.7.1" class="ltx_text" style="font-size:80%;">OE</span></td>
</tr>
<tr id="S2.T1.6.3.3" class="ltx_tr">
<td id="S2.T1.6.3.3.1" class="ltx_td ltx_align_left"><span id="S2.T1.6.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">COCO-QA</span></td>
<td id="S2.T1.6.3.3.2" class="ltx_td ltx_align_right"><span id="S2.T1.6.3.3.2.1" class="ltx_text" style="font-size:80%;">123,287</span></td>
<td id="S2.T1.6.3.3.3" class="ltx_td ltx_align_right"><span id="S2.T1.6.3.3.3.1" class="ltx_text" style="font-size:80%;">117,684</span></td>
<td id="S2.T1.6.3.3.4" class="ltx_td ltx_align_center"><span id="S2.T1.6.3.3.4.1" class="ltx_text" style="font-size:80%;">Auto</span></td>
<td id="S2.T1.6.3.3.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.3.3.5.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S2.T1.6.3.3.6" class="ltx_td ltx_align_right"><span id="S2.T1.6.3.3.6.1" class="ltx_text" style="font-size:80%;">430</span></td>
<td id="S2.T1.6.3.3.7" class="ltx_td ltx_align_center"><span id="S2.T1.6.3.3.7.1" class="ltx_text" style="font-size:80%;">OE</span></td>
</tr>
<tr id="S2.T1.6.4.4" class="ltx_tr">
<td id="S2.T1.6.4.4.1" class="ltx_td ltx_align_left"><span id="S2.T1.6.4.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">COCO-VQA</span></td>
<td id="S2.T1.6.4.4.2" class="ltx_td ltx_align_right"><span id="S2.T1.6.4.4.2.1" class="ltx_text" style="font-size:80%;">204,721</span></td>
<td id="S2.T1.6.4.4.3" class="ltx_td ltx_align_right"><span id="S2.T1.6.4.4.3.1" class="ltx_text" style="font-size:80%;">614,163</span></td>
<td id="S2.T1.6.4.4.4" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.4.1" class="ltx_text" style="font-size:80%;">Manual</span></td>
<td id="S2.T1.6.4.4.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.5.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S2.T1.6.4.4.6" class="ltx_td ltx_align_right"><span id="S2.T1.6.4.4.6.1" class="ltx_text" style="font-size:80%;">145,172</span></td>
<td id="S2.T1.6.4.4.7" class="ltx_td ltx_align_center"><span id="S2.T1.6.4.4.7.1" class="ltx_text" style="font-size:80%;">Both</span></td>
</tr>
<tr id="S2.T1.6.5.5" class="ltx_tr">
<td id="S2.T1.6.5.5.1" class="ltx_td ltx_align_left"><span id="S2.T1.6.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Visual7W</span></td>
<td id="S2.T1.6.5.5.2" class="ltx_td ltx_align_right"><span id="S2.T1.6.5.5.2.1" class="ltx_text" style="font-size:80%;">47,300</span></td>
<td id="S2.T1.6.5.5.3" class="ltx_td ltx_align_right"><span id="S2.T1.6.5.5.3.1" class="ltx_text" style="font-size:80%;">327,939</span></td>
<td id="S2.T1.6.5.5.4" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.4.1" class="ltx_text" style="font-size:80%;">Manual</span></td>
<td id="S2.T1.6.5.5.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.5.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="S2.T1.6.5.5.6" class="ltx_td ltx_align_right"><span id="S2.T1.6.5.5.6.1" class="ltx_text" style="font-size:80%;">25,553</span></td>
<td id="S2.T1.6.5.5.7" class="ltx_td ltx_align_center"><span id="S2.T1.6.5.5.7.1" class="ltx_text" style="font-size:80%;">MC</span></td>
</tr>
<tr id="S2.T1.6.6.6" class="ltx_tr">
<td id="S2.T1.6.6.6.1" class="ltx_td ltx_align_left"><span id="S2.T1.6.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Visual Genome</span></td>
<td id="S2.T1.6.6.6.2" class="ltx_td ltx_align_right"><span id="S2.T1.6.6.6.2.1" class="ltx_text" style="font-size:80%;">108,000</span></td>
<td id="S2.T1.6.6.6.3" class="ltx_td ltx_align_right"><span id="S2.T1.6.6.6.3.1" class="ltx_text" style="font-size:80%;">1,773,358</span></td>
<td id="S2.T1.6.6.6.4" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.4.1" class="ltx_text" style="font-size:80%;">Manual</span></td>
<td id="S2.T1.6.6.6.5" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.5.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S2.T1.6.6.6.6" class="ltx_td ltx_align_right"><span id="S2.T1.6.6.6.6.1" class="ltx_text" style="font-size:80%;">207,675</span></td>
<td id="S2.T1.6.6.6.7" class="ltx_td ltx_align_center"><span id="S2.T1.6.6.6.7.1" class="ltx_text" style="font-size:80%;">OE</span></td>
</tr>
<tr id="S2.T1.6.7.7" class="ltx_tr">
<td id="S2.T1.6.7.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T1.6.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TDIUC (This Paper)</span></td>
<td id="S2.T1.6.7.7.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S2.T1.6.7.7.2.1" class="ltx_text" style="font-size:80%;">167,437</span></td>
<td id="S2.T1.6.7.7.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S2.T1.6.7.7.3.1" class="ltx_text" style="font-size:80%;">1,654,167</span></td>
<td id="S2.T1.6.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.6.7.7.4.1" class="ltx_text" style="font-size:80%;">Both</span></td>
<td id="S2.T1.6.7.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.6.7.7.5.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S2.T1.6.7.7.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="S2.T1.6.7.7.6.1" class="ltx_text" style="font-size:80%;">1,618</span></td>
<td id="S2.T1.6.7.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.6.7.7.7.1" class="ltx_text" style="font-size:80%;">OE</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>TDIUC for Nuanced VQA Analysis</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the past two years, multiple publicly released datasets have spurred the VQA research. However, due to the biases and issues with evaluation metrics, interpreting and comparing the performance of VQA systems can be opaque. We propose a new benchmark dataset that explicitly assigns questions into 12 distinct categories. This enables measuring performance within each category and understand which kind of questions are easy or hard for today’s best systems. Additionally, we use evaluation metrics that further compensate for the biases. We call the dataset the Task Driven Image Understanding Challenge (TDIUC). The overall statistics and example images of this dataset are shown in
Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Synthetic Datasets that Fight Bias ‣ 2 Background ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 TDIUC for Nuanced VQA Analysis ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> respectively.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1703.09684/assets/images/fig1.jpg" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="138" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.15.1.1" class="ltx_text" style="font-size:113%;">(a)</span> </span><span id="S3.F2.sf1.16.2" class="ltx_text ltx_font_bold">Q:</span> What color is the suitcase? <span id="S3.F2.sf1.17.3" class="ltx_text ltx_font_bold">A:</span> Absurd <span id="S3.F2.sf1.18.4" class="ltx_text ltx_font_bold">Q:</span> What color is the man’s hat? <span id="S3.F2.sf1.19.5" class="ltx_text ltx_font_bold">A: </span>White <span id="S3.F2.sf1.20.6" class="ltx_text ltx_font_bold">Q:</span> What sport is this? <span id="S3.F2.sf1.21.7" class="ltx_text ltx_font_bold">A:</span> Tennis</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1703.09684/assets/images/fig2.jpg" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="138" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.15.1.1" class="ltx_text" style="font-size:113%;">(b)</span> </span><span id="S3.F2.sf2.16.2" class="ltx_text ltx_font_bold">Q:</span> What is to the left of the blue bus? <span id="S3.F2.sf2.17.3" class="ltx_text ltx_font_bold">A:</span> Car <span id="S3.F2.sf2.18.4" class="ltx_text ltx_font_bold">Q:</span> Is there a train in the photo? <span id="S3.F2.sf2.19.5" class="ltx_text ltx_font_bold">A:</span> No <span id="S3.F2.sf2.20.6" class="ltx_text ltx_font_bold">Q:</span> How many bicycles are there? <span id="S3.F2.sf2.21.7" class="ltx_text ltx_font_bold">A:</span> One</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text" style="font-size:113%;">Figure 2</span>: </span><span id="S3.F2.5.2" class="ltx_text" style="font-size:113%;">Images from TDIUC and their corresponding question-answer pairs.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">TDIUC has 12 question-types that were chosen to represent both classical computer vision tasks and novel high-level vision tasks which require varying degrees of image understanding and reasoning. The question-types are:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Object Presence (e.g., ‘Is there a cat in the image?’)</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Subordinate Object Recognition (e.g., ‘What kind of furniture is in the picture?’)</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Counting (e.g., ’How many horses are there?’)</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Color Attributes (e.g., ‘What color is the man’s tie?’)</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Other Attributes (e.g., ‘What shape is the clock?’)</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Activity Recognition (e.g., ‘What is the girl doing?’)</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p">Sport Recognition (e.g.,‘What are they playing?’)</p>
</div>
</li>
<li id="S3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="S3.I1.i8.p1" class="ltx_para">
<p id="S3.I1.i8.p1.1" class="ltx_p">Positional Reasoning (e.g., ‘What is to the left of the man on the sofa?’)</p>
</div>
</li>
<li id="S3.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="S3.I1.i9.p1" class="ltx_para">
<p id="S3.I1.i9.p1.1" class="ltx_p">Scene Classification (e.g., ‘What room is this?’)</p>
</div>
</li>
<li id="S3.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="S3.I1.i10.p1" class="ltx_para">
<p id="S3.I1.i10.p1.1" class="ltx_p">Sentiment Understanding (e.g.,‘How is she feeling?’)</p>
</div>
</li>
<li id="S3.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span> 
<div id="S3.I1.i11.p1" class="ltx_para">
<p id="S3.I1.i11.p1.1" class="ltx_p">Object Utilities and Affordances (e.g.,‘What object can be used to break glass?’)</p>
</div>
</li>
<li id="S3.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span> 
<div id="S3.I1.i12.p1" class="ltx_para">
<p id="S3.I1.i12.p1.1" class="ltx_p">Absurd (i.e., Nonsensical queries about the image)</p>
</div>
</li>
</ol>
<p id="S3.p2.2" class="ltx_p">The number of each question-type in TDIUC is given in Table <a href="#S3.T2" title="Table 2 ‣ 3.4 Post Processing ‣ 3 TDIUC for Nuanced VQA Analysis ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The questions come from three sources. First, we imported a subset of questions from COCO-VQA and Visual Genome. Second, we created algorithms that generated questions from COCO’s semantic segmentation annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and Visual Genome’s objects and attributes annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Third, we used human annotators for certain question-types. In the following sections, we briefly describe each of these methods.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Importing Questions from Existing Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We imported questions from COCO-VQA and Visual Genome belonging to all question-types except ‘object utilities and affordances’. We did this by using a large number of templates and regular expressions. For Visual Genome, we imported questions that had one word answers. For COCO-VQA, we imported questions with one or two word answers and in which five or more annotators agreed.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For color questions, a question would be imported if it contained the word ‘color’ in it and the answer was a commonly used color. Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with ‘How many’ and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form ‘Are <svg version="1.1" width="4.23" height="9.61" overflow="visible"><g transform="translate(0,9.61) scale(1,-1)"><path d="M 0,0 20.76,0" stroke="#000000" stroke-width="0.4"></path><text x="0" y="0" transform="scale(1, -1)" fill="black">feeling <g transform="translate(0,9.61) scale(1,-1)"><path d="M 0,0 20.76,0" stroke="#000000" stroke-width="0.4"></path><text x="0" y="0" transform="scale(1, -1)" fill="black">?’ was classified as sentiment understanding and ‘What is to the right of/left of/ behind the <g transform="translate(0,9.61) scale(1,-1)"><path d="M 0,0 20.76,0" stroke="#000000" stroke-width="0.4"></path><text x="0" y="0" transform="scale(1, -1)" fill="black">?’ was classified as positional reasoning. Similarly, ‘What <text x="0" y="0" transform="scale(1, -1)" fill="black">&lt;OBJECT CATEGORY&gt;</text> is in the image?’ and similar templates were used to populate subordinate object recognition questions. This method was used for questions about the season and weather as well, e.g., ‘What season is this?’, ‘Is this rainy/sunny/cloudy?’, or ‘What is the weather like?’ were imported
to scene classification.</text></g></text></g></text></g></svg></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Generating Questions using Image Annotations</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Images in the COCO dataset and Visual Genome both have individual regions with semantic knowledge attached to them. We exploit this information to generate new questions using question templates. To introduce variety, we define multiple templates for each question-type and use the annotations to populate them. For example, for counting we use <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">8</annotation></semantics></math> templates, e.g., ‘How many <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;objects&gt;</span> are there?’, ‘How many <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">&lt;objects&gt;</span> are in the photo?’, etc. Since the COCO and Visual Genome use different annotation formats, we discuss them separately.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Questions Using COCO annotations</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Sport recognition, counting, subordinate object recognition, object presence, scene understanding, positional reasoning, and absurd questions were created from COCO, similar to the scheme used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. For <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">counting</span>, we count the number of object instances in an image annotation. To minimize ambiguity, this was only done if objects covered an area of at least 2,000 pixels.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">For <span id="S3.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">subordinate object recognition</span>, we create questions that require identifying an object’s subordinate-level object classification based on its larger semantic category. To do this, we use COCO supercategories, which are semantic concepts encompassing several objects under a common theme, e.g., the supercategory ‘furniture’ contains chair, couch, etc. If the image contains only one type of furniture, then a question similar to ‘What kind of furniture is in the picture?’ is generated because the answer is not ambiguous. Using similar heuristics, we create questions about identifying food, electronic appliances, kitchen appliances, animals, and vehicles.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">For <span id="S3.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">object presence</span> questions, we find images with objects that have an area larger than 2,000 pixels and produce a question similar to ‘Is there a <span id="S3.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> in the picture?’ These questions will have ‘yes’ as an answer. To create negative questions, we ask questions about COCO objects that are not present in an image. To make this harder, we prioritize the creation of questions referring to absent objects that belong to the same supercategory of objects that are present in the image. A street scene is more likely to contain trucks and cars than it is to contain couches and televisions. Therefore, it is more difficult to answer ‘Is there a truck?’ in a street scene than it is to answer ‘Is there a couch?’</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">For <span id="S3.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">sport recognition</span> questions, we detect the presence of specific sports equipment in the annotations and ask questions about the type of sport being played. Images must only contain sports equipment for one particular sport. A similar approach was used to create scene understanding questions. For example, if a toilet and a sink are present in annotations, the room is a bathroom and an appropriate scene recognition question can be created. Additionally, we use the supercategories ‘indoor’ and ‘outdoor’ to ask questions about where a photo was taken.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">For creating <span id="S3.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">positional reasoning</span> questions, we use the relative locations of bounding boxes to create questions similar to ‘What is to the left/right of <span id="S3.SS2.SSS1.p5.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span>?’ This can be ambiguous due to overlapping objects, so we employ the following heuristics to eliminate ambiguity: 1) The vertical separation between the two bounding boxes should be within a small threshold; 2) The objects should not overlap by more than the half the length of its counterpart; and 3) The objects should not be horizontally separated by more than a distance threshold, determined by subjectively judging optimal separation to reduce ambiguity. We tried to generate above/below questions, but the results were unreliable.</p>
</div>
<div id="S3.SS2.SSS1.p6" class="ltx_para">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p"><span id="S3.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Absurd questions</span> test the ability of an algorithm to judge when a question is not answerable based on the image’s content. To make these, we make a list of the objects that are absent from a given image, and then we find questions from rest of TDIUC that ask about these absent objects, with the exception of yes/no and counting questions. This includes questions imported from COCO-VQA, auto-generated questions, and manually created questions. We make a list of all possible questions that would be ‘absurd’ for each image and we uniformly sample three questions per image. In effect, we will have same question repeated multiple times throughout the dataset, where it can either be a genuine question or a nonsensical question. The algorithm must answer ‘Does Not Apply’ if the question is absurd.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Questions Using Visual Genome annotations</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Visual Genome’s annotations contain region descriptions, relationship graphs, and object boundaries. However, the annotations can be both non-exhaustive and duplicated, which makes using them to automatically make QA pairs difficult. We only use Visual Genome to make color and positional reasoning questions. The methods we used are similar to those used with COCO, but additional precautions were needed due to quirks in their annotations. Additional details are provided in the Appendix.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Manual Annotation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these. Twelve volunteer annotators were trained to generate these questions, and they used a web-based annotation tool that we developed. They were shown random images from COCO and Visual Genome and could also upload images.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Post Processing</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Post processing was performed on questions from all sources. All numbers were converted to text, e.g., 2 became two. All answers were converted to lowercase, and trailing punctuation was stripped. Duplicate questions for the same image were removed. All questions had to have answers that appeared at least twice. The dataset was split into train and test splits with 70% for train and 30% for test.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.4.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S3.T2.5.2" class="ltx_text" style="font-size:113%;">The number of questions per type in TDIUC.</span></figcaption>
<table id="S3.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.6.1.1" class="ltx_tr">
<td id="S3.T2.6.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T2.6.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Questions</span></th>
<th id="S3.T2.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Unique Answers</span></th>
</tr>
<tr id="S3.T2.6.2.2" class="ltx_tr">
<td id="S3.T2.6.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Scene Recognition</span></td>
<td id="S3.T2.6.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.6.2.2.2.1" class="ltx_text" style="font-size:80%;">66,706</span></td>
<td id="S3.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.6.2.2.3.1" class="ltx_text" style="font-size:80%;">83</span></td>
</tr>
<tr id="S3.T2.6.3.3" class="ltx_tr">
<td id="S3.T2.6.3.3.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.3.3.1.1" class="ltx_text" style="font-size:80%;">Sport Recognition</span></td>
<td id="S3.T2.6.3.3.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.3.3.2.1" class="ltx_text" style="font-size:80%;">31,644</span></td>
<td id="S3.T2.6.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.3.3.3.1" class="ltx_text" style="font-size:80%;">12</span></td>
</tr>
<tr id="S3.T2.6.4.4" class="ltx_tr">
<td id="S3.T2.6.4.4.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Color Attributes</span></td>
<td id="S3.T2.6.4.4.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.4.4.2.1" class="ltx_text" style="font-size:80%;">195,564</span></td>
<td id="S3.T2.6.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.4.4.3.1" class="ltx_text" style="font-size:80%;">16</span></td>
</tr>
<tr id="S3.T2.6.5.5" class="ltx_tr">
<td id="S3.T2.6.5.5.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Other Attributes</span></td>
<td id="S3.T2.6.5.5.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.5.5.2.1" class="ltx_text" style="font-size:80%;">28,676</span></td>
<td id="S3.T2.6.5.5.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.5.5.3.1" class="ltx_text" style="font-size:80%;">625</span></td>
</tr>
<tr id="S3.T2.6.6.6" class="ltx_tr">
<td id="S3.T2.6.6.6.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.6.6.1.1" class="ltx_text" style="font-size:80%;">Activity Recognition</span></td>
<td id="S3.T2.6.6.6.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.6.6.2.1" class="ltx_text" style="font-size:80%;">8,530</span></td>
<td id="S3.T2.6.6.6.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.6.6.3.1" class="ltx_text" style="font-size:80%;">13</span></td>
</tr>
<tr id="S3.T2.6.7.7" class="ltx_tr">
<td id="S3.T2.6.7.7.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.7.7.1.1" class="ltx_text" style="font-size:80%;">Positional Reasoning</span></td>
<td id="S3.T2.6.7.7.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.7.7.2.1" class="ltx_text" style="font-size:80%;">38,326</span></td>
<td id="S3.T2.6.7.7.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.7.7.3.1" class="ltx_text" style="font-size:80%;">1,300</span></td>
</tr>
<tr id="S3.T2.6.8.8" class="ltx_tr">
<td id="S3.T2.6.8.8.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.8.8.1.1" class="ltx_text" style="font-size:80%;">Sub. Object Recognition</span></td>
<td id="S3.T2.6.8.8.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.8.8.2.1" class="ltx_text" style="font-size:80%;">93,555</span></td>
<td id="S3.T2.6.8.8.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.8.8.3.1" class="ltx_text" style="font-size:80%;">385</span></td>
</tr>
<tr id="S3.T2.6.9.9" class="ltx_tr">
<td id="S3.T2.6.9.9.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.9.9.1.1" class="ltx_text" style="font-size:80%;">Absurd</span></td>
<td id="S3.T2.6.9.9.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.9.9.2.1" class="ltx_text" style="font-size:80%;">366,654</span></td>
<td id="S3.T2.6.9.9.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.9.9.3.1" class="ltx_text" style="font-size:80%;">1</span></td>
</tr>
<tr id="S3.T2.6.10.10" class="ltx_tr">
<td id="S3.T2.6.10.10.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.10.10.1.1" class="ltx_text" style="font-size:80%;">Utility/Affordance</span></td>
<td id="S3.T2.6.10.10.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.10.10.2.1" class="ltx_text" style="font-size:80%;">521</span></td>
<td id="S3.T2.6.10.10.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.10.10.3.1" class="ltx_text" style="font-size:80%;">187</span></td>
</tr>
<tr id="S3.T2.6.11.11" class="ltx_tr">
<td id="S3.T2.6.11.11.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.11.11.1.1" class="ltx_text" style="font-size:80%;">Object Presence</span></td>
<td id="S3.T2.6.11.11.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.11.11.2.1" class="ltx_text" style="font-size:80%;">657,134</span></td>
<td id="S3.T2.6.11.11.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.11.11.3.1" class="ltx_text" style="font-size:80%;">2</span></td>
</tr>
<tr id="S3.T2.6.12.12" class="ltx_tr">
<td id="S3.T2.6.12.12.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.12.12.1.1" class="ltx_text" style="font-size:80%;">Counting</span></td>
<td id="S3.T2.6.12.12.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.12.12.2.1" class="ltx_text" style="font-size:80%;">164,762</span></td>
<td id="S3.T2.6.12.12.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.12.12.3.1" class="ltx_text" style="font-size:80%;">16</span></td>
</tr>
<tr id="S3.T2.6.13.13" class="ltx_tr">
<td id="S3.T2.6.13.13.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.13.13.1.1" class="ltx_text" style="font-size:80%;">Sentiment Understanding</span></td>
<td id="S3.T2.6.13.13.2" class="ltx_td ltx_align_right"><span id="S3.T2.6.13.13.2.1" class="ltx_text" style="font-size:80%;">2,095</span></td>
<td id="S3.T2.6.13.13.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.13.13.3.1" class="ltx_text" style="font-size:80%;">54</span></td>
</tr>
<tr id="S3.T2.6.14.14" class="ltx_tr">
<td id="S3.T2.6.14.14.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.6.14.14.1.1" class="ltx_text" style="font-size:80%;">Grand Total</span></td>
<td id="S3.T2.6.14.14.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T2.6.14.14.2.1" class="ltx_text" style="font-size:80%;">1,654,167</span></td>
<td id="S3.T2.6.14.14.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T2.6.14.14.3.1" class="ltx_text" style="font-size:80%;">1,618</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Evaluation Metric</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">One of the main goals of VQA research is to build computer vision systems capable of many tasks, instead of only having expertise at one specific task (e.g., object recognition). For this reason, some have argued that VQA is a kind of Visual Turing Test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. However, if simple accuracy is used for evaluating performance, then it is hard to know if a system succeeds at this goal because some question-types have far more questions than others. In VQA, skewed distributions of question-types are to be expected. If each test question is treated equally, then it is difficult to assess performance on rarer question-types and to compensate for bias. We propose multiple measures to compensate for bias and skewed distributions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To compensate for the skewed question-type distribution, we compute accuracy for each of the 12 question-types separately. However, it is also important to have a final unified accuracy metric. Our overall metrics are the arithmetic and harmonic means across all per question-type accuracies, referred to as arithmetic mean-per-type (Arithmetic MPT) accuracy and harmonic mean-per-type accuracy (Harmonic MPT). Unlike the Arithmetic MPT, Harmonic MPT measures the ability of a system to have high scores across all question-types and is skewed towards lowest performing categories.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We also use normalized metrics that compensate for bias in the form of imbalance in the distribution of answers within each question-type, e.g., the most repeated answer ‘two’ covers over 35% of all the counting-type questions. To do this, we compute the accuracy for each unique answer separately within a question-type and then average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean per-type (N-MPT) and harmonic N-MPT scores. A large discrepancy between unnormalized and normalized scores suggests an algorithm is not generalizing to rarer answers.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Algorithms for VQA</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">While there are alternative formulations (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Almost all systems use CNN features to represent the image and either a recurrent neural network (RNN) or a bag-of-words model for the question. We briefly review some of these systems, focusing on the models we compare in experiments. For a more comprehensive review, see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, where the image features come from the last hidden layer of a CNN. These simple approaches often work well and can be competitive with complex attentive models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Spatial attention has been heavily investigated in VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. These systems weigh the visual features based on their relevance to the question, instead of using global features, e.g., from the last hidden layer of a CNN. For example, to answer ‘What color is the bear?’ they aim emphasize the visual features around the bear and suppress other features.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The MCB system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> won the CVPR-2016 VQA Workshop Challenge. In addition to using spatial attention, it implicitly computes the outer product between the image and question features to ensure that all of their elements interact. Explicitly computing the outer product would be slow and extremely high dimensional, so it is done using an efficient approximation. It uses an long short-term memory (LSTM) networks to embed the question.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The neural module network (NMN) is an especially interesting compositional approach to VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The main idea is to compose a series of discrete modules (sub-networks) that can be executed collectively to answer a given question. To achieve this, they use a variety of modules, e.g., the <span id="S5.p5.1.1" class="ltx_text ltx_font_typewriter">find(x)</span> module outputs a heat map for detecting <math id="S5.p5.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.p5.1.m1.1a"><mi id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><ci id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">x</annotation></semantics></math>. To arrange the modules, the question is first parsed into a concise expression (called an S-expression), e.g., ‘What is to the right of the car?’ is parsed into <span id="S5.p5.1.2" class="ltx_text ltx_font_typewriter">(what car);(what right);(what (and car right))</span>. Using these expressions, modules are composed into a sequence to answer the query.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">The multi-step recurrent answering units (RAU) model for VQA is another state-of-the-art method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Each inference step in RAU consists of a complete answering block that takes in an image, a question, and the output from the previous LSTM step. Each of these is part of a larger LSTM network that progressively reasons about the question.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.6.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S6.T3.7.2" class="ltx_text" style="font-size:113%;">Results for all VQA models. The unnormalized accuracy for each question-type is shown. Overall performance is reported using 5 metrics. Overall (Arithmetic MPT) and Overall (Harmonic MPT) are averages of these sub-scores, providing a clearer picture of performance across question-types than simple accuracy. Overall Arithmetic N-MPT and Harmonic N-MPT normalize across unique answers to better analyze the impact of answer imbalance (see Sec. <a href="#S4" title="4 Proposed Evaluation Metric ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Normalized scores for individual question-types are presented in the appendix table <a href="#A1.T5" title="Table 5 ‣ A.3 Train and Test Split ‣ Appendix A Additional Details About TDIUC ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. * denotes training without absurd questions.</span></figcaption>
<table id="S6.T3.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.8.1.1" class="ltx_tr">
<th id="S6.T3.8.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S6.T3.8.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">YES</span></td>
<td id="S6.T3.8.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">REP</span></td>
<td id="S6.T3.8.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IMG</span></td>
<td id="S6.T3.8.1.1.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">QUES</span></td>
<td id="S6.T3.8.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q+I</span></td>
<td id="S6.T3.8.1.1.7" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">*Q+I</span></td>
<td id="S6.T3.8.1.1.8" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MLP</span></td>
<td id="S6.T3.8.1.1.9" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
<td id="S6.T3.8.1.1.10" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">*MCB</span></td>
<td id="S6.T3.8.1.1.11" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB-A</span></td>
<td id="S6.T3.8.1.1.12" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">NMN</span></td>
<td id="S6.T3.8.1.1.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt"><span id="S6.T3.8.1.1.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RAU</span></td>
</tr>
<tr id="S6.T3.8.2.2" class="ltx_tr">
<th id="S6.T3.8.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T3.8.2.2.1.1" class="ltx_text" style="font-size:80%;">Scene Recognition</span></th>
<td id="S6.T3.8.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.2.1" class="ltx_text" style="font-size:80%;">26.90</span></td>
<td id="S6.T3.8.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.3.1" class="ltx_text" style="font-size:80%;">26.90</span></td>
<td id="S6.T3.8.2.2.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.4.1" class="ltx_text" style="font-size:80%;">14.25</span></td>
<td id="S6.T3.8.2.2.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.5.1" class="ltx_text" style="font-size:80%;">53.18</span></td>
<td id="S6.T3.8.2.2.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.6.1" class="ltx_text" style="font-size:80%;">72.19</span></td>
<td id="S6.T3.8.2.2.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.7.1" class="ltx_text" style="font-size:80%;">72.75</span></td>
<td id="S6.T3.8.2.2.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.8.1" class="ltx_text" style="font-size:80%;">91.45</span></td>
<td id="S6.T3.8.2.2.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.9.1" class="ltx_text" style="font-size:80%;">92.04</span></td>
<td id="S6.T3.8.2.2.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.10.1" class="ltx_text" style="font-size:80%;">91.87</span></td>
<td id="S6.T3.8.2.2.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.11.1" class="ltx_text" style="font-size:80%;">93.06</span></td>
<td id="S6.T3.8.2.2.12" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.12.1" class="ltx_text" style="font-size:80%;">91.88</span></td>
<td id="S6.T3.8.2.2.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S6.T3.8.2.2.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">93.96</span></td>
</tr>
<tr id="S6.T3.8.3.3" class="ltx_tr">
<th id="S6.T3.8.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.3.3.1.1" class="ltx_text" style="font-size:80%;">Sport Recognition</span></th>
<td id="S6.T3.8.3.3.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.3.3.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.3.1" class="ltx_text" style="font-size:80%;">22.05</span></td>
<td id="S6.T3.8.3.3.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.4.1" class="ltx_text" style="font-size:80%;">18.61</span></td>
<td id="S6.T3.8.3.3.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.5.1" class="ltx_text" style="font-size:80%;">18.87</span></td>
<td id="S6.T3.8.3.3.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.6.1" class="ltx_text" style="font-size:80%;">85.16</span></td>
<td id="S6.T3.8.3.3.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.7.1" class="ltx_text" style="font-size:80%;">89.40</span></td>
<td id="S6.T3.8.3.3.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.8.1" class="ltx_text" style="font-size:80%;">90.24</span></td>
<td id="S6.T3.8.3.3.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.9.1" class="ltx_text" style="font-size:80%;">92.47</span></td>
<td id="S6.T3.8.3.3.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.10.1" class="ltx_text" style="font-size:80%;">92.47</span></td>
<td id="S6.T3.8.3.3.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.11.1" class="ltx_text" style="font-size:80%;">92.77</span></td>
<td id="S6.T3.8.3.3.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.3.3.12.1" class="ltx_text" style="font-size:80%;">89.99</span></td>
<td id="S6.T3.8.3.3.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.3.3.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">93.47</span></td>
</tr>
<tr id="S6.T3.8.4.4" class="ltx_tr">
<th id="S6.T3.8.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.4.4.1.1" class="ltx_text" style="font-size:80%;">Color Attributes</span></th>
<td id="S6.T3.8.4.4.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.4.4.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.3.1" class="ltx_text" style="font-size:80%;">22.74</span></td>
<td id="S6.T3.8.4.4.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.4.1" class="ltx_text" style="font-size:80%;">0.92</span></td>
<td id="S6.T3.8.4.4.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.5.1" class="ltx_text" style="font-size:80%;">37.60</span></td>
<td id="S6.T3.8.4.4.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.6.1" class="ltx_text" style="font-size:80%;">43.69</span></td>
<td id="S6.T3.8.4.4.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.7.1" class="ltx_text" style="font-size:80%;">50.52</span></td>
<td id="S6.T3.8.4.4.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.8.1" class="ltx_text" style="font-size:80%;">53.64</span></td>
<td id="S6.T3.8.4.4.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.9.1" class="ltx_text" style="font-size:80%;">56.93</span></td>
<td id="S6.T3.8.4.4.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.10.1" class="ltx_text" style="font-size:80%;">57.07</span></td>
<td id="S6.T3.8.4.4.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">68.54</span></td>
<td id="S6.T3.8.4.4.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.4.4.12.1" class="ltx_text" style="font-size:80%;">54.91</span></td>
<td id="S6.T3.8.4.4.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.4.4.13.1" class="ltx_text" style="font-size:80%;">66.86</span></td>
</tr>
<tr id="S6.T3.8.5.5" class="ltx_tr">
<th id="S6.T3.8.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.5.5.1.1" class="ltx_text" style="font-size:80%;">Other Attributes</span></th>
<td id="S6.T3.8.5.5.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.5.5.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.3.1" class="ltx_text" style="font-size:80%;">24.23</span></td>
<td id="S6.T3.8.5.5.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.4.1" class="ltx_text" style="font-size:80%;">2.07</span></td>
<td id="S6.T3.8.5.5.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.5.1" class="ltx_text" style="font-size:80%;">36.13</span></td>
<td id="S6.T3.8.5.5.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.6.1" class="ltx_text" style="font-size:80%;">42.89</span></td>
<td id="S6.T3.8.5.5.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.7.1" class="ltx_text" style="font-size:80%;">51.47</span></td>
<td id="S6.T3.8.5.5.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.8.1" class="ltx_text" style="font-size:80%;">41.79</span></td>
<td id="S6.T3.8.5.5.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.9.1" class="ltx_text" style="font-size:80%;">53.24</span></td>
<td id="S6.T3.8.5.5.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.10.1" class="ltx_text" style="font-size:80%;">54.62</span></td>
<td id="S6.T3.8.5.5.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">56.72</span></td>
<td id="S6.T3.8.5.5.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.5.5.12.1" class="ltx_text" style="font-size:80%;">47.66</span></td>
<td id="S6.T3.8.5.5.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.5.5.13.1" class="ltx_text" style="font-size:80%;">56.49</span></td>
</tr>
<tr id="S6.T3.8.6.6" class="ltx_tr">
<th id="S6.T3.8.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.6.6.1.1" class="ltx_text" style="font-size:80%;">Activity Recognition</span></th>
<td id="S6.T3.8.6.6.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.6.6.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.3.1" class="ltx_text" style="font-size:80%;">21.63</span></td>
<td id="S6.T3.8.6.6.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.4.1" class="ltx_text" style="font-size:80%;">3.06</span></td>
<td id="S6.T3.8.6.6.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.5.1" class="ltx_text" style="font-size:80%;">10.81</span></td>
<td id="S6.T3.8.6.6.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.6.1" class="ltx_text" style="font-size:80%;">24.16</span></td>
<td id="S6.T3.8.6.6.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.7.1" class="ltx_text" style="font-size:80%;">48.55</span></td>
<td id="S6.T3.8.6.6.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.8.1" class="ltx_text" style="font-size:80%;">39.22</span></td>
<td id="S6.T3.8.6.6.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.9.1" class="ltx_text" style="font-size:80%;">51.42</span></td>
<td id="S6.T3.8.6.6.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">53.58</span></td>
<td id="S6.T3.8.6.6.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.11.1" class="ltx_text" style="font-size:80%;">52.35</span></td>
<td id="S6.T3.8.6.6.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.6.6.12.1" class="ltx_text" style="font-size:80%;">44.26</span></td>
<td id="S6.T3.8.6.6.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.6.6.13.1" class="ltx_text" style="font-size:80%;">51.60</span></td>
</tr>
<tr id="S6.T3.8.7.7" class="ltx_tr">
<th id="S6.T3.8.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.7.7.1.1" class="ltx_text" style="font-size:80%;">Positional Reasoning</span></th>
<td id="S6.T3.8.7.7.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.7.7.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.3.1" class="ltx_text" style="font-size:80%;">6.05</span></td>
<td id="S6.T3.8.7.7.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.4.1" class="ltx_text" style="font-size:80%;">2.23</span></td>
<td id="S6.T3.8.7.7.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.5.1" class="ltx_text" style="font-size:80%;">14.23</span></td>
<td id="S6.T3.8.7.7.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.6.1" class="ltx_text" style="font-size:80%;">25.15</span></td>
<td id="S6.T3.8.7.7.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.7.1" class="ltx_text" style="font-size:80%;">27.73</span></td>
<td id="S6.T3.8.7.7.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.8.1" class="ltx_text" style="font-size:80%;">21.87</span></td>
<td id="S6.T3.8.7.7.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.9.1" class="ltx_text" style="font-size:80%;">33.34</span></td>
<td id="S6.T3.8.7.7.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.10.1" class="ltx_text" style="font-size:80%;">33.02</span></td>
<td id="S6.T3.8.7.7.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">35.40</span></td>
<td id="S6.T3.8.7.7.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.7.7.12.1" class="ltx_text" style="font-size:80%;">27.92</span></td>
<td id="S6.T3.8.7.7.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.7.7.13.1" class="ltx_text" style="font-size:80%;">35.26</span></td>
</tr>
<tr id="S6.T3.8.8.8" class="ltx_tr">
<th id="S6.T3.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.8.8.1.1" class="ltx_text" style="font-size:80%;">Sub. Object Recognition</span></th>
<td id="S6.T3.8.8.8.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.8.8.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.3.1" class="ltx_text" style="font-size:80%;">7.16</span></td>
<td id="S6.T3.8.8.8.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.4.1" class="ltx_text" style="font-size:80%;">10.55</span></td>
<td id="S6.T3.8.8.8.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.5.1" class="ltx_text" style="font-size:80%;">21.40</span></td>
<td id="S6.T3.8.8.8.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.6.1" class="ltx_text" style="font-size:80%;">80.92</span></td>
<td id="S6.T3.8.8.8.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.7.1" class="ltx_text" style="font-size:80%;">81.66</span></td>
<td id="S6.T3.8.8.8.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.8.1" class="ltx_text" style="font-size:80%;">80.55</span></td>
<td id="S6.T3.8.8.8.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.9.1" class="ltx_text" style="font-size:80%;">84.63</span></td>
<td id="S6.T3.8.8.8.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.10.1" class="ltx_text" style="font-size:80%;">84.58</span></td>
<td id="S6.T3.8.8.8.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.11.1" class="ltx_text" style="font-size:80%;">85.54</span></td>
<td id="S6.T3.8.8.8.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.8.8.12.1" class="ltx_text" style="font-size:80%;">82.02</span></td>
<td id="S6.T3.8.8.8.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.8.8.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">86.11</span></td>
</tr>
<tr id="S6.T3.8.9.9" class="ltx_tr">
<th id="S6.T3.8.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.9.9.1.1" class="ltx_text" style="font-size:80%;">Absurd</span></th>
<td id="S6.T3.8.9.9.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.9.9.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">100.00</span></td>
<td id="S6.T3.8.9.9.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.4.1" class="ltx_text" style="font-size:80%;">19.97</span></td>
<td id="S6.T3.8.9.9.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.5.1" class="ltx_text" style="font-size:80%;">96.71</span></td>
<td id="S6.T3.8.9.9.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.6.1" class="ltx_text" style="font-size:80%;">96.98</span></td>
<td id="S6.T3.8.9.9.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.7.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="S6.T3.8.9.9.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.8.1" class="ltx_text" style="font-size:80%;">95.96</span></td>
<td id="S6.T3.8.9.9.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.9.1" class="ltx_text" style="font-size:80%;">83.44</span></td>
<td id="S6.T3.8.9.9.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.10.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="S6.T3.8.9.9.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.11.1" class="ltx_text" style="font-size:80%;">84.82</span></td>
<td id="S6.T3.8.9.9.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.9.9.12.1" class="ltx_text" style="font-size:80%;">87.51</span></td>
<td id="S6.T3.8.9.9.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.9.9.13.1" class="ltx_text" style="font-size:80%;">96.08</span></td>
</tr>
<tr id="S6.T3.8.10.10" class="ltx_tr">
<th id="S6.T3.8.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.10.10.1.1" class="ltx_text" style="font-size:80%;">Utility and Affordances</span></th>
<td id="S6.T3.8.10.10.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.2.1" class="ltx_text" style="font-size:80%;">11.70</span></td>
<td id="S6.T3.8.10.10.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.3.1" class="ltx_text" style="font-size:80%;">11.70</span></td>
<td id="S6.T3.8.10.10.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.4.1" class="ltx_text" style="font-size:80%;">5.26</span></td>
<td id="S6.T3.8.10.10.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.5.1" class="ltx_text" style="font-size:80%;">16.37</span></td>
<td id="S6.T3.8.10.10.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.6.1" class="ltx_text" style="font-size:80%;">24.56</span></td>
<td id="S6.T3.8.10.10.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.7.1" class="ltx_text" style="font-size:80%;">30.99</span></td>
<td id="S6.T3.8.10.10.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.8.1" class="ltx_text" style="font-size:80%;">13.45</span></td>
<td id="S6.T3.8.10.10.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.9.1" class="ltx_text" style="font-size:80%;">33.92</span></td>
<td id="S6.T3.8.10.10.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.10.1" class="ltx_text" style="font-size:80%;">29.24</span></td>
<td id="S6.T3.8.10.10.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">35.09</span></td>
<td id="S6.T3.8.10.10.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.10.10.12.1" class="ltx_text" style="font-size:80%;">25.15</span></td>
<td id="S6.T3.8.10.10.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.10.10.13.1" class="ltx_text" style="font-size:80%;">31.58</span></td>
</tr>
<tr id="S6.T3.8.11.11" class="ltx_tr">
<th id="S6.T3.8.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.11.11.1.1" class="ltx_text" style="font-size:80%;">Object Presence</span></th>
<td id="S6.T3.8.11.11.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.2.1" class="ltx_text" style="font-size:80%;">50.00</span></td>
<td id="S6.T3.8.11.11.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.3.1" class="ltx_text" style="font-size:80%;">50.00</span></td>
<td id="S6.T3.8.11.11.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.4.1" class="ltx_text" style="font-size:80%;">20.73</span></td>
<td id="S6.T3.8.11.11.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.5.1" class="ltx_text" style="font-size:80%;">69.06</span></td>
<td id="S6.T3.8.11.11.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.6.1" class="ltx_text" style="font-size:80%;">69.43</span></td>
<td id="S6.T3.8.11.11.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.7.1" class="ltx_text" style="font-size:80%;">69.50</span></td>
<td id="S6.T3.8.11.11.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.8.1" class="ltx_text" style="font-size:80%;">92.33</span></td>
<td id="S6.T3.8.11.11.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.9.1" class="ltx_text" style="font-size:80%;">91.84</span></td>
<td id="S6.T3.8.11.11.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.10.1" class="ltx_text" style="font-size:80%;">91.55</span></td>
<td id="S6.T3.8.11.11.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.11.1" class="ltx_text" style="font-size:80%;">93.64</span></td>
<td id="S6.T3.8.11.11.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.11.11.12.1" class="ltx_text" style="font-size:80%;">92.50</span></td>
<td id="S6.T3.8.11.11.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.11.11.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.38</span></td>
</tr>
<tr id="S6.T3.8.12.12" class="ltx_tr">
<th id="S6.T3.8.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.12.12.1.1" class="ltx_text" style="font-size:80%;">Counting</span></th>
<td id="S6.T3.8.12.12.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.12.12.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.3.1" class="ltx_text" style="font-size:80%;">36.19</span></td>
<td id="S6.T3.8.12.12.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.4.1" class="ltx_text" style="font-size:80%;">0.30</span></td>
<td id="S6.T3.8.12.12.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.5.1" class="ltx_text" style="font-size:80%;">44.51</span></td>
<td id="S6.T3.8.12.12.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.6.1" class="ltx_text" style="font-size:80%;">44.82</span></td>
<td id="S6.T3.8.12.12.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.7.1" class="ltx_text" style="font-size:80%;">44.84</span></td>
<td id="S6.T3.8.12.12.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.8.1" class="ltx_text" style="font-size:80%;">51.12</span></td>
<td id="S6.T3.8.12.12.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.9.1" class="ltx_text" style="font-size:80%;">50.29</span></td>
<td id="S6.T3.8.12.12.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.10.1" class="ltx_text" style="font-size:80%;">50.07</span></td>
<td id="S6.T3.8.12.12.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">51.01</span></td>
<td id="S6.T3.8.12.12.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.12.12.12.1" class="ltx_text" style="font-size:80%;">49.21</span></td>
<td id="S6.T3.8.12.12.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.12.12.13.1" class="ltx_text" style="font-size:80%;">48.43</span></td>
</tr>
<tr id="S6.T3.8.13.13" class="ltx_tr">
<th id="S6.T3.8.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.13.13.1.1" class="ltx_text" style="font-size:80%;">Sentiment Understanding</span></th>
<td id="S6.T3.8.13.13.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.2.1" class="ltx_text" style="font-size:80%;">44.64</span></td>
<td id="S6.T3.8.13.13.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.3.1" class="ltx_text" style="font-size:80%;">44.64</span></td>
<td id="S6.T3.8.13.13.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.4.1" class="ltx_text" style="font-size:80%;">15.93</span></td>
<td id="S6.T3.8.13.13.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.5.1" class="ltx_text" style="font-size:80%;">52.84</span></td>
<td id="S6.T3.8.13.13.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.6.1" class="ltx_text" style="font-size:80%;">53.00</span></td>
<td id="S6.T3.8.13.13.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.7.1" class="ltx_text" style="font-size:80%;">59.94</span></td>
<td id="S6.T3.8.13.13.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.8.1" class="ltx_text" style="font-size:80%;">58.33</span></td>
<td id="S6.T3.8.13.13.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.9.1" class="ltx_text" style="font-size:80%;">65.46</span></td>
<td id="S6.T3.8.13.13.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.10.1" class="ltx_text" style="font-size:80%;">66.25</span></td>
<td id="S6.T3.8.13.13.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">66.25</span></td>
<td id="S6.T3.8.13.13.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.13.13.12.1" class="ltx_text" style="font-size:80%;">58.04</span></td>
<td id="S6.T3.8.13.13.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.13.13.13.1" class="ltx_text" style="font-size:80%;">60.09</span></td>
</tr>
<tr id="S6.T3.8.14.14" class="ltx_tr">
<th id="S6.T3.8.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T3.8.14.14.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic MPT)</span></th>
<td id="S6.T3.8.14.14.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.2.1" class="ltx_text" style="font-size:80%;">11.10</span></td>
<td id="S6.T3.8.14.14.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.3.1" class="ltx_text" style="font-size:80%;">31.11</span></td>
<td id="S6.T3.8.14.14.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.4.1" class="ltx_text" style="font-size:80%;">9.49</span></td>
<td id="S6.T3.8.14.14.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.5.1" class="ltx_text" style="font-size:80%;">39.31</span></td>
<td id="S6.T3.8.14.14.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.6.1" class="ltx_text" style="font-size:80%;">55.25</span></td>
<td id="S6.T3.8.14.14.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.7.1" class="ltx_text" style="font-size:80%;">57.03</span></td>
<td id="S6.T3.8.14.14.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.8.1" class="ltx_text" style="font-size:80%;">60.87</span></td>
<td id="S6.T3.8.14.14.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.9.1" class="ltx_text" style="font-size:80%;">65.75</span></td>
<td id="S6.T3.8.14.14.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.10.1" class="ltx_text" style="font-size:80%;">66.07</span></td>
<td id="S6.T3.8.14.14.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67.90</span></td>
<td id="S6.T3.8.14.14.12" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.12.1" class="ltx_text" style="font-size:80%;">62.59</span></td>
<td id="S6.T3.8.14.14.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S6.T3.8.14.14.13.1" class="ltx_text" style="font-size:80%;">67.81</span></td>
</tr>
<tr id="S6.T3.8.15.15" class="ltx_tr">
<th id="S6.T3.8.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.15.15.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic MPT)</span></th>
<td id="S6.T3.8.15.15.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.15.15.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.3.1" class="ltx_text" style="font-size:80%;">17.53</span></td>
<td id="S6.T3.8.15.15.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.4.1" class="ltx_text" style="font-size:80%;">1.92</span></td>
<td id="S6.T3.8.15.15.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.5.1" class="ltx_text" style="font-size:80%;">25.93</span></td>
<td id="S6.T3.8.15.15.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.6.1" class="ltx_text" style="font-size:80%;">44.13</span></td>
<td id="S6.T3.8.15.15.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.7.1" class="ltx_text" style="font-size:80%;">50.30</span></td>
<td id="S6.T3.8.15.15.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.8.1" class="ltx_text" style="font-size:80%;">42.80</span></td>
<td id="S6.T3.8.15.15.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.9.1" class="ltx_text" style="font-size:80%;">58.03</span></td>
<td id="S6.T3.8.15.15.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.10.1" class="ltx_text" style="font-size:80%;">55.43</span></td>
<td id="S6.T3.8.15.15.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">60.47</span></td>
<td id="S6.T3.8.15.15.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.15.15.12.1" class="ltx_text" style="font-size:80%;">51.87</span></td>
<td id="S6.T3.8.15.15.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.15.15.13.1" class="ltx_text" style="font-size:80%;">59.00</span></td>
</tr>
<tr id="S6.T3.8.16.16" class="ltx_tr">
<th id="S6.T3.8.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T3.8.16.16.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic N-MPT)</span></th>
<td id="S6.T3.8.16.16.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.2.1" class="ltx_text" style="font-size:80%;">4.87</span></td>
<td id="S6.T3.8.16.16.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.3.1" class="ltx_text" style="font-size:80%;">15.63</span></td>
<td id="S6.T3.8.16.16.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.4.1" class="ltx_text" style="font-size:80%;">5.82</span></td>
<td id="S6.T3.8.16.16.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.5.1" class="ltx_text" style="font-size:80%;">21.46</span></td>
<td id="S6.T3.8.16.16.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.6.1" class="ltx_text" style="font-size:80%;">29.47</span></td>
<td id="S6.T3.8.16.16.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.7.1" class="ltx_text" style="font-size:80%;">28.10</span></td>
<td id="S6.T3.8.16.16.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.8.1" class="ltx_text" style="font-size:80%;">31.36</span></td>
<td id="S6.T3.8.16.16.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.9.1" class="ltx_text" style="font-size:80%;">39.81</span></td>
<td id="S6.T3.8.16.16.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.10.1" class="ltx_text" style="font-size:80%;">35.49</span></td>
<td id="S6.T3.8.16.16.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.24</span></td>
<td id="S6.T3.8.16.16.12" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.12.1" class="ltx_text" style="font-size:80%;">34.00</span></td>
<td id="S6.T3.8.16.16.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S6.T3.8.16.16.13.1" class="ltx_text" style="font-size:80%;">41.04</span></td>
</tr>
<tr id="S6.T3.8.17.17" class="ltx_tr">
<th id="S6.T3.8.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S6.T3.8.17.17.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic N-MPT)</span></th>
<td id="S6.T3.8.17.17.2" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="S6.T3.8.17.17.3" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.3.1" class="ltx_text" style="font-size:80%;">0.83</span></td>
<td id="S6.T3.8.17.17.4" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.4.1" class="ltx_text" style="font-size:80%;">1.91</span></td>
<td id="S6.T3.8.17.17.5" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.5.1" class="ltx_text" style="font-size:80%;">8.42</span></td>
<td id="S6.T3.8.17.17.6" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.6.1" class="ltx_text" style="font-size:80%;">14.99</span></td>
<td id="S6.T3.8.17.17.7" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.7.1" class="ltx_text" style="font-size:80%;">18.30</span></td>
<td id="S6.T3.8.17.17.8" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.8.1" class="ltx_text" style="font-size:80%;">9.46</span></td>
<td id="S6.T3.8.17.17.9" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.9.1" class="ltx_text" style="font-size:80%;">24.77</span></td>
<td id="S6.T3.8.17.17.10" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.10.1" class="ltx_text" style="font-size:80%;">23.20</span></td>
<td id="S6.T3.8.17.17.11" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">27.28</span></td>
<td id="S6.T3.8.17.17.12" class="ltx_td ltx_align_right"><span id="S6.T3.8.17.17.12.1" class="ltx_text" style="font-size:80%;">16.67</span></td>
<td id="S6.T3.8.17.17.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S6.T3.8.17.17.13.1" class="ltx_text" style="font-size:80%;">23.99</span></td>
</tr>
<tr id="S6.T3.8.18.18" class="ltx_tr">
<th id="S6.T3.8.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.1.1" class="ltx_text" style="font-size:80%;">Simple Accuracy</span></th>
<td id="S6.T3.8.18.18.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.2.1" class="ltx_text" style="font-size:80%;">21.14</span></td>
<td id="S6.T3.8.18.18.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.3.1" class="ltx_text" style="font-size:80%;">51.15</span></td>
<td id="S6.T3.8.18.18.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.4.1" class="ltx_text" style="font-size:80%;">14.54</span></td>
<td id="S6.T3.8.18.18.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.5.1" class="ltx_text" style="font-size:80%;">62.74</span></td>
<td id="S6.T3.8.18.18.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.6.1" class="ltx_text" style="font-size:80%;">69.53</span></td>
<td id="S6.T3.8.18.18.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.7.1" class="ltx_text" style="font-size:80%;">63.30</span></td>
<td id="S6.T3.8.18.18.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.8.1" class="ltx_text" style="font-size:80%;">81.07</span></td>
<td id="S6.T3.8.18.18.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.9.1" class="ltx_text" style="font-size:80%;">79.20</span></td>
<td id="S6.T3.8.18.18.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.10.1" class="ltx_text" style="font-size:80%;">78.06</span></td>
<td id="S6.T3.8.18.18.11" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.11.1" class="ltx_text" style="font-size:80%;">81.86</span></td>
<td id="S6.T3.8.18.18.12" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.12.1" class="ltx_text" style="font-size:80%;">79.56</span></td>
<td id="S6.T3.8.18.18.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t"><span id="S6.T3.8.18.18.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">84.26</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">We trained multiple baseline models as well as state-of-the-art VQA methods on TDIUC. The methods we use are:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">YES</span>: Predicts ‘yes’ for all questions.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">REP</span>: Predicts the most repeated answer in a question-type category using an oracle.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">QUES</span>: A linear softmax classifier given only question features (image blind).</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">IMG</span>: A linear softmax classifier given only image features (question blind).</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p"><span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Q+I</span>: A linear classifier given the question and image..</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p"><span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">MLP</span>: A 4-layer MLP fed question and image features.</p>
</div>
</li>
<li id="S6.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i7.p1" class="ltx_para">
<p id="S6.I1.i7.p1.1" class="ltx_p"><span id="S6.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">MCB</span>: MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> without spatial attention.</p>
</div>
</li>
<li id="S6.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i8.p1" class="ltx_para">
<p id="S6.I1.i8.p1.1" class="ltx_p"><span id="S6.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">MCB-A</span>: MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with spatial attention.</p>
</div>
</li>
<li id="S6.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i9.p1" class="ltx_para">
<p id="S6.I1.i9.p1.1" class="ltx_p"><span id="S6.I1.i9.p1.1.1" class="ltx_text ltx_font_bold">NMN</span>: NMN from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> with minor modifications.</p>
</div>
</li>
<li id="S6.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i10.p1" class="ltx_para">
<p id="S6.I1.i10.p1.1" class="ltx_p"><span id="S6.I1.i10.p1.1.1" class="ltx_text ltx_font_bold">RAU</span>: RAU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> with minor modifications.</p>
</div>
</li>
</ul>
<p id="S6.p1.1" class="ltx_p">For image features, ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="448\times 448" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">448</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">448</cn><cn type="integer" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">448\times 448</annotation></semantics></math> images was used for all models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">QUES and IMG provide information about biases in the dataset. QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to embed the question, as was done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. For image features, these all use the ‘pool5’ layer of ResNet-152 normalized to unit length. MLP is a 4-layer net with a softmax output layer. The 3 ReLU hidden layers have 6000, 4000, and 2000 units, respectively. During training, dropout (0.3) was used for the hidden layers.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">For MCB, MCB-A, NMN and RAU, we used publicly available code to train them on TDIUC. The experimental setup and hyperparamters were kept unchanged from the default choices in the code, except for upgrading NMN and RAU’s visual representation to both use ResNet-152.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Results on TDIUC for these models are given in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Accuracy scores are given for each of the 12 question-types in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and scores that are normalized by using mean-per-unique-answer are given in appendix Table <a href="#A1.T5" title="Table 5 ‣ A.3 Train and Test Split ‣ Appendix A Additional Details About TDIUC ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Detailed Analysis of VQA Models</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Easy Question-Types for Today’s Methods</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.2" class="ltx_p">By inspecting Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that some question-types are comparatively easy (<math id="S7.SS1.p1.1.m1.1" class="ltx_Math" alttext="&gt;90" display="inline"><semantics id="S7.SS1.p1.1.m1.1a"><mrow id="S7.SS1.p1.1.m1.1.1" xref="S7.SS1.p1.1.m1.1.1.cmml"><mi id="S7.SS1.p1.1.m1.1.1.2" xref="S7.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S7.SS1.p1.1.m1.1.1.1" xref="S7.SS1.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S7.SS1.p1.1.m1.1.1.3" xref="S7.SS1.p1.1.m1.1.1.3.cmml">90</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p1.1.m1.1b"><apply id="S7.SS1.p1.1.m1.1.1.cmml" xref="S7.SS1.p1.1.m1.1.1"><gt id="S7.SS1.p1.1.m1.1.1.1.cmml" xref="S7.SS1.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S7.SS1.p1.1.m1.1.1.2.cmml" xref="S7.SS1.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S7.SS1.p1.1.m1.1.1.3.cmml" xref="S7.SS1.p1.1.m1.1.1.3">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p1.1.m1.1c">&gt;90</annotation></semantics></math>%) under MPT: scene recognition, sport recognition, and object presence. High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. <a href="#S7.SS4" title="7.4 Effects of Including Absurd Questions ‣ 7 Detailed Analysis of VQA Models ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>. Subordinate object recognition is moderately high (<math id="S7.SS1.p1.2.m2.1" class="ltx_Math" alttext="&gt;80" display="inline"><semantics id="S7.SS1.p1.2.m2.1a"><mrow id="S7.SS1.p1.2.m2.1.1" xref="S7.SS1.p1.2.m2.1.1.cmml"><mi id="S7.SS1.p1.2.m2.1.1.2" xref="S7.SS1.p1.2.m2.1.1.2.cmml"></mi><mo id="S7.SS1.p1.2.m2.1.1.1" xref="S7.SS1.p1.2.m2.1.1.1.cmml">&gt;</mo><mn id="S7.SS1.p1.2.m2.1.1.3" xref="S7.SS1.p1.2.m2.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p1.2.m2.1b"><apply id="S7.SS1.p1.2.m2.1.1.cmml" xref="S7.SS1.p1.2.m2.1.1"><gt id="S7.SS1.p1.2.m2.1.1.1.cmml" xref="S7.SS1.p1.2.m2.1.1.1"></gt><csymbol cd="latexml" id="S7.SS1.p1.2.m2.1.1.2.cmml" xref="S7.SS1.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S7.SS1.p1.2.m2.1.1.3.cmml" xref="S7.SS1.p1.2.m2.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p1.2.m2.1c">&gt;80</annotation></semantics></math>%), despite having a large number of unique answers. Accuracy on counting is low across all methods, despite a large number of training data. For the remaining question-types, more analysis is needed to pinpoint whether the weaker performance is due to lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Effects of the Proposed Accuracy Metrics</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP’s score is highly determined by performance on categories with a large number of examples, such as ‘absurd’ and ‘object presence.’ Using MPT, we find that both NMN and MCB outperform MLP. Inspecting normalized scores for each question-type (Appendix Table <a href="#A1.T5" title="Table 5 ‣ A.3 Train and Test Split ‣ Appendix A Additional Details About TDIUC ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) shows an even more pronounced differences, which is also reflected in arithmetic N-MPT score presented in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This indicates that MLP is prone to overfitting. Similar observations can be made for MCB-A compared to RAU, where RAU outperforms MCB-A using simple accuracy, but scores lower on <span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">all</span> the metrics designed to compensate for the skewed answer distribution and bias.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">Comparing the unnormalized and normalized metrics can help us determine the generalization capacity of the VQA algorithms for a given question-type. A large difference in these scores suggests that an algorithm is relying on the skewed answer distribution to obtain high scores. We found that for MCB-A, the accuracy on subordinate object recognition drops from 85.54% with unnormalized to 23.22% with normalized, and for scene recognition it drops from 93.06% (unnormalized) to 38.53% (normalized). Both these categories have a heavily skewed answer distribution; the top-25 answers in subordinate object recognition and the top-5 answers in scene recognition cover over 80% of all questions in their respective question-types. This shows that question-types that appear to be easy may simply be due to the algorithms learning the answer statistics. A truly easy question-type will have similar performance for both unnormalized and normalized metrics. For example, sport recognition shows only 17.39% drop compared to a 30.21% drop for counting, despite counting having same number of unique answers and far more training data. By comparing relative drop in performance between normalized and unnormalized metric, we can also <span id="S7.SS2.p2.1.1" class="ltx_text ltx_font_italic">compare</span> the generalization capability of the algorithms, e.g., for subordinate object recognition, RAU has higher unnormalized score (86.11%) compared to MCB-A (85.54%). However, for normalized scores, MCB-A has significantly higher performance (23.22%) than RAU (21.67%). This shows RAU may be more dependent on the answer distribution. Similar observations can be made for MLP compared to MCB.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Can Algorithms Predict Rare Answers?</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">In the previous section, we saw that the VQA models struggle to correctly predict rarer answers. Are the less repeated questions <span id="S7.SS3.p1.1.1" class="ltx_text ltx_font_italic">actually</span> harder to answer, or are the algorithms simply biased toward more frequent answers? To study this, we created a subset of TDIUC that only consisted of questions that have answers repeated less than 1000 times. We call this dataset TDIUC-Tail, which has 46,590 train and 22,065 test questions. Then, we trained MCB on: 1) the full TDIUC dataset; and 2) TDIUC-Tail. Both versions were evaluated on the validation split of TDIUC-Tail.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">We found that MCB trained only on TDIUC-Tail outperformed MCB trained on all of TDIUC across all question-types (details are in appendix Table <a href="#A2.T6" title="Table 6 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A2.T7" title="Table 7 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This shows that MCB is capable of learning to correctly predict rarer answers, but it is simply biased towards predicting more common answers to maximize overall accuracy. Using normalized accuracy disincentivizes the VQA algorithms’ reliance on the answer statistics, and for deploying a VQA system it may be useful to optimize directly for N-MPT.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Effects of Including Absurd Questions</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">Absurd questions force a VQA system to look at the image to answer the question. In TDIUC, these questions are sampled from the rest of the dataset, and they have a high prior probability of being answered ‘Does not apply.’ This is corroborated by the QUES model, which achieves a high accuracy on absurd; however, for the same questions when they are genuine for an image, it only achieves 6.77% accuracy on these questions. Good absurd performance is achieved by sacrificing performance on other categories. A robust VQA system should be able to detect absurd questions without then failing on others. By examining the accuracy on real questions that are identical to absurd questions, we can quantify an algorithm’s ability to differentiate the absurd questions from the real ones. We found that simpler models had much lower accuracy on these questions, (QUES: 6.77%, Q+I: 34%), compared to more complex models (MCB: 62.44%, MCB-A: 68.83%).</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">To further study this, we we trained two VQA systems, Q+I and MCB, both with and without absurd. The results are presented in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For Q+I trained without absurd questions, accuracies for other categories increase considerably compared to Q+I trained with full TDIUC, especially for question-types that are used to sample absurd questions, e.g., activity recognition (24% when trained with absurd and 48% without). Arithmetic MPT accuracy for the Q+I model that is trained without absurd (57.03%) is also substantially greater than MPT for the model trained with absurd (51.45% for all categories except absurd). This suggests that Q+I is not properly discriminating between absurd and real questions and is biased towards mis-identifying genuine questions as being absurd. In contrast, MCB, a more capable model, produces worse results for absurd, but the version trained without absurd shows much smaller differences than Q+I, which shows that MCB is more capable of identifying absurd questions.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Effects of Balancing Object Presence</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">In Sec. <a href="#S7.SS3" title="7.3 Can Algorithms Predict Rare Answers? ‣ 7 Detailed Analysis of VQA Models ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>, we saw that a skewed answer distribution can impact generalization. This effect is strong even for simple questions and affects even the most sophisticated algorithms. Consider MCB-A when it is trained on both COCO-VQA and Visual Genome, i.e., the winner of the CVPR-2016 VQA Workshop Challenge. When it is evaluated on object presence questions from TDIUC, which contains 50% ‘yes’ and 50% ‘no’ questions, it correctly predicts ‘yes’ answers with 86.3% accuracy, but only 11.2% for questions with ‘no’ as an answer. However, after training it on TDIUC, MCB-A is able to achieve 95.02% for ‘yes’ and 92.26% for ‘no.’ MCB-A performed poorly by learning the biases in the COCO-VQA dataset, but it is capable of performing well when the dataset is unbiased. Similar observations about balancing yes/no questions were made in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Datasets could balance simple categories like object presence, but extending the same idea to all other categories is a challenging task and undermines the natural statistics of the real-world. Adopting mean-per-class and normalized accuracy metrics can help compensate for this problem.</p>
</div>
</section>
<section id="S7.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6 </span>Advantages of Attentive Models</h3>

<div id="S7.SS6.p1" class="ltx_para">
<p id="S7.SS6.p1.1" class="ltx_p">By breaking questions into types, we can assess which types benefit the most from attention. We do this by comparing the MCB model with and without attention, i.e., MCB and MCB-A. As seen in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, attention helped improve results on several question categories. The most pronounced increases are for color recognition, attribute recognition, absurd, and counting. All of these question-types require the algorithm to detect specified object(s) (or lack thereof) to be answered correctly. MCB-A computes attention using local features from different spatial locations, instead of global image features. This aids in localizing individual objects. The attention mechanism learns the relative importance of these features. RAU also utilizes spatial attention and shows similar increments.</p>
</div>
</section>
<section id="S7.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7 </span>Compositional and Modular Approaches</h3>

<div id="S7.SS7.p1" class="ltx_para">
<p id="S7.SS7.p1.1" class="ltx_p">NMN, and, to a lesser extent, RAU propose compositional approaches for VQA. For COCO-VQA, NMN has performed worse than some MLP models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> using simple accuracy. We hoped that it would achieve better performance than other models for questions that require logically analyzing an image in a step-by-step manner, e.g., positional reasoning. However, while NMN did perform better than MLP using MPT and N-MPT metric, we did not see any substantial benefits in specific question-types. This may be because NMN is limited by the quality of the ‘S-expression’ parser, which produces incorrect or misleading parses in many cases. For example, ‘What color is the jacket of the man on the far left?’ is parsed as <span id="S7.SS7.p1.1.1" class="ltx_text ltx_font_typewriter">(color jacket);(color leave);(color (and jacket leave))</span>. This expression not only fails to parse ‘the man’, which is a crucial element needed to correctly answer the question, but also wrongly interprets ‘left’ as past tense of leave.</p>
</div>
<div id="S7.SS7.p2" class="ltx_para">
<p id="S7.SS7.p2.1" class="ltx_p">RAU performs inference over multiple hops, and because each hop contains a complete VQA system, it can learn to solve different tasks in each step. Since it is trained end-to-end, it does not need to rely on rigid question parses. It showed very good performance in detecting absurd questions and also performed well on other categories.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We introduced TDIUC, a VQA dataset that consists of 12 explicitly defined question-types, including absurd questions, and we used it to perform a rigorous analysis of recent VQA algorithms. We proposed new evaluation metrics to compensate for biases in VQA datasets. Results show that the absurd questions and the new evaluation metrics enable a deeper understanding of VQA algorithm behavior.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Deep compositional question answering with neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Learning to compose neural networks for question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
L. Fei-Fei, R. Fergus, and P. Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">One-shot learning of object categories.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Analysis and Machine Intelligence</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">,
28:594–611, 2006.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Are you talking to a machine? Dataset and methods for multilingual
image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
I. Ilievski, S. Yan, and J. Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A focused dynamic attention model for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1604.01485</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. Jabri, A. Joulin, and L. van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Revisiting visual question answering baselines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
A. Jiang, F. Wang, F. Porikli, and Y. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Compositional memory for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1511.05676</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and
R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Answer-type prediction for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Visual question answering: Datasets, algorithms, and future
challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle, M. Yousefhussien, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Data augmentation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Natural Language Generation
(INLG)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Kim, S.-W. Lee, D.-H. Kwak, M.-O. Heo, J. Kim, J.-W. Ha, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Multimodal residual learning for visual QA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and
S. Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Skip-thought vectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Visual Genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">. 2014.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J. Lu, J. Yang, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Hierarchical question-image co-attention for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski, M. Rohrbach, and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
H. Noh and B. Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Training recurrent answering units with joint loss minimization for
VQA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.03647</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
H. Noh, P. H. Seo, and B. Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Image question answering using convolutional neural network with
dynamic parameter prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
M. Ren, R. Kiros, and R. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
K. Saito, A. Shin, Y. Ushiku, and T. Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Dualnet: Domain-invariant network for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.06108</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
K. J. Shih, S. Singh, and D. Hoiem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Where to look: Focus regions for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. v. d. Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Visual question answering: A survey of methods and datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Q. Wu, P. Wang, C. Shen, A. van den Hengel, and A. R. Dick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Ask me anything: Free-form visual question answering based on
knowledge from external sources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
C. Xiong, S. Merity, and R. Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Dynamic memory networks for visual and textual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
H. Xu and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Yin and yang: Balancing and answering binary visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Simple baseline for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.02167, 2015.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Visual7w: Grounded question answering in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details About TDIUC</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we will provide additional details about the TDIUC dataset creation and additional statistics that were omitted from the main paper due to inadequate space.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Questions using Visual Genome Annotations</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">As mentioned in the main text, Visual Genome’s annotations are both non-exhaustive and duplicated. This makes using them to automatically make question-answer (QA) pairs difficult. Due to these issues, we only used them to make two types of questions: Color Attributes and Positional Reasoning. Moreover, a number of restrictions needed to be placed, which are outlined below.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">For making Color Attribute questions, we make use of the attributes metadata in the Visual Genome annotations to populate the template ‘What color is the <span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span>?’ However, Visual Genome metadata can contain several color attributes for the same object as well as different names for the same object. Since the annotators type the name of the object manually rather than choosing from a predetermined set of objects, the same object can be referred by different names, e.g., ‘xbox controller,’ ‘game controller,’ ‘joystick,’ and ‘controller’ can all refer to same object in an image. The object name is sometimes also accompanied by its color, e.g., ‘white horse’ instead of ‘horse’ which makes asking the Color Attribute question ‘What color is the white horse?’ pointless. One potential solution is to use the wordnet ‘synset’ which accompanies every object annotation in the Visual Genome annotations. Synsets are used to group different variations of the common objects names under a single noun from wordnet. However, we found that the synset matching was erroneous in numerous instances, where the object category was misrepresented by the given synset. For example, A ‘controller’ is matched with synset ‘accountant’ even when the ‘controller’ is referring to a game controller. Similarly, a ‘cd’ is matched with synset of ‘cadmium.’ To avoid these problems we made a set of stringent requirements before making questions:</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">The chosen object should only have a single attribute that belongs to a set of commonly used colors.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">The chosen object name or synset must be one of the 91 common objects in the MS-COCO annotations.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">There must be only one instance of the chosen object.</p>
</div>
</li>
</ol>
<p id="A1.SS1.p2.2" class="ltx_p">Using these criteria, we found that we could safely ask the question of the form ‘What color is the <span id="A1.SS1.p2.2.1" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span>?’.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.3.2" class="ltx_text" style="font-size:90%;">The number of questions produced via each source.</span></figcaption>
<table id="A1.T4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.4.1.1" class="ltx_tr">
<td id="A1.T4.4.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T4.4.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T4.4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Questions</span></td>
<td id="A1.T4.4.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T4.4.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Images</span></td>
<td id="A1.T4.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T4.4.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Unique Answers</span></td>
</tr>
<tr id="A1.T4.4.2.2" class="ltx_tr">
<td id="A1.T4.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T4.4.2.2.1.1" class="ltx_text" style="font-size:80%;">Imported (VQA)</span></td>
<td id="A1.T4.4.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T4.4.2.2.2.1" class="ltx_text" style="font-size:80%;">49,990</span></td>
<td id="A1.T4.4.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T4.4.2.2.3.1" class="ltx_text" style="font-size:80%;">43,636</span></td>
<td id="A1.T4.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T4.4.2.2.4.1" class="ltx_text" style="font-size:80%;">812</span></td>
</tr>
<tr id="A1.T4.4.3.3" class="ltx_tr">
<td id="A1.T4.4.3.3.1" class="ltx_td ltx_align_left"><span id="A1.T4.4.3.3.1.1" class="ltx_text" style="font-size:80%;">Imported (Genome)</span></td>
<td id="A1.T4.4.3.3.2" class="ltx_td ltx_align_right"><span id="A1.T4.4.3.3.2.1" class="ltx_text" style="font-size:80%;">310,225</span></td>
<td id="A1.T4.4.3.3.3" class="ltx_td ltx_align_right"><span id="A1.T4.4.3.3.3.1" class="ltx_text" style="font-size:80%;">89,039</span></td>
<td id="A1.T4.4.3.3.4" class="ltx_td ltx_align_center"><span id="A1.T4.4.3.3.4.1" class="ltx_text" style="font-size:80%;">1,446</span></td>
</tr>
<tr id="A1.T4.4.4.4" class="ltx_tr">
<td id="A1.T4.4.4.4.1" class="ltx_td ltx_align_left"><span id="A1.T4.4.4.4.1.1" class="ltx_text" style="font-size:80%;">Generated (COCO)</span></td>
<td id="A1.T4.4.4.4.2" class="ltx_td ltx_align_right"><span id="A1.T4.4.4.4.2.1" class="ltx_text" style="font-size:80%;">1,286,624</span></td>
<td id="A1.T4.4.4.4.3" class="ltx_td ltx_align_right"><span id="A1.T4.4.4.4.3.1" class="ltx_text" style="font-size:80%;">122,218</span></td>
<td id="A1.T4.4.4.4.4" class="ltx_td ltx_align_center"><span id="A1.T4.4.4.4.4.1" class="ltx_text" style="font-size:80%;">108</span></td>
</tr>
<tr id="A1.T4.4.5.5" class="ltx_tr">
<td id="A1.T4.4.5.5.1" class="ltx_td ltx_align_left"><span id="A1.T4.4.5.5.1.1" class="ltx_text" style="font-size:80%;">Generated (Genome)</span></td>
<td id="A1.T4.4.5.5.2" class="ltx_td ltx_align_right"><span id="A1.T4.4.5.5.2.1" class="ltx_text" style="font-size:80%;">6,391</span></td>
<td id="A1.T4.4.5.5.3" class="ltx_td ltx_align_right"><span id="A1.T4.4.5.5.3.1" class="ltx_text" style="font-size:80%;">5,988</span></td>
<td id="A1.T4.4.5.5.4" class="ltx_td ltx_align_center"><span id="A1.T4.4.5.5.4.1" class="ltx_text" style="font-size:80%;">675</span></td>
</tr>
<tr id="A1.T4.4.6.6" class="ltx_tr">
<td id="A1.T4.4.6.6.1" class="ltx_td ltx_align_left"><span id="A1.T4.4.6.6.1.1" class="ltx_text" style="font-size:80%;">Manual</span></td>
<td id="A1.T4.4.6.6.2" class="ltx_td ltx_align_right"><span id="A1.T4.4.6.6.2.1" class="ltx_text" style="font-size:80%;">937</span></td>
<td id="A1.T4.4.6.6.3" class="ltx_td ltx_align_right"><span id="A1.T4.4.6.6.3.1" class="ltx_text" style="font-size:80%;">740</span></td>
<td id="A1.T4.4.6.6.4" class="ltx_td ltx_align_center"><span id="A1.T4.4.6.6.4.1" class="ltx_text" style="font-size:80%;">218</span></td>
</tr>
<tr id="A1.T4.4.7.7" class="ltx_tr">
<td id="A1.T4.4.7.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A1.T4.4.7.7.1.1" class="ltx_text" style="font-size:80%;">Grand Total</span></td>
<td id="A1.T4.4.7.7.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T4.4.7.7.2.1" class="ltx_text" style="font-size:80%;">1,654,167</span></td>
<td id="A1.T4.4.7.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T4.4.7.7.3.1" class="ltx_text" style="font-size:80%;">167,437</span></td>
<td id="A1.T4.4.7.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T4.4.7.7.4.1" class="ltx_text" style="font-size:80%;">1,618</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">Similarly, for making Positional Reasoning questions, we used the relationships metadata in the Visual Genome annotations. The relationships metadata connects two objects by a relationship phrase. Many of these relationships describe the positions of the two objects, e.g., A is ‘on right’ of B, where ‘on right’ is one of the example relationship clause from Visual Genome, with the object A as the subject and the object B as the object. This can be used to generate Positional Reasoning questions. Again, we take several measures to avoid ambiguity. First, we only use objects that appear once in the image because ‘What is to the left of A’ can be ambiguous if there are two instances of the object A. However, since visual genome annotations are non-exhaustive, there may still (rarely) be more than one instance of object A that was not annotated. To disambiguate such cases, we use the attributes metadata to further specify the object wherever possible, e.g., instead of asking ‘What is to the right of the bus?’, we ask ‘What is to the right of the green bus?’</p>
</div>
<div id="A1.SS1.p4" class="ltx_para">
<p id="A1.SS1.p4.1" class="ltx_p">Due to a these stringent criteria, we could only create a small number of questions using Visual Genome annotations compared to other sources. The number of questions produced via each source is shown in Table <a href="#A1.T4" title="Table 4 ‣ A.1 Questions using Visual Genome Annotations ‣ Appendix A Additional Details About TDIUC ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A1.F3" class="ltx_figure"><img src="/html/1703.09684/assets/images/distribution.png" id="A1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="363" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="A1.F3.3.2" class="ltx_text" style="font-size:90%;">Answer distributions for the answers for each of the question-types. This shows the relative frequency of each unique answer within a question-type, so for some question-types, e.g., counting, even slim bars contain a fairly large number of instances with that answer. Similarly, for less populated question-types such as utility and affordances, even large bars represents only a small number of training examples.</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Answer Distribution</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Figure <a href="#A1.F3" title="Figure 3 ‣ A.1 Questions using Visual Genome Annotations ‣ Appendix A Additional Details About TDIUC ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the answer distribution for the different question-types. We can see that some categories, such as counting, scene recognition and sentiment understanding, have a very large share of questions represented by only a few top answers. In such cases, the performance of a VQA algorithm can be inflated unless the evaluation metric compensates for this bias. In other cases, such as positional reasoning and object utility and affordances, the answers are much more varied, with top-50 answers covering less than 60% of all answers.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">We have completely balanced answer distribution for object presence questions, where exactly 50% of questions being answered ‘yes’ and the remaining 50% of the questions are answered ‘no’. For other categories, we have tried to design our question generation algorithms so that a single answer does not have a significant majority within a question type. For example, while scene understanding has top-4 answers covering over 85% of all the questions, there are roughly as many ‘no’ questions (most common answer) as there are ‘yes’ questions (second most-common answer). Similar distributions can be seen for counting, where ‘two’ (most-common answer) is repeated almost as many times as ‘one’ (second most-common answer). By having at least the top-2 answers split almost equally, we remove the incentive for an algorithm to perform well using simple mode guessing, even when using the simple accuracy metric.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Train and Test Split</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">In the paper, we mentioned that we split the entire collection into 70% train and 30% test/validation. To do this, we not only need to have a roughly equal distribution of question types and answers, but also need to make sure that the multiple questions for same image do not end up in two different splits, i.e., the same image cannot occur in both the train and the test partitions. So, we took following measures to split the questions into train-test splits. First, we split all the images into three separate clusters.</p>
<ol id="A1.I2" class="ltx_enumerate">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">Manually uploaded images, which includes all the images manually uploaded by our volunteer annotators.</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">Images from the COCO dataset, including all the images for questions generated from COCO annotations and those imported from COCO-VQA dataset. In addition, a large number of Visual Genome questions also refer to COCO images. So, some questions that are generated and imported from Visual Genome are also included in this cluster.</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">Images exclusively in the Visual Genome dataset, which includes images for a part of the questions imported from Visual Genome and those generated using that dataset.</p>
</div>
</li>
</ol>
<p id="A1.SS3.p1.2" class="ltx_p">We follow simple rules to split each of these clusters of images into either belonging to the train or test splits.</p>
<ol id="A1.I3" class="ltx_enumerate">
<li id="A1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I3.i1.p1" class="ltx_para">
<p id="A1.I3.i1.p1.1" class="ltx_p">All the questions belonging to images coming from the ‘train2014’ split of COCO images are assigned to the train split and all the questions belonging to images from the ‘val2014’ split are assigned to test split.</p>
</div>
</li>
<li id="A1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I3.i2.p1" class="ltx_para">
<p id="A1.I3.i2.p1.1" class="ltx_p">For manual and Visual Genome images, we randomly split 70% of images to train and rest to test.</p>
</div>
</li>
</ol>
</div>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.4.1.1" class="ltx_text" style="font-size:113%;">Table 5</span>: </span><span id="A1.T5.5.2" class="ltx_text" style="font-size:113%;">Results for all the VQA models. The normalized accuracy for each question-type is shown here. The models are identical to the ones in Table 3 in main paper. Overall performance is, again, reported using all 5 metrics. Overall (Arithmetic N-MPT) and Overall (Harmonic N-MPT) are averages of the reported sub-scores. Similarly, Arithmetic MPT and Harmonic MPT are averages of sub-scores reported in Table 3 in the main paper. * denotes training without absurd questions.</span></figcaption>
<table id="A1.T5.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T5.6.1.1" class="ltx_tr">
<th id="A1.T5.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="A1.T5.6.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">YES</span></td>
<td id="A1.T5.6.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">REP</span></td>
<td id="A1.T5.6.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IMG</span></td>
<td id="A1.T5.6.1.1.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">QUES</span></td>
<td id="A1.T5.6.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q+I</span></td>
<td id="A1.T5.6.1.1.7" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">*Q+I</span></td>
<td id="A1.T5.6.1.1.8" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MLP</span></td>
<td id="A1.T5.6.1.1.9" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
<td id="A1.T5.6.1.1.10" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">*MCB</span></td>
<td id="A1.T5.6.1.1.11" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB-A</span></td>
<td id="A1.T5.6.1.1.12" class="ltx_td ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">NMN</span></td>
<td id="A1.T5.6.1.1.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt"><span id="A1.T5.6.1.1.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RAU</span></td>
</tr>
<tr id="A1.T5.6.2.2" class="ltx_tr">
<th id="A1.T5.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T5.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Scene Recognition</span></th>
<td id="A1.T5.6.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.2.1" class="ltx_text" style="font-size:80%;">2.08</span></td>
<td id="A1.T5.6.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.3.1" class="ltx_text" style="font-size:80%;">2.08</span></td>
<td id="A1.T5.6.2.2.4" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.4.1" class="ltx_text" style="font-size:80%;">2.83</span></td>
<td id="A1.T5.6.2.2.5" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.5.1" class="ltx_text" style="font-size:80%;">13.67</span></td>
<td id="A1.T5.6.2.2.6" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.6.1" class="ltx_text" style="font-size:80%;">25.35</span></td>
<td id="A1.T5.6.2.2.7" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.7.1" class="ltx_text" style="font-size:80%;">24.96</span></td>
<td id="A1.T5.6.2.2.8" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.8.1" class="ltx_text" style="font-size:80%;">20.54</span></td>
<td id="A1.T5.6.2.2.9" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.9.1" class="ltx_text" style="font-size:80%;">36.34</span></td>
<td id="A1.T5.6.2.2.10" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.10.1" class="ltx_text" style="font-size:80%;">32.55</span></td>
<td id="A1.T5.6.2.2.11" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">38.53</span></td>
<td id="A1.T5.6.2.2.12" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.12.1" class="ltx_text" style="font-size:80%;">29.06</span></td>
<td id="A1.T5.6.2.2.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="A1.T5.6.2.2.13.1" class="ltx_text" style="font-size:80%;">32.69</span></td>
</tr>
<tr id="A1.T5.6.3.3" class="ltx_tr">
<th id="A1.T5.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.3.3.1.1" class="ltx_text" style="font-size:80%;">Sport Recognition</span></th>
<td id="A1.T5.6.3.3.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.3.3.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.3.1" class="ltx_text" style="font-size:80%;">9.09</span></td>
<td id="A1.T5.6.3.3.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.4.1" class="ltx_text" style="font-size:80%;">12.57</span></td>
<td id="A1.T5.6.3.3.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.5.1" class="ltx_text" style="font-size:80%;">11.09</span></td>
<td id="A1.T5.6.3.3.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.6.1" class="ltx_text" style="font-size:80%;">51.48</span></td>
<td id="A1.T5.6.3.3.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.7.1" class="ltx_text" style="font-size:80%;">60.31</span></td>
<td id="A1.T5.6.3.3.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.8.1" class="ltx_text" style="font-size:80%;">60.81</span></td>
<td id="A1.T5.6.3.3.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.9.1" class="ltx_text" style="font-size:80%;">75.25</span></td>
<td id="A1.T5.6.3.3.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.10.1" class="ltx_text" style="font-size:80%;">73.64</span></td>
<td id="A1.T5.6.3.3.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">75.38</span></td>
<td id="A1.T5.6.3.3.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.3.3.12.1" class="ltx_text" style="font-size:80%;">63.51</span></td>
<td id="A1.T5.6.3.3.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.3.3.13.1" class="ltx_text" style="font-size:80%;">73.60</span></td>
</tr>
<tr id="A1.T5.6.4.4" class="ltx_tr">
<th id="A1.T5.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Color Attributes</span></th>
<td id="A1.T5.6.4.4.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.4.4.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.3.1" class="ltx_text" style="font-size:80%;">6.25</span></td>
<td id="A1.T5.6.4.4.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.4.1" class="ltx_text" style="font-size:80%;">1.77</span></td>
<td id="A1.T5.6.4.4.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.5.1" class="ltx_text" style="font-size:80%;">20.10</span></td>
<td id="A1.T5.6.4.4.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.6.1" class="ltx_text" style="font-size:80%;">25.45</span></td>
<td id="A1.T5.6.4.4.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.7.1" class="ltx_text" style="font-size:80%;">30.37</span></td>
<td id="A1.T5.6.4.4.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.8.1" class="ltx_text" style="font-size:80%;">30.97</span></td>
<td id="A1.T5.6.4.4.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.9.1" class="ltx_text" style="font-size:80%;">36.98</span></td>
<td id="A1.T5.6.4.4.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.10.1" class="ltx_text" style="font-size:80%;">37.54</span></td>
<td id="A1.T5.6.4.4.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">49.40</span></td>
<td id="A1.T5.6.4.4.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.4.4.12.1" class="ltx_text" style="font-size:80%;">33.06</span></td>
<td id="A1.T5.6.4.4.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.4.4.13.1" class="ltx_text" style="font-size:80%;">46.79</span></td>
</tr>
<tr id="A1.T5.6.5.5" class="ltx_tr">
<th id="A1.T5.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Other Attributes</span></th>
<td id="A1.T5.6.5.5.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.5.5.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.3.1" class="ltx_text" style="font-size:80%;">0.31</span></td>
<td id="A1.T5.6.5.5.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.4.1" class="ltx_text" style="font-size:80%;">1.16</span></td>
<td id="A1.T5.6.5.5.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.5.1" class="ltx_text" style="font-size:80%;">6.21</span></td>
<td id="A1.T5.6.5.5.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.6.1" class="ltx_text" style="font-size:80%;">6.98</span></td>
<td id="A1.T5.6.5.5.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.7.1" class="ltx_text" style="font-size:80%;">9.51</span></td>
<td id="A1.T5.6.5.5.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.8.1" class="ltx_text" style="font-size:80%;">2.84</span></td>
<td id="A1.T5.6.5.5.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.9.1" class="ltx_text" style="font-size:80%;">13.90</span></td>
<td id="A1.T5.6.5.5.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.10.1" class="ltx_text" style="font-size:80%;">15.04</span></td>
<td id="A1.T5.6.5.5.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">15.09</span></td>
<td id="A1.T5.6.5.5.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.5.5.12.1" class="ltx_text" style="font-size:80%;">7.10</span></td>
<td id="A1.T5.6.5.5.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.5.5.13.1" class="ltx_text" style="font-size:80%;">12.11</span></td>
</tr>
<tr id="A1.T5.6.6.6" class="ltx_tr">
<th id="A1.T5.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.6.6.1.1" class="ltx_text" style="font-size:80%;">Activity Recognition</span></th>
<td id="A1.T5.6.6.6.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.6.6.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.3.1" class="ltx_text" style="font-size:80%;">7.69</span></td>
<td id="A1.T5.6.6.6.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.4.1" class="ltx_text" style="font-size:80%;">2.88</span></td>
<td id="A1.T5.6.6.6.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.5.1" class="ltx_text" style="font-size:80%;">7.59</span></td>
<td id="A1.T5.6.6.6.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.6.1" class="ltx_text" style="font-size:80%;">16.09</span></td>
<td id="A1.T5.6.6.6.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.7.1" class="ltx_text" style="font-size:80%;">39.35</span></td>
<td id="A1.T5.6.6.6.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.8.1" class="ltx_text" style="font-size:80%;">24.95</span></td>
<td id="A1.T5.6.6.6.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.9.1" class="ltx_text" style="font-size:80%;">46.57</span></td>
<td id="A1.T5.6.6.6.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.10.1" class="ltx_text" style="font-size:80%;">48.27</span></td>
<td id="A1.T5.6.6.6.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">48.47</span></td>
<td id="A1.T5.6.6.6.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.6.6.12.1" class="ltx_text" style="font-size:80%;">22.79</span></td>
<td id="A1.T5.6.6.6.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.6.6.13.1" class="ltx_text" style="font-size:80%;">46.65</span></td>
</tr>
<tr id="A1.T5.6.7.7" class="ltx_tr">
<th id="A1.T5.6.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.7.7.1.1" class="ltx_text" style="font-size:80%;">Positional Reasoning</span></th>
<td id="A1.T5.6.7.7.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.7.7.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.3.1" class="ltx_text" style="font-size:80%;">0.15</span></td>
<td id="A1.T5.6.7.7.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.4.1" class="ltx_text" style="font-size:80%;">0.70</span></td>
<td id="A1.T5.6.7.7.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.5.1" class="ltx_text" style="font-size:80%;">4.03</span></td>
<td id="A1.T5.6.7.7.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.6.1" class="ltx_text" style="font-size:80%;">6.26</span></td>
<td id="A1.T5.6.7.7.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.7.1" class="ltx_text" style="font-size:80%;">8.59</span></td>
<td id="A1.T5.6.7.7.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.8.1" class="ltx_text" style="font-size:80%;">2.99</span></td>
<td id="A1.T5.6.7.7.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.9.1" class="ltx_text" style="font-size:80%;">9.29</span></td>
<td id="A1.T5.6.7.7.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.10.1" class="ltx_text" style="font-size:80%;">9.39</span></td>
<td id="A1.T5.6.7.7.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">10.76</span></td>
<td id="A1.T5.6.7.7.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.7.7.12.1" class="ltx_text" style="font-size:80%;">6.37</span></td>
<td id="A1.T5.6.7.7.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.7.7.13.1" class="ltx_text" style="font-size:80%;">9.60</span></td>
</tr>
<tr id="A1.T5.6.8.8" class="ltx_tr">
<th id="A1.T5.6.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.8.8.1.1" class="ltx_text" style="font-size:80%;">Sub. Object Recognition</span></th>
<td id="A1.T5.6.8.8.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.8.8.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.3.1" class="ltx_text" style="font-size:80%;">0.47</span></td>
<td id="A1.T5.6.8.8.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.4.1" class="ltx_text" style="font-size:80%;">3.16</span></td>
<td id="A1.T5.6.8.8.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.5.1" class="ltx_text" style="font-size:80%;">3.72</span></td>
<td id="A1.T5.6.8.8.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.6.1" class="ltx_text" style="font-size:80%;">15.91</span></td>
<td id="A1.T5.6.8.8.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.7.1" class="ltx_text" style="font-size:80%;">16.97</span></td>
<td id="A1.T5.6.8.8.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.8.1" class="ltx_text" style="font-size:80%;">14.85</span></td>
<td id="A1.T5.6.8.8.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.9.1" class="ltx_text" style="font-size:80%;">22.07</span></td>
<td id="A1.T5.6.8.8.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.10.1" class="ltx_text" style="font-size:80%;">23.05</span></td>
<td id="A1.T5.6.8.8.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">23.22</span></td>
<td id="A1.T5.6.8.8.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.8.8.12.1" class="ltx_text" style="font-size:80%;">16.83</span></td>
<td id="A1.T5.6.8.8.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.8.8.13.1" class="ltx_text" style="font-size:80%;">21.67</span></td>
</tr>
<tr id="A1.T5.6.9.9" class="ltx_tr">
<th id="A1.T5.6.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.9.9.1.1" class="ltx_text" style="font-size:80%;">Absurd</span></th>
<td id="A1.T5.6.9.9.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.9.9.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">100.00</span></td>
<td id="A1.T5.6.9.9.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.4.1" class="ltx_text" style="font-size:80%;">19.97</span></td>
<td id="A1.T5.6.9.9.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.5.1" class="ltx_text" style="font-size:80%;">96.71</span></td>
<td id="A1.T5.6.9.9.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.6.1" class="ltx_text" style="font-size:80%;">96.98</span></td>
<td id="A1.T5.6.9.9.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.7.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A1.T5.6.9.9.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.8.1" class="ltx_text" style="font-size:80%;">95.96</span></td>
<td id="A1.T5.6.9.9.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.9.1" class="ltx_text" style="font-size:80%;">83.44</span></td>
<td id="A1.T5.6.9.9.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.10.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A1.T5.6.9.9.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.11.1" class="ltx_text" style="font-size:80%;">84.82</span></td>
<td id="A1.T5.6.9.9.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.9.9.12.1" class="ltx_text" style="font-size:80%;">87.51</span></td>
<td id="A1.T5.6.9.9.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.9.9.13.1" class="ltx_text" style="font-size:80%;">96.08</span></td>
</tr>
<tr id="A1.T5.6.10.10" class="ltx_tr">
<th id="A1.T5.6.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.10.10.1.1" class="ltx_text" style="font-size:80%;">Utility and Affordances</span></th>
<td id="A1.T5.6.10.10.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.2.1" class="ltx_text" style="font-size:80%;">1.22</span></td>
<td id="A1.T5.6.10.10.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.3.1" class="ltx_text" style="font-size:80%;">1.22</span></td>
<td id="A1.T5.6.10.10.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.4.1" class="ltx_text" style="font-size:80%;">1.34</span></td>
<td id="A1.T5.6.10.10.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.5.1" class="ltx_text" style="font-size:80%;">9.23</span></td>
<td id="A1.T5.6.10.10.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.6.1" class="ltx_text" style="font-size:80%;">16.85</span></td>
<td id="A1.T5.6.10.10.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.7.1" class="ltx_text" style="font-size:80%;">21.97</span></td>
<td id="A1.T5.6.10.10.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.8.1" class="ltx_text" style="font-size:80%;">6.18</span></td>
<td id="A1.T5.6.10.10.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.9.1" class="ltx_text" style="font-size:80%;">24.07</span></td>
<td id="A1.T5.6.10.10.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.10.1" class="ltx_text" style="font-size:80%;">23.33</span></td>
<td id="A1.T5.6.10.10.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">26.20</span></td>
<td id="A1.T5.6.10.10.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.10.10.12.1" class="ltx_text" style="font-size:80%;">19.55</span></td>
<td id="A1.T5.6.10.10.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.10.10.13.1" class="ltx_text" style="font-size:80%;">21.38</span></td>
</tr>
<tr id="A1.T5.6.11.11" class="ltx_tr">
<th id="A1.T5.6.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.11.11.1.1" class="ltx_text" style="font-size:80%;">Object Presence</span></th>
<td id="A1.T5.6.11.11.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.2.1" class="ltx_text" style="font-size:80%;">50.00</span></td>
<td id="A1.T5.6.11.11.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.3.1" class="ltx_text" style="font-size:80%;">50.00</span></td>
<td id="A1.T5.6.11.11.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.4.1" class="ltx_text" style="font-size:80%;">20.73</span></td>
<td id="A1.T5.6.11.11.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.5.1" class="ltx_text" style="font-size:80%;">69.06</span></td>
<td id="A1.T5.6.11.11.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.6.1" class="ltx_text" style="font-size:80%;">69.43</span></td>
<td id="A1.T5.6.11.11.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.7.1" class="ltx_text" style="font-size:80%;">69.50</span></td>
<td id="A1.T5.6.11.11.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.8.1" class="ltx_text" style="font-size:80%;">92.33</span></td>
<td id="A1.T5.6.11.11.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.9.1" class="ltx_text" style="font-size:80%;">91.84</span></td>
<td id="A1.T5.6.11.11.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.10.1" class="ltx_text" style="font-size:80%;">91.95</span></td>
<td id="A1.T5.6.11.11.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.11.1" class="ltx_text" style="font-size:80%;">93.64</span></td>
<td id="A1.T5.6.11.11.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.11.11.12.1" class="ltx_text" style="font-size:80%;">92.50</span></td>
<td id="A1.T5.6.11.11.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.11.11.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.38</span></td>
</tr>
<tr id="A1.T5.6.12.12" class="ltx_tr">
<th id="A1.T5.6.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.12.12.1.1" class="ltx_text" style="font-size:80%;">Counting</span></th>
<td id="A1.T5.6.12.12.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.12.12.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.3.1" class="ltx_text" style="font-size:80%;">6.25</span></td>
<td id="A1.T5.6.12.12.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.4.1" class="ltx_text" style="font-size:80%;">1.31</span></td>
<td id="A1.T5.6.12.12.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.5.1" class="ltx_text" style="font-size:80%;">10.30</span></td>
<td id="A1.T5.6.12.12.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.6.1" class="ltx_text" style="font-size:80%;">14.61</span></td>
<td id="A1.T5.6.12.12.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.7.1" class="ltx_text" style="font-size:80%;">14.62</span></td>
<td id="A1.T5.6.12.12.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.8.1" class="ltx_text" style="font-size:80%;">16.43</span></td>
<td id="A1.T5.6.12.12.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.9.1" class="ltx_text" style="font-size:80%;">17.83</span></td>
<td id="A1.T5.6.12.12.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.10.1" class="ltx_text" style="font-size:80%;">18.09</span></td>
<td id="A1.T5.6.12.12.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.11.1" class="ltx_text" style="font-size:80%;">20.80</span></td>
<td id="A1.T5.6.12.12.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.12.12.12.1" class="ltx_text" style="font-size:80%;">15.52</span></td>
<td id="A1.T5.6.12.12.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.12.12.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">23.11</span></td>
</tr>
<tr id="A1.T5.6.13.13" class="ltx_tr">
<th id="A1.T5.6.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.13.13.1.1" class="ltx_text" style="font-size:80%;">Sentiment Understanding</span></th>
<td id="A1.T5.6.13.13.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.2.1" class="ltx_text" style="font-size:80%;">4.00</span></td>
<td id="A1.T5.6.13.13.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.3.1" class="ltx_text" style="font-size:80%;">4.00</span></td>
<td id="A1.T5.6.13.13.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.4.1" class="ltx_text" style="font-size:80%;">1.43</span></td>
<td id="A1.T5.6.13.13.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.5.1" class="ltx_text" style="font-size:80%;">5.80</span></td>
<td id="A1.T5.6.13.13.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.6.1" class="ltx_text" style="font-size:80%;">8.18</span></td>
<td id="A1.T5.6.13.13.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.7.1" class="ltx_text" style="font-size:80%;">12.94</span></td>
<td id="A1.T5.6.13.13.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.8.1" class="ltx_text" style="font-size:80%;">7.49</span></td>
<td id="A1.T5.6.13.13.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.9.1" class="ltx_text" style="font-size:80%;">20.09</span></td>
<td id="A1.T5.6.13.13.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.10.1" class="ltx_text" style="font-size:80%;">17.49</span></td>
<td id="A1.T5.6.13.13.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20.41</span></td>
<td id="A1.T5.6.13.13.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.13.13.12.1" class="ltx_text" style="font-size:80%;">9.22</span></td>
<td id="A1.T5.6.13.13.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.13.13.13.1" class="ltx_text" style="font-size:80%;">14.43</span></td>
</tr>
<tr id="A1.T5.6.14.14" class="ltx_tr">
<th id="A1.T5.6.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T5.6.14.14.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic MPT)</span></th>
<td id="A1.T5.6.14.14.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.2.1" class="ltx_text" style="font-size:80%;">11.10</span></td>
<td id="A1.T5.6.14.14.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.3.1" class="ltx_text" style="font-size:80%;">31.11</span></td>
<td id="A1.T5.6.14.14.4" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.4.1" class="ltx_text" style="font-size:80%;">9.49</span></td>
<td id="A1.T5.6.14.14.5" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.5.1" class="ltx_text" style="font-size:80%;">39.31</span></td>
<td id="A1.T5.6.14.14.6" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.6.1" class="ltx_text" style="font-size:80%;">55.25</span></td>
<td id="A1.T5.6.14.14.7" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.7.1" class="ltx_text" style="font-size:80%;">57.03</span></td>
<td id="A1.T5.6.14.14.8" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.8.1" class="ltx_text" style="font-size:80%;">60.87</span></td>
<td id="A1.T5.6.14.14.9" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.9.1" class="ltx_text" style="font-size:80%;">65.75</span></td>
<td id="A1.T5.6.14.14.10" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.10.1" class="ltx_text" style="font-size:80%;">66.07</span></td>
<td id="A1.T5.6.14.14.11" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67.90</span></td>
<td id="A1.T5.6.14.14.12" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.12.1" class="ltx_text" style="font-size:80%;">62.59</span></td>
<td id="A1.T5.6.14.14.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="A1.T5.6.14.14.13.1" class="ltx_text" style="font-size:80%;">67.81</span></td>
</tr>
<tr id="A1.T5.6.15.15" class="ltx_tr">
<th id="A1.T5.6.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.15.15.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic MPT)</span></th>
<td id="A1.T5.6.15.15.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.15.15.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.3.1" class="ltx_text" style="font-size:80%;">17.53</span></td>
<td id="A1.T5.6.15.15.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.4.1" class="ltx_text" style="font-size:80%;">1.92</span></td>
<td id="A1.T5.6.15.15.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.5.1" class="ltx_text" style="font-size:80%;">25.93</span></td>
<td id="A1.T5.6.15.15.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.6.1" class="ltx_text" style="font-size:80%;">44.13</span></td>
<td id="A1.T5.6.15.15.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.7.1" class="ltx_text" style="font-size:80%;">50.30</span></td>
<td id="A1.T5.6.15.15.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.8.1" class="ltx_text" style="font-size:80%;">42.80</span></td>
<td id="A1.T5.6.15.15.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.9.1" class="ltx_text" style="font-size:80%;">58.03</span></td>
<td id="A1.T5.6.15.15.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.10.1" class="ltx_text" style="font-size:80%;">55.43</span></td>
<td id="A1.T5.6.15.15.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">60.47</span></td>
<td id="A1.T5.6.15.15.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.15.15.12.1" class="ltx_text" style="font-size:80%;">51.87</span></td>
<td id="A1.T5.6.15.15.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.15.15.13.1" class="ltx_text" style="font-size:80%;">59.00</span></td>
</tr>
<tr id="A1.T5.6.16.16" class="ltx_tr">
<th id="A1.T5.6.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T5.6.16.16.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic N-MPT)</span></th>
<td id="A1.T5.6.16.16.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.2.1" class="ltx_text" style="font-size:80%;">4.87</span></td>
<td id="A1.T5.6.16.16.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.3.1" class="ltx_text" style="font-size:80%;">15.63</span></td>
<td id="A1.T5.6.16.16.4" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.4.1" class="ltx_text" style="font-size:80%;">5.82</span></td>
<td id="A1.T5.6.16.16.5" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.5.1" class="ltx_text" style="font-size:80%;">21.46</span></td>
<td id="A1.T5.6.16.16.6" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.6.1" class="ltx_text" style="font-size:80%;">29.47</span></td>
<td id="A1.T5.6.16.16.7" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.7.1" class="ltx_text" style="font-size:80%;">28.10</span></td>
<td id="A1.T5.6.16.16.8" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.8.1" class="ltx_text" style="font-size:80%;">31.36</span></td>
<td id="A1.T5.6.16.16.9" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.9.1" class="ltx_text" style="font-size:80%;">39.81</span></td>
<td id="A1.T5.6.16.16.10" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.10.1" class="ltx_text" style="font-size:80%;">35.49</span></td>
<td id="A1.T5.6.16.16.11" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.24</span></td>
<td id="A1.T5.6.16.16.12" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.12.1" class="ltx_text" style="font-size:80%;">34.00</span></td>
<td id="A1.T5.6.16.16.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="A1.T5.6.16.16.13.1" class="ltx_text" style="font-size:80%;">41.04</span></td>
</tr>
<tr id="A1.T5.6.17.17" class="ltx_tr">
<th id="A1.T5.6.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A1.T5.6.17.17.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic N-MPT)</span></th>
<td id="A1.T5.6.17.17.2" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.2.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
<td id="A1.T5.6.17.17.3" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.3.1" class="ltx_text" style="font-size:80%;">0.83</span></td>
<td id="A1.T5.6.17.17.4" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.4.1" class="ltx_text" style="font-size:80%;">1.91</span></td>
<td id="A1.T5.6.17.17.5" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.5.1" class="ltx_text" style="font-size:80%;">8.42</span></td>
<td id="A1.T5.6.17.17.6" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.6.1" class="ltx_text" style="font-size:80%;">14.99</span></td>
<td id="A1.T5.6.17.17.7" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.7.1" class="ltx_text" style="font-size:80%;">18.30</span></td>
<td id="A1.T5.6.17.17.8" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.8.1" class="ltx_text" style="font-size:80%;">9.46</span></td>
<td id="A1.T5.6.17.17.9" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.9.1" class="ltx_text" style="font-size:80%;">24.77</span></td>
<td id="A1.T5.6.17.17.10" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.10.1" class="ltx_text" style="font-size:80%;">23.20</span></td>
<td id="A1.T5.6.17.17.11" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">27.28</span></td>
<td id="A1.T5.6.17.17.12" class="ltx_td ltx_align_right"><span id="A1.T5.6.17.17.12.1" class="ltx_text" style="font-size:80%;">16.67</span></td>
<td id="A1.T5.6.17.17.13" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T5.6.17.17.13.1" class="ltx_text" style="font-size:80%;">23.99</span></td>
</tr>
<tr id="A1.T5.6.18.18" class="ltx_tr">
<th id="A1.T5.6.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.1.1" class="ltx_text" style="font-size:80%;">Simple Accuracy</span></th>
<td id="A1.T5.6.18.18.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.2.1" class="ltx_text" style="font-size:80%;">21.14</span></td>
<td id="A1.T5.6.18.18.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.3.1" class="ltx_text" style="font-size:80%;">51.15</span></td>
<td id="A1.T5.6.18.18.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.4.1" class="ltx_text" style="font-size:80%;">14.54</span></td>
<td id="A1.T5.6.18.18.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.5.1" class="ltx_text" style="font-size:80%;">62.74</span></td>
<td id="A1.T5.6.18.18.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.6.1" class="ltx_text" style="font-size:80%;">69.53</span></td>
<td id="A1.T5.6.18.18.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.7.1" class="ltx_text" style="font-size:80%;">63.30</span></td>
<td id="A1.T5.6.18.18.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.8.1" class="ltx_text" style="font-size:80%;">81.07</span></td>
<td id="A1.T5.6.18.18.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.9.1" class="ltx_text" style="font-size:80%;">79.20</span></td>
<td id="A1.T5.6.18.18.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.10.1" class="ltx_text" style="font-size:80%;">78.06</span></td>
<td id="A1.T5.6.18.18.11" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.11.1" class="ltx_text" style="font-size:80%;">81.86</span></td>
<td id="A1.T5.6.18.18.12" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.12.1" class="ltx_text" style="font-size:80%;">79.56</span></td>
<td id="A1.T5.6.18.18.13" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t"><span id="A1.T5.6.18.18.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">84.26</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Experimental Results</h2>

<figure id="A2.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A2.T6.4.1.1" class="ltx_text" style="font-size:113%;">Table 6</span>: </span><span id="A2.T6.5.2" class="ltx_text" style="font-size:113%;">Results on TDIUC-Tail for MCB model when trained on full TDIUC dataset vs when trained only on TDIUC-Tail. The un-normalized scores for each question-types and five different overall scores are shown here</span></figcaption>
<table id="A2.T6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.6.1.1" class="ltx_tr">
<th id="A2.T6.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A2.T6.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A2.T6.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T6.6.1.1.2.1.1" class="ltx_tr">
<td id="A2.T6.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.6.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
</tr>
<tr id="A2.T6.6.1.1.2.1.2" class="ltx_tr">
<td id="A2.T6.6.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.6.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TDIUC-Full</span></td>
</tr>
</table>
</th>
<th id="A2.T6.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A2.T6.6.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T6.6.1.1.3.1.1" class="ltx_tr">
<td id="A2.T6.6.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.6.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
</tr>
<tr id="A2.T6.6.1.1.3.1.2" class="ltx_tr">
<td id="A2.T6.6.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.6.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TDIUC-Tail</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.6.2.1" class="ltx_tr">
<th id="A2.T6.6.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T6.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Scene Recognition</span></th>
<td id="A2.T6.6.2.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.2.1.2.1" class="ltx_text" style="font-size:80%;">61.64</span></td>
<td id="A2.T6.6.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.2.1.3.1" class="ltx_text" style="font-size:80%;">66.59</span></td>
</tr>
<tr id="A2.T6.6.3.2" class="ltx_tr">
<th id="A2.T6.6.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.3.2.1.1" class="ltx_text" style="font-size:80%;">Sport Recognition</span></th>
<td id="A2.T6.6.3.2.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.3.2.2.1" class="ltx_text" style="font-size:80%;">71.61</span></td>
<td id="A2.T6.6.3.2.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.3.2.3.1" class="ltx_text" style="font-size:80%;">93.74</span></td>
</tr>
<tr id="A2.T6.6.4.3" class="ltx_tr">
<th id="A2.T6.6.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.4.3.1.1" class="ltx_text" style="font-size:80%;">Color Attributes</span></th>
<td id="A2.T6.6.4.3.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.4.3.2.1" class="ltx_text" style="font-size:80%;">6.83</span></td>
<td id="A2.T6.6.4.3.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.4.3.3.1" class="ltx_text" style="font-size:80%;">84.34</span></td>
</tr>
<tr id="A2.T6.6.5.4" class="ltx_tr">
<th id="A2.T6.6.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.5.4.1.1" class="ltx_text" style="font-size:80%;">Other Attributes</span></th>
<td id="A2.T6.6.5.4.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.5.4.2.1" class="ltx_text" style="font-size:80%;">32.80</span></td>
<td id="A2.T6.6.5.4.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.5.4.3.1" class="ltx_text" style="font-size:80%;">43.37</span></td>
</tr>
<tr id="A2.T6.6.6.5" class="ltx_tr">
<th id="A2.T6.6.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.6.5.1.1" class="ltx_text" style="font-size:80%;">Activity Recognition</span></th>
<td id="A2.T6.6.6.5.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.6.5.2.1" class="ltx_text" style="font-size:80%;">51.79</span></td>
<td id="A2.T6.6.6.5.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.6.5.3.1" class="ltx_text" style="font-size:80%;">74.40</span></td>
</tr>
<tr id="A2.T6.6.7.6" class="ltx_tr">
<th id="A2.T6.6.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.7.6.1.1" class="ltx_text" style="font-size:80%;">Positional Reasoning</span></th>
<td id="A2.T6.6.7.6.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.7.6.2.1" class="ltx_text" style="font-size:80%;">25.16</span></td>
<td id="A2.T6.6.7.6.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.7.6.3.1" class="ltx_text" style="font-size:80%;">29.59</span></td>
</tr>
<tr id="A2.T6.6.8.7" class="ltx_tr">
<th id="A2.T6.6.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.8.7.1.1" class="ltx_text" style="font-size:80%;">Object Recognition</span></th>
<td id="A2.T6.6.8.7.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.8.7.2.1" class="ltx_text" style="font-size:80%;">63.90</span></td>
<td id="A2.T6.6.8.7.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.8.7.3.1" class="ltx_text" style="font-size:80%;">75.89</span></td>
</tr>
<tr id="A2.T6.6.9.8" class="ltx_tr">
<th id="A2.T6.6.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.9.8.1.1" class="ltx_text" style="font-size:80%;">Absurd</span></th>
<td id="A2.T6.6.9.8.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.9.8.2.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A2.T6.6.9.8.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.9.8.3.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
</tr>
<tr id="A2.T6.6.10.9" class="ltx_tr">
<th id="A2.T6.6.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.10.9.1.1" class="ltx_text" style="font-size:80%;">Utility and Affordances</span></th>
<td id="A2.T6.6.10.9.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.10.9.2.1" class="ltx_text" style="font-size:80%;">16.67</span></td>
<td id="A2.T6.6.10.9.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.10.9.3.1" class="ltx_text" style="font-size:80%;">17.59</span></td>
</tr>
<tr id="A2.T6.6.11.10" class="ltx_tr">
<th id="A2.T6.6.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.11.10.1.1" class="ltx_text" style="font-size:80%;">Object Presence</span></th>
<td id="A2.T6.6.11.10.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.11.10.2.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A2.T6.6.11.10.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.11.10.3.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
</tr>
<tr id="A2.T6.6.12.11" class="ltx_tr">
<th id="A2.T6.6.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.12.11.1.1" class="ltx_text" style="font-size:80%;">Counting</span></th>
<td id="A2.T6.6.12.11.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.12.11.2.1" class="ltx_text" style="font-size:80%;">4.87</span></td>
<td id="A2.T6.6.12.11.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.12.11.3.1" class="ltx_text" style="font-size:80%;">29.83</span></td>
</tr>
<tr id="A2.T6.6.13.12" class="ltx_tr">
<th id="A2.T6.6.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.13.12.1.1" class="ltx_text" style="font-size:80%;">Sentiment Understanding</span></th>
<td id="A2.T6.6.13.12.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.13.12.2.1" class="ltx_text" style="font-size:80%;">41.30</span></td>
<td id="A2.T6.6.13.12.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.13.12.3.1" class="ltx_text" style="font-size:80%;">50.72</span></td>
</tr>
<tr id="A2.T6.6.14.13" class="ltx_tr">
<th id="A2.T6.6.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T6.6.14.13.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic MPT)</span></th>
<td id="A2.T6.6.14.13.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.14.13.2.1" class="ltx_text" style="font-size:80%;">37.66</span></td>
<td id="A2.T6.6.14.13.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.14.13.3.1" class="ltx_text" style="font-size:80%;">51.61</span></td>
</tr>
<tr id="A2.T6.6.15.14" class="ltx_tr">
<th id="A2.T6.6.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.15.14.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic MPT)</span></th>
<td id="A2.T6.6.15.14.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.15.14.2.1" class="ltx_text" style="font-size:80%;">17.51</span></td>
<td id="A2.T6.6.15.14.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.15.14.3.1" class="ltx_text" style="font-size:80%;">43.27</span></td>
</tr>
<tr id="A2.T6.6.16.15" class="ltx_tr">
<th id="A2.T6.6.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T6.6.16.15.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic N-MPT)</span></th>
<td id="A2.T6.6.16.15.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.16.15.2.1" class="ltx_text" style="font-size:80%;">19.49</span></td>
<td id="A2.T6.6.16.15.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T6.6.16.15.3.1" class="ltx_text" style="font-size:80%;">34.44</span></td>
</tr>
<tr id="A2.T6.6.17.16" class="ltx_tr">
<th id="A2.T6.6.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T6.6.17.16.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic N-MPT)</span></th>
<td id="A2.T6.6.17.16.2" class="ltx_td ltx_align_right"><span id="A2.T6.6.17.16.2.1" class="ltx_text" style="font-size:80%;">11.37</span></td>
<td id="A2.T6.6.17.16.3" class="ltx_td ltx_align_right"><span id="A2.T6.6.17.16.3.1" class="ltx_text" style="font-size:80%;">22.32</span></td>
</tr>
<tr id="A2.T6.6.18.17" class="ltx_tr">
<th id="A2.T6.6.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T6.6.18.17.1.1" class="ltx_text" style="font-size:80%;">Simple Accuracy</span></th>
<td id="A2.T6.6.18.17.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A2.T6.6.18.17.2.1" class="ltx_text" style="font-size:80%;">38.55</span></td>
<td id="A2.T6.6.18.17.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A2.T6.6.18.17.3.1" class="ltx_text" style="font-size:80%;">50.11</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="A2.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A2.T7.4.1.1" class="ltx_text" style="font-size:113%;">Table 7</span>: </span><span id="A2.T7.5.2" class="ltx_text" style="font-size:113%;">Results on TDIUC-Tail for MCB model when trained on full TDIUC dataset vs when trained only on TDIUC-Tail. The normalized scores for each question-types and five different overall scores are shown here</span></figcaption>
<table id="A2.T7.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T7.6.1.1" class="ltx_tr">
<th id="A2.T7.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A2.T7.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A2.T7.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T7.6.1.1.2.1.1" class="ltx_tr">
<td id="A2.T7.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.6.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
</tr>
<tr id="A2.T7.6.1.1.2.1.2" class="ltx_tr">
<td id="A2.T7.6.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.6.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TDIUC-Full</span></td>
</tr>
</table>
</th>
<th id="A2.T7.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A2.T7.6.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T7.6.1.1.3.1.1" class="ltx_tr">
<td id="A2.T7.6.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.6.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MCB</span></td>
</tr>
<tr id="A2.T7.6.1.1.3.1.2" class="ltx_tr">
<td id="A2.T7.6.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.6.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TDIUC-Tail</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T7.6.2.1" class="ltx_tr">
<th id="A2.T7.6.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T7.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Scene Recognition</span></th>
<td id="A2.T7.6.2.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.2.1.2.1" class="ltx_text" style="font-size:80%;">24.86</span></td>
<td id="A2.T7.6.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.2.1.3.1" class="ltx_text" style="font-size:80%;">29.18</span></td>
</tr>
<tr id="A2.T7.6.3.2" class="ltx_tr">
<th id="A2.T7.6.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.3.2.1.1" class="ltx_text" style="font-size:80%;">Sport Recognition</span></th>
<td id="A2.T7.6.3.2.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.3.2.2.1" class="ltx_text" style="font-size:80%;">54.82</span></td>
<td id="A2.T7.6.3.2.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.3.2.3.1" class="ltx_text" style="font-size:80%;">62.74</span></td>
</tr>
<tr id="A2.T7.6.4.3" class="ltx_tr">
<th id="A2.T7.6.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.4.3.1.1" class="ltx_text" style="font-size:80%;">Color Attributes</span></th>
<td id="A2.T7.6.4.3.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.4.3.2.1" class="ltx_text" style="font-size:80%;">7.03</span></td>
<td id="A2.T7.6.4.3.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.4.3.3.1" class="ltx_text" style="font-size:80%;">84.40</span></td>
</tr>
<tr id="A2.T7.6.5.4" class="ltx_tr">
<th id="A2.T7.6.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.5.4.1.1" class="ltx_text" style="font-size:80%;">Other Attributes</span></th>
<td id="A2.T7.6.5.4.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.5.4.2.1" class="ltx_text" style="font-size:80%;">13.04</span></td>
<td id="A2.T7.6.5.4.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.5.4.3.1" class="ltx_text" style="font-size:80%;">17.01</span></td>
</tr>
<tr id="A2.T7.6.6.5" class="ltx_tr">
<th id="A2.T7.6.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.6.5.1.1" class="ltx_text" style="font-size:80%;">Activity Recognition</span></th>
<td id="A2.T7.6.6.5.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.6.5.2.1" class="ltx_text" style="font-size:80%;">45.48</span></td>
<td id="A2.T7.6.6.5.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.6.5.3.1" class="ltx_text" style="font-size:80%;">64.83</span></td>
</tr>
<tr id="A2.T7.6.7.6" class="ltx_tr">
<th id="A2.T7.6.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.7.6.1.1" class="ltx_text" style="font-size:80%;">Positional Reasoning</span></th>
<td id="A2.T7.6.7.6.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.7.6.2.1" class="ltx_text" style="font-size:80%;">7.46</span></td>
<td id="A2.T7.6.7.6.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.7.6.3.1" class="ltx_text" style="font-size:80%;">10.99</span></td>
</tr>
<tr id="A2.T7.6.8.7" class="ltx_tr">
<th id="A2.T7.6.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.8.7.1.1" class="ltx_text" style="font-size:80%;">Object Recognition</span></th>
<td id="A2.T7.6.8.7.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.8.7.2.1" class="ltx_text" style="font-size:80%;">12.55</span></td>
<td id="A2.T7.6.8.7.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.8.7.3.1" class="ltx_text" style="font-size:80%;">24.20</span></td>
</tr>
<tr id="A2.T7.6.9.8" class="ltx_tr">
<th id="A2.T7.6.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.9.8.1.1" class="ltx_text" style="font-size:80%;">Absurd</span></th>
<td id="A2.T7.6.9.8.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.9.8.2.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A2.T7.6.9.8.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.9.8.3.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
</tr>
<tr id="A2.T7.6.10.9" class="ltx_tr">
<th id="A2.T7.6.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.10.9.1.1" class="ltx_text" style="font-size:80%;">Utility and Affordances</span></th>
<td id="A2.T7.6.10.9.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.10.9.2.1" class="ltx_text" style="font-size:80%;">12.37</span></td>
<td id="A2.T7.6.10.9.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.10.9.3.1" class="ltx_text" style="font-size:80%;">14.02</span></td>
</tr>
<tr id="A2.T7.6.11.10" class="ltx_tr">
<th id="A2.T7.6.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.11.10.1.1" class="ltx_text" style="font-size:80%;">Object Presence</span></th>
<td id="A2.T7.6.11.10.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.11.10.2.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="A2.T7.6.11.10.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.11.10.3.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
</tr>
<tr id="A2.T7.6.12.11" class="ltx_tr">
<th id="A2.T7.6.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.12.11.1.1" class="ltx_text" style="font-size:80%;">Counting</span></th>
<td id="A2.T7.6.12.11.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.12.11.2.1" class="ltx_text" style="font-size:80%;">4.87</span></td>
<td id="A2.T7.6.12.11.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.12.11.3.1" class="ltx_text" style="font-size:80%;">18.96</span></td>
</tr>
<tr id="A2.T7.6.13.12" class="ltx_tr">
<th id="A2.T7.6.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.13.12.1.1" class="ltx_text" style="font-size:80%;">Sentiment Understanding</span></th>
<td id="A2.T7.6.13.12.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.13.12.2.1" class="ltx_text" style="font-size:80%;">12.45</span></td>
<td id="A2.T7.6.13.12.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.13.12.3.1" class="ltx_text" style="font-size:80%;">18.08</span></td>
</tr>
<tr id="A2.T7.6.14.13" class="ltx_tr">
<th id="A2.T7.6.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T7.6.14.13.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic MPT)</span></th>
<td id="A2.T7.6.14.13.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.14.13.2.1" class="ltx_text" style="font-size:80%;">37.66</span></td>
<td id="A2.T7.6.14.13.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.14.13.3.1" class="ltx_text" style="font-size:80%;">51.61</span></td>
</tr>
<tr id="A2.T7.6.15.14" class="ltx_tr">
<th id="A2.T7.6.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.15.14.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic MPT)</span></th>
<td id="A2.T7.6.15.14.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.15.14.2.1" class="ltx_text" style="font-size:80%;">17.51</span></td>
<td id="A2.T7.6.15.14.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.15.14.3.1" class="ltx_text" style="font-size:80%;">43.27</span></td>
</tr>
<tr id="A2.T7.6.16.15" class="ltx_tr">
<th id="A2.T7.6.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T7.6.16.15.1.1" class="ltx_text" style="font-size:80%;">Overall (Arithmetic N-MPT)</span></th>
<td id="A2.T7.6.16.15.2" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.16.15.2.1" class="ltx_text" style="font-size:80%;">19.49</span></td>
<td id="A2.T7.6.16.15.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T7.6.16.15.3.1" class="ltx_text" style="font-size:80%;">34.44</span></td>
</tr>
<tr id="A2.T7.6.17.16" class="ltx_tr">
<th id="A2.T7.6.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T7.6.17.16.1.1" class="ltx_text" style="font-size:80%;">Overall (Harmonic N-MPT)</span></th>
<td id="A2.T7.6.17.16.2" class="ltx_td ltx_align_right"><span id="A2.T7.6.17.16.2.1" class="ltx_text" style="font-size:80%;">11.37</span></td>
<td id="A2.T7.6.17.16.3" class="ltx_td ltx_align_right"><span id="A2.T7.6.17.16.3.1" class="ltx_text" style="font-size:80%;">22.32</span></td>
</tr>
<tr id="A2.T7.6.18.17" class="ltx_tr">
<th id="A2.T7.6.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T7.6.18.17.1.1" class="ltx_text" style="font-size:80%;">Simple Accuracy</span></th>
<td id="A2.T7.6.18.17.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A2.T7.6.18.17.2.1" class="ltx_text" style="font-size:80%;">38.55</span></td>
<td id="A2.T7.6.18.17.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="A2.T7.6.18.17.3.1" class="ltx_text" style="font-size:80%;">50.11</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this section, we present additional experimental results that were omitted from the main paper due to inadequate space. First, the detailed normalized scores for each of the question-types is presented in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. To compute these scores, the accuracy for each unique answer is calculated separately within a question-type and averaged. Second, we present the results from the experiment in section <a href="#S7.SS3" title="7.3 Can Algorithms Predict Rare Answers? ‣ 7 Detailed Analysis of VQA Models ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a> in table <a href="#A2.T6" title="Table 6 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (Unnormalized) and table<a href="#A2.T7" title="Table 7 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (Normalized). The results are evaluated on TDIUC-Tail, which is a subset of TDIUC that only consists of questions that have answers repeated less than 1000 times (uncommon answers). Note that the TDIUC-Tail excludes the absurd and the object presence question-types, as they do not contain any questions with uncommon answers. The algorithms are identical in both Table <a href="#A2.T6" title="Table 6 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A2.T7" title="Table 7 ‣ Appendix B Additional Experimental Results ‣ An Analysis of Visual Question Answering Algorithms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and are named as follows:</p>
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">MCB TDIUC-Full</span> : MCB model trained on whole of the TDIUC dataset and evaluated on TDIUC-Tail.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">MCB TDIUC-Tail</span> : MCB model trained and evaluated on TDIUC-Tail.</p>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1703.09683" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1703.09684" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1703.09684">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1703.09684" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1703.09685" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 23:55:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
