<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task</title>
<!--Generated on Fri Oct  4 12:43:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03381v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S1" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S2" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data Selection and Filtering</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS1" title="In 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>ParIce</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS2" title="In 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Filtering the OPUS Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS3" title="In 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS4" title="In 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Other Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>System Description</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4.SS1" title="In 4 System Description â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>The pipeline</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S5" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S6" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#A1" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>OPUS Texts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#A2" title="In Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Filtering steps</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cogs in a Machine, Doing What Theyâ€™re Meant to Do 
<br class="ltx_break"/>â€“ The AMI Submission to the WMT24 General Translation Task</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Atli Jasonarson, Hinrik Hafsteinsson, Bjarki Ãrmannsson, SteinÃ¾Ã³r SteingrÃ­msson 
<br class="ltx_break"/>The Ãrni MagnÃºsson Institute for Icelandic Studies
<br class="ltx_break"/>ReykjavÃ­k, Iceland
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">atli.jasonarson,hinrik.hafsteinsson,bjarki.armannsson,
<br class="ltx_break"/>steinthor.steingrimsson@arnastofnun.is</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">This paper presents the submission of the Ãrni Magnusson Instituteâ€™s team to the WMT24 General translation task. We work on the English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" stretchy="false" xref="id1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">â†’</annotation></semantics></math>Icelandic translation direction. Our system comprises four translation models and a grammar correction model. For training our models we carefully curate our datasets, aggressively filtering out sentence pairs that may detrimentally affect the quality of our systemâ€™s output. Some of our data are collected from human translations and some are synthetically generated. A part of the synthetic data is generated using an LLM, and we find that it increases the translation capability of our system significantly.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We describe our submission to the 2024 WMT general translation task.
Large Language Models (LLMs) have become near-ubiquitous in the field of Natural Language Processing (NLP) in the last couple of years. They have shown remarkable translation capabilities (see e.g. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib44" title="">Xu etÂ al.</a></cite>,Â <cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib44" title="">2024a</a></cite>), but require significantly larger computational resources than previous neural MT (NMT) models, both for training and inference. Most openly available LLMs are primarily trained on English texts and may therefore need further training in order to be able to translate from or into less-resourced languages, such as Icelandic.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The ALMA models <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib44" title="">2024a</a>)</cite> are LLM-based translation models, built on LLaMA-2. They have been trained to translate ten directions, including English<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mo id="S1.p2.1.m1.1.1" stretchy="false" xref="S1.p2.1.m1.1.1.cmml">â†”</mo><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">â†”</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">â†”</annotation></semantics></math>Icelandic. We explore the capabilities of some of these models, the 7B and 13B parameter versions of ALMA-R <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib45" title="">2024b</a>)</cite>, and find that they generate very competitive translations as measured against the Englishâ€“Icelandic WMT21 test sets <cite class="ltx_cite ltx_citemacro_cite">Akhbardeh etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib1" title="">2021</a>)</cite>, especially from Icelandic into English. Unfortunately, using our settings the translation speed was quite slow (approximately one sentence per second) on an NVIDIA A100 GPU card.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We are interested in building faster models so we use the more traditional encoder-decoder Transformer architecture described in <cite class="ltx_cite ltx_citemacro_citet">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib41" title="">2017</a>)</cite>. We collect all parallel data available to us for our language pair, generate additional synthetic pairs using the ALMA-R 13B parameter model and apply iterative back-translation using our own models. We apply filters to remove sentence pairs that may have detrimental effects on the models output.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We train four Transformer models<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Models available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/arnastofnun" title="">https://huggingface.co/arnastofnun</a>.</span></span></span> of varying sizes and let each model generate five translation candidates. A spelling and grammar checking model is then applied to the translations to generate â€œcorrectedâ€ versions of the sentences. Finally the best candidate is selected from the pool of translations, corrected or not, using a reranking model.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We evaluate our models and approaches on the WMT21 test set for English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" stretchy="false" xref="S1.p5.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">â†’</annotation></semantics></math>Icelandic.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.2">We only submit a system for the English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mo id="S2.p1.1.m1.1.1" stretchy="false" xref="S2.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">â†’</annotation></semantics></math>Icelandic translation direction. This language pair was previously one of the pairs for the WMT General Translation shared task in 2021 but prior to that, limited work had been published on MT for Icelandic. <cite class="ltx_cite ltx_citemacro_citet">Brandt etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib7" title="">2011</a>)</cite> describe a rule-based system for translating Icelandic<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mo id="S2.p1.2.m2.1.1" stretchy="false" xref="S2.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">â†’</annotation></semantics></math>English, based on Apertium <cite class="ltx_cite ltx_citemacro_cite">Forcada etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib11" title="">2011</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">JÃ³nsson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib18" title="">2020</a>)</cite> was the first published work describing SMT and NMT for Icelandic. Since 2021 the WMT21 evaluation data, as well as various parallel corpora projects, have made it more accessible to train and evaluate MT systems translating to or from Icelandic, and with that the language has been included in various research projects. We believe this is an indicator of the importance of evaluation campaigns, such as the ones run in association with the WMT conferences, for less prominent languages.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Our approach uses an ensemble of four different translation models and a reranking model to select the best candidate. This is a common approach, motivated by the intuition that different systems may have different strengths. In recent work, <cite class="ltx_cite ltx_citemacro_citet">Toral etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib39" title="">2023</a>)</cite> use this approach in their experiments with literary translations. In their work on bidirectional reranking, <cite class="ltx_cite ltx_citemacro_citet">Imamura and Sumita (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib12" title="">2017</a>)</cite> discuss reranking and ensembling for MT in some detail. Examples from the period of statistical MT include the work of <cite class="ltx_cite ltx_citemacro_citet">Olteanu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib24" title="">2006</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib42" title="">2007</a>)</cite>, describing language model-based reranking on hypotheses generated by phrase-based SMT systems.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data Selection and Filtering</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Various parallel data are available for the Englishâ€“Icelandic language pair. ParIce <cite class="ltx_cite ltx_citemacro_cite">Barkarson and SteingrÃ­msson (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib6" title="">2019</a>)</cite> is partly a collection of parallel corpora available elsewhere, which has been realigned and refiltered, and partly data compiled for that project, the largest source being regulatory texts published in relation with the European Economic Area (EEA) agreement. Data for the Englishâ€“Icelandic language pair were collected within the Paracrawl project <cite class="ltx_cite ltx_citemacro_cite">BaÃ±Ã³n etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib4" title="">2020</a>)</cite>, CCMatrix <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib31" title="">2021</a>)</cite>, MaCoCu <cite class="ltx_cite ltx_citemacro_cite">BaÃ±Ã³n etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib5" title="">2022</a>)</cite> and HPLT <cite class="ltx_cite ltx_citemacro_cite">Aulamo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib3" title="">2023</a>)</cite>. Data for the language pair are also available from multiple smaller datasets distributed on OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib38" title="">2020</a>)</cite>. We utilize all these datasets in training our models.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We also use synthetic data: Backtranslations made available by <cite class="ltx_cite ltx_citemacro_citet">JÃ³nsson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib17" title="">2022</a>)</cite>, translations generated using the ALMA-R 13B parameter model and backtranslations generated by our trained models. We describe these in more detail in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS3" title="3.3 Synthetic Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Khayrallah and Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib21" title="">2018</a>)</cite> show that incorrect translations, untranslated target text, misalignments, and other noisy segments in training data can have a detrimental effect on the quality of translations generated by NMT systems trained on that data. By filtering our training data rather aggressively, we try to minimize such noise.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>ParIce</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Even though care has been taken to realign and refilter data for the ParIce corpus, <cite class="ltx_cite ltx_citemacro_citet">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib34" title="">2023</a>)</cite> show that it still contains noise, such as misalignments and mistranslations, that may be detrimental when training NMT systems. They refilter the data using a combination of approaches: Shallow filters based on simple heuristics, by using Bicleaner <cite class="ltx_cite ltx_citemacro_cite">SÃ¡nchez-Cartagena etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib30" title="">2018</a>); RamÃ­rez-SÃ¡nchez etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib27" title="">2020</a>)</cite> and by employing classifiers (support vector machine-based ones <cite class="ltx_cite ltx_citemacro_cite">Cortes and Vapnik (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib8" title="">1995</a>)</cite> had the best outcome) with a combination of scoring mechanisms, including LASER <cite class="ltx_cite ltx_citemacro_cite">Artetxe and Schwenk (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib2" title="">2019</a>)</cite>, LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib10" title="">2022</a>)</cite>, NMTScore <cite class="ltx_cite ltx_citemacro_cite">Vamvas and Sennrich (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib40" title="">2022</a>)</cite> using the M2M100 multilingual translation model <cite class="ltx_cite ltx_citemacro_cite">Fan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib9" title="">2021</a>)</cite>, and WAScore, a word alignment-based score devised to measure word-level parallelism, introduced in <cite class="ltx_cite ltx_citemacro_citet">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib35" title="">2021</a>)</cite>. In <cite class="ltx_cite ltx_citemacro_citet">SteingrÃ­msson (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib36" title="">2023</a>)</cite> these data are processed further by realigning the EEA texts in the ParIce corpus using SentAlign <cite class="ltx_cite ltx_citemacro_cite">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib37" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">As the basis for our training we use the ParIce dataset, processed as described above, as well as parallel data extracted from Wikipedia using the comparable corpora mining approach described in <cite class="ltx_cite ltx_citemacro_citep">(SteingrÃ­msson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib35" title="">2021</a>)</cite> and sentence pairs extracted from version 9 of Paracrawl using the filtering approaches described above and in <cite class="ltx_cite ltx_citemacro_citet">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib34" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S3.F1.g1" src="extracted/5901757/filter_stats.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Each filtering stepâ€™s effect on OPUS dataset size</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Filtering the OPUS Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">An overview of the data for Icelandic-English parallel texts sourced from the OPUS catalog is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#A1" title="Appendix A OPUS Texts â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">A</span></a>. This data, accounting for redundant sentence pairs, amounts to 21.167.708<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This applies to the state of the OPUS catalog at the time of development, i.e., April 2024.</span></span></span> sentence pairs. At face value, this is a substantial amount of available data. However, the quality of these parallel texts is not reliable, with noisy and incorrect pairs being prevalent throughout most individual datasets in the catalog. To remedy this, and thus ensure that the data sourced via OPUS can be used effectively in our project, we applied an aggressive, sequential filtering process, with the goal of whittling away the majority of the low-quality sentence pairs.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Our sequential filtering process consists of ten individual steps, most of which only remove sentences from the data without modifying the content of other sentences. The process is <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">sequential</span>, in that the input of a filtering step is the output of the previous filtering step. Furthermore, the order of these steps is decided to ensure optimal processing time of the filters so that computationally heavy filtering steps process the least amount of data, which minimizes run time. For a detailed overview of each filtering step, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#A2" title="Appendix B Filtering steps â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The effects of each filtering step on the data amount is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.F1" title="Figure 1 â€£ 3.1 ParIce â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">1</span></a>.
To ensure that our filtering methods affected our implementation positively, we intermittently added the output of the filtering process to our training pipeline and evaluated the performance. In particular, we used this approach to dial in the optimal LaBSE and NMT score cutoffs in our filters.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The final output of our filtering process produces a relatively high-quality data set of 2.056.704 English-Icelandic sentence pairs (roughly 9.71% of the original 21.167.708 raw sentence pairs sourced from the OPUS catalog), which we then add to our training data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The dataset made available by <cite class="ltx_cite ltx_citemacro_citet">JÃ³nsson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib17" title="">2022</a>)</cite> contains translations from Europarl, Newscrawl, Wikipedia and the IGC. We perform a filtering step similar to the one used applied on the OPUS data, consisting of a length filter, removing all sentences that have fewer than four word tokens and more than 150, an overlap filter, removing all sentence pairs that share 40% or more of word tokens, and a symbol filter removing all sentence pairs where more than 20% of characters in one of the sentences is non-alphabetical. Furthermore we use two scoring mechanisms for filtering, LaBSE, using a score threshold of 0.8, and NMTScore with a threshold of 0.4. These scores are selected based on the evaluation in <cite class="ltx_cite ltx_citemacro_cite">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib34" title="">2023</a>)</cite>. After filtering, we are left with 4.4M sentence pairs from this dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We use the 13B parameter ALMA-R model to translate English sentences from Newscrawl to Icelandic and Icelandic texts from the Icelandic Gigaword Corpus (IGC) <cite class="ltx_cite ltx_citemacro_cite">SteingrÃ­msson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib33" title="">2018</a>)</cite> to English. The Icelandic texts are sampled from three different subcorpora of the IGC, comprising news, scholarly journals, and literary texts. For each source sentence we generate five translations and use LaBSE to select the two best ones, granted that they exceed a threshold of a LaBSE score of 0.8 and pass through the three shallow filters described above: length, overlap and symbol filters. Our final set contains 8.9M sentence pairs translated from Icelandic to English and 700K sentence pairs translated from English to Icelandic.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Finally, we do iterative back-translation. We use the same training data as described above to train models to translate texts from the IGC to English. For the back-translations we use Transformer<sub class="ltx_sub" id="S3.SS3.p3.1.1">BIG</sub> models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib41" title="">2017</a>)</cite>, as described in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.T1" title="Table 1 â€£ 3.3 Synthetic Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">1</span></a>. We use the same approach as before, generate five translations for each sentence and use LaBSE to select the two best ones, as long as they exceed the threshold of 0.8 and are not filtered out by the other filters. We do two iterations of translating and training models in both translation directions using backtranslated data. This results in a total of approximately 60M sentence pairs.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.5.5.6"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.6.1">model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1"><math alttext="d_{model}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><msub id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">d</mi><mrow id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T1.1.1.1.m1.1.1.3.2" xref="S3.T1.1.1.1.m1.1.1.3.2.cmml">m</mi><mo id="S3.T1.1.1.1.m1.1.1.3.1" xref="S3.T1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.3" xref="S3.T1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.T1.1.1.1.m1.1.1.3.1a" xref="S3.T1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.4" xref="S3.T1.1.1.1.m1.1.1.3.4.cmml">d</mi><mo id="S3.T1.1.1.1.m1.1.1.3.1b" xref="S3.T1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.5" xref="S3.T1.1.1.1.m1.1.1.3.5.cmml">e</mi><mo id="S3.T1.1.1.1.m1.1.1.3.1c" xref="S3.T1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.6" xref="S3.T1.1.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">ğ‘‘</ci><apply id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3"><times id="S3.T1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.1.1.1.m1.1.1.3.2">ğ‘š</ci><ci id="S3.T1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S3.T1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T1.1.1.1.m1.1.1.3.4">ğ‘‘</ci><ci id="S3.T1.1.1.1.m1.1.1.3.5.cmml" xref="S3.T1.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="S3.T1.1.1.1.m1.1.1.3.6.cmml" xref="S3.T1.1.1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">d_{model}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.2"><math alttext="d_{ff}" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><msub id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.m1.1.1.2.cmml">d</mi><mrow id="S3.T1.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.m1.1.1.3.cmml"><mi id="S3.T1.2.2.2.m1.1.1.3.2" xref="S3.T1.2.2.2.m1.1.1.3.2.cmml">f</mi><mo id="S3.T1.2.2.2.m1.1.1.3.1" xref="S3.T1.2.2.2.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.2.2.2.m1.1.1.3.3" xref="S3.T1.2.2.2.m1.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T1.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.m1.1.1.2">ğ‘‘</ci><apply id="S3.T1.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3"><times id="S3.T1.2.2.2.m1.1.1.3.1.cmml" xref="S3.T1.2.2.2.m1.1.1.3.1"></times><ci id="S3.T1.2.2.2.m1.1.1.3.2.cmml" xref="S3.T1.2.2.2.m1.1.1.3.2">ğ‘“</ci><ci id="S3.T1.2.2.2.m1.1.1.3.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">d_{ff}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.3.3"><math alttext="h" class="ltx_Math" display="inline" id="S3.T1.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.m1.1a"><mi id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.m1.1d">italic_h</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.4.4"><math alttext="N_{enc}" class="ltx_Math" display="inline" id="S3.T1.4.4.4.m1.1"><semantics id="S3.T1.4.4.4.m1.1a"><msub id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml"><mi id="S3.T1.4.4.4.m1.1.1.2" xref="S3.T1.4.4.4.m1.1.1.2.cmml">N</mi><mrow id="S3.T1.4.4.4.m1.1.1.3" xref="S3.T1.4.4.4.m1.1.1.3.cmml"><mi id="S3.T1.4.4.4.m1.1.1.3.2" xref="S3.T1.4.4.4.m1.1.1.3.2.cmml">e</mi><mo id="S3.T1.4.4.4.m1.1.1.3.1" xref="S3.T1.4.4.4.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.4.4.4.m1.1.1.3.3" xref="S3.T1.4.4.4.m1.1.1.3.3.cmml">n</mi><mo id="S3.T1.4.4.4.m1.1.1.3.1a" xref="S3.T1.4.4.4.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.4.4.4.m1.1.1.3.4" xref="S3.T1.4.4.4.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><apply id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.4.4.4.m1.1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">subscript</csymbol><ci id="S3.T1.4.4.4.m1.1.1.2.cmml" xref="S3.T1.4.4.4.m1.1.1.2">ğ‘</ci><apply id="S3.T1.4.4.4.m1.1.1.3.cmml" xref="S3.T1.4.4.4.m1.1.1.3"><times id="S3.T1.4.4.4.m1.1.1.3.1.cmml" xref="S3.T1.4.4.4.m1.1.1.3.1"></times><ci id="S3.T1.4.4.4.m1.1.1.3.2.cmml" xref="S3.T1.4.4.4.m1.1.1.3.2">ğ‘’</ci><ci id="S3.T1.4.4.4.m1.1.1.3.3.cmml" xref="S3.T1.4.4.4.m1.1.1.3.3">ğ‘›</ci><ci id="S3.T1.4.4.4.m1.1.1.3.4.cmml" xref="S3.T1.4.4.4.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">N_{enc}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.m1.1d">italic_N start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.5.5.5"><math alttext="N_{dec}" class="ltx_Math" display="inline" id="S3.T1.5.5.5.m1.1"><semantics id="S3.T1.5.5.5.m1.1a"><msub id="S3.T1.5.5.5.m1.1.1" xref="S3.T1.5.5.5.m1.1.1.cmml"><mi id="S3.T1.5.5.5.m1.1.1.2" xref="S3.T1.5.5.5.m1.1.1.2.cmml">N</mi><mrow id="S3.T1.5.5.5.m1.1.1.3" xref="S3.T1.5.5.5.m1.1.1.3.cmml"><mi id="S3.T1.5.5.5.m1.1.1.3.2" xref="S3.T1.5.5.5.m1.1.1.3.2.cmml">d</mi><mo id="S3.T1.5.5.5.m1.1.1.3.1" xref="S3.T1.5.5.5.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.5.5.5.m1.1.1.3.3" xref="S3.T1.5.5.5.m1.1.1.3.3.cmml">e</mi><mo id="S3.T1.5.5.5.m1.1.1.3.1a" xref="S3.T1.5.5.5.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.T1.5.5.5.m1.1.1.3.4" xref="S3.T1.5.5.5.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.1b"><apply id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.5.5.5.m1.1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">subscript</csymbol><ci id="S3.T1.5.5.5.m1.1.1.2.cmml" xref="S3.T1.5.5.5.m1.1.1.2">ğ‘</ci><apply id="S3.T1.5.5.5.m1.1.1.3.cmml" xref="S3.T1.5.5.5.m1.1.1.3"><times id="S3.T1.5.5.5.m1.1.1.3.1.cmml" xref="S3.T1.5.5.5.m1.1.1.3.1"></times><ci id="S3.T1.5.5.5.m1.1.1.3.2.cmml" xref="S3.T1.5.5.5.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.T1.5.5.5.m1.1.1.3.3.cmml" xref="S3.T1.5.5.5.m1.1.1.3.3">ğ‘’</ci><ci id="S3.T1.5.5.5.m1.1.1.3.4.cmml" xref="S3.T1.5.5.5.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.1c">N_{dec}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.m1.1d">italic_N start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.1"><math alttext="Base" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.1"><semantics id="S3.T1.6.6.1.m1.1a"><mrow id="S3.T1.6.6.1.m1.1.1" xref="S3.T1.6.6.1.m1.1.1.cmml"><mi id="S3.T1.6.6.1.m1.1.1.2" xref="S3.T1.6.6.1.m1.1.1.2.cmml">B</mi><mo id="S3.T1.6.6.1.m1.1.1.1" xref="S3.T1.6.6.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.6.6.1.m1.1.1.3" xref="S3.T1.6.6.1.m1.1.1.3.cmml">a</mi><mo id="S3.T1.6.6.1.m1.1.1.1a" xref="S3.T1.6.6.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.6.6.1.m1.1.1.4" xref="S3.T1.6.6.1.m1.1.1.4.cmml">s</mi><mo id="S3.T1.6.6.1.m1.1.1.1b" xref="S3.T1.6.6.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.6.6.1.m1.1.1.5" xref="S3.T1.6.6.1.m1.1.1.5.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.1b"><apply id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1"><times id="S3.T1.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1.1"></times><ci id="S3.T1.6.6.1.m1.1.1.2.cmml" xref="S3.T1.6.6.1.m1.1.1.2">ğµ</ci><ci id="S3.T1.6.6.1.m1.1.1.3.cmml" xref="S3.T1.6.6.1.m1.1.1.3">ğ‘</ci><ci id="S3.T1.6.6.1.m1.1.1.4.cmml" xref="S3.T1.6.6.1.m1.1.1.4">ğ‘ </ci><ci id="S3.T1.6.6.1.m1.1.1.5.cmml" xref="S3.T1.6.6.1.m1.1.1.5">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.1c">Base</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.1.m1.1d">italic_B italic_a italic_s italic_e</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.3">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.4">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.5">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6">6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.7.7.1"><math alttext="Base_{deep}" class="ltx_Math" display="inline" id="S3.T1.7.7.1.m1.1"><semantics id="S3.T1.7.7.1.m1.1a"><mrow id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.1.m1.1.1.2" xref="S3.T1.7.7.1.m1.1.1.2.cmml">B</mi><mo id="S3.T1.7.7.1.m1.1.1.1" xref="S3.T1.7.7.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.7.7.1.m1.1.1.3" xref="S3.T1.7.7.1.m1.1.1.3.cmml">a</mi><mo id="S3.T1.7.7.1.m1.1.1.1a" xref="S3.T1.7.7.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.7.7.1.m1.1.1.4" xref="S3.T1.7.7.1.m1.1.1.4.cmml">s</mi><mo id="S3.T1.7.7.1.m1.1.1.1b" xref="S3.T1.7.7.1.m1.1.1.1.cmml">â¢</mo><msub id="S3.T1.7.7.1.m1.1.1.5" xref="S3.T1.7.7.1.m1.1.1.5.cmml"><mi id="S3.T1.7.7.1.m1.1.1.5.2" xref="S3.T1.7.7.1.m1.1.1.5.2.cmml">e</mi><mrow id="S3.T1.7.7.1.m1.1.1.5.3" xref="S3.T1.7.7.1.m1.1.1.5.3.cmml"><mi id="S3.T1.7.7.1.m1.1.1.5.3.2" xref="S3.T1.7.7.1.m1.1.1.5.3.2.cmml">d</mi><mo id="S3.T1.7.7.1.m1.1.1.5.3.1" xref="S3.T1.7.7.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S3.T1.7.7.1.m1.1.1.5.3.3" xref="S3.T1.7.7.1.m1.1.1.5.3.3.cmml">e</mi><mo id="S3.T1.7.7.1.m1.1.1.5.3.1a" xref="S3.T1.7.7.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S3.T1.7.7.1.m1.1.1.5.3.4" xref="S3.T1.7.7.1.m1.1.1.5.3.4.cmml">e</mi><mo id="S3.T1.7.7.1.m1.1.1.5.3.1b" xref="S3.T1.7.7.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S3.T1.7.7.1.m1.1.1.5.3.5" xref="S3.T1.7.7.1.m1.1.1.5.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><apply id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"><times id="S3.T1.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1.1"></times><ci id="S3.T1.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.1.m1.1.1.2">ğµ</ci><ci id="S3.T1.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.1.m1.1.1.3">ğ‘</ci><ci id="S3.T1.7.7.1.m1.1.1.4.cmml" xref="S3.T1.7.7.1.m1.1.1.4">ğ‘ </ci><apply id="S3.T1.7.7.1.m1.1.1.5.cmml" xref="S3.T1.7.7.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.T1.7.7.1.m1.1.1.5.1.cmml" xref="S3.T1.7.7.1.m1.1.1.5">subscript</csymbol><ci id="S3.T1.7.7.1.m1.1.1.5.2.cmml" xref="S3.T1.7.7.1.m1.1.1.5.2">ğ‘’</ci><apply id="S3.T1.7.7.1.m1.1.1.5.3.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3"><times id="S3.T1.7.7.1.m1.1.1.5.3.1.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3.1"></times><ci id="S3.T1.7.7.1.m1.1.1.5.3.2.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3.2">ğ‘‘</ci><ci id="S3.T1.7.7.1.m1.1.1.5.3.3.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3.3">ğ‘’</ci><ci id="S3.T1.7.7.1.m1.1.1.5.3.4.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3.4">ğ‘’</ci><ci id="S3.T1.7.7.1.m1.1.1.5.3.5.cmml" xref="S3.T1.7.7.1.m1.1.1.5.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">Base_{deep}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.1.m1.1d">italic_B italic_a italic_s italic_e start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.2">512</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.3">2048</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.4">8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.5">36</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.6">12</td>
</tr>
<tr class="ltx_tr" id="S3.T1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.8.8.1"><math alttext="Big" class="ltx_Math" display="inline" id="S3.T1.8.8.1.m1.1"><semantics id="S3.T1.8.8.1.m1.1a"><mrow id="S3.T1.8.8.1.m1.1.1" xref="S3.T1.8.8.1.m1.1.1.cmml"><mi id="S3.T1.8.8.1.m1.1.1.2" xref="S3.T1.8.8.1.m1.1.1.2.cmml">B</mi><mo id="S3.T1.8.8.1.m1.1.1.1" xref="S3.T1.8.8.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.8.8.1.m1.1.1.3" xref="S3.T1.8.8.1.m1.1.1.3.cmml">i</mi><mo id="S3.T1.8.8.1.m1.1.1.1a" xref="S3.T1.8.8.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.8.8.1.m1.1.1.4" xref="S3.T1.8.8.1.m1.1.1.4.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.1.m1.1b"><apply id="S3.T1.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.1.m1.1.1"><times id="S3.T1.8.8.1.m1.1.1.1.cmml" xref="S3.T1.8.8.1.m1.1.1.1"></times><ci id="S3.T1.8.8.1.m1.1.1.2.cmml" xref="S3.T1.8.8.1.m1.1.1.2">ğµ</ci><ci id="S3.T1.8.8.1.m1.1.1.3.cmml" xref="S3.T1.8.8.1.m1.1.1.3">ğ‘–</ci><ci id="S3.T1.8.8.1.m1.1.1.4.cmml" xref="S3.T1.8.8.1.m1.1.1.4">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.1.m1.1c">Big</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.1.m1.1d">italic_B italic_i italic_g</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.3">4096</td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.4">16</td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.5">6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.6">6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.T1.9.9.1"><math alttext="Big_{deep}" class="ltx_Math" display="inline" id="S3.T1.9.9.1.m1.1"><semantics id="S3.T1.9.9.1.m1.1a"><mrow id="S3.T1.9.9.1.m1.1.1" xref="S3.T1.9.9.1.m1.1.1.cmml"><mi id="S3.T1.9.9.1.m1.1.1.2" xref="S3.T1.9.9.1.m1.1.1.2.cmml">B</mi><mo id="S3.T1.9.9.1.m1.1.1.1" xref="S3.T1.9.9.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.T1.9.9.1.m1.1.1.3" xref="S3.T1.9.9.1.m1.1.1.3.cmml">i</mi><mo id="S3.T1.9.9.1.m1.1.1.1a" xref="S3.T1.9.9.1.m1.1.1.1.cmml">â¢</mo><msub id="S3.T1.9.9.1.m1.1.1.4" xref="S3.T1.9.9.1.m1.1.1.4.cmml"><mi id="S3.T1.9.9.1.m1.1.1.4.2" xref="S3.T1.9.9.1.m1.1.1.4.2.cmml">g</mi><mrow id="S3.T1.9.9.1.m1.1.1.4.3" xref="S3.T1.9.9.1.m1.1.1.4.3.cmml"><mi id="S3.T1.9.9.1.m1.1.1.4.3.2" xref="S3.T1.9.9.1.m1.1.1.4.3.2.cmml">d</mi><mo id="S3.T1.9.9.1.m1.1.1.4.3.1" xref="S3.T1.9.9.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S3.T1.9.9.1.m1.1.1.4.3.3" xref="S3.T1.9.9.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S3.T1.9.9.1.m1.1.1.4.3.1a" xref="S3.T1.9.9.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S3.T1.9.9.1.m1.1.1.4.3.4" xref="S3.T1.9.9.1.m1.1.1.4.3.4.cmml">e</mi><mo id="S3.T1.9.9.1.m1.1.1.4.3.1b" xref="S3.T1.9.9.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S3.T1.9.9.1.m1.1.1.4.3.5" xref="S3.T1.9.9.1.m1.1.1.4.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.1.m1.1b"><apply id="S3.T1.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.1.m1.1.1"><times id="S3.T1.9.9.1.m1.1.1.1.cmml" xref="S3.T1.9.9.1.m1.1.1.1"></times><ci id="S3.T1.9.9.1.m1.1.1.2.cmml" xref="S3.T1.9.9.1.m1.1.1.2">ğµ</ci><ci id="S3.T1.9.9.1.m1.1.1.3.cmml" xref="S3.T1.9.9.1.m1.1.1.3">ğ‘–</ci><apply id="S3.T1.9.9.1.m1.1.1.4.cmml" xref="S3.T1.9.9.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.T1.9.9.1.m1.1.1.4.1.cmml" xref="S3.T1.9.9.1.m1.1.1.4">subscript</csymbol><ci id="S3.T1.9.9.1.m1.1.1.4.2.cmml" xref="S3.T1.9.9.1.m1.1.1.4.2">ğ‘”</ci><apply id="S3.T1.9.9.1.m1.1.1.4.3.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3"><times id="S3.T1.9.9.1.m1.1.1.4.3.1.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3.1"></times><ci id="S3.T1.9.9.1.m1.1.1.4.3.2.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3.2">ğ‘‘</ci><ci id="S3.T1.9.9.1.m1.1.1.4.3.3.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3.3">ğ‘’</ci><ci id="S3.T1.9.9.1.m1.1.1.4.3.4.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3.4">ğ‘’</ci><ci id="S3.T1.9.9.1.m1.1.1.4.3.5.cmml" xref="S3.T1.9.9.1.m1.1.1.4.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.1.m1.1c">Big_{deep}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.1.m1.1d">italic_B italic_i italic_g start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.9.9.2">1024</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.9.9.3">4096</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.9.9.4">16</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.9.9.5">36</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.9.9.6">12</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model dimensions, heads and number of layers.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">chrF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.1.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.2">50.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T2.1.3.2.1">Baseline+lexicon</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.2">50.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T2.1.4.3.1">Baseline+OPUS</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.2">53.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.4.1">Baseline+JÃ³nsson</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.2">53.5</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.5.1">Baseline+JÃ³nsson+SMT</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.2">53.2</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.6.1">Baseline+JÃ³nsson+ALMA</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.2">54.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.7.1">Baseline+JÃ³nsson+ALMA+OPUS</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.2">55.1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.8.1">Baseline+JÃ³nsson+ALMA+OPUS+BT1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.2">56.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T2.1.10.9.1">Baseline+JÃ³nsson+ALMA+OPUS+BT2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.10.9.2">56.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The table shows that when most of the datasets in our experiments are added to the training data the quality, as measured by chrF, increases. Exceptions to that are the experiments with adding token-pairs from an English-Icelandic lexicon and with using backtranslations generated by an SMT system. These two datasets are therefore not used in our final systems.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Other Data</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To decide which datasets to use, we trained Transformer<sub class="ltx_sub" id="S3.SS4.p1.1.1">BASE</sub> models as described in <cite class="ltx_cite ltx_citemacro_citet">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib41" title="">2017</a>)</cite> and evaluated the models using the test set from WMT21. We started by training a baseline system using the dataset described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS1" title="3.1 ParIce â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We then added different datasets to the baseline data, trained new systems and evaluated them. If the new dataset seemed to improve the output we used that for our final system. In addition to previously described datasets we tried generating backtranslations using SMT and to add data from a bilingual lexicon using token-pair training as described by <cite class="ltx_cite ltx_citemacro_citet">Jones etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib16" title="">2023</a>)</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.T2" title="Table 2 â€£ 3.3 Synthetic Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">2</span></a> shows chrF scores <cite class="ltx_cite ltx_citemacro_cite">PopoviÄ‡ (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib25" title="">2015</a>)</cite> for our different experiments.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The total number of sentence pairs used for training is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.T3" title="Table 3 â€£ 3.4 Other Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1">Sentence Pairs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.2.1.1">Base</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.1.2">2,277,023</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.3.2.1">OPUS-filtered</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.2.2">2,056,704</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.4.3.1">MiÃ°eind-BT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.3.2">2,559,806</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.5.4.1">MiÃ°eind-FT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.4.2">1,837,945</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.6.5.1">ALMA-BT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.5.2">8,927,720</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.7.6.1">ALMA-FT</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.6.2">700,253</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.8.7.1">IGC-BT-1</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.7.2">27,794,398</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.T3.1.9.8.1">IGC-BT-2</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.9.8.2">33,465,175</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Datasets used for training and number of sentence pairs in each dataset.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>System Description</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our motivation for using multiple models is twofold: First, we want to use models that are computationally inexpensive to run and so we train models that can run on one consumer grade GPU. Second, systems of different sizes may have complementary strengths and so training multiple systems and reranking the results may give us better results than any one model.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We train four encoder-decoder Transformer models, all of which play a part in the translation pipeline. Two of the models follow the exact architecture described inÂ <cite class="ltx_cite ltx_citemacro_citet">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib41" title="">2017</a>)</cite>, i.e. the â€˜baseâ€™ and â€˜bigâ€™ versions of the original Transformer model, while the other two are deeper, using 36 encoder layers and 12 decoder layers instead of six. The difference between the four models is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.T1" title="Table 1 â€£ 3.3 Synthetic Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The outputs from the translation models undergo two post-processing steps. First, they are run through a grammatical error correction model, a version of the byte-level sequence-to-sequence model ByT5Â <cite class="ltx_cite ltx_citemacro_cite">Xue etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib46" title="">2022</a>)</cite> that has been fine-tuned by Â <cite class="ltx_cite ltx_citemacro_citet">IngÃ³lfsdÃ³ttir etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib13" title="">2023</a>)</cite> to correct spelling errors in Icelandic as well as handling more complex grammatical, semantic and stylistic issues. Second, we fix punctuation errors which translation models are prone to making when translating into Icelandic (mostly to do with quotation marks, which are different in Icelandic and English) as well as some that might be unique to our system, such as their incapability to translate emojis. As the grammatical error correction model proved too aggressive for our purposes, merging and splitting some sentences, normalizing informal language usage and hashtags, etc., we also revert some of the changes it introduced.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Using the WMT21 test set we experiment with an ensemble approach, using <span class="ltx_text ltx_font_smallcaps" id="S4.p4.1.1">CometKiwi-da-22</span>Â <cite class="ltx_cite ltx_citemacro_cite">Rei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib29" title="">2022</a>)</cite> to select the best sentence out of 20 hypotheses made by the four models (each model generates five hypotheses using beam search with beam size 12). This raises the chrF score to 58.3 for our evaluation set. On top of this we add the spelling and grammar error correction, which gives us a very modest increase in quality as measured by chrF, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4.T4" title="Table 4 â€£ 4 System Description â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">We investigate whether the <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.1">CometKiwi-da-22</span> model prefers the output from some of the translation models over the others. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4.T5" title="Table 5 â€£ 4.1 The pipeline â€£ 4 System Description â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">5</span></a> shows which translation models generated the translations ultimately chosen by the scoring model when experimenting on the WMT21 evaluation set of 1000 sentences. While translations by the deeper model are more likely to be selected, it is evident that all models are contributing, with the final selection containing 753 translation generated by only one model, and of these all models contribute over 150 translations each. 247 of the selected translations were generated by more than one model (non-unique translations). An ensemble approach thus seems to be likely to improve overall translation quality.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.4.5.1.1.1">model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.4.5.1.2">chrF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1"><math alttext="Base" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mrow id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.m1.1.1.2.cmml">B</mi><mo id="S4.T4.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.m1.1.1.3.cmml">a</mi><mo id="S4.T4.1.1.1.m1.1.1.1a" xref="S4.T4.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.1.1.1.m1.1.1.4" xref="S4.T4.1.1.1.m1.1.1.4.cmml">s</mi><mo id="S4.T4.1.1.1.m1.1.1.1b" xref="S4.T4.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.1.1.1.m1.1.1.5" xref="S4.T4.1.1.1.m1.1.1.5.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><times id="S4.T4.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1.1"></times><ci id="S4.T4.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.m1.1.1.2">ğµ</ci><ci id="S4.T4.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3">ğ‘</ci><ci id="S4.T4.1.1.1.m1.1.1.4.cmml" xref="S4.T4.1.1.1.m1.1.1.4">ğ‘ </ci><ci id="S4.T4.1.1.1.m1.1.1.5.cmml" xref="S4.T4.1.1.1.m1.1.1.5">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">Base</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">italic_B italic_a italic_s italic_e</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2">56.8</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.2.1"><math alttext="Base_{deep}" class="ltx_Math" display="inline" id="S4.T4.2.2.1.m1.1"><semantics id="S4.T4.2.2.1.m1.1a"><mrow id="S4.T4.2.2.1.m1.1.1" xref="S4.T4.2.2.1.m1.1.1.cmml"><mi id="S4.T4.2.2.1.m1.1.1.2" xref="S4.T4.2.2.1.m1.1.1.2.cmml">B</mi><mo id="S4.T4.2.2.1.m1.1.1.1" xref="S4.T4.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.2.2.1.m1.1.1.3" xref="S4.T4.2.2.1.m1.1.1.3.cmml">a</mi><mo id="S4.T4.2.2.1.m1.1.1.1a" xref="S4.T4.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.2.2.1.m1.1.1.4" xref="S4.T4.2.2.1.m1.1.1.4.cmml">s</mi><mo id="S4.T4.2.2.1.m1.1.1.1b" xref="S4.T4.2.2.1.m1.1.1.1.cmml">â¢</mo><msub id="S4.T4.2.2.1.m1.1.1.5" xref="S4.T4.2.2.1.m1.1.1.5.cmml"><mi id="S4.T4.2.2.1.m1.1.1.5.2" xref="S4.T4.2.2.1.m1.1.1.5.2.cmml">e</mi><mrow id="S4.T4.2.2.1.m1.1.1.5.3" xref="S4.T4.2.2.1.m1.1.1.5.3.cmml"><mi id="S4.T4.2.2.1.m1.1.1.5.3.2" xref="S4.T4.2.2.1.m1.1.1.5.3.2.cmml">d</mi><mo id="S4.T4.2.2.1.m1.1.1.5.3.1" xref="S4.T4.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T4.2.2.1.m1.1.1.5.3.3" xref="S4.T4.2.2.1.m1.1.1.5.3.3.cmml">e</mi><mo id="S4.T4.2.2.1.m1.1.1.5.3.1a" xref="S4.T4.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T4.2.2.1.m1.1.1.5.3.4" xref="S4.T4.2.2.1.m1.1.1.5.3.4.cmml">e</mi><mo id="S4.T4.2.2.1.m1.1.1.5.3.1b" xref="S4.T4.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T4.2.2.1.m1.1.1.5.3.5" xref="S4.T4.2.2.1.m1.1.1.5.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.1.m1.1b"><apply id="S4.T4.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.1.m1.1.1"><times id="S4.T4.2.2.1.m1.1.1.1.cmml" xref="S4.T4.2.2.1.m1.1.1.1"></times><ci id="S4.T4.2.2.1.m1.1.1.2.cmml" xref="S4.T4.2.2.1.m1.1.1.2">ğµ</ci><ci id="S4.T4.2.2.1.m1.1.1.3.cmml" xref="S4.T4.2.2.1.m1.1.1.3">ğ‘</ci><ci id="S4.T4.2.2.1.m1.1.1.4.cmml" xref="S4.T4.2.2.1.m1.1.1.4">ğ‘ </ci><apply id="S4.T4.2.2.1.m1.1.1.5.cmml" xref="S4.T4.2.2.1.m1.1.1.5"><csymbol cd="ambiguous" id="S4.T4.2.2.1.m1.1.1.5.1.cmml" xref="S4.T4.2.2.1.m1.1.1.5">subscript</csymbol><ci id="S4.T4.2.2.1.m1.1.1.5.2.cmml" xref="S4.T4.2.2.1.m1.1.1.5.2">ğ‘’</ci><apply id="S4.T4.2.2.1.m1.1.1.5.3.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3"><times id="S4.T4.2.2.1.m1.1.1.5.3.1.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3.1"></times><ci id="S4.T4.2.2.1.m1.1.1.5.3.2.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3.2">ğ‘‘</ci><ci id="S4.T4.2.2.1.m1.1.1.5.3.3.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3.3">ğ‘’</ci><ci id="S4.T4.2.2.1.m1.1.1.5.3.4.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3.4">ğ‘’</ci><ci id="S4.T4.2.2.1.m1.1.1.5.3.5.cmml" xref="S4.T4.2.2.1.m1.1.1.5.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.1.m1.1c">Base_{deep}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.1.m1.1d">italic_B italic_a italic_s italic_e start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2">57.1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.3.1"><math alttext="Big" class="ltx_Math" display="inline" id="S4.T4.3.3.1.m1.1"><semantics id="S4.T4.3.3.1.m1.1a"><mrow id="S4.T4.3.3.1.m1.1.1" xref="S4.T4.3.3.1.m1.1.1.cmml"><mi id="S4.T4.3.3.1.m1.1.1.2" xref="S4.T4.3.3.1.m1.1.1.2.cmml">B</mi><mo id="S4.T4.3.3.1.m1.1.1.1" xref="S4.T4.3.3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.3.3.1.m1.1.1.3" xref="S4.T4.3.3.1.m1.1.1.3.cmml">i</mi><mo id="S4.T4.3.3.1.m1.1.1.1a" xref="S4.T4.3.3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.3.3.1.m1.1.1.4" xref="S4.T4.3.3.1.m1.1.1.4.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.1.m1.1b"><apply id="S4.T4.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.1.m1.1.1"><times id="S4.T4.3.3.1.m1.1.1.1.cmml" xref="S4.T4.3.3.1.m1.1.1.1"></times><ci id="S4.T4.3.3.1.m1.1.1.2.cmml" xref="S4.T4.3.3.1.m1.1.1.2">ğµ</ci><ci id="S4.T4.3.3.1.m1.1.1.3.cmml" xref="S4.T4.3.3.1.m1.1.1.3">ğ‘–</ci><ci id="S4.T4.3.3.1.m1.1.1.4.cmml" xref="S4.T4.3.3.1.m1.1.1.4">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.1.m1.1c">Big</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.1.m1.1d">italic_B italic_i italic_g</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.2">57.7</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.4.1"><math alttext="Big_{deep}" class="ltx_Math" display="inline" id="S4.T4.4.4.1.m1.1"><semantics id="S4.T4.4.4.1.m1.1a"><mrow id="S4.T4.4.4.1.m1.1.1" xref="S4.T4.4.4.1.m1.1.1.cmml"><mi id="S4.T4.4.4.1.m1.1.1.2" xref="S4.T4.4.4.1.m1.1.1.2.cmml">B</mi><mo id="S4.T4.4.4.1.m1.1.1.1" xref="S4.T4.4.4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T4.4.4.1.m1.1.1.3" xref="S4.T4.4.4.1.m1.1.1.3.cmml">i</mi><mo id="S4.T4.4.4.1.m1.1.1.1a" xref="S4.T4.4.4.1.m1.1.1.1.cmml">â¢</mo><msub id="S4.T4.4.4.1.m1.1.1.4" xref="S4.T4.4.4.1.m1.1.1.4.cmml"><mi id="S4.T4.4.4.1.m1.1.1.4.2" xref="S4.T4.4.4.1.m1.1.1.4.2.cmml">g</mi><mrow id="S4.T4.4.4.1.m1.1.1.4.3" xref="S4.T4.4.4.1.m1.1.1.4.3.cmml"><mi id="S4.T4.4.4.1.m1.1.1.4.3.2" xref="S4.T4.4.4.1.m1.1.1.4.3.2.cmml">d</mi><mo id="S4.T4.4.4.1.m1.1.1.4.3.1" xref="S4.T4.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T4.4.4.1.m1.1.1.4.3.3" xref="S4.T4.4.4.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S4.T4.4.4.1.m1.1.1.4.3.1a" xref="S4.T4.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T4.4.4.1.m1.1.1.4.3.4" xref="S4.T4.4.4.1.m1.1.1.4.3.4.cmml">e</mi><mo id="S4.T4.4.4.1.m1.1.1.4.3.1b" xref="S4.T4.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T4.4.4.1.m1.1.1.4.3.5" xref="S4.T4.4.4.1.m1.1.1.4.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.m1.1b"><apply id="S4.T4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1"><times id="S4.T4.4.4.1.m1.1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1.1"></times><ci id="S4.T4.4.4.1.m1.1.1.2.cmml" xref="S4.T4.4.4.1.m1.1.1.2">ğµ</ci><ci id="S4.T4.4.4.1.m1.1.1.3.cmml" xref="S4.T4.4.4.1.m1.1.1.3">ğ‘–</ci><apply id="S4.T4.4.4.1.m1.1.1.4.cmml" xref="S4.T4.4.4.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T4.4.4.1.m1.1.1.4.1.cmml" xref="S4.T4.4.4.1.m1.1.1.4">subscript</csymbol><ci id="S4.T4.4.4.1.m1.1.1.4.2.cmml" xref="S4.T4.4.4.1.m1.1.1.4.2">ğ‘”</ci><apply id="S4.T4.4.4.1.m1.1.1.4.3.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3"><times id="S4.T4.4.4.1.m1.1.1.4.3.1.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3.1"></times><ci id="S4.T4.4.4.1.m1.1.1.4.3.2.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3.2">ğ‘‘</ci><ci id="S4.T4.4.4.1.m1.1.1.4.3.3.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3.3">ğ‘’</ci><ci id="S4.T4.4.4.1.m1.1.1.4.3.4.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3.4">ğ‘’</ci><ci id="S4.T4.4.4.1.m1.1.1.4.3.5.cmml" xref="S4.T4.4.4.1.m1.1.1.4.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.m1.1c">Big_{deep}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.1.m1.1d">italic_B italic_i italic_g start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.2">57.7</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.6.1.1">Ensemble+<span class="ltx_text ltx_font_smallcaps" id="S4.T4.4.6.1.1.1">CometKiwi</span>
</th>
<td class="ltx_td ltx_align_center" id="S4.T4.4.6.1.2">58.3</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.7.2.1">Ensemble+error correction</th>
<td class="ltx_td" id="S4.T4.4.7.2.2"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.8.3.1">Â Â Â Â Â Â +<span class="ltx_text ltx_font_smallcaps" id="S4.T4.4.8.3.1.1">CometKiwi</span>
</th>
<td class="ltx_td ltx_align_center" id="S4.T4.4.8.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.4.8.3.2.1">58.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T4.4.9.4.1">ALMA-R 7B</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.4.9.4.2">52.2</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.10.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.4.10.5.1">ALMA-R 13B</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.4.10.5.2">53.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>chrF scores for each of our models, compared with scores for the model ensembles and for the ALMA-R models. The scores are calculated on the WMT21 evaluation set.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>The pipeline</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Basing our system on the most succesful approach in our experiments, our translation pipeline consists of three steps: First, using each of our four models, we generate five translation hypotheses using beam search for all source paragraphs, resulting in a total of 20 candidates.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Furthermore, each paragraph is segmented into sentences, <math alttext="s_{1},\ldots,s_{n}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.3"><semantics id="S4.SS1.p2.1.m1.3a"><mrow id="S4.SS1.p2.1.m1.3.3.2" xref="S4.SS1.p2.1.m1.3.3.3.cmml"><msub id="S4.SS1.p2.1.m1.2.2.1.1" xref="S4.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="S4.SS1.p2.1.m1.2.2.1.1.2" xref="S4.SS1.p2.1.m1.2.2.1.1.2.cmml">s</mi><mn id="S4.SS1.p2.1.m1.2.2.1.1.3" xref="S4.SS1.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p2.1.m1.3.3.2.3" xref="S4.SS1.p2.1.m1.3.3.3.cmml">,</mo><mi id="S4.SS1.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p2.1.m1.1.1.cmml">â€¦</mi><mo id="S4.SS1.p2.1.m1.3.3.2.4" xref="S4.SS1.p2.1.m1.3.3.3.cmml">,</mo><msub id="S4.SS1.p2.1.m1.3.3.2.2" xref="S4.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="S4.SS1.p2.1.m1.3.3.2.2.2" xref="S4.SS1.p2.1.m1.3.3.2.2.2.cmml">s</mi><mi id="S4.SS1.p2.1.m1.3.3.2.2.3" xref="S4.SS1.p2.1.m1.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.3b"><list id="S4.SS1.p2.1.m1.3.3.3.cmml" xref="S4.SS1.p2.1.m1.3.3.2"><apply id="S4.SS1.p2.1.m1.2.2.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1.2">ğ‘ </ci><cn id="S4.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.2.2.1.1.3">1</cn></apply><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">â€¦</ci><apply id="S4.SS1.p2.1.m1.3.3.2.2.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2.2">ğ‘ </ci><ci id="S4.SS1.p2.1.m1.3.3.2.2.3.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2.3">ğ‘›</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.3c">s_{1},\ldots,s_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.3d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. For each sentence, every model produces five hypotheses. These hypotheses are evaluated using <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p2.1.1">CometKiwi-da-22</span>, and the highest-scoring hypothesis is selected for each sentence. The selected hypotheses are concatenated to form a new paragraph.
Finally, a single paragraph is created by combining the best translation of each sentence, leaving us with 25 translation candidates.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Each of these candidates is then corrected with regard to grammar, spelling and style using the ByT5 model described above.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">These two steps, translating the source text and correcting the translations, result in a total of 50 translation candidates. In order to find the best candidate we use <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p4.1.1">CometKiwi-da-22</span> to score all candidates. The highest scoring one is the selected translation of our system.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.4.5.1.1.1">model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.4.5.1.2.1">Selected</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.4.5.1.3.1">Unique</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.1.1"><math alttext="Base" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.m1.1.1.2.cmml">B</mi><mo id="S4.T5.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3.cmml">a</mi><mo id="S4.T5.1.1.1.m1.1.1.1a" xref="S4.T5.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.1.1.1.m1.1.1.4" xref="S4.T5.1.1.1.m1.1.1.4.cmml">s</mi><mo id="S4.T5.1.1.1.m1.1.1.1b" xref="S4.T5.1.1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.1.1.1.m1.1.1.5" xref="S4.T5.1.1.1.m1.1.1.5.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><times id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1.1"></times><ci id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2">ğµ</ci><ci id="S4.T5.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.m1.1.1.3">ğ‘</ci><ci id="S4.T5.1.1.1.m1.1.1.4.cmml" xref="S4.T5.1.1.1.m1.1.1.4">ğ‘ </ci><ci id="S4.T5.1.1.1.m1.1.1.5.cmml" xref="S4.T5.1.1.1.m1.1.1.5">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">Base</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">italic_B italic_a italic_s italic_e</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2">293</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3">158</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.2.1"><math alttext="Base_{deep}" class="ltx_Math" display="inline" id="S4.T5.2.2.1.m1.1"><semantics id="S4.T5.2.2.1.m1.1a"><mrow id="S4.T5.2.2.1.m1.1.1" xref="S4.T5.2.2.1.m1.1.1.cmml"><mi id="S4.T5.2.2.1.m1.1.1.2" xref="S4.T5.2.2.1.m1.1.1.2.cmml">B</mi><mo id="S4.T5.2.2.1.m1.1.1.1" xref="S4.T5.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.2.2.1.m1.1.1.3" xref="S4.T5.2.2.1.m1.1.1.3.cmml">a</mi><mo id="S4.T5.2.2.1.m1.1.1.1a" xref="S4.T5.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.2.2.1.m1.1.1.4" xref="S4.T5.2.2.1.m1.1.1.4.cmml">s</mi><mo id="S4.T5.2.2.1.m1.1.1.1b" xref="S4.T5.2.2.1.m1.1.1.1.cmml">â¢</mo><msub id="S4.T5.2.2.1.m1.1.1.5" xref="S4.T5.2.2.1.m1.1.1.5.cmml"><mi id="S4.T5.2.2.1.m1.1.1.5.2" xref="S4.T5.2.2.1.m1.1.1.5.2.cmml">e</mi><mrow id="S4.T5.2.2.1.m1.1.1.5.3" xref="S4.T5.2.2.1.m1.1.1.5.3.cmml"><mi id="S4.T5.2.2.1.m1.1.1.5.3.2" xref="S4.T5.2.2.1.m1.1.1.5.3.2.cmml">d</mi><mo id="S4.T5.2.2.1.m1.1.1.5.3.1" xref="S4.T5.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T5.2.2.1.m1.1.1.5.3.3" xref="S4.T5.2.2.1.m1.1.1.5.3.3.cmml">e</mi><mo id="S4.T5.2.2.1.m1.1.1.5.3.1a" xref="S4.T5.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T5.2.2.1.m1.1.1.5.3.4" xref="S4.T5.2.2.1.m1.1.1.5.3.4.cmml">e</mi><mo id="S4.T5.2.2.1.m1.1.1.5.3.1b" xref="S4.T5.2.2.1.m1.1.1.5.3.1.cmml">â¢</mo><mi id="S4.T5.2.2.1.m1.1.1.5.3.5" xref="S4.T5.2.2.1.m1.1.1.5.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.1.m1.1b"><apply id="S4.T5.2.2.1.m1.1.1.cmml" xref="S4.T5.2.2.1.m1.1.1"><times id="S4.T5.2.2.1.m1.1.1.1.cmml" xref="S4.T5.2.2.1.m1.1.1.1"></times><ci id="S4.T5.2.2.1.m1.1.1.2.cmml" xref="S4.T5.2.2.1.m1.1.1.2">ğµ</ci><ci id="S4.T5.2.2.1.m1.1.1.3.cmml" xref="S4.T5.2.2.1.m1.1.1.3">ğ‘</ci><ci id="S4.T5.2.2.1.m1.1.1.4.cmml" xref="S4.T5.2.2.1.m1.1.1.4">ğ‘ </ci><apply id="S4.T5.2.2.1.m1.1.1.5.cmml" xref="S4.T5.2.2.1.m1.1.1.5"><csymbol cd="ambiguous" id="S4.T5.2.2.1.m1.1.1.5.1.cmml" xref="S4.T5.2.2.1.m1.1.1.5">subscript</csymbol><ci id="S4.T5.2.2.1.m1.1.1.5.2.cmml" xref="S4.T5.2.2.1.m1.1.1.5.2">ğ‘’</ci><apply id="S4.T5.2.2.1.m1.1.1.5.3.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3"><times id="S4.T5.2.2.1.m1.1.1.5.3.1.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3.1"></times><ci id="S4.T5.2.2.1.m1.1.1.5.3.2.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3.2">ğ‘‘</ci><ci id="S4.T5.2.2.1.m1.1.1.5.3.3.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3.3">ğ‘’</ci><ci id="S4.T5.2.2.1.m1.1.1.5.3.4.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3.4">ğ‘’</ci><ci id="S4.T5.2.2.1.m1.1.1.5.3.5.cmml" xref="S4.T5.2.2.1.m1.1.1.5.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.1.m1.1c">Base_{deep}</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.1.m1.1d">italic_B italic_a italic_s italic_e start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2">347</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.3">186</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.3.3.1"><math alttext="Big" class="ltx_Math" display="inline" id="S4.T5.3.3.1.m1.1"><semantics id="S4.T5.3.3.1.m1.1a"><mrow id="S4.T5.3.3.1.m1.1.1" xref="S4.T5.3.3.1.m1.1.1.cmml"><mi id="S4.T5.3.3.1.m1.1.1.2" xref="S4.T5.3.3.1.m1.1.1.2.cmml">B</mi><mo id="S4.T5.3.3.1.m1.1.1.1" xref="S4.T5.3.3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.3.3.1.m1.1.1.3" xref="S4.T5.3.3.1.m1.1.1.3.cmml">i</mi><mo id="S4.T5.3.3.1.m1.1.1.1a" xref="S4.T5.3.3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.3.3.1.m1.1.1.4" xref="S4.T5.3.3.1.m1.1.1.4.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.1.m1.1b"><apply id="S4.T5.3.3.1.m1.1.1.cmml" xref="S4.T5.3.3.1.m1.1.1"><times id="S4.T5.3.3.1.m1.1.1.1.cmml" xref="S4.T5.3.3.1.m1.1.1.1"></times><ci id="S4.T5.3.3.1.m1.1.1.2.cmml" xref="S4.T5.3.3.1.m1.1.1.2">ğµ</ci><ci id="S4.T5.3.3.1.m1.1.1.3.cmml" xref="S4.T5.3.3.1.m1.1.1.3">ğ‘–</ci><ci id="S4.T5.3.3.1.m1.1.1.4.cmml" xref="S4.T5.3.3.1.m1.1.1.4">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.1.m1.1c">Big</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.1.m1.1d">italic_B italic_i italic_g</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.2">287</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3">163</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T5.4.4.1"><math alttext="Big_{deep}" class="ltx_Math" display="inline" id="S4.T5.4.4.1.m1.1"><semantics id="S4.T5.4.4.1.m1.1a"><mrow id="S4.T5.4.4.1.m1.1.1" xref="S4.T5.4.4.1.m1.1.1.cmml"><mi id="S4.T5.4.4.1.m1.1.1.2" xref="S4.T5.4.4.1.m1.1.1.2.cmml">B</mi><mo id="S4.T5.4.4.1.m1.1.1.1" xref="S4.T5.4.4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T5.4.4.1.m1.1.1.3" xref="S4.T5.4.4.1.m1.1.1.3.cmml">i</mi><mo id="S4.T5.4.4.1.m1.1.1.1a" xref="S4.T5.4.4.1.m1.1.1.1.cmml">â¢</mo><msub id="S4.T5.4.4.1.m1.1.1.4" xref="S4.T5.4.4.1.m1.1.1.4.cmml"><mi id="S4.T5.4.4.1.m1.1.1.4.2" xref="S4.T5.4.4.1.m1.1.1.4.2.cmml">g</mi><mrow id="S4.T5.4.4.1.m1.1.1.4.3" xref="S4.T5.4.4.1.m1.1.1.4.3.cmml"><mi id="S4.T5.4.4.1.m1.1.1.4.3.2" xref="S4.T5.4.4.1.m1.1.1.4.3.2.cmml">d</mi><mo id="S4.T5.4.4.1.m1.1.1.4.3.1" xref="S4.T5.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T5.4.4.1.m1.1.1.4.3.3" xref="S4.T5.4.4.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S4.T5.4.4.1.m1.1.1.4.3.1a" xref="S4.T5.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T5.4.4.1.m1.1.1.4.3.4" xref="S4.T5.4.4.1.m1.1.1.4.3.4.cmml">e</mi><mo id="S4.T5.4.4.1.m1.1.1.4.3.1b" xref="S4.T5.4.4.1.m1.1.1.4.3.1.cmml">â¢</mo><mi id="S4.T5.4.4.1.m1.1.1.4.3.5" xref="S4.T5.4.4.1.m1.1.1.4.3.5.cmml">p</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.m1.1b"><apply id="S4.T5.4.4.1.m1.1.1.cmml" xref="S4.T5.4.4.1.m1.1.1"><times id="S4.T5.4.4.1.m1.1.1.1.cmml" xref="S4.T5.4.4.1.m1.1.1.1"></times><ci id="S4.T5.4.4.1.m1.1.1.2.cmml" xref="S4.T5.4.4.1.m1.1.1.2">ğµ</ci><ci id="S4.T5.4.4.1.m1.1.1.3.cmml" xref="S4.T5.4.4.1.m1.1.1.3">ğ‘–</ci><apply id="S4.T5.4.4.1.m1.1.1.4.cmml" xref="S4.T5.4.4.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T5.4.4.1.m1.1.1.4.1.cmml" xref="S4.T5.4.4.1.m1.1.1.4">subscript</csymbol><ci id="S4.T5.4.4.1.m1.1.1.4.2.cmml" xref="S4.T5.4.4.1.m1.1.1.4.2">ğ‘”</ci><apply id="S4.T5.4.4.1.m1.1.1.4.3.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3"><times id="S4.T5.4.4.1.m1.1.1.4.3.1.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3.1"></times><ci id="S4.T5.4.4.1.m1.1.1.4.3.2.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3.2">ğ‘‘</ci><ci id="S4.T5.4.4.1.m1.1.1.4.3.3.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3.3">ğ‘’</ci><ci id="S4.T5.4.4.1.m1.1.1.4.3.4.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3.4">ğ‘’</ci><ci id="S4.T5.4.4.1.m1.1.1.4.3.5.cmml" xref="S4.T5.4.4.1.m1.1.1.4.3.5">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.m1.1c">Big_{deep}</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.1.m1.1d">italic_B italic_i italic_g start_POSTSUBSCRIPT italic_d italic_e italic_e italic_p end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.4.4.2">419</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.4.4.3">246</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The number of sentences generated by each model selected for the final output when translating the WMT21 test set. </figcaption>
</figure>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.4.1">System Name</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.5.1">Type</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1">AutoRank <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.1.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.1.m1.1a"><mo id="S4.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T6.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T6.2.2.2.1">MetricX <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.2.2.2.1.m1.1"><semantics id="S4.T6.2.2.2.1.m1.1a"><mo id="S4.T6.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T6.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.m1.1b"><ci id="S4.T6.2.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.2.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.3.1">CometKiwi <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.3.3.3.1.m1.1"><semantics id="S4.T6.3.3.3.1.m1.1a"><mo id="S4.T6.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T6.3.3.3.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.1.m1.1b"><ci id="S4.T6.3.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.3.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.3.1.m1.1d">â†‘</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.4.1.1">Unbabel-Tower70B</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.4.1.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.4.1.3">1.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.4.1.4">2.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.4.1.5">0.740</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.5.2.1">Claude-3.5</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.5.2.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.5.2.3">2.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.5.2.4">3.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.5.2.5">0.697</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.6.3.1">Dubformer</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.6.3.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.6.3.3">2.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.6.3.4">3.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.6.3.5">0.685</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.7.4.1">IKUN</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.7.4.2">Open</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.7.4.3">3.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.7.4.4">4.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.7.4.5">0.666</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.8.5.1">GPT-4</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.8.5.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.8.5.3">3.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.8.5.4">4.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.8.5.5">0.673</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.9.6.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.6.1.1">AMI</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.9.6.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.6.2.1">Open</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.9.6.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.6.3.1">3.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.9.6.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.6.4.1">4.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.9.6.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.6.5.1">0.663</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.10.7.1">IKUN-C</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.10.7.2">Constrained</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.10.7.3">3.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.10.7.4">4.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.10.7.5">0.657</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.11.8.1">TranssionMT</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.11.8.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.11.8.3">4.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.11.8.4">5.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.11.8.5">0.653</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.12.9.1">ONLINE-B</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.12.9.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.12.9.3">4.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.12.9.4">5.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.12.9.5">0.652</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.13.10.1">IOL-Research</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.13.10.2">Open</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.13.10.3">4.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.13.10.4">5.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.13.10.5">0.655</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.14.11.1">ONLINE-A</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.14.11.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.14.11.3">5.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.14.11.4">6.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.14.11.5">0.603</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.15.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.15.12.1">Llama3-70B</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.15.12.2">Open</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.15.12.3">6.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.15.12.4">8.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.15.12.5">0.586</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.16.13.1">ONLINE-G</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.16.13.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.16.13.3">6.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.16.13.4">7.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.16.13.5">0.573</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.17.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.17.14.1">CommandR-plus</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.17.14.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.17.14.3">9.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.17.14.4">10.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.17.14.5">0.487</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.18.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.18.15.1">Mistral-Large</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.18.15.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.18.15.3">10.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.18.15.4">10.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.18.15.5">0.465</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.19.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.19.16.1">Aya23</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.19.16.2">Open</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.19.16.3">15.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.19.16.4">14.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.19.16.5">0.311</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.20.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.20.17.1">Phi-3-Medium</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.20.17.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.20.17.3">16.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.20.17.4">15.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.20.17.5">0.278</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.21.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.21.18.1">ONLINE-W</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.21.18.2">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.21.18.3">18.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.21.18.4">19.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.21.18.5">0.296</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.22.19.1">TSU-HITs</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.22.19.2">Constrained</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.22.19.3">19.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.22.19.4">18.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.22.19.5">0.192</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.23.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.3.23.20.1">CycleL</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.23.20.2">Constrained</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.23.20.3">21.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.23.20.4">20.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.23.20.5">0.148</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Preliminary WMT24 General MT automatic ranking for English-Icelandic. Our system is in bold.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluate our system on the test data from WMT21. As expected, the bigger models perform better, but the best results are achieved by selecting translations from an ensemble of differently trained Transformer models. We use <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">CometKiwi-da-22</span> to select the best translation out of 20 hypotheses made by the four models, five hypotheses by each using beam search with beam size 12. This raises the chrF score to 58.3 and when we add error correction on top, the score is slightly higher, 58.4, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4.T4" title="Table 4 â€£ 4 System Description â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In the WMT24 general translation task, systems were evaluated using two automatic metrics, MetricX-23-XL <cite class="ltx_cite ltx_citemacro_cite">Juraska etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib20" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">CometKiwi-DA-XL</span> <cite class="ltx_cite ltx_citemacro_cite">Rei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib28" title="">2023</a>)</cite>, as well as by human evaluation. According to the automatic metrics, reported in <cite class="ltx_cite ltx_citemacro_citet">Kocmi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib22" title="">2024</a>)</cite>, our model is competitive among the open systems, although four closed systems achieve better scores. Results for the automatic metrics are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S4.T6" title="Table 6 â€£ 4.1 The pipeline â€£ 4 System Description â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We show that while Large Language Models have become nearly ubiquitous in Natural Language Processing, traditional encoder-decoder Transformer models remain a viable approach to machine translation, particularly when computational efficiency is a priority.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Nevertheless, our findings also reveal that integrating LLMs can be advantageous during the training process. Specifically, <span class="ltx_text" id="S6.p2.1.1">ALMA-R</span> 13B proved to be an important part of our training pipeline, as the synthetic data it generated increased the quality of our translation systems.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Furthermore, our results indicate that while more training data usually result in a better translation system, low-quality data, such as the backtranslations generated with an SMT system, can have a detrimental impact on performance. Similarly, our experiments with a bilingual lexicon using token-pair training negatively affected the systemâ€™s output. This may be due to a variety of reasons. Our SMT system could probably be improved as well as our approach to include data from a bilingual lexicon in the training data. This warrants further investigation.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Our filtering method, as described in Sections <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS2" title="3.2 Filtering the OPUS Datasets â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS3" title="3.3 Synthetic Data â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3.3</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#A2" title="Appendix B Filtering steps â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">B</span></a>, has proven effective, even though it may be argued that it is still somewhat crude and more work into minimizing the loss of useful sentence pairs and more effectively remove detrimental sentence pairs would very likely improve the training data and in turn the translation models. For example, while we use LaBSE, LASER and NMT to evaluate sentence pairs, we apply individual cutoff values for each score. A better approach could entail using a classifier to combine all metrics for an optimal result.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Although currently impractical at production-scale, genetic algorithms, as shown by <cite class="ltx_cite ltx_citemacro_citet">Jon and Bojar (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib14" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Jon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib15" title="">2023</a>)</cite>, show promising results in generating translation candidates. Given larger computational resources, similar approaches might prove useful and await future study.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akhbardeh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, OndÅ™ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, MartaÂ R. Costa-jussa, Cristina EspaÃ±a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, AllahseraÂ Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.1" title="">Findings of the 2021 conference on machine translation (WMT21)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 1â€“88, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe and Schwenk (2019)</span>
<span class="ltx_bibblock">
Mikel Artetxe and Holger Schwenk. 2019.

</span>
<span class="ltx_bibblock">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Transactions of the Association for Computational Linguistics</em>, 7:597â€“610.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aulamo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Mikko Aulamo, Nikolay Bogoychev, Shaoxiong Ji, Graeme Nail, Gema RamÃ­rez-SÃ¡nchez, JÃ¶rg Tiedemann, Jelmer vanÂ der Linde, and Jaume Zaragoza. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.61" title="">HPLT: High performance language technologies</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 517â€“518, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaÃ±Ã³n etÂ al. (2020)</span>
<span class="ltx_bibblock">
Marta BaÃ±Ã³n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel EsplÃ -Gomis, MikelÂ L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio OrtizÂ Rojas, Leopoldo PlaÂ Sempere, Gema RamÃ­rez-SÃ¡nchez, Elsa SarrÃ­as, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.417" title="">ParaCrawl: Web-Scale Acquisition of Parallel Corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4555â€“4567, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaÃ±Ã³n etÂ al. (2022)</span>
<span class="ltx_bibblock">
Marta BaÃ±Ã³n, Miquel EsplÃ -Gomis, MikelÂ L. Forcada, Cristian GarcÃ­a-Romero, Taja Kuzman, Nikola LjubeÅ¡iÄ‡, Rik van Noord, LeopoldoÂ Pla Sempere, Gema RamÃ­rez-SÃ¡nchez, Peter Rupnik, VÃ­t Suchomel, Antonio Toral, Tobias vanÂ der Werff, and Jaume Zaragoza. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.eamt-1.41" title="">MaCoCu: Massive collection and curation of monolingual and bilingual data: focus on under-resourced languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 23rd Annual Conference of the European Association for Machine Translation</em>, pages 303â€“304, Ghent, Belgium. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barkarson and SteingrÃ­msson (2019)</span>
<span class="ltx_bibblock">
StarkaÃ°ur Barkarson and SteinÃ¾Ã³r SteingrÃ­msson. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W19-6115" title="">Compiling and Filtering ParIce: An English-Icelandic Parallel Corpus</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 22nd Nordic Conference on Computational Linguistics</em>, pages 140â€“145, Turku, Finland. LinkÃ¶ping University Electronic Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brandt etÂ al. (2011)</span>
<span class="ltx_bibblock">
MarthaÂ DÃ­s Brandt, Hrafh Loftsson, Hlynur SigurÃ¾Ã³rsson, and FrancisÂ M. Tyers. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2011.eamt-1.30" title="">Apertium-IceNLP: A rule-based Icelandic to English machine translation system</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 15th Annual conference of the European Association for Machine Translation</em>, pages 217â€“224, Leuven, Belgium. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cortes and Vapnik (1995)</span>
<span class="ltx_bibblock">
Corinna Cortes and Vladimir Vapnik. 1995.

</span>
<span class="ltx_bibblock">Support-vector networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Machine learning</em>, 20(3):273â€“297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock">Beyond English-Centric Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">J. Mach. Learn. Res.</em>, 22(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.62" title="">Language-agnostic BERT Sentence Embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878â€“891, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forcada etÂ al. (2011)</span>
<span class="ltx_bibblock">
Mikel Forcada, Mireia GinestÃ­-Rosell, Jacob Nordfalk, Jim Oâ€™Regan, Sergio Ortiz-Rojas, Juan PÃ©rez-Ortiz, Felipe SÃ¡nchez-MartÃ­nez, Gema RamÃ­rez-SÃ¡nchez, and Francis Tyers. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10590-011-9090-0" title="">Apertium: A free/open-source platform for rule-based machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Machine Translation</em>, 25:127â€“144.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imamura and Sumita (2017)</span>
<span class="ltx_bibblock">
Kenji Imamura and Eiichiro Sumita. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W17-5711" title="">Ensemble and reranking: Using multiple models in the NICT-2 neural machine translation system at WAT2017</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</em>, pages 127â€“134, Taipei, Taiwan. Asian Federation of Natural Language Processing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IngÃ³lfsdÃ³ttir etÂ al. (2023)</span>
<span class="ltx_bibblock">
SvanhvÃ­tÂ Lilja IngÃ³lfsdÃ³ttir, Petur Ragnarsson, Haukur JÃ³nsson, Haukur Simonarson, Vilhjalmur Thorsteinsson, and VÃ©steinn SnÃ¦bjarnarson. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.402" title="">Byte-level grammatical error correction using synthetic and curated corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7299â€“7316, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jon and Bojar (2023)</span>
<span class="ltx_bibblock">
Josef Jon and OndÅ™ej Bojar. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.122" title="">Breeding machine translations: Evolutionary approach to survive and thrive in the world of automated evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2191â€“2212, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Josef Jon, Martin Popel, and OndÅ™ej Bojar. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.8" title="">CUNI at WMT23 general translation task: MT and a genetic algorithm</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 119â€“127, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alexander Jones, Isaac Caswell, Orhan Firat, and Ishank Saxena. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.26" title="">GATITOS: Using a new multilingual lexicon for low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 371â€“405, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">JÃ³nsson etÂ al. (2022)</span>
<span class="ltx_bibblock">
HaukurÂ PÃ¡ll JÃ³nsson, HaukurÂ Barri SÃ­monarson, PÃ©turÂ Orri Ragnarsson, SvanhvÃ­tÂ Lilja IngÃ³lfsdÃ³ttir, VilhjÃ¡lmur Ãorsteinsson, and VÃ©steinn SnÃ¦bjarnarson. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://hdl.handle.net/20.500.12537/260" title="">Long context synthetic translation pairs for english and icelandic (22.09)</a>.

</span>
<span class="ltx_bibblock">CLARIN-IS.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">JÃ³nsson etÂ al. (2020)</span>
<span class="ltx_bibblock">
HaukurÂ PÃ¡ll JÃ³nsson, HaukurÂ Barri SÃ­monarson, VÃ©steinn SnÃ¦bjarnarson, SteinÃ¾Ã³r SteingrÃ­msson, and Hrafn Loftsson. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-58323-1_10" title="">Experimenting with Different Machine Translation Models in Medium-Resource Settings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Text, Speech, and Dialogue - 23rd International Conference, TSD 2020, Brno, Czech Republic, September 8-11, 2020, Proceedings</em>, volume 12284 of <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">Lecture Notes in Computer Science</em>, pages 95â€“103. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin etÂ al. (2016)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:1607.01759</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juraska etÂ al. (2023)</span>
<span class="ltx_bibblock">
Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.63" title="">MetricX-23: The Google submission to the WMT 2023 metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 756â€“767, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah and Koehn (2018)</span>
<span class="ltx_bibblock">
Huda Khayrallah and Philipp Koehn. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-2709" title="">On the Impact of Various Types of Noise on Neural Machine Translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</em>, pages 74â€“83, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, Mariya Shmatova, SteinÃ¾Ã³r SteingrÃ­msson, and VilÃ©m Zouhar. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.19884" title="">Preliminary WMT24 Ranking of General MT Systems and LLMs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ArXiv</em>, abs/2407.19884.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakatani (2010)</span>
<span class="ltx_bibblock">
Shuyo Nakatani. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/shuyo/language-detection" title="">Language detection library for java</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olteanu etÂ al. (2006)</span>
<span class="ltx_bibblock">
Marian Olteanu, Pasin Suriyentrakorn, and Dan Moldovan. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W06-3122" title="">Language models and reranking for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings on the Workshop on Statistical Machine Translation</em>, pages 150â€“153, New York City. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PopoviÄ‡ (2015)</span>
<span class="ltx_bibblock">
Maja PopoviÄ‡. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W15-3049" title="">chrF: character n-gram F-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392â€“395, Lisbon, Portugal. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Better language models and their implications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">OpenAI blog</em>, 1(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RamÃ­rez-SÃ¡nchez etÂ al. (2020)</span>
<span class="ltx_bibblock">
Gema RamÃ­rez-SÃ¡nchez, Jaume Zaragoza-Bernabeu, Marta BaÃ±Ã³n, and SergioÂ Ortiz Rojas. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.31" title="">Bifixer and Bicleaner: two open-source tools to clean your parallel data.</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 291â€“298, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ricardo Rei, NunoÂ M. Guerreiro, JosÃ© Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, JosÃ©Â G. C.Â de Souza, and AndrÃ© Martins. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.73" title="">Scaling up CometKiwi: Unbabel-IST 2023 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 841â€“848, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ricardo Rei, Marcos Treviso, NunoÂ M. Guerreiro, Chrysoula Zerva, AnaÂ C Farinha, Christine Maroti, JosÃ©Â G. C.Â de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and AndrÃ© F.Â T. Martins. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.60" title="">CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 634â€“645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SÃ¡nchez-Cartagena etÂ al. (2018)</span>
<span class="ltx_bibblock">
VÃ­ctorÂ M. SÃ¡nchez-Cartagena, Marta BaÃ±Ã³n, Sergio Ortiz-Rojas, and Gema RamÃ­rez. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6488" title="">Prompsitâ€™s submission to WMT 2018 Parallel Corpus Filtering shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 955â€“962, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk etÂ al. (2021)</span>
<span class="ltx_bibblock">
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.507" title="">CCMatrix: Mining billions of high-quality parallel sentences on the web</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6490â€“6500, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahl (2024)</span>
<span class="ltx_bibblock">
PeterÂ M. Stahl. 2024.

</span>
<span class="ltx_bibblock">Lingua - an accurate natural language detection library for short and mixed-language text.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pemistahl/lingua-py" title="">https://github.com/pemistahl/lingua-py</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-08-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SteingrÃ­msson etÂ al. (2018)</span>
<span class="ltx_bibblock">
SteinÃ¾Ã³r SteingrÃ­msson, SigrÃºn HelgadÃ³ttir, EirÃ­kur RÃ¶gnvaldsson, StarkaÃ°ur Barkarson, and JÃ³n GuÃ°nason. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L18-1690" title="">RisamÃ¡lheild: A Very Large Icelandic Text Corpus</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</em>, LREC 2018, pages 4361â€“4366, Miyazaki, Japan.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SteingrÃ­msson etÂ al. (2023)</span>
<span class="ltx_bibblock">
SteinÃ¾Ã³r SteingrÃ­msson, Hrafn Loftsson, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.nodalida-1.58" title="">Filtering matters: Experiments in filtering training sets for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)</em>, pages 588â€“600, TÃ³rshavn, Faroe Islands. University of Tartu Library.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SteingrÃ­msson etÂ al. (2021)</span>
<span class="ltx_bibblock">
SteinÃ¾Ã³r SteingrÃ­msson, Pintu Lohar, Hrafn Loftsson, and Andy Way. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.bucc-1.3" title="">Effective Bitext Extraction From Comparable Corpora Using a Combination of Three Different Approaches</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)</em>, pages 8â€“17, Online (Virtual Mode). INCOMA Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SteingrÃ­msson (2023)</span>
<span class="ltx_bibblock">
SteinÃ¾Ã³r SteingrÃ­msson. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Effectively compiling parallel corpora for machine translation in resource-scarce conditions</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Reykjavik University.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SteingrÃ­msson etÂ al. (2023)</span>
<span class="ltx_bibblock">
SteinÃ¾Ã³r SteingrÃ­msson, Hrafn Loftsson, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-demo.22" title="">SentAlign: Accurate and scalable sentence alignment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 256â€“263, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020)</span>
<span class="ltx_bibblock">
JÃ¶rg Tiedemann and Santhosh Thottingal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.61" title="">OPUS-MT â€“ building open translation services for the world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 479â€“480, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral etÂ al. (2023)</span>
<span class="ltx_bibblock">
Antonio Toral, Andreas Cranenburgh, and Tia Nutters. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.4324/9781003357391-3" title="">Literary-adapted machine translation in a well-resourced language pair</a>.

</span>
<span class="ltx_bibblock">In Andrew Rothwell, Andy Way, and Roy Youdale, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Computer-Assisted Literary Translation</em>, pages 27â€“52. Routledge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vamvas and Sennrich (2022)</span>
<span class="ltx_bibblock">
Jannis Vamvas and Rico Sennrich. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.findings-emnlp.15" title="">NMTScore: A multilingual analysis of translation-based text similarity measures</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 198â€“213, Abu Dhabi, United Arab Emirates.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" title="">Attention is All you Need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information Processing Systems 30 (NIPS 2017)</em>, pages 5999â€“6009, Long Beach, California.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2007)</span>
<span class="ltx_bibblock">
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ASRU.2007.4430102" title="">Reranking machine translation hypotheses with structured and web-based language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2007 IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)</em>, pages 159â€“164.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wormer (2024)</span>
<span class="ltx_bibblock">
Titus Wormer. 2024.

</span>
<span class="ltx_bibblock">Franc - a natural language detection library.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/wooorm/franc" title="">https://github.com/wooorm/franc</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-08-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Haoran Xu, YoungÂ Jin Kim, Amr Sharaf, and HanyÂ Hassan Awadalla. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=farT6XXntP" title="">A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, BenjaminÂ Van Durme, Kenton Murray, and YoungÂ Jin Kim. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:267028540" title="">Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv</em>, abs/2401.08417.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2022)</span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00461" title="">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Transactions of the Association for Computational Linguistics</em>, 10:291â€“306.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>OPUS Texts</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The parallel texts we sourced from the OPUS catalog are listed in this section. The format of the list is as follows:

<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="A1.p1.1.1">Index</span>. <span class="ltx_text ltx_font_bold" id="A1.p1.1.2">Name</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.3">version</span>; sentence pairs

<br class="ltx_break"/>For brevity, the <span class="ltx_text ltx_font_italic" id="A1.p1.1.4">ELRC</span> parallel text names are abbreviated after the first entry in the list, with the <span class="ltx_text ltx_font_italic" id="A1.p1.1.5">ditto</span> symbol (â€˜"â€™) replacing the â€˜ELRCâ€™ part of the name.

<br class="ltx_break"/>1. <span class="ltx_text ltx_font_bold" id="A1.p1.1.6">CCAligned</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.7">v1</span>;  1,192,542 
<br class="ltx_break"/>2. <span class="ltx_text ltx_font_bold" id="A1.p1.1.8">CCMatrix</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.9">v1</span>;  8,723,145 
<br class="ltx_break"/>3. <span class="ltx_text ltx_font_bold" id="A1.p1.1.10">ECDC</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.11">v2016-03-16</span>;  2,512 
<br class="ltx_break"/>4. <span class="ltx_text ltx_font_bold" id="A1.p1.1.12">ELRC-2718-EMEA</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.13">v1</span>;  542,624 
<br class="ltx_break"/>5. <span class="ltx_text ltx_font_bold" id="A1.p1.1.14">"-3206-antibiotic</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.15">v1</span>;  816 
<br class="ltx_break"/>6. <span class="ltx_text ltx_font_bold" id="A1.p1.1.16">"-4295-www.malfong.is</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.17">v1</span>;  12,634 
<br class="ltx_break"/>7. <span class="ltx_text ltx_font_bold" id="A1.p1.1.18">"-4324-Government_Offices_I</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.19">v1</span>;  18,185 
<br class="ltx_break"/>8. <span class="ltx_text ltx_font_bold" id="A1.p1.1.20">"-4327-Government_Offices_I</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.21">v1</span>;  36,290 
<br class="ltx_break"/>9. <span class="ltx_text ltx_font_bold" id="A1.p1.1.22">"-4334-Rkiskaup_2020</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.23">v1</span>;  10,236 
<br class="ltx_break"/>10. <span class="ltx_text ltx_font_bold" id="A1.p1.1.24">"-4338-University_Iceland</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.25">v1</span>;  10,164 
<br class="ltx_break"/>11. <span class="ltx_text ltx_font_bold" id="A1.p1.1.26">"-502-Icelandic_Financial_</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.27">v1</span>;  1,525 
<br class="ltx_break"/>12. <span class="ltx_text ltx_font_bold" id="A1.p1.1.28">"-504-www.iceida.is</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.29">v1</span>;  1,055 
<br class="ltx_break"/>13. <span class="ltx_text ltx_font_bold" id="A1.p1.1.30">"-505-www.pfs.is</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.31">v1</span>;  2,866 
<br class="ltx_break"/>14. <span class="ltx_text ltx_font_bold" id="A1.p1.1.32">"-506-www.lanamal.is</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.33">v1</span>;  1,140 
<br class="ltx_break"/>15. <span class="ltx_text ltx_font_bold" id="A1.p1.1.34">"-5067-SciPar</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.35">v1</span>;  110,831 
<br class="ltx_break"/>16. <span class="ltx_text ltx_font_bold" id="A1.p1.1.36">"-508-Tilde_Statistics_Ice</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.37">v1</span>;  2,427 
<br class="ltx_break"/>17. <span class="ltx_text ltx_font_bold" id="A1.p1.1.38">"-509-Gallery_Iceland</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.39">v1</span>;  577 
<br class="ltx_break"/>18. <span class="ltx_text ltx_font_bold" id="A1.p1.1.40">"-510-Harpa_Reykjavik_Conc</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.41">v1</span>;  1,197 
<br class="ltx_break"/>19. <span class="ltx_text ltx_font_bold" id="A1.p1.1.42">"-511-bokmenntaborgin_is</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.43">v1</span>;  330 
<br class="ltx_break"/>20. <span class="ltx_text ltx_font_bold" id="A1.p1.1.44">"-516-Icelandic_Medicines</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.45">v1</span>;  711 
<br class="ltx_break"/>21. <span class="ltx_text ltx_font_bold" id="A1.p1.1.46">"-517-Icelandic_Directorat</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.47">v1</span>;  1,536 
<br class="ltx_break"/>22. <span class="ltx_text ltx_font_bold" id="A1.p1.1.48">"-597-www.nordisketax.net</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.49">v1</span>;  1,065 
<br class="ltx_break"/>23. <span class="ltx_text ltx_font_bold" id="A1.p1.1.50">"-718-Statistics_Iceland</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.51">v1</span>;  2,361 
<br class="ltx_break"/>24. <span class="ltx_text ltx_font_bold" id="A1.p1.1.52">"-728-www.norden.org</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.53">v1</span>;  41,073 
<br class="ltx_break"/>25. <span class="ltx_text ltx_font_bold" id="A1.p1.1.54">"-EMEA</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.55">v1</span>;  542,624 
<br class="ltx_break"/>26. <span class="ltx_text ltx_font_bold" id="A1.p1.1.56">"-antibiotic</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.57">v1</span>;  816 
<br class="ltx_break"/>27. <span class="ltx_text ltx_font_bold" id="A1.p1.1.58">"-www.norden.org</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.59">v1</span>;  41,073 
<br class="ltx_break"/>28. <span class="ltx_text ltx_font_bold" id="A1.p1.1.60">"-www.nordisketax.net</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.61">v1</span>;  1,065 
<br class="ltx_break"/>29. <span class="ltx_text ltx_font_bold" id="A1.p1.1.62">EUbookshop</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.63">v2</span>;  9,783 
<br class="ltx_break"/>30. <span class="ltx_text ltx_font_bold" id="A1.p1.1.64">GNOME</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.65">v1</span>;  28,776 
<br class="ltx_break"/>31. <span class="ltx_text ltx_font_bold" id="A1.p1.1.66">HPLT</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.67">v1</span>;  2,148,876 
<br class="ltx_break"/>32. <span class="ltx_text ltx_font_bold" id="A1.p1.1.68">KDE4</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.69">v2</span>;  98,989 
<br class="ltx_break"/>33. <span class="ltx_text ltx_font_bold" id="A1.p1.1.70">MaCoCu</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.71">v2</span>;  267,366 
<br class="ltx_break"/>34. <span class="ltx_text ltx_font_bold" id="A1.p1.1.72">MultiCCAligned</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.73">v1</span>;  1,192,537 
<br class="ltx_break"/>35. <span class="ltx_text ltx_font_bold" id="A1.p1.1.74">MultiHPLT</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.75">v1</span>;  2,148,855 
<br class="ltx_break"/>36. <span class="ltx_text ltx_font_bold" id="A1.p1.1.76">MultiMaCoCu</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.77">v2</span>;  267,366 
<br class="ltx_break"/>37. <span class="ltx_text ltx_font_bold" id="A1.p1.1.78">MultiParaCrawl</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.79">v7.1</span>;  2,392,423 
<br class="ltx_break"/>38. <span class="ltx_text ltx_font_bold" id="A1.p1.1.80">NLLB</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.81">v1</span>;  8,723,145 
<br class="ltx_break"/>39. <span class="ltx_text ltx_font_bold" id="A1.p1.1.82">OpenSubtitles</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.83">v1</span>;  7,138 
<br class="ltx_break"/>40. <span class="ltx_text ltx_font_bold" id="A1.p1.1.84">OpenSubtitles</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.85">v2016</span>;  1,359,224 
<br class="ltx_break"/>41. <span class="ltx_text ltx_font_bold" id="A1.p1.1.86">OpenSubtitles</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.87">v2018</span>;  1,569,189 
<br class="ltx_break"/>42. <span class="ltx_text ltx_font_bold" id="A1.p1.1.88">ParIce</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.89">v1</span>;  2,097,022 
<br class="ltx_break"/>43. <span class="ltx_text ltx_font_bold" id="A1.p1.1.90">ParaCrawl</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.91">v7.1</span>;  2,392,422 
<br class="ltx_break"/>44. <span class="ltx_text ltx_font_bold" id="A1.p1.1.92">ParaCrawl</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.93">v8</span>;  5,724,373 
<br class="ltx_break"/>45. <span class="ltx_text ltx_font_bold" id="A1.p1.1.94">ParaCrawl</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.95">v9</span>;  2,967,579 
<br class="ltx_break"/>46. <span class="ltx_text ltx_font_bold" id="A1.p1.1.96">QED</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.97">v2.0a</span>;  27,611 
<br class="ltx_break"/>47. <span class="ltx_text ltx_font_bold" id="A1.p1.1.98">TED2020</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.99">v1</span>;  2,430 
<br class="ltx_break"/>48. <span class="ltx_text ltx_font_bold" id="A1.p1.1.100">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.101">v2</span>;  8,139 
<br class="ltx_break"/>49. <span class="ltx_text ltx_font_bold" id="A1.p1.1.102">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.103">v20190709</span>;  9,436 
<br class="ltx_break"/>50. <span class="ltx_text ltx_font_bold" id="A1.p1.1.104">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.105">v2020-05-31</span>;  9,438 
<br class="ltx_break"/>51. <span class="ltx_text ltx_font_bold" id="A1.p1.1.106">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.107">v2020-11-09</span>;  9,440 
<br class="ltx_break"/>52. <span class="ltx_text ltx_font_bold" id="A1.p1.1.108">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.109">v2021-03-10</span>;  9,443 
<br class="ltx_break"/>53. <span class="ltx_text ltx_font_bold" id="A1.p1.1.110">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.111">v2021-07-22</span>;  9,443 
<br class="ltx_break"/>54. <span class="ltx_text ltx_font_bold" id="A1.p1.1.112">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.113">v2022-03-03</span>;  9,522 
<br class="ltx_break"/>55. <span class="ltx_text ltx_font_bold" id="A1.p1.1.114">Tatoeba</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.115">v2023-04-12</span>;  9,600 
<br class="ltx_break"/>56. <span class="ltx_text ltx_font_bold" id="A1.p1.1.116">TildeMODEL</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.117">v2018</span>;  420,712 
<br class="ltx_break"/>57. <span class="ltx_text ltx_font_bold" id="A1.p1.1.118">Ubuntu</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.119">v14.10</span>;  2,155 
<br class="ltx_break"/>58. <span class="ltx_text ltx_font_bold" id="A1.p1.1.120">WikiMatrix</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.121">v1</span>;  85,992 
<br class="ltx_break"/>59. <span class="ltx_text ltx_font_bold" id="A1.p1.1.122">WikiTitles</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.123">v3</span>;  50,176 
<br class="ltx_break"/>60. <span class="ltx_text ltx_font_bold" id="A1.p1.1.124">XLEnt</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.125">v1</span>;  962,661 
<br class="ltx_break"/>61. <span class="ltx_text ltx_font_bold" id="A1.p1.1.126">XLEnt</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.127">v1.1</span>;  962,661 
<br class="ltx_break"/>62. <span class="ltx_text ltx_font_bold" id="A1.p1.1.128">XLEnt</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.129">v1.2</span>;  962,661 
<br class="ltx_break"/>63. <span class="ltx_text ltx_font_bold" id="A1.p1.1.130">bible-uedin</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.131">v1</span>;  62,163 
<br class="ltx_break"/>64. <span class="ltx_text ltx_font_bold" id="A1.p1.1.132">wikimedia</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.133">v20190628</span>;  581 
<br class="ltx_break"/>65. <span class="ltx_text ltx_font_bold" id="A1.p1.1.134">wikimedia</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.135">v20210402</span>;  2,625 
<br class="ltx_break"/>66. <span class="ltx_text ltx_font_bold" id="A1.p1.1.136">wikimedia</span>; <span class="ltx_text ltx_font_italic" id="A1.p1.1.137">v20230407</span>;  4,471 
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Filtering steps</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">Filter 1. Sentence length
<br class="ltx_break"/></span>Sentences should contain at minimum four characters and at maximum 150 characters.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1"><span class="ltx_text ltx_font_bold" id="A2.p2.1.1">Filter 2. High inter-pair content overlap
<br class="ltx_break"/></span>Sentence pairs where the content of the source and target sentences are highly similar should be removed from the dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p3">
<p class="ltx_p" id="A2.p3.1"><span class="ltx_text ltx_font_bold" id="A2.p3.1.1">Filter 3. Character symbol filtering
<br class="ltx_break"/></span>All characters in the English and Icelandic alphabets (along with punctuation and numbers) designated as a set of allowed characters. Sentences containing less than 60% of these characters removed from the data and all characters outside the allowed set removed from the remaining sentences.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This is the last filtering step that inherently modifies the content inside individual sentences.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p4">
<p class="ltx_p" id="A2.p4.1"><span class="ltx_text ltx_font_bold" id="A2.p4.1.1">Filter 4. LaBSE scoring
<br class="ltx_break"/></span>We use score each sentence pair using LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib10" title="">2022</a>)</cite> and remove all sentences with a score lower than 0.8<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>This is a higher cutoff than the original LaBSE authors suggest to use, but our experiments suggets it better suits our data.</span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p5">
<p class="ltx_p" id="A2.p5.1"><span class="ltx_text ltx_font_bold" id="A2.p5.1.1">Filter 5. Language detection
<br class="ltx_break"/></span>We use various language detection software to gauge whether both the source and target sentences are in the correct language. The software we used was <span class="ltx_text ltx_font_italic" id="A2.p5.1.2">fasttext</span> <cite class="ltx_cite ltx_citemacro_cite">Joulin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib19" title="">2016</a>)</cite>, <span class="ltx_text ltx_font_italic" id="A2.p5.1.3">franc</span> <cite class="ltx_cite ltx_citemacro_cite">Wormer (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib43" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_italic" id="A2.p5.1.4">lingua</span> <cite class="ltx_cite ltx_citemacro_cite">Stahl (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib32" title="">2024</a>)</cite> and <span class="ltx_text ltx_font_italic" id="A2.p5.1.5">langdetect</span> <cite class="ltx_cite ltx_citemacro_cite">Nakatani (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib23" title="">2010</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p6">
<p class="ltx_p" id="A2.p6.1"><span class="ltx_text ltx_font_bold" id="A2.p6.1.1">Filter 6. Similar dataset pairs
<br class="ltx_break"/></span>As a safeguard, we remove any duplicate entries of our dataset if, for any reason, there remain duplicate instances after the previous filters. In our final experiment, this was rendered redundant, but was required in previous iterations and may prove useful in future iterations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p7">
<p class="ltx_p" id="A2.p7.1"><span class="ltx_text ltx_font_bold" id="A2.p7.1.1">Filter 7. Near-duplicate dataset pairs
<br class="ltx_break"/></span>Sentences are compared by removing content-specific words that are likely proper names and dates, etc., and comparing the remainder.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p8">
<p class="ltx_p" id="A2.p8.1"><span class="ltx_text ltx_font_bold" id="A2.p8.1.1">Filter 8. Likely machine-translated target sentences
<br class="ltx_break"/></span>A GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib26" title="">2019</a>)</cite> classifier is used to evaluate whether a given target sentence is machine-translated, based on a 10.000 sentence hand-evaluated reference set. If this is true for the target sentence, that pair is removed from the dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p9">
<p class="ltx_p" id="A2.p9.1"><span class="ltx_text ltx_font_bold" id="A2.p9.1.1">Filter 9. Existing datasets
<br class="ltx_break"/></span>As a final safeguard check, we remove any sentence pair that we already have on file in other datasets, as touched on in section <a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#S3.SS2" title="3.2 Filtering the OPUS Datasets â€£ 3 Data Selection and Filtering â€£ Cogs in a Machine, Doing What Theyâ€™re Meant to Do â€“ The AMI Submission to the WMT24 General Translation Task"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p10">
<p class="ltx_p" id="A2.p10.1"><span class="ltx_text ltx_font_bold" id="A2.p10.1.1">Filter 10. NMTScore cross-likelyhood 0.4
<br class="ltx_break"/></span>Finally, we use a translation cross-likelyhood NMTScore <cite class="ltx_cite ltx_citemacro_cite">Vamvas and Sennrich (<a class="ltx_ref" href="https://arxiv.org/html/2410.03381v1#bib.bib40" title="">2022</a>)</cite> to determine the translation quality of a given sentence pair. This step is computationally heavy and was therefore saved for last. Our experiments suggest that 0.4 is a suitable cutoff for our dataset.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 12:43:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
