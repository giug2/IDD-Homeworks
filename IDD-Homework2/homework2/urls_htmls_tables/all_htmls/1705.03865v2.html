<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1705.03865] Survey of Visual Question Answering: Datasets and Techniques</title><meta property="og:description" content="Visual question answering (or VQA) is a new and exciting problem that combines
natural language processing and computer vision techniques. We present
a survey of the various datasets and models that have been used to t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Survey of Visual Question Answering: Datasets and Techniques">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Survey of Visual Question Answering: Datasets and Techniques">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1705.03865">

<!--Generated on Fri Mar 15 22:19:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Survey of Visual Question Answering: Datasets and Techniques</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Akshay Kumar Gupta 
<br class="ltx_break">Indian Institute of Technology Delhi
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">cs5130275@cse.iitd.ac.in</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Visual question answering (or VQA) is a new and exciting problem that combines
natural language processing and computer vision techniques. We present
a survey of the various datasets and models that have been used to tackle
this task. The first part of this survey details the various datasets for VQA
and compares them along some common factors. The second part of this survey details
the different approaches for VQA, classified into four types: non-deep learning
models, deep learning models without attention, deep learning models with
attention, and other models which do not fit into the first three. Finally,
we compare the performances of these approaches and provide some directions for
future work.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering is a task that has emerged in the last few years and has been getting a lot of attention from the machine learning community <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib27" title="" class="ltx_ref">2016a</a>)</cite>. The task typically involves showing an image to a computer and asking a question about that image which the computer must answer. The answer could be in any of the following forms: a word, a phrase, a yes/no answer, choosing out of several possible answers, or a fill in the blank answer.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Visual question answering is an important and appealing task because it combines the fields of computer vision and natural language processing. Computer vision techniques must be used to understand the image and NLP techniques must be used to understand the question. Moreover, both must be combined to effectively answer the question in context of the image. This is challenging because historically both these fields have used distinct methods and models to solve their respective tasks.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This survey describes some prominent datasets and models that have been used to tackle the visual question answering task and provides a comparison on how well these models perform on the various datasets. Section 2 covers VQA datasets, Section 3 describes models and Section 4 discusses the results and provides some possible future directions.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Number of</span></td>
</tr>
<tr id="S1.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Images</span></td>
</tr>
</table>
</th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.3.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Number of</span></td>
</tr>
<tr id="S1.T1.1.1.1.3.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Questions</span></td>
</tr>
</table>
</th>
<th id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Avg. questions</span></td>
</tr>
<tr id="S1.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">per image</span></td>
</tr>
</table>
</th>
<th id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.5.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Average question</span></td>
</tr>
<tr id="S1.T1.1.1.1.5.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">length</span></td>
</tr>
</table>
</th>
<th id="S1.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.6.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Average answer</span></td>
</tr>
<tr id="S1.T1.1.1.1.6.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">length</span></td>
</tr>
</table>
</th>
<th id="S1.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S1.T1.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.7.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">Q/A</span></td>
</tr>
<tr id="S1.T1.1.1.1.7.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">generation</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<th id="S1.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S1.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">DAQUAR</span></th>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1,449</td>
<td id="S1.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">12,468</td>
<td id="S1.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">8.60</td>
<td id="S1.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">11.5</td>
<td id="S1.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">1.2</td>
<td id="S1.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">Human</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<th id="S1.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.3.2.1.1" class="ltx_text ltx_font_bold">Visual7W</span></th>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_center">47,300</td>
<td id="S1.T1.1.3.2.3" class="ltx_td ltx_align_center">327,939</td>
<td id="S1.T1.1.3.2.4" class="ltx_td ltx_align_center">6.93</td>
<td id="S1.T1.1.3.2.5" class="ltx_td ltx_align_center">6.9</td>
<td id="S1.T1.1.3.2.6" class="ltx_td ltx_align_center">2.0</td>
<td id="S1.T1.1.3.2.7" class="ltx_td ltx_align_center">Human</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<th id="S1.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">Visual Madlibs</span></th>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_center">10,738</td>
<td id="S1.T1.1.4.3.3" class="ltx_td ltx_align_center">360,001</td>
<td id="S1.T1.1.4.3.4" class="ltx_td ltx_align_center">33.52</td>
<td id="S1.T1.1.4.3.5" class="ltx_td ltx_align_center">4.9</td>
<td id="S1.T1.1.4.3.6" class="ltx_td ltx_align_center">2.8</td>
<td id="S1.T1.1.4.3.7" class="ltx_td ltx_align_center">Human</td>
</tr>
<tr id="S1.T1.1.5.4" class="ltx_tr">
<th id="S1.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.5.4.1.1" class="ltx_text ltx_font_bold">COCO-QA</span></th>
<td id="S1.T1.1.5.4.2" class="ltx_td ltx_align_center">117,684</td>
<td id="S1.T1.1.5.4.3" class="ltx_td ltx_align_center">117,684</td>
<td id="S1.T1.1.5.4.4" class="ltx_td ltx_align_center">1.00</td>
<td id="S1.T1.1.5.4.5" class="ltx_td ltx_align_center">9.65</td>
<td id="S1.T1.1.5.4.6" class="ltx_td ltx_align_center">1.0</td>
<td id="S1.T1.1.5.4.7" class="ltx_td ltx_align_center">Automatic</td>
</tr>
<tr id="S1.T1.1.6.5" class="ltx_tr">
<th id="S1.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.6.5.1.1" class="ltx_text ltx_font_bold">FM-IQA</span></th>
<td id="S1.T1.1.6.5.2" class="ltx_td ltx_align_center">158,392</td>
<td id="S1.T1.1.6.5.3" class="ltx_td ltx_align_center">316,193</td>
<td id="S1.T1.1.6.5.4" class="ltx_td ltx_align_center">1.99</td>
<td id="S1.T1.1.6.5.5" class="ltx_td ltx_align_center">7.38 (Chinese)</td>
<td id="S1.T1.1.6.5.6" class="ltx_td ltx_align_center">3.82 (Chinese)</td>
<td id="S1.T1.1.6.5.7" class="ltx_td ltx_align_center">Human</td>
</tr>
<tr id="S1.T1.1.7.6" class="ltx_tr">
<th id="S1.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.7.6.1.1" class="ltx_text ltx_font_bold">VQA (COCO)</span></th>
<td id="S1.T1.1.7.6.2" class="ltx_td ltx_align_center">204,721</td>
<td id="S1.T1.1.7.6.3" class="ltx_td ltx_align_center">614,163</td>
<td id="S1.T1.1.7.6.4" class="ltx_td ltx_align_center">3.00</td>
<td id="S1.T1.1.7.6.5" class="ltx_td ltx_align_center">6.2</td>
<td id="S1.T1.1.7.6.6" class="ltx_td ltx_align_center">1.1</td>
<td id="S1.T1.1.7.6.7" class="ltx_td ltx_align_center">Human</td>
</tr>
<tr id="S1.T1.1.8.7" class="ltx_tr">
<th id="S1.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S1.T1.1.8.7.1.1" class="ltx_text ltx_font_bold">VQA (Abstract)</span></th>
<td id="S1.T1.1.8.7.2" class="ltx_td ltx_align_center">50,000</td>
<td id="S1.T1.1.8.7.3" class="ltx_td ltx_align_center">150,000</td>
<td id="S1.T1.1.8.7.4" class="ltx_td ltx_align_center">3.00</td>
<td id="S1.T1.1.8.7.5" class="ltx_td ltx_align_center">6.2</td>
<td id="S1.T1.1.8.7.6" class="ltx_td ltx_align_center">1.1</td>
<td id="S1.T1.1.8.7.7" class="ltx_td ltx_align_center">Human</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>VQA Datasets</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several large-scale datasets have been released in the past 2-3 years for the VQA task. We discuss these datasets below. Table 1 contains a summary of these datasets.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>DAQUAR <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib17" title="" class="ltx_ref">2014</a>)</cite>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The DAtaset for QUestion Answering on Real-world images (or DAQUAR), released in 2015, was the first dataset and benchmark released for the VQA task. It takes images from the NYU-Depth V2 dataset which contains images along with their semantic segmentations. Every pixel of an image is labeled with an object class (or no object) out of 894 possible classes. The images are all of indoor scenes. There are a total of 1449 images (795 training, 654 test). The authors generated question answer pairs in two ways: 1) Automatically, using question templates. The authors define 9 templates for questions, whose answers can be taken from the existing NYU-Depth V2 dataset annotations. An example of a question template is ‘How many [object] are in [image id]?’. 2) Using human annotations. They asked 5 participants to generate questions and answers with the constraint that answers must be either colors, numbers or classes or sets of these. The resultant dataset contains a total of 12468 question-answer pairs (6794 training, 5674 test). A reduced dataset containing only 37 object classes is also available.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The authors propose two evaluation metrics for this dataset: One is simple accuracy, which is not a very good metric for multi-word answers, and the second is WUPS score which gives a score for a generated answer between 0.0 and 1.0 based on average match between answer and ground truth answers. Typically the WUPS score is thresholded at 0.9 (That is, if the WUPS score for an answer is above 0.9 then it is correct.)</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1705.03865/assets/daquar.png" id="S2.F1.g1" class="ltx_graphics ltx_img_square" width="208" height="205" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taken from <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Visual 7W is a dataset generated using images from the MS-COCO dataset <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite> for image captioning, recognition and segmentation. The Visual7W dataset gets it name from generating multiple-choice questions of the form (Who, What, Where, When, Why, How and Which). Workers on Amazon Mechanical Turk (AMT) were used to generate the questions. A separate set of three workers were used to rate the questions and those with less than two positive votes were discarded. Multiple choice answers were generated both automatically and by AMT workers. AMT workers were also asked to draw bounding boxes of objects mentioned in the question in the image, firstly to resolve textual ambiguity (Eg. An image has two red cars. Then ‘red car’ in the question could refer to either of these.), and secondly to enable answers of a visual nature (‘pointing’ at an object). The dataset contains 47,300 images and 327,939 questions.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The Visual Madlibs dataset is a fill-in-the-blanks as well as multiple choice dataset. Images are collected from MS-COCO. Descriptive fill-in-the-blank questions are generated automatically using templates and object information. Each question generated in this way is answered by a group of 3 AMT workers. The answer can be a word or a phrase. Multiple choices for the blanks are also provided as an additional evaluation benchmark. The dataset contains 10,738 images and 360,001 questions. The multiple choice questions are evaluated on the accuracy metric.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>COCO-QA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The COCO-QA dataset is another dataset based on MS-COCO. Both questions and answers are generated automatically using image captions from MS-COCO and broadly belong to four categories: Object, Number, Color and Location. There is one question per image and answers are single-word. The dataset contains a total of 123,287 images. Evaluation is done using either accuracy or WUPS score.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/1705.03865/assets/cocoqa.png" id="S2.F2.g1" class="ltx_graphics ltx_img_square" width="207" height="209" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Taken from <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite></figcaption>
</figure>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>FM-IQA <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">The Freestyle Multilingual Image Question Answering dataset (FM-IQA) takes images from the MS-COCO dataset and uses the Baidu crowdsourcing server to get workers to generate questions and answers. Answers can be words, phrases or full sentences. Question/answer pairs are available in Chinese as well as their English translations. The dataset contains 158,392 images and 316,193 questions. They propose human evaluation through a visual Turing Test, which may be one reason this dataset has not gained much popularity.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>
</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">The Visual Question Answering (VQA) dataset is the most widely used dataset for the VQA task. This dataset was released as part of the visual question answering challenge. It is divided into two parts: one dataset contains real-world images from MS-COCO, and another dataset contains abstract clipart scenes created from models of humans and animals to remove the need to process noisy images and only perform high level reasoning. Questions and answers are generated from crowd-sourced workers and 10 answers are obtained for each question from unique workers. Answers are typically a word or a short phrase. Approximately 40% of the questions have a yes or no answer. For evaluation, both open-ended answer generation as well as multiple choice formats are available. Multiple choice questions have 18 candidate responses. To evaluated open-ended answers, a machine generated answer is normalized by the VQA evaluation system and then evaluated as Score = min(#humans who provided that exact answer / 3, 1). So, an answer is considered completely correct if it matches the responses of at least three human annotators. If it matches none of the 10 candidate responses then it is given a score of 0. The original VQA dataset has 204,721 MS-COCO images with 614,163 questions and 50,000 abstract images with 150,000 questions. The 2017 iteration of the VQA challenge has a bigger dataset with a total of 265,016 MS-COCO and abstract images and an average of 5.4 questions per image. The exact number of questions is not mentioned on the challenge website.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/1705.03865/assets/visualqa.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="287" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Taken from <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Models</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The VQA task was proposed after deep learning approaches had already gained wide popularity due to their state-of-the-art performance on various vision and NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a href="#bib.bib12" title="" class="ltx_ref">2012</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Bahdanau et al. (<a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>. As a result almost all work on VQA in the literature involves deep learning approaches, as opposed to more classical approaches like graphical models. There are a couple of models which use a non-neural approach, which are detailed in the first subsection. In addition, several simple baselines that authors use involve non-neural methods, which are also described. The second sub-section describes deep learning models which do not involve the use of attention-based techniques. The third sub-section details attention-based deep learning models for VQA. Results of all the models described are summarized in Tables 2 and 3.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Non-deep learning approaches</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Answer Type Prediction (ATP)</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> propose a Bayesian framework for VQA in which they predict the answer type for a question and use this to generate the answer. The possible answer types vary across the datasets they consider. For instance, for COCO-QA they consider four answer types: object, color, counting, and location.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.4" class="ltx_p">Their model computes the probability of an answer <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">a</annotation></semantics></math> and answer type <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">t</annotation></semantics></math> given the image <math id="S3.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mi id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><ci id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">x</annotation></semantics></math> and question <math id="S3.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mi id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><ci id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">q</annotation></semantics></math>,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div id="S3.E1.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:227.6pt;height:13.9pt;vertical-align:-3.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.9pt,0.0pt) scale(0.99177470135939,0.99177470135939) ;">
<p id="S3.E1.m1.1.1.1.1" class="ltx_p"><math id="S3.E1.m1.1.1.1.1.m1.8" class="ltx_math_unparsed" alttext="P(A=a,T=t|x,q)=\frac{P(x|A=a,T=t,q)P(A=a|T=t,q)P(T=t|q)}{P(x|q)}" display="inline"><semantics id="S3.E1.m1.1.1.1.1.m1.8a"><mrow id="S3.E1.m1.1.1.1.1.m1.8.8"><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1"><mi id="S3.E1.m1.1.1.1.1.m1.8.8.1.3">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.8.8.1.2">​</mo><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.2">(</mo><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2"><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.1.1.2">A</mi><mo id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.1.1.1">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.1.1.3">a</mi></mrow><mo id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.3">,</mo><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2"><mi id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.2">T</mi><mo id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.1">=</mo><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.3"><mi id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.3.2">t</mi><mo fence="false" id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.3.1">|</mo><mrow id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.3.3.2"><mi id="S3.E1.m1.1.1.1.1.m1.6.6">x</mi><mo id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.1.2.2.3.3.2.1">,</mo><mi id="S3.E1.m1.1.1.1.1.m1.7.7">q</mi></mrow></mrow></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.8.8.1.1.1.3">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.m1.8.8.2">=</mo><mfrac id="S3.E1.m1.1.1.1.1.m1.5.5"><mrow id="S3.E1.m1.1.1.1.1.m1.4.4.4"><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.5">P</mi><mrow id="S3.E1.m1.1.1.1.1.m1.4.4.4.6"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.1">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.2">x</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.3">|</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.4">A</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.5">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.6">a</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.7">,</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.8">T</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.9">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.1.1.1.1">t</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.10">,</mo><mi id="S3.E1.m1.1.1.1.1.m1.2.2.2.2">q</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.6.11">)</mo></mrow><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.7">P</mi><mrow id="S3.E1.m1.1.1.1.1.m1.4.4.4.8"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.1">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.2">A</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.3">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.4">a</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.5">|</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.6">T</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.7">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.3.3.3.3">t</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.8">,</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.4">q</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.8.9">)</mo></mrow><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.9">P</mi><mrow id="S3.E1.m1.1.1.1.1.m1.4.4.4.10"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.1">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.2">T</mi><mo id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.3">=</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.4">t</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.5">|</mo><mi id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.6">q</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.4.4.4.10.7">)</mo></mrow></mrow><mrow id="S3.E1.m1.1.1.1.1.m1.5.5.5"><mi id="S3.E1.m1.1.1.1.1.m1.5.5.5.3">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.5.5.5.2">​</mo><mrow id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.2">(</mo><mrow id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.1"><mi id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.1.2">x</mi><mo fence="false" id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.1.1">|</mo><mi id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.1.3">q</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.5.5.5.1.1.3">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1.1.1.1.m1.8b">P(A=a,T=t|x,q)=\frac{P(x|A=a,T=t,q)P(A=a|T=t,q)P(T=t|q)}{P(x|q)}</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS1.p2.5" class="ltx_p">which follows from Bayes’ rule. They can then marginalize over all answer types to obtain <math id="S3.SS1.SSS1.p2.5.m1.3" class="ltx_Math" alttext="P(A=a|x,q)" display="inline"><semantics id="S3.SS1.SSS1.p2.5.m1.3a"><mrow id="S3.SS1.SSS1.p2.5.m1.3.3" xref="S3.SS1.SSS1.p2.5.m1.3.3.cmml"><mi id="S3.SS1.SSS1.p2.5.m1.3.3.3" xref="S3.SS1.SSS1.p2.5.m1.3.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p2.5.m1.3.3.2" xref="S3.SS1.SSS1.p2.5.m1.3.3.2.cmml">​</mo><mrow id="S3.SS1.SSS1.p2.5.m1.3.3.1.1" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.2" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.2" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.1" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.2" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.1" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.1.cmml">|</mo><mrow id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.2" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.1.cmml"><mi id="S3.SS1.SSS1.p2.5.m1.1.1" xref="S3.SS1.SSS1.p2.5.m1.1.1.cmml">x</mi><mo id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.2.1" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS1.p2.5.m1.2.2" xref="S3.SS1.SSS1.p2.5.m1.2.2.cmml">q</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.3" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m1.3b"><apply id="S3.SS1.SSS1.p2.5.m1.3.3.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3"><times id="S3.SS1.SSS1.p2.5.m1.3.3.2.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.2"></times><ci id="S3.SS1.SSS1.p2.5.m1.3.3.3.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.3">𝑃</ci><apply id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1"><eq id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.1"></eq><ci id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.2">𝐴</ci><apply id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3"><csymbol cd="latexml" id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.1">conditional</csymbol><ci id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.2">𝑎</ci><list id="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p2.5.m1.3.3.1.1.1.3.3.2"><ci id="S3.SS1.SSS1.p2.5.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m1.1.1">𝑥</ci><ci id="S3.SS1.SSS1.p2.5.m1.2.2.cmml" xref="S3.SS1.SSS1.p2.5.m1.2.2">𝑞</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m1.3c">P(A=a|x,q)</annotation></semantics></math>. The denominator is constant for a given question and image and can thus be ignored.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">They model the three probabilities in the numerator with three separate models. The second and third probabilities are both modeled using logistic regression. The features used for the question was the skip-thought vector representation <cite class="ltx_cite ltx_citemacro_cite">Kiros et al. (<a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite> of the question (They use the pre-trained skip thought model). The first probability is modeled as a conditional multivariate Gaussian, similar in principle to Quadratic Discriminant Analysis. The original image features are used in this model.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">The authors also introduced some simple VQA baselines, like feeding only image features or only question features to a logistic regression classifier, feeding both image and question features to a logistic regressor, and feeding the same features to a multi-layer perceptron. They evaluated results on the DAQUAR, COCO-QA, Visual7W and VQA datasets.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Multi-World QA <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib17" title="" class="ltx_ref">2014</a>)</cite>
</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.3" class="ltx_p">This paper models the probability of an answer given a question and an image as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.7" class="ltx_Math" alttext="P(A=a|Q,W)=\Sigma_{T}P(A=a|T,W)P(T|Q)" display="block"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><mi id="S3.E2.m1.5.5.1.3" xref="S3.E2.m1.5.5.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">A</mi><mo id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.E2.m1.5.5.1.1.1.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.3.1.cmml">|</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.3.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.3.1.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">Q</mi><mo id="S3.E2.m1.5.5.1.1.1.1.3.3.2.1" xref="S3.E2.m1.5.5.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">W</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.7.7.4" xref="S3.E2.m1.7.7.4.cmml">=</mo><mrow id="S3.E2.m1.7.7.3" xref="S3.E2.m1.7.7.3.cmml"><msub id="S3.E2.m1.7.7.3.4" xref="S3.E2.m1.7.7.3.4.cmml"><mi mathvariant="normal" id="S3.E2.m1.7.7.3.4.2" xref="S3.E2.m1.7.7.3.4.2.cmml">Σ</mi><mi id="S3.E2.m1.7.7.3.4.3" xref="S3.E2.m1.7.7.3.4.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.3.3" xref="S3.E2.m1.7.7.3.3.cmml">​</mo><mi id="S3.E2.m1.7.7.3.5" xref="S3.E2.m1.7.7.3.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.3.3a" xref="S3.E2.m1.7.7.3.3.cmml">​</mo><mrow id="S3.E2.m1.6.6.2.1.1" xref="S3.E2.m1.6.6.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.2.1.1.2" xref="S3.E2.m1.6.6.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.2.1.1.1" xref="S3.E2.m1.6.6.2.1.1.1.cmml"><mi id="S3.E2.m1.6.6.2.1.1.1.2" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml">A</mi><mo id="S3.E2.m1.6.6.2.1.1.1.1" xref="S3.E2.m1.6.6.2.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.6.6.2.1.1.1.3" xref="S3.E2.m1.6.6.2.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.2.1.1.1.3.2" xref="S3.E2.m1.6.6.2.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.E2.m1.6.6.2.1.1.1.3.1" xref="S3.E2.m1.6.6.2.1.1.1.3.1.cmml">|</mo><mrow id="S3.E2.m1.6.6.2.1.1.1.3.3.2" xref="S3.E2.m1.6.6.2.1.1.1.3.3.1.cmml"><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">T</mi><mo id="S3.E2.m1.6.6.2.1.1.1.3.3.2.1" xref="S3.E2.m1.6.6.2.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">W</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.6.6.2.1.1.3" xref="S3.E2.m1.6.6.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.3.3b" xref="S3.E2.m1.7.7.3.3.cmml">​</mo><mi id="S3.E2.m1.7.7.3.6" xref="S3.E2.m1.7.7.3.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.3.3c" xref="S3.E2.m1.7.7.3.3.cmml">​</mo><mrow id="S3.E2.m1.7.7.3.2.1" xref="S3.E2.m1.7.7.3.2.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.3.2.1.2" xref="S3.E2.m1.7.7.3.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.7.7.3.2.1.1" xref="S3.E2.m1.7.7.3.2.1.1.cmml"><mi id="S3.E2.m1.7.7.3.2.1.1.2" xref="S3.E2.m1.7.7.3.2.1.1.2.cmml">T</mi><mo fence="false" id="S3.E2.m1.7.7.3.2.1.1.1" xref="S3.E2.m1.7.7.3.2.1.1.1.cmml">|</mo><mi id="S3.E2.m1.7.7.3.2.1.1.3" xref="S3.E2.m1.7.7.3.2.1.1.3.cmml">Q</mi></mrow><mo stretchy="false" id="S3.E2.m1.7.7.3.2.1.3" xref="S3.E2.m1.7.7.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7"><eq id="S3.E2.m1.7.7.4.cmml" xref="S3.E2.m1.7.7.4"></eq><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><times id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2"></times><ci id="S3.E2.m1.5.5.1.3.cmml" xref="S3.E2.m1.5.5.1.3">𝑃</ci><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><eq id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"></eq><ci id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2">𝐴</ci><apply id="S3.E2.m1.5.5.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3"><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1">conditional</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2">𝑎</ci><list id="S3.E2.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑄</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑊</ci></list></apply></apply></apply><apply id="S3.E2.m1.7.7.3.cmml" xref="S3.E2.m1.7.7.3"><times id="S3.E2.m1.7.7.3.3.cmml" xref="S3.E2.m1.7.7.3.3"></times><apply id="S3.E2.m1.7.7.3.4.cmml" xref="S3.E2.m1.7.7.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.3.4.1.cmml" xref="S3.E2.m1.7.7.3.4">subscript</csymbol><ci id="S3.E2.m1.7.7.3.4.2.cmml" xref="S3.E2.m1.7.7.3.4.2">Σ</ci><ci id="S3.E2.m1.7.7.3.4.3.cmml" xref="S3.E2.m1.7.7.3.4.3">𝑇</ci></apply><ci id="S3.E2.m1.7.7.3.5.cmml" xref="S3.E2.m1.7.7.3.5">𝑃</ci><apply id="S3.E2.m1.6.6.2.1.1.1.cmml" xref="S3.E2.m1.6.6.2.1.1"><eq id="S3.E2.m1.6.6.2.1.1.1.1.cmml" xref="S3.E2.m1.6.6.2.1.1.1.1"></eq><ci id="S3.E2.m1.6.6.2.1.1.1.2.cmml" xref="S3.E2.m1.6.6.2.1.1.1.2">𝐴</ci><apply id="S3.E2.m1.6.6.2.1.1.1.3.cmml" xref="S3.E2.m1.6.6.2.1.1.1.3"><csymbol cd="latexml" id="S3.E2.m1.6.6.2.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.2.1.1.1.3.1">conditional</csymbol><ci id="S3.E2.m1.6.6.2.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.2.1.1.1.3.2">𝑎</ci><list id="S3.E2.m1.6.6.2.1.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.2.1.1.1.3.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑇</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑊</ci></list></apply></apply><ci id="S3.E2.m1.7.7.3.6.cmml" xref="S3.E2.m1.7.7.3.6">𝑃</ci><apply id="S3.E2.m1.7.7.3.2.1.1.cmml" xref="S3.E2.m1.7.7.3.2.1"><csymbol cd="latexml" id="S3.E2.m1.7.7.3.2.1.1.1.cmml" xref="S3.E2.m1.7.7.3.2.1.1.1">conditional</csymbol><ci id="S3.E2.m1.7.7.3.2.1.1.2.cmml" xref="S3.E2.m1.7.7.3.2.1.1.2">𝑇</ci><ci id="S3.E2.m1.7.7.3.2.1.1.3.cmml" xref="S3.E2.m1.7.7.3.2.1.1.3">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">P(A=a|Q,W)=\Sigma_{T}P(A=a|T,W)P(T|Q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">Here T is a latent variable corresponding to semantic tree obtained from a semantic parser run on the question. W is the world, which is a representation of the image. This can be just the original image or the image along with additional features obtained from segmentation. <math id="S3.SS1.SSS2.p1.1.m1.3" class="ltx_Math" alttext="P(A=a|T,W)" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.3a"><mrow id="S3.SS1.SSS2.p1.1.m1.3.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.3.3.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p1.1.m1.3.3.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p1.1.m1.3.3.1.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.1.cmml">|</mo><mrow id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">T</mi><mo id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.2.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS2.p1.1.m1.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.cmml">W</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.3b"><apply id="S3.SS1.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3"><times id="S3.SS1.SSS2.p1.1.m1.3.3.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.2"></times><ci id="S3.SS1.SSS2.p1.1.m1.3.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3">𝑃</ci><apply id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1"><eq id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.1"></eq><ci id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.2">𝐴</ci><apply id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3"><csymbol cd="latexml" id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.1">conditional</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.2">𝑎</ci><list id="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.1.1.1.3.3.2"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">𝑇</ci><ci id="S3.SS1.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2">𝑊</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.3c">P(A=a|T,W)</annotation></semantics></math> is evaluated using a deterministic evaluation function. <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="P(T|Q)" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.2.cmml">T</mi><mo fence="false" id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.3.cmml">Q</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><times id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2"></times><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">𝑃</ci><apply id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.3">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">P(T|Q)</annotation></semantics></math> is obtained by training a simple log-linear model. This model will be called SWQA (Single World Question Answering).</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The authors further extend the SWQA model to a multi-world scenario to model uncertainty in segmentation and class labeling. Different labelings lead to different worlds, so the probability is now modeled as</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div id="S3.E3.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:227.6pt;height:8.800000000000001pt;vertical-align:-2.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.8pt,0.5pt) scale(0.87821638705899,0.87821638705899) ;">
<p id="S3.E3.m1.1.1.1.1" class="ltx_p"><math id="S3.E3.m1.1.1.1.1.m1.8" class="ltx_Math" alttext="P(A=a|Q,W)=\Sigma_{W}\Sigma_{T}P(A=a|T,W)P(W|S)P(T|Q)" display="inline"><semantics id="S3.E3.m1.1.1.1.1.m1.8a"><mrow id="S3.E3.m1.1.1.1.1.m1.8.8" xref="S3.E3.m1.1.1.1.1.m1.8.8.cmml"><mrow id="S3.E3.m1.1.1.1.1.m1.5.5.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.5.5.1.3" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.5.5.1.2" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.2.cmml">A</mi><mo id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">|</mo><mrow id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.1.1" xref="S3.E3.m1.1.1.1.1.m1.1.1.cmml">Q</mi><mo id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E3.m1.1.1.1.1.m1.2.2" xref="S3.E3.m1.1.1.1.1.m1.2.2.cmml">W</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.m1.8.8.5" xref="S3.E3.m1.1.1.1.1.m1.8.8.5.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.m1.8.8.4" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.cmml"><msub id="S3.E3.m1.1.1.1.1.m1.8.8.4.5" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.2" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5.2.cmml">Σ</mi><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.3" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5.3.cmml">W</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.m1.8.8.4.6" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.2" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6.2.cmml">Σ</mi><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.3" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4a" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.7" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.7.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4b" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.2.cmml">A</mi><mo id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.2.cmml">a</mi><mo fence="false" id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.1.cmml">|</mo><mrow id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.3.3" xref="S3.E3.m1.1.1.1.1.m1.3.3.cmml">T</mi><mo id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E3.m1.1.1.1.1.m1.4.4" xref="S3.E3.m1.1.1.1.1.m1.4.4.cmml">W</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4c" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.8" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.8.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4d" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.2" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.2.cmml">W</mi><mo fence="false" id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.1.cmml">|</mo><mi id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.3" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4e" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.9" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.9.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.m1.8.8.4.4f" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.2" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.2" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.2.cmml">T</mi><mo fence="false" id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.1" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.1.cmml">|</mo><mi id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.3" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.3.cmml">Q</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.3" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1.1.1.1.m1.8b"><apply id="S3.E3.m1.1.1.1.1.m1.8.8.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8"><eq id="S3.E3.m1.1.1.1.1.m1.8.8.5.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.5"></eq><apply id="S3.E3.m1.1.1.1.1.m1.5.5.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1"><times id="S3.E3.m1.1.1.1.1.m1.5.5.1.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.2"></times><ci id="S3.E3.m1.1.1.1.1.m1.5.5.1.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.3">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1"><eq id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.1"></eq><ci id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.2">𝐴</ci><apply id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.1">conditional</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.2">𝑎</ci><list id="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.5.5.1.1.1.1.3.3.2"><ci id="S3.E3.m1.1.1.1.1.m1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.1.1">𝑄</ci><ci id="S3.E3.m1.1.1.1.1.m1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.2.2">𝑊</ci></list></apply></apply></apply><apply id="S3.E3.m1.1.1.1.1.m1.8.8.4.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4"><times id="S3.E3.m1.1.1.1.1.m1.8.8.4.4.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.4"></times><apply id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5.2">Σ</ci><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.5.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.5.3">𝑊</ci></apply><apply id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6.2">Σ</ci><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.6.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.6.3">𝑇</ci></apply><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.7.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.7">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1"><eq id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.1"></eq><ci id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.2">𝐴</ci><apply id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.1">conditional</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.2">𝑎</ci><list id="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.6.6.2.1.1.1.3.3.2"><ci id="S3.E3.m1.1.1.1.1.m1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.3.3">𝑇</ci><ci id="S3.E3.m1.1.1.1.1.m1.4.4.cmml" xref="S3.E3.m1.1.1.1.1.m1.4.4">𝑊</ci></list></apply></apply><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.8.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.8">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.1">conditional</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.2">𝑊</ci><ci id="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.7.7.3.2.1.1.3">𝑆</ci></apply><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.9.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.9">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.1">conditional</csymbol><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.2">𝑇</ci><ci id="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.m1.8.8.4.3.1.1.3">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1.1.1.1.m1.8c">P(A=a|Q,W)=\Sigma_{W}\Sigma_{T}P(A=a|T,W)P(W|S)P(T|Q)</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p2.2" class="ltx_p">Here S is a set of segments along with a distribution over class labels for each segment. So sampling from the distribution for each segment will give us one possible world. As the above equation becomes intractable, the authors sample a fixed number of worlds from S. This model will be called MWQA (Multi World Question Answering).</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">These models are evaluated on the DAQUAR dataset.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Non-attention Deep Learning Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Deep learning models for VQA typically involve the use of Convolutional Neural Networks (CNNs) to embed the image and word embeddings such as Word2Vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib19" title="" class="ltx_ref">2013</a>)</cite> along with Recurrent Neural Networks (RNNs) to embed the question. These embeddings are combined and processed in various ways to obtain the answer. The following model descriptions assume that the reader is familiar with both CNNs <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a href="#bib.bib12" title="" class="ltx_ref">2012</a>)</cite> as well as RNN-variants like Long Short Term Memory units (LSTMs) <cite class="ltx_cite ltx_citemacro_cite">Hochreiter and Schmidhuber (<a href="#bib.bib9" title="" class="ltx_ref">1997</a>)</cite> and Gated Recurrent Units (GRUs) <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib7" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Some approaches do not involve the use of RNNs. These are discussed first.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>iBOWIMG</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib33" title="" class="ltx_ref">2015</a>)</cite> propose a baseline model called iBOWIMG for VQA. They use the output of a later layer of the pre-trained GoogLe Net model for image classification <cite class="ltx_cite ltx_citemacro_cite">Szegedy et al. (<a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite> to extract image features. The word embeddings of each word in the question are taken as the text features, so the text features are simple bag-of-words. The image and text features are concatenated and softmax regression is performed across the answer classes. They showed that this model achieved performance comparable to several RNN based approaches on the VQA dataset.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Full-CNN</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite> propose a CNN-only model that we refer to here as Full-CNN. They use three different CNNs: an image CNN to encode the image, a question CNN to encode the question, and a join CNN to combine the image and question encoding together and produce a joint representation.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The image CNN uses the same architecture as VGGnet <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>)</cite> and obtains a 4096-length vector from the second-last layer of this network. This is passed through another fully connected layer to get the image representation vector of size 400. The question CNN involves three layers of convolution + max pooling. The size of the convolutional receptive field is set to 3. In other words, the kernel looks at a word along with its immediate neighbors. The joint CNN, which they call the multi-modal CNN, performs convolution across the question representation with receptive field size 2. Each convolution operation is provided the full image representation. The final representation from the multi-modal CNN is given to a softmax layer to predict the answer. The model is evaluated on the DAQUAR and COCO-QA datasets.
<br class="ltx_break">
<br class="ltx_break">The following models use both CNNs as well as RNNs.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Ask Your Neurons (AYN) <cite class="ltx_cite ltx_citemacro_cite">Malinowski et al. (<a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.5" class="ltx_p">This model uses a CNN to encode the image <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">x</annotation></semantics></math> and obtain a continuous vector representation of the image. The question <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">q</annotation></semantics></math> is encoded using an LSTM or a GRU network for which the input at time step <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mi id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><ci id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">t</annotation></semantics></math> is the word embedding for the <math id="S3.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="t^{th}" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><msup id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml">t</mi><mrow id="S3.SS2.SSS3.p1.4.m4.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><apply id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2">𝑡</ci><apply id="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3"><times id="S3.SS2.SSS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">t^{th}</annotation></semantics></math> question word <math id="S3.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="q_{t}" display="inline"><semantics id="S3.SS2.SSS3.p1.5.m5.1a"><msub id="S3.SS2.SSS3.p1.5.m5.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS3.p1.5.m5.1.1.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS3.p1.5.m5.1.1.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m5.1b"><apply id="S3.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.2">𝑞</ci><ci id="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m5.1c">q_{t}</annotation></semantics></math>, as well as the encoded image vector. The hidden vector obtained at the final time step is the question encoding. A simple bag of words baseline the authors use is to encode the question as the sum of all the word embeddings.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">The answer can be decoded in two different ways, either as a classification over different answers, or as a generation of the answer. Classification is performed by a fully connected layer followed by a softmax over possible answers. Generation, on the other hand, is performed by a decoder LSTM. This LSTM at each time point takes as input the previously generated word, as well as the question and image encoding. The next word is predicted using a softmax over the vocabulary. An important point to note is that this model shares some weights between the encoder and decoder LSTMs. The model is evaluated on the DAQUAR dataset.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Vis+LSTM <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>
</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">This model is very similar to the AYN model. The model uses the final layer of VGGnet to obtain the image encoding. They use an LSTM to encode the question. In contrast to the previous model, they provide the encoded image as the first ‘word’ to this LSTM network, before the question. The output of this LSTM goes through a fully connected followed by softmax layer. They call this model Vis+LSTM.</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<p id="S3.SS2.SSS4.p2.1" class="ltx_p">The authors also propose a 2Vis+BLSTM model, which uses a bidirectional LSTM instead. The backward LSTM gets the image encoding as first input as well. The outputs of both LSTMs are concatenated and then passed through a fully connected and softmax layer.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3">DAQUAR (Reduced)</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3">DAQUAR (All)</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3">COCO-QA</th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.2.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S3.T2.1.2.2.2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.3.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.3.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.3.1.2.1.1" class="ltx_text ltx_font_bold">at 0.9 (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T2.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.4.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.4.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.4.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.4.1.2.1.1" class="ltx_text ltx_font_bold">at 0 (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.5.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.5.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S3.T2.1.2.2.5.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.5.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.6.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.6.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.6.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.6.1.2.1.1" class="ltx_text ltx_font_bold">at 0.9 (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T2.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.7.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.7.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.7.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.7.1.2.1.1" class="ltx_text ltx_font_bold">at 0 (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.8.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.8.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S3.T2.1.2.2.8.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.8.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.9.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.9.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.9.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.9.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.9.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.9.1.2.1.1" class="ltx_text ltx_font_bold">at 0.9 (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.2.2.10.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.2.2.10.1.1" class="ltx_tr">
<td id="S3.T2.1.2.2.10.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.10.1.1.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
<tr id="S3.T2.1.2.2.10.1.2" class="ltx_tr">
<td id="S3.T2.1.2.2.10.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.2.2.10.1.2.1.1" class="ltx_text ltx_font_bold">at 0 (%)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<th id="S3.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.1.3.1.1.1" class="ltx_text ltx_font_bold">SWQA</span></th>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">9.69</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">14.73</td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.57</td>
<td id="S3.T2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">7.86</td>
<td id="S3.T2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">11.86</td>
<td id="S3.T2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.79</td>
<td id="S3.T2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<th id="S3.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.4.2.1.1" class="ltx_text ltx_font_bold">MWQA</span></th>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center">12.73</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_center">18.10</td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">51.47</td>
<td id="S3.T2.1.4.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.4.2.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.2.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.2.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<th id="S3.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.5.3.1.1" class="ltx_text ltx_font_bold">Vis+LSTM</span></th>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_center">34.41</td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_center">46.05</td>
<td id="S3.T2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">82.23</td>
<td id="S3.T2.1.5.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.5.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.5.3.8" class="ltx_td ltx_align_center">53.31</td>
<td id="S3.T2.1.5.3.9" class="ltx_td ltx_align_center">63.91</td>
<td id="S3.T2.1.5.3.10" class="ltx_td ltx_align_center">88.25</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<th id="S3.T2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.6.4.1.1" class="ltx_text ltx_font_bold">AYN</span></th>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_center">34.68</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_center">40.76</td>
<td id="S3.T2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">79.54</td>
<td id="S3.T2.1.6.4.5" class="ltx_td ltx_align_center">21.67</td>
<td id="S3.T2.1.6.4.6" class="ltx_td ltx_align_center">27.99</td>
<td id="S3.T2.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r">65.11</td>
<td id="S3.T2.1.6.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.6.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.6.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<th id="S3.T2.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.7.5.1.1" class="ltx_text ltx_font_bold">2Vis+BLSTM</span></th>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_center">35.78</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_center">46.83</td>
<td id="S3.T2.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">82.15</td>
<td id="S3.T2.1.7.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.7.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.7.5.8" class="ltx_td ltx_align_center">55.09</td>
<td id="S3.T2.1.7.5.9" class="ltx_td ltx_align_center">65.34</td>
<td id="S3.T2.1.7.5.10" class="ltx_td ltx_align_center">88.64</td>
</tr>
<tr id="S3.T2.1.8.6" class="ltx_tr">
<th id="S3.T2.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.8.6.1.1" class="ltx_text ltx_font_bold">Full-CNN</span></th>
<td id="S3.T2.1.8.6.2" class="ltx_td ltx_align_center">42.76</td>
<td id="S3.T2.1.8.6.3" class="ltx_td ltx_align_center">47.58</td>
<td id="S3.T2.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">82.60</td>
<td id="S3.T2.1.8.6.5" class="ltx_td ltx_align_center">23.40</td>
<td id="S3.T2.1.8.6.6" class="ltx_td ltx_align_center">29.59</td>
<td id="S3.T2.1.8.6.7" class="ltx_td ltx_align_center ltx_border_r">62.95</td>
<td id="S3.T2.1.8.6.8" class="ltx_td ltx_align_center">54.95</td>
<td id="S3.T2.1.8.6.9" class="ltx_td ltx_align_center">65.36</td>
<td id="S3.T2.1.8.6.10" class="ltx_td ltx_align_center">88.58</td>
</tr>
<tr id="S3.T2.1.9.7" class="ltx_tr">
<th id="S3.T2.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.9.7.1.1" class="ltx_text ltx_font_bold">DPPnet</span></th>
<td id="S3.T2.1.9.7.2" class="ltx_td ltx_align_center">44.48</td>
<td id="S3.T2.1.9.7.3" class="ltx_td ltx_align_center">49.56</td>
<td id="S3.T2.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r">83.95</td>
<td id="S3.T2.1.9.7.5" class="ltx_td ltx_align_center">28.98</td>
<td id="S3.T2.1.9.7.6" class="ltx_td ltx_align_center">34.80</td>
<td id="S3.T2.1.9.7.7" class="ltx_td ltx_align_center ltx_border_r">67.81</td>
<td id="S3.T2.1.9.7.8" class="ltx_td ltx_align_center">61.19</td>
<td id="S3.T2.1.9.7.9" class="ltx_td ltx_align_center">70.84</td>
<td id="S3.T2.1.9.7.10" class="ltx_td ltx_align_center">90.61</td>
</tr>
<tr id="S3.T2.1.10.8" class="ltx_tr">
<th id="S3.T2.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.10.8.1.1" class="ltx_text ltx_font_bold">ATP</span></th>
<td id="S3.T2.1.10.8.2" class="ltx_td ltx_align_center">45.17</td>
<td id="S3.T2.1.10.8.3" class="ltx_td ltx_align_center">49.74</td>
<td id="S3.T2.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.10.8.4.1" class="ltx_text ltx_font_bold">85.13</span></td>
<td id="S3.T2.1.10.8.5" class="ltx_td ltx_align_center">28.96</td>
<td id="S3.T2.1.10.8.6" class="ltx_td ltx_align_center">34.74</td>
<td id="S3.T2.1.10.8.7" class="ltx_td ltx_align_center ltx_border_r">67.33</td>
<td id="S3.T2.1.10.8.8" class="ltx_td ltx_align_center">63.18</td>
<td id="S3.T2.1.10.8.9" class="ltx_td ltx_align_center">73.14</td>
<td id="S3.T2.1.10.8.10" class="ltx_td ltx_align_center">91.32</td>
</tr>
<tr id="S3.T2.1.11.9" class="ltx_tr">
<th id="S3.T2.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.11.9.1.1" class="ltx_text ltx_font_bold">SAN</span></th>
<td id="S3.T2.1.11.9.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.11.9.2.1" class="ltx_text ltx_font_bold">45.50</span></td>
<td id="S3.T2.1.11.9.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.11.9.3.1" class="ltx_text ltx_font_bold">50.20</span></td>
<td id="S3.T2.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r">83.60</td>
<td id="S3.T2.1.11.9.5" class="ltx_td ltx_align_center"><span id="S3.T2.1.11.9.5.1" class="ltx_text ltx_font_bold">29.30</span></td>
<td id="S3.T2.1.11.9.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.11.9.6.1" class="ltx_text ltx_font_bold">35.10</span></td>
<td id="S3.T2.1.11.9.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.11.9.7.1" class="ltx_text ltx_font_bold">68.60</span></td>
<td id="S3.T2.1.11.9.8" class="ltx_td ltx_align_center">61.60</td>
<td id="S3.T2.1.11.9.9" class="ltx_td ltx_align_center">71.60</td>
<td id="S3.T2.1.11.9.10" class="ltx_td ltx_align_center">90.90</td>
</tr>
<tr id="S3.T2.1.12.10" class="ltx_tr">
<th id="S3.T2.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.12.10.1.1" class="ltx_text ltx_font_bold">CoAtt</span></th>
<td id="S3.T2.1.12.10.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.12.10.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.12.10.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.12.10.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.12.10.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.12.10.8" class="ltx_td ltx_align_center">65.40</td>
<td id="S3.T2.1.12.10.9" class="ltx_td ltx_align_center">75.10</td>
<td id="S3.T2.1.12.10.10" class="ltx_td ltx_align_center">92.00</td>
</tr>
<tr id="S3.T2.1.13.11" class="ltx_tr">
<th id="S3.T2.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.13.11.1.1" class="ltx_text ltx_font_bold">AMA</span></th>
<td id="S3.T2.1.13.11.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.13.11.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.13.11.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.13.11.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.13.11.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.13.11.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.13.11.8" class="ltx_td ltx_align_center"><span id="S3.T2.1.13.11.8.1" class="ltx_text ltx_font_bold">69.73</span></td>
<td id="S3.T2.1.13.11.9" class="ltx_td ltx_align_center"><span id="S3.T2.1.13.11.9.1" class="ltx_text ltx_font_bold">77.14</span></td>
<td id="S3.T2.1.13.11.10" class="ltx_td ltx_align_center"><span id="S3.T2.1.13.11.10.1" class="ltx_text ltx_font_bold">92.50</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of various models on DAQUAR (reduced), DAQUAR (full), COCO-QA</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Dynamic Parameter Prediction (DPPnet) <cite class="ltx_cite ltx_citemacro_cite">Noh et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">The authors of this paper argue that having a fixed set of parameters is not powerful enough for the VQA task. They take the architecture of VGG net, remove its final softmax layer and add three more fully connected layers, the last of which is followed by a softmax over possible answers. The second of these fully connected layers does not have a fixed set of parameters. Instead, the parameters come from a GRU network. This GRU network is used to encode the question, and the output from the network is passed through a fully connected layer to give a small vector of candidate parameter weights. This vector is then mapped to the larger vector of parameter weights required by the second fully connected layer above, using an inverse hashing function. This hashing technique is included by the authors to avoid having to predict the full set of parameter weights which could be expensive and may lead to over-fitting. The dynamic parameter layer can alternatively be seen as multiplying the image representation and question representation together to get a joint representation, as opposed to combining them in a linear fashion. The model is evaluated on the DAQUAR, COCO-QA and VQA datasets.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Attention-based Deep Learning Techniques</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Attention based techniques are some of the most popular techniques that are being used across many tasks like machine translation <cite class="ltx_cite ltx_citemacro_cite">Bahdanau et al. (<a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite> etc. For the VQA task, attention models involve focusing on important parts of the image, question or both in order to effectively give an answer.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Where to Look</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.5" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Shih et al. (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite> propose an attention-based model henceforth referred to as WTL. They use VGGnet for encoding the image and concatenate the outputs of the final two layers of VGGnet to obtain image encoding. Question representation is obtained by averaging the word vectors of each word in the question. An attention vector is computed over the set of image features to decide which region in the image to give importance to. This vector is computed in the following way: If <math id="S3.SS3.SSS1.p1.1.m1.2" class="ltx_Math" alttext="V=(\overrightarrow{v_{1}},\overrightarrow{v_{2}}\ldots\overrightarrow{v_{K}})" display="inline"><semantics id="S3.SS3.SSS1.p1.1.m1.2a"><mrow id="S3.SS3.SSS1.p1.1.m1.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.2.2.3" xref="S3.SS3.SSS1.p1.1.m1.2.2.3.cmml">V</mi><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS3.SSS1.p1.1.m1.2.2.1.1" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.2.cmml">(</mo><mover accent="true" id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><msub id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.2.cmml">v</mi><mn id="S3.SS3.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS3.SSS1.p1.1.m1.1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml">→</mo></mover><mo id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mover accent="true" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.cmml"><msub id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.2.cmml">v</mi><mn id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.3" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.1" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.1.cmml">→</mo></mover><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1a" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mover accent="true" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.cmml"><msub id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.2.cmml">v</mi><mi id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.3" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.3.cmml">K</mi></msub><mo stretchy="false" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.1" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.1.cmml">→</mo></mover></mrow><mo stretchy="false" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.4" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.2b"><apply id="S3.SS3.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2"><eq id="S3.SS3.SSS1.p1.1.m1.2.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2"></eq><ci id="S3.SS3.SSS1.p1.1.m1.2.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.3">𝑉</ci><interval closure="open" id="S3.SS3.SSS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.1">→</ci><apply id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.2">𝑣</ci><cn type="integer" id="S3.SS3.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.3">1</cn></apply></apply><apply id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1"><times id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.1"></times><apply id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2"><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.1">→</ci><apply id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.2">𝑣</ci><cn type="integer" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.2.2.3">2</cn></apply></apply><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.3">…</ci><apply id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4"><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.1">→</ci><apply id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.2">𝑣</ci><ci id="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.1.1.1.4.2.3">𝐾</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.2c">V=(\overrightarrow{v_{1}},\overrightarrow{v_{2}}\ldots\overrightarrow{v_{K}})</annotation></semantics></math> is the set of image features, and <math id="S3.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\overrightarrow{q}" display="inline"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mover accent="true" id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">q</mi><mo stretchy="false" id="S3.SS3.SSS1.p1.2.m2.1.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><ci id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1">→</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">\overrightarrow{q}</annotation></semantics></math> is the question embedding, then the importance of the <math id="S3.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><msup id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p1.3.m3.1.1.2" xref="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml">j</mi><mrow id="S3.SS3.SSS1.p1.3.m3.1.1.3" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.3.m3.1.1.3.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><apply id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.2">𝑗</ci><apply id="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3"><times id="S3.SS3.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS3.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS3.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">j^{th}</annotation></semantics></math> region is computed as 
<br class="ltx_break"><math id="S3.SS3.SSS1.p1.4.m4.2" class="ltx_Math" alttext="g_{j}=(A\overrightarrow{v_{j}}+b^{A})^{T}(B\overrightarrow{q}+b^{B})" display="inline"><semantics id="S3.SS3.SSS1.p1.4.m4.2a"><mrow id="S3.SS3.SSS1.p1.4.m4.2.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.cmml"><msub id="S3.SS3.SSS1.p1.4.m4.2.2.4" xref="S3.SS3.SSS1.p1.4.m4.2.2.4.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.2.2.4.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.4.2.cmml">g</mi><mi id="S3.SS3.SSS1.p1.4.m4.2.2.4.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.4.3.cmml">j</mi></msub><mo id="S3.SS3.SSS1.p1.4.m4.2.2.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.3.cmml">=</mo><mrow id="S3.SS3.SSS1.p1.4.m4.2.2.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.cmml"><msup id="S3.SS3.SSS1.p1.4.m4.1.1.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.cmml"><mrow id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.cmml"><mrow id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.1.cmml">​</mo><mover accent="true" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.2.cmml">v</mi><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.1.cmml">→</mo></mover></mrow><mo id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.1.cmml">+</mo><msup id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.2" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.2.cmml">b</mi><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.3.cmml">A</mi></msup></mrow><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.3" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.4.m4.2.2.2.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.cmml"><mrow id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.1" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.1.cmml">​</mo><mover accent="true" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.2.cmml">q</mi><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.1" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.1.cmml">→</mo></mover></mrow><mo id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.1" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.1.cmml">+</mo><msup id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.cmml"><mi id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.2" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.2.cmml">b</mi><mi id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.3.cmml">B</mi></msup></mrow><mo stretchy="false" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.3" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.2b"><apply id="S3.SS3.SSS1.p1.4.m4.2.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2"><eq id="S3.SS3.SSS1.p1.4.m4.2.2.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.3"></eq><apply id="S3.SS3.SSS1.p1.4.m4.2.2.4.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.2.2.4.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.4">subscript</csymbol><ci id="S3.SS3.SSS1.p1.4.m4.2.2.4.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.4.2">𝑔</ci><ci id="S3.SS3.SSS1.p1.4.m4.2.2.4.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.4.3">𝑗</ci></apply><apply id="S3.SS3.SSS1.p1.4.m4.2.2.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2"><times id="S3.SS3.SSS1.p1.4.m4.2.2.2.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.3"></times><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1">superscript</csymbol><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1"><plus id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.1"></plus><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2"><times id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.1"></times><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.2">𝐴</ci><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3"><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.1">→</ci><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.2">𝑣</ci><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.2.3.2.3">𝑗</ci></apply></apply></apply><apply id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.1.1.1.3.3">𝐴</ci></apply></apply><ci id="S3.SS3.SSS1.p1.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1.1.1.3">𝑇</ci></apply><apply id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1"><plus id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.1"></plus><apply id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2"><times id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.1"></times><ci id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.2">𝐵</ci><apply id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3"><ci id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.1">→</ci><ci id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.2.3.2">𝑞</ci></apply></apply><apply id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.2.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.2">𝑏</ci><ci id="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.3.cmml" xref="S3.SS3.SSS1.p1.4.m4.2.2.2.2.1.1.3.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.2c">g_{j}=(A\overrightarrow{v_{j}}+b^{A})^{T}(B\overrightarrow{q}+b^{B})</annotation></semantics></math>
<br class="ltx_break">The attention weights are obtained on normalising <math id="S3.SS3.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\overrightarrow{g}" display="inline"><semantics id="S3.SS3.SSS1.p1.5.m5.1a"><mover accent="true" id="S3.SS3.SSS1.p1.5.m5.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS1.p1.5.m5.1.1.2" xref="S3.SS3.SSS1.p1.5.m5.1.1.2.cmml">g</mi><mo stretchy="false" id="S3.SS3.SSS1.p1.5.m5.1.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.1b"><apply id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1"><ci id="S3.SS3.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1.1">→</ci><ci id="S3.SS3.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1.2">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.1c">\overrightarrow{g}</annotation></semantics></math>. The final image representation is an attention weighted sum of the different regions. This is concatenated to the question embedding and passed to a dense+softmax layer.
The model is evaluated on the VQA dataset. The loss function is a max margin based loss that takes into account the VQA evaluation metric.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Recurrent Spatial Attention (R-SA) <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.7" class="ltx_p">This model is a step above the previous model in two ways. Firstly, it uses LSTMs to encode the question, and secondly, it computes attention over the image repeatedly after scanning each word of the question. More concretely, we repeatedly compute an attention weighted sum of image features, <math id="S3.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="r_{t}" display="inline"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml">r</mi><mi id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">𝑟</ci><ci id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">r_{t}</annotation></semantics></math>, at each time step <math id="S3.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.SSS2.p1.2.m2.1a"><mi id="S3.SS3.SSS2.p1.2.m2.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.1b"><ci id="S3.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.1c">t</annotation></semantics></math> of the LSTM. <math id="S3.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="r_{t}" display="inline"><semantics id="S3.SS3.SSS2.p1.3.m3.1a"><msub id="S3.SS3.SSS2.p1.3.m3.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.1.1.2" xref="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml">r</mi><mi id="S3.SS3.SSS2.p1.3.m3.1.1.3" xref="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.1b"><apply id="S3.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.2">𝑟</ci><ci id="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.1c">r_{t}</annotation></semantics></math> goes as an additional input to the next time step of LSTM. The attention weights <math id="S3.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="a_{t}" display="inline"><semantics id="S3.SS3.SSS2.p1.4.m4.1a"><msub id="S3.SS3.SSS2.p1.4.m4.1.1" xref="S3.SS3.SSS2.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS2.p1.4.m4.1.1.2" xref="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml">a</mi><mi id="S3.SS3.SSS2.p1.4.m4.1.1.3" xref="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m4.1b"><apply id="S3.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.2">𝑎</ci><ci id="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m4.1c">a_{t}</annotation></semantics></math> used to obtain <math id="S3.SS3.SSS2.p1.5.m5.1" class="ltx_Math" alttext="r_{t}" display="inline"><semantics id="S3.SS3.SSS2.p1.5.m5.1a"><msub id="S3.SS3.SSS2.p1.5.m5.1.1" xref="S3.SS3.SSS2.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS2.p1.5.m5.1.1.2" xref="S3.SS3.SSS2.p1.5.m5.1.1.2.cmml">r</mi><mi id="S3.SS3.SSS2.p1.5.m5.1.1.3" xref="S3.SS3.SSS2.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m5.1b"><apply id="S3.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.2">𝑟</ci><ci id="S3.SS3.SSS2.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m5.1c">r_{t}</annotation></semantics></math> are computed using a dense+softmax layer over the previous hidden state of the LSTM <math id="S3.SS3.SSS2.p1.6.m6.1" class="ltx_Math" alttext="h_{t-1}" display="inline"><semantics id="S3.SS3.SSS2.p1.6.m6.1a"><msub id="S3.SS3.SSS2.p1.6.m6.1.1" xref="S3.SS3.SSS2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS2.p1.6.m6.1.1.2" xref="S3.SS3.SSS2.p1.6.m6.1.1.2.cmml">h</mi><mrow id="S3.SS3.SSS2.p1.6.m6.1.1.3" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS3.SSS2.p1.6.m6.1.1.3.2" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S3.SS3.SSS2.p1.6.m6.1.1.3.1" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.1.cmml">−</mo><mn id="S3.SS3.SSS2.p1.6.m6.1.1.3.3" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.6.m6.1b"><apply id="S3.SS3.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1.2">ℎ</ci><apply id="S3.SS3.SSS2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1.3"><minus id="S3.SS3.SSS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.1"></minus><ci id="S3.SS3.SSS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS3.SSS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.6.m6.1c">h_{t-1}</annotation></semantics></math> and the image itself. Thus intuitively as we read the question we repeatedly decide which parts of the image to attend to, and parts to attend to now depend both on the current word as well as the previous attention weighted image through <math id="S3.SS3.SSS2.p1.7.m7.1" class="ltx_Math" alttext="h_{t-1}" display="inline"><semantics id="S3.SS3.SSS2.p1.7.m7.1a"><msub id="S3.SS3.SSS2.p1.7.m7.1.1" xref="S3.SS3.SSS2.p1.7.m7.1.1.cmml"><mi id="S3.SS3.SSS2.p1.7.m7.1.1.2" xref="S3.SS3.SSS2.p1.7.m7.1.1.2.cmml">h</mi><mrow id="S3.SS3.SSS2.p1.7.m7.1.1.3" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.cmml"><mi id="S3.SS3.SSS2.p1.7.m7.1.1.3.2" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.2.cmml">t</mi><mo id="S3.SS3.SSS2.p1.7.m7.1.1.3.1" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.1.cmml">−</mo><mn id="S3.SS3.SSS2.p1.7.m7.1.1.3.3" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.7.m7.1b"><apply id="S3.SS3.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.7.m7.1.1.1.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.7.m7.1.1.2.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1.2">ℎ</ci><apply id="S3.SS3.SSS2.p1.7.m7.1.1.3.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1.3"><minus id="S3.SS3.SSS2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.1"></minus><ci id="S3.SS3.SSS2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS3.SSS2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.7.m7.1c">h_{t-1}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">This model is evaluated on the Visual7W dataset, for both the textual answering task as well as the pointing task (which points out a region in the image as an answer). The softmax cross-entropy loss between actual and predicted answer was used for the textual answering task. For the pointing task, log likelihood of a candidate region is obtained by taking dot product of feature representing that region and last state of LSTM. Again cross-entropy loss was used to train the model.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="5">Test-Development</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="5">Test-Standard</th>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<th id="S3.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4">Open Ended</th>
<th id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">M.C.</th>
<th id="S3.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4">Open Ended</th>
<th id="S3.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">M.C.</th>
</tr>
<tr id="S3.T3.1.3.3" class="ltx_tr">
<th id="S3.T3.1.3.3.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Y/N</th>
<th id="S3.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Number</th>
<th id="S3.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Other</th>
<th id="S3.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
<th id="S3.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">All</th>
<th id="S3.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Y/N</th>
<th id="S3.T3.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">Number</th>
<th id="S3.T3.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Other</th>
<th id="S3.T3.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
<th id="S3.T3.1.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.4.1" class="ltx_tr">
<th id="S3.T3.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T3.1.4.1.1.1" class="ltx_text ltx_font_bold">iBOWIMG</span></th>
<td id="S3.T3.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">76.5</td>
<td id="S3.T3.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S3.T3.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">42.6</td>
<td id="S3.T3.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">55.7</td>
<td id="S3.T3.1.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T3.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t">76.8</td>
<td id="S3.T3.1.4.1.8" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S3.T3.1.4.1.9" class="ltx_td ltx_align_center ltx_border_t">42.6</td>
<td id="S3.T3.1.4.1.10" class="ltx_td ltx_align_center ltx_border_t">55.9</td>
<td id="S3.T3.1.4.1.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T3.1.5.2" class="ltx_tr">
<th id="S3.T3.1.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.5.2.1.1" class="ltx_text ltx_font_bold">DPPnet</span></th>
<td id="S3.T3.1.5.2.2" class="ltx_td ltx_align_center">80.7</td>
<td id="S3.T3.1.5.2.3" class="ltx_td ltx_align_center">37.2</td>
<td id="S3.T3.1.5.2.4" class="ltx_td ltx_align_center">41.7</td>
<td id="S3.T3.1.5.2.5" class="ltx_td ltx_align_center">57.2</td>
<td id="S3.T3.1.5.2.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.5.2.7" class="ltx_td ltx_align_center">80.3</td>
<td id="S3.T3.1.5.2.8" class="ltx_td ltx_align_center">36.9</td>
<td id="S3.T3.1.5.2.9" class="ltx_td ltx_align_center">42.2</td>
<td id="S3.T3.1.5.2.10" class="ltx_td ltx_align_center">57.4</td>
<td id="S3.T3.1.5.2.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T3.1.6.3" class="ltx_tr">
<th id="S3.T3.1.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.6.3.1.1" class="ltx_text ltx_font_bold">WTL</span></th>
<td id="S3.T3.1.6.3.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.6" class="ltx_td ltx_align_center ltx_border_r">62.4</td>
<td id="S3.T3.1.6.3.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.10" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.6.3.11" class="ltx_td ltx_align_center">62.4</td>
</tr>
<tr id="S3.T3.1.7.4" class="ltx_tr">
<th id="S3.T3.1.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.7.4.1.1" class="ltx_text ltx_font_bold">AYN</span></th>
<td id="S3.T3.1.7.4.2" class="ltx_td ltx_align_center">78.4</td>
<td id="S3.T3.1.7.4.3" class="ltx_td ltx_align_center">36.4</td>
<td id="S3.T3.1.7.4.4" class="ltx_td ltx_align_center">46.3</td>
<td id="S3.T3.1.7.4.5" class="ltx_td ltx_align_center">58.4</td>
<td id="S3.T3.1.7.4.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.7.4.7" class="ltx_td ltx_align_center">78.2</td>
<td id="S3.T3.1.7.4.8" class="ltx_td ltx_align_center">36.3</td>
<td id="S3.T3.1.7.4.9" class="ltx_td ltx_align_center">46.3</td>
<td id="S3.T3.1.7.4.10" class="ltx_td ltx_align_center">58.4</td>
<td id="S3.T3.1.7.4.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T3.1.8.5" class="ltx_tr">
<th id="S3.T3.1.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.8.5.1.1" class="ltx_text ltx_font_bold">SAN</span></th>
<td id="S3.T3.1.8.5.2" class="ltx_td ltx_align_center">79.3</td>
<td id="S3.T3.1.8.5.3" class="ltx_td ltx_align_center">36.6</td>
<td id="S3.T3.1.8.5.4" class="ltx_td ltx_align_center">46.1</td>
<td id="S3.T3.1.8.5.5" class="ltx_td ltx_align_center">58.7</td>
<td id="S3.T3.1.8.5.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.8.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.8.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.8.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.8.5.10" class="ltx_td ltx_align_center">58.9</td>
<td id="S3.T3.1.8.5.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T3.1.9.6" class="ltx_tr">
<th id="S3.T3.1.9.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.9.6.1.1" class="ltx_text ltx_font_bold">ATP</span></th>
<td id="S3.T3.1.9.6.2" class="ltx_td ltx_align_center">80.5</td>
<td id="S3.T3.1.9.6.3" class="ltx_td ltx_align_center">37.5</td>
<td id="S3.T3.1.9.6.4" class="ltx_td ltx_align_center">46.7</td>
<td id="S3.T3.1.9.6.5" class="ltx_td ltx_align_center">59.6</td>
<td id="S3.T3.1.9.6.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.9.6.7" class="ltx_td ltx_align_center">80.3</td>
<td id="S3.T3.1.9.6.8" class="ltx_td ltx_align_center"><span id="S3.T3.1.9.6.8.1" class="ltx_text ltx_font_bold">37.8</span></td>
<td id="S3.T3.1.9.6.9" class="ltx_td ltx_align_center"><span id="S3.T3.1.9.6.9.1" class="ltx_text ltx_font_bold">47.6</span></td>
<td id="S3.T3.1.9.6.10" class="ltx_td ltx_align_center">60.1</td>
<td id="S3.T3.1.9.6.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T3.1.10.7" class="ltx_tr">
<th id="S3.T3.1.10.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.10.7.1.1" class="ltx_text ltx_font_bold">NMN</span></th>
<td id="S3.T3.1.10.7.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.10.7.2.1" class="ltx_text ltx_font_bold">81.2</span></td>
<td id="S3.T3.1.10.7.3" class="ltx_td ltx_align_center">38.0</td>
<td id="S3.T3.1.10.7.4" class="ltx_td ltx_align_center">44.0</td>
<td id="S3.T3.1.10.7.5" class="ltx_td ltx_align_center">58.6</td>
<td id="S3.T3.1.10.7.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.10.7.7" class="ltx_td ltx_align_center"><span id="S3.T3.1.10.7.7.1" class="ltx_text ltx_font_bold">81.2</span></td>
<td id="S3.T3.1.10.7.8" class="ltx_td ltx_align_center">37.7</td>
<td id="S3.T3.1.10.7.9" class="ltx_td ltx_align_center">44.0</td>
<td id="S3.T3.1.10.7.10" class="ltx_td ltx_align_center">58.7</td>
<td id="S3.T3.1.10.7.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T3.1.11.8" class="ltx_tr">
<th id="S3.T3.1.11.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.11.8.1.1" class="ltx_text ltx_font_bold">CoAtt</span></th>
<td id="S3.T3.1.11.8.2" class="ltx_td ltx_align_center">79.7</td>
<td id="S3.T3.1.11.8.3" class="ltx_td ltx_align_center"><span id="S3.T3.1.11.8.3.1" class="ltx_text ltx_font_bold">38.7</span></td>
<td id="S3.T3.1.11.8.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.11.8.4.1" class="ltx_text ltx_font_bold">51.7</span></td>
<td id="S3.T3.1.11.8.5" class="ltx_td ltx_align_center"><span id="S3.T3.1.11.8.5.1" class="ltx_text ltx_font_bold">61.8</span></td>
<td id="S3.T3.1.11.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.1.11.8.6.1" class="ltx_text ltx_font_bold">65.8</span></td>
<td id="S3.T3.1.11.8.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.11.8.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.11.8.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T3.1.11.8.10" class="ltx_td ltx_align_center"><span id="S3.T3.1.11.8.10.1" class="ltx_text ltx_font_bold">62.1</span></td>
<td id="S3.T3.1.11.8.11" class="ltx_td ltx_align_center"><span id="S3.T3.1.11.8.11.1" class="ltx_text ltx_font_bold">66.1</span></td>
</tr>
<tr id="S3.T3.1.12.9" class="ltx_tr">
<th id="S3.T3.1.12.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T3.1.12.9.1.1" class="ltx_text ltx_font_bold">AMA</span></th>
<td id="S3.T3.1.12.9.2" class="ltx_td ltx_align_center">81.01</td>
<td id="S3.T3.1.12.9.3" class="ltx_td ltx_align_center">38.42</td>
<td id="S3.T3.1.12.9.4" class="ltx_td ltx_align_center">45.23</td>
<td id="S3.T3.1.12.9.5" class="ltx_td ltx_align_center">59.17</td>
<td id="S3.T3.1.12.9.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T3.1.12.9.7" class="ltx_td ltx_align_center">81.07</td>
<td id="S3.T3.1.12.9.8" class="ltx_td ltx_align_center">37.12</td>
<td id="S3.T3.1.12.9.9" class="ltx_td ltx_align_center">45.83</td>
<td id="S3.T3.1.12.9.10" class="ltx_td ltx_align_center">59.44</td>
<td id="S3.T3.1.12.9.11" class="ltx_td ltx_align_center">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of various models on VQA dataset</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Stacked Attention Networks (SAN) <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">This model is similar in spirit to the previous model in that it repeatedly computes attention over the image to get finer-grained visual information to predict the answer. However, while the previous model does this word by word, this model first encodes the entire question using either an LSTM or a CNN. This question encoding is used to attend over the image using a similar equation as before. Then the attention weighted image is concatenated with the question encoding and used to again compute attention over the original image. This can be repeated <math id="S3.SS3.SSS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><mi id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><ci id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">k</annotation></semantics></math> times after which the question and the final image representation are used to predict the answer. The authors argue that this sort of ‘stacked’ attention helps the model to iteratively discard unimportant regions of the image. The authors experiment with k=1 and k=2 and report results on DAQUAR, COCO-QA and VQA datasets.</p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Hierarchical Co-attention (CoAtt) <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.5" class="ltx_p">This paper differs from the previous attention based methods in that in addition to modelling the visual attention, it also models question attention, that is, which part of the question to give importance to. They model two forms of co-attention: 1) Parallel co-attention, in which image and question attend over each other simultaneously. This is done by computing an affinity matrix <math id="S3.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="C=tanh(Q^{T}WI)" display="inline"><semantics id="S3.SS3.SSS4.p1.1.m1.1a"><mrow id="S3.SS3.SSS4.p1.1.m1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS4.p1.1.m1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.cmml">C</mi><mo id="S3.SS3.SSS4.p1.1.m1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS3.SSS4.p1.1.m1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.cmml"><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.4" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.2a" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.5" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.2b" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.6" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.2c" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml"><msup id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.2.cmml">Q</mi><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1a" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.4" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.4.cmml">I</mi></mrow><mo stretchy="false" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.1.m1.1b"><apply id="S3.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1"><eq id="S3.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.2"></eq><ci id="S3.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.3">𝐶</ci><apply id="S3.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1"><times id="S3.SS3.SSS4.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.2"></times><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.3">𝑡</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.4.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.4">𝑎</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.5.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.5">𝑛</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.6.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.6">ℎ</ci><apply id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1"><times id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.1"></times><apply id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.2">𝑄</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.3">𝑊</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.4.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.4">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.1.m1.1c">C=tanh(Q^{T}WI)</annotation></semantics></math> where <math id="S3.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS3.SSS4.p1.2.m2.1a"><mi id="S3.SS3.SSS4.p1.2.m2.1.1" xref="S3.SS3.SSS4.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.2.m2.1b"><ci id="S3.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.2.m2.1c">W</annotation></semantics></math> is a learnable weight matrix. <math id="S3.SS3.SSS4.p1.3.m3.1" class="ltx_Math" alttext="C_{ij}" display="inline"><semantics id="S3.SS3.SSS4.p1.3.m3.1a"><msub id="S3.SS3.SSS4.p1.3.m3.1.1" xref="S3.SS3.SSS4.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS4.p1.3.m3.1.1.2" xref="S3.SS3.SSS4.p1.3.m3.1.1.2.cmml">C</mi><mrow id="S3.SS3.SSS4.p1.3.m3.1.1.3" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.SSS4.p1.3.m3.1.1.3.2" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.3.m3.1.1.3.1" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS4.p1.3.m3.1.1.3.3" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.3.m3.1b"><apply id="S3.SS3.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1.2">𝐶</ci><apply id="S3.SS3.SSS4.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1.3"><times id="S3.SS3.SSS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.1"></times><ci id="S3.SS3.SSS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS3.SSS4.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.3.m3.1c">C_{ij}</annotation></semantics></math> represents the affinity of the <math id="S3.SS3.SSS4.p1.4.m4.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS3.SSS4.p1.4.m4.1a"><msup id="S3.SS3.SSS4.p1.4.m4.1.1" xref="S3.SS3.SSS4.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS4.p1.4.m4.1.1.2" xref="S3.SS3.SSS4.p1.4.m4.1.1.2.cmml">i</mi><mrow id="S3.SS3.SSS4.p1.4.m4.1.1.3" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS3.SSS4.p1.4.m4.1.1.3.2" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.4.m4.1.1.3.1" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS4.p1.4.m4.1.1.3.3" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.4.m4.1b"><apply id="S3.SS3.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.SSS4.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.2">𝑖</ci><apply id="S3.SS3.SSS4.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.3"><times id="S3.SS3.SSS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.1"></times><ci id="S3.SS3.SSS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.2">𝑡</ci><ci id="S3.SS3.SSS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.4.m4.1c">i^{th}</annotation></semantics></math> word in the question and <math id="S3.SS3.SSS4.p1.5.m5.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S3.SS3.SSS4.p1.5.m5.1a"><msup id="S3.SS3.SSS4.p1.5.m5.1.1" xref="S3.SS3.SSS4.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS4.p1.5.m5.1.1.2" xref="S3.SS3.SSS4.p1.5.m5.1.1.2.cmml">j</mi><mrow id="S3.SS3.SSS4.p1.5.m5.1.1.3" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.cmml"><mi id="S3.SS3.SSS4.p1.5.m5.1.1.3.2" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.5.m5.1.1.3.1" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS4.p1.5.m5.1.1.3.3" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.5.m5.1b"><apply id="S3.SS3.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.SSS4.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.2">𝑗</ci><apply id="S3.SS3.SSS4.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3"><times id="S3.SS3.SSS4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.1"></times><ci id="S3.SS3.SSS4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.2">𝑡</ci><ci id="S3.SS3.SSS4.p1.5.m5.1.1.3.3.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.5.m5.1c">j^{th}</annotation></semantics></math> region in the image. This matrix C is used to obtained both image and question attention vectors. 2) Alternating co-attention. In this we iteratively attend on image followed by query followed by image again (similar to SANs in spirit).</p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p id="S3.SS3.SSS4.p2.1" class="ltx_p">One additional idea that the authors use is encode the question at different levels of abstraction: word, phrase and question level. Question level representation is obtained by LSTM while word and phrase level representation are obtained from CNNs. They present results on VQA and COCO-QA datasets</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Other Models</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The following models use more ideas than simply changing how to attend to the image or question and as such do not fit in the previous sections.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Neural Module Networks (NMNs) <cite class="ltx_cite ltx_citemacro_cite">Andreas et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">This model involves generating a neural network on the fly for each individual image and question. This is done through choosing from various sub-modules based on the question and composing these to generate the neural network. Modules are of five kinds: Attention[c] (which computes an attention map for a given image and given c; c can be ‘dog’ for instance, then Attention[dog] will try to find a dog), classification[c] (which outputs a distribution over labels belonging to c for a given image and attention map; c can be ‘color’), reattention[c] (which takes an attention map and recomputes it based on c; c can be ‘above’ which means shift attention upward), Measurement[c] (which outputs a distribution over labels based on attention map alone) and combination[c] (which merges two attention maps as specified by c; c could be ‘and’ or ‘or’).</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">To decide which modules to compose together, they first parse the question using a dependency parser and use this dependency to create a symbolic expression based on the head word. An example from the paper is ‘What is standing on the field?’ becomes what(stand). These symbolic forms are then used to identify which modules to use. The whole system is then trained end to end through backpropagation. The authors test their model on the VQA dataset and also a more challenging synthetic dataset as they found that the VQA dataset did not require too much high level reasoning or composition.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Incorporating Knowledge Bases</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib28" title="" class="ltx_ref">2016b</a>)</cite> present the Ask Me Anything (AMA) model, that tries to leverage information from an external knowledge base to help guide visual question answering. It first obtains a set of attributes like object names, properties etc. of the images based on caption of the image. Image captioning model is trained on using standard image captioning techniques on the MS-COCO dataset. There are 256 possible attributes and the attribute generator is trained on MS-COCO using a variation of the VGG net. The top five attributes are used to generate queries for the DBpedia database <cite class="ltx_cite ltx_citemacro_cite">Auer et al. (<a href="#bib.bib3" title="" class="ltx_ref">2007</a>)</cite>. Each query returns a text which is summarized using Doc2Vec <cite class="ltx_cite ltx_citemacro_cite">Le and Mikolov (<a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite>. This summary is passed as an additional input to the decoder LSTM which generates the answer. The authors show results on VQA and COCO-QA datasets.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion and Future Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As has been the trend in recent years, deep learning models outperform earlier graphical model based approaches across all VQA datasets. However, it is interesting to note that the Answer Type Prediction (ATP) model performs better than the non-attention models, which proves that simply introducing convolutional and/or recurrent neural networks is not enough: identifying parts of the image that are relevant in a principled manner is important. ATP is even competitive with or better than some attention models like Where to Look (WTL) and Stacked Attention Networks (SAN).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Significant improvement is shown by Hierarchical Co-Attention Networks (CoAtt), which was the first to attend on the question in addition to the image. This may be helpful especially for longer questions, which are harder to encode into a single vector representation by LSTMs/GRUs, so first encoding each word and then using the image to attend to important words helps the model perform better. The Neural Module Networks (NMN) uses the novel and interesting idea of automatically composing sub-modules for each image/question pair which performs similar to CoAtt on the VQA dataset, but outperforms all models on a synthetic dataset requiring more high level reasoning, indicating that this could be a valuable approach in the real world. However, more investigation is required to judge the performance of this model.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The best performing model on COCO-QA is Ask Me Anything (AMA) which incorporates information from an external knowledge base (DBpedia). A possible reason for improved performance is that the knowledge base helps answer questions that involve world or common sense knowledge that may not be present in the dataset. The performance of this model is not as good on VQA dataset, which might be because not too many questions in this dataset require world knowledge. This model naturally gives rise to two avenues for future work. The first would be recognizing when external knowledge is needed: some sort of model hybrid of CoAtt and AMA along with a decision maker for whether to access the KB might provide the best of both worlds. The decision might even be a soft one to enable end to end training. The second direction would be exploring the use of other knowledge bases like Freebase <cite class="ltx_cite ltx_citemacro_cite">Bollacker et al. (<a href="#bib.bib5" title="" class="ltx_ref">2008</a>)</cite>, NELL <cite class="ltx_cite ltx_citemacro_cite">Carlson et al. (<a href="#bib.bib6" title="" class="ltx_ref">2010</a>)</cite> or OpenIE extractions <cite class="ltx_cite ltx_citemacro_cite">Schmitz et al. (<a href="#bib.bib22" title="" class="ltx_ref">2012</a>)</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">As we have seen, novel ways of computing attention continue to improve performance on this task. This has been seen in the textual question answering task as well <cite class="ltx_cite ltx_citemacro_cite">Xiong et al. (<a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Seo et al. (<a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>, so more recent models from that space can be used to guide VQA models. A study providing an estimated upper bound on performance for the various VQA datasets would be very valuable as well to get an idea for the scope of possible improvement, especially for COCO-QA which is automatically generated. Finally, most VQA tasks treat answering as a classification task. Only the VQA dataset allows for answer generation in a limited manner. It would be interesting to explore answering as a generation task more deeply, but dataset collection and effective evaluation methodologies for this remain an open question.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The field of VQA has grown by leaps and bounds despite being introduced just a few years ago. Deep learning methods for VQA continue to be the models receiving the most attention and showing state-of-the-art results. We surveyed the most prominent of these models and listed their performance over various large-scale datasets. Significant improvements in performance continue to be seen on many datasets, which means there is still plenty of room for future innovation in this task.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock">Deep compositional question answering with neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al. (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard
Cyganiak, and Zachary Ives. 2007.

</span>
<span class="ltx_bibblock">Dbpedia: A nucleus for a web of open data.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">The semantic web</span> pages 722–735.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.0473</span> .

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bollacker et al. (2008)</span>
<span class="ltx_bibblock">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring
human knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2008 ACM SIGMOD international conference
on Management of data</span>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlson et al. (2010)</span>
<span class="ltx_bibblock">
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam
R. Hruschka Jr., and Tom M. Mitchell. 2010.

</span>
<span class="ltx_bibblock">Toward an architecture for never-ending language learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">AAAI</span>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span> .

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2015)</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Neural computation</span> 9(8):1735–1780.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2016)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2016.

</span>
<span class="ltx_bibblock">Answer-type prediction for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiros et al. (2015)</span>
<span class="ltx_bibblock">
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015.

</span>
<span class="ltx_bibblock">Skip-thought vectors.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Mikolov (2014)</span>
<span class="ltx_bibblock">
Quoc V Le and Tomas Mikolov. 2014.

</span>
<span class="ltx_bibblock">Distributed representations of sentences and documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ICML</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2015)</span>
<span class="ltx_bibblock">
Lin Ma, Zhengdong Lu, and Hang Li. 2015.

</span>
<span class="ltx_bibblock">Learning to answer questions from image using convolutional neural
network.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.00333</span> .

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. (2016)</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2016.

</span>
<span class="ltx_bibblock">Ask your neurons: A deep learning approach to visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1605.02697</span> .

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noh et al. (2016)</span>
<span class="ltx_bibblock">
Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2016.

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with
dynamic parameter prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmitz et al. (2012)</span>
<span class="ltx_bibblock">
Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012.

</span>
<span class="ltx_bibblock">Open language learning for information extraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo et al. (2016)</span>
<span class="ltx_bibblock">
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016.

</span>
<span class="ltx_bibblock">Bidirectional attention flow for machine comprehension.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01603</span> .

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih et al. (2016)</span>
<span class="ltx_bibblock">
Kevin J Shih, Saurabh Singh, and Derek Hoiem. 2016.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span> .

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2015)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016a)</span>
<span class="ltx_bibblock">
Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den
Hengel. 2016a.

</span>
<span class="ltx_bibblock">Visual question answering: A survey of methods and datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.05910</span> .

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016b)</span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel.
2016b.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2016)</span>
<span class="ltx_bibblock">
Caiming Xiong, Victor Zhong, and Richard Socher. 2016.

</span>
<span class="ltx_bibblock">Dynamic coattention networks for question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01604</span> .

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2015)</span>
<span class="ltx_bibblock">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan
Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">ICML</span>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2015)</span>
<span class="ltx_bibblock">
Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. 2015.

</span>
<span class="ltx_bibblock">Visual madlibs: Fill in the blank description generation and question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2015)</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.
2015.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span> .

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1705.03864" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1705.03865" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1705.03865">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1705.03865" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1705.03866" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar 15 22:19:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
