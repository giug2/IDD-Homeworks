<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.04689] A Privacy Preserving System for Movie Recommendations Using Federated Learning</title><meta property="og:description" content="Recommender systems have become ubiquitous in the past years. They solve the tyranny of choice problem faced by many users, and are utilized by many online businesses to drive engagement and sales. Besides other critic…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Privacy Preserving System for Movie Recommendations Using Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Privacy Preserving System for Movie Recommendations Using Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.04689">

<!--Generated on Thu Feb 29 21:14:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="federated learning,  distributed learning,  federated recommender systems,  neural network compression">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Privacy Preserving System for Movie Recommendations Using Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Neumann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1907-8329" title="ORCID identifier" class="ltx_ref">0000-0003-1907-8329</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id46.1.id1" class="ltx_text ltx_affiliation_position">Scientific Researcher</span><span id="id47.2.id2" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, HHI</span><span id="id48.3.id3" class="ltx_text ltx_affiliation_department">[</span>0]Department of Artificial Intelligence
<span id="id49.4.id4" class="ltx_text ltx_affiliation_department">[</span>1]Efficient Deep Learning Group
<span id="id50.5.id5" class="ltx_text ltx_affiliation_streetaddress">Einsteinufer 37</span><span id="id51.6.id6" class="ltx_text ltx_affiliation_postcode">10587</span><span id="id52.7.id7" class="ltx_text ltx_affiliation_city">Berlin</span><span id="id53.8.id8" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:david.neumann@hhi.fraunhofer.de">david.neumann@hhi.fraunhofer.de</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Lutz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2973-0096" title="ORCID identifier" class="ltx_ref">0000-0002-2973-0096</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id54.1.id1" class="ltx_text ltx_affiliation_position">Student Research Assistant</span><span id="id55.2.id2" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, HHI</span><span id="id56.3.id3" class="ltx_text ltx_affiliation_department">[</span>0]Department of Artificial Intelligence
<span id="id57.4.id4" class="ltx_text ltx_affiliation_department">[</span>1]Efficient Deep Learning Group
<span id="id58.5.id5" class="ltx_text ltx_affiliation_streetaddress">Einsteinufer 37</span><span id="id59.6.id6" class="ltx_text ltx_affiliation_postcode">10587</span><span id="id60.7.id7" class="ltx_text ltx_affiliation_city">Berlin</span><span id="id61.8.id8" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:andreas.lutz@hhi.fraunhofer.de">andreas.lutz@hhi.fraunhofer.de</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karsten Müller
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-8611-7864" title="ORCID identifier" class="ltx_ref">0000-0001-8611-7864</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id62.1.id1" class="ltx_text ltx_affiliation_position">Head of Efficient Deep Learning Group</span><span id="id63.2.id2" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, HHI</span><span id="id64.3.id3" class="ltx_text ltx_affiliation_department">[</span>0]Department of Artificial Intelligence
<span id="id65.4.id4" class="ltx_text ltx_affiliation_department">[</span>1]Efficient Deep Learning Group
<span id="id66.5.id5" class="ltx_text ltx_affiliation_streetaddress">Einsteinufer 37</span><span id="id67.6.id6" class="ltx_text ltx_affiliation_postcode">10587</span><span id="id68.7.id7" class="ltx_text ltx_affiliation_city">Berlin</span><span id="id69.8.id8" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:karsten.mueller@hhi.fraunhofer.de">karsten.mueller@hhi.fraunhofer.de</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wojciech Samek
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-6283-3265" title="ORCID identifier" class="ltx_ref">0000-0002-6283-3265</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id70.1.id1" class="ltx_text ltx_affiliation_position">Head of Department of Artificial Intelligence and Head of Explainable AI Group</span><span id="id71.2.id2" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, HHI</span><span id="id72.3.id3" class="ltx_text ltx_affiliation_department">[</span>0]Department of Artificial Intelligence
<span id="id73.4.id4" class="ltx_text ltx_affiliation_department">[</span>1]Explainable AI Group
<span id="id74.5.id5" class="ltx_text ltx_affiliation_streetaddress">Einsteinufer 37</span><span id="id75.6.id6" class="ltx_text ltx_affiliation_postcode">10587</span><span id="id76.7.id7" class="ltx_text ltx_affiliation_city">Berlin</span><span id="id77.8.id8" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id78.9.id1" class="ltx_text ltx_affiliation_position">Professor</span><span id="id79.10.id2" class="ltx_text ltx_affiliation_institution">Technical University of Berlin</span><span id="id80.11.id3" class="ltx_text ltx_affiliation_department">Department of Electrical Engineering and Computer Science</span><span id="id81.12.id4" class="ltx_text ltx_affiliation_streetaddress">Marchstraße 23</span><span id="id82.13.id5" class="ltx_text ltx_affiliation_postcode">10587</span><span id="id83.14.id6" class="ltx_text ltx_affiliation_city">Berlin</span><span id="id84.15.id7" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wojciech.samek@hhi.fraunhofer.de">wojciech.samek@hhi.fraunhofer.de</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id85.id1" class="ltx_p">Recommender systems have become ubiquitous in the past years. They solve the <em id="id85.id1.1" class="ltx_emph ltx_font_italic">tyranny of choice</em> problem faced by many users, and are utilized by many online businesses to drive engagement and sales. Besides other criticisms, like creating filter bubbles within social networks, recommender systems are often reproved for collecting considerable amounts of personal data. However, to personalize recommendations, personal information is fundamentally required. A recent distributed learning scheme called federated learning has made it possible to learn from personal user data without its central collection. Consequently, we present a recommender system for movie recommendations, which provides privacy and thus trustworthiness on multiple levels: First and foremost, it is trained using federated learning and thus, by its very nature, privacy-preserving, while still enabling users to benefit from global insights. Furthermore, a novel federated learning scheme, called FedQ, is employed, which not only addresses the problem of non-i.i.d.-ness and small local datasets, but also prevents input data reconstruction attacks by aggregating client updates early. Finally, to reduce the communication overhead, compression is applied, which significantly compresses the exchanged neural network parametrizations to a fraction of their original size. We conjecture that this may also improve data privacy through its lossy quantization stage.</p>
</div>
<div class="ltx_keywords">federated learning, distributed learning, federated recommender systems, neural network compression
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TORS</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>1</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>1</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>1</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>1</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3634686</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Distributed artificial intelligence</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Security and privacy Privacy-preserving protocols</span></span></span><span id="id12" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning approaches</span></span></span><span id="id13" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Security and privacy Privacy protections</span></span></span><span id="id14" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization Client-server architectures</span></span></span>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was created as part of the COPA EUROPE project (COllaborative Platform for trAnsmedia storytelling and cross channel distribution of EUROPEan sport events), which has received funding from the European Union’s Sponsor Horizon 2020 Research and Innovation Programme <a target="_blank" href="https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en</a> under Grant Agreement No. Grant #957059.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Due to the ever-increasing sizes of corpora of items such as movies, articles, games, non-digital goods, etc., the task of finding novel and engaging content or products for each individual user or customer becomes increasingly difficult, even with the help of search engines. This problem is known as the <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">tyranny of choice</em> <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a href="#bib.bib100" title="" class="ltx_ref">2004</a>)</cite>. Therefore, well-engineered <a href="#id38.38.id38"><span href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">recommender systems</span></span></a> are one of the most important pieces of technology for the success of many digital enterprises, providing them with the required engagement and sales. Harvard Business Review even calls <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> the single most important algorithmic distinction between “born digital” enterprises and legacy companies <cite class="ltx_cite ltx_citemacro_citep">(Schrage, <a href="#bib.bib99" title="" class="ltx_ref">2017</a>)</cite>. 80% of the content people watch on Netflix sources from a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, and they estimate that recommendations and personalization save them 1 billion USD per year <cite class="ltx_cite ltx_citemacro_citep">(Gomez-Uribe and Hunt, <a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>. 35% of what customers purchase at Amazon comes from a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(MacKenzie et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2013</a>)</cite>. At Airbnb, search ranking and similar listing recommendations drive 99% of all booking conversions <cite class="ltx_cite ltx_citemacro_citep">(Grbovic and Cheng, <a href="#bib.bib35" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Accordingly, a growing number of online businesses are adopting <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> to expand customer engagement and sales. This causes a worrying trend of companies gathering and storing continuously increasing amounts of personal customer data. Even with data protection legislation like the <a href="#id11.11.id11"><span href="#id11.11.id11" title="European Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">European Union’s</span></span></a>  <a href="#id22.22.id22"><span href="#id22.22.id22" title="General Data Protection Regulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">General Data Protection Regulation</span></span></a> (<a href="#id22.22.id22"><abbr href="#id22.22.id22" title="General Data Protection Regulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GDPR</span></abbr></a>) <cite class="ltx_cite ltx_citemacro_citep">(European Parliament, <a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite> it is opaque to users what data is collected and arduous to take agency over one’s personal data. All this gathered and derived personal information is at risk of being misused or leaked.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">On one hand, in order to improve the personalization of customer recommendations, personal information is indispensable. On the other hand, the principles of data economy and data avoidance are essential to preserve user’s privacy, and provide them with control over their own personal data. Recently,  <a href="#id20.20.id20"><span href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated learning</span></span></a> (<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>) was introduced as a distributed  <a href="#id29.29.id29"><span href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">machine learning</span></span></a> (<a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a>) method, which avoids the centralized accumulation of user data entirely and thus provides data privacy. Unlike regular <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> training algorithms with centrally collected data, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> is designed to leave the data at its origin and instead train many models or variants of one model on each of these local datasets. The clients only share the training updates, which are then aggregated into an updated global model. As a result, all participating clients benefit from distributively training the model on all data, without ever sharing the data itself. Accordingly, this scheme, first introduced by <cite class="ltx_cite ltx_citemacro_citet">Konečný et al<span class="ltx_text">.</span> (<a href="#bib.bib59" title="" class="ltx_ref">2016</a>)</cite>, is aimed towards scenarios in which the local data is privacy sensitive and thus owners do not want to disclose it.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While classical <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> approaches usually only require user interaction data as input signals, modern approaches can use more privacy-sensitive input signals, such as age, gender, country of origin, and device information. This has the potential to further improve the predictive power of <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>. The privacy-preserving nature of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> makes it a perfect fit for training <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> models without users having to give up their personal data. Furthermore, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> helps to distribute the burden of data storage and the computational overhead of training among many clients. On the contrary, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> also has the following disadvantages: (1) <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">training time</em> will be increased as compared to traditional central training, because client devices are less capable and not always available, (2) <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">non- <a href="#id26.26.id26"><span href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">independent and identically distributed</span></span></a> (<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>) data</em> can hinder convergence and result in a model with lower performance than its centrally trained counterpart, (3) <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">battery usage</em> of mobile client devices will increase due to the complex computations required to train the model, resulting in shorter battery lives, and (4) the <em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">communication overhead</em> of continuously exchanging training updates between the clients and the central server, which is especially problematic when clients are on a metered mobile connection.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The exploration of the combination of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> towards the subfield of <a href="#id16.16.id16"><span href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">federated recommender systems</span></span></a> has only recently started and has not yet been fully explored in the literature with only a few publications available on this topic. Therefore, this work introduces an end-to-end, high-performance, scalable <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> solution for movie recommendations, which is entirely driven by <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and addresses common issues of <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a>. System scalability is verified through experiments conducted on more than 162,000 <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients. To our best knowledge, this is the first work with this client range.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The proposed system inherently provides privacy and thus trustworthiness on several levels: First, through the federated training that only transmits  <a href="#id34.34.id34"><span href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">neural network</span></span></a> (<a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a>) parameters, while every participating client’s personal data remains private. Second, early aggregation of client updates prevents input data reconstruction attacks. And third, we apply lossy  <a href="#id35.35.id35"><span href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">neural network coding</span></span></a> (<a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a>) compression methods that not only provide significant communication reduction, but we also conjecture that its quantization acts as a parameter obfuscation and thus may also strengthen the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setup against input data reconstruction attacks.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">A common challenge among <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> is that most users only produce extraordinarily little training data, while a tiny fraction of highly engaged users produce a lot of training data. In classical <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> this is primarily an issue because these few users dominate the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> and suppress the interests of less frequent users. In <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a> this poses the additional problem of small noisy updates, which can hinder global model convergence. To counteract this problem, we introduce a technique to chain client trainings together in a privacy-preserving manner, in order to produce more stable model updates. To summarize, our contributions are as follows:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A privacy-preserving movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> trained end-to-end using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Extreme scalability with experimental evidence for more than 162,000 clients</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Compressed communication between central server and clients with state-of-the-art <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a></p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Novel queue-based federated training to address non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> and imbalanced local datasets</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Recommender Systems</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Initially <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> for collaborative filtering tasks were often modeled using matrix factorization techniques. The general idea is to embed input signals, like users and items, in a joint latent space, and quantify the similarity between them using an interaction function, which is the dot product in the simplest case <cite class="ltx_cite ltx_citemacro_citep">(Koren et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2009</a>)</cite>. Several approaches in the literature were introduced to enhance the predictive power of the model, e.g., incorporating additional features <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2011</a>)</cite> or combining it with neighborhood models <cite class="ltx_cite ltx_citemacro_citep">(Koren, <a href="#bib.bib60" title="" class="ltx_ref">2008</a>)</cite>. Since matrix factorization relies on linear dependencies between the input signals, substituting any arbitrary function for the inner product led to promising results. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib42" title="" class="ltx_ref">He et al<span class="ltx_text">.</span></a></cite> utilized a  <a href="#id9.9.id9"><span href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">deep neural network</span></span></a> (<a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a>) for this task, which proved to be better suited for capturing the latent structures in the data, resulting in higher prediction accuracy <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>; Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. Several architectures in the context of  <a href="#id8.8.id8"><span href="#id8.8.id8" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">deep learning</span></span></a> (<a href="#id8.8.id8"><abbr href="#id8.8.id8" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DL</span></abbr></a>) were proposed to further improve the baseline models for <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib17" title="" class="ltx_ref">Choe et al<span class="ltx_text">.</span></a></cite> utilized a  <a href="#id40.40.id40"><span href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">recurrent neural network</span></span></a> (<a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RNN</span></abbr></a>) to include time series data from the previous items the user has interacted with <cite class="ltx_cite ltx_citemacro_citep">(Choe et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. To address the limits of <a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> for sequential recommendations <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib111" title="" class="ltx_ref">Tang and Wang</a></cite> proposed a  <a href="#id6.6.id6"><span href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">convolutional neural network</span></span></a> (<a href="#id6.6.id6"><abbr href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a>) to incorporate the fact that dependency relations were not necessarily the consequence of consecutive user-item interactions <cite class="ltx_cite ltx_citemacro_citep">(Tang and Wang, <a href="#bib.bib111" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib101" title="" class="ltx_ref">Sedhain et al<span class="ltx_text">.</span></a></cite> used an item-based autoencoder to reconstruct ratings received as an input <cite class="ltx_cite ltx_citemacro_citep">(Sedhain et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2015</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib124" title="" class="ltx_ref">Wu et al<span class="ltx_text">.</span></a></cite> enhanced this approach by employing a denoising autoencoder, which can handle corrupted data <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2016</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib128" title="" class="ltx_ref">Ying et al<span class="ltx_text">.</span></a></cite> used a  <a href="#id21.21.id21"><span href="#id21.21.id21" title="graph convolutional network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">graph convolutional network</span></span></a> (<a href="#id21.21.id21"><abbr href="#id21.21.id21" title="graph convolutional network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GCN</span></abbr></a>) to combine graph convolutions and efficient random walks to solve scalability issues faced in web-scale recommendation tasks <cite class="ltx_cite ltx_citemacro_citep">(Ying et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<span id="S2.SS2.p1.13" class="ltx_ERROR undefined">\Ac</span>
<p id="S2.SS2.p1.12" class="ltx_p">fl is a recently proposed distributed learning scheme, which was originally proposed by <cite class="ltx_cite ltx_citemacro_citet">Konečný et al<span class="ltx_text">.</span> (<a href="#bib.bib59" title="" class="ltx_ref">2016</a>)</cite>, where a set of client devices <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">C</annotation></semantics></math>, jointly train an <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> model <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="M_{Global}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">M</mi><mrow id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS2.p1.2.m2.1.1.3.2" xref="S2.SS2.p1.2.m2.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.3" xref="S2.SS2.p1.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1a" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.4" xref="S2.SS2.p1.2.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1b" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.5" xref="S2.SS2.p1.2.m2.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1c" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.6" xref="S2.SS2.p1.2.m2.1.1.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1d" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.7" xref="S2.SS2.p1.2.m2.1.1.3.7.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">𝑀</ci><apply id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3"><times id="S2.SS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3.1"></times><ci id="S2.SS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2">𝐺</ci><ci id="S2.SS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3.3">𝑙</ci><ci id="S2.SS2.p1.2.m2.1.1.3.4.cmml" xref="S2.SS2.p1.2.m2.1.1.3.4">𝑜</ci><ci id="S2.SS2.p1.2.m2.1.1.3.5.cmml" xref="S2.SS2.p1.2.m2.1.1.3.5">𝑏</ci><ci id="S2.SS2.p1.2.m2.1.1.3.6.cmml" xref="S2.SS2.p1.2.m2.1.1.3.6">𝑎</ci><ci id="S2.SS2.p1.2.m2.1.1.3.7.cmml" xref="S2.SS2.p1.2.m2.1.1.3.7">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">M_{Global}</annotation></semantics></math> on their private datasets <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">𝒟</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝒟</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\mathcal{D}_{i}</annotation></semantics></math>. Usually, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> is performed under the supervision of a central coordinating server. In traditional <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> the local client datasets would be accumulated into a central dataset <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{Central}=\bigcup_{i=1}^{\left|C\right|}\mathcal{D}_{i}" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mrow id="S2.SS2.p1.4.m4.1.2" xref="S2.SS2.p1.4.m4.1.2.cmml"><msub id="S2.SS2.p1.4.m4.1.2.2" xref="S2.SS2.p1.4.m4.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.4.m4.1.2.2.2" xref="S2.SS2.p1.4.m4.1.2.2.2.cmml">𝒟</mi><mrow id="S2.SS2.p1.4.m4.1.2.2.3" xref="S2.SS2.p1.4.m4.1.2.2.3.cmml"><mi id="S2.SS2.p1.4.m4.1.2.2.3.2" xref="S2.SS2.p1.4.m4.1.2.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.3" xref="S2.SS2.p1.4.m4.1.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1a" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.4" xref="S2.SS2.p1.4.m4.1.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1b" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.5" xref="S2.SS2.p1.4.m4.1.2.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1c" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.6" xref="S2.SS2.p1.4.m4.1.2.2.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1d" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.7" xref="S2.SS2.p1.4.m4.1.2.2.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m4.1.2.2.3.1e" xref="S2.SS2.p1.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S2.SS2.p1.4.m4.1.2.2.3.8" xref="S2.SS2.p1.4.m4.1.2.2.3.8.cmml">l</mi></mrow></msub><mo rspace="0.111em" id="S2.SS2.p1.4.m4.1.2.1" xref="S2.SS2.p1.4.m4.1.2.1.cmml">=</mo><mrow id="S2.SS2.p1.4.m4.1.2.3" xref="S2.SS2.p1.4.m4.1.2.3.cmml"><msubsup id="S2.SS2.p1.4.m4.1.2.3.1" xref="S2.SS2.p1.4.m4.1.2.3.1.cmml"><mo id="S2.SS2.p1.4.m4.1.2.3.1.2.2" xref="S2.SS2.p1.4.m4.1.2.3.1.2.2.cmml">⋃</mo><mrow id="S2.SS2.p1.4.m4.1.2.3.1.2.3" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.cmml"><mi id="S2.SS2.p1.4.m4.1.2.3.1.2.3.2" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.2.cmml">i</mi><mo id="S2.SS2.p1.4.m4.1.2.3.1.2.3.1" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.1.cmml">=</mo><mn id="S2.SS2.p1.4.m4.1.2.3.1.2.3.3" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.3.cmml">1</mn></mrow><mrow id="S2.SS2.p1.4.m4.1.1.1.3" xref="S2.SS2.p1.4.m4.1.1.1.2.cmml"><mo id="S2.SS2.p1.4.m4.1.1.1.3.1" xref="S2.SS2.p1.4.m4.1.1.1.2.1.cmml">|</mo><mi id="S2.SS2.p1.4.m4.1.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.1.cmml">C</mi><mo id="S2.SS2.p1.4.m4.1.1.1.3.2" xref="S2.SS2.p1.4.m4.1.1.1.2.1.cmml">|</mo></mrow></msubsup><msub id="S2.SS2.p1.4.m4.1.2.3.2" xref="S2.SS2.p1.4.m4.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.4.m4.1.2.3.2.2" xref="S2.SS2.p1.4.m4.1.2.3.2.2.cmml">𝒟</mi><mi id="S2.SS2.p1.4.m4.1.2.3.2.3" xref="S2.SS2.p1.4.m4.1.2.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.2.cmml" xref="S2.SS2.p1.4.m4.1.2"><eq id="S2.SS2.p1.4.m4.1.2.1.cmml" xref="S2.SS2.p1.4.m4.1.2.1"></eq><apply id="S2.SS2.p1.4.m4.1.2.2.cmml" xref="S2.SS2.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.2.2.1.cmml" xref="S2.SS2.p1.4.m4.1.2.2">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.2.2.2.cmml" xref="S2.SS2.p1.4.m4.1.2.2.2">𝒟</ci><apply id="S2.SS2.p1.4.m4.1.2.2.3.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3"><times id="S2.SS2.p1.4.m4.1.2.2.3.1.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.1"></times><ci id="S2.SS2.p1.4.m4.1.2.2.3.2.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.2">𝐶</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.3.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.3">𝑒</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.4.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.4">𝑛</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.5.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.5">𝑡</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.6.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.6">𝑟</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.7.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.7">𝑎</ci><ci id="S2.SS2.p1.4.m4.1.2.2.3.8.cmml" xref="S2.SS2.p1.4.m4.1.2.2.3.8">𝑙</ci></apply></apply><apply id="S2.SS2.p1.4.m4.1.2.3.cmml" xref="S2.SS2.p1.4.m4.1.2.3"><apply id="S2.SS2.p1.4.m4.1.2.3.1.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.2.3.1.1.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1">superscript</csymbol><apply id="S2.SS2.p1.4.m4.1.2.3.1.2.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.2.3.1.2.1.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1">subscript</csymbol><union id="S2.SS2.p1.4.m4.1.2.3.1.2.2.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1.2.2"></union><apply id="S2.SS2.p1.4.m4.1.2.3.1.2.3.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3"><eq id="S2.SS2.p1.4.m4.1.2.3.1.2.3.1.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.1"></eq><ci id="S2.SS2.p1.4.m4.1.2.3.1.2.3.2.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S2.SS2.p1.4.m4.1.2.3.1.2.3.3.cmml" xref="S2.SS2.p1.4.m4.1.2.3.1.2.3.3">1</cn></apply></apply><apply id="S2.SS2.p1.4.m4.1.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.1.3"><abs id="S2.SS2.p1.4.m4.1.1.1.2.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1.3.1"></abs><ci id="S2.SS2.p1.4.m4.1.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1.1">𝐶</ci></apply></apply><apply id="S2.SS2.p1.4.m4.1.2.3.2.cmml" xref="S2.SS2.p1.4.m4.1.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.2.3.2.1.cmml" xref="S2.SS2.p1.4.m4.1.2.3.2">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.2.3.2.2.cmml" xref="S2.SS2.p1.4.m4.1.2.3.2.2">𝒟</ci><ci id="S2.SS2.p1.4.m4.1.2.3.2.3.cmml" xref="S2.SS2.p1.4.m4.1.2.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\mathcal{D}_{Central}=\bigcup_{i=1}^{\left|C\right|}\mathcal{D}_{i}</annotation></semantics></math> on which a central model <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="M_{Central}" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><msub id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">M</mi><mrow id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml"><mi id="S2.SS2.p1.5.m5.1.1.3.2" xref="S2.SS2.p1.5.m5.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1a" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.4" xref="S2.SS2.p1.5.m5.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1b" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.5" xref="S2.SS2.p1.5.m5.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1c" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.6" xref="S2.SS2.p1.5.m5.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1d" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.7" xref="S2.SS2.p1.5.m5.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.1e" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.8" xref="S2.SS2.p1.5.m5.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">𝑀</ci><apply id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3"><times id="S2.SS2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3.1"></times><ci id="S2.SS2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2">𝐶</ci><ci id="S2.SS2.p1.5.m5.1.1.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3">𝑒</ci><ci id="S2.SS2.p1.5.m5.1.1.3.4.cmml" xref="S2.SS2.p1.5.m5.1.1.3.4">𝑛</ci><ci id="S2.SS2.p1.5.m5.1.1.3.5.cmml" xref="S2.SS2.p1.5.m5.1.1.3.5">𝑡</ci><ci id="S2.SS2.p1.5.m5.1.1.3.6.cmml" xref="S2.SS2.p1.5.m5.1.1.3.6">𝑟</ci><ci id="S2.SS2.p1.5.m5.1.1.3.7.cmml" xref="S2.SS2.p1.5.m5.1.1.3.7">𝑎</ci><ci id="S2.SS2.p1.5.m5.1.1.3.8.cmml" xref="S2.SS2.p1.5.m5.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">M_{Central}</annotation></semantics></math> is trained. In <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the local datasets are never disclosed by the clients. Instead, the central server initializes a global model <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="M_{Global}" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><msub id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2" xref="S2.SS2.p1.6.m6.1.1.2.cmml">M</mi><mrow id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3.cmml"><mi id="S2.SS2.p1.6.m6.1.1.3.2" xref="S2.SS2.p1.6.m6.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m6.1.1.3.1" xref="S2.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.6.m6.1.1.3.3" xref="S2.SS2.p1.6.m6.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m6.1.1.3.1a" xref="S2.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.6.m6.1.1.3.4" xref="S2.SS2.p1.6.m6.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m6.1.1.3.1b" xref="S2.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.6.m6.1.1.3.5" xref="S2.SS2.p1.6.m6.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m6.1.1.3.1c" xref="S2.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.6.m6.1.1.3.6" xref="S2.SS2.p1.6.m6.1.1.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m6.1.1.3.1d" xref="S2.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.6.m6.1.1.3.7" xref="S2.SS2.p1.6.m6.1.1.3.7.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2">𝑀</ci><apply id="S2.SS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3"><times id="S2.SS2.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.p1.6.m6.1.1.3.1"></times><ci id="S2.SS2.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.p1.6.m6.1.1.3.2">𝐺</ci><ci id="S2.SS2.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3.3">𝑙</ci><ci id="S2.SS2.p1.6.m6.1.1.3.4.cmml" xref="S2.SS2.p1.6.m6.1.1.3.4">𝑜</ci><ci id="S2.SS2.p1.6.m6.1.1.3.5.cmml" xref="S2.SS2.p1.6.m6.1.1.3.5">𝑏</ci><ci id="S2.SS2.p1.6.m6.1.1.3.6.cmml" xref="S2.SS2.p1.6.m6.1.1.3.6">𝑎</ci><ci id="S2.SS2.p1.6.m6.1.1.3.7.cmml" xref="S2.SS2.p1.6.m6.1.1.3.7">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">M_{Global}</annotation></semantics></math> parameterized by a vector <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="\boldsymbol{\theta}\in\mathbb{R}^{d}" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mrow id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.p1.7.m7.1.1.2" xref="S2.SS2.p1.7.m7.1.1.2.cmml">𝜽</mi><mo id="S2.SS2.p1.7.m7.1.1.1" xref="S2.SS2.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S2.SS2.p1.7.m7.1.1.3" xref="S2.SS2.p1.7.m7.1.1.3.cmml"><mi id="S2.SS2.p1.7.m7.1.1.3.2" xref="S2.SS2.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS2.p1.7.m7.1.1.3.3" xref="S2.SS2.p1.7.m7.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1"><in id="S2.SS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1.1"></in><ci id="S2.SS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.p1.7.m7.1.1.2">𝜽</ci><apply id="S2.SS2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.3.1.cmml" xref="S2.SS2.p1.7.m7.1.1.3">superscript</csymbol><ci id="S2.SS2.p1.7.m7.1.1.3.2.cmml" xref="S2.SS2.p1.7.m7.1.1.3.2">ℝ</ci><ci id="S2.SS2.p1.7.m7.1.1.3.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">\boldsymbol{\theta}\in\mathbb{R}^{d}</annotation></semantics></math>, that is sent to all clients <math id="S2.SS2.p1.8.m8.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S2.SS2.p1.8.m8.1a"><msub id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml"><mi id="S2.SS2.p1.8.m8.1.1.2" xref="S2.SS2.p1.8.m8.1.1.2.cmml">c</mi><mi id="S2.SS2.p1.8.m8.1.1.3" xref="S2.SS2.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><apply id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2">𝑐</ci><ci id="S2.SS2.p1.8.m8.1.1.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">c_{i}</annotation></semantics></math> which train the global model on their local datasets <math id="S2.SS2.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="S2.SS2.p1.9.m9.1a"><msub id="S2.SS2.p1.9.m9.1.1" xref="S2.SS2.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.9.m9.1.1.2" xref="S2.SS2.p1.9.m9.1.1.2.cmml">𝒟</mi><mi id="S2.SS2.p1.9.m9.1.1.3" xref="S2.SS2.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.1b"><apply id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.9.m9.1.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.p1.9.m9.1.1.2.cmml" xref="S2.SS2.p1.9.m9.1.1.2">𝒟</ci><ci id="S2.SS2.p1.9.m9.1.1.3.cmml" xref="S2.SS2.p1.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.1c">\mathcal{D}_{i}</annotation></semantics></math>, effectively optimizing their local objective <math id="S2.SS2.p1.10.m10.2" class="ltx_Math" alttext="\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta})" display="inline"><semantics id="S2.SS2.p1.10.m10.2a"><mrow id="S2.SS2.p1.10.m10.2.2" xref="S2.SS2.p1.10.m10.2.2.cmml"><msub id="S2.SS2.p1.10.m10.2.2.3" xref="S2.SS2.p1.10.m10.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.10.m10.2.2.3.2" xref="S2.SS2.p1.10.m10.2.2.3.2.cmml">ℒ</mi><mi id="S2.SS2.p1.10.m10.2.2.3.3" xref="S2.SS2.p1.10.m10.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.10.m10.2.2.2" xref="S2.SS2.p1.10.m10.2.2.2.cmml">​</mo><mrow id="S2.SS2.p1.10.m10.2.2.1.1" xref="S2.SS2.p1.10.m10.2.2.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.10.m10.2.2.1.1.2" xref="S2.SS2.p1.10.m10.2.2.1.2.cmml">(</mo><msub id="S2.SS2.p1.10.m10.2.2.1.1.1" xref="S2.SS2.p1.10.m10.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.10.m10.2.2.1.1.1.2" xref="S2.SS2.p1.10.m10.2.2.1.1.1.2.cmml">𝒟</mi><mi id="S2.SS2.p1.10.m10.2.2.1.1.1.3" xref="S2.SS2.p1.10.m10.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS2.p1.10.m10.2.2.1.1.3" xref="S2.SS2.p1.10.m10.2.2.1.2.cmml">;</mo><mi id="S2.SS2.p1.10.m10.1.1" xref="S2.SS2.p1.10.m10.1.1.cmml">𝜽</mi><mo stretchy="false" id="S2.SS2.p1.10.m10.2.2.1.1.4" xref="S2.SS2.p1.10.m10.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m10.2b"><apply id="S2.SS2.p1.10.m10.2.2.cmml" xref="S2.SS2.p1.10.m10.2.2"><times id="S2.SS2.p1.10.m10.2.2.2.cmml" xref="S2.SS2.p1.10.m10.2.2.2"></times><apply id="S2.SS2.p1.10.m10.2.2.3.cmml" xref="S2.SS2.p1.10.m10.2.2.3"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m10.2.2.3.1.cmml" xref="S2.SS2.p1.10.m10.2.2.3">subscript</csymbol><ci id="S2.SS2.p1.10.m10.2.2.3.2.cmml" xref="S2.SS2.p1.10.m10.2.2.3.2">ℒ</ci><ci id="S2.SS2.p1.10.m10.2.2.3.3.cmml" xref="S2.SS2.p1.10.m10.2.2.3.3">𝑖</ci></apply><list id="S2.SS2.p1.10.m10.2.2.1.2.cmml" xref="S2.SS2.p1.10.m10.2.2.1.1"><apply id="S2.SS2.p1.10.m10.2.2.1.1.1.cmml" xref="S2.SS2.p1.10.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m10.2.2.1.1.1.1.cmml" xref="S2.SS2.p1.10.m10.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.10.m10.2.2.1.1.1.2.cmml" xref="S2.SS2.p1.10.m10.2.2.1.1.1.2">𝒟</ci><ci id="S2.SS2.p1.10.m10.2.2.1.1.1.3.cmml" xref="S2.SS2.p1.10.m10.2.2.1.1.1.3">𝑖</ci></apply><ci id="S2.SS2.p1.10.m10.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1">𝜽</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m10.2c">\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta})</annotation></semantics></math>, which results in a local update <math id="S2.SS2.p1.11.m11.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S2.SS2.p1.11.m11.1a"><msub id="S2.SS2.p1.11.m11.1.1" xref="S2.SS2.p1.11.m11.1.1.cmml"><mi id="S2.SS2.p1.11.m11.1.1.2" xref="S2.SS2.p1.11.m11.1.1.2.cmml">U</mi><mi id="S2.SS2.p1.11.m11.1.1.3" xref="S2.SS2.p1.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.11.m11.1b"><apply id="S2.SS2.p1.11.m11.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.11.m11.1.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1">subscript</csymbol><ci id="S2.SS2.p1.11.m11.1.1.2.cmml" xref="S2.SS2.p1.11.m11.1.1.2">𝑈</ci><ci id="S2.SS2.p1.11.m11.1.1.3.cmml" xref="S2.SS2.p1.11.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.11.m11.1c">U_{i}</annotation></semantics></math><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Depending on the specific <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> algorithm that is being used, these local updates are of different type, e.g., in <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a> an update is represented by the gradient <cite class="ltx_cite ltx_citemacro_citep">(Konečný et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2016</a>)</cite>, in <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> the update is represented by the parametrization of the updated local model <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>, and in federated distillation the update is represented by the soft labels that were produced by the updated local model on a central training dataset <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite>.</span></span></span>. The local update is then sent back to the central server, which uses an aggregation operator to combine the updates into an updated global model <math id="S2.SS2.p1.12.m12.6" class="ltx_Math" alttext="M^{{}^{\prime}}_{Global}=\text{Agg}\left\{U_{i}\mid i\in\left\{1,2,\dots,|C|\right\}\right\}" display="inline"><semantics id="S2.SS2.p1.12.m12.6a"><mrow id="S2.SS2.p1.12.m12.6.6" xref="S2.SS2.p1.12.m12.6.6.cmml"><msubsup id="S2.SS2.p1.12.m12.6.6.4" xref="S2.SS2.p1.12.m12.6.6.4.cmml"><mi id="S2.SS2.p1.12.m12.6.6.4.2.2" xref="S2.SS2.p1.12.m12.6.6.4.2.2.cmml">M</mi><mrow id="S2.SS2.p1.12.m12.6.6.4.3" xref="S2.SS2.p1.12.m12.6.6.4.3.cmml"><mi id="S2.SS2.p1.12.m12.6.6.4.3.2" xref="S2.SS2.p1.12.m12.6.6.4.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.4.3.1" xref="S2.SS2.p1.12.m12.6.6.4.3.1.cmml">​</mo><mi id="S2.SS2.p1.12.m12.6.6.4.3.3" xref="S2.SS2.p1.12.m12.6.6.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.4.3.1a" xref="S2.SS2.p1.12.m12.6.6.4.3.1.cmml">​</mo><mi id="S2.SS2.p1.12.m12.6.6.4.3.4" xref="S2.SS2.p1.12.m12.6.6.4.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.4.3.1b" xref="S2.SS2.p1.12.m12.6.6.4.3.1.cmml">​</mo><mi id="S2.SS2.p1.12.m12.6.6.4.3.5" xref="S2.SS2.p1.12.m12.6.6.4.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.4.3.1c" xref="S2.SS2.p1.12.m12.6.6.4.3.1.cmml">​</mo><mi id="S2.SS2.p1.12.m12.6.6.4.3.6" xref="S2.SS2.p1.12.m12.6.6.4.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.4.3.1d" xref="S2.SS2.p1.12.m12.6.6.4.3.1.cmml">​</mo><mi id="S2.SS2.p1.12.m12.6.6.4.3.7" xref="S2.SS2.p1.12.m12.6.6.4.3.7.cmml">l</mi></mrow><msup id="S2.SS2.p1.12.m12.6.6.4.2.3" xref="S2.SS2.p1.12.m12.6.6.4.2.3.cmml"><mi id="S2.SS2.p1.12.m12.6.6.4.2.3a" xref="S2.SS2.p1.12.m12.6.6.4.2.3.cmml"></mi><mo id="S2.SS2.p1.12.m12.6.6.4.2.3.1" xref="S2.SS2.p1.12.m12.6.6.4.2.3.1.cmml">′</mo></msup></msubsup><mo id="S2.SS2.p1.12.m12.6.6.3" xref="S2.SS2.p1.12.m12.6.6.3.cmml">=</mo><mrow id="S2.SS2.p1.12.m12.6.6.2" xref="S2.SS2.p1.12.m12.6.6.2.cmml"><mtext id="S2.SS2.p1.12.m12.6.6.2.4" xref="S2.SS2.p1.12.m12.6.6.2.4a.cmml">Agg</mtext><mo lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.2.3" xref="S2.SS2.p1.12.m12.6.6.2.3.cmml">​</mo><mrow id="S2.SS2.p1.12.m12.6.6.2.2.2" xref="S2.SS2.p1.12.m12.6.6.2.2.3.cmml"><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.3" xref="S2.SS2.p1.12.m12.6.6.2.2.3.1.cmml">{</mo><msub id="S2.SS2.p1.12.m12.5.5.1.1.1.1" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1.cmml"><mi id="S2.SS2.p1.12.m12.5.5.1.1.1.1.2" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1.2.cmml">U</mi><mi id="S2.SS2.p1.12.m12.5.5.1.1.1.1.3" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1.3.cmml">i</mi></msub><mo fence="true" lspace="0em" rspace="0em" id="S2.SS2.p1.12.m12.6.6.2.2.2.4" xref="S2.SS2.p1.12.m12.6.6.2.2.3.1.cmml">∣</mo><mrow id="S2.SS2.p1.12.m12.6.6.2.2.2.2" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.cmml"><mi id="S2.SS2.p1.12.m12.6.6.2.2.2.2.3" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.3.cmml">i</mi><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.2" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.2.cmml">∈</mo><mrow id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml"><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.2" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml">{</mo><mn id="S2.SS2.p1.12.m12.1.1" xref="S2.SS2.p1.12.m12.1.1.cmml">1</mn><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.3" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml">,</mo><mn id="S2.SS2.p1.12.m12.2.2" xref="S2.SS2.p1.12.m12.2.2.cmml">2</mn><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.4" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p1.12.m12.3.3" xref="S2.SS2.p1.12.m12.3.3.cmml">…</mi><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.5" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml">,</mo><mrow id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.2" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.2.1" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.1.1.cmml">|</mo><mi id="S2.SS2.p1.12.m12.4.4" xref="S2.SS2.p1.12.m12.4.4.cmml">C</mi><mo stretchy="false" id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.2.2" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.1.1.cmml">|</mo></mrow><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.6" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml">}</mo></mrow></mrow><mo id="S2.SS2.p1.12.m12.6.6.2.2.2.5" xref="S2.SS2.p1.12.m12.6.6.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.12.m12.6b"><apply id="S2.SS2.p1.12.m12.6.6.cmml" xref="S2.SS2.p1.12.m12.6.6"><eq id="S2.SS2.p1.12.m12.6.6.3.cmml" xref="S2.SS2.p1.12.m12.6.6.3"></eq><apply id="S2.SS2.p1.12.m12.6.6.4.cmml" xref="S2.SS2.p1.12.m12.6.6.4"><csymbol cd="ambiguous" id="S2.SS2.p1.12.m12.6.6.4.1.cmml" xref="S2.SS2.p1.12.m12.6.6.4">subscript</csymbol><apply id="S2.SS2.p1.12.m12.6.6.4.2.cmml" xref="S2.SS2.p1.12.m12.6.6.4"><csymbol cd="ambiguous" id="S2.SS2.p1.12.m12.6.6.4.2.1.cmml" xref="S2.SS2.p1.12.m12.6.6.4">superscript</csymbol><ci id="S2.SS2.p1.12.m12.6.6.4.2.2.cmml" xref="S2.SS2.p1.12.m12.6.6.4.2.2">𝑀</ci><apply id="S2.SS2.p1.12.m12.6.6.4.2.3.cmml" xref="S2.SS2.p1.12.m12.6.6.4.2.3"><ci id="S2.SS2.p1.12.m12.6.6.4.2.3.1.cmml" xref="S2.SS2.p1.12.m12.6.6.4.2.3.1">′</ci></apply></apply><apply id="S2.SS2.p1.12.m12.6.6.4.3.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3"><times id="S2.SS2.p1.12.m12.6.6.4.3.1.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.1"></times><ci id="S2.SS2.p1.12.m12.6.6.4.3.2.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.2">𝐺</ci><ci id="S2.SS2.p1.12.m12.6.6.4.3.3.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.3">𝑙</ci><ci id="S2.SS2.p1.12.m12.6.6.4.3.4.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.4">𝑜</ci><ci id="S2.SS2.p1.12.m12.6.6.4.3.5.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.5">𝑏</ci><ci id="S2.SS2.p1.12.m12.6.6.4.3.6.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.6">𝑎</ci><ci id="S2.SS2.p1.12.m12.6.6.4.3.7.cmml" xref="S2.SS2.p1.12.m12.6.6.4.3.7">𝑙</ci></apply></apply><apply id="S2.SS2.p1.12.m12.6.6.2.cmml" xref="S2.SS2.p1.12.m12.6.6.2"><times id="S2.SS2.p1.12.m12.6.6.2.3.cmml" xref="S2.SS2.p1.12.m12.6.6.2.3"></times><ci id="S2.SS2.p1.12.m12.6.6.2.4a.cmml" xref="S2.SS2.p1.12.m12.6.6.2.4"><mtext id="S2.SS2.p1.12.m12.6.6.2.4.cmml" xref="S2.SS2.p1.12.m12.6.6.2.4">Agg</mtext></ci><apply id="S2.SS2.p1.12.m12.6.6.2.2.3.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2"><csymbol cd="latexml" id="S2.SS2.p1.12.m12.6.6.2.2.3.1.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.3">conditional-set</csymbol><apply id="S2.SS2.p1.12.m12.5.5.1.1.1.1.cmml" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.12.m12.5.5.1.1.1.1.1.cmml" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.12.m12.5.5.1.1.1.1.2.cmml" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1.2">𝑈</ci><ci id="S2.SS2.p1.12.m12.5.5.1.1.1.1.3.cmml" xref="S2.SS2.p1.12.m12.5.5.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS2.p1.12.m12.6.6.2.2.2.2.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2"><in id="S2.SS2.p1.12.m12.6.6.2.2.2.2.2.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.2"></in><ci id="S2.SS2.p1.12.m12.6.6.2.2.2.2.3.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.3">𝑖</ci><set id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.2.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1"><cn type="integer" id="S2.SS2.p1.12.m12.1.1.cmml" xref="S2.SS2.p1.12.m12.1.1">1</cn><cn type="integer" id="S2.SS2.p1.12.m12.2.2.cmml" xref="S2.SS2.p1.12.m12.2.2">2</cn><ci id="S2.SS2.p1.12.m12.3.3.cmml" xref="S2.SS2.p1.12.m12.3.3">…</ci><apply id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.1.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.2"><abs id="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.1.1.cmml" xref="S2.SS2.p1.12.m12.6.6.2.2.2.2.1.1.1.2.1"></abs><ci id="S2.SS2.p1.12.m12.4.4.cmml" xref="S2.SS2.p1.12.m12.4.4">𝐶</ci></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.12.m12.6c">M^{{}^{\prime}}_{Global}=\text{Agg}\left\{U_{i}\mid i\in\left\{1,2,\dots,|C|\right\}\right\}</annotation></semantics></math>. This process is repeated until a suitable convergence metric is met. The objective of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> can therefore be stated as the following minimization problem <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="A6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.14" class="ltx_Math" alttext="\displaystyle\min_{\boldsymbol{\theta}\in\mathbb{R}^{d}}\sum_{i=1}^{\left|C\right|}\frac{\left|\mathcal{D}_{i}\right|}{\left|\bigcup_{j=1}^{\left|C\right|}\mathcal{D}_{j}\right|}\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta}),\text{where}\qquad\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta})=\frac{1}{\left|\mathcal{D}_{i}\right|}\sum_{x,y\in\mathcal{D}_{i}}\ell(x,y;\boldsymbol{\theta})," display="inline"><semantics id="S2.E1.m1.14a"><mrow id="S2.E1.m1.14.14.1" xref="S2.E1.m1.14.14.1.1.cmml"><mrow id="S2.E1.m1.14.14.1.1" xref="S2.E1.m1.14.14.1.1.cmml"><mrow id="S2.E1.m1.14.14.1.1.2.2" xref="S2.E1.m1.14.14.1.1.2.3.cmml"><mrow id="S2.E1.m1.14.14.1.1.1.1.1" xref="S2.E1.m1.14.14.1.1.1.1.1.cmml"><munder id="S2.E1.m1.14.14.1.1.1.1.1.3" xref="S2.E1.m1.14.14.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.14.14.1.1.1.1.1.3.2" xref="S2.E1.m1.14.14.1.1.1.1.1.3.2.cmml">min</mi><mrow id="S2.E1.m1.14.14.1.1.1.1.1.3.3" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.14.14.1.1.1.1.1.3.3.2" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.2.cmml">𝜽</mi><mo id="S2.E1.m1.14.14.1.1.1.1.1.3.3.1" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.1.cmml">∈</mo><msup id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.2.cmml">ℝ</mi><mi id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.3.cmml">d</mi></msup></mrow></munder><mo lspace="0.167em" rspace="0em" id="S2.E1.m1.14.14.1.1.1.1.1.2" xref="S2.E1.m1.14.14.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.14.14.1.1.1.1.1.1" xref="S2.E1.m1.14.14.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E1.m1.14.14.1.1.1.1.1.1.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.cmml"><munderover id="S2.E1.m1.14.14.1.1.1.1.1.1.2a" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.2.cmml"><mo id="S2.E1.m1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">C</mi><mo id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover></mstyle><mrow id="S2.E1.m1.14.14.1.1.1.1.1.1.1" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mfrac id="S2.E1.m1.4.4a" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.2.cmml"><mo id="S2.E1.m1.2.2.1.1.2" xref="S2.E1.m1.2.2.1.2.1.cmml">|</mo><msub id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.2.cmml">𝒟</mi><mi id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.2.1.cmml">|</mo></mrow><mrow id="S2.E1.m1.4.4.3.2" xref="S2.E1.m1.4.4.3.3.cmml"><mo id="S2.E1.m1.4.4.3.2.2" xref="S2.E1.m1.4.4.3.3.1.cmml">|</mo><mrow id="S2.E1.m1.4.4.3.2.1" xref="S2.E1.m1.4.4.3.2.1.cmml"><msubsup id="S2.E1.m1.4.4.3.2.1.1" xref="S2.E1.m1.4.4.3.2.1.1.cmml"><mo lspace="0em" id="S2.E1.m1.4.4.3.2.1.1.2.2" xref="S2.E1.m1.4.4.3.2.1.1.2.2.cmml">⋃</mo><mrow id="S2.E1.m1.4.4.3.2.1.1.2.3" xref="S2.E1.m1.4.4.3.2.1.1.2.3.cmml"><mi id="S2.E1.m1.4.4.3.2.1.1.2.3.2" xref="S2.E1.m1.4.4.3.2.1.1.2.3.2.cmml">j</mi><mo id="S2.E1.m1.4.4.3.2.1.1.2.3.1" xref="S2.E1.m1.4.4.3.2.1.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.4.4.3.2.1.1.2.3.3" xref="S2.E1.m1.4.4.3.2.1.1.2.3.3.cmml">1</mn></mrow><mrow id="S2.E1.m1.3.3.2.1.1.3" xref="S2.E1.m1.3.3.2.1.1.2.cmml"><mo id="S2.E1.m1.3.3.2.1.1.3.1" xref="S2.E1.m1.3.3.2.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.3.3.2.1.1.1" xref="S2.E1.m1.3.3.2.1.1.1.cmml">C</mi><mo id="S2.E1.m1.3.3.2.1.1.3.2" xref="S2.E1.m1.3.3.2.1.1.2.1.cmml">|</mo></mrow></msubsup><msub id="S2.E1.m1.4.4.3.2.1.2" xref="S2.E1.m1.4.4.3.2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.4.3.2.1.2.2" xref="S2.E1.m1.4.4.3.2.1.2.2.cmml">𝒟</mi><mi id="S2.E1.m1.4.4.3.2.1.2.3" xref="S2.E1.m1.4.4.3.2.1.2.3.cmml">j</mi></msub></mrow><mo id="S2.E1.m1.4.4.3.2.3" xref="S2.E1.m1.4.4.3.3.1.cmml">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.2.cmml">ℒ</mi><mi id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.2a" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒟</mi><mi id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E1.m1.8.8" xref="S2.E1.m1.8.8.cmml">𝜽</mi><mo stretchy="false" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.14.14.1.1.2.2.3" xref="S2.E1.m1.14.14.1.1.2.3.cmml">,</mo><mtext id="S2.E1.m1.13.13" xref="S2.E1.m1.13.13a.cmml">where</mtext><mspace width="2em" id="S2.E1.m1.14.14.1.1.2.2.4" xref="S2.E1.m1.14.14.1.1.2.3.cmml"></mspace><mrow id="S2.E1.m1.14.14.1.1.2.2.2" xref="S2.E1.m1.14.14.1.1.2.2.2.cmml"><msub id="S2.E1.m1.14.14.1.1.2.2.2.3" xref="S2.E1.m1.14.14.1.1.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.14.14.1.1.2.2.2.3.2" xref="S2.E1.m1.14.14.1.1.2.2.2.3.2.cmml">ℒ</mi><mi id="S2.E1.m1.14.14.1.1.2.2.2.3.3" xref="S2.E1.m1.14.14.1.1.2.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.14.14.1.1.2.2.2.2" xref="S2.E1.m1.14.14.1.1.2.2.2.2.cmml">​</mo><mrow id="S2.E1.m1.14.14.1.1.2.2.2.1.1" xref="S2.E1.m1.14.14.1.1.2.2.2.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.14.14.1.1.2.2.2.1.1.2" xref="S2.E1.m1.14.14.1.1.2.2.2.1.2.cmml">(</mo><msub id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.2" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.2.cmml">𝒟</mi><mi id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.3" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.14.14.1.1.2.2.2.1.1.3" xref="S2.E1.m1.14.14.1.1.2.2.2.1.2.cmml">;</mo><mi id="S2.E1.m1.9.9" xref="S2.E1.m1.9.9.cmml">𝜽</mi><mo stretchy="false" id="S2.E1.m1.14.14.1.1.2.2.2.1.1.4" xref="S2.E1.m1.14.14.1.1.2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.14.14.1.1.3" xref="S2.E1.m1.14.14.1.1.3.cmml">=</mo><mrow id="S2.E1.m1.14.14.1.1.4" xref="S2.E1.m1.14.14.1.1.4.cmml"><mstyle displaystyle="true" id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mfrac id="S2.E1.m1.5.5a" xref="S2.E1.m1.5.5.cmml"><mn id="S2.E1.m1.5.5.3" xref="S2.E1.m1.5.5.3.cmml">1</mn><mrow id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.2.cmml"><mo id="S2.E1.m1.5.5.1.1.2" xref="S2.E1.m1.5.5.1.2.1.cmml">|</mo><msub id="S2.E1.m1.5.5.1.1.1" xref="S2.E1.m1.5.5.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.5.5.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.2.cmml">𝒟</mi><mi id="S2.E1.m1.5.5.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.5.5.1.1.3" xref="S2.E1.m1.5.5.1.2.1.cmml">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.E1.m1.14.14.1.1.4.1" xref="S2.E1.m1.14.14.1.1.4.1.cmml">​</mo><mrow id="S2.E1.m1.14.14.1.1.4.2" xref="S2.E1.m1.14.14.1.1.4.2.cmml"><mstyle displaystyle="true" id="S2.E1.m1.14.14.1.1.4.2.1" xref="S2.E1.m1.14.14.1.1.4.2.1.cmml"><munder id="S2.E1.m1.14.14.1.1.4.2.1a" xref="S2.E1.m1.14.14.1.1.4.2.1.cmml"><mo movablelimits="false" id="S2.E1.m1.14.14.1.1.4.2.1.2" xref="S2.E1.m1.14.14.1.1.4.2.1.2.cmml">∑</mo><mrow id="S2.E1.m1.7.7.2" xref="S2.E1.m1.7.7.2.cmml"><mrow id="S2.E1.m1.7.7.2.4.2" xref="S2.E1.m1.7.7.2.4.1.cmml"><mi id="S2.E1.m1.6.6.1.1" xref="S2.E1.m1.6.6.1.1.cmml">x</mi><mo id="S2.E1.m1.7.7.2.4.2.1" xref="S2.E1.m1.7.7.2.4.1.cmml">,</mo><mi id="S2.E1.m1.7.7.2.2" xref="S2.E1.m1.7.7.2.2.cmml">y</mi></mrow><mo id="S2.E1.m1.7.7.2.3" xref="S2.E1.m1.7.7.2.3.cmml">∈</mo><msub id="S2.E1.m1.7.7.2.5" xref="S2.E1.m1.7.7.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.7.7.2.5.2" xref="S2.E1.m1.7.7.2.5.2.cmml">𝒟</mi><mi id="S2.E1.m1.7.7.2.5.3" xref="S2.E1.m1.7.7.2.5.3.cmml">i</mi></msub></mrow></munder></mstyle><mrow id="S2.E1.m1.14.14.1.1.4.2.2" xref="S2.E1.m1.14.14.1.1.4.2.2.cmml"><mi mathvariant="normal" id="S2.E1.m1.14.14.1.1.4.2.2.2" xref="S2.E1.m1.14.14.1.1.4.2.2.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.14.14.1.1.4.2.2.1" xref="S2.E1.m1.14.14.1.1.4.2.2.1.cmml">​</mo><mrow id="S2.E1.m1.14.14.1.1.4.2.2.3.2" xref="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.14.14.1.1.4.2.2.3.2.1" xref="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml">(</mo><mi id="S2.E1.m1.10.10" xref="S2.E1.m1.10.10.cmml">x</mi><mo id="S2.E1.m1.14.14.1.1.4.2.2.3.2.2" xref="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml">,</mo><mi id="S2.E1.m1.11.11" xref="S2.E1.m1.11.11.cmml">y</mi><mo id="S2.E1.m1.14.14.1.1.4.2.2.3.2.3" xref="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml">;</mo><mi id="S2.E1.m1.12.12" xref="S2.E1.m1.12.12.cmml">𝜽</mi><mo stretchy="false" id="S2.E1.m1.14.14.1.1.4.2.2.3.2.4" xref="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.14.14.1.2" xref="S2.E1.m1.14.14.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.14b"><apply id="S2.E1.m1.14.14.1.1.cmml" xref="S2.E1.m1.14.14.1"><eq id="S2.E1.m1.14.14.1.1.3.cmml" xref="S2.E1.m1.14.14.1.1.3"></eq><list id="S2.E1.m1.14.14.1.1.2.3.cmml" xref="S2.E1.m1.14.14.1.1.2.2"><apply id="S2.E1.m1.14.14.1.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1"><times id="S2.E1.m1.14.14.1.1.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.2"></times><apply id="S2.E1.m1.14.14.1.1.1.1.1.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3">subscript</csymbol><min id="S2.E1.m1.14.14.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.2"></min><apply id="S2.E1.m1.14.14.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3"><in id="S2.E1.m1.14.14.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.1"></in><ci id="S2.E1.m1.14.14.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.2">𝜽</ci><apply id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3">superscript</csymbol><ci id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.2">ℝ</ci><ci id="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.3.3.3.3">𝑑</ci></apply></apply></apply><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1"><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.2"></sum><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3"><eq id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.3"><abs id="S2.E1.m1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.3.1"></abs><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝐶</ci></apply></apply><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1"><times id="S2.E1.m1.14.14.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4"><divide id="S2.E1.m1.4.4.4.cmml" xref="S2.E1.m1.4.4"></divide><apply id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.1"><abs id="S2.E1.m1.2.2.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.2"></abs><apply id="S2.E1.m1.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2">𝒟</ci><ci id="S2.E1.m1.2.2.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E1.m1.4.4.3.3.cmml" xref="S2.E1.m1.4.4.3.2"><abs id="S2.E1.m1.4.4.3.3.1.cmml" xref="S2.E1.m1.4.4.3.2.2"></abs><apply id="S2.E1.m1.4.4.3.2.1.cmml" xref="S2.E1.m1.4.4.3.2.1"><apply id="S2.E1.m1.4.4.3.2.1.1.cmml" xref="S2.E1.m1.4.4.3.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.2.1.1.1.cmml" xref="S2.E1.m1.4.4.3.2.1.1">superscript</csymbol><apply id="S2.E1.m1.4.4.3.2.1.1.2.cmml" xref="S2.E1.m1.4.4.3.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.2.1.1.2.1.cmml" xref="S2.E1.m1.4.4.3.2.1.1">subscript</csymbol><union id="S2.E1.m1.4.4.3.2.1.1.2.2.cmml" xref="S2.E1.m1.4.4.3.2.1.1.2.2"></union><apply id="S2.E1.m1.4.4.3.2.1.1.2.3.cmml" xref="S2.E1.m1.4.4.3.2.1.1.2.3"><eq id="S2.E1.m1.4.4.3.2.1.1.2.3.1.cmml" xref="S2.E1.m1.4.4.3.2.1.1.2.3.1"></eq><ci id="S2.E1.m1.4.4.3.2.1.1.2.3.2.cmml" xref="S2.E1.m1.4.4.3.2.1.1.2.3.2">𝑗</ci><cn type="integer" id="S2.E1.m1.4.4.3.2.1.1.2.3.3.cmml" xref="S2.E1.m1.4.4.3.2.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E1.m1.3.3.2.1.1.2.cmml" xref="S2.E1.m1.3.3.2.1.1.3"><abs id="S2.E1.m1.3.3.2.1.1.2.1.cmml" xref="S2.E1.m1.3.3.2.1.1.3.1"></abs><ci id="S2.E1.m1.3.3.2.1.1.1.cmml" xref="S2.E1.m1.3.3.2.1.1.1">𝐶</ci></apply></apply><apply id="S2.E1.m1.4.4.3.2.1.2.cmml" xref="S2.E1.m1.4.4.3.2.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.2.1.2.1.cmml" xref="S2.E1.m1.4.4.3.2.1.2">subscript</csymbol><ci id="S2.E1.m1.4.4.3.2.1.2.2.cmml" xref="S2.E1.m1.4.4.3.2.1.2.2">𝒟</ci><ci id="S2.E1.m1.4.4.3.2.1.2.3.cmml" xref="S2.E1.m1.4.4.3.2.1.2.3">𝑗</ci></apply></apply></apply></apply><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.2">ℒ</ci><ci id="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1"><apply id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.2">𝒟</ci><ci id="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.14.14.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S2.E1.m1.8.8.cmml" xref="S2.E1.m1.8.8">𝜽</ci></list></apply></apply></apply><ci id="S2.E1.m1.13.13a.cmml" xref="S2.E1.m1.13.13"><mtext id="S2.E1.m1.13.13.cmml" xref="S2.E1.m1.13.13">where</mtext></ci><apply id="S2.E1.m1.14.14.1.1.2.2.2.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2"><times id="S2.E1.m1.14.14.1.1.2.2.2.2.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.2"></times><apply id="S2.E1.m1.14.14.1.1.2.2.2.3.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.2.2.2.3.1.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.3">subscript</csymbol><ci id="S2.E1.m1.14.14.1.1.2.2.2.3.2.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.3.2">ℒ</ci><ci id="S2.E1.m1.14.14.1.1.2.2.2.3.3.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.3.3">𝑖</ci></apply><list id="S2.E1.m1.14.14.1.1.2.2.2.1.2.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1"><apply id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.2.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.2">𝒟</ci><ci id="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.3.cmml" xref="S2.E1.m1.14.14.1.1.2.2.2.1.1.1.3">𝑖</ci></apply><ci id="S2.E1.m1.9.9.cmml" xref="S2.E1.m1.9.9">𝜽</ci></list></apply></list><apply id="S2.E1.m1.14.14.1.1.4.cmml" xref="S2.E1.m1.14.14.1.1.4"><times id="S2.E1.m1.14.14.1.1.4.1.cmml" xref="S2.E1.m1.14.14.1.1.4.1"></times><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><divide id="S2.E1.m1.5.5.2.cmml" xref="S2.E1.m1.5.5"></divide><cn type="integer" id="S2.E1.m1.5.5.3.cmml" xref="S2.E1.m1.5.5.3">1</cn><apply id="S2.E1.m1.5.5.1.2.cmml" xref="S2.E1.m1.5.5.1.1"><abs id="S2.E1.m1.5.5.1.2.1.cmml" xref="S2.E1.m1.5.5.1.1.2"></abs><apply id="S2.E1.m1.5.5.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.2">𝒟</ci><ci id="S2.E1.m1.5.5.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.14.14.1.1.4.2.cmml" xref="S2.E1.m1.14.14.1.1.4.2"><apply id="S2.E1.m1.14.14.1.1.4.2.1.cmml" xref="S2.E1.m1.14.14.1.1.4.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.14.14.1.1.4.2.1.1.cmml" xref="S2.E1.m1.14.14.1.1.4.2.1">subscript</csymbol><sum id="S2.E1.m1.14.14.1.1.4.2.1.2.cmml" xref="S2.E1.m1.14.14.1.1.4.2.1.2"></sum><apply id="S2.E1.m1.7.7.2.cmml" xref="S2.E1.m1.7.7.2"><in id="S2.E1.m1.7.7.2.3.cmml" xref="S2.E1.m1.7.7.2.3"></in><list id="S2.E1.m1.7.7.2.4.1.cmml" xref="S2.E1.m1.7.7.2.4.2"><ci id="S2.E1.m1.6.6.1.1.cmml" xref="S2.E1.m1.6.6.1.1">𝑥</ci><ci id="S2.E1.m1.7.7.2.2.cmml" xref="S2.E1.m1.7.7.2.2">𝑦</ci></list><apply id="S2.E1.m1.7.7.2.5.cmml" xref="S2.E1.m1.7.7.2.5"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.2.5.1.cmml" xref="S2.E1.m1.7.7.2.5">subscript</csymbol><ci id="S2.E1.m1.7.7.2.5.2.cmml" xref="S2.E1.m1.7.7.2.5.2">𝒟</ci><ci id="S2.E1.m1.7.7.2.5.3.cmml" xref="S2.E1.m1.7.7.2.5.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.14.14.1.1.4.2.2.cmml" xref="S2.E1.m1.14.14.1.1.4.2.2"><times id="S2.E1.m1.14.14.1.1.4.2.2.1.cmml" xref="S2.E1.m1.14.14.1.1.4.2.2.1"></times><ci id="S2.E1.m1.14.14.1.1.4.2.2.2.cmml" xref="S2.E1.m1.14.14.1.1.4.2.2.2">ℓ</ci><vector id="S2.E1.m1.14.14.1.1.4.2.2.3.1.cmml" xref="S2.E1.m1.14.14.1.1.4.2.2.3.2"><ci id="S2.E1.m1.10.10.cmml" xref="S2.E1.m1.10.10">𝑥</ci><ci id="S2.E1.m1.11.11.cmml" xref="S2.E1.m1.11.11">𝑦</ci><ci id="S2.E1.m1.12.12.cmml" xref="S2.E1.m1.12.12">𝜽</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.14c">\displaystyle\min_{\boldsymbol{\theta}\in\mathbb{R}^{d}}\sum_{i=1}^{\left|C\right|}\frac{\left|\mathcal{D}_{i}\right|}{\left|\bigcup_{j=1}^{\left|C\right|}\mathcal{D}_{j}\right|}\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta}),\text{where}\qquad\mathcal{L}_{i}(\mathcal{D}_{i};\boldsymbol{\theta})=\frac{1}{\left|\mathcal{D}_{i}\right|}\sum_{x,y\in\mathcal{D}_{i}}\ell(x,y;\boldsymbol{\theta}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.9" class="ltx_p">with <math id="S2.SS2.p3.1.m1.3" class="ltx_Math" alttext="\ell(x,y;\boldsymbol{\theta})" display="inline"><semantics id="S2.SS2.p3.1.m1.3a"><mrow id="S2.SS2.p3.1.m1.3.4" xref="S2.SS2.p3.1.m1.3.4.cmml"><mi mathvariant="normal" id="S2.SS2.p3.1.m1.3.4.2" xref="S2.SS2.p3.1.m1.3.4.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.3.4.1" xref="S2.SS2.p3.1.m1.3.4.1.cmml">​</mo><mrow id="S2.SS2.p3.1.m1.3.4.3.2" xref="S2.SS2.p3.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS2.p3.1.m1.3.4.3.2.1" xref="S2.SS2.p3.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">x</mi><mo id="S2.SS2.p3.1.m1.3.4.3.2.2" xref="S2.SS2.p3.1.m1.3.4.3.1.cmml">,</mo><mi id="S2.SS2.p3.1.m1.2.2" xref="S2.SS2.p3.1.m1.2.2.cmml">y</mi><mo id="S2.SS2.p3.1.m1.3.4.3.2.3" xref="S2.SS2.p3.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p3.1.m1.3.3" xref="S2.SS2.p3.1.m1.3.3.cmml">𝜽</mi><mo stretchy="false" id="S2.SS2.p3.1.m1.3.4.3.2.4" xref="S2.SS2.p3.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.3b"><apply id="S2.SS2.p3.1.m1.3.4.cmml" xref="S2.SS2.p3.1.m1.3.4"><times id="S2.SS2.p3.1.m1.3.4.1.cmml" xref="S2.SS2.p3.1.m1.3.4.1"></times><ci id="S2.SS2.p3.1.m1.3.4.2.cmml" xref="S2.SS2.p3.1.m1.3.4.2">ℓ</ci><vector id="S2.SS2.p3.1.m1.3.4.3.1.cmml" xref="S2.SS2.p3.1.m1.3.4.3.2"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝑥</ci><ci id="S2.SS2.p3.1.m1.2.2.cmml" xref="S2.SS2.p3.1.m1.2.2">𝑦</ci><ci id="S2.SS2.p3.1.m1.3.3.cmml" xref="S2.SS2.p3.1.m1.3.3">𝜽</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.3c">\ell(x,y;\boldsymbol{\theta})</annotation></semantics></math> denoting the loss of the client model on input <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">x</annotation></semantics></math> with ground-truth <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mi id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><ci id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">y</annotation></semantics></math>, given the model parametrization <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">\boldsymbol{\theta}</annotation></semantics></math>. <span id="S2.SS2.p3.9.1" class="ltx_ERROR undefined">\Ac</span>fl allows the global model <math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="M_{Global}" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><msub id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml"><mi id="S2.SS2.p3.5.m5.1.1.2" xref="S2.SS2.p3.5.m5.1.1.2.cmml">M</mi><mrow id="S2.SS2.p3.5.m5.1.1.3" xref="S2.SS2.p3.5.m5.1.1.3.cmml"><mi id="S2.SS2.p3.5.m5.1.1.3.2" xref="S2.SS2.p3.5.m5.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.3.1" xref="S2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3.3" xref="S2.SS2.p3.5.m5.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.3.1a" xref="S2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3.4" xref="S2.SS2.p3.5.m5.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.3.1b" xref="S2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3.5" xref="S2.SS2.p3.5.m5.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.3.1c" xref="S2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3.6" xref="S2.SS2.p3.5.m5.1.1.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.3.1d" xref="S2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3.7" xref="S2.SS2.p3.5.m5.1.1.3.7.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><apply id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2">𝑀</ci><apply id="S2.SS2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3"><times id="S2.SS2.p3.5.m5.1.1.3.1.cmml" xref="S2.SS2.p3.5.m5.1.1.3.1"></times><ci id="S2.SS2.p3.5.m5.1.1.3.2.cmml" xref="S2.SS2.p3.5.m5.1.1.3.2">𝐺</ci><ci id="S2.SS2.p3.5.m5.1.1.3.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3">𝑙</ci><ci id="S2.SS2.p3.5.m5.1.1.3.4.cmml" xref="S2.SS2.p3.5.m5.1.1.3.4">𝑜</ci><ci id="S2.SS2.p3.5.m5.1.1.3.5.cmml" xref="S2.SS2.p3.5.m5.1.1.3.5">𝑏</ci><ci id="S2.SS2.p3.5.m5.1.1.3.6.cmml" xref="S2.SS2.p3.5.m5.1.1.3.6">𝑎</ci><ci id="S2.SS2.p3.5.m5.1.1.3.7.cmml" xref="S2.SS2.p3.5.m5.1.1.3.7">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">M_{Global}</annotation></semantics></math> to train on significantly more data than if each client had only trained on its private data. Thus, under ideal conditions, given a performance metric <math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mi id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><ci id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">P</annotation></semantics></math>, the performance of the global model <math id="S2.SS2.p3.7.m7.1" class="ltx_Math" alttext="P_{Global}" display="inline"><semantics id="S2.SS2.p3.7.m7.1a"><msub id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml"><mi id="S2.SS2.p3.7.m7.1.1.2" xref="S2.SS2.p3.7.m7.1.1.2.cmml">P</mi><mrow id="S2.SS2.p3.7.m7.1.1.3" xref="S2.SS2.p3.7.m7.1.1.3.cmml"><mi id="S2.SS2.p3.7.m7.1.1.3.2" xref="S2.SS2.p3.7.m7.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.3.1" xref="S2.SS2.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.7.m7.1.1.3.3" xref="S2.SS2.p3.7.m7.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.3.1a" xref="S2.SS2.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.7.m7.1.1.3.4" xref="S2.SS2.p3.7.m7.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.3.1b" xref="S2.SS2.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.7.m7.1.1.3.5" xref="S2.SS2.p3.7.m7.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.3.1c" xref="S2.SS2.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.7.m7.1.1.3.6" xref="S2.SS2.p3.7.m7.1.1.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.3.1d" xref="S2.SS2.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.7.m7.1.1.3.7" xref="S2.SS2.p3.7.m7.1.1.3.7.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.1b"><apply id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.7.m7.1.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p3.7.m7.1.1.2.cmml" xref="S2.SS2.p3.7.m7.1.1.2">𝑃</ci><apply id="S2.SS2.p3.7.m7.1.1.3.cmml" xref="S2.SS2.p3.7.m7.1.1.3"><times id="S2.SS2.p3.7.m7.1.1.3.1.cmml" xref="S2.SS2.p3.7.m7.1.1.3.1"></times><ci id="S2.SS2.p3.7.m7.1.1.3.2.cmml" xref="S2.SS2.p3.7.m7.1.1.3.2">𝐺</ci><ci id="S2.SS2.p3.7.m7.1.1.3.3.cmml" xref="S2.SS2.p3.7.m7.1.1.3.3">𝑙</ci><ci id="S2.SS2.p3.7.m7.1.1.3.4.cmml" xref="S2.SS2.p3.7.m7.1.1.3.4">𝑜</ci><ci id="S2.SS2.p3.7.m7.1.1.3.5.cmml" xref="S2.SS2.p3.7.m7.1.1.3.5">𝑏</ci><ci id="S2.SS2.p3.7.m7.1.1.3.6.cmml" xref="S2.SS2.p3.7.m7.1.1.3.6">𝑎</ci><ci id="S2.SS2.p3.7.m7.1.1.3.7.cmml" xref="S2.SS2.p3.7.m7.1.1.3.7">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.1c">P_{Global}</annotation></semantics></math> should be better than that of each individual client <math id="S2.SS2.p3.8.m8.5" class="ltx_Math" alttext="\forall i\in\left\{1,2,\dots,|C|\right\}:P_{Global}&gt;P_{i}" display="inline"><semantics id="S2.SS2.p3.8.m8.5a"><mrow id="S2.SS2.p3.8.m8.5.5" xref="S2.SS2.p3.8.m8.5.5.cmml"><mrow id="S2.SS2.p3.8.m8.5.5.1" xref="S2.SS2.p3.8.m8.5.5.1.cmml"><mrow id="S2.SS2.p3.8.m8.5.5.1.3" xref="S2.SS2.p3.8.m8.5.5.1.3.cmml"><mo rspace="0.167em" id="S2.SS2.p3.8.m8.5.5.1.3.1" xref="S2.SS2.p3.8.m8.5.5.1.3.1.cmml">∀</mo><mi id="S2.SS2.p3.8.m8.5.5.1.3.2" xref="S2.SS2.p3.8.m8.5.5.1.3.2.cmml">i</mi></mrow><mo id="S2.SS2.p3.8.m8.5.5.1.2" xref="S2.SS2.p3.8.m8.5.5.1.2.cmml">∈</mo><mrow id="S2.SS2.p3.8.m8.5.5.1.1.1" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml"><mo id="S2.SS2.p3.8.m8.5.5.1.1.1.2" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml">{</mo><mn id="S2.SS2.p3.8.m8.1.1" xref="S2.SS2.p3.8.m8.1.1.cmml">1</mn><mo id="S2.SS2.p3.8.m8.5.5.1.1.1.3" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml">,</mo><mn id="S2.SS2.p3.8.m8.2.2" xref="S2.SS2.p3.8.m8.2.2.cmml">2</mn><mo id="S2.SS2.p3.8.m8.5.5.1.1.1.4" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p3.8.m8.3.3" xref="S2.SS2.p3.8.m8.3.3.cmml">…</mi><mo id="S2.SS2.p3.8.m8.5.5.1.1.1.5" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml">,</mo><mrow id="S2.SS2.p3.8.m8.5.5.1.1.1.1.2" xref="S2.SS2.p3.8.m8.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p3.8.m8.5.5.1.1.1.1.2.1" xref="S2.SS2.p3.8.m8.5.5.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS2.p3.8.m8.4.4" xref="S2.SS2.p3.8.m8.4.4.cmml">C</mi><mo stretchy="false" id="S2.SS2.p3.8.m8.5.5.1.1.1.1.2.2" xref="S2.SS2.p3.8.m8.5.5.1.1.1.1.1.1.cmml">|</mo></mrow><mo rspace="0.278em" id="S2.SS2.p3.8.m8.5.5.1.1.1.6" xref="S2.SS2.p3.8.m8.5.5.1.1.2.cmml">}</mo></mrow></mrow><mo rspace="0.278em" id="S2.SS2.p3.8.m8.5.5.2" xref="S2.SS2.p3.8.m8.5.5.2.cmml">:</mo><mrow id="S2.SS2.p3.8.m8.5.5.3" xref="S2.SS2.p3.8.m8.5.5.3.cmml"><msub id="S2.SS2.p3.8.m8.5.5.3.2" xref="S2.SS2.p3.8.m8.5.5.3.2.cmml"><mi id="S2.SS2.p3.8.m8.5.5.3.2.2" xref="S2.SS2.p3.8.m8.5.5.3.2.2.cmml">P</mi><mrow id="S2.SS2.p3.8.m8.5.5.3.2.3" xref="S2.SS2.p3.8.m8.5.5.3.2.3.cmml"><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.2" xref="S2.SS2.p3.8.m8.5.5.3.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.8.m8.5.5.3.2.3.1" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.3" xref="S2.SS2.p3.8.m8.5.5.3.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.8.m8.5.5.3.2.3.1a" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.4" xref="S2.SS2.p3.8.m8.5.5.3.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.8.m8.5.5.3.2.3.1b" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.5" xref="S2.SS2.p3.8.m8.5.5.3.2.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.8.m8.5.5.3.2.3.1c" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.6" xref="S2.SS2.p3.8.m8.5.5.3.2.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.8.m8.5.5.3.2.3.1d" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.8.m8.5.5.3.2.3.7" xref="S2.SS2.p3.8.m8.5.5.3.2.3.7.cmml">l</mi></mrow></msub><mo id="S2.SS2.p3.8.m8.5.5.3.1" xref="S2.SS2.p3.8.m8.5.5.3.1.cmml">&gt;</mo><msub id="S2.SS2.p3.8.m8.5.5.3.3" xref="S2.SS2.p3.8.m8.5.5.3.3.cmml"><mi id="S2.SS2.p3.8.m8.5.5.3.3.2" xref="S2.SS2.p3.8.m8.5.5.3.3.2.cmml">P</mi><mi id="S2.SS2.p3.8.m8.5.5.3.3.3" xref="S2.SS2.p3.8.m8.5.5.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.8.m8.5b"><apply id="S2.SS2.p3.8.m8.5.5.cmml" xref="S2.SS2.p3.8.m8.5.5"><ci id="S2.SS2.p3.8.m8.5.5.2.cmml" xref="S2.SS2.p3.8.m8.5.5.2">:</ci><apply id="S2.SS2.p3.8.m8.5.5.1.cmml" xref="S2.SS2.p3.8.m8.5.5.1"><in id="S2.SS2.p3.8.m8.5.5.1.2.cmml" xref="S2.SS2.p3.8.m8.5.5.1.2"></in><apply id="S2.SS2.p3.8.m8.5.5.1.3.cmml" xref="S2.SS2.p3.8.m8.5.5.1.3"><csymbol cd="latexml" id="S2.SS2.p3.8.m8.5.5.1.3.1.cmml" xref="S2.SS2.p3.8.m8.5.5.1.3.1">for-all</csymbol><ci id="S2.SS2.p3.8.m8.5.5.1.3.2.cmml" xref="S2.SS2.p3.8.m8.5.5.1.3.2">𝑖</ci></apply><set id="S2.SS2.p3.8.m8.5.5.1.1.2.cmml" xref="S2.SS2.p3.8.m8.5.5.1.1.1"><cn type="integer" id="S2.SS2.p3.8.m8.1.1.cmml" xref="S2.SS2.p3.8.m8.1.1">1</cn><cn type="integer" id="S2.SS2.p3.8.m8.2.2.cmml" xref="S2.SS2.p3.8.m8.2.2">2</cn><ci id="S2.SS2.p3.8.m8.3.3.cmml" xref="S2.SS2.p3.8.m8.3.3">…</ci><apply id="S2.SS2.p3.8.m8.5.5.1.1.1.1.1.cmml" xref="S2.SS2.p3.8.m8.5.5.1.1.1.1.2"><abs id="S2.SS2.p3.8.m8.5.5.1.1.1.1.1.1.cmml" xref="S2.SS2.p3.8.m8.5.5.1.1.1.1.2.1"></abs><ci id="S2.SS2.p3.8.m8.4.4.cmml" xref="S2.SS2.p3.8.m8.4.4">𝐶</ci></apply></set></apply><apply id="S2.SS2.p3.8.m8.5.5.3.cmml" xref="S2.SS2.p3.8.m8.5.5.3"><gt id="S2.SS2.p3.8.m8.5.5.3.1.cmml" xref="S2.SS2.p3.8.m8.5.5.3.1"></gt><apply id="S2.SS2.p3.8.m8.5.5.3.2.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2"><csymbol cd="ambiguous" id="S2.SS2.p3.8.m8.5.5.3.2.1.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2">subscript</csymbol><ci id="S2.SS2.p3.8.m8.5.5.3.2.2.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.2">𝑃</ci><apply id="S2.SS2.p3.8.m8.5.5.3.2.3.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3"><times id="S2.SS2.p3.8.m8.5.5.3.2.3.1.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.1"></times><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.2.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.2">𝐺</ci><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.3.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.3">𝑙</ci><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.4.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.4">𝑜</ci><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.5.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.5">𝑏</ci><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.6.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.6">𝑎</ci><ci id="S2.SS2.p3.8.m8.5.5.3.2.3.7.cmml" xref="S2.SS2.p3.8.m8.5.5.3.2.3.7">𝑙</ci></apply></apply><apply id="S2.SS2.p3.8.m8.5.5.3.3.cmml" xref="S2.SS2.p3.8.m8.5.5.3.3"><csymbol cd="ambiguous" id="S2.SS2.p3.8.m8.5.5.3.3.1.cmml" xref="S2.SS2.p3.8.m8.5.5.3.3">subscript</csymbol><ci id="S2.SS2.p3.8.m8.5.5.3.3.2.cmml" xref="S2.SS2.p3.8.m8.5.5.3.3.2">𝑃</ci><ci id="S2.SS2.p3.8.m8.5.5.3.3.3.cmml" xref="S2.SS2.p3.8.m8.5.5.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.8.m8.5c">\forall i\in\left\{1,2,\dots,|C|\right\}:P_{Global}&gt;P_{i}</annotation></semantics></math>. <span id="S2.SS2.p3.9.2" class="ltx_ERROR undefined">\Ac</span>fl permits a certain degree of deviation from the performance of an equivalent centrally trained model but provides data security and privacy protection in return. Still, the goal is to minimize the deviation <math id="S2.SS2.p3.9.m9.1" class="ltx_Math" alttext="|P_{Central}-P_{Global}|" display="inline"><semantics id="S2.SS2.p3.9.m9.1a"><mrow id="S2.SS2.p3.9.m9.1.1.1" xref="S2.SS2.p3.9.m9.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p3.9.m9.1.1.1.2" xref="S2.SS2.p3.9.m9.1.1.2.1.cmml">|</mo><mrow id="S2.SS2.p3.9.m9.1.1.1.1" xref="S2.SS2.p3.9.m9.1.1.1.1.cmml"><msub id="S2.SS2.p3.9.m9.1.1.1.1.2" xref="S2.SS2.p3.9.m9.1.1.1.1.2.cmml"><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.2" xref="S2.SS2.p3.9.m9.1.1.1.1.2.2.cmml">P</mi><mrow id="S2.SS2.p3.9.m9.1.1.1.1.2.3" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.cmml"><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.2" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.3" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1a" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.4" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1b" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.5" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1c" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.6" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1d" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.7" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1e" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.2.3.8" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.8.cmml">l</mi></mrow></msub><mo id="S2.SS2.p3.9.m9.1.1.1.1.1" xref="S2.SS2.p3.9.m9.1.1.1.1.1.cmml">−</mo><msub id="S2.SS2.p3.9.m9.1.1.1.1.3" xref="S2.SS2.p3.9.m9.1.1.1.1.3.cmml"><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.2" xref="S2.SS2.p3.9.m9.1.1.1.1.3.2.cmml">P</mi><mrow id="S2.SS2.p3.9.m9.1.1.1.1.3.3" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.cmml"><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.2" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.3" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1a" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.4" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1b" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.5" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1c" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.6" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1d" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p3.9.m9.1.1.1.1.3.3.7" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.7.cmml">l</mi></mrow></msub></mrow><mo stretchy="false" id="S2.SS2.p3.9.m9.1.1.1.3" xref="S2.SS2.p3.9.m9.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.9.m9.1b"><apply id="S2.SS2.p3.9.m9.1.1.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1"><abs id="S2.SS2.p3.9.m9.1.1.2.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.2"></abs><apply id="S2.SS2.p3.9.m9.1.1.1.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1"><minus id="S2.SS2.p3.9.m9.1.1.1.1.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.1"></minus><apply id="S2.SS2.p3.9.m9.1.1.1.1.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.9.m9.1.1.1.1.2.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.2">𝑃</ci><apply id="S2.SS2.p3.9.m9.1.1.1.1.2.3.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3"><times id="S2.SS2.p3.9.m9.1.1.1.1.2.3.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.1"></times><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.2">𝐶</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.3.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.3">𝑒</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.4.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.4">𝑛</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.5.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.5">𝑡</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.6.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.6">𝑟</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.7.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.7">𝑎</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.2.3.8.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.2.3.8">𝑙</ci></apply></apply><apply id="S2.SS2.p3.9.m9.1.1.1.1.3.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.9.m9.1.1.1.1.3.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.2">𝑃</ci><apply id="S2.SS2.p3.9.m9.1.1.1.1.3.3.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3"><times id="S2.SS2.p3.9.m9.1.1.1.1.3.3.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.1"></times><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.2.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.2">𝐺</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.3.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.3">𝑙</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.4.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.4">𝑜</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.5.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.5">𝑏</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.6.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.6">𝑎</ci><ci id="S2.SS2.p3.9.m9.1.1.1.1.3.3.7.cmml" xref="S2.SS2.p3.9.m9.1.1.1.1.3.3.7">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.9.m9.1c">|P_{Central}-P_{Global}|</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">In the original <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> scheme,  <a href="#id18.18.id18"><span href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated stochastic gradient descent</span></span></a> (<a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a>), proposed by <cite class="ltx_cite ltx_citemacro_citet">Konečný et al<span class="ltx_text">.</span> (<a href="#bib.bib59" title="" class="ltx_ref">2016</a>)</cite>, the clients perform a training step and send the computed gradient back to the central server, which averages the gradient across all clients and applies it to the global model. Since then, several other methods have been proposed in the literature. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib84" title="" class="ltx_ref">McMahan et al<span class="ltx_text">.</span></a></cite> proposed  <a href="#id12.12.id12"><span href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated averaging</span></span></a> (<a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>), where the clients train for multiple local epochs and send their updated local model to the central server instead of the gradient. The updated parameters are then weighted proportionally by the number of local training samples available to each client and then averaged by the central server <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>. Furthermore, they employ client sub-sampling, a technique where only a random subset of clients is selected for each communication round <cite class="ltx_cite ltx_citemacro_citep">(Fraboni et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> can be seen as a generalization of <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a>, which only executes a single iteration of gradient descent in each round of communication <cite class="ltx_cite ltx_citemacro_citep">(Shokri and Shmatikov, <a href="#bib.bib106" title="" class="ltx_ref">2015</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>. Although there were theoretical guarantees for the convergence of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> in cases of heterogeneous data, impractical assumptions such as strong convexity or smoothness of the objective function needed to hold <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib11" title="" class="ltx_ref">Chai et al<span class="ltx_text">.</span></a></cite> showed experimentally, that <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> could lose up to 9% accuracy in comparison to <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, when dealing with non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib68" title="" class="ltx_ref">Li et al<span class="ltx_text">.</span></a></cite> tackled this problem and presented a generalization of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>. They introduced a surrogate objective to constrain the locally updated parameters to be close to the current global model. This helped to stabilize convergence behavior resulting in a significant increase in test accuracy by 22% on average <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib71" title="" class="ltx_ref">Li et al<span class="ltx_text">.</span></a></cite> proposed to only share the trainable parameters of  <a href="#id2.2.id2"><span href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">batch normalization</span></span></a> (<a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a>) with the central server without communicating their running averages of the batch statistics to the server. Aggregating the trainable parameters from all clients but keeping the running averages local helps to alleviate the problem of feature shift in non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> training scenarios <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib53" title="" class="ltx_ref">Karimireddy et al<span class="ltx_text">.</span></a></cite> utilize control variates as a variance reduction technique to approximate the update direction of the server model and each client model. The client drift, which naturally arises from training on different local data distributions, can be estimated by the difference between these update directions and is corrected by adding it in the local training of each client <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib9" title="" class="ltx_ref">Cao et al<span class="ltx_text">.</span></a></cite> rely on clustering the clients according to the classes of data they possess. They only average parameters from the same group while updating the central server model, guaranteeing that parameters are only averaged on a set of clients with a comparable data distribution <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib102" title="" class="ltx_ref">Seol and Kim</a></cite> propose a two step approach. Firstly, they use data oversampling to eliminate data class imbalances among clients. In the second step the clients are selected in such a way, that their data distribution is nearly uniform. Furthermore, the central server constantly adjusts the amount of data for local training, the batch size, and the learning rate of the clients to avoid performance degradation <cite class="ltx_cite ltx_citemacro_citep">(Seol and Kim, <a href="#bib.bib102" title="" class="ltx_ref">2023</a>)</cite>. We also address data heterogeneity and introduce our own generalization of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, named  <a href="#id15.15.id15"><span href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated learning with client queuing</span></span></a> (<a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>).</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Although <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> operates in a decentralized environment, the participating client’s privacy may be compromised by merely transmitting the training update. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib30" title="" class="ltx_ref">Geiping et al<span class="ltx_text">.</span></a></cite> reconstructed high-resolution images by examining the data present in each client’s communicated gradients <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib22" title="" class="ltx_ref">Dimitrov et al<span class="ltx_text">.</span></a></cite> were also able to extract sensitive information contained in the weights obtained by the <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> procedure. Therefore, the concept of differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork and Roth, <a href="#bib.bib24" title="" class="ltx_ref">2014</a>)</cite> is often applied in the setting of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. When working with aggregated data, differential privacy can be utilized to protect the private information contained in individual data points. Differential privacy achieves this data protection by perturbing the data points with random noise. This exploits the fact that a single data point has relatively little impact on the aggregated data as a whole, but adding random noise alters the individual data points to a degree that no useful information can be extracted from them <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib23" title="" class="ltx_ref">2008</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib117" title="" class="ltx_ref">Wei et al<span class="ltx_text">.</span></a></cite> proposed to add specific noise to the parameters of each client before aggregation by the central server <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2020a</a>)</cite>. This ensures a decent training accuracy while a certain level of privacy is maintained, if there are a sufficiently large number of clients involved <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2020a</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Phong et al<span class="ltx_text">.</span> (<a href="#bib.bib92" title="" class="ltx_ref">2018</a>)</cite> proposed to use homomorphic encryption in the more general setting of distributed training and <cite class="ltx_cite ltx_citemacro_citet">Fang and Quan (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> suggested to use it in the setting of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. Homomorphic encryption is a specialized encryption scheme that allows performing certain mathematical operations on the data without decrypting it.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Communication-Efficient Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">When dealing with mobile clients, internet connections may be inconsistent and potentially have high latency. Even when <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients are connected via reliable network connections, mobile connections are usually still bandwidth-constrained and, in many cases, even metered. During the course of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, training updates must be exchanged a multitude of times. Therefore, a central goal in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> is communication minimization. When communicating model parametrizations, possible solutions to this include several size reduction techniques: <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">Sparsification/Pruning</span> excludes single neurons (unstructured) or entire layers of neurons (structured) from an <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a>. While sparsification only sets excluded neurons to 0, pruning actually removes them <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">1990</a>)</cite>. Sparsified models are more amenable to compression, but still have their original size when uncompressed. Pruned models, on the other hand, already have a reduction in size even without compression. The disadvantage of pruned networks is that they may require specialized software and/or hardware to be used, while sparsified models can run on regular software and hardware. <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_bold">Distillation</span> is a technique for transferring the knowledge of a teacher model into a smaller student model. This is done by minimizing the difference between the output of the student model and the output of the teacher model (also known as soft labels) on data points from a separate dataset <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2015</a>)</cite>. In <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_bold">quantization</span> the weights of an <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> are constrained to a discrete set of values so that they can be represented with fewer bits <cite class="ltx_cite ltx_citemacro_citep">(Gholami et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>. <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_bold">Lossless compression</span> techniques encode the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> data in a way that removes redundancy and thus reduces its size <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">There are many works that have developed communication efficient <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> solutions using the above-mentioned techniques or combinations of them <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2019</a>; Konečný et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2016</a>; Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2020</a>)</cite>, and even some with specialized techniques, such as federated dropout <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2018a</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib58" title="" class="ltx_ref">Konečný et al<span class="ltx_text">.</span></a></cite> propose employing quantization, random rotations, and sub-sampling to compress the updated model parameters of the clients before sending them to the central server <cite class="ltx_cite ltx_citemacro_citep">(Konečný et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2016</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib122" title="" class="ltx_ref">Wu et al<span class="ltx_text">.</span></a></cite> adopt an orthogonal strategy: The clients train a teacher model on their local data and distill it into a smaller student model. Instead of communicating the gradients of the teacher models, the clients compress and send the gradients of the smaller student models <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib97" title="" class="ltx_ref">Sattler et al<span class="ltx_text">.</span></a></cite> introduce a compression framework combining communication delay methods, gradient sparsification, binarization, and optimal weight update encoding to reduce the upstream communication cost in distributed learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2019</a>)</cite>. To adapt it to the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib98" title="" class="ltx_ref">Sattler et al<span class="ltx_text">.</span></a></cite> enhance this approach, taking the compression of the downstream communication and the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> local data distribution of the clients into account. They construct a framework combining a novel top-<math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">k</annotation></semantics></math> gradient sparsification method with ternarization and optimal Golomb encoding of updated client model parameters <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2020</a>)</cite>. Another emerging field of research considers combinations of differential privacy and quantization methods in order to reduce communication costs. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib64" title="" class="ltx_ref">Lang and Shlezinger</a></cite> demonstrated that, within their framework, it is possible to quantize data at a given bit rate without sacrificing a specified level of privacy or degrading model performance <cite class="ltx_cite ltx_citemacro_citep">(Lang and Shlezinger, <a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>. They enhanced methods proposed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib93" title="" class="ltx_ref">Reisizadeh et al<span class="ltx_text">.</span></a></cite> and <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib58" title="" class="ltx_ref">Konečný et al<span class="ltx_text">.</span></a></cite>, which solely use quantization and do not include privacy-related considerations.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Federated Recommender Systems</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The current public discussion of <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> (often just referred to as the algorithm or AI personalization), focuses, among other topics, on their invasive behavior concerning personal data collection <cite class="ltx_cite ltx_citemacro_citep">(Hermann, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>; Kozyreva et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021</a>; Golbeck, <a href="#bib.bib33" title="" class="ltx_ref">2016</a>; Lam et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2006</a>)</cite>. This might create a negative relationship between user and <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> potentially resulting in anything from user discontent to “algorithmic hate” <cite class="ltx_cite ltx_citemacro_citep">(Smith et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2022</a>)</cite>. <span id="S2.SS4.p1.1.1" class="ltx_ERROR undefined">\Acp</span>recsys are arguably a vital part of the user experience on the internet since, without them, the flood of content would be barely manageable. Therefore, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> may be part of the solution to the privacy problem of <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> by training the recommender models directly on user devices and thereby entirely circumventing the need for gathering private information.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<span id="S2.SS4.p2.1" class="ltx_ERROR undefined">\Ac</span>
<p id="S2.SS4.p2.2" class="ltx_p">fl has already been proven to work well in many other domains, e.g., cancer research <cite class="ltx_cite ltx_citemacro_citep">(Rønn Hansen et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2022</a>)</cite>, natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2022</a>)</cite>, graph <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>, image classification <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2019</a>)</cite>, transfer learning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2020</a>)</cite>, language models <cite class="ltx_cite ltx_citemacro_citep">(Brendan McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>, mobile keyboard prediction <cite class="ltx_cite ltx_citemacro_citep">(Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite>, and keyword spotting <cite class="ltx_cite ltx_citemacro_citep">(Leroy et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2019</a>)</cite>, so it is reasonable to anticipate that it is likewise effective in the domain of <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>. In fact, there are numerous methods in the literature to incorporate current <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> frameworks into <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. They can be classified as either focusing on learning algorithms <cite class="ltx_cite ltx_citemacro_citep">(Ammad-ud-din et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>, security <cite class="ltx_cite ltx_citemacro_citep">(Ribero et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2022</a>)</cite>, or optimization models <cite class="ltx_cite ltx_citemacro_citep">(Muhammad et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite>, depending on the task’s objective <cite class="ltx_cite ltx_citemacro_citep">(Alamgir et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>. Matrix factorization is a commonly utilized approach in the first scenario. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib4" title="" class="ltx_ref">Ammad-ud-din et al<span class="ltx_text">.</span></a></cite> were among the pioneers in this emerging field by introducing this model to address collaborative filtering tasks in the context of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. They constructed a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> that gives personalized recommendations based on users’ implicit feedback <cite class="ltx_cite ltx_citemacro_citep">(Ammad-ud-din et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib75" title="" class="ltx_ref">Lin et al<span class="ltx_text">.</span></a></cite> designed a new federated rating prediction mechanism for explicit responses. They employed user averaging and hybrid filling in order to keep the system computationally efficient and the communication costs moderately low <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">To increase the model capabilities for each client, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib50" title="" class="ltx_ref">Jia and Lei</a></cite> incorporated a bias term for the input signals. Additionally, weights on the local devices were adjusted, so that any unreasonable user rating is removed <cite class="ltx_cite ltx_citemacro_citep">(Jia and Lei, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>. On the other hand, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib28" title="" class="ltx_ref">Flanagan et al<span class="ltx_text">.</span></a></cite> employed a similar strategy, enhancing the model’s capacity by incorporating input from other data sources <cite class="ltx_cite ltx_citemacro_citep">(Flanagan et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib115" title="" class="ltx_ref">Wang et al<span class="ltx_text">.</span></a></cite> introduced a new algorithmic approach by combining matrix factorization with <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>. They demonstrated, that the cost of communication with the central server for non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data was decreased by limiting the number of local training iterations <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">As previously shown, private information can be reconstructed from the clients’ transmitted parameters. In order to remedy this, a variety of privacy preserving techniques based on encryption, obfuscation, or masking can be utilized <cite class="ltx_cite ltx_citemacro_citep">(Asad et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. Communication of encrypted data between the central server and its clients is made possible through the use of homomorphic encryption, allowing for intermediate calculations without the need to first decrypt the data. As a result, the central server is unable to infer the data it is working with <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2018</a>)</cite>. For this reason, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib12" title="" class="ltx_ref">Chai et al<span class="ltx_text">.</span></a></cite> propose a secure matrix factorization framework to handle data leakage. They showed how privacy could be compromised by intercepting the clients’ gradient updates sent in two consecutive communication rounds to the central server. To address this problem, they encrypted the clients’ gradients before sending them to the central server <cite class="ltx_cite ltx_citemacro_citep">(Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib132" title="" class="ltx_ref">Zhang and Jiang</a></cite> enhanced the approach by clustering the encrypted user embeddings to reduce the dimension of the user-item matrix, improving the recommendation’s accuracy <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Jiang, <a href="#bib.bib132" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib76" title="" class="ltx_ref">Lin et al<span class="ltx_text">.</span></a></cite> utilized a different cryptographic technique: they applied secret sharing, wherein a group of clients can only reconstruct sensitive information if they collaborate by combining their shares <cite class="ltx_cite ltx_citemacro_citep">(Shamir, <a href="#bib.bib104" title="" class="ltx_ref">1979</a>)</cite>. By applying this concept to the clients’ locally computed gradients, the authors managed to construct a <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> framework that provides strong privacy guarantees on the clients’ individual data <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021b</a>)</cite>. Another technique concerns secure multi-party computation, that refers to a protocol for computing a function based on the data of a group of clients without disclosing private information to one another <cite class="ltx_cite ltx_citemacro_citep">(Cramer et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib91" title="" class="ltx_ref">Perifanis and Efraimidis</a></cite> utilized this approach in the setting of federated  <a href="#id32.32.id32"><span href="#id32.32.id32" title="neural collaborative filtering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">neural collaborative filtering</span></span></a> (<a href="#id32.32.id32"><abbr href="#id32.32.id32" title="neural collaborative filtering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NCF</span></abbr></a>). They demonstrated that employing a secure multi-party computation protocol for <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> protects privacy when dealing with an honest but curious entity without compromising the quality of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Perifanis and Efraimidis, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Differential privacy falls in the category of privacy preservation techniques that use obfuscation. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib94" title="" class="ltx_ref">Ribero et al<span class="ltx_text">.</span></a></cite> added differential privacy to <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> utilizing a matrix factorization technique. They succeeded in balancing the privacy loss posed by the repetitive nature of the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> process by only requiring a few rounds of communication <cite class="ltx_cite ltx_citemacro_citep">(Ribero et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib126" title="" class="ltx_ref">Yang et al<span class="ltx_text">.</span></a></cite> designed a matrix factorization-based <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> that adds Laplacian random noise to the users’ encrypted item embeddings, ensuring a high level of security <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib85" title="" class="ltx_ref">Minto et al<span class="ltx_text">.</span></a></cite> proposed a system combining differential privacy and implicit user feedback. They constrained the number of local gradient updates sent by the users by the level of privacy each user tries to maintain <cite class="ltx_cite ltx_citemacro_citep">(Minto et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2021</a>)</cite>. We also address the problem of privacy preservation by obfuscation: Instead of applying random noise to the weight updates that are sent to the central server, the weights are quantized, which is both conducive to privacy preservation and reducing the communication overhead. We later provide a detailed attack analysis of the exchanged model parameters, that are potentially susceptible to leak information about the underlying datasets of the participating clients. We present specific attacks applicable to our scenario and examine how their requirements and assumptions do not apply to our approach to privacy preservation, thus rendering them ineffective.</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<p id="S2.SS4.p6.1" class="ltx_p">Another method of achieving data security is by introducing pseudo interactions in order to mask user behavior in <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a>. This protection mechanism is implemented by adding artificial interactions with randomly selected items to users. This causes the central server to be unable to determine the real set of items a user has interacted with, as the uploaded gradient was computed with respect to both real and artificial interactions. <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021a</a>)</cite>. Since this method produces noisy gradients, degrading the model performance, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib72" title="" class="ltx_ref">Liang et al<span class="ltx_text">.</span></a></cite> introduced denoising clients in the training process <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2021</a>)</cite>. Another approach that hits the same mark, but entirely foregoes <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> was presented by <cite class="ltx_cite ltx_citemacro_citet">Wainakh et al<span class="ltx_text">.</span> (<a href="#bib.bib113" title="" class="ltx_ref">2019</a>)</cite>. They employ a random walk-based approach to decentralized optimization, where a randomly chosen client trains its local model for one or multiple epochs before sending its updated parameters to a randomly selected neighboring client according to the underlying graph structure <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2022</a>; Triastcyn et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib113" title="" class="ltx_ref">Wainakh et al<span class="ltx_text">.</span></a></cite> adapt this approach to account for privacy by introducing the anonymous random walk technique where clients, instead of training a model, can choose to add their own data to an existing dataset that was sent by a neighboring client in a prior round. The accumulated data can then be uploaded to the central server for centralized training. Due to the nature of the random walk, neither the clients nor the central server know where the individual samples of the accumulated dataset originate from, thus effectively masking the users’ identities.</p>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<p id="S2.SS4.p7.1" class="ltx_p">Dealing with the statistical heterogeneity of the clients’ local data in the context of <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a> is a different area of research. There are various proposed strategies for addressing this issue, which primarily include clustering and meta learning <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib51" title="" class="ltx_ref">Jie et al<span class="ltx_text">.</span></a></cite> designed a <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> utilizing a clustering approach based on historical parameters to form homogeneous groups of clients, in which a personalized model can be trained. These parameters are retrieved by averaging the model parameters from the clients’ last communication rounds with the central server <cite class="ltx_cite ltx_citemacro_citep">(Jie et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib13" title="" class="ltx_ref">Chen et al<span class="ltx_text">.</span></a></cite> proposed a different method based on model-agnostic meta-learning, which is a training paradigm where a meta-learner is employed to rapidly train models on new tasks. The meta-learner itself is a trainable algorithm that trains a model on a task, which consists of a support set and a query set. The model is trained using the support set and then evaluated on the query set. Based on this evaluation, a loss is computed, which reflects the ability of the meta-learner to train the model. The meta-learner is then updated to minimize this loss. For example, the meta-learner in the  <a href="#id28.28.id28"><span href="#id28.28.id28" title="model-agnostic meta-learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">model-agnostic meta-learning</span></span></a> (<a href="#id28.28.id28"><abbr href="#id28.28.id28" title="model-agnostic meta-learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MAML</span></abbr></a>) <cite class="ltx_cite ltx_citemacro_citep">(Finn et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> algorithm is used to provide an initial set of parameters for the model that is trained on the task. Meta-learning algorithms are known to generalize effectively to new tasks, which makes them well-suited for tackling the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> problem in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. For this reason, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib13" title="" class="ltx_ref">Chen et al<span class="ltx_text">.</span></a></cite> adapted <a href="#id28.28.id28"><abbr href="#id28.28.id28" title="model-agnostic meta-learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MAML</span></abbr></a>, as well as another meta-learning algorithm called Meta-SGD, to the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting, which enabled them to reach higher model performance than the <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> baseline <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>. Our <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> was not only affected by heterogeneous client data but also by exceedingly small local datasets. Our approach to non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, therefore differs greatly from the two above-mentioned approaches, as neither clustering nor meta-learning are capable of handling truly small local datasets.</p>
</div>
<div id="S2.SS4.p8" class="ltx_para">
<p id="S2.SS4.p8.1" class="ltx_p">The clients’ potentially constrained resources are the subject of another line of research. Therefore, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib87" title="" class="ltx_ref">Muhammad et al<span class="ltx_text">.</span></a></cite> utilized a simple <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> with small embedding sizes to balance the number of learnable parameters and the accuracy of the resulting recommendations. Additionally, they presented a new sampling technique coupled with an active aggregation method, which reduced communication costs and produced more accurate models even at an early stage of training <cite class="ltx_cite ltx_citemacro_citep">(Muhammad et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib131" title="" class="ltx_ref">Zhang et al<span class="ltx_text">.</span></a></cite> addressed related problems and developed a new framework that effectively integrates a novel matrix factorization technique with privacy via a federated discrete optimization algorithm. Although the model’s RAM, storage, and communication bandwidth requirements were modest, performance was not affected and was even superior to related state of the art techniques <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2022</a>)</cite>. Our suggested approach combines all three of the aforementioned sorts of objectives: We balance the model complexity and capacity by opting for a simple, yet scalable <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> architecture. This results in remaining resource-efficient on the client side, while still maintaining the possibility of scaling up. Additionally, we anticipate that applying quantization will provide a specific amount of privacy while also lowering the burden associated with exchanging parameters with the central server via potentially bandwidth-constrained network connections.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we propose a framework for a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> that is trained end-to-end using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. Before examining the design of the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> and its components, we want to motivate our decisions with a problem statement. Then, we will explore the general architecture of many complex information retrieval systems on which the architecture of our <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> is based and show how each of these components is constructed. Finally, we will demonstrate how all of this translates into an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting and how we alleviate the problems that arise from such a setup.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Problem Statement</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The research documented in this work was conducted as part of the COPA EUROPE project, which is a beneficiary of the <a href="#id11.11.id11"><abbr href="#id11.11.id11" title="European Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EU’s</span></abbr></a> Horizon 2020 Research and Innovation Programme. The project aims to create a live-streaming and  <a href="#id45.45.id45"><span href="#id45.45.id45" title="video-on-demand" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">video-on-demand</span></span></a> (<a href="#id45.45.id45"><abbr href="#id45.45.id45" title="video-on-demand" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VoD</span></abbr></a>) platform that provides users with sports and esports content. To keep users engaged, discoverability of the content is key, therefore, one part of the project aims at developing a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>. Specifically, the objective was to develop a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting to provide high-quality recommendations while preserving the user’s privacy. From the project’s goals and objectives, the following requirements for the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> can be derived:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Large Client Population</span> – A live-streaming and <a href="#id45.45.id45"><abbr href="#id45.45.id45" title="video-on-demand" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VoD</span></abbr></a> platform for sports and esports may build a large user base, which results in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client population that comprises hundreds of thousands or even millions of clients.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Large Video Catalog</span> – With dozens of types of sports and esports games covered, and hundreds of leagues, tournaments and events, the catalog of live streams and <a href="#id45.45.id45"><abbr href="#id45.45.id45" title="video-on-demand" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VoD</span></abbr></a> content may grow substantially over time.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Increased Personalization</span> – The <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setup is meant to enable the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> to leverage more personal user data in addition to user-item interactions for higher personalization without requiring the data to ever leave the user’s device. The requirement to take advantage of more personal user data implies that the employed <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> model must be able to handle multiple data modalities and learn complex, non-linear dependencies between features contained in this data.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Substantial Communication Overhead</span> – The potentially large client population leads to a very significant communication overhead for the central server. Furthermore, the clients are expected to use mobile devices that may lack a reliable, high-bandwidth internet connection. Therefore, it is of paramount importance to reduce the communication overhead incurred by the constant communication between the central server and its clients.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The following sections will detail how these requirements were translated into the architecture of the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> and the design of its components. All decisions concerning architecture and design, as well as the research into privacy-preservation, scalability, <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a>, and the handling of non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> and imbalanced local datasets were motivated and informed by these requirements.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Recommender System Architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> is required to handle a large user base and movie catalog, we decided to follow the well-known three-stage funnel-like architecture, which is also employed by other forms of information retrieval systems. These three phases comprise: candidate generation, ranking, and re-ranking (cf. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2. Recommender System Architecture ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The candidate generation phase takes the entire corpus of movies and narrows it down to usually a couple hundred movies that are somewhat relevant to the user. This phase must be fast because it must sift through possibly millions of movies, which in turn means that not all of the resulting elements are 100% relevant to the user. The ranking phase has a more complex model of the user’s interest. It scores each of the candidate movies and ranks them by their scores. This two-step approach to the generation of recommendations greatly expedites the retrieval process. If each item in the corpus had to be ranked individually, this process would not scale well to the large item corpora. Finally, the re-ranking phase is an optional phase, which can implement hand-crafted rules to improve recommendations. This can include rules such as removing click-bait content, enforcing age restrictions, ensuring freshness, and promoting predefined content. These systems will be further explored in the following sections.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="231" height="162" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Flow diagram of the “funnel-like” three-stage <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> architecture of the proposed <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, consisting of candidate generation, ranking, and re-ranking stages (inspired by Figure 2 in <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F1.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F1.5" class="ltx_p ltx_figure_panel ltx_align_center">Flow diagram of the “funnel-like” three-stage <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> architecture of the proposed <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, consisting of candidate generation, ranking, and re-ranking stages (inspired by Figure 2 in <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>).</p>
</div>
</div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Candidate Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Candidate generation is comprised of an algorithm that is trained to select a small number (usually in the order of hundreds) of items from a vast corpus of items (usually in the order of millions) that are generally relevant to the user. One classical approach to candidate generation is matrix factorization. Non-linear models, such as <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a>, however, are capable of forming a much deeper “understanding” of the latent structures in the data and <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> have been used in <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> since at least 2016 <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. Although there have been attempts to adapt classical <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> algorithms for the use in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, e.g., matrix factorization <cite class="ltx_cite ltx_citemacro_citep">(Ammad-ud-din et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>, gradient-based learning algorithms are much better suited and well-researched within the framework of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. Furthermore, <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> allow for much more fine-grained control over model architecture decisions and are capable of handling a diverse set of input data modalities, which is one of the project’s requirements. For this reason, we decided to use a <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> architecture for our candidate generation model.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Prior to choosing a specific design, the training objective must be formulated. For <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a> there are many different objectives that are commonly used, e.g., rating prediction, watch time prediction, click-through-rate prediction, and watch prediction. Since the algorithm has to be able to sift through millions of items, the underlying model must be simple and, most importantly, fast. Therefore, we decided to train the candidate generation model on next watch prediction. This means that it receives a list of past movie watches of a user as input and predicts a probability distribution over all movies in the corpus. The top-<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">k</annotation></semantics></math> movies can then be interpreted as the movies that the user will most likely watch next. So instead of performing inference on all movies in the corpus, the model only has to be invoked once to retrieve a list of candidate recommendations.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The chosen architecture for the candidate generator model is shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3. Candidate Generation ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and inspired by the architecture used in <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. An experiment using various recurrent architectures was conducted, but the chosen <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> architecture is the best tradeoff between model performance and size. The results of this experiment can be found in Appendix <a href="#A2.SS1" title="B.1. Model Type Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. The first layer of the model is an embedding layer, which takes the sparse one-hot encoded movie watches and embeds them into a 64-dimensional dense vector space. The size of the embedding vectors was experimentally determined. The experiment results can be found in Appendix <a href="#A2.SS2" title="B.2. Movie Embedding Layer Size Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>. In contrast to recurrent <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a>, non-recurrent <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> require inputs of a fixed size. However, the watch histories have variable length and can consist of any number between 1 and <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">window size</em> movie watches. To provide the required fixed-length input for the model, the embedded movie watches are then averaged. In practice, other input features could be added here and concatenated to the watch history vector. For example, user-level information could be utilized to improve predictions, if past movie watches are not available or a user only has a few of them, thereby solving the cold-start problem for new users. Unfortunately, we are restrained by the lack of a suitable dataset, which includes user-level information.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The inputs are then fed into a funnel, or tower-like architecture of multiple fully-connected layers with  <a href="#id39.39.id39"><span href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">rectified linear unit</span></span></a> (<a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a>) activations. The final fully-connected layer prior to the output layer is of size 256 and each preceding layer doubles this number, i.e., for a three-layer architecture, the first fully-connected layer is of size 1024, the second of size 512, and the final layer of size 256. As already mentioned, the size of the model has a substantial impact in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting. Consequently, an experiment was conducted to determine the optimal number of hidden layers. The results of this experiment are presented in Appendix <a href="#A2.SS3" title="B.3. Number of Hidden Layers Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Finally, the next-watch prediction is realized in terms of a classification task, therefore, the output layer of the candidate generator model has as many outputs as there are movies in the corpus. The model is then trained using the softmax cross-entropy loss. A detailed breakdown of the layers that comprise the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture of the candidate generator model is presented in Table <a href="#A2.T3" title="Table 3 ‣ B.4. Candidate Generator Model Architecture ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in Appendix <a href="#A2.SS4" title="B.4. Candidate Generator Model Architecture ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2303.04689/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="111" height="215" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S3.F2.4.2" class="ltx_ERROR undefined">\Ac</span><span id="S3.F2.5.3" class="ltx_text" style="font-size:90%;">dnn candidate generator model architecture of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S3.F2.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S3.F2.7" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F2.8" class="ltx_p ltx_figure_panel ltx_align_center">dnn candidate generator model architecture of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Ranking</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The ranking phase of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> receives the candidate recommendations from the candidate generator phase and ranks them by user relevance. Since it only has to be invoked for a small subset of all movies in the corpus, processing speed is less crucial in contrast to the previous candidate generator model. Therefore, a more precise and complex representation of the user’s interests can be learned. Note that the model must be trained within the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> environment and thus should not be selected too large.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Learning to rank is a well-studied <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2007</a>)</cite> problem within <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> and there are numerous approaches, ranging from simple point-wise models, that directly predict a rank, and pair-wise models, which learn to rank two items relative to each other, to more elaborate list-wise models, which learn to rank items in a list <cite class="ltx_cite ltx_citemacro_citep">(Liu, <a href="#bib.bib77" title="" class="ltx_ref">2009</a>)</cite>. In the case of a movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, the ranker model can be implemented as a rating prediction, where the predicted rating is used to sort the items. We decided on this simple approach. It turned out that a simple regression model tended to learn to predict the mean rating if trained without any constraints. Therefore, we decided to re-formulate the problem as a classification task, as the dataset being used contains a discrete set of possible ratings between 0.5 and 5.0 in steps of 0.5, resulting in 10 distinct classes. This approach performs considerably better.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">The base architecture of the ranker model is almost equivalent to the design of the candidate generator. The input features, user ID, movie ID, and movie genres are embedded using embedding layers. The optimal embedding sizes were experimentally determined to be 32 for users, 128 for movies, and 16 for genres. A detailed description of these experiments can be found in Appendix <a href="#A3.SS1" title="C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>. The genre embeddings are then averaged and the resulting vectors are concatenated to form the input of a tower-like classifier, which consists of a single fully-connected layer that outputs a probability distribution over the set of possible ratings. Just like in the case of the candidate generator, we considered adding multiple hidden layers, but experiments with varying numbers of hidden layers determined that a single layer is sufficient. The hidden layer experiments are described in Appendix <a href="#A3.SS2" title="C.2. Number of Hidden Layers Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.2</span></a>. Again, more movie-level or user-level information could be added as input features here. Only rating timestamps are provided in the dataset which can be utilized as additional user-level information. By correlating the movies in the dataset with an online movie database, further movie-level information can be retrieved. Therefore, we decided to add the age of the rating and the age of the movie as further input features to determine the efficacy of adding more input signals to the model. A detailed discussion of this can be found in Section <a href="#S4.SS1.SSS2" title="4.1.2. Dataset Preprocessing ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>. The architecture of the ranker model is shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.4. Ranking ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2303.04689/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="214" height="215" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S3.F3.4.2" class="ltx_ERROR undefined">\Ac</span><span id="S3.F3.5.3" class="ltx_text" style="font-size:90%;">dnn ranker model architecture of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S3.F3.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S3.F3.7" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.8" class="ltx_p ltx_figure_panel ltx_align_center">dnn ranker model architecture of the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>.</p>
</div>
</div>
</figure>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Since the classes, distinguished by the ranker model, have a hierarchical relation to each other, we considered using other loss functions than softmax cross-entropy. We have experimentally tested other loss functions, but in practice, softmax cross-entropy provides the best results. The results of the experiment can be found in Appendix <a href="#A3.SS3" title="C.3. Loss Function Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.3</span></a>. A detailed breakdown of the layers that comprise the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture of the ranker model is presented in Table <a href="#A3.T4" title="Table 4 ‣ C.4. Ranker Model Architecture ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a href="#A3.SS4" title="C.4. Ranker Model Architecture ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.4</span></a>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Re-ranking</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The re-ranking phase is an optional step that is often overlooked in <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> research, but plays a crucial role in real-world applications. It implements hand-crafted rules to improve recommendations. Examples are the removal of click-bait content, enforcing age restrictions, ensuring freshness, and promoting predefined content. Here, ensuring freshness is probably one of the most important aspects. The candidate generation and ranking phases do not take freshness of the recommended content into consideration, as the ratio between novel and more established content is often hand-tuned (also described as <em id="S3.SS5.p1.1.1" class="ltx_emph ltx_font_italic">exploration</em> vs. <em id="S3.SS5.p1.1.2" class="ltx_emph ltx_font_italic">exploitation</em> trade-off). Age restrictions are also important, as the candidate generator model has no filter in place to prevent recommending age-restricted movies to underage users. Both the candidate generator and the ranker models are static, i.e., given the same input, they will always produce the same output (unless further trained in the meantime). Therefore, the re-ranking phase should also randomly select a subset of the final recommendations, e.g., weighted by the rank predicted by the ranker model, in order to ensure that the user will see something different every time they are presented with recommendations. Mixing in some predefined content, for example movies that have just been released, is an effective way of overcoming the cold-start problem for new content. This would increase the chances of new movies being watched and thus generating training data that can be used to recommend the movies later. Finally, the topic of click-bait detection is an interesting one, but it is considered out-of-scope in this work. As the re-ranking phase only consists of hand-crafted rules and thus does not affect the proposed method, we will abstain from delving deeper into its implementation.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Federated Recommender Systems at Scale Using Queue-Based Federated Learning</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Many variants and adaptations were introduced to <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, among which <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite> is one of the most prevalent. In <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, the server initializes a global model, which is sent to all clients. The clients then proceed to train the model on their local data and send the updated model back to the central server. The central server then aggregates the client models into a new global model by averaging them (usually the mean weighted by the number of samples that the clients trained on is used). The process can be seen in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.6. Federated Recommender Systems at Scale Using Queue-Based Federated Learning ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the algorithm is detailed in Algorithm <a href="#algorithm1" title="In 3.6. Federated Recommender Systems at Scale Using Queue-Based Federated Learning ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. <span id="S3.SS6.p1.1.1" class="ltx_ERROR undefined">\Ac</span>fedavg has been proven successful in many <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> tasks despite theoretical predictions suggesting otherwise <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib114" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="299" height="145" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The typical <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> scenario with a central coordinating server and several clients with their local data. The central server sends a global model to the clients, which then perform training on local data. The resulting updated local models are sent back to the central server, which aggregates them into a new global model by averaging the model weights.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F4.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F4.5" class="ltx_p ltx_figure_panel ltx_align_center">The typical <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> scenario with a central coordinating server and several clients with their local data. The central server sends a global model to the clients, which then perform training on local data. The resulting updated local models are sent back to the central server, which aggregates them into a new global model by averaging the model weights.</p>
</div>
</div>
</figure>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.28" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.28.29" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.29.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span><span id="algorithm1.28.29.2" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="algorithm1.28.30" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.30.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</div>
<div id="algorithm1.28.31" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.31.1.1.1" class="ltx_text" style="font-size:90%;">3</span></span><span id="algorithm1.28.31.2" class="ltx_text" style="font-size:90%;">







</span>
</div>
<div id="algorithm1.8.8" class="ltx_listingline">
<span id="algorithm1.8.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input :</span><span id="algorithm1.8.8.2" class="ltx_text" style="font-size:90%;"> 
</span><math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mi mathsize="90%" id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><ci id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">C</annotation></semantics></math><span id="algorithm1.8.8.3" class="ltx_text" style="font-size:90%;"> is the set of all clients, </span><math id="algorithm1.2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="algorithm1.2.2.m2.1a"><msub id="algorithm1.2.2.m2.1.1" xref="algorithm1.2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.2.2.m2.1.1.2" xref="algorithm1.2.2.m2.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.2.2.m2.1.1.3" xref="algorithm1.2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m2.1b"><apply id="algorithm1.2.2.m2.1.1.cmml" xref="algorithm1.2.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.2.2.m2.1.1.1.cmml" xref="algorithm1.2.2.m2.1.1">subscript</csymbol><ci id="algorithm1.2.2.m2.1.1.2.cmml" xref="algorithm1.2.2.m2.1.1.2">𝒟</ci><ci id="algorithm1.2.2.m2.1.1.3.cmml" xref="algorithm1.2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m2.1c">\mathcal{D}_{i}</annotation></semantics></math><span id="algorithm1.8.8.4" class="ltx_text" style="font-size:90%;"> is the local dataset of client </span><math id="algorithm1.3.3.m3.1" class="ltx_Math" alttext="c_{i}\in C" display="inline"><semantics id="algorithm1.3.3.m3.1a"><mrow id="algorithm1.3.3.m3.1.1" xref="algorithm1.3.3.m3.1.1.cmml"><msub id="algorithm1.3.3.m3.1.1.2" xref="algorithm1.3.3.m3.1.1.2.cmml"><mi mathsize="90%" id="algorithm1.3.3.m3.1.1.2.2" xref="algorithm1.3.3.m3.1.1.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm1.3.3.m3.1.1.2.3" xref="algorithm1.3.3.m3.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="algorithm1.3.3.m3.1.1.1" xref="algorithm1.3.3.m3.1.1.1.cmml">∈</mo><mi mathsize="90%" id="algorithm1.3.3.m3.1.1.3" xref="algorithm1.3.3.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m3.1b"><apply id="algorithm1.3.3.m3.1.1.cmml" xref="algorithm1.3.3.m3.1.1"><in id="algorithm1.3.3.m3.1.1.1.cmml" xref="algorithm1.3.3.m3.1.1.1"></in><apply id="algorithm1.3.3.m3.1.1.2.cmml" xref="algorithm1.3.3.m3.1.1.2"><csymbol cd="ambiguous" id="algorithm1.3.3.m3.1.1.2.1.cmml" xref="algorithm1.3.3.m3.1.1.2">subscript</csymbol><ci id="algorithm1.3.3.m3.1.1.2.2.cmml" xref="algorithm1.3.3.m3.1.1.2.2">𝑐</ci><ci id="algorithm1.3.3.m3.1.1.2.3.cmml" xref="algorithm1.3.3.m3.1.1.2.3">𝑖</ci></apply><ci id="algorithm1.3.3.m3.1.1.3.cmml" xref="algorithm1.3.3.m3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m3.1c">c_{i}\in C</annotation></semantics></math><span id="algorithm1.8.8.5" class="ltx_text" style="font-size:90%;">, </span><math id="algorithm1.4.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="algorithm1.4.4.m4.1a"><mi mathsize="90%" id="algorithm1.4.4.m4.1.1" xref="algorithm1.4.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m4.1b"><ci id="algorithm1.4.4.m4.1.1.cmml" xref="algorithm1.4.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m4.1c">T</annotation></semantics></math><span id="algorithm1.8.8.6" class="ltx_text" style="font-size:90%;"> is the number of communication rounds, </span><math id="algorithm1.5.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="algorithm1.5.5.m5.1a"><mi mathsize="90%" id="algorithm1.5.5.m5.1.1" xref="algorithm1.5.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m5.1b"><ci id="algorithm1.5.5.m5.1.1.cmml" xref="algorithm1.5.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m5.1c">N</annotation></semantics></math><span id="algorithm1.8.8.7" class="ltx_text" style="font-size:90%;"> is the number of clients per communication round, </span><math id="algorithm1.6.6.m6.1" class="ltx_Math" alttext="B" display="inline"><semantics id="algorithm1.6.6.m6.1a"><mi mathsize="90%" id="algorithm1.6.6.m6.1.1" xref="algorithm1.6.6.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m6.1b"><ci id="algorithm1.6.6.m6.1.1.cmml" xref="algorithm1.6.6.m6.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m6.1c">B</annotation></semantics></math><span id="algorithm1.8.8.8" class="ltx_text" style="font-size:90%;"> is the batch size, </span><math id="algorithm1.7.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="algorithm1.7.7.m7.1a"><mi mathsize="90%" id="algorithm1.7.7.m7.1.1" xref="algorithm1.7.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m7.1b"><ci id="algorithm1.7.7.m7.1.1.cmml" xref="algorithm1.7.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m7.1c">E</annotation></semantics></math><span id="algorithm1.8.8.9" class="ltx_text" style="font-size:90%;"> is the number of local epochs, and </span><math id="algorithm1.8.8.m8.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="algorithm1.8.8.m8.1a"><mi mathsize="90%" id="algorithm1.8.8.m8.1.1" xref="algorithm1.8.8.m8.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m8.1b"><ci id="algorithm1.8.8.m8.1.1.cmml" xref="algorithm1.8.8.m8.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m8.1c">\eta</annotation></semantics></math><span id="algorithm1.8.8.10" class="ltx_text" style="font-size:90%;"> is the learning rate
</span>
</div>
<div id="algorithm1.9.9" class="ltx_listingline">
<span id="algorithm1.9.9.1" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm1.9.9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Output :</span><span id="algorithm1.9.9.3" class="ltx_text" style="font-size:90%;"> Global Model Parametrization </span><math id="algorithm1.9.9.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{T}" display="inline"><semantics id="algorithm1.9.9.m1.1a"><msub id="algorithm1.9.9.m1.1.1" xref="algorithm1.9.9.m1.1.1.cmml"><mi mathsize="90%" id="algorithm1.9.9.m1.1.1.2" xref="algorithm1.9.9.m1.1.1.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm1.9.9.m1.1.1.3" xref="algorithm1.9.9.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m1.1b"><apply id="algorithm1.9.9.m1.1.1.cmml" xref="algorithm1.9.9.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1">subscript</csymbol><ci id="algorithm1.9.9.m1.1.1.2.cmml" xref="algorithm1.9.9.m1.1.1.2">𝜽</ci><ci id="algorithm1.9.9.m1.1.1.3.cmml" xref="algorithm1.9.9.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m1.1c">\boldsymbol{\theta}_{T}</annotation></semantics></math>
</div>
<div id="algorithm1.28.32" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.32.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span><span id="algorithm1.28.32.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.28.33" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.33.1.1.1" class="ltx_text" style="font-size:90%;">5</span></span><span id="algorithm1.28.33.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.28.34" class="ltx_listingline">
<span id="algorithm1.28.34.1" class="ltx_text" style="font-size:90%;"><span id="algorithm1.28.34.1.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm1.28.34.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">Runs on the central server */</span>
</div>
<div id="algorithm1.10.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.10.10.1.1.1" class="ltx_text" style="font-size:90%;">6</span></span><span id="algorithm1.10.10.2" class="ltx_text" style="font-size:90%;">
Initialize </span><math id="algorithm1.10.10.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{0}" display="inline"><semantics id="algorithm1.10.10.m1.1a"><msub id="algorithm1.10.10.m1.1.1" xref="algorithm1.10.10.m1.1.1.cmml"><mi mathsize="90%" id="algorithm1.10.10.m1.1.1.2" xref="algorithm1.10.10.m1.1.1.2.cmml">𝜽</mi><mn mathsize="90%" id="algorithm1.10.10.m1.1.1.3" xref="algorithm1.10.10.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m1.1b"><apply id="algorithm1.10.10.m1.1.1.cmml" xref="algorithm1.10.10.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.1.1.1.cmml" xref="algorithm1.10.10.m1.1.1">subscript</csymbol><ci id="algorithm1.10.10.m1.1.1.2.cmml" xref="algorithm1.10.10.m1.1.1.2">𝜽</ci><cn type="integer" id="algorithm1.10.10.m1.1.1.3.cmml" xref="algorithm1.10.10.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m1.1c">\boldsymbol{\theta}_{0}</annotation></semantics></math>
</div>
<div id="algorithm1.11.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.11.11.1.1.1" class="ltx_text" style="font-size:90%;">7</span></span><span id="algorithm1.11.11.2" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm1.11.11.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm1.11.11.4" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.11.11.5" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm1.11.11.6" class="ltx_text" style="font-size:90%;"> communication round </span><math id="algorithm1.11.11.m1.3" class="ltx_Math" alttext="t=1,\dots,T" display="inline"><semantics id="algorithm1.11.11.m1.3a"><mrow id="algorithm1.11.11.m1.3.4" xref="algorithm1.11.11.m1.3.4.cmml"><mi mathsize="90%" id="algorithm1.11.11.m1.3.4.2" xref="algorithm1.11.11.m1.3.4.2.cmml">t</mi><mo mathsize="90%" id="algorithm1.11.11.m1.3.4.1" xref="algorithm1.11.11.m1.3.4.1.cmml">=</mo><mrow id="algorithm1.11.11.m1.3.4.3.2" xref="algorithm1.11.11.m1.3.4.3.1.cmml"><mn mathsize="90%" id="algorithm1.11.11.m1.1.1" xref="algorithm1.11.11.m1.1.1.cmml">1</mn><mo mathsize="90%" id="algorithm1.11.11.m1.3.4.3.2.1" xref="algorithm1.11.11.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm1.11.11.m1.2.2" xref="algorithm1.11.11.m1.2.2.cmml">…</mi><mo mathsize="90%" id="algorithm1.11.11.m1.3.4.3.2.2" xref="algorithm1.11.11.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="algorithm1.11.11.m1.3.3" xref="algorithm1.11.11.m1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m1.3b"><apply id="algorithm1.11.11.m1.3.4.cmml" xref="algorithm1.11.11.m1.3.4"><eq id="algorithm1.11.11.m1.3.4.1.cmml" xref="algorithm1.11.11.m1.3.4.1"></eq><ci id="algorithm1.11.11.m1.3.4.2.cmml" xref="algorithm1.11.11.m1.3.4.2">𝑡</ci><list id="algorithm1.11.11.m1.3.4.3.1.cmml" xref="algorithm1.11.11.m1.3.4.3.2"><cn type="integer" id="algorithm1.11.11.m1.1.1.cmml" xref="algorithm1.11.11.m1.1.1">1</cn><ci id="algorithm1.11.11.m1.2.2.cmml" xref="algorithm1.11.11.m1.2.2">…</ci><ci id="algorithm1.11.11.m1.3.3.cmml" xref="algorithm1.11.11.m1.3.3">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m1.3c">t=1,\dots,T</annotation></semantics></math><span id="algorithm1.11.11.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.11.11.8" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm1.13.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.13.13.1.1.1" class="ltx_text" style="font-size:90%;">8</span></span><span id="algorithm1.13.13.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.13.13.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.13.13.4" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm1.12.12.m1.1" class="ltx_Math" alttext="C_{t}\leftarrow N" display="inline"><semantics id="algorithm1.12.12.m1.1a"><mrow id="algorithm1.12.12.m1.1.1" xref="algorithm1.12.12.m1.1.1.cmml"><msub id="algorithm1.12.12.m1.1.1.2" xref="algorithm1.12.12.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm1.12.12.m1.1.1.2.2" xref="algorithm1.12.12.m1.1.1.2.2.cmml">C</mi><mi mathsize="90%" id="algorithm1.12.12.m1.1.1.2.3" xref="algorithm1.12.12.m1.1.1.2.3.cmml">t</mi></msub><mo mathsize="90%" stretchy="false" id="algorithm1.12.12.m1.1.1.1" xref="algorithm1.12.12.m1.1.1.1.cmml">←</mo><mi mathsize="90%" id="algorithm1.12.12.m1.1.1.3" xref="algorithm1.12.12.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m1.1b"><apply id="algorithm1.12.12.m1.1.1.cmml" xref="algorithm1.12.12.m1.1.1"><ci id="algorithm1.12.12.m1.1.1.1.cmml" xref="algorithm1.12.12.m1.1.1.1">←</ci><apply id="algorithm1.12.12.m1.1.1.2.cmml" xref="algorithm1.12.12.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.2.1.cmml" xref="algorithm1.12.12.m1.1.1.2">subscript</csymbol><ci id="algorithm1.12.12.m1.1.1.2.2.cmml" xref="algorithm1.12.12.m1.1.1.2.2">𝐶</ci><ci id="algorithm1.12.12.m1.1.1.2.3.cmml" xref="algorithm1.12.12.m1.1.1.2.3">𝑡</ci></apply><ci id="algorithm1.12.12.m1.1.1.3.cmml" xref="algorithm1.12.12.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m1.1c">C_{t}\leftarrow N</annotation></semantics></math><span id="algorithm1.13.13.5" class="ltx_text" style="font-size:90%;"> random clients sub-sampled from </span><math id="algorithm1.13.13.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="algorithm1.13.13.m2.1a"><mi mathsize="90%" id="algorithm1.13.13.m2.1.1" xref="algorithm1.13.13.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m2.1b"><ci id="algorithm1.13.13.m2.1.1.cmml" xref="algorithm1.13.13.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m2.1c">C</annotation></semantics></math><span id="algorithm1.13.13.6" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm1.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.14.14.1.1.1" class="ltx_text" style="font-size:90%;">9</span></span><span id="algorithm1.14.14.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.14.14.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.14.14.4" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm1.14.14.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm1.14.14.6" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.14.14.7" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm1.14.14.8" class="ltx_text" style="font-size:90%;"> client </span><math id="algorithm1.14.14.m1.1" class="ltx_Math" alttext="c_{i}\in C_{t}" display="inline"><semantics id="algorithm1.14.14.m1.1a"><mrow id="algorithm1.14.14.m1.1.1" xref="algorithm1.14.14.m1.1.1.cmml"><msub id="algorithm1.14.14.m1.1.1.2" xref="algorithm1.14.14.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm1.14.14.m1.1.1.2.2" xref="algorithm1.14.14.m1.1.1.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm1.14.14.m1.1.1.2.3" xref="algorithm1.14.14.m1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="algorithm1.14.14.m1.1.1.1" xref="algorithm1.14.14.m1.1.1.1.cmml">∈</mo><msub id="algorithm1.14.14.m1.1.1.3" xref="algorithm1.14.14.m1.1.1.3.cmml"><mi mathsize="90%" id="algorithm1.14.14.m1.1.1.3.2" xref="algorithm1.14.14.m1.1.1.3.2.cmml">C</mi><mi mathsize="90%" id="algorithm1.14.14.m1.1.1.3.3" xref="algorithm1.14.14.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m1.1b"><apply id="algorithm1.14.14.m1.1.1.cmml" xref="algorithm1.14.14.m1.1.1"><in id="algorithm1.14.14.m1.1.1.1.cmml" xref="algorithm1.14.14.m1.1.1.1"></in><apply id="algorithm1.14.14.m1.1.1.2.cmml" xref="algorithm1.14.14.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.14.14.m1.1.1.2.1.cmml" xref="algorithm1.14.14.m1.1.1.2">subscript</csymbol><ci id="algorithm1.14.14.m1.1.1.2.2.cmml" xref="algorithm1.14.14.m1.1.1.2.2">𝑐</ci><ci id="algorithm1.14.14.m1.1.1.2.3.cmml" xref="algorithm1.14.14.m1.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.14.14.m1.1.1.3.cmml" xref="algorithm1.14.14.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.14.14.m1.1.1.3.1.cmml" xref="algorithm1.14.14.m1.1.1.3">subscript</csymbol><ci id="algorithm1.14.14.m1.1.1.3.2.cmml" xref="algorithm1.14.14.m1.1.1.3.2">𝐶</ci><ci id="algorithm1.14.14.m1.1.1.3.3.cmml" xref="algorithm1.14.14.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m1.1c">c_{i}\in C_{t}</annotation></semantics></math><span id="algorithm1.14.14.9" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.14.14.10" class="ltx_text ltx_font_bold" style="font-size:90%;">in parallel</span><span id="algorithm1.14.14.11" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.14.14.12" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm1.16.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.16.16.2.1.1" class="ltx_text" style="font-size:90%;">10</span></span><span id="algorithm1.16.16.3" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.16.16.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.16.16.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.16.16.6" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm1.15.15.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}^{i}_{t}\leftarrow" display="inline"><semantics id="algorithm1.15.15.m1.1a"><mrow id="algorithm1.15.15.m1.1.1" xref="algorithm1.15.15.m1.1.1.cmml"><msubsup id="algorithm1.15.15.m1.1.1.2" xref="algorithm1.15.15.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm1.15.15.m1.1.1.2.2.2" xref="algorithm1.15.15.m1.1.1.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm1.15.15.m1.1.1.2.3" xref="algorithm1.15.15.m1.1.1.2.3.cmml">t</mi><mi mathsize="90%" id="algorithm1.15.15.m1.1.1.2.2.3" xref="algorithm1.15.15.m1.1.1.2.2.3.cmml">i</mi></msubsup><mo mathsize="90%" stretchy="false" id="algorithm1.15.15.m1.1.1.1" xref="algorithm1.15.15.m1.1.1.1.cmml">←</mo><mi id="algorithm1.15.15.m1.1.1.3" xref="algorithm1.15.15.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.m1.1b"><apply id="algorithm1.15.15.m1.1.1.cmml" xref="algorithm1.15.15.m1.1.1"><ci id="algorithm1.15.15.m1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1.1">←</ci><apply id="algorithm1.15.15.m1.1.1.2.cmml" xref="algorithm1.15.15.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.2.1.cmml" xref="algorithm1.15.15.m1.1.1.2">subscript</csymbol><apply id="algorithm1.15.15.m1.1.1.2.2.cmml" xref="algorithm1.15.15.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.2.2.1.cmml" xref="algorithm1.15.15.m1.1.1.2">superscript</csymbol><ci id="algorithm1.15.15.m1.1.1.2.2.2.cmml" xref="algorithm1.15.15.m1.1.1.2.2.2">𝜽</ci><ci id="algorithm1.15.15.m1.1.1.2.2.3.cmml" xref="algorithm1.15.15.m1.1.1.2.2.3">𝑖</ci></apply><ci id="algorithm1.15.15.m1.1.1.2.3.cmml" xref="algorithm1.15.15.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="algorithm1.15.15.m1.1.1.3.cmml" xref="algorithm1.15.15.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.m1.1c">\boldsymbol{\theta}^{i}_{t}\leftarrow</annotation></semantics></math><span id="algorithm1.16.16.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.16.16.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">UpdateClient<math id="algorithm1.16.16.1.m1.2" class="ltx_Math" alttext="(\boldsymbol{\theta}_{t-1},i)" display="inline"><semantics id="algorithm1.16.16.1.m1.2a"><mrow id="algorithm1.16.16.1.m1.2.2.1" xref="algorithm1.16.16.1.m1.2.2.2.cmml"><mo stretchy="false" id="algorithm1.16.16.1.m1.2.2.1.2" xref="algorithm1.16.16.1.m1.2.2.2.cmml">(</mo><msub id="algorithm1.16.16.1.m1.2.2.1.1" xref="algorithm1.16.16.1.m1.2.2.1.1.cmml"><mi id="algorithm1.16.16.1.m1.2.2.1.1.2" xref="algorithm1.16.16.1.m1.2.2.1.1.2.cmml">𝜽</mi><mrow id="algorithm1.16.16.1.m1.2.2.1.1.3" xref="algorithm1.16.16.1.m1.2.2.1.1.3.cmml"><mi id="algorithm1.16.16.1.m1.2.2.1.1.3.2" xref="algorithm1.16.16.1.m1.2.2.1.1.3.2.cmml">t</mi><mo id="algorithm1.16.16.1.m1.2.2.1.1.3.1" xref="algorithm1.16.16.1.m1.2.2.1.1.3.1.cmml">−</mo><mn id="algorithm1.16.16.1.m1.2.2.1.1.3.3" xref="algorithm1.16.16.1.m1.2.2.1.1.3.3.cmml">1</mn></mrow></msub><mo id="algorithm1.16.16.1.m1.2.2.1.3" xref="algorithm1.16.16.1.m1.2.2.2.cmml">,</mo><mi id="algorithm1.16.16.1.m1.1.1" xref="algorithm1.16.16.1.m1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.16.16.1.m1.2.2.1.4" xref="algorithm1.16.16.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.1.m1.2b"><interval closure="open" id="algorithm1.16.16.1.m1.2.2.2.cmml" xref="algorithm1.16.16.1.m1.2.2.1"><apply id="algorithm1.16.16.1.m1.2.2.1.1.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.1.m1.2.2.1.1.1.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1">subscript</csymbol><ci id="algorithm1.16.16.1.m1.2.2.1.1.2.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1.2">𝜽</ci><apply id="algorithm1.16.16.1.m1.2.2.1.1.3.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1.3"><minus id="algorithm1.16.16.1.m1.2.2.1.1.3.1.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1.3.1"></minus><ci id="algorithm1.16.16.1.m1.2.2.1.1.3.2.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1.3.2">𝑡</ci><cn type="integer" id="algorithm1.16.16.1.m1.2.2.1.1.3.3.cmml" xref="algorithm1.16.16.1.m1.2.2.1.1.3.3">1</cn></apply></apply><ci id="algorithm1.16.16.1.m1.1.1.cmml" xref="algorithm1.16.16.1.m1.1.1">𝑖</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.1.m1.2c">(\boldsymbol{\theta}_{t-1},i)</annotation></semantics></math></span><span id="algorithm1.16.16.8" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm1.28.35" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.35.1.1.1" class="ltx_text" style="font-size:90%;">11</span></span><span id="algorithm1.28.35.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.35.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.35.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.28.35.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.17.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.17.17.1.1.1" class="ltx_text" style="font-size:90%;">12</span></span><span id="algorithm1.17.17.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.17.17.3" class="ltx_text" style="font-size:90%;">   </span><math id="algorithm1.17.17.m1.2" class="ltx_Math" alttext="\boldsymbol{\theta}_{t}\leftarrow\sum_{c_{i}\in C_{t}}\frac{\left|\mathcal{D}_{i}\right|}{\left|\bigcup_{c_{j}\in C_{t}}\mathcal{D}_{j}\right|}\boldsymbol{\theta}^{i}_{t}" display="inline"><semantics id="algorithm1.17.17.m1.2a"><mrow id="algorithm1.17.17.m1.2.3" xref="algorithm1.17.17.m1.2.3.cmml"><msub id="algorithm1.17.17.m1.2.3.2" xref="algorithm1.17.17.m1.2.3.2.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.2.2" xref="algorithm1.17.17.m1.2.3.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.2.3" xref="algorithm1.17.17.m1.2.3.2.3.cmml">t</mi></msub><mo mathsize="90%" rspace="0.111em" stretchy="false" id="algorithm1.17.17.m1.2.3.1" xref="algorithm1.17.17.m1.2.3.1.cmml">←</mo><mrow id="algorithm1.17.17.m1.2.3.3" xref="algorithm1.17.17.m1.2.3.3.cmml"><msub id="algorithm1.17.17.m1.2.3.3.1" xref="algorithm1.17.17.m1.2.3.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="algorithm1.17.17.m1.2.3.3.1.2" xref="algorithm1.17.17.m1.2.3.3.1.2.cmml">∑</mo><mrow id="algorithm1.17.17.m1.2.3.3.1.3" xref="algorithm1.17.17.m1.2.3.3.1.3.cmml"><msub id="algorithm1.17.17.m1.2.3.3.1.3.2" xref="algorithm1.17.17.m1.2.3.3.1.3.2.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.1.3.2.2" xref="algorithm1.17.17.m1.2.3.3.1.3.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.1.3.2.3" xref="algorithm1.17.17.m1.2.3.3.1.3.2.3.cmml">i</mi></msub><mo mathsize="90%" id="algorithm1.17.17.m1.2.3.3.1.3.1" xref="algorithm1.17.17.m1.2.3.3.1.3.1.cmml">∈</mo><msub id="algorithm1.17.17.m1.2.3.3.1.3.3" xref="algorithm1.17.17.m1.2.3.3.1.3.3.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.1.3.3.2" xref="algorithm1.17.17.m1.2.3.3.1.3.3.2.cmml">C</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.1.3.3.3" xref="algorithm1.17.17.m1.2.3.3.1.3.3.3.cmml">t</mi></msub></mrow></msub><mrow id="algorithm1.17.17.m1.2.3.3.2" xref="algorithm1.17.17.m1.2.3.3.2.cmml"><mfrac id="algorithm1.17.17.m1.2.2" xref="algorithm1.17.17.m1.2.2.cmml"><mrow id="algorithm1.17.17.m1.1.1.1.1" xref="algorithm1.17.17.m1.1.1.1.2.cmml"><mo id="algorithm1.17.17.m1.1.1.1.1.2" xref="algorithm1.17.17.m1.1.1.1.2.1.cmml">|</mo><msub id="algorithm1.17.17.m1.1.1.1.1.1" xref="algorithm1.17.17.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.17.17.m1.1.1.1.1.1.2" xref="algorithm1.17.17.m1.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.17.17.m1.1.1.1.1.1.3" xref="algorithm1.17.17.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm1.17.17.m1.1.1.1.1.3" xref="algorithm1.17.17.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="algorithm1.17.17.m1.2.2.2.1" xref="algorithm1.17.17.m1.2.2.2.2.cmml"><mo id="algorithm1.17.17.m1.2.2.2.1.2" xref="algorithm1.17.17.m1.2.2.2.2.1.cmml">|</mo><mrow id="algorithm1.17.17.m1.2.2.2.1.1" xref="algorithm1.17.17.m1.2.2.2.1.1.cmml"><mstyle displaystyle="false" id="algorithm1.17.17.m1.2.2.2.1.1.1" xref="algorithm1.17.17.m1.2.2.2.1.1.1.cmml"><msub id="algorithm1.17.17.m1.2.2.2.1.1.1a" xref="algorithm1.17.17.m1.2.2.2.1.1.1.cmml"><mo maxsize="63%" minsize="63%" stretchy="true" id="algorithm1.17.17.m1.2.2.2.1.1.1.2" xref="algorithm1.17.17.m1.2.2.2.1.1.1.2.cmml">⋃</mo><mrow id="algorithm1.17.17.m1.2.2.2.1.1.1.3" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.cmml"><msub id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.2" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.3" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.3.cmml">j</mi></msub><mo mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.1" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.1.cmml">∈</mo><msub id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.2" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.2.cmml">C</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.3" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.3.cmml">t</mi></msub></mrow></msub></mstyle><msub id="algorithm1.17.17.m1.2.2.2.1.1.2" xref="algorithm1.17.17.m1.2.2.2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.2.2" xref="algorithm1.17.17.m1.2.2.2.1.1.2.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.2.2.1.1.2.3" xref="algorithm1.17.17.m1.2.2.2.1.1.2.3.cmml">j</mi></msub></mrow><mo id="algorithm1.17.17.m1.2.2.2.1.3" xref="algorithm1.17.17.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="algorithm1.17.17.m1.2.3.3.2.1" xref="algorithm1.17.17.m1.2.3.3.2.1.cmml">​</mo><msubsup id="algorithm1.17.17.m1.2.3.3.2.2" xref="algorithm1.17.17.m1.2.3.3.2.2.cmml"><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.2.2.2.2" xref="algorithm1.17.17.m1.2.3.3.2.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.2.2.3" xref="algorithm1.17.17.m1.2.3.3.2.2.3.cmml">t</mi><mi mathsize="90%" id="algorithm1.17.17.m1.2.3.3.2.2.2.3" xref="algorithm1.17.17.m1.2.3.3.2.2.2.3.cmml">i</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.2b"><apply id="algorithm1.17.17.m1.2.3.cmml" xref="algorithm1.17.17.m1.2.3"><ci id="algorithm1.17.17.m1.2.3.1.cmml" xref="algorithm1.17.17.m1.2.3.1">←</ci><apply id="algorithm1.17.17.m1.2.3.2.cmml" xref="algorithm1.17.17.m1.2.3.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.2.1.cmml" xref="algorithm1.17.17.m1.2.3.2">subscript</csymbol><ci id="algorithm1.17.17.m1.2.3.2.2.cmml" xref="algorithm1.17.17.m1.2.3.2.2">𝜽</ci><ci id="algorithm1.17.17.m1.2.3.2.3.cmml" xref="algorithm1.17.17.m1.2.3.2.3">𝑡</ci></apply><apply id="algorithm1.17.17.m1.2.3.3.cmml" xref="algorithm1.17.17.m1.2.3.3"><apply id="algorithm1.17.17.m1.2.3.3.1.cmml" xref="algorithm1.17.17.m1.2.3.3.1"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.3.1.1.cmml" xref="algorithm1.17.17.m1.2.3.3.1">subscript</csymbol><sum id="algorithm1.17.17.m1.2.3.3.1.2.cmml" xref="algorithm1.17.17.m1.2.3.3.1.2"></sum><apply id="algorithm1.17.17.m1.2.3.3.1.3.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3"><in id="algorithm1.17.17.m1.2.3.3.1.3.1.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.1"></in><apply id="algorithm1.17.17.m1.2.3.3.1.3.2.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.3.1.3.2.1.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.2">subscript</csymbol><ci id="algorithm1.17.17.m1.2.3.3.1.3.2.2.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.2.2">𝑐</ci><ci id="algorithm1.17.17.m1.2.3.3.1.3.2.3.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.2.3">𝑖</ci></apply><apply id="algorithm1.17.17.m1.2.3.3.1.3.3.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.3"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.3.1.3.3.1.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.3">subscript</csymbol><ci id="algorithm1.17.17.m1.2.3.3.1.3.3.2.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.3.2">𝐶</ci><ci id="algorithm1.17.17.m1.2.3.3.1.3.3.3.cmml" xref="algorithm1.17.17.m1.2.3.3.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="algorithm1.17.17.m1.2.3.3.2.cmml" xref="algorithm1.17.17.m1.2.3.3.2"><times id="algorithm1.17.17.m1.2.3.3.2.1.cmml" xref="algorithm1.17.17.m1.2.3.3.2.1"></times><apply id="algorithm1.17.17.m1.2.2.cmml" xref="algorithm1.17.17.m1.2.2"><divide id="algorithm1.17.17.m1.2.2.3.cmml" xref="algorithm1.17.17.m1.2.2"></divide><apply id="algorithm1.17.17.m1.1.1.1.2.cmml" xref="algorithm1.17.17.m1.1.1.1.1"><abs id="algorithm1.17.17.m1.1.1.1.2.1.cmml" xref="algorithm1.17.17.m1.1.1.1.1.2"></abs><apply id="algorithm1.17.17.m1.1.1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.1.1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.17.17.m1.1.1.1.1.1.2.cmml" xref="algorithm1.17.17.m1.1.1.1.1.1.2">𝒟</ci><ci id="algorithm1.17.17.m1.1.1.1.1.1.3.cmml" xref="algorithm1.17.17.m1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="algorithm1.17.17.m1.2.2.2.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1"><abs id="algorithm1.17.17.m1.2.2.2.2.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.2"></abs><apply id="algorithm1.17.17.m1.2.2.2.1.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1"><apply id="algorithm1.17.17.m1.2.2.2.1.1.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.2.2.1.1.1.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1">subscript</csymbol><union id="algorithm1.17.17.m1.2.2.2.1.1.1.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.2"></union><apply id="algorithm1.17.17.m1.2.2.2.1.1.1.3.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3"><in id="algorithm1.17.17.m1.2.2.2.1.1.1.3.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.1"></in><apply id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2">subscript</csymbol><ci id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.2">𝑐</ci><ci id="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.3.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.2.3">𝑗</ci></apply><apply id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3">subscript</csymbol><ci id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.2">𝐶</ci><ci id="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.3.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="algorithm1.17.17.m1.2.2.2.1.1.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.2.2.1.1.2.1.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.2">subscript</csymbol><ci id="algorithm1.17.17.m1.2.2.2.1.1.2.2.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.2.2">𝒟</ci><ci id="algorithm1.17.17.m1.2.2.2.1.1.2.3.cmml" xref="algorithm1.17.17.m1.2.2.2.1.1.2.3">𝑗</ci></apply></apply></apply></apply><apply id="algorithm1.17.17.m1.2.3.3.2.2.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.3.2.2.1.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2">subscript</csymbol><apply id="algorithm1.17.17.m1.2.3.3.2.2.2.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.2.3.3.2.2.2.1.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2">superscript</csymbol><ci id="algorithm1.17.17.m1.2.3.3.2.2.2.2.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2.2.2">𝜽</ci><ci id="algorithm1.17.17.m1.2.3.3.2.2.2.3.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2.2.3">𝑖</ci></apply><ci id="algorithm1.17.17.m1.2.3.3.2.2.3.cmml" xref="algorithm1.17.17.m1.2.3.3.2.2.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.2c">\boldsymbol{\theta}_{t}\leftarrow\sum_{c_{i}\in C_{t}}\frac{\left|\mathcal{D}_{i}\right|}{\left|\bigcup_{c_{j}\in C_{t}}\mathcal{D}_{j}\right|}\boldsymbol{\theta}^{i}_{t}</annotation></semantics></math>
</div>
<div id="algorithm1.28.36" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.36.1.1.1" class="ltx_text" style="font-size:90%;">13</span></span><span id="algorithm1.28.36.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.36.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.28.36.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.18.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.18.18.1.1.1" class="ltx_text" style="font-size:90%;">14</span></span><span id="algorithm1.18.18.2" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="algorithm1.18.18.3" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.18.18.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{T}" display="inline"><semantics id="algorithm1.18.18.m1.1a"><msub id="algorithm1.18.18.m1.1.1" xref="algorithm1.18.18.m1.1.1.cmml"><mi mathsize="90%" id="algorithm1.18.18.m1.1.1.2" xref="algorithm1.18.18.m1.1.1.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm1.18.18.m1.1.1.3" xref="algorithm1.18.18.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m1.1b"><apply id="algorithm1.18.18.m1.1.1.cmml" xref="algorithm1.18.18.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.1.1.1.cmml" xref="algorithm1.18.18.m1.1.1">subscript</csymbol><ci id="algorithm1.18.18.m1.1.1.2.cmml" xref="algorithm1.18.18.m1.1.1.2">𝜽</ci><ci id="algorithm1.18.18.m1.1.1.3.cmml" xref="algorithm1.18.18.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m1.1c">\boldsymbol{\theta}_{T}</annotation></semantics></math>
</div>
<div id="algorithm1.28.37" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.37.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span><span id="algorithm1.28.37.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.28.38" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.38.1.1.1" class="ltx_text" style="font-size:90%;">16</span></span><span id="algorithm1.28.38.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.19.19" class="ltx_listingline">
<span id="algorithm1.19.19.2" class="ltx_text" style="font-size:90%;"><span id="algorithm1.19.19.2.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm1.19.19.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">Runs on client <math id="algorithm1.19.19.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="algorithm1.19.19.1.m1.1a"><mi mathcolor="#0000FF" id="algorithm1.19.19.1.m1.1.1" xref="algorithm1.19.19.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.1.m1.1b"><ci id="algorithm1.19.19.1.m1.1.1.cmml" xref="algorithm1.19.19.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.1.m1.1c">i</annotation></semantics></math> */</span>
</div>
<div id="algorithm1.21.21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.21.21.3.1.1" class="ltx_text" style="font-size:90%;">17</span></span><span id="algorithm1.21.21.4" class="ltx_text" style="font-size:90%;">
</span><em id="algorithm1.21.21.2" class="ltx_emph ltx_font_typewriter" style="font-size:90%;">UpdateClient(<em id="algorithm1.21.21.2.2.2" class="ltx_emph ltx_font_serif ltx_font_italic"><math id="algorithm1.20.20.1.1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><semantics id="algorithm1.20.20.1.1.1.m1.1a"><mi id="algorithm1.20.20.1.1.1.m1.1.1" xref="algorithm1.20.20.1.1.1.m1.1.1.cmml">𝛉</mi><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.1.1.1.m1.1b"><ci id="algorithm1.20.20.1.1.1.m1.1.1.cmml" xref="algorithm1.20.20.1.1.1.m1.1.1">𝛉</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.1.1.1.m1.1c">\boldsymbol{\theta}</annotation></semantics></math>, <math id="algorithm1.21.21.2.2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="algorithm1.21.21.2.2.2.m2.1a"><mi id="algorithm1.21.21.2.2.2.m2.1.1" xref="algorithm1.21.21.2.2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.2.2.2.m2.1b"><ci id="algorithm1.21.21.2.2.2.m2.1.1.cmml" xref="algorithm1.21.21.2.2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.2.2.2.m2.1c">i</annotation></semantics></math></em>)</em><span id="algorithm1.21.21.5" class="ltx_text ltx_font_bold" style="font-size:90%;">:</span>
</div>
<div id="algorithm1.22.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.22.22.1.1.1" class="ltx_text" style="font-size:90%;">18</span></span><span id="algorithm1.22.22.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.22.22.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.22.22.4" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm1.22.22.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm1.22.22.6" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.22.22.7" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm1.22.22.8" class="ltx_text" style="font-size:90%;"> local epoch </span><math id="algorithm1.22.22.m1.3" class="ltx_Math" alttext="e=1,\dots,E" display="inline"><semantics id="algorithm1.22.22.m1.3a"><mrow id="algorithm1.22.22.m1.3.4" xref="algorithm1.22.22.m1.3.4.cmml"><mi mathsize="90%" id="algorithm1.22.22.m1.3.4.2" xref="algorithm1.22.22.m1.3.4.2.cmml">e</mi><mo mathsize="90%" id="algorithm1.22.22.m1.3.4.1" xref="algorithm1.22.22.m1.3.4.1.cmml">=</mo><mrow id="algorithm1.22.22.m1.3.4.3.2" xref="algorithm1.22.22.m1.3.4.3.1.cmml"><mn mathsize="90%" id="algorithm1.22.22.m1.1.1" xref="algorithm1.22.22.m1.1.1.cmml">1</mn><mo mathsize="90%" id="algorithm1.22.22.m1.3.4.3.2.1" xref="algorithm1.22.22.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm1.22.22.m1.2.2" xref="algorithm1.22.22.m1.2.2.cmml">…</mi><mo mathsize="90%" id="algorithm1.22.22.m1.3.4.3.2.2" xref="algorithm1.22.22.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="algorithm1.22.22.m1.3.3" xref="algorithm1.22.22.m1.3.3.cmml">E</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.22.22.m1.3b"><apply id="algorithm1.22.22.m1.3.4.cmml" xref="algorithm1.22.22.m1.3.4"><eq id="algorithm1.22.22.m1.3.4.1.cmml" xref="algorithm1.22.22.m1.3.4.1"></eq><ci id="algorithm1.22.22.m1.3.4.2.cmml" xref="algorithm1.22.22.m1.3.4.2">𝑒</ci><list id="algorithm1.22.22.m1.3.4.3.1.cmml" xref="algorithm1.22.22.m1.3.4.3.2"><cn type="integer" id="algorithm1.22.22.m1.1.1.cmml" xref="algorithm1.22.22.m1.1.1">1</cn><ci id="algorithm1.22.22.m1.2.2.cmml" xref="algorithm1.22.22.m1.2.2">…</ci><ci id="algorithm1.22.22.m1.3.3.cmml" xref="algorithm1.22.22.m1.3.3">𝐸</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.22.22.m1.3c">e=1,\dots,E</annotation></semantics></math><span id="algorithm1.22.22.9" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.22.22.10" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm1.25.25" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.25.25.1.1.1" class="ltx_text" style="font-size:90%;">19</span></span><span id="algorithm1.25.25.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.25.25.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.25.25.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.25.25.5" class="ltx_text" style="font-size:90%;">
split </span><math id="algorithm1.23.23.m1.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="algorithm1.23.23.m1.1a"><msub id="algorithm1.23.23.m1.1.1" xref="algorithm1.23.23.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.23.23.m1.1.1.2" xref="algorithm1.23.23.m1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.23.23.m1.1.1.3" xref="algorithm1.23.23.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m1.1b"><apply id="algorithm1.23.23.m1.1.1.cmml" xref="algorithm1.23.23.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.1.1.1.cmml" xref="algorithm1.23.23.m1.1.1">subscript</csymbol><ci id="algorithm1.23.23.m1.1.1.2.cmml" xref="algorithm1.23.23.m1.1.1.2">𝒟</ci><ci id="algorithm1.23.23.m1.1.1.3.cmml" xref="algorithm1.23.23.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m1.1c">\mathcal{D}_{i}</annotation></semantics></math><span id="algorithm1.25.25.6" class="ltx_text" style="font-size:90%;"> into </span><math id="algorithm1.24.24.m2.1" class="ltx_Math" alttext="\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil" display="inline"><semantics id="algorithm1.24.24.m2.1a"><mrow id="algorithm1.24.24.m2.1.2.2" xref="algorithm1.24.24.m2.1.2.1.cmml"><mo id="algorithm1.24.24.m2.1.2.2.1" xref="algorithm1.24.24.m2.1.2.1.1.cmml">⌈</mo><mfrac id="algorithm1.24.24.m2.1.1" xref="algorithm1.24.24.m2.1.1.cmml"><mrow id="algorithm1.24.24.m2.1.1.1.1" xref="algorithm1.24.24.m2.1.1.1.2.cmml"><mo id="algorithm1.24.24.m2.1.1.1.1.2" xref="algorithm1.24.24.m2.1.1.1.2.1.cmml">|</mo><msub id="algorithm1.24.24.m2.1.1.1.1.1" xref="algorithm1.24.24.m2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.24.24.m2.1.1.1.1.1.2" xref="algorithm1.24.24.m2.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.24.24.m2.1.1.1.1.1.3" xref="algorithm1.24.24.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm1.24.24.m2.1.1.1.1.3" xref="algorithm1.24.24.m2.1.1.1.2.1.cmml">|</mo></mrow><mi mathsize="90%" id="algorithm1.24.24.m2.1.1.3" xref="algorithm1.24.24.m2.1.1.3.cmml">B</mi></mfrac><mo id="algorithm1.24.24.m2.1.2.2.2" xref="algorithm1.24.24.m2.1.2.1.1.cmml">⌉</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m2.1b"><apply id="algorithm1.24.24.m2.1.2.1.cmml" xref="algorithm1.24.24.m2.1.2.2"><ceiling id="algorithm1.24.24.m2.1.2.1.1.cmml" xref="algorithm1.24.24.m2.1.2.2.1"></ceiling><apply id="algorithm1.24.24.m2.1.1.cmml" xref="algorithm1.24.24.m2.1.1"><divide id="algorithm1.24.24.m2.1.1.2.cmml" xref="algorithm1.24.24.m2.1.1"></divide><apply id="algorithm1.24.24.m2.1.1.1.2.cmml" xref="algorithm1.24.24.m2.1.1.1.1"><abs id="algorithm1.24.24.m2.1.1.1.2.1.cmml" xref="algorithm1.24.24.m2.1.1.1.1.2"></abs><apply id="algorithm1.24.24.m2.1.1.1.1.1.cmml" xref="algorithm1.24.24.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.24.24.m2.1.1.1.1.1.1.cmml" xref="algorithm1.24.24.m2.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.24.24.m2.1.1.1.1.1.2.cmml" xref="algorithm1.24.24.m2.1.1.1.1.1.2">𝒟</ci><ci id="algorithm1.24.24.m2.1.1.1.1.1.3.cmml" xref="algorithm1.24.24.m2.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="algorithm1.24.24.m2.1.1.3.cmml" xref="algorithm1.24.24.m2.1.1.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m2.1c">\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil</annotation></semantics></math><span id="algorithm1.25.25.7" class="ltx_text" style="font-size:90%;"> batches of size </span><math id="algorithm1.25.25.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="algorithm1.25.25.m3.1a"><mi mathsize="90%" id="algorithm1.25.25.m3.1.1" xref="algorithm1.25.25.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="algorithm1.25.25.m3.1b"><ci id="algorithm1.25.25.m3.1.1.cmml" xref="algorithm1.25.25.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.25.25.m3.1c">B</annotation></semantics></math><span id="algorithm1.25.25.8" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm1.26.26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.26.26.1.1.1" class="ltx_text" style="font-size:90%;">20</span></span><span id="algorithm1.26.26.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.26.26.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.26.26.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.26.26.5" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm1.26.26.6" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm1.26.26.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.26.26.8" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm1.26.26.9" class="ltx_text" style="font-size:90%;"> batch </span><math id="algorithm1.26.26.m1.4" class="ltx_Math" alttext="b=1,\dots,\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil" display="inline"><semantics id="algorithm1.26.26.m1.4a"><mrow id="algorithm1.26.26.m1.4.4" xref="algorithm1.26.26.m1.4.4.cmml"><mi mathsize="90%" id="algorithm1.26.26.m1.4.4.3" xref="algorithm1.26.26.m1.4.4.3.cmml">b</mi><mo mathsize="90%" id="algorithm1.26.26.m1.4.4.2" xref="algorithm1.26.26.m1.4.4.2.cmml">=</mo><mrow id="algorithm1.26.26.m1.4.4.1.1" xref="algorithm1.26.26.m1.4.4.1.2.cmml"><mn mathsize="90%" id="algorithm1.26.26.m1.2.2" xref="algorithm1.26.26.m1.2.2.cmml">1</mn><mo mathsize="90%" id="algorithm1.26.26.m1.4.4.1.1.2" xref="algorithm1.26.26.m1.4.4.1.2.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm1.26.26.m1.3.3" xref="algorithm1.26.26.m1.3.3.cmml">…</mi><mo mathsize="90%" id="algorithm1.26.26.m1.4.4.1.1.3" xref="algorithm1.26.26.m1.4.4.1.2.cmml">,</mo><mrow id="algorithm1.26.26.m1.4.4.1.1.1.2" xref="algorithm1.26.26.m1.4.4.1.1.1.1.cmml"><mo id="algorithm1.26.26.m1.4.4.1.1.1.2.1" xref="algorithm1.26.26.m1.4.4.1.1.1.1.1.cmml">⌈</mo><mfrac id="algorithm1.26.26.m1.1.1" xref="algorithm1.26.26.m1.1.1.cmml"><mrow id="algorithm1.26.26.m1.1.1.1.1" xref="algorithm1.26.26.m1.1.1.1.2.cmml"><mo id="algorithm1.26.26.m1.1.1.1.1.2" xref="algorithm1.26.26.m1.1.1.1.2.1.cmml">|</mo><msub id="algorithm1.26.26.m1.1.1.1.1.1" xref="algorithm1.26.26.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.26.26.m1.1.1.1.1.1.2" xref="algorithm1.26.26.m1.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm1.26.26.m1.1.1.1.1.1.3" xref="algorithm1.26.26.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm1.26.26.m1.1.1.1.1.3" xref="algorithm1.26.26.m1.1.1.1.2.1.cmml">|</mo></mrow><mi mathsize="90%" id="algorithm1.26.26.m1.1.1.3" xref="algorithm1.26.26.m1.1.1.3.cmml">B</mi></mfrac><mo id="algorithm1.26.26.m1.4.4.1.1.1.2.2" xref="algorithm1.26.26.m1.4.4.1.1.1.1.1.cmml">⌉</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.26.26.m1.4b"><apply id="algorithm1.26.26.m1.4.4.cmml" xref="algorithm1.26.26.m1.4.4"><eq id="algorithm1.26.26.m1.4.4.2.cmml" xref="algorithm1.26.26.m1.4.4.2"></eq><ci id="algorithm1.26.26.m1.4.4.3.cmml" xref="algorithm1.26.26.m1.4.4.3">𝑏</ci><list id="algorithm1.26.26.m1.4.4.1.2.cmml" xref="algorithm1.26.26.m1.4.4.1.1"><cn type="integer" id="algorithm1.26.26.m1.2.2.cmml" xref="algorithm1.26.26.m1.2.2">1</cn><ci id="algorithm1.26.26.m1.3.3.cmml" xref="algorithm1.26.26.m1.3.3">…</ci><apply id="algorithm1.26.26.m1.4.4.1.1.1.1.cmml" xref="algorithm1.26.26.m1.4.4.1.1.1.2"><ceiling id="algorithm1.26.26.m1.4.4.1.1.1.1.1.cmml" xref="algorithm1.26.26.m1.4.4.1.1.1.2.1"></ceiling><apply id="algorithm1.26.26.m1.1.1.cmml" xref="algorithm1.26.26.m1.1.1"><divide id="algorithm1.26.26.m1.1.1.2.cmml" xref="algorithm1.26.26.m1.1.1"></divide><apply id="algorithm1.26.26.m1.1.1.1.2.cmml" xref="algorithm1.26.26.m1.1.1.1.1"><abs id="algorithm1.26.26.m1.1.1.1.2.1.cmml" xref="algorithm1.26.26.m1.1.1.1.1.2"></abs><apply id="algorithm1.26.26.m1.1.1.1.1.1.cmml" xref="algorithm1.26.26.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.26.26.m1.1.1.1.1.1.1.cmml" xref="algorithm1.26.26.m1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.26.26.m1.1.1.1.1.1.2.cmml" xref="algorithm1.26.26.m1.1.1.1.1.1.2">𝒟</ci><ci id="algorithm1.26.26.m1.1.1.1.1.1.3.cmml" xref="algorithm1.26.26.m1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="algorithm1.26.26.m1.1.1.3.cmml" xref="algorithm1.26.26.m1.1.1.3">𝐵</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.26.26.m1.4c">b=1,\dots,\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil</annotation></semantics></math><span id="algorithm1.26.26.10" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm1.26.26.11" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm1.27.27" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.27.27.1.1.1" class="ltx_text" style="font-size:90%;">21</span></span><span id="algorithm1.27.27.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.27.27.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.27.27.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.27.27.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.27.27.6" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm1.27.27.m1.4" class="ltx_Math" alttext="\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\nabla\mathcal{L}_{i}(\mathcal{D}_{i,b};\boldsymbol{\theta})" display="inline"><semantics id="algorithm1.27.27.m1.4a"><mrow id="algorithm1.27.27.m1.4.4" xref="algorithm1.27.27.m1.4.4.cmml"><mi mathsize="90%" id="algorithm1.27.27.m1.4.4.3" xref="algorithm1.27.27.m1.4.4.3.cmml">𝜽</mi><mo mathsize="90%" stretchy="false" id="algorithm1.27.27.m1.4.4.2" xref="algorithm1.27.27.m1.4.4.2.cmml">←</mo><mrow id="algorithm1.27.27.m1.4.4.1" xref="algorithm1.27.27.m1.4.4.1.cmml"><mi mathsize="90%" id="algorithm1.27.27.m1.4.4.1.3" xref="algorithm1.27.27.m1.4.4.1.3.cmml">𝜽</mi><mo mathsize="90%" id="algorithm1.27.27.m1.4.4.1.2" xref="algorithm1.27.27.m1.4.4.1.2.cmml">−</mo><mrow id="algorithm1.27.27.m1.4.4.1.1" xref="algorithm1.27.27.m1.4.4.1.1.cmml"><mi mathsize="90%" id="algorithm1.27.27.m1.4.4.1.1.3" xref="algorithm1.27.27.m1.4.4.1.1.3.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="algorithm1.27.27.m1.4.4.1.1.2" xref="algorithm1.27.27.m1.4.4.1.1.2.cmml">​</mo><mrow id="algorithm1.27.27.m1.4.4.1.1.4" xref="algorithm1.27.27.m1.4.4.1.1.4.cmml"><mo mathsize="90%" rspace="0.167em" id="algorithm1.27.27.m1.4.4.1.1.4.1" xref="algorithm1.27.27.m1.4.4.1.1.4.1.cmml">∇</mo><msub id="algorithm1.27.27.m1.4.4.1.1.4.2" xref="algorithm1.27.27.m1.4.4.1.1.4.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.27.27.m1.4.4.1.1.4.2.2" xref="algorithm1.27.27.m1.4.4.1.1.4.2.2.cmml">ℒ</mi><mi mathsize="90%" id="algorithm1.27.27.m1.4.4.1.1.4.2.3" xref="algorithm1.27.27.m1.4.4.1.1.4.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="algorithm1.27.27.m1.4.4.1.1.2a" xref="algorithm1.27.27.m1.4.4.1.1.2.cmml">​</mo><mrow id="algorithm1.27.27.m1.4.4.1.1.1.1" xref="algorithm1.27.27.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="algorithm1.27.27.m1.4.4.1.1.1.1.2" xref="algorithm1.27.27.m1.4.4.1.1.1.2.cmml">(</mo><msub id="algorithm1.27.27.m1.4.4.1.1.1.1.1" xref="algorithm1.27.27.m1.4.4.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm1.27.27.m1.4.4.1.1.1.1.1.2" xref="algorithm1.27.27.m1.4.4.1.1.1.1.1.2.cmml">𝒟</mi><mrow id="algorithm1.27.27.m1.2.2.2.4" xref="algorithm1.27.27.m1.2.2.2.3.cmml"><mi mathsize="90%" id="algorithm1.27.27.m1.1.1.1.1" xref="algorithm1.27.27.m1.1.1.1.1.cmml">i</mi><mo mathsize="90%" id="algorithm1.27.27.m1.2.2.2.4.1" xref="algorithm1.27.27.m1.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="algorithm1.27.27.m1.2.2.2.2" xref="algorithm1.27.27.m1.2.2.2.2.cmml">b</mi></mrow></msub><mo mathsize="90%" id="algorithm1.27.27.m1.4.4.1.1.1.1.3" xref="algorithm1.27.27.m1.4.4.1.1.1.2.cmml">;</mo><mi mathsize="90%" id="algorithm1.27.27.m1.3.3" xref="algorithm1.27.27.m1.3.3.cmml">𝜽</mi><mo maxsize="90%" minsize="90%" id="algorithm1.27.27.m1.4.4.1.1.1.1.4" xref="algorithm1.27.27.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.27.27.m1.4b"><apply id="algorithm1.27.27.m1.4.4.cmml" xref="algorithm1.27.27.m1.4.4"><ci id="algorithm1.27.27.m1.4.4.2.cmml" xref="algorithm1.27.27.m1.4.4.2">←</ci><ci id="algorithm1.27.27.m1.4.4.3.cmml" xref="algorithm1.27.27.m1.4.4.3">𝜽</ci><apply id="algorithm1.27.27.m1.4.4.1.cmml" xref="algorithm1.27.27.m1.4.4.1"><minus id="algorithm1.27.27.m1.4.4.1.2.cmml" xref="algorithm1.27.27.m1.4.4.1.2"></minus><ci id="algorithm1.27.27.m1.4.4.1.3.cmml" xref="algorithm1.27.27.m1.4.4.1.3">𝜽</ci><apply id="algorithm1.27.27.m1.4.4.1.1.cmml" xref="algorithm1.27.27.m1.4.4.1.1"><times id="algorithm1.27.27.m1.4.4.1.1.2.cmml" xref="algorithm1.27.27.m1.4.4.1.1.2"></times><ci id="algorithm1.27.27.m1.4.4.1.1.3.cmml" xref="algorithm1.27.27.m1.4.4.1.1.3">𝜂</ci><apply id="algorithm1.27.27.m1.4.4.1.1.4.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4"><ci id="algorithm1.27.27.m1.4.4.1.1.4.1.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4.1">∇</ci><apply id="algorithm1.27.27.m1.4.4.1.1.4.2.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4.2"><csymbol cd="ambiguous" id="algorithm1.27.27.m1.4.4.1.1.4.2.1.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4.2">subscript</csymbol><ci id="algorithm1.27.27.m1.4.4.1.1.4.2.2.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4.2.2">ℒ</ci><ci id="algorithm1.27.27.m1.4.4.1.1.4.2.3.cmml" xref="algorithm1.27.27.m1.4.4.1.1.4.2.3">𝑖</ci></apply></apply><list id="algorithm1.27.27.m1.4.4.1.1.1.2.cmml" xref="algorithm1.27.27.m1.4.4.1.1.1.1"><apply id="algorithm1.27.27.m1.4.4.1.1.1.1.1.cmml" xref="algorithm1.27.27.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.27.27.m1.4.4.1.1.1.1.1.1.cmml" xref="algorithm1.27.27.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.27.27.m1.4.4.1.1.1.1.1.2.cmml" xref="algorithm1.27.27.m1.4.4.1.1.1.1.1.2">𝒟</ci><list id="algorithm1.27.27.m1.2.2.2.3.cmml" xref="algorithm1.27.27.m1.2.2.2.4"><ci id="algorithm1.27.27.m1.1.1.1.1.cmml" xref="algorithm1.27.27.m1.1.1.1.1">𝑖</ci><ci id="algorithm1.27.27.m1.2.2.2.2.cmml" xref="algorithm1.27.27.m1.2.2.2.2">𝑏</ci></list></apply><ci id="algorithm1.27.27.m1.3.3.cmml" xref="algorithm1.27.27.m1.3.3">𝜽</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.27.27.m1.4c">\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\nabla\mathcal{L}_{i}(\mathcal{D}_{i,b};\boldsymbol{\theta})</annotation></semantics></math>
</div>
<div id="algorithm1.28.39" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.39.1.1.1" class="ltx_text" style="font-size:90%;">22</span></span><span id="algorithm1.28.39.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.39.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.39.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.39.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.28.39.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.28.28" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.28.1.1.1" class="ltx_text" style="font-size:90%;">23</span></span><span id="algorithm1.28.28.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.28.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.28.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.28.28.5" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="algorithm1.28.28.6" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.28.28.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><semantics id="algorithm1.28.28.m1.1a"><mi mathsize="90%" id="algorithm1.28.28.m1.1.1" xref="algorithm1.28.28.m1.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="algorithm1.28.28.m1.1b"><ci id="algorithm1.28.28.m1.1.1.cmml" xref="algorithm1.28.28.m1.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.28.28.m1.1c">\boldsymbol{\theta}</annotation></semantics></math>
</div>
<div id="algorithm1.28.40" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.40.1.1.1" class="ltx_text" style="font-size:90%;">24</span></span><span id="algorithm1.28.40.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.40.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.40.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm1.28.40.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm1.28.41" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.28.41.1.1.1" class="ltx_text" style="font-size:90%;">25</span></span><span id="algorithm1.28.41.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.28.41.3" class="ltx_text" style="font-size:90%;">   </span>
</div>
<div id="algorithm1.28.42" class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.35.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite></figcaption>
</figure>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">A significant challenge of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> lies, however, in dealing with non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> client data. The data generating distribution may be different for each client, i.e., the data is not independent and identically distributed between the clients. This means that the local objective of each client may differ, sometimes even significantly, from the global training objective, which may lead to conflicting model updates being sent to the central server that hinders the convergence of the global model. There are different types of non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness, which include:</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Covariate Shift</span> – Local samples may have a different statistical distribution compared to the samples of other clients</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Prior Probability Shift</span> – The labels of the local samples may have a different statistical distribution compared to the samples of other clients</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Concept Shift</span> – Local samples have the same labels as other clients, but they correspond to different features, or local samples have the same features as other clients, but they correspond to different labels</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Imbalanced Data</span> – The data available at the clients may vary significantly in size</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">Many different techniques have been proposed to alleviate the problems associated with non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data, cf. <cite class="ltx_cite ltx_citemacro_citet">Zhu et al<span class="ltx_text">.</span> (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>)</cite> for a timely overview of different techniques.</p>
</div>
<div id="S3.SS6.p5" class="ltx_para">
<p id="S3.SS6.p5.1" class="ltx_p">Clients with limited local data are another issue that has a comparable effect to non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness. In the case of movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>, it is common that most users have only watched a few dozen or maybe a few hundred movies. This can lead to exceedingly small, noisy updates of the local model, which result in the global model not converging. Both the problem of imbalanced data and small local datasets can be attenuated by weighting the local model updates during aggregation by the local dataset size of the client. But this also has the unwanted effect of suppressing the interests of many users with little training data and amplifying the interests of a few users with a lot of training data.</p>
</div>
<div id="S3.SS6.p6" class="ltx_para">
<p id="S3.SS6.p6.1" class="ltx_p">We address both problems of non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and small local datasets by chaining client trainings together. The central server selects a random subset of the client population for each communication round before further subdividing them into small queues of a specified size. The clients constituting a specific queue are assigned uniformly at random from the client subset. The first client in each queue receives the global model for local training, while each consecutive client receives the local model of the client prior to it. The local models of the last client in each of these queues are then aggregated by the central server, similar to <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>. The goal of chaining multiple client trainings is that the resulting model updates are less noisy because they were not only exposed to more data but also to data from multiple different distributions, in contrast to what would normally be possible. Since no client in a queue has information about the origin of its local model nor about its position in the queue, this method is still at least as privacy-preserving as regular <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. We call this technique <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>. Algorithm <a href="#algorithm2" title="In 3.6. Federated Recommender Systems at Scale Using Queue-Based Federated Learning ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the exact training protocol that we follow.</p>
</div>
<figure id="algorithm2" class="ltx_float ltx_algorithm">
<div id="algorithm2.36" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm2.36.37" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.37.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span><span id="algorithm2.36.37.2" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="algorithm2.36.38" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.38.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</div>
<div id="algorithm2.36.39" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.39.1.1.1" class="ltx_text" style="font-size:90%;">3</span></span><span id="algorithm2.36.39.2" class="ltx_text" style="font-size:90%;">







</span>
</div>
<div id="algorithm2.11.11" class="ltx_listingline">
<span id="algorithm2.11.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input :</span><span id="algorithm2.11.11.2" class="ltx_text" style="font-size:90%;"> 
</span><math id="algorithm2.1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="algorithm2.1.1.m1.1a"><mi mathsize="90%" id="algorithm2.1.1.m1.1.1" xref="algorithm2.1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="algorithm2.1.1.m1.1b"><ci id="algorithm2.1.1.m1.1.1.cmml" xref="algorithm2.1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.1.1.m1.1c">C</annotation></semantics></math><span id="algorithm2.11.11.3" class="ltx_text" style="font-size:90%;"> is the set of all clients, </span><math id="algorithm2.2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="algorithm2.2.2.m2.1a"><msub id="algorithm2.2.2.m2.1.1" xref="algorithm2.2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.2.2.m2.1.1.2" xref="algorithm2.2.2.m2.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.2.2.m2.1.1.3" xref="algorithm2.2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm2.2.2.m2.1b"><apply id="algorithm2.2.2.m2.1.1.cmml" xref="algorithm2.2.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm2.2.2.m2.1.1.1.cmml" xref="algorithm2.2.2.m2.1.1">subscript</csymbol><ci id="algorithm2.2.2.m2.1.1.2.cmml" xref="algorithm2.2.2.m2.1.1.2">𝒟</ci><ci id="algorithm2.2.2.m2.1.1.3.cmml" xref="algorithm2.2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.2.2.m2.1c">\mathcal{D}_{i}</annotation></semantics></math><span id="algorithm2.11.11.4" class="ltx_text" style="font-size:90%;"> is the local dataset of client </span><math id="algorithm2.3.3.m3.1" class="ltx_Math" alttext="c_{i}\in C" display="inline"><semantics id="algorithm2.3.3.m3.1a"><mrow id="algorithm2.3.3.m3.1.1" xref="algorithm2.3.3.m3.1.1.cmml"><msub id="algorithm2.3.3.m3.1.1.2" xref="algorithm2.3.3.m3.1.1.2.cmml"><mi mathsize="90%" id="algorithm2.3.3.m3.1.1.2.2" xref="algorithm2.3.3.m3.1.1.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm2.3.3.m3.1.1.2.3" xref="algorithm2.3.3.m3.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="algorithm2.3.3.m3.1.1.1" xref="algorithm2.3.3.m3.1.1.1.cmml">∈</mo><mi mathsize="90%" id="algorithm2.3.3.m3.1.1.3" xref="algorithm2.3.3.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.3.3.m3.1b"><apply id="algorithm2.3.3.m3.1.1.cmml" xref="algorithm2.3.3.m3.1.1"><in id="algorithm2.3.3.m3.1.1.1.cmml" xref="algorithm2.3.3.m3.1.1.1"></in><apply id="algorithm2.3.3.m3.1.1.2.cmml" xref="algorithm2.3.3.m3.1.1.2"><csymbol cd="ambiguous" id="algorithm2.3.3.m3.1.1.2.1.cmml" xref="algorithm2.3.3.m3.1.1.2">subscript</csymbol><ci id="algorithm2.3.3.m3.1.1.2.2.cmml" xref="algorithm2.3.3.m3.1.1.2.2">𝑐</ci><ci id="algorithm2.3.3.m3.1.1.2.3.cmml" xref="algorithm2.3.3.m3.1.1.2.3">𝑖</ci></apply><ci id="algorithm2.3.3.m3.1.1.3.cmml" xref="algorithm2.3.3.m3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.3.3.m3.1c">c_{i}\in C</annotation></semantics></math><span id="algorithm2.11.11.5" class="ltx_text" style="font-size:90%;">, </span><math id="algorithm2.4.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="algorithm2.4.4.m4.1a"><mi mathsize="90%" id="algorithm2.4.4.m4.1.1" xref="algorithm2.4.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algorithm2.4.4.m4.1b"><ci id="algorithm2.4.4.m4.1.1.cmml" xref="algorithm2.4.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.4.4.m4.1c">T</annotation></semantics></math><span id="algorithm2.11.11.6" class="ltx_text" style="font-size:90%;"> is the number of communication rounds, </span><math id="algorithm2.5.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="algorithm2.5.5.m5.1a"><mi mathsize="90%" id="algorithm2.5.5.m5.1.1" xref="algorithm2.5.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="algorithm2.5.5.m5.1b"><ci id="algorithm2.5.5.m5.1.1.cmml" xref="algorithm2.5.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.5.5.m5.1c">N</annotation></semantics></math><span id="algorithm2.11.11.7" class="ltx_text" style="font-size:90%;"> is the number of clients per communication round, </span><math id="algorithm2.6.6.m6.1" class="ltx_Math" alttext="L" display="inline"><semantics id="algorithm2.6.6.m6.1a"><mi mathsize="90%" id="algorithm2.6.6.m6.1.1" xref="algorithm2.6.6.m6.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="algorithm2.6.6.m6.1b"><ci id="algorithm2.6.6.m6.1.1.cmml" xref="algorithm2.6.6.m6.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.6.6.m6.1c">L</annotation></semantics></math><span id="algorithm2.11.11.8" class="ltx_text" style="font-size:90%;"> is the client queue length, where </span><math id="algorithm2.7.7.m7.1" class="ltx_Math" alttext="L" display="inline"><semantics id="algorithm2.7.7.m7.1a"><mi mathsize="90%" id="algorithm2.7.7.m7.1.1" xref="algorithm2.7.7.m7.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="algorithm2.7.7.m7.1b"><ci id="algorithm2.7.7.m7.1.1.cmml" xref="algorithm2.7.7.m7.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.7.7.m7.1c">L</annotation></semantics></math><span id="algorithm2.11.11.9" class="ltx_text" style="font-size:90%;"> divides </span><math id="algorithm2.8.8.m8.1" class="ltx_Math" alttext="N" display="inline"><semantics id="algorithm2.8.8.m8.1a"><mi mathsize="90%" id="algorithm2.8.8.m8.1.1" xref="algorithm2.8.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="algorithm2.8.8.m8.1b"><ci id="algorithm2.8.8.m8.1.1.cmml" xref="algorithm2.8.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.8.8.m8.1c">N</annotation></semantics></math><span id="algorithm2.11.11.10" class="ltx_text" style="font-size:90%;">, </span><math id="algorithm2.9.9.m9.1" class="ltx_Math" alttext="B" display="inline"><semantics id="algorithm2.9.9.m9.1a"><mi mathsize="90%" id="algorithm2.9.9.m9.1.1" xref="algorithm2.9.9.m9.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="algorithm2.9.9.m9.1b"><ci id="algorithm2.9.9.m9.1.1.cmml" xref="algorithm2.9.9.m9.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.9.9.m9.1c">B</annotation></semantics></math><span id="algorithm2.11.11.11" class="ltx_text" style="font-size:90%;"> is the batch size, </span><math id="algorithm2.10.10.m10.1" class="ltx_Math" alttext="E" display="inline"><semantics id="algorithm2.10.10.m10.1a"><mi mathsize="90%" id="algorithm2.10.10.m10.1.1" xref="algorithm2.10.10.m10.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="algorithm2.10.10.m10.1b"><ci id="algorithm2.10.10.m10.1.1.cmml" xref="algorithm2.10.10.m10.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.10.10.m10.1c">E</annotation></semantics></math><span id="algorithm2.11.11.12" class="ltx_text" style="font-size:90%;"> is the number of local epochs, and </span><math id="algorithm2.11.11.m11.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="algorithm2.11.11.m11.1a"><mi mathsize="90%" id="algorithm2.11.11.m11.1.1" xref="algorithm2.11.11.m11.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="algorithm2.11.11.m11.1b"><ci id="algorithm2.11.11.m11.1.1.cmml" xref="algorithm2.11.11.m11.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.11.11.m11.1c">\eta</annotation></semantics></math><span id="algorithm2.11.11.13" class="ltx_text" style="font-size:90%;"> is the learning rate
</span>
</div>
<div id="algorithm2.12.12" class="ltx_listingline">
<span id="algorithm2.12.12.1" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.12.12.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Output :</span><span id="algorithm2.12.12.3" class="ltx_text" style="font-size:90%;"> Global Model Parametrization </span><math id="algorithm2.12.12.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{T}" display="inline"><semantics id="algorithm2.12.12.m1.1a"><msub id="algorithm2.12.12.m1.1.1" xref="algorithm2.12.12.m1.1.1.cmml"><mi mathsize="90%" id="algorithm2.12.12.m1.1.1.2" xref="algorithm2.12.12.m1.1.1.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.12.12.m1.1.1.3" xref="algorithm2.12.12.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm2.12.12.m1.1b"><apply id="algorithm2.12.12.m1.1.1.cmml" xref="algorithm2.12.12.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.12.12.m1.1.1.1.cmml" xref="algorithm2.12.12.m1.1.1">subscript</csymbol><ci id="algorithm2.12.12.m1.1.1.2.cmml" xref="algorithm2.12.12.m1.1.1.2">𝜽</ci><ci id="algorithm2.12.12.m1.1.1.3.cmml" xref="algorithm2.12.12.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.12.12.m1.1c">\boldsymbol{\theta}_{T}</annotation></semantics></math>
</div>
<div id="algorithm2.36.40" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.40.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span><span id="algorithm2.36.40.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.41" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.41.1.1.1" class="ltx_text" style="font-size:90%;">5</span></span><span id="algorithm2.36.41.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.42" class="ltx_listingline">
<span id="algorithm2.36.42.1" class="ltx_text" style="font-size:90%;"><span id="algorithm2.36.42.1.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm2.36.42.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">Runs on the central server */</span>
</div>
<div id="algorithm2.13.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.13.13.1.1.1" class="ltx_text" style="font-size:90%;">6</span></span><span id="algorithm2.13.13.2" class="ltx_text" style="font-size:90%;">
Initialize </span><math id="algorithm2.13.13.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{0}" display="inline"><semantics id="algorithm2.13.13.m1.1a"><msub id="algorithm2.13.13.m1.1.1" xref="algorithm2.13.13.m1.1.1.cmml"><mi mathsize="90%" id="algorithm2.13.13.m1.1.1.2" xref="algorithm2.13.13.m1.1.1.2.cmml">𝜽</mi><mn mathsize="90%" id="algorithm2.13.13.m1.1.1.3" xref="algorithm2.13.13.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm2.13.13.m1.1b"><apply id="algorithm2.13.13.m1.1.1.cmml" xref="algorithm2.13.13.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.13.13.m1.1.1.1.cmml" xref="algorithm2.13.13.m1.1.1">subscript</csymbol><ci id="algorithm2.13.13.m1.1.1.2.cmml" xref="algorithm2.13.13.m1.1.1.2">𝜽</ci><cn type="integer" id="algorithm2.13.13.m1.1.1.3.cmml" xref="algorithm2.13.13.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.13.13.m1.1c">\boldsymbol{\theta}_{0}</annotation></semantics></math><span id="algorithm2.13.13.3" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.14.14.1.1.1" class="ltx_text" style="font-size:90%;">7</span></span><span id="algorithm2.14.14.2" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.14.14.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm2.14.14.4" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.14.14.5" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm2.14.14.6" class="ltx_text" style="font-size:90%;"> communication round </span><math id="algorithm2.14.14.m1.3" class="ltx_Math" alttext="t=1,\dots,T" display="inline"><semantics id="algorithm2.14.14.m1.3a"><mrow id="algorithm2.14.14.m1.3.4" xref="algorithm2.14.14.m1.3.4.cmml"><mi mathsize="90%" id="algorithm2.14.14.m1.3.4.2" xref="algorithm2.14.14.m1.3.4.2.cmml">t</mi><mo mathsize="90%" id="algorithm2.14.14.m1.3.4.1" xref="algorithm2.14.14.m1.3.4.1.cmml">=</mo><mrow id="algorithm2.14.14.m1.3.4.3.2" xref="algorithm2.14.14.m1.3.4.3.1.cmml"><mn mathsize="90%" id="algorithm2.14.14.m1.1.1" xref="algorithm2.14.14.m1.1.1.cmml">1</mn><mo mathsize="90%" id="algorithm2.14.14.m1.3.4.3.2.1" xref="algorithm2.14.14.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm2.14.14.m1.2.2" xref="algorithm2.14.14.m1.2.2.cmml">…</mi><mo mathsize="90%" id="algorithm2.14.14.m1.3.4.3.2.2" xref="algorithm2.14.14.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="algorithm2.14.14.m1.3.3" xref="algorithm2.14.14.m1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.14.14.m1.3b"><apply id="algorithm2.14.14.m1.3.4.cmml" xref="algorithm2.14.14.m1.3.4"><eq id="algorithm2.14.14.m1.3.4.1.cmml" xref="algorithm2.14.14.m1.3.4.1"></eq><ci id="algorithm2.14.14.m1.3.4.2.cmml" xref="algorithm2.14.14.m1.3.4.2">𝑡</ci><list id="algorithm2.14.14.m1.3.4.3.1.cmml" xref="algorithm2.14.14.m1.3.4.3.2"><cn type="integer" id="algorithm2.14.14.m1.1.1.cmml" xref="algorithm2.14.14.m1.1.1">1</cn><ci id="algorithm2.14.14.m1.2.2.cmml" xref="algorithm2.14.14.m1.2.2">…</ci><ci id="algorithm2.14.14.m1.3.3.cmml" xref="algorithm2.14.14.m1.3.3">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.14.14.m1.3c">t=1,\dots,T</annotation></semantics></math><span id="algorithm2.14.14.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.14.14.8" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm2.16.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.16.16.1.1.1" class="ltx_text" style="font-size:90%;">8</span></span><span id="algorithm2.16.16.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.16.16.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.16.16.4" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.15.15.m1.1" class="ltx_Math" alttext="C_{t}\leftarrow N" display="inline"><semantics id="algorithm2.15.15.m1.1a"><mrow id="algorithm2.15.15.m1.1.1" xref="algorithm2.15.15.m1.1.1.cmml"><msub id="algorithm2.15.15.m1.1.1.2" xref="algorithm2.15.15.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm2.15.15.m1.1.1.2.2" xref="algorithm2.15.15.m1.1.1.2.2.cmml">C</mi><mi mathsize="90%" id="algorithm2.15.15.m1.1.1.2.3" xref="algorithm2.15.15.m1.1.1.2.3.cmml">t</mi></msub><mo mathsize="90%" stretchy="false" id="algorithm2.15.15.m1.1.1.1" xref="algorithm2.15.15.m1.1.1.1.cmml">←</mo><mi mathsize="90%" id="algorithm2.15.15.m1.1.1.3" xref="algorithm2.15.15.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.15.15.m1.1b"><apply id="algorithm2.15.15.m1.1.1.cmml" xref="algorithm2.15.15.m1.1.1"><ci id="algorithm2.15.15.m1.1.1.1.cmml" xref="algorithm2.15.15.m1.1.1.1">←</ci><apply id="algorithm2.15.15.m1.1.1.2.cmml" xref="algorithm2.15.15.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.15.15.m1.1.1.2.1.cmml" xref="algorithm2.15.15.m1.1.1.2">subscript</csymbol><ci id="algorithm2.15.15.m1.1.1.2.2.cmml" xref="algorithm2.15.15.m1.1.1.2.2">𝐶</ci><ci id="algorithm2.15.15.m1.1.1.2.3.cmml" xref="algorithm2.15.15.m1.1.1.2.3">𝑡</ci></apply><ci id="algorithm2.15.15.m1.1.1.3.cmml" xref="algorithm2.15.15.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.15.15.m1.1c">C_{t}\leftarrow N</annotation></semantics></math><span id="algorithm2.16.16.5" class="ltx_text" style="font-size:90%;"> random clients sub-sampled from </span><math id="algorithm2.16.16.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="algorithm2.16.16.m2.1a"><mi mathsize="90%" id="algorithm2.16.16.m2.1.1" xref="algorithm2.16.16.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="algorithm2.16.16.m2.1b"><ci id="algorithm2.16.16.m2.1.1.cmml" xref="algorithm2.16.16.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.16.16.m2.1c">C</annotation></semantics></math><span id="algorithm2.16.16.6" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.17.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.17.17.1.1.1" class="ltx_text" style="font-size:90%;">9</span></span><span id="algorithm2.17.17.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.17.17.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.17.17.4" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.17.17.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm2.17.17.6" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm2.17.17.m1.3" class="ltx_Math" alttext="k=1,\dots,\frac{N}{L}" display="inline"><semantics id="algorithm2.17.17.m1.3a"><mrow id="algorithm2.17.17.m1.3.4" xref="algorithm2.17.17.m1.3.4.cmml"><mi mathsize="90%" id="algorithm2.17.17.m1.3.4.2" xref="algorithm2.17.17.m1.3.4.2.cmml">k</mi><mo mathsize="90%" id="algorithm2.17.17.m1.3.4.1" xref="algorithm2.17.17.m1.3.4.1.cmml">=</mo><mrow id="algorithm2.17.17.m1.3.4.3.2" xref="algorithm2.17.17.m1.3.4.3.1.cmml"><mn mathsize="90%" id="algorithm2.17.17.m1.1.1" xref="algorithm2.17.17.m1.1.1.cmml">1</mn><mo mathsize="90%" id="algorithm2.17.17.m1.3.4.3.2.1" xref="algorithm2.17.17.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm2.17.17.m1.2.2" xref="algorithm2.17.17.m1.2.2.cmml">…</mi><mo mathsize="90%" id="algorithm2.17.17.m1.3.4.3.2.2" xref="algorithm2.17.17.m1.3.4.3.1.cmml">,</mo><mfrac id="algorithm2.17.17.m1.3.3" xref="algorithm2.17.17.m1.3.3.cmml"><mi mathsize="90%" id="algorithm2.17.17.m1.3.3.2" xref="algorithm2.17.17.m1.3.3.2.cmml">N</mi><mi mathsize="90%" id="algorithm2.17.17.m1.3.3.3" xref="algorithm2.17.17.m1.3.3.3.cmml">L</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.17.17.m1.3b"><apply id="algorithm2.17.17.m1.3.4.cmml" xref="algorithm2.17.17.m1.3.4"><eq id="algorithm2.17.17.m1.3.4.1.cmml" xref="algorithm2.17.17.m1.3.4.1"></eq><ci id="algorithm2.17.17.m1.3.4.2.cmml" xref="algorithm2.17.17.m1.3.4.2">𝑘</ci><list id="algorithm2.17.17.m1.3.4.3.1.cmml" xref="algorithm2.17.17.m1.3.4.3.2"><cn type="integer" id="algorithm2.17.17.m1.1.1.cmml" xref="algorithm2.17.17.m1.1.1">1</cn><ci id="algorithm2.17.17.m1.2.2.cmml" xref="algorithm2.17.17.m1.2.2">…</ci><apply id="algorithm2.17.17.m1.3.3.cmml" xref="algorithm2.17.17.m1.3.3"><divide id="algorithm2.17.17.m1.3.3.1.cmml" xref="algorithm2.17.17.m1.3.3"></divide><ci id="algorithm2.17.17.m1.3.3.2.cmml" xref="algorithm2.17.17.m1.3.3.2">𝑁</ci><ci id="algorithm2.17.17.m1.3.3.3.cmml" xref="algorithm2.17.17.m1.3.3.3">𝐿</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.17.17.m1.3c">k=1,\dots,\frac{N}{L}</annotation></semantics></math><span id="algorithm2.17.17.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.17.17.8" class="ltx_text ltx_font_bold" style="font-size:90%;">in parallel</span><span id="algorithm2.17.17.9" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.17.17.10" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm2.18.18" class="ltx_listingline">
<span id="algorithm2.18.18.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.18.18.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.18.18.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.18.18.5" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.18.18.6" class="ltx_text" style="font-size:90%;"><span id="algorithm2.18.18.6.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm2.18.18.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">First client in the <math id="algorithm2.18.18.1.m1.1" class="ltx_Math" alttext="k^{\text{th}}" display="inline"><semantics id="algorithm2.18.18.1.m1.1a"><msup id="algorithm2.18.18.1.m1.1.1" xref="algorithm2.18.18.1.m1.1.1.cmml"><mi mathcolor="#0000FF" id="algorithm2.18.18.1.m1.1.1.2" xref="algorithm2.18.18.1.m1.1.1.2.cmml">k</mi><mtext class="ltx_mathvariant_monospace" mathcolor="#0000FF" id="algorithm2.18.18.1.m1.1.1.3" xref="algorithm2.18.18.1.m1.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="algorithm2.18.18.1.m1.1b"><apply id="algorithm2.18.18.1.m1.1.1.cmml" xref="algorithm2.18.18.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.18.18.1.m1.1.1.1.cmml" xref="algorithm2.18.18.1.m1.1.1">superscript</csymbol><ci id="algorithm2.18.18.1.m1.1.1.2.cmml" xref="algorithm2.18.18.1.m1.1.1.2">𝑘</ci><ci id="algorithm2.18.18.1.m1.1.1.3a.cmml" xref="algorithm2.18.18.1.m1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathcolor="#0000FF" mathsize="70%" id="algorithm2.18.18.1.m1.1.1.3.cmml" xref="algorithm2.18.18.1.m1.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.18.18.1.m1.1c">k^{\text{th}}</annotation></semantics></math> queue receives the global model */</span>
</div>
<div id="algorithm2.19.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.19.19.1.1.1" class="ltx_text" style="font-size:90%;">10</span></span><span id="algorithm2.19.19.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.19.19.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.19.19.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.19.19.5" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.19.19.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}^{k}_{t}\leftarrow\boldsymbol{\theta}_{t-1}" display="inline"><semantics id="algorithm2.19.19.m1.1a"><mrow id="algorithm2.19.19.m1.1.1" xref="algorithm2.19.19.m1.1.1.cmml"><msubsup id="algorithm2.19.19.m1.1.1.2" xref="algorithm2.19.19.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm2.19.19.m1.1.1.2.2.2" xref="algorithm2.19.19.m1.1.1.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.19.19.m1.1.1.2.3" xref="algorithm2.19.19.m1.1.1.2.3.cmml">t</mi><mi mathsize="90%" id="algorithm2.19.19.m1.1.1.2.2.3" xref="algorithm2.19.19.m1.1.1.2.2.3.cmml">k</mi></msubsup><mo mathsize="90%" stretchy="false" id="algorithm2.19.19.m1.1.1.1" xref="algorithm2.19.19.m1.1.1.1.cmml">←</mo><msub id="algorithm2.19.19.m1.1.1.3" xref="algorithm2.19.19.m1.1.1.3.cmml"><mi mathsize="90%" id="algorithm2.19.19.m1.1.1.3.2" xref="algorithm2.19.19.m1.1.1.3.2.cmml">𝜽</mi><mrow id="algorithm2.19.19.m1.1.1.3.3" xref="algorithm2.19.19.m1.1.1.3.3.cmml"><mi mathsize="90%" id="algorithm2.19.19.m1.1.1.3.3.2" xref="algorithm2.19.19.m1.1.1.3.3.2.cmml">t</mi><mo mathsize="90%" id="algorithm2.19.19.m1.1.1.3.3.1" xref="algorithm2.19.19.m1.1.1.3.3.1.cmml">−</mo><mn mathsize="90%" id="algorithm2.19.19.m1.1.1.3.3.3" xref="algorithm2.19.19.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.19.19.m1.1b"><apply id="algorithm2.19.19.m1.1.1.cmml" xref="algorithm2.19.19.m1.1.1"><ci id="algorithm2.19.19.m1.1.1.1.cmml" xref="algorithm2.19.19.m1.1.1.1">←</ci><apply id="algorithm2.19.19.m1.1.1.2.cmml" xref="algorithm2.19.19.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.19.19.m1.1.1.2.1.cmml" xref="algorithm2.19.19.m1.1.1.2">subscript</csymbol><apply id="algorithm2.19.19.m1.1.1.2.2.cmml" xref="algorithm2.19.19.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.19.19.m1.1.1.2.2.1.cmml" xref="algorithm2.19.19.m1.1.1.2">superscript</csymbol><ci id="algorithm2.19.19.m1.1.1.2.2.2.cmml" xref="algorithm2.19.19.m1.1.1.2.2.2">𝜽</ci><ci id="algorithm2.19.19.m1.1.1.2.2.3.cmml" xref="algorithm2.19.19.m1.1.1.2.2.3">𝑘</ci></apply><ci id="algorithm2.19.19.m1.1.1.2.3.cmml" xref="algorithm2.19.19.m1.1.1.2.3">𝑡</ci></apply><apply id="algorithm2.19.19.m1.1.1.3.cmml" xref="algorithm2.19.19.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm2.19.19.m1.1.1.3.1.cmml" xref="algorithm2.19.19.m1.1.1.3">subscript</csymbol><ci id="algorithm2.19.19.m1.1.1.3.2.cmml" xref="algorithm2.19.19.m1.1.1.3.2">𝜽</ci><apply id="algorithm2.19.19.m1.1.1.3.3.cmml" xref="algorithm2.19.19.m1.1.1.3.3"><minus id="algorithm2.19.19.m1.1.1.3.3.1.cmml" xref="algorithm2.19.19.m1.1.1.3.3.1"></minus><ci id="algorithm2.19.19.m1.1.1.3.3.2.cmml" xref="algorithm2.19.19.m1.1.1.3.3.2">𝑡</ci><cn type="integer" id="algorithm2.19.19.m1.1.1.3.3.3.cmml" xref="algorithm2.19.19.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.19.19.m1.1c">\boldsymbol{\theta}^{k}_{t}\leftarrow\boldsymbol{\theta}_{t-1}</annotation></semantics></math>
</div>
<div id="algorithm2.36.43" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.43.1.1.1" class="ltx_text" style="font-size:90%;">11</span></span><span id="algorithm2.36.43.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.43.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.43.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.43.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.44" class="ltx_listingline">
<span id="algorithm2.36.44.1" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.44.2" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.44.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.44.4" class="ltx_text" style="font-size:90%;"><span id="algorithm2.36.44.4.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm2.36.44.5" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">Dataset sizes of the clients in the queue are aggregated for the weighted mean */</span>
</div>
<div id="algorithm2.20.20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.20.20.1.1.1" class="ltx_text" style="font-size:90%;">12</span></span><span id="algorithm2.20.20.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.20.20.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.20.20.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.20.20.5" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.20.20.m1.1" class="ltx_Math" alttext="s_{k}\leftarrow 0" display="inline"><semantics id="algorithm2.20.20.m1.1a"><mrow id="algorithm2.20.20.m1.1.1" xref="algorithm2.20.20.m1.1.1.cmml"><msub id="algorithm2.20.20.m1.1.1.2" xref="algorithm2.20.20.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm2.20.20.m1.1.1.2.2" xref="algorithm2.20.20.m1.1.1.2.2.cmml">s</mi><mi mathsize="90%" id="algorithm2.20.20.m1.1.1.2.3" xref="algorithm2.20.20.m1.1.1.2.3.cmml">k</mi></msub><mo mathsize="90%" stretchy="false" id="algorithm2.20.20.m1.1.1.1" xref="algorithm2.20.20.m1.1.1.1.cmml">←</mo><mn mathsize="90%" id="algorithm2.20.20.m1.1.1.3" xref="algorithm2.20.20.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.20.20.m1.1b"><apply id="algorithm2.20.20.m1.1.1.cmml" xref="algorithm2.20.20.m1.1.1"><ci id="algorithm2.20.20.m1.1.1.1.cmml" xref="algorithm2.20.20.m1.1.1.1">←</ci><apply id="algorithm2.20.20.m1.1.1.2.cmml" xref="algorithm2.20.20.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.20.20.m1.1.1.2.1.cmml" xref="algorithm2.20.20.m1.1.1.2">subscript</csymbol><ci id="algorithm2.20.20.m1.1.1.2.2.cmml" xref="algorithm2.20.20.m1.1.1.2.2">𝑠</ci><ci id="algorithm2.20.20.m1.1.1.2.3.cmml" xref="algorithm2.20.20.m1.1.1.2.3">𝑘</ci></apply><cn type="integer" id="algorithm2.20.20.m1.1.1.3.cmml" xref="algorithm2.20.20.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.20.20.m1.1c">s_{k}\leftarrow 0</annotation></semantics></math><span id="algorithm2.20.20.6" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.22.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.22.22.1.1.1" class="ltx_text" style="font-size:90%;">13</span></span><span id="algorithm2.22.22.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.22.22.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.22.22.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.22.22.5" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.22.22.6" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm2.22.22.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.22.22.8" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm2.22.22.9" class="ltx_text" style="font-size:90%;"> client </span><math id="algorithm2.21.21.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="algorithm2.21.21.m1.1a"><msub id="algorithm2.21.21.m1.1.1" xref="algorithm2.21.21.m1.1.1.cmml"><mi mathsize="90%" id="algorithm2.21.21.m1.1.1.2" xref="algorithm2.21.21.m1.1.1.2.cmml">c</mi><mi mathsize="90%" id="algorithm2.21.21.m1.1.1.3" xref="algorithm2.21.21.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm2.21.21.m1.1b"><apply id="algorithm2.21.21.m1.1.1.cmml" xref="algorithm2.21.21.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.21.21.m1.1.1.1.cmml" xref="algorithm2.21.21.m1.1.1">subscript</csymbol><ci id="algorithm2.21.21.m1.1.1.2.cmml" xref="algorithm2.21.21.m1.1.1.2">𝑐</ci><ci id="algorithm2.21.21.m1.1.1.3.cmml" xref="algorithm2.21.21.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.21.21.m1.1c">c_{i}</annotation></semantics></math><span id="algorithm2.22.22.10" class="ltx_text" style="font-size:90%;"> in </span><math id="algorithm2.22.22.m2.1" class="ltx_math_unparsed" alttext="C_{t}[(k-1)L+1:kL]" display="inline"><semantics id="algorithm2.22.22.m2.1a"><mrow id="algorithm2.22.22.m2.1b"><msub id="algorithm2.22.22.m2.1.1"><mi mathsize="90%" id="algorithm2.22.22.m2.1.1.2">C</mi><mi mathsize="90%" id="algorithm2.22.22.m2.1.1.3">t</mi></msub><mrow id="algorithm2.22.22.m2.1.2"><mo maxsize="90%" minsize="90%" id="algorithm2.22.22.m2.1.2.1">[</mo><mrow id="algorithm2.22.22.m2.1.2.2"><mo maxsize="90%" minsize="90%" id="algorithm2.22.22.m2.1.2.2.1">(</mo><mi mathsize="90%" id="algorithm2.22.22.m2.1.2.2.2">k</mi><mo mathsize="90%" id="algorithm2.22.22.m2.1.2.2.3">−</mo><mn mathsize="90%" id="algorithm2.22.22.m2.1.2.2.4">1</mn><mo maxsize="90%" minsize="90%" id="algorithm2.22.22.m2.1.2.2.5">)</mo></mrow><mi mathsize="90%" id="algorithm2.22.22.m2.1.2.3">L</mi><mo mathsize="90%" id="algorithm2.22.22.m2.1.2.4">+</mo><mn mathsize="90%" id="algorithm2.22.22.m2.1.2.5">1</mn><mo lspace="0.278em" mathsize="90%" rspace="0.278em" id="algorithm2.22.22.m2.1.2.6">:</mo><mi mathsize="90%" id="algorithm2.22.22.m2.1.2.7">k</mi><mi mathsize="90%" id="algorithm2.22.22.m2.1.2.8">L</mi><mo maxsize="90%" minsize="90%" id="algorithm2.22.22.m2.1.2.9">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="algorithm2.22.22.m2.1c">C_{t}[(k-1)L+1:kL]</annotation></semantics></math><span id="algorithm2.22.22.11" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.22.22.12" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm2.24.24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.24.24.2.1.1" class="ltx_text" style="font-size:90%;">14</span></span><span id="algorithm2.24.24.3" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.24.24.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.24.24.5" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.24.24.6" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.24.24.7" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.23.23.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}^{k}_{t}\leftarrow" display="inline"><semantics id="algorithm2.23.23.m1.1a"><mrow id="algorithm2.23.23.m1.1.1" xref="algorithm2.23.23.m1.1.1.cmml"><msubsup id="algorithm2.23.23.m1.1.1.2" xref="algorithm2.23.23.m1.1.1.2.cmml"><mi mathsize="90%" id="algorithm2.23.23.m1.1.1.2.2.2" xref="algorithm2.23.23.m1.1.1.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.23.23.m1.1.1.2.3" xref="algorithm2.23.23.m1.1.1.2.3.cmml">t</mi><mi mathsize="90%" id="algorithm2.23.23.m1.1.1.2.2.3" xref="algorithm2.23.23.m1.1.1.2.2.3.cmml">k</mi></msubsup><mo mathsize="90%" stretchy="false" id="algorithm2.23.23.m1.1.1.1" xref="algorithm2.23.23.m1.1.1.1.cmml">←</mo><mi id="algorithm2.23.23.m1.1.1.3" xref="algorithm2.23.23.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.23.23.m1.1b"><apply id="algorithm2.23.23.m1.1.1.cmml" xref="algorithm2.23.23.m1.1.1"><ci id="algorithm2.23.23.m1.1.1.1.cmml" xref="algorithm2.23.23.m1.1.1.1">←</ci><apply id="algorithm2.23.23.m1.1.1.2.cmml" xref="algorithm2.23.23.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.23.23.m1.1.1.2.1.cmml" xref="algorithm2.23.23.m1.1.1.2">subscript</csymbol><apply id="algorithm2.23.23.m1.1.1.2.2.cmml" xref="algorithm2.23.23.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.23.23.m1.1.1.2.2.1.cmml" xref="algorithm2.23.23.m1.1.1.2">superscript</csymbol><ci id="algorithm2.23.23.m1.1.1.2.2.2.cmml" xref="algorithm2.23.23.m1.1.1.2.2.2">𝜽</ci><ci id="algorithm2.23.23.m1.1.1.2.2.3.cmml" xref="algorithm2.23.23.m1.1.1.2.2.3">𝑘</ci></apply><ci id="algorithm2.23.23.m1.1.1.2.3.cmml" xref="algorithm2.23.23.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="algorithm2.23.23.m1.1.1.3.cmml" xref="algorithm2.23.23.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.23.23.m1.1c">\boldsymbol{\theta}^{k}_{t}\leftarrow</annotation></semantics></math><span id="algorithm2.24.24.8" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.24.24.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">UpdateClient<math id="algorithm2.24.24.1.m1.2" class="ltx_Math" alttext="(\boldsymbol{\theta}^{k}_{t},i)" display="inline"><semantics id="algorithm2.24.24.1.m1.2a"><mrow id="algorithm2.24.24.1.m1.2.2.1" xref="algorithm2.24.24.1.m1.2.2.2.cmml"><mo stretchy="false" id="algorithm2.24.24.1.m1.2.2.1.2" xref="algorithm2.24.24.1.m1.2.2.2.cmml">(</mo><msubsup id="algorithm2.24.24.1.m1.2.2.1.1" xref="algorithm2.24.24.1.m1.2.2.1.1.cmml"><mi id="algorithm2.24.24.1.m1.2.2.1.1.2.2" xref="algorithm2.24.24.1.m1.2.2.1.1.2.2.cmml">𝜽</mi><mi id="algorithm2.24.24.1.m1.2.2.1.1.3" xref="algorithm2.24.24.1.m1.2.2.1.1.3.cmml">t</mi><mi id="algorithm2.24.24.1.m1.2.2.1.1.2.3" xref="algorithm2.24.24.1.m1.2.2.1.1.2.3.cmml">k</mi></msubsup><mo id="algorithm2.24.24.1.m1.2.2.1.3" xref="algorithm2.24.24.1.m1.2.2.2.cmml">,</mo><mi id="algorithm2.24.24.1.m1.1.1" xref="algorithm2.24.24.1.m1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm2.24.24.1.m1.2.2.1.4" xref="algorithm2.24.24.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.24.24.1.m1.2b"><interval closure="open" id="algorithm2.24.24.1.m1.2.2.2.cmml" xref="algorithm2.24.24.1.m1.2.2.1"><apply id="algorithm2.24.24.1.m1.2.2.1.1.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="algorithm2.24.24.1.m1.2.2.1.1.1.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1">subscript</csymbol><apply id="algorithm2.24.24.1.m1.2.2.1.1.2.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="algorithm2.24.24.1.m1.2.2.1.1.2.1.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1">superscript</csymbol><ci id="algorithm2.24.24.1.m1.2.2.1.1.2.2.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1.2.2">𝜽</ci><ci id="algorithm2.24.24.1.m1.2.2.1.1.2.3.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1.2.3">𝑘</ci></apply><ci id="algorithm2.24.24.1.m1.2.2.1.1.3.cmml" xref="algorithm2.24.24.1.m1.2.2.1.1.3">𝑡</ci></apply><ci id="algorithm2.24.24.1.m1.1.1.cmml" xref="algorithm2.24.24.1.m1.1.1">𝑖</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.24.24.1.m1.2c">(\boldsymbol{\theta}^{k}_{t},i)</annotation></semantics></math></span><span id="algorithm2.24.24.9" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.25.25" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.25.25.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span><span id="algorithm2.25.25.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.25.25.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.25.25.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.25.25.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.25.25.6" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.25.25.m1.1" class="ltx_Math" alttext="s_{k}\leftarrow s_{k}+\left|\mathcal{D}_{i}\right|" display="inline"><semantics id="algorithm2.25.25.m1.1a"><mrow id="algorithm2.25.25.m1.1.1" xref="algorithm2.25.25.m1.1.1.cmml"><msub id="algorithm2.25.25.m1.1.1.3" xref="algorithm2.25.25.m1.1.1.3.cmml"><mi mathsize="90%" id="algorithm2.25.25.m1.1.1.3.2" xref="algorithm2.25.25.m1.1.1.3.2.cmml">s</mi><mi mathsize="90%" id="algorithm2.25.25.m1.1.1.3.3" xref="algorithm2.25.25.m1.1.1.3.3.cmml">k</mi></msub><mo mathsize="90%" stretchy="false" id="algorithm2.25.25.m1.1.1.2" xref="algorithm2.25.25.m1.1.1.2.cmml">←</mo><mrow id="algorithm2.25.25.m1.1.1.1" xref="algorithm2.25.25.m1.1.1.1.cmml"><msub id="algorithm2.25.25.m1.1.1.1.3" xref="algorithm2.25.25.m1.1.1.1.3.cmml"><mi mathsize="90%" id="algorithm2.25.25.m1.1.1.1.3.2" xref="algorithm2.25.25.m1.1.1.1.3.2.cmml">s</mi><mi mathsize="90%" id="algorithm2.25.25.m1.1.1.1.3.3" xref="algorithm2.25.25.m1.1.1.1.3.3.cmml">k</mi></msub><mo mathsize="90%" id="algorithm2.25.25.m1.1.1.1.2" xref="algorithm2.25.25.m1.1.1.1.2.cmml">+</mo><mrow id="algorithm2.25.25.m1.1.1.1.1.1" xref="algorithm2.25.25.m1.1.1.1.1.2.cmml"><mo id="algorithm2.25.25.m1.1.1.1.1.1.2" xref="algorithm2.25.25.m1.1.1.1.1.2.1.cmml">|</mo><msub id="algorithm2.25.25.m1.1.1.1.1.1.1" xref="algorithm2.25.25.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.25.25.m1.1.1.1.1.1.1.2" xref="algorithm2.25.25.m1.1.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.25.25.m1.1.1.1.1.1.1.3" xref="algorithm2.25.25.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm2.25.25.m1.1.1.1.1.1.3" xref="algorithm2.25.25.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.25.25.m1.1b"><apply id="algorithm2.25.25.m1.1.1.cmml" xref="algorithm2.25.25.m1.1.1"><ci id="algorithm2.25.25.m1.1.1.2.cmml" xref="algorithm2.25.25.m1.1.1.2">←</ci><apply id="algorithm2.25.25.m1.1.1.3.cmml" xref="algorithm2.25.25.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm2.25.25.m1.1.1.3.1.cmml" xref="algorithm2.25.25.m1.1.1.3">subscript</csymbol><ci id="algorithm2.25.25.m1.1.1.3.2.cmml" xref="algorithm2.25.25.m1.1.1.3.2">𝑠</ci><ci id="algorithm2.25.25.m1.1.1.3.3.cmml" xref="algorithm2.25.25.m1.1.1.3.3">𝑘</ci></apply><apply id="algorithm2.25.25.m1.1.1.1.cmml" xref="algorithm2.25.25.m1.1.1.1"><plus id="algorithm2.25.25.m1.1.1.1.2.cmml" xref="algorithm2.25.25.m1.1.1.1.2"></plus><apply id="algorithm2.25.25.m1.1.1.1.3.cmml" xref="algorithm2.25.25.m1.1.1.1.3"><csymbol cd="ambiguous" id="algorithm2.25.25.m1.1.1.1.3.1.cmml" xref="algorithm2.25.25.m1.1.1.1.3">subscript</csymbol><ci id="algorithm2.25.25.m1.1.1.1.3.2.cmml" xref="algorithm2.25.25.m1.1.1.1.3.2">𝑠</ci><ci id="algorithm2.25.25.m1.1.1.1.3.3.cmml" xref="algorithm2.25.25.m1.1.1.1.3.3">𝑘</ci></apply><apply id="algorithm2.25.25.m1.1.1.1.1.2.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1"><abs id="algorithm2.25.25.m1.1.1.1.1.2.1.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1.2"></abs><apply id="algorithm2.25.25.m1.1.1.1.1.1.1.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm2.25.25.m1.1.1.1.1.1.1.1.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1.1">subscript</csymbol><ci id="algorithm2.25.25.m1.1.1.1.1.1.1.2.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1.1.2">𝒟</ci><ci id="algorithm2.25.25.m1.1.1.1.1.1.1.3.cmml" xref="algorithm2.25.25.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.25.25.m1.1c">s_{k}\leftarrow s_{k}+\left|\mathcal{D}_{i}\right|</annotation></semantics></math><span id="algorithm2.25.25.7" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.36.45" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.45.1.1.1" class="ltx_text" style="font-size:90%;">16</span></span><span id="algorithm2.36.45.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.45.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.45.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.45.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.45.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.46" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.46.1.1.1" class="ltx_text" style="font-size:90%;">17</span></span><span id="algorithm2.36.46.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.46.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.46.4" class="ltx_text" style="font-size:90%;">   </span>
</div>
<div id="algorithm2.36.47" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.47.1.1.1" class="ltx_text" style="font-size:90%;">18</span></span><span id="algorithm2.36.47.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.47.3" class="ltx_text" style="font-size:90%;">   </span>
</div>
<div id="algorithm2.26.26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.26.26.1.1.1" class="ltx_text" style="font-size:90%;">19</span></span><span id="algorithm2.26.26.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.26.26.3" class="ltx_text" style="font-size:90%;">   </span><math id="algorithm2.26.26.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{t}\leftarrow\sum_{k=1}^{\frac{N}{L}}\frac{s_{k}}{\left|\bigcup_{c_{i}\in C_{t}}\mathcal{D}_{i}\right|}\boldsymbol{\theta}^{k}_{t}" display="inline"><semantics id="algorithm2.26.26.m1.1a"><mrow id="algorithm2.26.26.m1.1.2" xref="algorithm2.26.26.m1.1.2.cmml"><msub id="algorithm2.26.26.m1.1.2.2" xref="algorithm2.26.26.m1.1.2.2.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.2.2" xref="algorithm2.26.26.m1.1.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.2.3" xref="algorithm2.26.26.m1.1.2.2.3.cmml">t</mi></msub><mo mathsize="90%" rspace="0.111em" stretchy="false" id="algorithm2.26.26.m1.1.2.1" xref="algorithm2.26.26.m1.1.2.1.cmml">←</mo><mrow id="algorithm2.26.26.m1.1.2.3" xref="algorithm2.26.26.m1.1.2.3.cmml"><msubsup id="algorithm2.26.26.m1.1.2.3.1" xref="algorithm2.26.26.m1.1.2.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="algorithm2.26.26.m1.1.2.3.1.2.2" xref="algorithm2.26.26.m1.1.2.3.1.2.2.cmml">∑</mo><mrow id="algorithm2.26.26.m1.1.2.3.1.2.3" xref="algorithm2.26.26.m1.1.2.3.1.2.3.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.1.2.3.2" xref="algorithm2.26.26.m1.1.2.3.1.2.3.2.cmml">k</mi><mo mathsize="90%" id="algorithm2.26.26.m1.1.2.3.1.2.3.1" xref="algorithm2.26.26.m1.1.2.3.1.2.3.1.cmml">=</mo><mn mathsize="90%" id="algorithm2.26.26.m1.1.2.3.1.2.3.3" xref="algorithm2.26.26.m1.1.2.3.1.2.3.3.cmml">1</mn></mrow><mfrac id="algorithm2.26.26.m1.1.2.3.1.3" xref="algorithm2.26.26.m1.1.2.3.1.3.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.1.3.2" xref="algorithm2.26.26.m1.1.2.3.1.3.2.cmml">N</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.1.3.3" xref="algorithm2.26.26.m1.1.2.3.1.3.3.cmml">L</mi></mfrac></msubsup><mrow id="algorithm2.26.26.m1.1.2.3.2" xref="algorithm2.26.26.m1.1.2.3.2.cmml"><mfrac id="algorithm2.26.26.m1.1.1" xref="algorithm2.26.26.m1.1.1.cmml"><msub id="algorithm2.26.26.m1.1.1.3" xref="algorithm2.26.26.m1.1.1.3.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.3.2" xref="algorithm2.26.26.m1.1.1.3.2.cmml">s</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.3.3" xref="algorithm2.26.26.m1.1.1.3.3.cmml">k</mi></msub><mrow id="algorithm2.26.26.m1.1.1.1.1" xref="algorithm2.26.26.m1.1.1.1.2.cmml"><mo id="algorithm2.26.26.m1.1.1.1.1.2" xref="algorithm2.26.26.m1.1.1.1.2.1.cmml">|</mo><mrow id="algorithm2.26.26.m1.1.1.1.1.1" xref="algorithm2.26.26.m1.1.1.1.1.1.cmml"><mstyle displaystyle="false" id="algorithm2.26.26.m1.1.1.1.1.1.1" xref="algorithm2.26.26.m1.1.1.1.1.1.1.cmml"><msub id="algorithm2.26.26.m1.1.1.1.1.1.1a" xref="algorithm2.26.26.m1.1.1.1.1.1.1.cmml"><mo maxsize="63%" minsize="63%" stretchy="true" id="algorithm2.26.26.m1.1.1.1.1.1.1.2" xref="algorithm2.26.26.m1.1.1.1.1.1.1.2.cmml">⋃</mo><mrow id="algorithm2.26.26.m1.1.1.1.1.1.1.3" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.cmml"><msub id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.2" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.2.cmml">c</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.3" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.1" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.1.cmml">∈</mo><msub id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.2" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.2.cmml">C</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.3" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.3.cmml">t</mi></msub></mrow></msub></mstyle><msub id="algorithm2.26.26.m1.1.1.1.1.1.2" xref="algorithm2.26.26.m1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.2.2" xref="algorithm2.26.26.m1.1.1.1.1.1.2.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.1.1.1.1.2.3" xref="algorithm2.26.26.m1.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="algorithm2.26.26.m1.1.1.1.1.3" xref="algorithm2.26.26.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="algorithm2.26.26.m1.1.2.3.2.1" xref="algorithm2.26.26.m1.1.2.3.2.1.cmml">​</mo><msubsup id="algorithm2.26.26.m1.1.2.3.2.2" xref="algorithm2.26.26.m1.1.2.3.2.2.cmml"><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.2.2.2.2" xref="algorithm2.26.26.m1.1.2.3.2.2.2.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.2.2.3" xref="algorithm2.26.26.m1.1.2.3.2.2.3.cmml">t</mi><mi mathsize="90%" id="algorithm2.26.26.m1.1.2.3.2.2.2.3" xref="algorithm2.26.26.m1.1.2.3.2.2.2.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.26.26.m1.1b"><apply id="algorithm2.26.26.m1.1.2.cmml" xref="algorithm2.26.26.m1.1.2"><ci id="algorithm2.26.26.m1.1.2.1.cmml" xref="algorithm2.26.26.m1.1.2.1">←</ci><apply id="algorithm2.26.26.m1.1.2.2.cmml" xref="algorithm2.26.26.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.2.2.1.cmml" xref="algorithm2.26.26.m1.1.2.2">subscript</csymbol><ci id="algorithm2.26.26.m1.1.2.2.2.cmml" xref="algorithm2.26.26.m1.1.2.2.2">𝜽</ci><ci id="algorithm2.26.26.m1.1.2.2.3.cmml" xref="algorithm2.26.26.m1.1.2.2.3">𝑡</ci></apply><apply id="algorithm2.26.26.m1.1.2.3.cmml" xref="algorithm2.26.26.m1.1.2.3"><apply id="algorithm2.26.26.m1.1.2.3.1.cmml" xref="algorithm2.26.26.m1.1.2.3.1"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.2.3.1.1.cmml" xref="algorithm2.26.26.m1.1.2.3.1">superscript</csymbol><apply id="algorithm2.26.26.m1.1.2.3.1.2.cmml" xref="algorithm2.26.26.m1.1.2.3.1"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.2.3.1.2.1.cmml" xref="algorithm2.26.26.m1.1.2.3.1">subscript</csymbol><sum id="algorithm2.26.26.m1.1.2.3.1.2.2.cmml" xref="algorithm2.26.26.m1.1.2.3.1.2.2"></sum><apply id="algorithm2.26.26.m1.1.2.3.1.2.3.cmml" xref="algorithm2.26.26.m1.1.2.3.1.2.3"><eq id="algorithm2.26.26.m1.1.2.3.1.2.3.1.cmml" xref="algorithm2.26.26.m1.1.2.3.1.2.3.1"></eq><ci id="algorithm2.26.26.m1.1.2.3.1.2.3.2.cmml" xref="algorithm2.26.26.m1.1.2.3.1.2.3.2">𝑘</ci><cn type="integer" id="algorithm2.26.26.m1.1.2.3.1.2.3.3.cmml" xref="algorithm2.26.26.m1.1.2.3.1.2.3.3">1</cn></apply></apply><apply id="algorithm2.26.26.m1.1.2.3.1.3.cmml" xref="algorithm2.26.26.m1.1.2.3.1.3"><divide id="algorithm2.26.26.m1.1.2.3.1.3.1.cmml" xref="algorithm2.26.26.m1.1.2.3.1.3"></divide><ci id="algorithm2.26.26.m1.1.2.3.1.3.2.cmml" xref="algorithm2.26.26.m1.1.2.3.1.3.2">𝑁</ci><ci id="algorithm2.26.26.m1.1.2.3.1.3.3.cmml" xref="algorithm2.26.26.m1.1.2.3.1.3.3">𝐿</ci></apply></apply><apply id="algorithm2.26.26.m1.1.2.3.2.cmml" xref="algorithm2.26.26.m1.1.2.3.2"><times id="algorithm2.26.26.m1.1.2.3.2.1.cmml" xref="algorithm2.26.26.m1.1.2.3.2.1"></times><apply id="algorithm2.26.26.m1.1.1.cmml" xref="algorithm2.26.26.m1.1.1"><divide id="algorithm2.26.26.m1.1.1.2.cmml" xref="algorithm2.26.26.m1.1.1"></divide><apply id="algorithm2.26.26.m1.1.1.3.cmml" xref="algorithm2.26.26.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.1.3.1.cmml" xref="algorithm2.26.26.m1.1.1.3">subscript</csymbol><ci id="algorithm2.26.26.m1.1.1.3.2.cmml" xref="algorithm2.26.26.m1.1.1.3.2">𝑠</ci><ci id="algorithm2.26.26.m1.1.1.3.3.cmml" xref="algorithm2.26.26.m1.1.1.3.3">𝑘</ci></apply><apply id="algorithm2.26.26.m1.1.1.1.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1"><abs id="algorithm2.26.26.m1.1.1.1.2.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.2"></abs><apply id="algorithm2.26.26.m1.1.1.1.1.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1"><apply id="algorithm2.26.26.m1.1.1.1.1.1.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.1.1.1.1.1.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1">subscript</csymbol><union id="algorithm2.26.26.m1.1.1.1.1.1.1.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.2"></union><apply id="algorithm2.26.26.m1.1.1.1.1.1.1.3.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3"><in id="algorithm2.26.26.m1.1.1.1.1.1.1.3.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.1"></in><apply id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.2">𝑐</ci><ci id="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.3.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.2">𝐶</ci><ci id="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.3.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="algorithm2.26.26.m1.1.1.1.1.1.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.1.1.1.1.2.1.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.2">subscript</csymbol><ci id="algorithm2.26.26.m1.1.1.1.1.1.2.2.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.2.2">𝒟</ci><ci id="algorithm2.26.26.m1.1.1.1.1.1.2.3.cmml" xref="algorithm2.26.26.m1.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply></apply><apply id="algorithm2.26.26.m1.1.2.3.2.2.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.2.3.2.2.1.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2">subscript</csymbol><apply id="algorithm2.26.26.m1.1.2.3.2.2.2.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="algorithm2.26.26.m1.1.2.3.2.2.2.1.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2">superscript</csymbol><ci id="algorithm2.26.26.m1.1.2.3.2.2.2.2.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2.2.2">𝜽</ci><ci id="algorithm2.26.26.m1.1.2.3.2.2.2.3.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2.2.3">𝑘</ci></apply><ci id="algorithm2.26.26.m1.1.2.3.2.2.3.cmml" xref="algorithm2.26.26.m1.1.2.3.2.2.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.26.26.m1.1c">\boldsymbol{\theta}_{t}\leftarrow\sum_{k=1}^{\frac{N}{L}}\frac{s_{k}}{\left|\bigcup_{c_{i}\in C_{t}}\mathcal{D}_{i}\right|}\boldsymbol{\theta}^{k}_{t}</annotation></semantics></math><span id="algorithm2.26.26.4" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.36.48" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.48.1.1.1" class="ltx_text" style="font-size:90%;">20</span></span><span id="algorithm2.36.48.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.48.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.48.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.27.27" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.27.27.1.1.1" class="ltx_text" style="font-size:90%;">21</span></span><span id="algorithm2.27.27.2" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="algorithm2.27.27.3" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm2.27.27.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}_{T}" display="inline"><semantics id="algorithm2.27.27.m1.1a"><msub id="algorithm2.27.27.m1.1.1" xref="algorithm2.27.27.m1.1.1.cmml"><mi mathsize="90%" id="algorithm2.27.27.m1.1.1.2" xref="algorithm2.27.27.m1.1.1.2.cmml">𝜽</mi><mi mathsize="90%" id="algorithm2.27.27.m1.1.1.3" xref="algorithm2.27.27.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm2.27.27.m1.1b"><apply id="algorithm2.27.27.m1.1.1.cmml" xref="algorithm2.27.27.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.27.27.m1.1.1.1.cmml" xref="algorithm2.27.27.m1.1.1">subscript</csymbol><ci id="algorithm2.27.27.m1.1.1.2.cmml" xref="algorithm2.27.27.m1.1.1.2">𝜽</ci><ci id="algorithm2.27.27.m1.1.1.3.cmml" xref="algorithm2.27.27.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.27.27.m1.1c">\boldsymbol{\theta}_{T}</annotation></semantics></math><span id="algorithm2.27.27.4" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.36.49" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.49.1.1.1" class="ltx_text" style="font-size:90%;">22</span></span><span id="algorithm2.36.49.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.50" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.50.1.1.1" class="ltx_text" style="font-size:90%;">23</span></span><span id="algorithm2.36.50.2" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.28.28" class="ltx_listingline">
<span id="algorithm2.28.28.2" class="ltx_text" style="font-size:90%;"><span id="algorithm2.28.28.2.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;color:#0000FF;">/* </span></span><span id="algorithm2.28.28.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0000FF;">Runs on client <math id="algorithm2.28.28.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="algorithm2.28.28.1.m1.1a"><mi mathcolor="#0000FF" id="algorithm2.28.28.1.m1.1.1" xref="algorithm2.28.28.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="algorithm2.28.28.1.m1.1b"><ci id="algorithm2.28.28.1.m1.1.1.cmml" xref="algorithm2.28.28.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.28.28.1.m1.1c">i</annotation></semantics></math> */</span>
</div>
<div id="algorithm2.29.29" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.29.29.2.1.1" class="ltx_text" style="font-size:90%;">24</span></span><span id="algorithm2.29.29.3" class="ltx_text" style="font-size:90%;">
</span><em id="algorithm2.29.29.1" class="ltx_emph ltx_font_typewriter" style="font-size:90%;">UpdateClient(<em id="algorithm2.29.29.1.1.1" class="ltx_emph ltx_font_serif ltx_font_italic"><math id="algorithm2.29.29.1.1.1.m1.2" class="ltx_Math" alttext="\boldsymbol{\theta},i" display="inline"><semantics id="algorithm2.29.29.1.1.1.m1.2a"><mrow id="algorithm2.29.29.1.1.1.m1.2.3.2" xref="algorithm2.29.29.1.1.1.m1.2.3.1.cmml"><mi id="algorithm2.29.29.1.1.1.m1.1.1" xref="algorithm2.29.29.1.1.1.m1.1.1.cmml">𝛉</mi><mo id="algorithm2.29.29.1.1.1.m1.2.3.2.1" xref="algorithm2.29.29.1.1.1.m1.2.3.1.cmml">,</mo><mi id="algorithm2.29.29.1.1.1.m1.2.2" xref="algorithm2.29.29.1.1.1.m1.2.2.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.29.29.1.1.1.m1.2b"><list id="algorithm2.29.29.1.1.1.m1.2.3.1.cmml" xref="algorithm2.29.29.1.1.1.m1.2.3.2"><ci id="algorithm2.29.29.1.1.1.m1.1.1.cmml" xref="algorithm2.29.29.1.1.1.m1.1.1">𝛉</ci><ci id="algorithm2.29.29.1.1.1.m1.2.2.cmml" xref="algorithm2.29.29.1.1.1.m1.2.2">𝑖</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.29.29.1.1.1.m1.2c">\boldsymbol{\theta},i</annotation></semantics></math></em>)</em><span id="algorithm2.29.29.4" class="ltx_text ltx_font_bold" style="font-size:90%;">:</span>
</div>
<div id="algorithm2.30.30" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.30.30.1.1.1" class="ltx_text" style="font-size:90%;">25</span></span><span id="algorithm2.30.30.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.30.30.3" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.30.30.4" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.30.30.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm2.30.30.6" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.30.30.7" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm2.30.30.8" class="ltx_text" style="font-size:90%;"> local epoch </span><math id="algorithm2.30.30.m1.3" class="ltx_Math" alttext="e=1,\dots,E" display="inline"><semantics id="algorithm2.30.30.m1.3a"><mrow id="algorithm2.30.30.m1.3.4" xref="algorithm2.30.30.m1.3.4.cmml"><mi mathsize="90%" id="algorithm2.30.30.m1.3.4.2" xref="algorithm2.30.30.m1.3.4.2.cmml">e</mi><mo mathsize="90%" id="algorithm2.30.30.m1.3.4.1" xref="algorithm2.30.30.m1.3.4.1.cmml">=</mo><mrow id="algorithm2.30.30.m1.3.4.3.2" xref="algorithm2.30.30.m1.3.4.3.1.cmml"><mn mathsize="90%" id="algorithm2.30.30.m1.1.1" xref="algorithm2.30.30.m1.1.1.cmml">1</mn><mo mathsize="90%" id="algorithm2.30.30.m1.3.4.3.2.1" xref="algorithm2.30.30.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm2.30.30.m1.2.2" xref="algorithm2.30.30.m1.2.2.cmml">…</mi><mo mathsize="90%" id="algorithm2.30.30.m1.3.4.3.2.2" xref="algorithm2.30.30.m1.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="algorithm2.30.30.m1.3.3" xref="algorithm2.30.30.m1.3.3.cmml">E</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.30.30.m1.3b"><apply id="algorithm2.30.30.m1.3.4.cmml" xref="algorithm2.30.30.m1.3.4"><eq id="algorithm2.30.30.m1.3.4.1.cmml" xref="algorithm2.30.30.m1.3.4.1"></eq><ci id="algorithm2.30.30.m1.3.4.2.cmml" xref="algorithm2.30.30.m1.3.4.2">𝑒</ci><list id="algorithm2.30.30.m1.3.4.3.1.cmml" xref="algorithm2.30.30.m1.3.4.3.2"><cn type="integer" id="algorithm2.30.30.m1.1.1.cmml" xref="algorithm2.30.30.m1.1.1">1</cn><ci id="algorithm2.30.30.m1.2.2.cmml" xref="algorithm2.30.30.m1.2.2">…</ci><ci id="algorithm2.30.30.m1.3.3.cmml" xref="algorithm2.30.30.m1.3.3">𝐸</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.30.30.m1.3c">e=1,\dots,E</annotation></semantics></math><span id="algorithm2.30.30.9" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.30.30.10" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm2.33.33" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.33.33.1.1.1" class="ltx_text" style="font-size:90%;">26</span></span><span id="algorithm2.33.33.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.33.33.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.33.33.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.33.33.5" class="ltx_text" style="font-size:90%;">
Split </span><math id="algorithm2.31.31.m1.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="algorithm2.31.31.m1.1a"><msub id="algorithm2.31.31.m1.1.1" xref="algorithm2.31.31.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.31.31.m1.1.1.2" xref="algorithm2.31.31.m1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.31.31.m1.1.1.3" xref="algorithm2.31.31.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm2.31.31.m1.1b"><apply id="algorithm2.31.31.m1.1.1.cmml" xref="algorithm2.31.31.m1.1.1"><csymbol cd="ambiguous" id="algorithm2.31.31.m1.1.1.1.cmml" xref="algorithm2.31.31.m1.1.1">subscript</csymbol><ci id="algorithm2.31.31.m1.1.1.2.cmml" xref="algorithm2.31.31.m1.1.1.2">𝒟</ci><ci id="algorithm2.31.31.m1.1.1.3.cmml" xref="algorithm2.31.31.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.31.31.m1.1c">\mathcal{D}_{i}</annotation></semantics></math><span id="algorithm2.33.33.6" class="ltx_text" style="font-size:90%;"> into </span><math id="algorithm2.32.32.m2.1" class="ltx_Math" alttext="\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil" display="inline"><semantics id="algorithm2.32.32.m2.1a"><mrow id="algorithm2.32.32.m2.1.2.2" xref="algorithm2.32.32.m2.1.2.1.cmml"><mo id="algorithm2.32.32.m2.1.2.2.1" xref="algorithm2.32.32.m2.1.2.1.1.cmml">⌈</mo><mfrac id="algorithm2.32.32.m2.1.1" xref="algorithm2.32.32.m2.1.1.cmml"><mrow id="algorithm2.32.32.m2.1.1.1.1" xref="algorithm2.32.32.m2.1.1.1.2.cmml"><mo id="algorithm2.32.32.m2.1.1.1.1.2" xref="algorithm2.32.32.m2.1.1.1.2.1.cmml">|</mo><msub id="algorithm2.32.32.m2.1.1.1.1.1" xref="algorithm2.32.32.m2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.32.32.m2.1.1.1.1.1.2" xref="algorithm2.32.32.m2.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.32.32.m2.1.1.1.1.1.3" xref="algorithm2.32.32.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm2.32.32.m2.1.1.1.1.3" xref="algorithm2.32.32.m2.1.1.1.2.1.cmml">|</mo></mrow><mi mathsize="90%" id="algorithm2.32.32.m2.1.1.3" xref="algorithm2.32.32.m2.1.1.3.cmml">B</mi></mfrac><mo id="algorithm2.32.32.m2.1.2.2.2" xref="algorithm2.32.32.m2.1.2.1.1.cmml">⌉</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.32.32.m2.1b"><apply id="algorithm2.32.32.m2.1.2.1.cmml" xref="algorithm2.32.32.m2.1.2.2"><ceiling id="algorithm2.32.32.m2.1.2.1.1.cmml" xref="algorithm2.32.32.m2.1.2.2.1"></ceiling><apply id="algorithm2.32.32.m2.1.1.cmml" xref="algorithm2.32.32.m2.1.1"><divide id="algorithm2.32.32.m2.1.1.2.cmml" xref="algorithm2.32.32.m2.1.1"></divide><apply id="algorithm2.32.32.m2.1.1.1.2.cmml" xref="algorithm2.32.32.m2.1.1.1.1"><abs id="algorithm2.32.32.m2.1.1.1.2.1.cmml" xref="algorithm2.32.32.m2.1.1.1.1.2"></abs><apply id="algorithm2.32.32.m2.1.1.1.1.1.cmml" xref="algorithm2.32.32.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm2.32.32.m2.1.1.1.1.1.1.cmml" xref="algorithm2.32.32.m2.1.1.1.1.1">subscript</csymbol><ci id="algorithm2.32.32.m2.1.1.1.1.1.2.cmml" xref="algorithm2.32.32.m2.1.1.1.1.1.2">𝒟</ci><ci id="algorithm2.32.32.m2.1.1.1.1.1.3.cmml" xref="algorithm2.32.32.m2.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="algorithm2.32.32.m2.1.1.3.cmml" xref="algorithm2.32.32.m2.1.1.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.32.32.m2.1c">\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil</annotation></semantics></math><span id="algorithm2.33.33.7" class="ltx_text" style="font-size:90%;"> batches of size </span><math id="algorithm2.33.33.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="algorithm2.33.33.m3.1a"><mi mathsize="90%" id="algorithm2.33.33.m3.1.1" xref="algorithm2.33.33.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="algorithm2.33.33.m3.1b"><ci id="algorithm2.33.33.m3.1.1.cmml" xref="algorithm2.33.33.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.33.33.m3.1c">B</annotation></semantics></math><span id="algorithm2.33.33.8" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.34.34" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.34.34.1.1.1" class="ltx_text" style="font-size:90%;">27</span></span><span id="algorithm2.34.34.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.34.34.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.34.34.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.34.34.5" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm2.34.34.6" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="algorithm2.34.34.7" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.34.34.8" class="ltx_text ltx_font_bold" style="font-size:90%;">each</span><span id="algorithm2.34.34.9" class="ltx_text" style="font-size:90%;"> batch </span><math id="algorithm2.34.34.m1.4" class="ltx_Math" alttext="b=1,\dots,\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil" display="inline"><semantics id="algorithm2.34.34.m1.4a"><mrow id="algorithm2.34.34.m1.4.4" xref="algorithm2.34.34.m1.4.4.cmml"><mi mathsize="90%" id="algorithm2.34.34.m1.4.4.3" xref="algorithm2.34.34.m1.4.4.3.cmml">b</mi><mo mathsize="90%" id="algorithm2.34.34.m1.4.4.2" xref="algorithm2.34.34.m1.4.4.2.cmml">=</mo><mrow id="algorithm2.34.34.m1.4.4.1.1" xref="algorithm2.34.34.m1.4.4.1.2.cmml"><mn mathsize="90%" id="algorithm2.34.34.m1.2.2" xref="algorithm2.34.34.m1.2.2.cmml">1</mn><mo mathsize="90%" id="algorithm2.34.34.m1.4.4.1.1.2" xref="algorithm2.34.34.m1.4.4.1.2.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="algorithm2.34.34.m1.3.3" xref="algorithm2.34.34.m1.3.3.cmml">…</mi><mo mathsize="90%" id="algorithm2.34.34.m1.4.4.1.1.3" xref="algorithm2.34.34.m1.4.4.1.2.cmml">,</mo><mrow id="algorithm2.34.34.m1.4.4.1.1.1.2" xref="algorithm2.34.34.m1.4.4.1.1.1.1.cmml"><mo id="algorithm2.34.34.m1.4.4.1.1.1.2.1" xref="algorithm2.34.34.m1.4.4.1.1.1.1.1.cmml">⌈</mo><mfrac id="algorithm2.34.34.m1.1.1" xref="algorithm2.34.34.m1.1.1.cmml"><mrow id="algorithm2.34.34.m1.1.1.1.1" xref="algorithm2.34.34.m1.1.1.1.2.cmml"><mo id="algorithm2.34.34.m1.1.1.1.1.2" xref="algorithm2.34.34.m1.1.1.1.2.1.cmml">|</mo><msub id="algorithm2.34.34.m1.1.1.1.1.1" xref="algorithm2.34.34.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.34.34.m1.1.1.1.1.1.2" xref="algorithm2.34.34.m1.1.1.1.1.1.2.cmml">𝒟</mi><mi mathsize="90%" id="algorithm2.34.34.m1.1.1.1.1.1.3" xref="algorithm2.34.34.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="algorithm2.34.34.m1.1.1.1.1.3" xref="algorithm2.34.34.m1.1.1.1.2.1.cmml">|</mo></mrow><mi mathsize="90%" id="algorithm2.34.34.m1.1.1.3" xref="algorithm2.34.34.m1.1.1.3.cmml">B</mi></mfrac><mo id="algorithm2.34.34.m1.4.4.1.1.1.2.2" xref="algorithm2.34.34.m1.4.4.1.1.1.1.1.cmml">⌉</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.34.34.m1.4b"><apply id="algorithm2.34.34.m1.4.4.cmml" xref="algorithm2.34.34.m1.4.4"><eq id="algorithm2.34.34.m1.4.4.2.cmml" xref="algorithm2.34.34.m1.4.4.2"></eq><ci id="algorithm2.34.34.m1.4.4.3.cmml" xref="algorithm2.34.34.m1.4.4.3">𝑏</ci><list id="algorithm2.34.34.m1.4.4.1.2.cmml" xref="algorithm2.34.34.m1.4.4.1.1"><cn type="integer" id="algorithm2.34.34.m1.2.2.cmml" xref="algorithm2.34.34.m1.2.2">1</cn><ci id="algorithm2.34.34.m1.3.3.cmml" xref="algorithm2.34.34.m1.3.3">…</ci><apply id="algorithm2.34.34.m1.4.4.1.1.1.1.cmml" xref="algorithm2.34.34.m1.4.4.1.1.1.2"><ceiling id="algorithm2.34.34.m1.4.4.1.1.1.1.1.cmml" xref="algorithm2.34.34.m1.4.4.1.1.1.2.1"></ceiling><apply id="algorithm2.34.34.m1.1.1.cmml" xref="algorithm2.34.34.m1.1.1"><divide id="algorithm2.34.34.m1.1.1.2.cmml" xref="algorithm2.34.34.m1.1.1"></divide><apply id="algorithm2.34.34.m1.1.1.1.2.cmml" xref="algorithm2.34.34.m1.1.1.1.1"><abs id="algorithm2.34.34.m1.1.1.1.2.1.cmml" xref="algorithm2.34.34.m1.1.1.1.1.2"></abs><apply id="algorithm2.34.34.m1.1.1.1.1.1.cmml" xref="algorithm2.34.34.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm2.34.34.m1.1.1.1.1.1.1.cmml" xref="algorithm2.34.34.m1.1.1.1.1.1">subscript</csymbol><ci id="algorithm2.34.34.m1.1.1.1.1.1.2.cmml" xref="algorithm2.34.34.m1.1.1.1.1.1.2">𝒟</ci><ci id="algorithm2.34.34.m1.1.1.1.1.1.3.cmml" xref="algorithm2.34.34.m1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="algorithm2.34.34.m1.1.1.3.cmml" xref="algorithm2.34.34.m1.1.1.3">𝐵</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.34.34.m1.4c">b=1,\dots,\left\lceil\frac{\left|\mathcal{D}_{i}\right|}{B}\right\rceil</annotation></semantics></math><span id="algorithm2.34.34.10" class="ltx_text" style="font-size:90%;"> </span><span id="algorithm2.34.34.11" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div id="algorithm2.35.35" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.35.35.1.1.1" class="ltx_text" style="font-size:90%;">28</span></span><span id="algorithm2.35.35.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.35.35.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.35.35.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.35.35.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.35.35.6" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm2.35.35.m1.4" class="ltx_Math" alttext="\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\nabla\mathcal{L}_{i}(\mathcal{D}_{i,b};\boldsymbol{\theta})" display="inline"><semantics id="algorithm2.35.35.m1.4a"><mrow id="algorithm2.35.35.m1.4.4" xref="algorithm2.35.35.m1.4.4.cmml"><mi mathsize="90%" id="algorithm2.35.35.m1.4.4.3" xref="algorithm2.35.35.m1.4.4.3.cmml">𝜽</mi><mo mathsize="90%" stretchy="false" id="algorithm2.35.35.m1.4.4.2" xref="algorithm2.35.35.m1.4.4.2.cmml">←</mo><mrow id="algorithm2.35.35.m1.4.4.1" xref="algorithm2.35.35.m1.4.4.1.cmml"><mi mathsize="90%" id="algorithm2.35.35.m1.4.4.1.3" xref="algorithm2.35.35.m1.4.4.1.3.cmml">𝜽</mi><mo mathsize="90%" id="algorithm2.35.35.m1.4.4.1.2" xref="algorithm2.35.35.m1.4.4.1.2.cmml">−</mo><mrow id="algorithm2.35.35.m1.4.4.1.1" xref="algorithm2.35.35.m1.4.4.1.1.cmml"><mi mathsize="90%" id="algorithm2.35.35.m1.4.4.1.1.3" xref="algorithm2.35.35.m1.4.4.1.1.3.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="algorithm2.35.35.m1.4.4.1.1.2" xref="algorithm2.35.35.m1.4.4.1.1.2.cmml">​</mo><mrow id="algorithm2.35.35.m1.4.4.1.1.4" xref="algorithm2.35.35.m1.4.4.1.1.4.cmml"><mo mathsize="90%" rspace="0.167em" id="algorithm2.35.35.m1.4.4.1.1.4.1" xref="algorithm2.35.35.m1.4.4.1.1.4.1.cmml">∇</mo><msub id="algorithm2.35.35.m1.4.4.1.1.4.2" xref="algorithm2.35.35.m1.4.4.1.1.4.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.35.35.m1.4.4.1.1.4.2.2" xref="algorithm2.35.35.m1.4.4.1.1.4.2.2.cmml">ℒ</mi><mi mathsize="90%" id="algorithm2.35.35.m1.4.4.1.1.4.2.3" xref="algorithm2.35.35.m1.4.4.1.1.4.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="algorithm2.35.35.m1.4.4.1.1.2a" xref="algorithm2.35.35.m1.4.4.1.1.2.cmml">​</mo><mrow id="algorithm2.35.35.m1.4.4.1.1.1.1" xref="algorithm2.35.35.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="algorithm2.35.35.m1.4.4.1.1.1.1.2" xref="algorithm2.35.35.m1.4.4.1.1.1.2.cmml">(</mo><msub id="algorithm2.35.35.m1.4.4.1.1.1.1.1" xref="algorithm2.35.35.m1.4.4.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="algorithm2.35.35.m1.4.4.1.1.1.1.1.2" xref="algorithm2.35.35.m1.4.4.1.1.1.1.1.2.cmml">𝒟</mi><mrow id="algorithm2.35.35.m1.2.2.2.4" xref="algorithm2.35.35.m1.2.2.2.3.cmml"><mi mathsize="90%" id="algorithm2.35.35.m1.1.1.1.1" xref="algorithm2.35.35.m1.1.1.1.1.cmml">i</mi><mo mathsize="90%" id="algorithm2.35.35.m1.2.2.2.4.1" xref="algorithm2.35.35.m1.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="algorithm2.35.35.m1.2.2.2.2" xref="algorithm2.35.35.m1.2.2.2.2.cmml">b</mi></mrow></msub><mo mathsize="90%" id="algorithm2.35.35.m1.4.4.1.1.1.1.3" xref="algorithm2.35.35.m1.4.4.1.1.1.2.cmml">;</mo><mi mathsize="90%" id="algorithm2.35.35.m1.3.3" xref="algorithm2.35.35.m1.3.3.cmml">𝜽</mi><mo maxsize="90%" minsize="90%" id="algorithm2.35.35.m1.4.4.1.1.1.1.4" xref="algorithm2.35.35.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm2.35.35.m1.4b"><apply id="algorithm2.35.35.m1.4.4.cmml" xref="algorithm2.35.35.m1.4.4"><ci id="algorithm2.35.35.m1.4.4.2.cmml" xref="algorithm2.35.35.m1.4.4.2">←</ci><ci id="algorithm2.35.35.m1.4.4.3.cmml" xref="algorithm2.35.35.m1.4.4.3">𝜽</ci><apply id="algorithm2.35.35.m1.4.4.1.cmml" xref="algorithm2.35.35.m1.4.4.1"><minus id="algorithm2.35.35.m1.4.4.1.2.cmml" xref="algorithm2.35.35.m1.4.4.1.2"></minus><ci id="algorithm2.35.35.m1.4.4.1.3.cmml" xref="algorithm2.35.35.m1.4.4.1.3">𝜽</ci><apply id="algorithm2.35.35.m1.4.4.1.1.cmml" xref="algorithm2.35.35.m1.4.4.1.1"><times id="algorithm2.35.35.m1.4.4.1.1.2.cmml" xref="algorithm2.35.35.m1.4.4.1.1.2"></times><ci id="algorithm2.35.35.m1.4.4.1.1.3.cmml" xref="algorithm2.35.35.m1.4.4.1.1.3">𝜂</ci><apply id="algorithm2.35.35.m1.4.4.1.1.4.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4"><ci id="algorithm2.35.35.m1.4.4.1.1.4.1.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4.1">∇</ci><apply id="algorithm2.35.35.m1.4.4.1.1.4.2.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4.2"><csymbol cd="ambiguous" id="algorithm2.35.35.m1.4.4.1.1.4.2.1.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4.2">subscript</csymbol><ci id="algorithm2.35.35.m1.4.4.1.1.4.2.2.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4.2.2">ℒ</ci><ci id="algorithm2.35.35.m1.4.4.1.1.4.2.3.cmml" xref="algorithm2.35.35.m1.4.4.1.1.4.2.3">𝑖</ci></apply></apply><list id="algorithm2.35.35.m1.4.4.1.1.1.2.cmml" xref="algorithm2.35.35.m1.4.4.1.1.1.1"><apply id="algorithm2.35.35.m1.4.4.1.1.1.1.1.cmml" xref="algorithm2.35.35.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm2.35.35.m1.4.4.1.1.1.1.1.1.cmml" xref="algorithm2.35.35.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="algorithm2.35.35.m1.4.4.1.1.1.1.1.2.cmml" xref="algorithm2.35.35.m1.4.4.1.1.1.1.1.2">𝒟</ci><list id="algorithm2.35.35.m1.2.2.2.3.cmml" xref="algorithm2.35.35.m1.2.2.2.4"><ci id="algorithm2.35.35.m1.1.1.1.1.cmml" xref="algorithm2.35.35.m1.1.1.1.1">𝑖</ci><ci id="algorithm2.35.35.m1.2.2.2.2.cmml" xref="algorithm2.35.35.m1.2.2.2.2">𝑏</ci></list></apply><ci id="algorithm2.35.35.m1.3.3.cmml" xref="algorithm2.35.35.m1.3.3">𝜽</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.35.35.m1.4c">\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\nabla\mathcal{L}_{i}(\mathcal{D}_{i,b};\boldsymbol{\theta})</annotation></semantics></math><span id="algorithm2.35.35.7" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.36.51" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.51.1.1.1" class="ltx_text" style="font-size:90%;">29</span></span><span id="algorithm2.36.51.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.51.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.51.4" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.51.5" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.51.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.36" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.36.1.1.1" class="ltx_text" style="font-size:90%;">30</span></span><span id="algorithm2.36.36.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.36.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.36.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.36.5" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="algorithm2.36.36.6" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm2.36.36.m1.1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><semantics id="algorithm2.36.36.m1.1a"><mi mathsize="90%" id="algorithm2.36.36.m1.1.1" xref="algorithm2.36.36.m1.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="algorithm2.36.36.m1.1b"><ci id="algorithm2.36.36.m1.1.1.cmml" xref="algorithm2.36.36.m1.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm2.36.36.m1.1c">\boldsymbol{\theta}</annotation></semantics></math><span id="algorithm2.36.36.7" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm2.36.52" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.52.1.1.1" class="ltx_text" style="font-size:90%;">31</span></span><span id="algorithm2.36.52.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.52.3" class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.52.4" class="ltx_text" style="font-size:90%;">   </span><span id="algorithm2.36.52.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="algorithm2.36.53" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm2.36.53.1.1.1" class="ltx_text" style="font-size:90%;">32</span></span><span id="algorithm2.36.53.2" class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm2.36.53.3" class="ltx_text" style="font-size:90%;">   </span>
</div>
<div id="algorithm2.36.54" class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="algorithm2.39.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span><a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a></figcaption>
</figure>
<div id="S3.SS6.p7" class="ltx_para">
<p id="S3.SS6.p7.3" class="ltx_p">For the complexity analysis, we compare <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> to its baseline, <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, with respect to the expected time the central server needs to wait before it can aggregate the updated model parameters of the clients in each communication round. The number of local update steps on the <math id="S3.SS6.p7.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS6.p7.1.m1.1a"><mi id="S3.SS6.p7.1.m1.1.1" xref="S3.SS6.p7.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p7.1.m1.1b"><ci id="S3.SS6.p7.1.m1.1.1.cmml" xref="S3.SS6.p7.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p7.1.m1.1c">i</annotation></semantics></math>th client are given by <math id="S3.SS6.p7.2.m2.1" class="ltx_Math" alttext="E\cdot\frac{\left|\mathcal{D}_{i}\right|}{B}" display="inline"><semantics id="S3.SS6.p7.2.m2.1a"><mrow id="S3.SS6.p7.2.m2.1.2" xref="S3.SS6.p7.2.m2.1.2.cmml"><mi id="S3.SS6.p7.2.m2.1.2.2" xref="S3.SS6.p7.2.m2.1.2.2.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p7.2.m2.1.2.1" xref="S3.SS6.p7.2.m2.1.2.1.cmml">⋅</mo><mfrac id="S3.SS6.p7.2.m2.1.1" xref="S3.SS6.p7.2.m2.1.1.cmml"><mrow id="S3.SS6.p7.2.m2.1.1.1.1" xref="S3.SS6.p7.2.m2.1.1.1.2.cmml"><mo id="S3.SS6.p7.2.m2.1.1.1.1.2" xref="S3.SS6.p7.2.m2.1.1.1.2.1.cmml">|</mo><msub id="S3.SS6.p7.2.m2.1.1.1.1.1" xref="S3.SS6.p7.2.m2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p7.2.m2.1.1.1.1.1.2" xref="S3.SS6.p7.2.m2.1.1.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS6.p7.2.m2.1.1.1.1.1.3" xref="S3.SS6.p7.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS6.p7.2.m2.1.1.1.1.3" xref="S3.SS6.p7.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mi id="S3.SS6.p7.2.m2.1.1.3" xref="S3.SS6.p7.2.m2.1.1.3.cmml">B</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p7.2.m2.1b"><apply id="S3.SS6.p7.2.m2.1.2.cmml" xref="S3.SS6.p7.2.m2.1.2"><ci id="S3.SS6.p7.2.m2.1.2.1.cmml" xref="S3.SS6.p7.2.m2.1.2.1">⋅</ci><ci id="S3.SS6.p7.2.m2.1.2.2.cmml" xref="S3.SS6.p7.2.m2.1.2.2">𝐸</ci><apply id="S3.SS6.p7.2.m2.1.1.cmml" xref="S3.SS6.p7.2.m2.1.1"><divide id="S3.SS6.p7.2.m2.1.1.2.cmml" xref="S3.SS6.p7.2.m2.1.1"></divide><apply id="S3.SS6.p7.2.m2.1.1.1.2.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1"><abs id="S3.SS6.p7.2.m2.1.1.1.2.1.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1.2"></abs><apply id="S3.SS6.p7.2.m2.1.1.1.1.1.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p7.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS6.p7.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1.1.2">𝒟</ci><ci id="S3.SS6.p7.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS6.p7.2.m2.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S3.SS6.p7.2.m2.1.1.3.cmml" xref="S3.SS6.p7.2.m2.1.1.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p7.2.m2.1c">E\cdot\frac{\left|\mathcal{D}_{i}\right|}{B}</annotation></semantics></math>. This implies, that each client performs <math id="S3.SS6.p7.3.m3.4" class="ltx_Math" alttext="E\cdot\left[\frac{\mathbb{E}[\left|\mathcal{D}_{i}\right|]}{B}\right]=E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]" display="inline"><semantics id="S3.SS6.p7.3.m3.4a"><mrow id="S3.SS6.p7.3.m3.4.5" xref="S3.SS6.p7.3.m3.4.5.cmml"><mrow id="S3.SS6.p7.3.m3.4.5.2" xref="S3.SS6.p7.3.m3.4.5.2.cmml"><mi id="S3.SS6.p7.3.m3.4.5.2.2" xref="S3.SS6.p7.3.m3.4.5.2.2.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p7.3.m3.4.5.2.1" xref="S3.SS6.p7.3.m3.4.5.2.1.cmml">⋅</mo><mrow id="S3.SS6.p7.3.m3.4.5.2.3.2" xref="S3.SS6.p7.3.m3.4.5.2.3.1.cmml"><mo id="S3.SS6.p7.3.m3.4.5.2.3.2.1" xref="S3.SS6.p7.3.m3.4.5.2.3.1.1.cmml">[</mo><mfrac id="S3.SS6.p7.3.m3.1.1" xref="S3.SS6.p7.3.m3.1.1.cmml"><mrow id="S3.SS6.p7.3.m3.1.1.1" xref="S3.SS6.p7.3.m3.1.1.1.cmml"><mi id="S3.SS6.p7.3.m3.1.1.1.3" xref="S3.SS6.p7.3.m3.1.1.1.3.cmml">𝔼</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p7.3.m3.1.1.1.2" xref="S3.SS6.p7.3.m3.1.1.1.2.cmml">​</mo><mrow id="S3.SS6.p7.3.m3.1.1.1.1.1" xref="S3.SS6.p7.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS6.p7.3.m3.1.1.1.1.1.2" xref="S3.SS6.p7.3.m3.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.2.cmml"><mo id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.2" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.2" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.3" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS6.p7.3.m3.1.1.1.1.1.3" xref="S3.SS6.p7.3.m3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mi id="S3.SS6.p7.3.m3.1.1.3" xref="S3.SS6.p7.3.m3.1.1.3.cmml">B</mi></mfrac><mo id="S3.SS6.p7.3.m3.4.5.2.3.2.2" xref="S3.SS6.p7.3.m3.4.5.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S3.SS6.p7.3.m3.4.5.1" xref="S3.SS6.p7.3.m3.4.5.1.cmml">=</mo><mrow id="S3.SS6.p7.3.m3.4.5.3" xref="S3.SS6.p7.3.m3.4.5.3.cmml"><mi id="S3.SS6.p7.3.m3.4.5.3.2" xref="S3.SS6.p7.3.m3.4.5.3.2.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p7.3.m3.4.5.3.1" xref="S3.SS6.p7.3.m3.4.5.3.1.cmml">⋅</mo><mrow id="S3.SS6.p7.3.m3.4.5.3.3.2" xref="S3.SS6.p7.3.m3.4.5.3.3.1.cmml"><mo id="S3.SS6.p7.3.m3.4.5.3.3.2.1" xref="S3.SS6.p7.3.m3.4.5.3.3.1.1.cmml">[</mo><mfrac id="S3.SS6.p7.3.m3.4.4" xref="S3.SS6.p7.3.m3.4.4.cmml"><mrow id="S3.SS6.p7.3.m3.3.3.2" xref="S3.SS6.p7.3.m3.3.3.2.cmml"><mstyle displaystyle="false" id="S3.SS6.p7.3.m3.3.3.2.3" xref="S3.SS6.p7.3.m3.3.3.2.3.cmml"><msubsup id="S3.SS6.p7.3.m3.3.3.2.3a" xref="S3.SS6.p7.3.m3.3.3.2.3.cmml"><mo id="S3.SS6.p7.3.m3.3.3.2.3.2.2" xref="S3.SS6.p7.3.m3.3.3.2.3.2.2.cmml">∑</mo><mrow id="S3.SS6.p7.3.m3.3.3.2.3.2.3" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.cmml"><mi id="S3.SS6.p7.3.m3.3.3.2.3.2.3.2" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.2.cmml">i</mi><mo id="S3.SS6.p7.3.m3.3.3.2.3.2.3.1" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.1.cmml">=</mo><mn id="S3.SS6.p7.3.m3.3.3.2.3.2.3.3" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.SS6.p7.3.m3.2.2.1.1.1.3" xref="S3.SS6.p7.3.m3.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS6.p7.3.m3.2.2.1.1.1.3.1" xref="S3.SS6.p7.3.m3.2.2.1.1.1.2.1.cmml">|</mo><mi id="S3.SS6.p7.3.m3.2.2.1.1.1.1" xref="S3.SS6.p7.3.m3.2.2.1.1.1.1.cmml">C</mi><mo stretchy="false" id="S3.SS6.p7.3.m3.2.2.1.1.1.3.2" xref="S3.SS6.p7.3.m3.2.2.1.1.1.2.1.cmml">|</mo></mrow></msubsup></mstyle><mrow id="S3.SS6.p7.3.m3.3.3.2.2.1" xref="S3.SS6.p7.3.m3.3.3.2.2.2.cmml"><mo id="S3.SS6.p7.3.m3.3.3.2.2.1.2" xref="S3.SS6.p7.3.m3.3.3.2.2.2.1.cmml">|</mo><msub id="S3.SS6.p7.3.m3.3.3.2.2.1.1" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p7.3.m3.3.3.2.2.1.1.2" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1.2.cmml">𝒟</mi><mi id="S3.SS6.p7.3.m3.3.3.2.2.1.1.3" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.SS6.p7.3.m3.3.3.2.2.1.3" xref="S3.SS6.p7.3.m3.3.3.2.2.2.1.cmml">|</mo></mrow></mrow><mrow id="S3.SS6.p7.3.m3.4.4.3" xref="S3.SS6.p7.3.m3.4.4.3.cmml"><mrow id="S3.SS6.p7.3.m3.4.4.3.3.2" xref="S3.SS6.p7.3.m3.4.4.3.3.1.cmml"><mo stretchy="false" id="S3.SS6.p7.3.m3.4.4.3.3.2.1" xref="S3.SS6.p7.3.m3.4.4.3.3.1.1.cmml">|</mo><mi id="S3.SS6.p7.3.m3.4.4.3.1" xref="S3.SS6.p7.3.m3.4.4.3.1.cmml">C</mi><mo rspace="0.055em" stretchy="false" id="S3.SS6.p7.3.m3.4.4.3.3.2.2" xref="S3.SS6.p7.3.m3.4.4.3.3.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS6.p7.3.m3.4.4.3.2" xref="S3.SS6.p7.3.m3.4.4.3.2.cmml">⋅</mo><mi id="S3.SS6.p7.3.m3.4.4.3.4" xref="S3.SS6.p7.3.m3.4.4.3.4.cmml">B</mi></mrow></mfrac><mo id="S3.SS6.p7.3.m3.4.5.3.3.2.2" xref="S3.SS6.p7.3.m3.4.5.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p7.3.m3.4b"><apply id="S3.SS6.p7.3.m3.4.5.cmml" xref="S3.SS6.p7.3.m3.4.5"><eq id="S3.SS6.p7.3.m3.4.5.1.cmml" xref="S3.SS6.p7.3.m3.4.5.1"></eq><apply id="S3.SS6.p7.3.m3.4.5.2.cmml" xref="S3.SS6.p7.3.m3.4.5.2"><ci id="S3.SS6.p7.3.m3.4.5.2.1.cmml" xref="S3.SS6.p7.3.m3.4.5.2.1">⋅</ci><ci id="S3.SS6.p7.3.m3.4.5.2.2.cmml" xref="S3.SS6.p7.3.m3.4.5.2.2">𝐸</ci><apply id="S3.SS6.p7.3.m3.4.5.2.3.1.cmml" xref="S3.SS6.p7.3.m3.4.5.2.3.2"><csymbol cd="latexml" id="S3.SS6.p7.3.m3.4.5.2.3.1.1.cmml" xref="S3.SS6.p7.3.m3.4.5.2.3.2.1">delimited-[]</csymbol><apply id="S3.SS6.p7.3.m3.1.1.cmml" xref="S3.SS6.p7.3.m3.1.1"><divide id="S3.SS6.p7.3.m3.1.1.2.cmml" xref="S3.SS6.p7.3.m3.1.1"></divide><apply id="S3.SS6.p7.3.m3.1.1.1.cmml" xref="S3.SS6.p7.3.m3.1.1.1"><times id="S3.SS6.p7.3.m3.1.1.1.2.cmml" xref="S3.SS6.p7.3.m3.1.1.1.2"></times><ci id="S3.SS6.p7.3.m3.1.1.1.3.cmml" xref="S3.SS6.p7.3.m3.1.1.1.3">𝔼</ci><apply id="S3.SS6.p7.3.m3.1.1.1.1.2.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS6.p7.3.m3.1.1.1.1.2.1.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS6.p7.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1"><abs id="S3.SS6.p7.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.2"></abs><apply id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.2">𝒟</ci><ci id="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS6.p7.3.m3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply><ci id="S3.SS6.p7.3.m3.1.1.3.cmml" xref="S3.SS6.p7.3.m3.1.1.3">𝐵</ci></apply></apply></apply><apply id="S3.SS6.p7.3.m3.4.5.3.cmml" xref="S3.SS6.p7.3.m3.4.5.3"><ci id="S3.SS6.p7.3.m3.4.5.3.1.cmml" xref="S3.SS6.p7.3.m3.4.5.3.1">⋅</ci><ci id="S3.SS6.p7.3.m3.4.5.3.2.cmml" xref="S3.SS6.p7.3.m3.4.5.3.2">𝐸</ci><apply id="S3.SS6.p7.3.m3.4.5.3.3.1.cmml" xref="S3.SS6.p7.3.m3.4.5.3.3.2"><csymbol cd="latexml" id="S3.SS6.p7.3.m3.4.5.3.3.1.1.cmml" xref="S3.SS6.p7.3.m3.4.5.3.3.2.1">delimited-[]</csymbol><apply id="S3.SS6.p7.3.m3.4.4.cmml" xref="S3.SS6.p7.3.m3.4.4"><divide id="S3.SS6.p7.3.m3.4.4.4.cmml" xref="S3.SS6.p7.3.m3.4.4"></divide><apply id="S3.SS6.p7.3.m3.3.3.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2"><apply id="S3.SS6.p7.3.m3.3.3.2.3.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS6.p7.3.m3.3.3.2.3.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3">superscript</csymbol><apply id="S3.SS6.p7.3.m3.3.3.2.3.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS6.p7.3.m3.3.3.2.3.2.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3">subscript</csymbol><sum id="S3.SS6.p7.3.m3.3.3.2.3.2.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3.2.2"></sum><apply id="S3.SS6.p7.3.m3.3.3.2.3.2.3.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3"><eq id="S3.SS6.p7.3.m3.3.3.2.3.2.3.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.1"></eq><ci id="S3.SS6.p7.3.m3.3.3.2.3.2.3.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.SS6.p7.3.m3.3.3.2.3.2.3.3.cmml" xref="S3.SS6.p7.3.m3.3.3.2.3.2.3.3">1</cn></apply></apply><apply id="S3.SS6.p7.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS6.p7.3.m3.2.2.1.1.1.3"><abs id="S3.SS6.p7.3.m3.2.2.1.1.1.2.1.cmml" xref="S3.SS6.p7.3.m3.2.2.1.1.1.3.1"></abs><ci id="S3.SS6.p7.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS6.p7.3.m3.2.2.1.1.1.1">𝐶</ci></apply></apply><apply id="S3.SS6.p7.3.m3.3.3.2.2.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1"><abs id="S3.SS6.p7.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1.2"></abs><apply id="S3.SS6.p7.3.m3.3.3.2.2.1.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p7.3.m3.3.3.2.2.1.1.1.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1">subscript</csymbol><ci id="S3.SS6.p7.3.m3.3.3.2.2.1.1.2.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1.2">𝒟</ci><ci id="S3.SS6.p7.3.m3.3.3.2.2.1.1.3.cmml" xref="S3.SS6.p7.3.m3.3.3.2.2.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.SS6.p7.3.m3.4.4.3.cmml" xref="S3.SS6.p7.3.m3.4.4.3"><ci id="S3.SS6.p7.3.m3.4.4.3.2.cmml" xref="S3.SS6.p7.3.m3.4.4.3.2">⋅</ci><apply id="S3.SS6.p7.3.m3.4.4.3.3.1.cmml" xref="S3.SS6.p7.3.m3.4.4.3.3.2"><abs id="S3.SS6.p7.3.m3.4.4.3.3.1.1.cmml" xref="S3.SS6.p7.3.m3.4.4.3.3.2.1"></abs><ci id="S3.SS6.p7.3.m3.4.4.3.1.cmml" xref="S3.SS6.p7.3.m3.4.4.3.1">𝐶</ci></apply><ci id="S3.SS6.p7.3.m3.4.4.3.4.cmml" xref="S3.SS6.p7.3.m3.4.4.3.4">𝐵</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p7.3.m3.4c">E\cdot\left[\frac{\mathbb{E}[\left|\mathcal{D}_{i}\right|]}{B}\right]=E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]</annotation></semantics></math> steps on average, where the expectation is over the random selection of a client, which follows the uniform distribution <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>. Therefore, the expected time complexity for a single communication round, depending on the utilized algorithm, can be expressed as:</p>
</div>
<div id="S3.SS6.p8" class="ltx_para">
<table id="S3.Ex1" class="ltx_equationgroup ltx_eqn_table">

<tbody id="S3.Ex1X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell">
<a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref ltx_markedasmath"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a><span id="S3.Ex1X.2.1.1.1" class="ltx_text ltx_markedasmath ltx_font_italic">:</span>
</td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1X.4.1.1.m1.4" class="ltx_Math" alttext="\displaystyle\mathcal{O}\left(P\cdot E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]\right)" display="inline"><semantics id="S3.Ex1X.4.1.1.m1.4a"><mrow id="S3.Ex1X.4.1.1.m1.4.4" xref="S3.Ex1X.4.1.1.m1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1X.4.1.1.m1.4.4.3" xref="S3.Ex1X.4.1.1.m1.4.4.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.Ex1X.4.1.1.m1.4.4.2" xref="S3.Ex1X.4.1.1.m1.4.4.2.cmml">​</mo><mrow id="S3.Ex1X.4.1.1.m1.4.4.1.1" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.cmml"><mo id="S3.Ex1X.4.1.1.m1.4.4.1.1.2" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.cmml">(</mo><mrow id="S3.Ex1X.4.1.1.m1.4.4.1.1.1" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.cmml"><mi id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.2" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1.cmml">⋅</mo><mi id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.3" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.3.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1a" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1.cmml">⋅</mo><mrow id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.2" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.1.cmml"><mo id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.2.1" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.1.1.cmml">[</mo><mstyle displaystyle="true" id="S3.Ex1X.4.1.1.m1.3.3" xref="S3.Ex1X.4.1.1.m1.3.3.cmml"><mfrac id="S3.Ex1X.4.1.1.m1.3.3a" xref="S3.Ex1X.4.1.1.m1.3.3.cmml"><mrow id="S3.Ex1X.4.1.1.m1.2.2.2" xref="S3.Ex1X.4.1.1.m1.2.2.2.cmml"><msubsup id="S3.Ex1X.4.1.1.m1.2.2.2.3" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.cmml"><mo id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.2" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.cmml"><mi id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.2" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.2.cmml">i</mi><mo id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.1" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.3" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.3" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.3.1" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.2.1.cmml">|</mo><mi id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.1" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.1.cmml">C</mi><mo stretchy="false" id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.3.2" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></msubsup><mrow id="S3.Ex1X.4.1.1.m1.2.2.2.2.1" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.2.cmml"><mo lspace="0em" id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.2" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.2" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.2.cmml">𝒟</mi><mi id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.3" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.3" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow><mrow id="S3.Ex1X.4.1.1.m1.3.3.3" xref="S3.Ex1X.4.1.1.m1.3.3.3.cmml"><mrow id="S3.Ex1X.4.1.1.m1.3.3.3.3.2" xref="S3.Ex1X.4.1.1.m1.3.3.3.3.1.cmml"><mo stretchy="false" id="S3.Ex1X.4.1.1.m1.3.3.3.3.2.1" xref="S3.Ex1X.4.1.1.m1.3.3.3.3.1.1.cmml">|</mo><mi id="S3.Ex1X.4.1.1.m1.3.3.3.1" xref="S3.Ex1X.4.1.1.m1.3.3.3.1.cmml">C</mi><mo rspace="0.055em" stretchy="false" id="S3.Ex1X.4.1.1.m1.3.3.3.3.2.2" xref="S3.Ex1X.4.1.1.m1.3.3.3.3.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.Ex1X.4.1.1.m1.3.3.3.2" xref="S3.Ex1X.4.1.1.m1.3.3.3.2.cmml">⋅</mo><mi id="S3.Ex1X.4.1.1.m1.3.3.3.4" xref="S3.Ex1X.4.1.1.m1.3.3.3.4.cmml">B</mi></mrow></mfrac></mstyle><mo id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.2.2" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.1.1.cmml">]</mo></mrow></mrow><mo id="S3.Ex1X.4.1.1.m1.4.4.1.1.3" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1X.4.1.1.m1.4b"><apply id="S3.Ex1X.4.1.1.m1.4.4.cmml" xref="S3.Ex1X.4.1.1.m1.4.4"><times id="S3.Ex1X.4.1.1.m1.4.4.2.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.2"></times><ci id="S3.Ex1X.4.1.1.m1.4.4.3.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.3">𝒪</ci><apply id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1"><ci id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.1">⋅</ci><ci id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.2.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.2">𝑃</ci><ci id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.3.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.3">𝐸</ci><apply id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.1.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.2"><csymbol cd="latexml" id="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.4.4.1.1.1.4.2.1">delimited-[]</csymbol><apply id="S3.Ex1X.4.1.1.m1.3.3.cmml" xref="S3.Ex1X.4.1.1.m1.3.3"><divide id="S3.Ex1X.4.1.1.m1.3.3.4.cmml" xref="S3.Ex1X.4.1.1.m1.3.3"></divide><apply id="S3.Ex1X.4.1.1.m1.2.2.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2"><apply id="S3.Ex1X.4.1.1.m1.2.2.2.3.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1X.4.1.1.m1.2.2.2.3.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3">superscript</csymbol><apply id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3">subscript</csymbol><sum id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.2"></sum><apply id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3"><eq id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.1"></eq><ci id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.3.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.3.2.3.3">1</cn></apply></apply><apply id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.3"><abs id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.3.1"></abs><ci id="S3.Ex1X.4.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.1.1.1.1.1.1">𝐶</ci></apply></apply><apply id="S3.Ex1X.4.1.1.m1.2.2.2.2.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1"><abs id="S3.Ex1X.4.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.2"></abs><apply id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.2">𝒟</ci><ci id="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.3.cmml" xref="S3.Ex1X.4.1.1.m1.2.2.2.2.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.Ex1X.4.1.1.m1.3.3.3.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3"><ci id="S3.Ex1X.4.1.1.m1.3.3.3.2.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3.2">⋅</ci><apply id="S3.Ex1X.4.1.1.m1.3.3.3.3.1.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3.3.2"><abs id="S3.Ex1X.4.1.1.m1.3.3.3.3.1.1.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3.3.2.1"></abs><ci id="S3.Ex1X.4.1.1.m1.3.3.3.1.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3.1">𝐶</ci></apply><ci id="S3.Ex1X.4.1.1.m1.3.3.3.4.cmml" xref="S3.Ex1X.4.1.1.m1.3.3.3.4">𝐵</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1X.4.1.1.m1.4c">\displaystyle\mathcal{O}\left(P\cdot E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex1Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell">
<a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref ltx_markedasmath"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a><span id="S3.Ex1Xa.2.1.1.1" class="ltx_text ltx_markedasmath ltx_font_italic">:</span>
</td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1Xa.4.1.1.m1.4" class="ltx_Math" alttext="\displaystyle\mathcal{O}\left(\mathbf{L}\cdot P\cdot E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]\right)," display="inline"><semantics id="S3.Ex1Xa.4.1.1.m1.4a"><mrow id="S3.Ex1Xa.4.1.1.m1.4.4.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1Xa.4.1.1.m1.4.4.1.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.3" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.2.cmml">​</mo><mrow id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.cmml"><mo id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.2.cmml">𝐋</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.3" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.3.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1a" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.4" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.4.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1b" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1.cmml">⋅</mo><mrow id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.1.cmml"><mo id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.2.1" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.1.1.cmml">[</mo><mstyle displaystyle="true" id="S3.Ex1Xa.4.1.1.m1.3.3" xref="S3.Ex1Xa.4.1.1.m1.3.3.cmml"><mfrac id="S3.Ex1Xa.4.1.1.m1.3.3a" xref="S3.Ex1Xa.4.1.1.m1.3.3.cmml"><mrow id="S3.Ex1Xa.4.1.1.m1.2.2.2" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.cmml"><msubsup id="S3.Ex1Xa.4.1.1.m1.2.2.2.3" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.cmml"><mo id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.2" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.cmml"><mi id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.2" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.2.cmml">i</mi><mo id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.1" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.3" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.3" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.3.1" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.2.1.cmml">|</mo><mi id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.1" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.1.cmml">C</mi><mo stretchy="false" id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.3.2" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></msubsup><mrow id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.2.cmml"><mo lspace="0em" id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.2" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.2" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.2.cmml">𝒟</mi><mi id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.3" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.3" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow><mrow id="S3.Ex1Xa.4.1.1.m1.3.3.3" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.cmml"><mrow id="S3.Ex1Xa.4.1.1.m1.3.3.3.3.2" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.3.1.cmml"><mo stretchy="false" id="S3.Ex1Xa.4.1.1.m1.3.3.3.3.2.1" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.3.1.1.cmml">|</mo><mi id="S3.Ex1Xa.4.1.1.m1.3.3.3.1" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.1.cmml">C</mi><mo rspace="0.055em" stretchy="false" id="S3.Ex1Xa.4.1.1.m1.3.3.3.3.2.2" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.3.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.Ex1Xa.4.1.1.m1.3.3.3.2" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.2.cmml">⋅</mo><mi id="S3.Ex1Xa.4.1.1.m1.3.3.3.4" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.4.cmml">B</mi></mrow></mfrac></mstyle><mo id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.2.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.1.1.cmml">]</mo></mrow></mrow><mo id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.3" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1Xa.4.1.1.m1.4.4.1.2" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1Xa.4.1.1.m1.4b"><apply id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1"><times id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.2"></times><ci id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.3">𝒪</ci><apply id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1"><ci id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.1">⋅</ci><ci id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.2">𝐋</ci><ci id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.3">𝑃</ci><ci id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.4">𝐸</ci><apply id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.2"><csymbol cd="latexml" id="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.4.4.1.1.1.1.1.5.2.1">delimited-[]</csymbol><apply id="S3.Ex1Xa.4.1.1.m1.3.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3"><divide id="S3.Ex1Xa.4.1.1.m1.3.3.4.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3"></divide><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2"><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3">superscript</csymbol><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3">subscript</csymbol><sum id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.2"></sum><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3"><eq id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.1"></eq><ci id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.3.2.3.3">1</cn></apply></apply><apply id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.3"><abs id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.3.1"></abs><ci id="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.1.1.1.1.1.1">𝐶</ci></apply></apply><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1"><abs id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.2"></abs><apply id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.2">𝒟</ci><ci id="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.2.2.2.2.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.Ex1Xa.4.1.1.m1.3.3.3.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3"><ci id="S3.Ex1Xa.4.1.1.m1.3.3.3.2.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.2">⋅</ci><apply id="S3.Ex1Xa.4.1.1.m1.3.3.3.3.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.3.2"><abs id="S3.Ex1Xa.4.1.1.m1.3.3.3.3.1.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.3.2.1"></abs><ci id="S3.Ex1Xa.4.1.1.m1.3.3.3.1.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.1">𝐶</ci></apply><ci id="S3.Ex1Xa.4.1.1.m1.3.3.3.4.cmml" xref="S3.Ex1Xa.4.1.1.m1.3.3.3.4">𝐵</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xa.4.1.1.m1.4c">\displaystyle\mathcal{O}\left(\mathbf{L}\cdot P\cdot E\cdot\left[\frac{\sum_{i=1}^{|C|}\left|\mathcal{D}_{i}\right|}{|C|\cdot B}\right]\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS6.p9" class="ltx_para">
<p id="S3.SS6.p9.2" class="ltx_p">where <math id="S3.SS6.p9.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS6.p9.1.m1.1a"><mi id="S3.SS6.p9.1.m1.1.1" xref="S3.SS6.p9.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p9.1.m1.1b"><ci id="S3.SS6.p9.1.m1.1.1.cmml" xref="S3.SS6.p9.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p9.1.m1.1c">P</annotation></semantics></math> denotes the time of a forward and backward pass on the client’s local model on a batch of data <cite class="ltx_cite ltx_citemacro_citep">(Dimitrov et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>. Furthermore, it was assumed that the communication time with the central server is dominated by the average local training time for each client. In summary, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> requires <math id="S3.SS6.p9.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS6.p9.2.m2.1a"><mi id="S3.SS6.p9.2.m2.1.1" xref="S3.SS6.p9.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p9.2.m2.1b"><ci id="S3.SS6.p9.2.m2.1.1.cmml" xref="S3.SS6.p9.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p9.2.m2.1c">L</annotation></semantics></math> times as much time as <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> for each communication round.</p>
</div>
<div id="S3.SS6.p10" class="ltx_para">
<p id="S3.SS6.p10.1" class="ltx_p">During the development of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, further techniques for addressing non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and small local datasets in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> that are partially comparable to <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> have emerged, for which the similarities with and differences to <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> are discussed in Appendix <a href="#A6" title="Appendix F FedQ and Other Client Chaining Techniques ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7. </span>Achieving Communication Efficiency</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Besides the problems of data heterogeneity and clients having very little local data, constantly communicating model parametrizations can also lead to a significant overhead. The candidate generator and the ranker models are, depending on the sizes of the embeddings and the number of hidden layers, between 60MB and 120MB in size. Given the massive scale of the user base of a typical movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> can result in multiple gigabytes of data that must be communicated in each communication round, even at low client sub-sampling rates. Furthermore, the clients are relatively resource constrained, so communication reduction techniques that require complex processing, such as pruning or learned quantization, are not an option.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">A recent standard for <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a>, ISO/IEC 15938-17:2022 (MPEG-7 part 17) <cite class="ltx_cite ltx_citemacro_citep">(Moving Picture Experts Group  working group of ISO/IEC(2021), <a href="#bib.bib86" title="" class="ltx_ref">MPEG</a>; International Organization for Standardization (2022), <a href="#bib.bib47" title="" class="ltx_ref">ISO</a>; Haase et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021</a>; Kirchhoffer et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>A standards-compliant implementation of the <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> standard, under a permissive license, is available on GitHub: <a target="_blank" href="https://github.com/fraunhoferhhi/nncodec" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fraunhoferhhi/nncodec</a>.</span></span></span>, which is based on the  <a href="#id7.7.id7"><span href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Deep Context-Adaptive Binary Arithmetic Coding</span></span></a> (<a href="#id7.7.id7"><abbr href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DeepCABAC</span></abbr></a>) <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> compression algorithm <cite class="ltx_cite ltx_citemacro_citep">(Wiedemann et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2020a</a>)</cite>, has shown excellent compression results and requires only little or no preprocessing. Furthermore, it has already been shown to exhibit remarkably high performance in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setup <cite class="ltx_cite ltx_citemacro_citep">(Neumann et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite>. In its coding core, <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> combines specific quantization methods that are adapted to the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> layers, followed by a context-adaptive binary arithmetic coding method, which reduces data redundancy.</p>
</div>
<div id="S3.SS7.p3" class="ltx_para">
<p id="S3.SS7.p3.1" class="ltx_p">Employing the <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> standard to compress the upstream and downstream communication in our proposed <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> is motivated by the fact that the coding engine at its core, <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DeepCABAC</span></abbr></a>, permits higher compression performance on a variety of <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architectures than comparable techniques in the literature <cite class="ltx_cite ltx_citemacro_citep">(Wiedemann et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020b</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib121" title="" class="ltx_ref">Wiedemann et al<span class="ltx_text">.</span></a></cite> showed that the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> can be compressed by a factor of <span id="S3.SS7.p3.1.1" class="ltx_text ltx_font_bold">50.6</span> on average with negligible loss in performance. Comparable coders based on the weighted Lloyd algorithm <cite class="ltx_cite ltx_citemacro_citep">(Lloyd, <a href="#bib.bib80" title="" class="ltx_ref">1982</a>; Choi et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> or uniform quantization <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2016</a>; Choi et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> only managed to compress the models by factors of 13.6 and 5.7 respectively. For example, the authors obtained a compression ratio of 1.58% with an accuracy of 69.43% for the VGG16 architecture, whereas comparable literature reports only a compression ratio of 2.05% with an accuracy of 68.83%. Similar results are obtained for the MobileNet-v1 and MixNet architectures, resulting in a compression ratio gain of 3.6 and 92.1 percentage points, respectively, without affecting the model performance. These results are obtained by simply applying <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DeepCABAC</span></abbr></a>, they do not require the use of any optimization techniques, such as bias correction, distillation, or fine-tuning, rendering the <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> standard a straightforward plug-and-play procedure <cite class="ltx_cite ltx_citemacro_citep">(Wiedemann et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020b</a>; Neumann et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS7.p4" class="ltx_para">
<p id="S3.SS7.p4.1" class="ltx_p">There are more specialized techniques for reducing the communication overhead in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> that are, however, less comparable to <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> as they are not based on entropy coding. For example, FedFast <cite class="ltx_cite ltx_citemacro_citep">(Muhammad et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite> is an alternative to <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, which increases convergence speeds of the models and thus reduces the number of times updates have to be communicated between server and client. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib87" title="" class="ltx_ref">Muhammad et al<span class="ltx_text">.</span></a></cite> provide an experimental evaluation of their method on MovieLens 1M <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite>, MovieLens 100K <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite>, TripAdvisor hotel reviews <cite class="ltx_cite ltx_citemacro_citep">(Alam et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>, and the Yelp dataset <cite class="ltx_cite ltx_citemacro_citep">(Yelp, <a href="#bib.bib127" title="" class="ltx_ref">2021</a>)</cite>. On MovieLens 100K, FedFast required ~24.2%<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib87" title="" class="ltx_ref">Muhammad et al<span class="ltx_text">.</span></a></cite> claim that for MovieLens 100K FedFast already reached the same performance as <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> at communication round 30, which would correspond to approximately 33 times less communicated data, but their own training graphs suggest that this only happened at approximately communication round 196, which corresponds to the factor of approximately 4 that we reported here.</span></span></span><sup id="S3.SS7.p4.1.1" class="ltx_sup">,</sup><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib87" title="" class="ltx_ref">Muhammad et al<span class="ltx_text">.</span></a></cite> do not publish communication cost savings, so the values presented here were read from the training curves in Figure 3 <cite class="ltx_cite ltx_citemacro_citep">(Muhammad et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite> and are therefore only approximations.</span></span></span> of the communication rounds to achieve the highest performance of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, which corresponds to around 4 times less data communicated. On MovieLens 1M, FedFast reached the best performance of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> even faster, i.e., after only ~1.13%<a href="#footnote4" title="footnote 4 ‣ 3.7. Achieving Communication Efficiency ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> of the communication rounds that <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> required, which means that approximately 88 times less data was communicated. For TripAdvisor, FedFast only required ~7.5%<a href="#footnote4" title="footnote 4 ‣ 3.7. Achieving Communication Efficiency ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a><sup id="S3.SS7.p4.1.2" class="ltx_sup">,</sup><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Again, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib87" title="" class="ltx_ref">Muhammad et al<span class="ltx_text">.</span></a></cite> claim that FedFast was 20 times faster than <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, although their own training curves suggest it was closer to the factor of 13 reported here.</span></span></span> of time to reach the highest performance of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> as compared to the time that <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> required, which resulted in around 13 times less communication cost. Finally, FedFast required only ~17.8%<a href="#footnote4" title="footnote 4 ‣ 3.7. Achieving Communication Efficiency ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> of the communication rounds to reach the highest performance of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, in contrast to how many communication rounds <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> required to reach the performance. This reduces the communication cost of FedFast by almost a factor of 6. These results are, however, not comparable to the compression performance of other methods, as they measure the communication cost required to reach the highest accuracy of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, which, however, performs very poorly as compared to FedFast and does not even converge in the case of the TripAdvisor and Yelp datasets. Under realistic conditions, one would not stop the training there, but train the model until convergence, which in some cases happened much later. For example, the training curves presented in Figure 3 <cite class="ltx_cite ltx_citemacro_citep">(Muhammad et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite> seem to suggest that for MovieLens 100K and the Yelp dataset FedFast only reached its own highest accuracy at the very end of the training, after 1,000 communication rounds.</p>
</div>
<div id="S3.SS7.p5" class="ltx_para">
<p id="S3.SS7.p5.1" class="ltx_p">Another interesting approach is that of FedKD <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2022</a>)</cite>, where the clients train a teacher model, which is then distilled into a smaller student model. <span id="S3.SS7.p5.1.1" class="ltx_ERROR undefined">\Ac</span>fl clients communicate the compressed gradients of the student models, which substantially reduces the communication overhead. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib122" title="" class="ltx_ref">Wu et al<span class="ltx_text">.</span></a></cite> report that they accrued 18.6 times less communication cost per client on the MIND <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib123" title="" class="ltx_ref">2020</a>)</cite> dataset and 19.9 times less communication cost per client on the ADR <cite class="ltx_cite ltx_citemacro_citep">(Weissenbacher et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2018</a>)</cite> dataset as compared to directly using the larger teacher model, with no loss in performance. Both FedFast and FedKD, however, require substantial changes to the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> pipeline, while <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DeepCABAC</span></abbr></a> consistently offers high, in many cases even the highest reduction in size, while being a plug-and-play solution, that only needs to be applied to the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model. This justifies our choice of utilizing the <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> framework for our <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a>, since we can expect to have higher compression performances than the previously proposed coding techniques in the literature, without having to integrate any complex optimization techniques.</p>
</div>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8. </span>Data Security &amp; Privacy Protection</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">To achieve the goal of data security and privacy protection <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, incorporates the principles of <em id="S3.SS8.p1.1.1" class="ltx_emph ltx_font_italic">data minimization</em>, i.e., processing the data as early as possible (data processing is carried out on the client’s device), only collecting data that is absolutely necessary (e.g., in <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> only model parametrizations are transmitted), and discarding any obtained data as soon as possible (after the client models were aggregated into an updated global model, the local models are discarded). Furthermore, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> employs the principle of <em id="S3.SS8.p1.1.2" class="ltx_emph ltx_font_italic">anonymization</em>, i.e., no conclusions about the originator shall be drawn from the respective data. In terms of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, this implies that, ideally, only sending training updates should prevent the central server from deriving any further information about its clients. In practice, however, it has been shown that local samples can be reverse-engineered from the gradients <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite> in <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a>. To alleviate this problem, anonymization techniques, such as differential privacy, where random noise is added to client data communication <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2020a</a>)</cite>, or homomorphic encryption, where encrypted client updates can be aggregated without decrypting them <cite class="ltx_cite ltx_citemacro_citep">(Phong et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2018</a>; Fang and Quan, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, can be utilized.</p>
</div>
<div id="S3.SS8.p2" class="ltx_para">
<p id="S3.SS8.p2.1" class="ltx_p">Generally, these kinds of attacks are performed by the central server, who has access to the gradient updates sent by the clients. The attacks reconstruct the client’s input data by starting with some arbitrary, e.g., randomly initialized input data, and adapting this dummy data in such a way that the distance between its gradient and the actual gradient received from the client is minimized, for example, by solving the following optimization problem <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>; Dimitrov et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>:</p>
</div>
<div id="S3.SS8.p3" class="ltx_para">
<table id="A6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.8" class="ltx_Math" alttext="\displaystyle\underset{\widetilde{x}}{\text{argmin}}\;\text{dist}(\nabla_{\boldsymbol{\theta}}\,\ell(\widetilde{x},y;\boldsymbol{\theta}),\nabla_{\boldsymbol{\theta}}\,\ell(x,y;\boldsymbol{\theta}))\text{,}" display="inline"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8" xref="S3.E2.m1.8.8.cmml"><munder accentunder="true" id="S3.E2.m1.8.8.4" xref="S3.E2.m1.8.8.4.cmml"><mtext id="S3.E2.m1.8.8.4.2" xref="S3.E2.m1.8.8.4.2a.cmml">argmin</mtext><mover accent="true" id="S3.E2.m1.8.8.4.1" xref="S3.E2.m1.8.8.4.1.cmml"><mi id="S3.E2.m1.8.8.4.1.2" xref="S3.E2.m1.8.8.4.1.2.cmml">x</mi><mo id="S3.E2.m1.8.8.4.1.1" xref="S3.E2.m1.8.8.4.1.1.cmml">~</mo></mover></munder><mo lspace="0.280em" rspace="0em" id="S3.E2.m1.8.8.3" xref="S3.E2.m1.8.8.3.cmml">​</mo><mtext id="S3.E2.m1.8.8.5" xref="S3.E2.m1.8.8.5a.cmml">dist</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.3a" xref="S3.E2.m1.8.8.3.cmml">​</mo><mrow id="S3.E2.m1.8.8.2.2" xref="S3.E2.m1.8.8.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.2.2.3" xref="S3.E2.m1.8.8.2.3.cmml">(</mo><mrow id="S3.E2.m1.7.7.1.1.1" xref="S3.E2.m1.7.7.1.1.1.cmml"><mrow id="S3.E2.m1.7.7.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.2.cmml"><msub id="S3.E2.m1.7.7.1.1.1.2.1" xref="S3.E2.m1.7.7.1.1.1.2.1.cmml"><mo rspace="0.167em" id="S3.E2.m1.7.7.1.1.1.2.1.2" xref="S3.E2.m1.7.7.1.1.1.2.1.2.cmml">∇</mo><mi id="S3.E2.m1.7.7.1.1.1.2.1.3" xref="S3.E2.m1.7.7.1.1.1.2.1.3.cmml">𝜽</mi></msub><mi mathvariant="normal" id="S3.E2.m1.7.7.1.1.1.2.2" xref="S3.E2.m1.7.7.1.1.1.2.2.cmml">ℓ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.7.7.1.1.1.3.2" xref="S3.E2.m1.7.7.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.3.2.1" xref="S3.E2.m1.7.7.1.1.1.3.1.cmml">(</mo><mover accent="true" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">x</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">~</mo></mover><mo id="S3.E2.m1.7.7.1.1.1.3.2.2" xref="S3.E2.m1.7.7.1.1.1.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">y</mi><mo id="S3.E2.m1.7.7.1.1.1.3.2.3" xref="S3.E2.m1.7.7.1.1.1.3.1.cmml">;</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">𝜽</mi><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.3.2.4" xref="S3.E2.m1.7.7.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.2.2.4" xref="S3.E2.m1.8.8.2.3.cmml">,</mo><mrow id="S3.E2.m1.8.8.2.2.2" xref="S3.E2.m1.8.8.2.2.2.cmml"><mrow id="S3.E2.m1.8.8.2.2.2.2" xref="S3.E2.m1.8.8.2.2.2.2.cmml"><msub id="S3.E2.m1.8.8.2.2.2.2.1" xref="S3.E2.m1.8.8.2.2.2.2.1.cmml"><mo rspace="0.167em" id="S3.E2.m1.8.8.2.2.2.2.1.2" xref="S3.E2.m1.8.8.2.2.2.2.1.2.cmml">∇</mo><mi id="S3.E2.m1.8.8.2.2.2.2.1.3" xref="S3.E2.m1.8.8.2.2.2.2.1.3.cmml">𝜽</mi></msub><mi mathvariant="normal" id="S3.E2.m1.8.8.2.2.2.2.2" xref="S3.E2.m1.8.8.2.2.2.2.2.cmml">ℓ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.2.2.2.1" xref="S3.E2.m1.8.8.2.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.8.8.2.2.2.3.2" xref="S3.E2.m1.8.8.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.2.2.2.3.2.1" xref="S3.E2.m1.8.8.2.2.2.3.1.cmml">(</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">x</mi><mo id="S3.E2.m1.8.8.2.2.2.3.2.2" xref="S3.E2.m1.8.8.2.2.2.3.1.cmml">,</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">y</mi><mo id="S3.E2.m1.8.8.2.2.2.3.2.3" xref="S3.E2.m1.8.8.2.2.2.3.1.cmml">;</mo><mi id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">𝜽</mi><mo stretchy="false" id="S3.E2.m1.8.8.2.2.2.3.2.4" xref="S3.E2.m1.8.8.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.8.8.2.2.5" xref="S3.E2.m1.8.8.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.3b" xref="S3.E2.m1.8.8.3.cmml">​</mo><mtext id="S3.E2.m1.8.8.6" xref="S3.E2.m1.8.8.6a.cmml">,</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.cmml" xref="S3.E2.m1.8.8"><times id="S3.E2.m1.8.8.3.cmml" xref="S3.E2.m1.8.8.3"></times><apply id="S3.E2.m1.8.8.4.cmml" xref="S3.E2.m1.8.8.4"><apply id="S3.E2.m1.8.8.4.1.cmml" xref="S3.E2.m1.8.8.4.1"><ci id="S3.E2.m1.8.8.4.1.1.cmml" xref="S3.E2.m1.8.8.4.1.1">~</ci><ci id="S3.E2.m1.8.8.4.1.2.cmml" xref="S3.E2.m1.8.8.4.1.2">𝑥</ci></apply><ci id="S3.E2.m1.8.8.4.2a.cmml" xref="S3.E2.m1.8.8.4.2"><mtext id="S3.E2.m1.8.8.4.2.cmml" xref="S3.E2.m1.8.8.4.2">argmin</mtext></ci></apply><ci id="S3.E2.m1.8.8.5a.cmml" xref="S3.E2.m1.8.8.5"><mtext id="S3.E2.m1.8.8.5.cmml" xref="S3.E2.m1.8.8.5">dist</mtext></ci><interval closure="open" id="S3.E2.m1.8.8.2.3.cmml" xref="S3.E2.m1.8.8.2.2"><apply id="S3.E2.m1.7.7.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1"><times id="S3.E2.m1.7.7.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1"></times><apply id="S3.E2.m1.7.7.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.2"><apply id="S3.E2.m1.7.7.1.1.1.2.1.cmml" xref="S3.E2.m1.7.7.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.1.2.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.2.1">subscript</csymbol><ci id="S3.E2.m1.7.7.1.1.1.2.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.2.1.2">∇</ci><ci id="S3.E2.m1.7.7.1.1.1.2.1.3.cmml" xref="S3.E2.m1.7.7.1.1.1.2.1.3">𝜽</ci></apply><ci id="S3.E2.m1.7.7.1.1.1.2.2.cmml" xref="S3.E2.m1.7.7.1.1.1.2.2">ℓ</ci></apply><vector id="S3.E2.m1.7.7.1.1.1.3.1.cmml" xref="S3.E2.m1.7.7.1.1.1.3.2"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">~</ci><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑥</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑦</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝜽</ci></vector></apply><apply id="S3.E2.m1.8.8.2.2.2.cmml" xref="S3.E2.m1.8.8.2.2.2"><times id="S3.E2.m1.8.8.2.2.2.1.cmml" xref="S3.E2.m1.8.8.2.2.2.1"></times><apply id="S3.E2.m1.8.8.2.2.2.2.cmml" xref="S3.E2.m1.8.8.2.2.2.2"><apply id="S3.E2.m1.8.8.2.2.2.2.1.cmml" xref="S3.E2.m1.8.8.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.2.2.2.2.1.1.cmml" xref="S3.E2.m1.8.8.2.2.2.2.1">subscript</csymbol><ci id="S3.E2.m1.8.8.2.2.2.2.1.2.cmml" xref="S3.E2.m1.8.8.2.2.2.2.1.2">∇</ci><ci id="S3.E2.m1.8.8.2.2.2.2.1.3.cmml" xref="S3.E2.m1.8.8.2.2.2.2.1.3">𝜽</ci></apply><ci id="S3.E2.m1.8.8.2.2.2.2.2.cmml" xref="S3.E2.m1.8.8.2.2.2.2.2">ℓ</ci></apply><vector id="S3.E2.m1.8.8.2.2.2.3.1.cmml" xref="S3.E2.m1.8.8.2.2.2.3.2"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑥</ci><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝑦</ci><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">𝜽</ci></vector></apply></interval><ci id="S3.E2.m1.8.8.6a.cmml" xref="S3.E2.m1.8.8.6"><mtext id="S3.E2.m1.8.8.6.cmml" xref="S3.E2.m1.8.8.6">,</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">\displaystyle\underset{\widetilde{x}}{\text{argmin}}\;\text{dist}(\nabla_{\boldsymbol{\theta}}\,\ell(\widetilde{x},y;\boldsymbol{\theta}),\nabla_{\boldsymbol{\theta}}\,\ell(x,y;\boldsymbol{\theta}))\text{,}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS8.p4" class="ltx_para">
<p id="S3.SS8.p4.11" class="ltx_p">where <math id="S3.SS8.p4.1.m1.1" class="ltx_Math" alttext="\widetilde{x}" display="inline"><semantics id="S3.SS8.p4.1.m1.1a"><mover accent="true" id="S3.SS8.p4.1.m1.1.1" xref="S3.SS8.p4.1.m1.1.1.cmml"><mi id="S3.SS8.p4.1.m1.1.1.2" xref="S3.SS8.p4.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS8.p4.1.m1.1.1.1" xref="S3.SS8.p4.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.1.m1.1b"><apply id="S3.SS8.p4.1.m1.1.1.cmml" xref="S3.SS8.p4.1.m1.1.1"><ci id="S3.SS8.p4.1.m1.1.1.1.cmml" xref="S3.SS8.p4.1.m1.1.1.1">~</ci><ci id="S3.SS8.p4.1.m1.1.1.2.cmml" xref="S3.SS8.p4.1.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.1.m1.1c">\widetilde{x}</annotation></semantics></math> denotes the dummy input data, <math id="S3.SS8.p4.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS8.p4.2.m2.1a"><mi id="S3.SS8.p4.2.m2.1.1" xref="S3.SS8.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.2.m2.1b"><ci id="S3.SS8.p4.2.m2.1.1.cmml" xref="S3.SS8.p4.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.2.m2.1c">x</annotation></semantics></math> the unseen training sample of client <math id="S3.SS8.p4.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS8.p4.3.m3.1a"><mi id="S3.SS8.p4.3.m3.1.1" xref="S3.SS8.p4.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.3.m3.1b"><ci id="S3.SS8.p4.3.m3.1.1.cmml" xref="S3.SS8.p4.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.3.m3.1c">c</annotation></semantics></math>, <math id="S3.SS8.p4.4.m4.3" class="ltx_Math" alttext="\nabla_{\boldsymbol{\theta}}\,\ell(x,y;\boldsymbol{\theta})" display="inline"><semantics id="S3.SS8.p4.4.m4.3a"><mrow id="S3.SS8.p4.4.m4.3.4" xref="S3.SS8.p4.4.m4.3.4.cmml"><mrow id="S3.SS8.p4.4.m4.3.4.2" xref="S3.SS8.p4.4.m4.3.4.2.cmml"><msub id="S3.SS8.p4.4.m4.3.4.2.1" xref="S3.SS8.p4.4.m4.3.4.2.1.cmml"><mo id="S3.SS8.p4.4.m4.3.4.2.1.2" xref="S3.SS8.p4.4.m4.3.4.2.1.2.cmml">∇</mo><mi id="S3.SS8.p4.4.m4.3.4.2.1.3" xref="S3.SS8.p4.4.m4.3.4.2.1.3.cmml">𝜽</mi></msub><mi mathvariant="normal" id="S3.SS8.p4.4.m4.3.4.2.2" xref="S3.SS8.p4.4.m4.3.4.2.2.cmml">ℓ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS8.p4.4.m4.3.4.1" xref="S3.SS8.p4.4.m4.3.4.1.cmml">​</mo><mrow id="S3.SS8.p4.4.m4.3.4.3.2" xref="S3.SS8.p4.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS8.p4.4.m4.3.4.3.2.1" xref="S3.SS8.p4.4.m4.3.4.3.1.cmml">(</mo><mi id="S3.SS8.p4.4.m4.1.1" xref="S3.SS8.p4.4.m4.1.1.cmml">x</mi><mo id="S3.SS8.p4.4.m4.3.4.3.2.2" xref="S3.SS8.p4.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS8.p4.4.m4.2.2" xref="S3.SS8.p4.4.m4.2.2.cmml">y</mi><mo id="S3.SS8.p4.4.m4.3.4.3.2.3" xref="S3.SS8.p4.4.m4.3.4.3.1.cmml">;</mo><mi id="S3.SS8.p4.4.m4.3.3" xref="S3.SS8.p4.4.m4.3.3.cmml">𝜽</mi><mo stretchy="false" id="S3.SS8.p4.4.m4.3.4.3.2.4" xref="S3.SS8.p4.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.4.m4.3b"><apply id="S3.SS8.p4.4.m4.3.4.cmml" xref="S3.SS8.p4.4.m4.3.4"><times id="S3.SS8.p4.4.m4.3.4.1.cmml" xref="S3.SS8.p4.4.m4.3.4.1"></times><apply id="S3.SS8.p4.4.m4.3.4.2.cmml" xref="S3.SS8.p4.4.m4.3.4.2"><apply id="S3.SS8.p4.4.m4.3.4.2.1.cmml" xref="S3.SS8.p4.4.m4.3.4.2.1"><csymbol cd="ambiguous" id="S3.SS8.p4.4.m4.3.4.2.1.1.cmml" xref="S3.SS8.p4.4.m4.3.4.2.1">subscript</csymbol><ci id="S3.SS8.p4.4.m4.3.4.2.1.2.cmml" xref="S3.SS8.p4.4.m4.3.4.2.1.2">∇</ci><ci id="S3.SS8.p4.4.m4.3.4.2.1.3.cmml" xref="S3.SS8.p4.4.m4.3.4.2.1.3">𝜽</ci></apply><ci id="S3.SS8.p4.4.m4.3.4.2.2.cmml" xref="S3.SS8.p4.4.m4.3.4.2.2">ℓ</ci></apply><vector id="S3.SS8.p4.4.m4.3.4.3.1.cmml" xref="S3.SS8.p4.4.m4.3.4.3.2"><ci id="S3.SS8.p4.4.m4.1.1.cmml" xref="S3.SS8.p4.4.m4.1.1">𝑥</ci><ci id="S3.SS8.p4.4.m4.2.2.cmml" xref="S3.SS8.p4.4.m4.2.2">𝑦</ci><ci id="S3.SS8.p4.4.m4.3.3.cmml" xref="S3.SS8.p4.4.m4.3.3">𝜽</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.4.m4.3c">\nabla_{\boldsymbol{\theta}}\,\ell(x,y;\boldsymbol{\theta})</annotation></semantics></math> the intercepted gradient of client <math id="S3.SS8.p4.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS8.p4.5.m5.1a"><mi id="S3.SS8.p4.5.m5.1.1" xref="S3.SS8.p4.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.5.m5.1b"><ci id="S3.SS8.p4.5.m5.1.1.cmml" xref="S3.SS8.p4.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.5.m5.1c">c</annotation></semantics></math>, <math id="S3.SS8.p4.6.m6.3" class="ltx_Math" alttext="\nabla_{\boldsymbol{\theta}}\,\ell(\widetilde{x},y;\boldsymbol{\theta})" display="inline"><semantics id="S3.SS8.p4.6.m6.3a"><mrow id="S3.SS8.p4.6.m6.3.4" xref="S3.SS8.p4.6.m6.3.4.cmml"><mrow id="S3.SS8.p4.6.m6.3.4.2" xref="S3.SS8.p4.6.m6.3.4.2.cmml"><msub id="S3.SS8.p4.6.m6.3.4.2.1" xref="S3.SS8.p4.6.m6.3.4.2.1.cmml"><mo id="S3.SS8.p4.6.m6.3.4.2.1.2" xref="S3.SS8.p4.6.m6.3.4.2.1.2.cmml">∇</mo><mi id="S3.SS8.p4.6.m6.3.4.2.1.3" xref="S3.SS8.p4.6.m6.3.4.2.1.3.cmml">𝜽</mi></msub><mi mathvariant="normal" id="S3.SS8.p4.6.m6.3.4.2.2" xref="S3.SS8.p4.6.m6.3.4.2.2.cmml">ℓ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS8.p4.6.m6.3.4.1" xref="S3.SS8.p4.6.m6.3.4.1.cmml">​</mo><mrow id="S3.SS8.p4.6.m6.3.4.3.2" xref="S3.SS8.p4.6.m6.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS8.p4.6.m6.3.4.3.2.1" xref="S3.SS8.p4.6.m6.3.4.3.1.cmml">(</mo><mover accent="true" id="S3.SS8.p4.6.m6.1.1" xref="S3.SS8.p4.6.m6.1.1.cmml"><mi id="S3.SS8.p4.6.m6.1.1.2" xref="S3.SS8.p4.6.m6.1.1.2.cmml">x</mi><mo id="S3.SS8.p4.6.m6.1.1.1" xref="S3.SS8.p4.6.m6.1.1.1.cmml">~</mo></mover><mo id="S3.SS8.p4.6.m6.3.4.3.2.2" xref="S3.SS8.p4.6.m6.3.4.3.1.cmml">,</mo><mi id="S3.SS8.p4.6.m6.2.2" xref="S3.SS8.p4.6.m6.2.2.cmml">y</mi><mo id="S3.SS8.p4.6.m6.3.4.3.2.3" xref="S3.SS8.p4.6.m6.3.4.3.1.cmml">;</mo><mi id="S3.SS8.p4.6.m6.3.3" xref="S3.SS8.p4.6.m6.3.3.cmml">𝜽</mi><mo stretchy="false" id="S3.SS8.p4.6.m6.3.4.3.2.4" xref="S3.SS8.p4.6.m6.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.6.m6.3b"><apply id="S3.SS8.p4.6.m6.3.4.cmml" xref="S3.SS8.p4.6.m6.3.4"><times id="S3.SS8.p4.6.m6.3.4.1.cmml" xref="S3.SS8.p4.6.m6.3.4.1"></times><apply id="S3.SS8.p4.6.m6.3.4.2.cmml" xref="S3.SS8.p4.6.m6.3.4.2"><apply id="S3.SS8.p4.6.m6.3.4.2.1.cmml" xref="S3.SS8.p4.6.m6.3.4.2.1"><csymbol cd="ambiguous" id="S3.SS8.p4.6.m6.3.4.2.1.1.cmml" xref="S3.SS8.p4.6.m6.3.4.2.1">subscript</csymbol><ci id="S3.SS8.p4.6.m6.3.4.2.1.2.cmml" xref="S3.SS8.p4.6.m6.3.4.2.1.2">∇</ci><ci id="S3.SS8.p4.6.m6.3.4.2.1.3.cmml" xref="S3.SS8.p4.6.m6.3.4.2.1.3">𝜽</ci></apply><ci id="S3.SS8.p4.6.m6.3.4.2.2.cmml" xref="S3.SS8.p4.6.m6.3.4.2.2">ℓ</ci></apply><vector id="S3.SS8.p4.6.m6.3.4.3.1.cmml" xref="S3.SS8.p4.6.m6.3.4.3.2"><apply id="S3.SS8.p4.6.m6.1.1.cmml" xref="S3.SS8.p4.6.m6.1.1"><ci id="S3.SS8.p4.6.m6.1.1.1.cmml" xref="S3.SS8.p4.6.m6.1.1.1">~</ci><ci id="S3.SS8.p4.6.m6.1.1.2.cmml" xref="S3.SS8.p4.6.m6.1.1.2">𝑥</ci></apply><ci id="S3.SS8.p4.6.m6.2.2.cmml" xref="S3.SS8.p4.6.m6.2.2">𝑦</ci><ci id="S3.SS8.p4.6.m6.3.3.cmml" xref="S3.SS8.p4.6.m6.3.3">𝜽</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.6.m6.3c">\nabla_{\boldsymbol{\theta}}\,\ell(\widetilde{x},y;\boldsymbol{\theta})</annotation></semantics></math> the gradient computed on the dummy input data <math id="S3.SS8.p4.7.m7.1" class="ltx_Math" alttext="\widetilde{x}" display="inline"><semantics id="S3.SS8.p4.7.m7.1a"><mover accent="true" id="S3.SS8.p4.7.m7.1.1" xref="S3.SS8.p4.7.m7.1.1.cmml"><mi id="S3.SS8.p4.7.m7.1.1.2" xref="S3.SS8.p4.7.m7.1.1.2.cmml">x</mi><mo id="S3.SS8.p4.7.m7.1.1.1" xref="S3.SS8.p4.7.m7.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.7.m7.1b"><apply id="S3.SS8.p4.7.m7.1.1.cmml" xref="S3.SS8.p4.7.m7.1.1"><ci id="S3.SS8.p4.7.m7.1.1.1.cmml" xref="S3.SS8.p4.7.m7.1.1.1">~</ci><ci id="S3.SS8.p4.7.m7.1.1.2.cmml" xref="S3.SS8.p4.7.m7.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.7.m7.1c">\widetilde{x}</annotation></semantics></math> with the ground-truth <math id="S3.SS8.p4.8.m8.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS8.p4.8.m8.1a"><mi id="S3.SS8.p4.8.m8.1.1" xref="S3.SS8.p4.8.m8.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.8.m8.1b"><ci id="S3.SS8.p4.8.m8.1.1.cmml" xref="S3.SS8.p4.8.m8.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.8.m8.1c">y</annotation></semantics></math>, which can, for example, be extracted from the gradient of the output layer <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>)</cite>, <math id="S3.SS8.p4.9.m9.1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><semantics id="S3.SS8.p4.9.m9.1a"><mi id="S3.SS8.p4.9.m9.1.1" xref="S3.SS8.p4.9.m9.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.9.m9.1b"><ci id="S3.SS8.p4.9.m9.1.1.cmml" xref="S3.SS8.p4.9.m9.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.9.m9.1c">\boldsymbol{\theta}</annotation></semantics></math> the parametrization of the updated local model of client <math id="S3.SS8.p4.10.m10.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS8.p4.10.m10.1a"><mi id="S3.SS8.p4.10.m10.1.1" xref="S3.SS8.p4.10.m10.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.10.m10.1b"><ci id="S3.SS8.p4.10.m10.1.1.cmml" xref="S3.SS8.p4.10.m10.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.10.m10.1c">c</annotation></semantics></math>, and <math id="S3.SS8.p4.11.m11.1" class="ltx_Math" alttext="\text{dist}(\cdot)" display="inline"><semantics id="S3.SS8.p4.11.m11.1a"><mrow id="S3.SS8.p4.11.m11.1.2" xref="S3.SS8.p4.11.m11.1.2.cmml"><mtext id="S3.SS8.p4.11.m11.1.2.2" xref="S3.SS8.p4.11.m11.1.2.2a.cmml">dist</mtext><mo lspace="0em" rspace="0em" id="S3.SS8.p4.11.m11.1.2.1" xref="S3.SS8.p4.11.m11.1.2.1.cmml">​</mo><mrow id="S3.SS8.p4.11.m11.1.2.3.2" xref="S3.SS8.p4.11.m11.1.2.cmml"><mo stretchy="false" id="S3.SS8.p4.11.m11.1.2.3.2.1" xref="S3.SS8.p4.11.m11.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS8.p4.11.m11.1.1" xref="S3.SS8.p4.11.m11.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS8.p4.11.m11.1.2.3.2.2" xref="S3.SS8.p4.11.m11.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p4.11.m11.1b"><apply id="S3.SS8.p4.11.m11.1.2.cmml" xref="S3.SS8.p4.11.m11.1.2"><times id="S3.SS8.p4.11.m11.1.2.1.cmml" xref="S3.SS8.p4.11.m11.1.2.1"></times><ci id="S3.SS8.p4.11.m11.1.2.2a.cmml" xref="S3.SS8.p4.11.m11.1.2.2"><mtext id="S3.SS8.p4.11.m11.1.2.2.cmml" xref="S3.SS8.p4.11.m11.1.2.2">dist</mtext></ci><ci id="S3.SS8.p4.11.m11.1.1.cmml" xref="S3.SS8.p4.11.m11.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p4.11.m11.1c">\text{dist}(\cdot)</annotation></semantics></math> a distance function. For example, <cite class="ltx_cite ltx_citemacro_citet">Geiping et al<span class="ltx_text">.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite> show that in many cases it is possible to use such a technique to reconstruct training images almost perfectly from the gradient, thus demonstrating that <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a> is not as privacy-preserving as thought. A related method, proposed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib12" title="" class="ltx_ref">Chai et al<span class="ltx_text">.</span></a></cite>, is able to reverse-engineer a user’s rating information from two consecutive gradient updates in a <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> based on matrix factorization, where the factorization is learned by the users using  <a href="#id41.41.id41"><span href="#id41.41.id41" title="stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">stochastic gradient descent</span></span></a> (<a href="#id41.41.id41"><abbr href="#id41.41.id41" title="stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGD</span></abbr></a>) <cite class="ltx_cite ltx_citemacro_citep">(Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. The attack proposed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib12" title="" class="ltx_ref">Chai et al<span class="ltx_text">.</span></a></cite> is specifically tailored towards federated matrix factorization and is therefore not applicable to our scenario. Furthermore, both methods need to intercept the client’s gradient updates and are therefore only pertinent to <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a>. <span id="S3.SS8.p4.11.1" class="ltx_ERROR undefined">\Ac</span>fedq on the other hand, which is employed by us, does not share the gradient but the updated local model and is thus not vulnerable to these kinds of attacks.</p>
</div>
<div id="S3.SS8.p5" class="ltx_para">
<p id="S3.SS8.p5.1" class="ltx_p">And still, <cite class="ltx_cite ltx_citemacro_citet">Dimitrov et al<span class="ltx_text">.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> showed that it is possible to reconstruct training images in realistic <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> settings. Despite the method’s success with a single client relying on many local training rounds, attacking aggregated parameter updates from multiple clients, even if only a few of them are used, significantly degrades the reconstruction performance. Using the  <a href="#id19.19.id19"><span href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated EMNIST</span></span></a> (<a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>) dataset for demonstration, they specifically showed that attacking the averaged updates of just four clients instead of one significantly lowers the average reconstruction performance of images with <a href="#id36.36.id36"><span href="#id36.36.id36" title="peak signal-to-noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">peak signal-to-noise ratios</span></span></a> of 20 or above by 35.8 percentage points, which is evaluated on 100 randomly selected clients from the training set. When conducting this experiment, they chose an optimal configuration of local epochs and batch sizes for the clients. In addition, they rely on the unrealistic assumption that the label counts are known. Having to estimate them, degraded the attack performance by 17 percentage points using the updated parameters of only a single client. In <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, an attacker can easily retrieve the parameter updates of individual clients, thus making this kind of attack highly effective. But, by the very nature of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, a potential attacker usually only receives aggregated parameter updates from multiple clients. Thus, using a reasonably large queue of clients should guarantee a high level of data security.</p>
</div>
<div id="S3.SS8.p6" class="ltx_para">
<p id="S3.SS8.p6.1" class="ltx_p">Some recent works have tried to employ gradient/parameter obfuscation to counteract these kinds of attacks. For example, differential privacy is an obfuscation method, where random noise is added to the client updates. While differential privacy is one of the most prevalent obfuscation schemes, others, like gradient quantization and magnitude pruning, have been proposed. For example, <cite class="ltx_cite ltx_citemacro_citet">Wei et al<span class="ltx_text">.</span> (<a href="#bib.bib118" title="" class="ltx_ref">2020b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhu et al<span class="ltx_text">.</span> (<a href="#bib.bib135" title="" class="ltx_ref">2019</a>)</cite> showed that gradient sparsification is a well-functioning approach to mitigate data reconstruction attacks. <cite class="ltx_cite ltx_citemacro_citet">Ovi et al<span class="ltx_text">.</span> (<a href="#bib.bib89" title="" class="ltx_ref">2022</a>)</cite> demonstrated the efficacy of using mixed precision quantization to counteract gradient leakage attacks. They quantized the model gradients of the clients after local training to 16-bit and 8-bit integers before sending them to the central server and showed experimentally that no information was leaked. They ran the attack for 450 iterations both on <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setups where the communicated gradients were quantized, as well as baseline <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setups without gradient quantization using the  <a href="#id30.30.id30"><span href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Modified NIST Database</span></span></a> (<a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a>), Fashion-<a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a>, and CIFAR-10 datasets. In the baseline experiments, training images could be extracted from the gradients after 20 iterations for <a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a>, 20 iterations for Fashion-<a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a>, and 40 iterations for CIFAR-10. In the experiments that applied gradient quantization they were not able to extract any training samples, even after 450 iterations of their attack. Our <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> module is capable of using an arbitrary number of quantization points for quantizing model updates, where the number of quantization points is fine-tuned for each layer. This results in a quantization that goes well below 16-bit and in many cases even below 8-bit quantization, which should result in better obfuscation.</p>
</div>
<div id="S3.SS8.p7" class="ltx_para">
<p id="S3.SS8.p7.1" class="ltx_p">These works are good indicators that gradient obfuscation techniques can be successfully employed to counteract attacks such as those proposed by <cite class="ltx_cite ltx_citemacro_citet">Geiping et al<span class="ltx_text">.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Dimitrov et al<span class="ltx_text">.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Yue et al<span class="ltx_text">.</span> (<a href="#bib.bib129" title="" class="ltx_ref">2022</a>)</cite>, however, call the effectiveness of gradient obfuscation into question, by proposing a novel data reconstruction attack scheme. However, they have only shown their attack to be effective in the domain of image classification, which is a special case, as even reconstructed images that diverge a lot from the actual input image, may contain enough visual information for human observers, while the same amount of reconstruction error on tabular data, as used in <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a>, would not be usable with the same amount of error. Also, they have only tried small batch sizes with a small number of local epochs and have only shown uniform quantization. Therefore, we still conjecture that the error induced by the mixed-precision quantization of our <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> module, may successfully obfuscate the information contained in the parameter updates sent to the central server or at least make it much harder for attackers to recover any useful information. This, however, remains to be tested in future work.</p>
</div>
<div id="S3.SS8.p8" class="ltx_para">
<p id="S3.SS8.p8.1" class="ltx_p">Finally, most of these attacks assume the central server to be the culprit who wants to reconstruct the input data of its clients. We want to note that outside attackers are usually incapable of intercepting any data from the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> process as simple techniques, such as employing SSL/TLS, can effectively mitigate these kinds of attacks.</p>
</div>
<div id="S3.SS8.p9" class="ltx_para">
<p id="S3.SS8.p9.1" class="ltx_p">This concludes our new proposed <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a>, which consists of a three-staged recommendation architecture, including a candidate generation, ranking, and re-ranking stage. Furthermore, the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> was extended to use <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, applying the developed <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> method to effectively operate with extremely high numbers of heterogeneous clients. The communication overhead introduced by constantly communicating parameter updates between the central server and the clients is alleviated by compressing the model parametrizations using a state-of-the-art <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> compression scheme. Finally, we have discussed the data security and privacy protection capabilities of the proposed architecture. In the following Section <a href="#S4" title="4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we will evaluate the performance of the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> system experimentally.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe the experiments performed using our <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> and evaluate its performance. We will first start by describing how the dataset was acquired and processed. Then, we will lay out a non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline to which we will compare the performance of the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> system. Then we will demonstrate that standard <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> only yields a moderate performance before applying the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> algorithm to improve performance to equal or even exceed the performance of the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline. Finally, we will show how the new <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> standard can be utilized to significantly decrease the communication overhead. All experiments were performed using PyTorch <cite class="ltx_cite ltx_citemacro_citep">(Paszke et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Among datasets suitable for movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>, the MovieLens dataset by <cite class="ltx_cite ltx_citemacro_citet">Harper and Konstan (<a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite> is one of the most widely known and used datasets. MovieLens comes in multiple different flavors, among which the <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">25M</em> variant is the latest stable benchmark dataset. It contains more than 25 million ratings across almost 60,000 movies made by more than 162,000 users. The MovieLens datasets consist of users, movies, ratings, and tags. As the 25M flavor of the MovieLens dataset is a stable benchmark dataset, it was chosen for our experiments.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Dataset Analysis</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">For the candidate generation model, we treat the ratings of the MovieLens dataset as movie watches to predict future watches from past viewing behavior. Therefore, the temporal cohesion of the ratings is particularly important. During an initial screening of the dataset, we observed that the data was inconsistent with “normal” viewing behavior, at least for a small number of random samples. For example, some users rated an infeasible number of movies in a single day, while other users had an impossibly high number of total ratings. Therefore, the MovieLens 25M dataset was inspected more closely in terms of four different metrics: (1) average times between ratings of all users in the dataset, i.e., the speed at which users have rated movies, (2) number of ratings per user, (3) number of ratings per movie, and (4) number of ratings cast by rating value. The results are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1.1. Dataset Analysis ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The MovieLens 25M dataset contains <span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">59,047</span> movies that have been rated <span id="S4.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_bold">25,000,095</span> times by <span id="S4.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_bold">162,541</span> users. <span id="S4.SS1.SSS1.p2.1.4" class="ltx_text ltx_font_bold">87.1%</span> of users have an average time of less than <span id="S4.SS1.SSS1.p2.1.5" class="ltx_text ltx_font_bold">1 minute</span> and <span id="S4.SS1.SSS1.p2.1.6" class="ltx_text ltx_font_bold">97.3%</span> have an average time of less than <span id="S4.SS1.SSS1.p2.1.7" class="ltx_text ltx_font_bold">1 hour</span> between two ratings. On average, there are <span id="S4.SS1.SSS1.p2.1.8" class="ltx_text ltx_font_bold">32.7</span> minutes between two ratings. The smallest number of ratings per user is <span id="S4.SS1.SSS1.p2.1.9" class="ltx_text ltx_font_bold">20</span> and the highest number of ratings of any user is <span id="S4.SS1.SSS1.p2.1.10" class="ltx_text ltx_font_bold">32,202</span>. On average, each user has <span id="S4.SS1.SSS1.p2.1.11" class="ltx_text ltx_font_bold">153.8</span> ratings. <span id="S4.SS1.SSS1.p2.1.12" class="ltx_text ltx_font_bold">58.8%</span> of movies have less than <span id="S4.SS1.SSS1.p2.1.13" class="ltx_text ltx_font_bold">10</span> ratings and <span id="S4.SS1.SSS1.p2.1.14" class="ltx_text ltx_font_bold">82.5%</span> have less than <span id="S4.SS1.SSS1.p2.1.15" class="ltx_text ltx_font_bold">100</span> ratings. On average, each movie has <span id="S4.SS1.SSS1.p2.1.16" class="ltx_text ltx_font_bold">423.4</span> ratings. The smallest number of ratings per movie is <span id="S4.SS1.SSS1.p2.1.17" class="ltx_text ltx_font_bold">1</span> and the highest number of ratings of any movie is <span id="S4.SS1.SSS1.p2.1.18" class="ltx_text ltx_font_bold">81,491</span>. The <span id="S4.SS1.SSS1.p2.1.19" class="ltx_text ltx_font_bold">top-10</span> most-rated movies have amassed <span id="S4.SS1.SSS1.p2.1.20" class="ltx_text ltx_font_bold">2.8%</span> of all ratings.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="322" height="215" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.6.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S4.F5.7.2" class="ltx_text" style="font-size:90%;">In-depth analysis histograms of the MovieLens 25M dataset: <span id="S4.F5.7.2.1" class="ltx_text ltx_font_bold">(a)</span> average times between ratings of all users in the dataset, <span id="S4.F5.7.2.2" class="ltx_text ltx_font_bold">(b)</span> number of ratings per user, <span id="S4.F5.7.2.3" class="ltx_text ltx_font_bold">(c)</span> number of ratings per movie, and <span id="S4.F5.7.2.4" class="ltx_text ltx_font_bold">(d)</span> number of ratings of a specific value that were cast by the users.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F5.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.9" class="ltx_p ltx_figure_panel ltx_align_center">In-depth analysis histograms of the MovieLens 25M dataset: <span id="S4.F5.9.1" class="ltx_text ltx_font_bold">(a)</span> average times between ratings of all users in the dataset, <span id="S4.F5.9.2" class="ltx_text ltx_font_bold">(b)</span> number of ratings per user, <span id="S4.F5.9.3" class="ltx_text ltx_font_bold">(c)</span> number of ratings per movie, and <span id="S4.F5.9.4" class="ltx_text ltx_font_bold">(d)</span> number of ratings of a specific value that were cast by the users.</p>
</div>
</div>
</figure>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">These findings suggest that most of the ratings were performed in a way that indicates that the users of the MovieLens website have mass-rated movies, rather than individually casting the ratings after watching each movie. The ratings per movie are also highly imbalanced, as most movies have few ratings and a few movies have a large number of ratings. This is actually somewhat expected, as there are only a few “blockbuster” movies that many people watch, while most movies are only watched by very few people. Finally, the ratings are heavily skewed towards more positive evaluations: ratings of 3.0 and higher are significantly more prevalent than those of 2.5 and below.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p id="S4.SS1.SSS1.p4.1" class="ltx_p">The in-depth analysis suggests that the MovieLens dataset may not be suitable for next watch predictions, as the mass-ratings imply that the temporal order does not necessarily coincide with the order in which the movies were watched. To avoid an ill-posed task from the start, a set of experiments were performed, where the user ratings were sorted in multiple ways: by timestamp in ascending order, by timestamp in descending order, by rating in ascending order, by rating in descending order, and in random order. The results of this experiment are shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.1.1. Dataset Analysis ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The findings reveal that ordering the movie watches by timestamp yields a higher prediction performance, which is measured in terms of top-100 accuracy, than any other ordering<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Surprisingly, ordering the ratings from future to present, i.e., predicting past movie watches from future watch behavior, yields a slightly higher performance than the regular temporal ordering. A statistical fluke can be ruled out, as the experiment was repeated five times and the plot shows the minimum, maximum, and mean top-100 accuracy. We have no explanation for this interesting result.</span></span></span>. Ordering by rating already gives a lower prediction performance, but it is still higher than the performance for random order. This means that, despite the ratings not conforming to “normal” viewing behavior, the dataset is actually suitable for the purposes of training the candidate generator, because the assumption of temporal cohesion holds.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="181" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy results vs. number of epochs for different temporal orderings in the dataset.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F6.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F6.5" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy results vs. number of epochs for different temporal orderings in the dataset.</p>
</div>
</div>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Dataset Preprocessing</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The candidate generator and the ranker models each have different inputs and outputs, and therefore require a custom dataset that has to be derived from MovieLens. We refer to the dataset for the candidate generation model as the watch history dataset, and the dataset for the ranker model as the rating dataset.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">The samples of the former consist of a list of previous movies that a user has watched and a single future movie as prediction target. Since the movie prediction is performed on a per-user basis, the dataset is first grouped by users. Watch histories are made from consecutive movie watches; therefore, the ratings are then ordered by their timestamp. A sliding window is used to extract watch history samples from the movie watches of the users. The preprocessing of the dataset is visualized in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.1.2. Dataset Preprocessing ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The created samples are then stored in a suitable data format for the training, validation, and testing of the candidate generator model.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="141" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Preprocessing the MovieLens 25M dataset for the training of the candidate generator model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F7.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F7.5" class="ltx_p ltx_figure_panel ltx_align_center">Preprocessing the MovieLens 25M dataset for the training of the candidate generator model.</p>
</div>
</div>
</figure>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">The use of a sliding window with a defined upper limit for the number of movies in a watch history is based on the premise that the users’ tastes change over time. This implies that a watch becomes less predictive of subsequent watches the longer it lies in the past. Furthermore, depending on the dataset size and the number of trainable parameters, the candidate generator model has an upper capacity limit for learning structure. For too high values of <em id="S4.SS1.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">window size</em> the candidate generator model performs worse as it is unable to learn the complex correlations in the input data. To determine the optimal window size, multiple datasets with different window sizes were created and used for training the models. The results suggest an optimal window size of 7 (cf. Figure <a href="#S4.F8" title="Figure 8 ‣ 4.1.2. Dataset Preprocessing ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x8.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="181" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Determining the optimal window size for the MovieLens 25M dataset.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F8.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F8.5" class="ltx_p ltx_figure_panel ltx_align_center">Determining the optimal window size for the MovieLens 25M dataset.</p>
</div>
</div>
</figure>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">The rating dataset is much simpler, as the MovieLens samples do not have to be reinterpreted. Instead, the rating samples can be directly inferred from each MovieLens sample. Each of it consists of user ID, movie ID, genres of the movie, and user rating. Optionally, the age of the movie and the rating age can be added. The rating age is computed from the rating timestamps, while the movie age is derived from the movie release date, which was retrieved by cross-referencing the MovieLens movies with their corresponding entries in  <a href="#id43.43.id43"><span href="#id43.43.id43" title="The Movie Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">The Movie Database</span></span></a> (<a href="#id43.43.id43"><abbr href="#id43.43.id43" title="The Movie Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TMDb</span></abbr></a>). The movie age and the rating age are both normalized between -1 and 1. Adding the movie age should encourage the model to learn that certain users prefer older or newer movies. The rating age is used to provide the model with an understanding of the temporal component of ratings. During inference, the rating age can be set to 1 to ensure that the model does not take old information about the user into consideration, and thus makes predictions right at the end of the training window. A similar technique has been proposed by <cite class="ltx_cite ltx_citemacro_citet">Covington et al<span class="ltx_text">.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. In order to determine the efficacy of adding these two features, experiments were performed, whose results are presented in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1.2. Dataset Preprocessing ‣ 4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="363" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results vs. number of epochs for movie age, rating age, both, and none.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F9.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F9.5" class="ltx_p ltx_figure_panel ltx_align_center">Validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results vs. number of epochs for movie age, rating age, both, and none.</p>
</div>
</div>
</figure>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p">Using the movie age yields the best overall accuracy, closely followed by using neither movie nor rating age. Utilizing either the rating age alone or the rating age and the movie age together, results in slower convergence of the model, as well as lower overall accuracy. In terms of <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, using movie age, rating age, and using neither yield almost the same overall performance, while using both performs slightly worse. For this reason, we decided to only use the movie age and discard the rating age.</p>
</div>
<div id="S4.SS1.SSS2.p6" class="ltx_para">
<p id="S4.SS1.SSS2.p6.1" class="ltx_p">In order to perform <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments, both the watch history, as well as the rating datasets, were split into much smaller subsets for each <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client. Since the movie IDs, user IDs, and genres are fed into embedding layers, the datasets were not simply split randomly, but in a way, that the training data still contained all possible IDs. Otherwise, the validation and test subsets may end up containing IDs that the model was not trained on. For testing the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> pipeline, the datasets were randomly split into equal-sized subsets for all <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients, which ensures that the client datasets are balanced and somewhat <a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>. As the MovieLens dataset also contains user IDs, the samples could be split such that each <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client receives samples of a single MovieLens user. This allows for properly simulating real-world conditions with non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Baseline Experiments</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We first conducted a baseline experiment using the hyperparameters that were selected based on the experiments described in Appendices <a href="#A2" title="Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> and <a href="#A3" title="Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, as well as in Section <a href="#S4.SS1" title="4.1. Dataset ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. These experiments are used as a baseline for the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments. We trained the candidate generator and the ranker models five times each and present their minimum, maximum, and mean performance in Figure <a href="#S4.F10.sf2" title="In Figure 10 ‣ 4.2. Baseline Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>.</p>
</div>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x10.png" id="S4.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="122" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F10.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x11.png" id="S4.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="245" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F10.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S4.F10.5.2" class="ltx_text" style="font-size:90%;">Non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline experiment results for <span id="S4.F10.5.2.1" class="ltx_text ltx_font_bold">(<a href="#S4.F10.sf1" title="In Figure 10 ‣ 4.2. Baseline Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>)</span> the candidate generator and <span id="S4.F10.5.2.2" class="ltx_text ltx_font_bold">(<a href="#S4.F10.sf2" title="In Figure 10 ‣ 4.2. Baseline Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>)</span> the ranker.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F10.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F10.7" class="ltx_p ltx_figure_panel ltx_align_center">Non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline experiment results for <span id="S4.F10.7.1" class="ltx_text ltx_font_bold">(<a href="#S4.F10.sf1" title="In Figure 10 ‣ 4.2. Baseline Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>)</span> the candidate generator and <span id="S4.F10.7.2" class="ltx_text ltx_font_bold">(<a href="#S4.F10.sf2" title="In Figure 10 ‣ 4.2. Baseline Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>)</span> the ranker.</p>
</div>
</div>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">The candidate generator outputs a probability distribution over the entire corpus of movies in the MovieLens dataset, which means that it has to distinguish between almost 60,000 classes. Therefore, we report top-100 accuracy (also sometimes referred to as hit-ratio@<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">k</annotation></semantics></math>, where <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="k=100" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">k=100</annotation></semantics></math>), which rates a classification result as “correct” if the ground-truth next watched movie is among the 100 movies with the highest classification scores. For the ranker, we report accuracy, as well as <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, which measures how much the predicted rating differs from its ground-truth. The performance was measured on a validation subset of the dataset, which is distinct from the training subset. The highest final top-100 accuracy that was achieved by any of the five trained candidate generators was <span id="S4.SS2.p2.2.1" class="ltx_text ltx_font_bold">47.26%</span>, with an average top-100 accuracy of <span id="S4.SS2.p2.2.2" class="ltx_text ltx_font_bold">47.15%</span>. The best performing ranker model achieved a final accuracy of <span id="S4.SS2.p2.2.3" class="ltx_text ltx_font_bold">38.43%</span> and a final <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> of <span id="S4.SS2.p2.2.4" class="ltx_text ltx_font_bold">0.91</span>, with a mean final accuracy of <span id="S4.SS2.p2.2.5" class="ltx_text ltx_font_bold">38.31%</span> and a mean final <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> of <span id="S4.SS2.p2.2.6" class="ltx_text ltx_font_bold">0.93</span> across all five tries.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Federated Learning Experiments</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We subsequently performed <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments by simulating the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> process. A detailed description of how this <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> simulator operates can be found in Appendix <a href="#A1" title="Appendix A Federated Learning Simulator ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. The <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments use the same hyperparameter configuration as the baseline experiments, except for the learning rates of the candidate generator, which had to be decreased by one order of magnitude to stabilize the training. A broad range of different numbers of clients in the dataset were selected in order to simulate the impact of varied local data distributions on the performance of the global model. Different client sub-sampling rates were employed to determine the optimal number of clients per communication round for the individual scale of the experiments, ensuring accurate client updates to be aggregated by the central server. With a range of 1k to 150k clients, the underlying datasets were randomly split into equal-sized local datasets for the clients and randomly distributed among them, assuring that they have approximately the same local data-generating distribution, especially in the low scale experiments. As the number of clients grows, the sizes of the local datasets shrink, which in turn reduces the likelihood of receiving an <a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> subset of the underlying dataset and gradually increases the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness of the client data. The 162k experiments split the underlying dataset using the user IDs provided by MovieLens, thereby ensuring that each <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client receives the samples from exactly one real-world user. As a result, each client’s local dataset has a unique data-generating distribution. Additionally, in the 162k setup, the local datasets are imbalanced, as the users of the MovieLens dataset have varying numbers of samples. Furthermore, with an increasing number of clients the local datasets become smaller, thus increasing the negative effects from noisy updates. This setup allows us to clearly identify the effects of small local datasets and non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness to be compared to our <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> method. The results of these experiments are shown in Figure <a href="#S4.F11.sf2" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a>.</p>
</div>
<figure id="S4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x12.png" id="S4.F11.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="122" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F11.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x13.png" id="S4.F11.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="245" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F11.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.5.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="S4.F11.6.2" class="ltx_ERROR undefined">\Ac</span><span id="S4.F11.7.3" class="ltx_text" style="font-size:90%;">fl experiment results. For increased legibility, instead of showing the complete training graphs, only <span id="S4.F11.7.3.1" class="ltx_text ltx_font_bold">(<a href="#S4.F11.sf1" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(a)</span></a>)</span> the final validation top-100 accuracy, and <span id="S4.F11.7.3.2" class="ltx_text ltx_font_bold">(<a href="#S4.F11.sf2" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a>)</span> the final validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> are shown. For reference, the full training graphs are depicted in Appendix <a href="#A4" title="Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S4.F11.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S4.F11.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F11.10" class="ltx_p ltx_figure_panel ltx_align_center">fl experiment results. For increased legibility, instead of showing the complete training graphs, only <span id="S4.F11.10.1" class="ltx_text ltx_font_bold">(<a href="#S4.F11.sf1" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(a)</span></a>)</span> the final validation top-100 accuracy, and <span id="S4.F11.10.2" class="ltx_text ltx_font_bold">(<a href="#S4.F11.sf2" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a>)</span> the final validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> are shown. For reference, the full training graphs are depicted in Appendix <a href="#A4" title="Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</div>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As can be seen in Figure <a href="#S4.F11.sf1" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(a)</span></a>, the candidate generator is strongly affected by non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and small local datasets, as even the setup with only 1k clients already performs much worse compared to the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline, and increasing the number of clients decreases the performance significantly. The ranker, which can be seen in Figure <a href="#S4.F11.sf2" title="In Figure 11 ‣ 4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a>, is not as much affected by non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and small local datasets. Additionally, the performance drop from increasing the number of clients is not as pronounced as with the candidate generator. Still, the performance is significantly lower than the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline. The ranker performs better than the candidate generator, since the watch behavior varies more between users than rating behavior, e.g., two users with different watch histories may still rate the same movie similarly. Since the rating data is much more homogeneous, the data-generating distributions of the users do not differ as much as in the case of the watch history data.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Reasonably, one might expect that, as the number of clients grows and the sizes of the local datasets shrink, the performance of the candidate generator should gradually decline. The experiments, however, reveal that the performance declines between 1k and 100k clients, before increasing again with 150k and 162k clients. We believe this can be explained by viewing the performance penalty incurred by <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> compared to centralized training as a compound error. One of the components of this error arises from the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness of the clients, as the different local data generating distributions cause the clients to have disparate local objectives. This leads to contradicting client updates that, when averaged by the central server, can cancel out some of the training progress of other clients and result in an update to the global model that does not minimize the global objective. Another component of the error is caused by noisy client updates: The smaller the local dataset of a client is, the worse its estimation of the empirical loss becomes, which results in a noisy gradient and unstable training. Even with a homogeneous client population, this can lead to contradicting client updates, which cause the global model to not properly converge. In the setups with 1k and 10k clients, each client has a large local dataset, which results in stable local training and good client updates. This means that the compound error causing the decrease in performance is dominated by the increase in non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness. As the number of clients increases, the sizes of the local datasets decrease, which in turn increases the heterogeneity of the clients. But this decrease in the size of the local datasets also causes the client updates to become noisier and the error induced by noisy client updates to become a significant component of the compound error. This increase in both error components causes the sharp decline in performance between the setups with 10k and 100k clients. Between the setups with 100k, 150k, and 162k clients, the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and noisy client update error components do not significantly change, because the increase in the number of clients is not as large and the sizes of the local datasets do not change as much. At this point, a new error component comes into play: With the decrease of the local dataset sizes also comes a decrease in the number of different MovieLens users represented in the local datasets. In the setup with 100k clients the number of different MovieLens users represented in the local datasets becomes small enough that the global model starts to be negatively impacted by the heterogeneity of the local dataset samples. The 1k and 10k setups do not suffer from this, as their local datasets have samples from so many MovieLens users that the effect averages out. In the 150k setup the number of MovieLens users decreases and in the 162k setup it is even guaranteed that each client only has samples from a single MovieLens user, thus gradually decreasing the negative impact of the heterogeneity of the local dataset samples. Again, the ranker model is not as affected by this, since the rating data is much more homogeneous than the watch history data, as described above.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>FedQ Experiments</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">As described in the previous section, the effects of non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness and small local datasets result in a significant decrease in performance. Therefore, we employed the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> technique, described in Section <a href="#S3.SS6" title="3.6. Federated Recommender Systems at Scale Using Queue-Based Federated Learning ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>. We fixed the client sub-sampling rate at 1,000 clients per communication round and used varying queue lengths. In order to stabilize the training, the learning rate applied to the candidate generator was once again lowered in comparison to the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline experiments. The experiments were also conducted using the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> simulator described in Appendix <a href="#A1" title="Appendix A Federated Learning Simulator ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, the results of which can be seen in Figure <a href="#S4.F12.sf2" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a>.</p>
</div>
<figure id="S4.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x14.png" id="S4.F12.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="122" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F12.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x15.png" id="S4.F12.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="245" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F12.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.5.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>. </span><span id="S4.F12.6.2" class="ltx_ERROR undefined">\Ac</span><span id="S4.F12.7.3" class="ltx_text" style="font-size:90%;">fedq experiment results for <span id="S4.F12.7.3.1" class="ltx_text ltx_font_bold">(<a href="#S4.F12.sf1" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a>)</span> the candidate generator and <span id="S4.F12.7.3.2" class="ltx_text ltx_font_bold">(<a href="#S4.F12.sf2" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a>)</span> the ranker. For increased legibility, instead of showing the complete training graphs, only the final validation top-100 accuracy, and the final validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> are shown. For reference, the full training graphs are depicted in Appendix <a href="#A4" title="Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S4.F12.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S4.F12.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F12.10" class="ltx_p ltx_figure_panel ltx_align_center">fedq experiment results for <span id="S4.F12.10.1" class="ltx_text ltx_font_bold">(<a href="#S4.F12.sf1" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a>)</span> the candidate generator and <span id="S4.F12.10.2" class="ltx_text ltx_font_bold">(<a href="#S4.F12.sf2" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a>)</span> the ranker. For increased legibility, instead of showing the complete training graphs, only the final validation top-100 accuracy, and the final validation accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> are shown. For reference, the full training graphs are depicted in Appendix <a href="#A4" title="Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</div>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">As shown in Figure <a href="#S4.F12.sf1" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a>, the candidate generator now performs much better as compared to standard <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> and in particular for the setups with 1k and 10k clients, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> even outperforms its baseline. The latter may be caused by a regularizing effect. In addition, the setups with large numbers of clients not only perform much better, but also in the expected way, as the performance slightly decreases with an increased number of clients. This provides evidence for our hypothesis that the candidate generator started to perform slightly better with increasing numbers of clients due to noisy updates induced by the decreased heterogeneity of the local dataset samples. Since the clients now train the global model sequentially, the number of samples adding to the local model update has drastically increased, thus resulting in much higher quality local updates. The ranker likewise shows a comparable improvement in performance and outperformed its non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline, though with a smaller margin than the candidate generator, as shown in Figure <a href="#S4.F12.sf2" title="In Figure 12 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a>. Tables <a href="#S4.T1" title="Table 1 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 ‣ 4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compile the results of both the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments and clearly show that <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> outperforms <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> in every single experiment.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.T1.2" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S4.T1.2.1" class="ltx_tr">
<td id="S4.T1.2.1.1" class="ltx_td ltx_border_r" style="padding-bottom:-2.0pt;"></td>
<td id="S4.T1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;"><span id="S4.T1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1k Clients</span></td>
<td id="S4.T1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;"><span id="S4.T1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10k Clients</span></td>
<td id="S4.T1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;"><span id="S4.T1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">100k Clients</span></td>
<td id="S4.T1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;"><span id="S4.T1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">150k Clients</span></td>
<td id="S4.T1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;"><span id="S4.T1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">162k Clients</span></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.2.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.2.5.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.2.6.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
</tr>
<tr id="S4.T1.2.3" class="ltx_tr">
<td id="S4.T1.2.3.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.3.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="S4.T1.2.3.1.2" class="ltx_text" style="font-size:90%;">Sub-Sampling</span>
</td>
<td id="S4.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5">
<span id="S4.T1.2.3.2.1" class="ltx_ERROR undefined">\libertineSB</span><a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>
</td>
</tr>
<tr id="S4.T1.2.4" class="ltx_tr">
<td id="S4.T1.2.4.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T1.2.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.2.1" class="ltx_text" style="font-size:90%;">27.93%</span></td>
<td id="S4.T1.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
</tr>
<tr id="S4.T1.2.5" class="ltx_tr">
<td id="S4.T1.2.5.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T1.2.5.1.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T1.2.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.1" class="ltx_text" style="font-size:90%;">28.29%</span></td>
<td id="S4.T1.2.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.3.1" class="ltx_text" style="font-size:90%;">17.98%</span></td>
<td id="S4.T1.2.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.4.1" class="ltx_text" style="font-size:90%;">5.18%</span></td>
<td id="S4.T1.2.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.5.1" class="ltx_text" style="font-size:90%;">7.83%</span></td>
<td id="S4.T1.2.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.6.1" class="ltx_text" style="font-size:90%;">10.14%</span></td>
</tr>
<tr id="S4.T1.2.6" class="ltx_tr">
<td id="S4.T1.2.6.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T1.2.6.1.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="S4.T1.2.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.2.1" class="ltx_text" style="font-size:90%;">28.11%</span></td>
<td id="S4.T1.2.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.1" class="ltx_text" style="font-size:90%;">17.86%</span></td>
<td id="S4.T1.2.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.4.1" class="ltx_text" style="font-size:90%;">6.34%</span></td>
<td id="S4.T1.2.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.5.1" class="ltx_text" style="font-size:90%;">8.53%</span></td>
<td id="S4.T1.2.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.6.1" class="ltx_text" style="font-size:90%;">10.52%</span></td>
</tr>
<tr id="S4.T1.2.7" class="ltx_tr">
<td id="S4.T1.2.7.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T1.2.7.1.1" class="ltx_text" style="font-size:90%;">10,000</span></td>
<td id="S4.T1.2.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.3.1" class="ltx_text" style="font-size:90%;">17.91%</span></td>
<td id="S4.T1.2.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.1" class="ltx_text" style="font-size:90%;">6.39%</span></td>
<td id="S4.T1.2.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.5.1" class="ltx_text" style="font-size:90%;">6.68%</span></td>
<td id="S4.T1.2.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.6.1" class="ltx_text" style="font-size:90%;">11.64%</span></td>
</tr>
<tr id="S4.T1.2.8" class="ltx_tr">
<td id="S4.T1.2.8.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.8.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="S4.T1.2.8.1.2" class="ltx_text" style="font-size:90%;">Queue Length</span>
</td>
<td id="S4.T1.2.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5">
<span id="S4.T1.2.8.2.1" class="ltx_ERROR undefined">\libertineSB</span><a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>
</td>
</tr>
<tr id="S4.T1.2.9" class="ltx_tr">
<td id="S4.T1.2.9.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.9.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T1.2.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.2.1" class="ltx_text" style="font-size:90%;">49.61%</span></td>
<td id="S4.T1.2.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.3.1" class="ltx_text" style="font-size:90%;">26.41%</span></td>
<td id="S4.T1.2.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.4.1" class="ltx_text" style="font-size:90%;">17.59%</span></td>
<td id="S4.T1.2.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.5.1" class="ltx_text" style="font-size:90%;">16.68%</span></td>
<td id="S4.T1.2.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.6.1" class="ltx_text" style="font-size:90%;">17.01%</span></td>
</tr>
<tr id="S4.T1.2.10" class="ltx_tr">
<td id="S4.T1.2.10.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T1.2.10.1.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T1.2.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.2.1" class="ltx_text" style="font-size:90%;">51.57%</span></td>
<td id="S4.T1.2.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.3.1" class="ltx_text" style="font-size:90%;">48.80%</span></td>
<td id="S4.T1.2.10.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.4.1" class="ltx_text" style="font-size:90%;">25.35%</span></td>
<td id="S4.T1.2.10.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.5.1" class="ltx_text" style="font-size:90%;">22.66%</span></td>
<td id="S4.T1.2.10.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.6.1" class="ltx_text" style="font-size:90%;">23.72%</span></td>
</tr>
<tr id="S4.T1.2.11" class="ltx_tr">
<td id="S4.T1.2.11.1" class="ltx_td ltx_align_right ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T1.2.11.1.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="S4.T1.2.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.2.11.2.1" class="ltx_text" style="font-size:90%;">52.12%</span></td>
<td id="S4.T1.2.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.2.11.3.1" class="ltx_text" style="font-size:90%;">51.47%</span></td>
<td id="S4.T1.2.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.2.11.4.1" class="ltx_text" style="font-size:90%;">42.34%</span></td>
<td id="S4.T1.2.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.2.11.5.1" class="ltx_text" style="font-size:90%;">39.22%</span></td>
<td id="S4.T1.2.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.2.11.6.1" class="ltx_text" style="font-size:90%;">38.67%</span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of the candidate generator <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiment results. The table reports the final validation top-100 accuracies after 300 communication rounds.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.T1.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T1.6" class="ltx_p ltx_figure_panel">Comparison of the candidate generator <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiment results. The table reports the final validation top-100 accuracies after 300 communication rounds.</p>
</div>
</div>
</figure>
<figure id="S4.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.T2.2" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S4.T2.2.1" class="ltx_tr">
<td id="S4.T2.2.1.1" class="ltx_td ltx_border_r" style="padding-bottom:-2.0pt;"></td>
<td id="S4.T2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;" colspan="2"><span id="S4.T2.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1k Clients</span></td>
<td id="S4.T2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;" colspan="2"><span id="S4.T2.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10k Clients</span></td>
<td id="S4.T2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;" colspan="2"><span id="S4.T2.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">100k Clients</span></td>
<td id="S4.T2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;" colspan="2"><span id="S4.T2.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">150k Clients</span></td>
<td id="S4.T2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:-2.0pt;" colspan="2">
<span id="S4.T2.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">162k Clients</span><span id="S4.T2.2.1.6.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:9.3pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a></td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a></td>
<td id="S4.T2.2.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.6.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_r"><a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a></td>
<td id="S4.T2.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.8.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T2.2.2.9" class="ltx_td ltx_align_center ltx_border_r"><a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a></td>
<td id="S4.T2.2.2.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.10.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T2.2.2.11" class="ltx_td ltx_align_center ltx_border_r"><a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a></td>
</tr>
<tr id="S4.T2.2.3" class="ltx_tr">
<td id="S4.T2.2.3.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.2.3.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="S4.T2.2.3.1.2" class="ltx_text" style="font-size:90%;">Sub-Sampling</span>
</td>
<td id="S4.T2.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="10">
<span id="S4.T2.2.3.2.1" class="ltx_ERROR undefined">\libertineSB</span><a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>
</td>
</tr>
<tr id="S4.T2.2.4" class="ltx_tr">
<td id="S4.T2.2.4.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T2.2.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.4.2.1" class="ltx_text" style="font-size:90%;">27.78%</span></td>
<td id="S4.T2.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.3.1" class="ltx_text" style="font-size:90%;">1.22</span></td>
<td id="S4.T2.2.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.4.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.4.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.7.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.4.8.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.4.10.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.11.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
</tr>
<tr id="S4.T2.2.5" class="ltx_tr">
<td id="S4.T2.2.5.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T2.2.5.1.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T2.2.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.2.1" class="ltx_text" style="font-size:90%;">27.91%</span></td>
<td id="S4.T2.2.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.3.1" class="ltx_text" style="font-size:90%;">1.22</span></td>
<td id="S4.T2.2.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.4.1" class="ltx_text" style="font-size:90%;">26.32%</span></td>
<td id="S4.T2.2.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.5.1" class="ltx_text" style="font-size:90%;">1.32</span></td>
<td id="S4.T2.2.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.6.1" class="ltx_text" style="font-size:90%;">24.27%</span></td>
<td id="S4.T2.2.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.7.1" class="ltx_text" style="font-size:90%;">1.42</span></td>
<td id="S4.T2.2.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.8.1" class="ltx_text" style="font-size:90%;">24.80%</span></td>
<td id="S4.T2.2.5.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.9.1" class="ltx_text" style="font-size:90%;">1.42</span></td>
<td id="S4.T2.2.5.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.10.1" class="ltx_text" style="font-size:90%;">23.14%</span></td>
<td id="S4.T2.2.5.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.11.1" class="ltx_text" style="font-size:90%;">1.33</span></td>
</tr>
<tr id="S4.T2.2.6" class="ltx_tr">
<td id="S4.T2.2.6.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T2.2.6.1.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="S4.T2.2.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.2.1" class="ltx_text" style="font-size:90%;">27.94%</span></td>
<td id="S4.T2.2.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.1" class="ltx_text" style="font-size:90%;">1.21</span></td>
<td id="S4.T2.2.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.4.1" class="ltx_text" style="font-size:90%;">26.47%</span></td>
<td id="S4.T2.2.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.5.1" class="ltx_text" style="font-size:90%;">1.33</span></td>
<td id="S4.T2.2.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.1" class="ltx_text" style="font-size:90%;">24.47%</span></td>
<td id="S4.T2.2.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.7.1" class="ltx_text" style="font-size:90%;">1.42</span></td>
<td id="S4.T2.2.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.8.1" class="ltx_text" style="font-size:90%;">24.43%</span></td>
<td id="S4.T2.2.6.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.9.1" class="ltx_text" style="font-size:90%;">1.43</span></td>
<td id="S4.T2.2.6.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.10.1" class="ltx_text" style="font-size:90%;">23.51%</span></td>
<td id="S4.T2.2.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.11.1" class="ltx_text" style="font-size:90%;">1.34</span></td>
</tr>
<tr id="S4.T2.2.7" class="ltx_tr">
<td id="S4.T2.2.7.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T2.2.7.1.1" class="ltx_text" style="font-size:90%;">10,000</span></td>
<td id="S4.T2.2.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.4.1" class="ltx_text" style="font-size:90%;">26.36%</span></td>
<td id="S4.T2.2.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.5.1" class="ltx_text" style="font-size:90%;">1.33</span></td>
<td id="S4.T2.2.7.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.6.1" class="ltx_text" style="font-size:90%;">24.74%</span></td>
<td id="S4.T2.2.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.7.1" class="ltx_text" style="font-size:90%;">1.42</span></td>
<td id="S4.T2.2.7.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.8.1" class="ltx_text" style="font-size:90%;">24.61%</span></td>
<td id="S4.T2.2.7.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.9.1" class="ltx_text" style="font-size:90%;">1.42</span></td>
<td id="S4.T2.2.7.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.10.1" class="ltx_text" style="font-size:90%;">22.77%</span></td>
<td id="S4.T2.2.7.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.11.1" class="ltx_text" style="font-size:90%;">1.36</span></td>
</tr>
<tr id="S4.T2.2.8" class="ltx_tr">
<td id="S4.T2.2.8.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.2.8.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="S4.T2.2.8.1.2" class="ltx_text" style="font-size:90%;">Queue Length</span>
</td>
<td id="S4.T2.2.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="10">
<span id="S4.T2.2.8.2.1" class="ltx_ERROR undefined">\libertineSB</span><a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>
</td>
</tr>
<tr id="S4.T2.2.9" class="ltx_tr">
<td id="S4.T2.2.9.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.9.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T2.2.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.2.1" class="ltx_text" style="font-size:90%;">30.38%</span></td>
<td id="S4.T2.2.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.9.3.1" class="ltx_text" style="font-size:90%;">1.09</span></td>
<td id="S4.T2.2.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.4.1" class="ltx_text" style="font-size:90%;">27.85%</span></td>
<td id="S4.T2.2.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.9.5.1" class="ltx_text" style="font-size:90%;">1.22</span></td>
<td id="S4.T2.2.9.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.6.1" class="ltx_text" style="font-size:90%;">26.10%</span></td>
<td id="S4.T2.2.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.9.7.1" class="ltx_text" style="font-size:90%;">1.34</span></td>
<td id="S4.T2.2.9.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.8.1" class="ltx_text" style="font-size:90%;">26.10%</span></td>
<td id="S4.T2.2.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.9.9.1" class="ltx_text" style="font-size:90%;">1.34</span></td>
<td id="S4.T2.2.9.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.10.1" class="ltx_text" style="font-size:90%;">25.56%</span></td>
<td id="S4.T2.2.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.9.11.1" class="ltx_text" style="font-size:90%;">1.34</span></td>
</tr>
<tr id="S4.T2.2.10" class="ltx_tr">
<td id="S4.T2.2.10.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r"><span id="S4.T2.2.10.1.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T2.2.10.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.10.2.1" class="ltx_text" style="font-size:90%;">39.45%</span></td>
<td id="S4.T2.2.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.3.1" class="ltx_text" style="font-size:90%;">0.88</span></td>
<td id="S4.T2.2.10.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.10.4.1" class="ltx_text" style="font-size:90%;">30.37%</span></td>
<td id="S4.T2.2.10.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.5.1" class="ltx_text" style="font-size:90%;">1.1</span></td>
<td id="S4.T2.2.10.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.10.6.1" class="ltx_text" style="font-size:90%;">27.43%</span></td>
<td id="S4.T2.2.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.1" class="ltx_text" style="font-size:90%;">1.26</span></td>
<td id="S4.T2.2.10.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.10.8.1" class="ltx_text" style="font-size:90%;">27.33%</span></td>
<td id="S4.T2.2.10.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.9.1" class="ltx_text" style="font-size:90%;">1.26</span></td>
<td id="S4.T2.2.10.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.10.10.1" class="ltx_text" style="font-size:90%;">27.15%</span></td>
<td id="S4.T2.2.10.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.11.1" class="ltx_text" style="font-size:90%;">1.28</span></td>
</tr>
<tr id="S4.T2.2.11" class="ltx_tr">
<td id="S4.T2.2.11.1" class="ltx_td ltx_align_right ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T2.2.11.1.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="S4.T2.2.11.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.11.2.1" class="ltx_text" style="font-size:90%;">40.14%</span></td>
<td id="S4.T2.2.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.11.3.1" class="ltx_text" style="font-size:90%;">0.83</span></td>
<td id="S4.T2.2.11.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.11.4.1" class="ltx_text" style="font-size:90%;">39.07%</span></td>
<td id="S4.T2.2.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.11.5.1" class="ltx_text" style="font-size:90%;">0.91</span></td>
<td id="S4.T2.2.11.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.11.6.1" class="ltx_text" style="font-size:90%;">30.10%</span></td>
<td id="S4.T2.2.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.11.7.1" class="ltx_text" style="font-size:90%;">1.12</span></td>
<td id="S4.T2.2.11.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.11.8.1" class="ltx_text" style="font-size:90%;">29.98%</span></td>
<td id="S4.T2.2.11.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.11.9.1" class="ltx_text" style="font-size:90%;">1.12</span></td>
<td id="S4.T2.2.11.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.11.10.1" class="ltx_text" style="font-size:90%;">29.69%</span></td>
<td id="S4.T2.2.11.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.11.11.1" class="ltx_text" style="font-size:90%;">1.13</span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of the ranker <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiment results. The table reports the final validation accuracies and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">MSEs</span></abbr></a> after 300 communication rounds.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.T2.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T2.6" class="ltx_p ltx_figure_panel">Comparison of the ranker <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiment results. The table reports the final validation accuracies and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">MSEs</span></abbr></a> after 300 communication rounds.</p>
</div>
</div>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">To further investigate the efficacy of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, we have evaluated it using the LEAF benchmark <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite>, which is a benchmark for testing <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> algorithms. The LEAF benchmark includes multiple different datasets that can naturally be partitioned into local datasets for <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients, as well as accompanying <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> models and metrics. We have benchmarked <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> on four of LEAF’s datasets and <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architectures, which range from image classification using <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">CNNs</span></abbr></a>, to text classification and next word prediction using <a href="#id27.27.id27"><span href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">long short-term memory networks</span></span></a>. The results of these experiments and a detailed evaluation can be found in Appendix <a href="#A5" title="Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Communication Compression Experiments</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.4" class="ltx_p">As described in the Section <a href="#S3.SS7" title="3.7. Achieving Communication Efficiency ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7</span></a>, <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> has, due to the continuous exchange of local updates between clients and central server, a significant communication overhead. We employed the recent <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> standard to compress the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> parametrizations communicated between the clients and the central server. The coding engine uses parameter quantization as a lossy preprocessing step and <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="Deep Context-Adaptive Binary Arithmetic Coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DeepCABAC</span></abbr></a> as arithmetic coder. The quantization of the parameters requires a hyperparameter called  <a href="#id37.37.id37"><span href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">quantization parameter</span></span></a> (<a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a>), which controls the step size <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\delta</annotation></semantics></math> between quantization points and thus the rate-performance trade-off. A lower <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> results in a smaller step size and therefore in more quantization points and lower compression performance, while a higher <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> results in a larger step size and therefore in less quantization points and higher compression performance. To compute <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><mi id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">\delta</annotation></semantics></math> as demonstrated by Algorithm <a href="#algorithm3" title="In 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it is necessary to provide an additional parameter <math id="S4.SS5.p1.3.m3.1" class="ltx_Math" alttext="f_{QP}" display="inline"><semantics id="S4.SS5.p1.3.m3.1a"><msub id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mi id="S4.SS5.p1.3.m3.1.1.2" xref="S4.SS5.p1.3.m3.1.1.2.cmml">f</mi><mrow id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3.cmml"><mi id="S4.SS5.p1.3.m3.1.1.3.2" xref="S4.SS5.p1.3.m3.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.3.m3.1.1.3.1" xref="S4.SS5.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p1.3.m3.1.1.3.3" xref="S4.SS5.p1.3.m3.1.1.3.3.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p1.3.m3.1.1.2.cmml" xref="S4.SS5.p1.3.m3.1.1.2">𝑓</ci><apply id="S4.SS5.p1.3.m3.1.1.3.cmml" xref="S4.SS5.p1.3.m3.1.1.3"><times id="S4.SS5.p1.3.m3.1.1.3.1.cmml" xref="S4.SS5.p1.3.m3.1.1.3.1"></times><ci id="S4.SS5.p1.3.m3.1.1.3.2.cmml" xref="S4.SS5.p1.3.m3.1.1.3.2">𝑄</ci><ci id="S4.SS5.p1.3.m3.1.1.3.3.cmml" xref="S4.SS5.p1.3.m3.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">f_{QP}</annotation></semantics></math>, which incorporates the dependency between <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a> and the quantization step sizes. Lower values of <math id="S4.SS5.p1.4.m4.1" class="ltx_Math" alttext="f_{QP}" display="inline"><semantics id="S4.SS5.p1.4.m4.1a"><msub id="S4.SS5.p1.4.m4.1.1" xref="S4.SS5.p1.4.m4.1.1.cmml"><mi id="S4.SS5.p1.4.m4.1.1.2" xref="S4.SS5.p1.4.m4.1.1.2.cmml">f</mi><mrow id="S4.SS5.p1.4.m4.1.1.3" xref="S4.SS5.p1.4.m4.1.1.3.cmml"><mi id="S4.SS5.p1.4.m4.1.1.3.2" xref="S4.SS5.p1.4.m4.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.4.m4.1.1.3.1" xref="S4.SS5.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p1.4.m4.1.1.3.3" xref="S4.SS5.p1.4.m4.1.1.3.3.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><apply id="S4.SS5.p1.4.m4.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.4.m4.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS5.p1.4.m4.1.1.2.cmml" xref="S4.SS5.p1.4.m4.1.1.2">𝑓</ci><apply id="S4.SS5.p1.4.m4.1.1.3.cmml" xref="S4.SS5.p1.4.m4.1.1.3"><times id="S4.SS5.p1.4.m4.1.1.3.1.cmml" xref="S4.SS5.p1.4.m4.1.1.3.1"></times><ci id="S4.SS5.p1.4.m4.1.1.3.2.cmml" xref="S4.SS5.p1.4.m4.1.1.3.2">𝑄</ci><ci id="S4.SS5.p1.4.m4.1.1.3.3.cmml" xref="S4.SS5.p1.4.m4.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">f_{QP}</annotation></semantics></math> result in larger neighboring quantization step sizes<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/fraunhoferhhi/nncodec/wiki/usage" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fraunhoferhhi/nncodec/wiki/usage</a>.</span></span></span>.</p>
</div>
<figure id="algorithm3" class="ltx_float ltx_algorithm">
<div id="algorithm3.7" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm3.7.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm3.7.8.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span><span id="algorithm3.7.8.2" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="algorithm3.7.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm3.7.9.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span><span id="algorithm3.7.9.2" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="algorithm3.2.2" class="ltx_listingline">
<span id="algorithm3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input :</span><span id="algorithm3.2.2.2" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm3.1.1.m1.1" class="ltx_Math" alttext="QP" display="inline"><semantics id="algorithm3.1.1.m1.1a"><mrow id="algorithm3.1.1.m1.1.1" xref="algorithm3.1.1.m1.1.1.cmml"><mi mathsize="90%" id="algorithm3.1.1.m1.1.1.2" xref="algorithm3.1.1.m1.1.1.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.1.1.m1.1.1.1" xref="algorithm3.1.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.1.1.m1.1.1.3" xref="algorithm3.1.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm3.1.1.m1.1b"><apply id="algorithm3.1.1.m1.1.1.cmml" xref="algorithm3.1.1.m1.1.1"><times id="algorithm3.1.1.m1.1.1.1.cmml" xref="algorithm3.1.1.m1.1.1.1"></times><ci id="algorithm3.1.1.m1.1.1.2.cmml" xref="algorithm3.1.1.m1.1.1.2">𝑄</ci><ci id="algorithm3.1.1.m1.1.1.3.cmml" xref="algorithm3.1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.1.1.m1.1c">QP</annotation></semantics></math><span id="algorithm3.2.2.3" class="ltx_text" style="font-size:90%;"> is the quantization parameter and </span><math id="algorithm3.2.2.m2.1" class="ltx_Math" alttext="f_{QP}" display="inline"><semantics id="algorithm3.2.2.m2.1a"><msub id="algorithm3.2.2.m2.1.1" xref="algorithm3.2.2.m2.1.1.cmml"><mi mathsize="90%" id="algorithm3.2.2.m2.1.1.2" xref="algorithm3.2.2.m2.1.1.2.cmml">f</mi><mrow id="algorithm3.2.2.m2.1.1.3" xref="algorithm3.2.2.m2.1.1.3.cmml"><mi mathsize="90%" id="algorithm3.2.2.m2.1.1.3.2" xref="algorithm3.2.2.m2.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.2.2.m2.1.1.3.1" xref="algorithm3.2.2.m2.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.2.2.m2.1.1.3.3" xref="algorithm3.2.2.m2.1.1.3.3.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm3.2.2.m2.1b"><apply id="algorithm3.2.2.m2.1.1.cmml" xref="algorithm3.2.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm3.2.2.m2.1.1.1.cmml" xref="algorithm3.2.2.m2.1.1">subscript</csymbol><ci id="algorithm3.2.2.m2.1.1.2.cmml" xref="algorithm3.2.2.m2.1.1.2">𝑓</ci><apply id="algorithm3.2.2.m2.1.1.3.cmml" xref="algorithm3.2.2.m2.1.1.3"><times id="algorithm3.2.2.m2.1.1.3.1.cmml" xref="algorithm3.2.2.m2.1.1.3.1"></times><ci id="algorithm3.2.2.m2.1.1.3.2.cmml" xref="algorithm3.2.2.m2.1.1.3.2">𝑄</ci><ci id="algorithm3.2.2.m2.1.1.3.3.cmml" xref="algorithm3.2.2.m2.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.2.2.m2.1c">f_{QP}</annotation></semantics></math><span id="algorithm3.2.2.4" class="ltx_text" style="font-size:90%;"> is the regulating parameter for mapping between </span><a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a><span id="algorithm3.2.2.5" class="ltx_text" style="font-size:90%;"> and quantization step sizes</span>
</div>
<div id="algorithm3.3.3" class="ltx_listingline">
<span id="algorithm3.3.3.1" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm3.3.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Output :</span><span id="algorithm3.3.3.3" class="ltx_text" style="font-size:90%;"> Quantization step size </span><math id="algorithm3.3.3.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="algorithm3.3.3.m1.1a"><mi mathsize="90%" id="algorithm3.3.3.m1.1.1" xref="algorithm3.3.3.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="algorithm3.3.3.m1.1b"><ci id="algorithm3.3.3.m1.1.1.cmml" xref="algorithm3.3.3.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.3.3.m1.1c">\delta</annotation></semantics></math>
</div>
<div id="algorithm3.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm3.4.4.1.1.1" class="ltx_text" style="font-size:90%;">3</span></span><span id="algorithm3.4.4.2" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm3.4.4.m1.2" class="ltx_Math" alttext="m\leftarrow\left(1&lt;&lt;f_{QP}\right)+\left(QP+\left(\left(1&lt;&lt;f_{QP}\right)-1\right)\right)" display="inline"><semantics id="algorithm3.4.4.m1.2a"><mrow id="algorithm3.4.4.m1.2.2" xref="algorithm3.4.4.m1.2.2.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.4" xref="algorithm3.4.4.m1.2.2.4.cmml">m</mi><mo mathsize="90%" stretchy="false" id="algorithm3.4.4.m1.2.2.3" xref="algorithm3.4.4.m1.2.2.3.cmml">←</mo><mrow id="algorithm3.4.4.m1.2.2.2" xref="algorithm3.4.4.m1.2.2.2.cmml"><mrow id="algorithm3.4.4.m1.1.1.1.1.1" xref="algorithm3.4.4.m1.1.1.1.1.1.1.cmml"><mo id="algorithm3.4.4.m1.1.1.1.1.1.2" xref="algorithm3.4.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="algorithm3.4.4.m1.1.1.1.1.1.1" xref="algorithm3.4.4.m1.1.1.1.1.1.1.cmml"><mn mathsize="90%" id="algorithm3.4.4.m1.1.1.1.1.1.1.2" xref="algorithm3.4.4.m1.1.1.1.1.1.1.2.cmml">1</mn><mo mathsize="90%" id="algorithm3.4.4.m1.1.1.1.1.1.1.1" xref="algorithm3.4.4.m1.1.1.1.1.1.1.1.cmml">&lt;&lt;</mo><msub id="algorithm3.4.4.m1.1.1.1.1.1.1.3" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.1.1.1.1.1.1.3.2" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.2.cmml">f</mi><mrow id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.2" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.1" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.3" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.3.cmml">P</mi></mrow></msub></mrow><mo id="algorithm3.4.4.m1.1.1.1.1.1.3" xref="algorithm3.4.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo mathsize="90%" id="algorithm3.4.4.m1.2.2.2.3" xref="algorithm3.4.4.m1.2.2.2.3.cmml">+</mo><mrow id="algorithm3.4.4.m1.2.2.2.2.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.cmml"><mo id="algorithm3.4.4.m1.2.2.2.2.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.cmml"><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.3.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.4.4.m1.2.2.2.2.1.1.3.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.3.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.3.cmml">P</mi></mrow><mo mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.2.cmml">+</mo><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.cmml"><mo id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.cmml"><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mn mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml">&lt;&lt;</mo><msub id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.2.cmml">f</mi><mrow id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.cmml"><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.1" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.3.cmml">P</mi></mrow></msub></mrow><mo id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.2" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.2.cmml">−</mo><mn mathsize="90%" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="algorithm3.4.4.m1.2.2.2.2.1.3" xref="algorithm3.4.4.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm3.4.4.m1.2b"><apply id="algorithm3.4.4.m1.2.2.cmml" xref="algorithm3.4.4.m1.2.2"><ci id="algorithm3.4.4.m1.2.2.3.cmml" xref="algorithm3.4.4.m1.2.2.3">←</ci><ci id="algorithm3.4.4.m1.2.2.4.cmml" xref="algorithm3.4.4.m1.2.2.4">𝑚</ci><apply id="algorithm3.4.4.m1.2.2.2.cmml" xref="algorithm3.4.4.m1.2.2.2"><plus id="algorithm3.4.4.m1.2.2.2.3.cmml" xref="algorithm3.4.4.m1.2.2.2.3"></plus><apply id="algorithm3.4.4.m1.1.1.1.1.1.1.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1"><csymbol cd="latexml" id="algorithm3.4.4.m1.1.1.1.1.1.1.1.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.1">much-less-than</csymbol><cn type="integer" id="algorithm3.4.4.m1.1.1.1.1.1.1.2.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.2">1</cn><apply id="algorithm3.4.4.m1.1.1.1.1.1.1.3.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="algorithm3.4.4.m1.1.1.1.1.1.1.3.1.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="algorithm3.4.4.m1.1.1.1.1.1.1.3.2.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.2">𝑓</ci><apply id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3"><times id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.1.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.1"></times><ci id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.2.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.2">𝑄</ci><ci id="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.3.cmml" xref="algorithm3.4.4.m1.1.1.1.1.1.1.3.3.3">𝑃</ci></apply></apply></apply><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1"><plus id="algorithm3.4.4.m1.2.2.2.2.1.1.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.2"></plus><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3"><times id="algorithm3.4.4.m1.2.2.2.2.1.1.3.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.1"></times><ci id="algorithm3.4.4.m1.2.2.2.2.1.1.3.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.2">𝑄</ci><ci id="algorithm3.4.4.m1.2.2.2.2.1.1.3.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.3.3">𝑃</ci></apply><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1"><minus id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.2"></minus><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.1">much-less-than</csymbol><cn type="integer" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.2">1</cn><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.2">𝑓</ci><apply id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3"><times id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.2">𝑄</ci><ci id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.1.1.1.3.3.3">𝑃</ci></apply></apply></apply><cn type="integer" id="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="algorithm3.4.4.m1.2.2.2.2.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.4.4.m1.2c">m\leftarrow\left(1&lt;&lt;f_{QP}\right)+\left(QP+\left(\left(1&lt;&lt;f_{QP}\right)-1\right)\right)</annotation></semantics></math><span id="algorithm3.4.4.3" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm3.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm3.5.5.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span><span id="algorithm3.5.5.2" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm3.5.5.m1.1" class="ltx_Math" alttext="s\leftarrow QP&gt;&gt;f_{QP}" display="inline"><semantics id="algorithm3.5.5.m1.1a"><mrow id="algorithm3.5.5.m1.1.1" xref="algorithm3.5.5.m1.1.1.cmml"><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.2" xref="algorithm3.5.5.m1.1.1.2.cmml">s</mi><mo mathsize="90%" stretchy="false" id="algorithm3.5.5.m1.1.1.3" xref="algorithm3.5.5.m1.1.1.3.cmml">←</mo><mrow id="algorithm3.5.5.m1.1.1.4" xref="algorithm3.5.5.m1.1.1.4.cmml"><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.4.2" xref="algorithm3.5.5.m1.1.1.4.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.5.5.m1.1.1.4.1" xref="algorithm3.5.5.m1.1.1.4.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.4.3" xref="algorithm3.5.5.m1.1.1.4.3.cmml">P</mi></mrow><mo lspace="0.278em" mathsize="90%" rspace="0.278em" id="algorithm3.5.5.m1.1.1.5" xref="algorithm3.5.5.m1.1.1.5.cmml">&gt;&gt;</mo><msub id="algorithm3.5.5.m1.1.1.6" xref="algorithm3.5.5.m1.1.1.6.cmml"><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.6.2" xref="algorithm3.5.5.m1.1.1.6.2.cmml">f</mi><mrow id="algorithm3.5.5.m1.1.1.6.3" xref="algorithm3.5.5.m1.1.1.6.3.cmml"><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.6.3.2" xref="algorithm3.5.5.m1.1.1.6.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.5.5.m1.1.1.6.3.1" xref="algorithm3.5.5.m1.1.1.6.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.5.5.m1.1.1.6.3.3" xref="algorithm3.5.5.m1.1.1.6.3.3.cmml">P</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm3.5.5.m1.1b"><apply id="algorithm3.5.5.m1.1.1.cmml" xref="algorithm3.5.5.m1.1.1"><and id="algorithm3.5.5.m1.1.1a.cmml" xref="algorithm3.5.5.m1.1.1"></and><apply id="algorithm3.5.5.m1.1.1b.cmml" xref="algorithm3.5.5.m1.1.1"><ci id="algorithm3.5.5.m1.1.1.3.cmml" xref="algorithm3.5.5.m1.1.1.3">←</ci><ci id="algorithm3.5.5.m1.1.1.2.cmml" xref="algorithm3.5.5.m1.1.1.2">𝑠</ci><apply id="algorithm3.5.5.m1.1.1.4.cmml" xref="algorithm3.5.5.m1.1.1.4"><times id="algorithm3.5.5.m1.1.1.4.1.cmml" xref="algorithm3.5.5.m1.1.1.4.1"></times><ci id="algorithm3.5.5.m1.1.1.4.2.cmml" xref="algorithm3.5.5.m1.1.1.4.2">𝑄</ci><ci id="algorithm3.5.5.m1.1.1.4.3.cmml" xref="algorithm3.5.5.m1.1.1.4.3">𝑃</ci></apply></apply><apply id="algorithm3.5.5.m1.1.1c.cmml" xref="algorithm3.5.5.m1.1.1"><csymbol cd="latexml" id="algorithm3.5.5.m1.1.1.5.cmml" xref="algorithm3.5.5.m1.1.1.5">much-greater-than</csymbol><share href="#algorithm3.5.5.m1.1.1.4.cmml" id="algorithm3.5.5.m1.1.1d.cmml" xref="algorithm3.5.5.m1.1.1"></share><apply id="algorithm3.5.5.m1.1.1.6.cmml" xref="algorithm3.5.5.m1.1.1.6"><csymbol cd="ambiguous" id="algorithm3.5.5.m1.1.1.6.1.cmml" xref="algorithm3.5.5.m1.1.1.6">subscript</csymbol><ci id="algorithm3.5.5.m1.1.1.6.2.cmml" xref="algorithm3.5.5.m1.1.1.6.2">𝑓</ci><apply id="algorithm3.5.5.m1.1.1.6.3.cmml" xref="algorithm3.5.5.m1.1.1.6.3"><times id="algorithm3.5.5.m1.1.1.6.3.1.cmml" xref="algorithm3.5.5.m1.1.1.6.3.1"></times><ci id="algorithm3.5.5.m1.1.1.6.3.2.cmml" xref="algorithm3.5.5.m1.1.1.6.3.2">𝑄</ci><ci id="algorithm3.5.5.m1.1.1.6.3.3.cmml" xref="algorithm3.5.5.m1.1.1.6.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.5.5.m1.1c">s\leftarrow QP&gt;&gt;f_{QP}</annotation></semantics></math><span id="algorithm3.5.5.3" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="algorithm3.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm3.6.6.1.1.1" class="ltx_text" style="font-size:90%;">5</span></span><span id="algorithm3.6.6.2" class="ltx_text" style="font-size:90%;">
</span><math id="algorithm3.6.6.m1.1" class="ltx_Math" alttext="\delta\leftarrow m\cdot 2.0^{s-f_{QP}}" display="inline"><semantics id="algorithm3.6.6.m1.1a"><mrow id="algorithm3.6.6.m1.1.1" xref="algorithm3.6.6.m1.1.1.cmml"><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.2" xref="algorithm3.6.6.m1.1.1.2.cmml">δ</mi><mo mathsize="90%" stretchy="false" id="algorithm3.6.6.m1.1.1.1" xref="algorithm3.6.6.m1.1.1.1.cmml">←</mo><mrow id="algorithm3.6.6.m1.1.1.3" xref="algorithm3.6.6.m1.1.1.3.cmml"><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.3.2" xref="algorithm3.6.6.m1.1.1.3.2.cmml">m</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="algorithm3.6.6.m1.1.1.3.1" xref="algorithm3.6.6.m1.1.1.3.1.cmml">⋅</mo><msup id="algorithm3.6.6.m1.1.1.3.3" xref="algorithm3.6.6.m1.1.1.3.3.cmml"><mn mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.2" xref="algorithm3.6.6.m1.1.1.3.3.2.cmml">2.0</mn><mrow id="algorithm3.6.6.m1.1.1.3.3.3" xref="algorithm3.6.6.m1.1.1.3.3.3.cmml"><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.3.2" xref="algorithm3.6.6.m1.1.1.3.3.3.2.cmml">s</mi><mo mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.3.1" xref="algorithm3.6.6.m1.1.1.3.3.3.1.cmml">−</mo><msub id="algorithm3.6.6.m1.1.1.3.3.3.3" xref="algorithm3.6.6.m1.1.1.3.3.3.3.cmml"><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.3.3.2" xref="algorithm3.6.6.m1.1.1.3.3.3.3.2.cmml">f</mi><mrow id="algorithm3.6.6.m1.1.1.3.3.3.3.3" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.cmml"><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.3.3.3.2" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="algorithm3.6.6.m1.1.1.3.3.3.3.3.1" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="algorithm3.6.6.m1.1.1.3.3.3.3.3.3" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.3.cmml">P</mi></mrow></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm3.6.6.m1.1b"><apply id="algorithm3.6.6.m1.1.1.cmml" xref="algorithm3.6.6.m1.1.1"><ci id="algorithm3.6.6.m1.1.1.1.cmml" xref="algorithm3.6.6.m1.1.1.1">←</ci><ci id="algorithm3.6.6.m1.1.1.2.cmml" xref="algorithm3.6.6.m1.1.1.2">𝛿</ci><apply id="algorithm3.6.6.m1.1.1.3.cmml" xref="algorithm3.6.6.m1.1.1.3"><ci id="algorithm3.6.6.m1.1.1.3.1.cmml" xref="algorithm3.6.6.m1.1.1.3.1">⋅</ci><ci id="algorithm3.6.6.m1.1.1.3.2.cmml" xref="algorithm3.6.6.m1.1.1.3.2">𝑚</ci><apply id="algorithm3.6.6.m1.1.1.3.3.cmml" xref="algorithm3.6.6.m1.1.1.3.3"><csymbol cd="ambiguous" id="algorithm3.6.6.m1.1.1.3.3.1.cmml" xref="algorithm3.6.6.m1.1.1.3.3">superscript</csymbol><cn type="float" id="algorithm3.6.6.m1.1.1.3.3.2.cmml" xref="algorithm3.6.6.m1.1.1.3.3.2">2.0</cn><apply id="algorithm3.6.6.m1.1.1.3.3.3.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3"><minus id="algorithm3.6.6.m1.1.1.3.3.3.1.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.1"></minus><ci id="algorithm3.6.6.m1.1.1.3.3.3.2.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.2">𝑠</ci><apply id="algorithm3.6.6.m1.1.1.3.3.3.3.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="algorithm3.6.6.m1.1.1.3.3.3.3.1.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3">subscript</csymbol><ci id="algorithm3.6.6.m1.1.1.3.3.3.3.2.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3.2">𝑓</ci><apply id="algorithm3.6.6.m1.1.1.3.3.3.3.3.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3"><times id="algorithm3.6.6.m1.1.1.3.3.3.3.3.1.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.1"></times><ci id="algorithm3.6.6.m1.1.1.3.3.3.3.3.2.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.2">𝑄</ci><ci id="algorithm3.6.6.m1.1.1.3.3.3.3.3.3.cmml" xref="algorithm3.6.6.m1.1.1.3.3.3.3.3.3">𝑃</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.6.6.m1.1c">\delta\leftarrow m\cdot 2.0^{s-f_{QP}}</annotation></semantics></math>
</div>
<div id="algorithm3.7.7" class="ltx_listingline">
<span id="algorithm3.7.7.1" class="ltx_text" style="font-size:90%;">
</span><span id="algorithm3.7.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span id="algorithm3.7.7.3" class="ltx_text" style="font-size:90%;"> </span><math id="algorithm3.7.7.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="algorithm3.7.7.m1.1a"><mi mathsize="90%" id="algorithm3.7.7.m1.1.1" xref="algorithm3.7.7.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="algorithm3.7.7.m1.1b"><ci id="algorithm3.7.7.m1.1.1.cmml" xref="algorithm3.7.7.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm3.7.7.m1.1c">\delta</annotation></semantics></math><span id="algorithm3.7.7.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="algorithm3.11.1.1" class="ltx_text ltx_font_bold">Algorithm 3</span> </span>Quantization step size</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Besides influencing the compression performance, the <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> also impacts the performance of the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model after decompression, i.e., if the <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> was chosen too large, the resulting performance is significantly decreased. In order to determine the optimal value of the <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> for the candidate generator and the ranker models, we performed an experiment, testing <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> values between -48 and 0. The results are shown in Figure <a href="#S4.F13.sf2" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>.</p>
</div>
<figure id="S4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x16.png" id="S4.F13.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="181" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F13.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x17.png" id="S4.F13.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="181" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F13.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.4.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>. </span><span id="S4.F13.5.2" class="ltx_text" style="font-size:90%;">Compression vs. accuracy experiment results for <span id="S4.F13.5.2.1" class="ltx_text ltx_font_bold">(<a href="#S4.F13.sf1" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(a)</span></a>)</span> the candidate generator and <span id="S4.F13.5.2.2" class="ltx_text ltx_font_bold">(<a href="#S4.F13.sf2" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>)</span> the ranker.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F13.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F13.7" class="ltx_p ltx_figure_panel ltx_align_center">Compression vs. accuracy experiment results for <span id="S4.F13.7.1" class="ltx_text ltx_font_bold">(<a href="#S4.F13.sf1" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(a)</span></a>)</span> the candidate generator and <span id="S4.F13.7.2" class="ltx_text ltx_font_bold">(<a href="#S4.F13.sf2" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>)</span> the ranker.</p>
</div>
</div>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">For <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> value should be chosen in a way to optimize the rate-distortion trade-off. As can be seen in Figure <a href="#S4.F13.sf2" title="In Figure 13 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>, a <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> range of <span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">-38</span> and <span id="S4.SS5.p3.1.2" class="ltx_text ltx_font_bold">-30</span> for the candidate generator, and <span id="S4.SS5.p3.1.3" class="ltx_text ltx_font_bold">-43</span> and <span id="S4.SS5.p3.1.4" class="ltx_text ltx_font_bold">-35</span> for the ranker results in compression rates with no or marginal performance degradation. Since the compression performance (per client) in our setting is independent from the number of clients, we only performed experiments with 100 clients and no client sub-sampling, i.e., all clients were included in every communication round. The experiments also perform <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 10. Besides the number of clients, the client sub-sampling rate, and the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> queue length, the other hyperparameters of the experimental setup are identical to the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments in Section <a href="#S4.SS3" title="4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure id="S4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x18.png" id="S4.F14.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F14.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x19.png" id="S4.F14.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F14.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.5.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>. </span><span id="S4.F14.6.2" class="ltx_ERROR undefined">\Ac</span><span id="S4.F14.7.3" class="ltx_text" style="font-size:90%;">fl with communication compression experiment results for <span id="S4.F14.7.3.1" class="ltx_text ltx_font_bold">(<a href="#S4.F14.sf1" title="In Figure 14 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a>)</span> the candidate generator and <span id="S4.F14.7.3.2" class="ltx_text ltx_font_bold">(<a href="#S4.F14.sf2" title="In Figure 14 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(b)</span></a>)</span> the ranker.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><span id="S4.F14.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_3"><span id="S4.F14.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F14.10" class="ltx_p ltx_figure_panel ltx_align_center">fl with communication compression experiment results for <span id="S4.F14.10.1" class="ltx_text ltx_font_bold">(<a href="#S4.F14.sf1" title="In Figure 14 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a>)</span> the candidate generator and <span id="S4.F14.10.2" class="ltx_text ltx_font_bold">(<a href="#S4.F14.sf2" title="In Figure 14 ‣ 4.5. Communication Compression Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(b)</span></a>)</span> the ranker.</p>
</div>
</div>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">The plots on the left of each sub-figure show the model performance, while the plots on the right of each sub-figure demonstrate the compression performance for different <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> values. According to the compression performance plots, the initial number of communicated MiB is slightly higher, as in the beginning weights are initialized with random values. During the course of training, the entropy of the weights decreases, resulting in better compression performance. After a few communication rounds, the compression performance saturates at an almost constant value. For the candidate generator, the space saving, as compared to uncompressed communication, varies between <span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">92.97%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -38 and <span id="S4.SS5.p4.1.2" class="ltx_text ltx_font_bold">95.37%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -30. For the ranker, the space saving, as compared to uncompressed communication, varies between <span id="S4.SS5.p4.1.3" class="ltx_text ltx_font_bold">85.88%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -43 and <span id="S4.SS5.p4.1.4" class="ltx_text ltx_font_bold">86.17%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -35. The space savings are lower in comparison to the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline, where the candidate generator achieved <span id="S4.SS5.p4.1.5" class="ltx_text ltx_font_bold">97.04%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -38 and <span id="S4.SS5.p4.1.6" class="ltx_text ltx_font_bold">98.39%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -30, and the ranker <span id="S4.SS5.p4.1.7" class="ltx_text ltx_font_bold">91.85%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -43 and <span id="S4.SS5.p4.1.8" class="ltx_text ltx_font_bold">96.4%</span> for <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -35. This seems to be an effect that is inherent to <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>. In centralized training, regularization methods produce small magnitude weights, which results in higher sparsity when applying quantization. The exact weights that are going towards zero can, however, differ between several training runs, which means that in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting each client can have different weights of small magnitude. Due to the averaging of the weights in <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, the produced global model will most likely be less sparse than its constituent local models. For example, the overall entropy of the candidate generator that was trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> with compression is <span id="S4.SS5.p4.1.9" class="ltx_text ltx_font_bold">4.41 bits</span>, while the overall entropy of the candidate generator that was trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> without compression is <span id="S4.SS5.p4.1.10" class="ltx_text ltx_font_bold">7.5 bits</span>, and the baseline candidate generator which was trained centrally without compression has an entropy of <span id="S4.SS5.p4.1.11" class="ltx_text ltx_font_bold">1.26 bits</span>. The overall entropy of the ranker that was trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> with compression is <span id="S4.SS5.p4.1.12" class="ltx_text ltx_font_bold">11.44 bits</span>, while the overall entropy of the ranker that was trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> without compression is <span id="S4.SS5.p4.1.13" class="ltx_text ltx_font_bold">12.59 bits</span> and the baseline ranker, which was trained centrally without compression, has an entropy of <span id="S4.SS5.p4.1.14" class="ltx_text ltx_font_bold">3.67 bits</span>. This shows that models that are trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> do, in fact, have a higher entropy and are thus less amenable to compression. Quantization on the other hand seems to induce more sparsity, thus lowering the resulting entropy for models trained with compression. Furthermore, this also shows why the ranker performs much worse in terms of space saving, as it has much higher entropy in general.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">The candidate generator has excellent model performance, even for higher <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a>, well outperforming the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline and showing the same performance characteristics as the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments presented in Section <a href="#S4.SS4" title="4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. As the loss in performance and the increase in compression performance are very small, any of the tested <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a> are well-suited to be used, so we selected a <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> of -30, which offers the best overall space saving of <span id="S4.SS5.p5.1.1" class="ltx_text ltx_font_bold">95.37%</span> and a top-100 accuracy of <span id="S4.SS5.p5.1.2" class="ltx_text ltx_font_bold">50.5%</span>, which is only one percentage point smaller than the best accuracy and well above the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline of 47.15%.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p id="S4.SS5.p6.1" class="ltx_p">The training of the ranker model, however, seems to be much more affected by the compression as compared to the candidate generator, although the increase in compression performance is exceedingly small with increasing <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a>. Only <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a> -43 and -41 manage to meet the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline and none of the <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">QPs</span></abbr></a> achieve a performance that is in line with the results of the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments. This is, however, to be expected since the lossy compression of <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> may hurt the performance of the models. In this case, the difference between the best performance reached with compression is only slightly lower than the best performance of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> without compression. Therefore, <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> -43 was selected as it slightly outperforms the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> baseline with <span id="S4.SS5.p6.1.1" class="ltx_text ltx_font_bold">38.85%</span> accuracy and an <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> of <span id="S4.SS5.p6.1.2" class="ltx_text ltx_font_bold">0.91</span> but reaches almost the same space savings as the smallest <a href="#id37.37.id37"><abbr href="#id37.37.id37" title="quantization parameter" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">QP</span></abbr></a> with <span id="S4.SS5.p6.1.3" class="ltx_text ltx_font_bold">85.88%</span> as compared to <span id="S4.SS5.p6.1.4" class="ltx_text ltx_font_bold">86.17%</span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion &amp; Outlook</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Modern <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RecSys’s</span></abbr></a>, especially the ones based on <a href="#id8.8.id8"><abbr href="#id8.8.id8" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DL</span></abbr></a>, benefit from increasing amounts of personal information about its users. This has resulted in the collection of substantial amounts of personal data on many platforms in recent years, leading to a data privacy problem. Here, <span id="S5.p1.1.1" class="ltx_ERROR undefined">\Ac</span>fl has emerged as a technique that intrinsically provides privacy and is therefore used in many scenarios where data privacy is of high priority. Consequently, we presented a movie <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, which is being trained end-to-end using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and scales well to exceptionally large numbers of users. We have identified major problems in such systems and proposed solutions to them. In particular, we have shown that the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness of the clients’ local datasets, as well as small local datasets can significantly degrade the federated training of a <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> and developed a novel technique, called <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, which satisfactorily counteracts this problem. Furthermore, the substantial overhead of constantly communicating <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> parametrizations between server and clients in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> poses a problem, especially when clients are connected via mobile internet connections. For this, we have shown that the most recent <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> compression technology can considerably reduce this communication overhead to a fraction of the uncompressed communication.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Beyond the proposed significant improvements to the overall <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, additional improvements can be achieved through further research. In the area of data privacy, differential privacy methods could be further investigated and combined with the quantization-induced privacy by <a href="#id35.35.id35"><abbr href="#id35.35.id35" title="neural network coding" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NNC</span></abbr></a> communication compression. Another topic of interest is the learning of embeddings in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting, which is known to be problematic. Solutions proposed in the literature, seem to all depend on the partial disclosure of client data. Here, future work could investigate possibilities of learning embeddings in an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting without disclosing private information. The space savings of the compression can be further improved by differential compression, i.e., only the difference between the global model and the updated local model is compressed, which is sparser and is thus more amenable to compression. Finally, the non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>-ness in the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> scenario originates from different user preferences. The local datasets within user groups of similar preference should be much more homogeneous, leading the way to further model performance improvements for <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FedRec’s</span></abbr></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Md. Hijbul Alam, Woo-Jong Ryu, and SangKeun Lee. 2016.

</span>
<span class="ltx_bibblock">Joint multi-grain topic sentiment: modeling semantic aspects for online reviews.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Information Sciences</em> 339 (2016), 206–223.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.ins.2016.01.013" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.ins.2016.01.013</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alamgir et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zareen Alamgir, Farwa K. Khan, and Saira Karim. 2022.

</span>
<span class="ltx_bibblock">Federated Recommenders: Methods, Challenges and Future.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Cluster Computing</em> 25, 6 (June 2022), 4075–4096.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10586-022-03644-w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10586-022-03644-w</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammad-ud-din et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Muhammad Ammad-ud-din, Elena Ivannikova, Suleiman A. Khan, Were Oyomno, Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. 2019.

</span>
<span class="ltx_bibblock">Federated Collaborative Filtering for Privacy-Preserving Personalized Recommendation System.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1901.09888 (Jan. 2019).

</span>
<span class="ltx_bibblock">arXiv:1901.09888 [cs.IR]

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asad et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Muhammad Asad, Saima Shaukat, Ehsan Javanmardi, Jin Nakazato, and Manabu Tsukada. 2023.

</span>
<span class="ltx_bibblock">A Comprehensive Survey on Privacy-Preserving Techniques in Federated Recommendation Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 13, 10 (2023).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/app13106201" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/app13106201</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brendan McMahan et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017.

</span>
<span class="ltx_bibblock">Learning Differentially Private Recurrent Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1710.06963 (Oct. 2017).

</span>
<span class="ltx_bibblock">arXiv:1710.06963 [cs.LG]

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Sebastian Caldas, J. Konečný, H. Brendan McMahan, and Ameet Talwalkar. 2018a.

</span>
<span class="ltx_bibblock">Expanding the Reach of Federated Learning by Reducing Client Resource Requirements.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1812.07210 (Dec. 2018).

</span>
<span class="ltx_bibblock">arXiv:1812.07210 [cs.LG]

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018b.

</span>
<span class="ltx_bibblock">LEAF: A Benchmark for Federated Settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1812.01097 (Dec. 2018).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1812.01097</a>
arXiv:1812.01097 [cs.LG]

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mei Cao, Yujie Zhang, Zezhong Ma, and Mengying Zhao. 2022.

</span>
<span class="ltx_bibblock">C2S: Class-aware client selection for effective aggregation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">High-Confidence Computing</em> 2, 3 (2022), 100068.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.hcc.2022.100068" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.hcc.2022.100068</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007.

</span>
<span class="ltx_bibblock">Learning to Rank: From Pairwise Approach to Listwise Approach. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th International Conference on Machine Learning</em> (Corvalis, Oregon, USA) <em id="bib.bib10.4.2" class="ltx_emph ltx_font_italic">(ICML ’07)</em>. Association for Computing Machinery, New York, NY, USA, 129–136.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1273496.1273513" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1273496.1273513</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Di Chai, Leye Wang, Kai Chen, and Qiang Yang. 2020.

</span>
<span class="ltx_bibblock">FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2011.09655" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2011.09655</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
D. Chai, L. Wang, K. Chen, and Q. Yang. 2021.

</span>
<span class="ltx_bibblock">Secure Federated Matrix Factorization.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> 36, 05 (Sept. 2021), 11–20.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/MIS.2020.3014880" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MIS.2020.3014880</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. 2018.

</span>
<span class="ltx_bibblock">Federated Meta-Learning with Fast Convergence and Efficient Communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> 1802.07876 (Feb. 2018).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1802.07876" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1802.07876</a>
arXiv:1802.07876 [cs.LG]

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Tianqi Chen, Zhao Zheng, Qiuxia Lu, Weinan Zhang, and Yong Yu. 2011.

</span>
<span class="ltx_bibblock">Feature-Based Matrix Factorization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.1109.2271" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.1109.2271</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wenlin Chen, Samuel Horvath, and Peter Richtarik. 2020.

</span>
<span class="ltx_bibblock">Optimal Client Sampling for Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2010.13723 (Oct. 2020).

</span>
<span class="ltx_bibblock">arXiv:2010.13723 [cs.LG]

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Kyunghyuna Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</em>. Association for Computational Linguistics, Doha, Qatar, 103–111.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3115/v1/W14-4012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3115/v1/W14-4012</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choe et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Byeongjin Choe, Taegwan Kang, and Kyomin Jung. 2021.

</span>
<span class="ltx_bibblock">Recommendation System With Hierarchical Recurrent Neural Network for Long-Term Time Series.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 9 (2021), 72033–72039.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2021.3079922" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2021.3079922</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. 2016.

</span>
<span class="ltx_bibblock">Towards the Limit of Network Quantization.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1612.01543 (Dec. 2016).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1612.01543" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1612.01543</a>
arXiv:1612.01543 [cs.CV]

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. 2017.

</span>
<span class="ltx_bibblock">EMNIST: Extending MNIST to handwritten letters. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">2017 International Joint Conference on Neural Networks (IJCNN)</em> (Anchorage, Alaska, United States of America). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 2921–2926.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/IJCNN.2017.7966217" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCNN.2017.7966217</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay Adams, and Emre Sargin. 2016.

</span>
<span class="ltx_bibblock">Deep Neural Networks for YouTube Recommendations. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th ACM Conference on Recommender Systems</em> (Boston, Massachusetts, USA) <em id="bib.bib20.4.2" class="ltx_emph ltx_font_italic">(RecSys ’16)</em>. ACM (Association for Computer Machinery), New York, NY, USA, 191–198.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2959100.2959190" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2959100.2959190</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cramer et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Ronald Cramer, Ivan Bjerre Damgård, et al<span id="bib.bib21.3.1" class="ltx_text">.</span> 2015.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.4.1" class="ltx_emph ltx_font_italic">Secure multiparty computation</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, Cambridge, United Kingdom.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dimitrov et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dimitar I. Dimitrov, Mislav Balunović, Nikola Konstantinov, and Martin Vechev. 2022.

</span>
<span class="ltx_bibblock">Data Leakage in Federated Averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2206.12395 (2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2206.12395" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2206.12395</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2008.

</span>
<span class="ltx_bibblock">Differential Privacy: A Survey of Results. In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Theory and Applications of Models of Computation</em>, Manindra Agrawal, Dingzhu Du, Zhenhua Duan, and Angsheng Li (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1–19.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork and Roth (2014)</span>
<span class="ltx_bibblock">
Cynthia Dwork and Aaron Roth. 2014.

</span>
<span class="ltx_bibblock">The Algorithmic Foundations of Differential Privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Found. Trends Theor. Comput. Sci.</em> 9, 3-4 (Aug. 2014), 211–407.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1561/0400000042" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1561/0400000042</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Parliament (2016)</span>
<span class="ltx_bibblock">
European Parliament. 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)</em>.

</span>
<span class="ltx_bibblock">European Union.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang and Quan (2021)</span>
<span class="ltx_bibblock">
Haokun Fang and Qian Quan. 2021.

</span>
<span class="ltx_bibblock">Privacy Preserving Machine Learning with Homomorphic Encryption and Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Future Internet</em> 13, 4 (2021), 94.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3390/fi13040094" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/fi13040094</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.

</span>
<span class="ltx_bibblock">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine Learning - Volume 70</em> (Sydney, NSW, Australia) <em id="bib.bib27.4.2" class="ltx_emph ltx_font_italic">(ICML’17)</em>. JMLR.org, 1269 Law Street, San Diego, CA 92109, 1126–1135.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flanagan et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Adrian Flanagan, Were Oyomno, Alexander Grigorievskiy, Kuan E. Tan, Suleiman A. Khan, and Muhammad Ammad-Ud-Din. 2021.

</span>
<span class="ltx_bibblock">Federated Multi-view Matrix Factorization for Personalized Recommendations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Discovery in Databases</em>. Springer International Publishing, Ghent, Belgium, 324–347.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-3-030-67661-2_20" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-67661-2_20</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraboni et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. 2021.

</span>
<span class="ltx_bibblock">A General Theory for Client Sampling in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2107.12211 (July 2021).

</span>
<span class="ltx_bibblock">arXiv:2107.12211 [cs.LG]

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting Gradients - How Easy is It to Break Privacy in Federated Learning?. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Neural Information Processing Systems</em> (Vancouver, British Columbia, Canada) <em id="bib.bib30.4.2" class="ltx_emph ltx_font_italic">(NIPS’20)</em>. Curran Associates Inc., Red Hook, NY, USA, Article 1421, 11 pages.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gholami et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021.

</span>
<span class="ltx_bibblock">A Survey of Quantization Methods for Efficient Neural Network Inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2103.13630 (March 2021).

</span>
<span class="ltx_bibblock">arXiv:2103.13630 [cs.CV]

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Go et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Alec Go, Richa Bhayani, and Lei Huang. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Twitter Sentiment Classification using Distant Supervision</em>.

</span>
<span class="ltx_bibblock">CS224N Project Report. Stanford.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golbeck (2016)</span>
<span class="ltx_bibblock">
Jennifer Golbeck. 2016.

</span>
<span class="ltx_bibblock">User Privacy Concerns with Common Data Used in Recommender Systems. In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Social Informatics</em>, Emma Spiro and Yong-Yeol Ahn (Eds.). Springer International Publishing, Cham, 468–480.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gomez-Uribe and Hunt (2016)</span>
<span class="ltx_bibblock">
Carlos A. Gomez-Uribe and Neil Hunt. 2016.

</span>
<span class="ltx_bibblock">The Netflix Recommender System: Algorithms, Business Value, and Innovation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Manage. Inf. Syst.</em> 6, 4, Article 13 (Dec. 2016), 19 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2843948" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2843948</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grbovic and Cheng (2018)</span>
<span class="ltx_bibblock">
Mihajlo Grbovic and Haibin Cheng. 2018.

</span>
<span class="ltx_bibblock">Real-Time Personalization Using Embeddings for Search Ranking at Airbnb. In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (London, United Kingdom) <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">(KDD ’18)</em>. Association for Computing Machinery, New York, NY, USA, 311–320.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3219819.3219885" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3219819.3219885</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grother and Hanaoka (1995)</span>
<span class="ltx_bibblock">
Patrick J. Grother and Kayee K. Hanaoka. 1995.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">NIST special database 19 handprinted forms and characters database</em>.

</span>
<span class="ltx_bibblock">Technical Report. National Institute of Standards and Technology.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18434/T4H01C" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18434/T4H01C</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haase et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Paul Haase, Daniel Becking, Heiner Kirchhoffer, Karsten Müller, Heiko Schwarz, Wojciech Samek, Detlev Marpe, and Thomas Wiegand. 2021.

</span>
<span class="ltx_bibblock">Encoder Optimizations For The NNR Standard On Neural Network Compression. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Image Processing (ICIP)</em> (Anchorage, Alaska, USA). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 3522–3526.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICIP42928.2021.9506655" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICIP42928.2021.9506655</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William J. Dally. 2016.

</span>
<span class="ltx_bibblock">Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. In <em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">4th International Conference on Learning Representations, ICLR, May 2-4, 2016, Conference Track Proceedings</em>, Yoshua Bengio and Yann LeCun (Eds.). ICLR, San Juan, Puerto Rico.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/1510.00149" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1510.00149</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated Learning for Mobile Keyboard Prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1811.03604 (Nov. 2018).

</span>
<span class="ltx_bibblock">arXiv:1811.03604 [cs.CL]

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)</span>
<span class="ltx_bibblock">
F. Maxwell Harper and Joseph A. Konstan. 2015.

</span>
<span class="ltx_bibblock">The MovieLens Datasets: History and Context.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Interact. Intell. Syst.</em> 5, 4, Article 19 (Dec. 2015), 19 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2827872" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2827872</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao Sun, Lifang He, Liangwei Yang, Philip S. Yu, Yu Rong, Peilin Zhao, Junzhou Huang, Murali Annavaram, and Salman Avestimehr. 2021.

</span>
<span class="ltx_bibblock">FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2104.07145 (April 2021).

</span>
<span class="ltx_bibblock">arXiv:2104.07145 [cs.LG]

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017.

</span>
<span class="ltx_bibblock">Neural Collaborative Filtering. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th International Conference on World Wide Web</em> (Perth, Australia) <em id="bib.bib42.4.2" class="ltx_emph ltx_font_italic">(WWW ’17)</em>. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 173–182.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3038912.3052569" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3038912.3052569</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermann (2022)</span>
<span class="ltx_bibblock">
Erik Hermann. 2022.

</span>
<span class="ltx_bibblock">Artificial intelligence and mass personalization of communication content—An ethical and literacy perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">New Media &amp; Society</em> 24, 5 (2022), 1258–1277.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/14614448211022702" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/14614448211022702</a>
arXiv:https://doi.org/10.1177/14614448211022702

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.

</span>
<span class="ltx_bibblock">Distilling the Knowledge in a Neural Network. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">NIPS Deep Learning and Representation Learning Workshop</em>. Morgan-Kaufmann, Montréal, Québec, Canada.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/1503.02531" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1503.02531</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long Short-Term Memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Neural Comput.</em> 9, 8 (Nov. 1997), 1735–1780.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1162/neco.1997.9.8.1735" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1162/neco.1997.9.8.1735</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ming Hu, Tian Liu, Zhiwei Ling, Zhihao Yue, and Mingsong Chen. 2022.

</span>
<span class="ltx_bibblock">FedCAT: Towards Accurate Federated Learning via Device Concatenation.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2202.12751 (Feb. 2022).

</span>
<span class="ltx_bibblock">arXiv:2202.12751 [cs.LG]

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">International Organization for Standardization (2022) (ISO)</span>
<span class="ltx_bibblock">International Organization for Standardization (ISO). 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Information technology - Multimedia content description interface — Part 17: Compression of neural networks for multimedia content description and analysis</em>.

</span>
<span class="ltx_bibblock">Standard. International Organization for Standardization (ISO), Geneva, Switzerland.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy (2015)</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy. 2015.

</span>
<span class="ltx_bibblock">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</em> (Lille, France) <em id="bib.bib48.2.2" class="ltx_emph ltx_font_italic">(ICML’15)</em>. JMLR.org, 1269 Law Street, San Diego, CA 92109, 448–456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. 2018.

</span>
<span class="ltx_bibblock">Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1811.11479 (Nov. 2018).

</span>
<span class="ltx_bibblock">arXiv:1811.11479 [cs.LG]

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia and Lei (2021)</span>
<span class="ltx_bibblock">
Junjie Jia and Zhipeng Lei. 2021.

</span>
<span class="ltx_bibblock">Personalized Recommendation Algorithm for Mobile Based on Federated Matrix Factorization.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Journal of Physics: Conference Series</em> 1802, 3 (March 2021), 032021.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1088/1742-6596/1802/3/032021" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1088/1742-6596/1802/3/032021</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jie et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhiyong Jie, Shuhong Chen, Junqiu Lai, Muhammad Arif, and Zongyuan He. 2022.

</span>
<span class="ltx_bibblock">Personalized federated recommendation system with historical parameter clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Journal of Ambient Intelligence and Humanized Computing</em> 14 (02 2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s12652-022-03709-z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s12652-022-03709-z</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamp et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Michael Kamp, Jonas Fischer, and Jilles Vreeken. 2021.

</span>
<span class="ltx_bibblock">Federated Learning from Small Datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2110.03469 (Oct. 2021).

</span>
<span class="ltx_bibblock">arXiv:2110.03469 [cs.LG]

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em> <em id="bib.bib53.4.2" class="ltx_emph ltx_font_italic">(Proceedings of Machine Learning Research, Vol. 119)</em>, Hal Daumé III and Aarti Singh (Eds.). PMLR, virtual, 5132–5143.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.mlr.press/v119/karimireddy20a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v119/karimireddy20a.html</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiefer and Wolfowitz (1952)</span>
<span class="ltx_bibblock">
J. Kiefer and J. Wolfowitz. 1952.

</span>
<span class="ltx_bibblock">Stochastic Estimation of the Maximum of a Regression Function.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">The Annals of Mathematical Statistics</em> 23, 3 (1952), 462–466.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://www.jstor.org/stable/2236690" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.jstor.org/stable/2236690</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jinsu Kim, Dongyoung Koo, Yuna Kim, Hyunsoo Yoon, Junbum Shin, and Sungwook Kim. 2018.

</span>
<span class="ltx_bibblock">Efficient Privacy-Preserving Matrix Factorization for Recommendation via Fully Homomorphic Encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Priv. Secur.</em> 21, 4, Article 17 (jun 2018), 30 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3212509" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3212509</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam: A Method for Stochastic Optimization. In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, Yoshua Bengio and Yann LeCun (Eds.). International Conference on Learning Representations, 2710 E Corridor Drive, Appleton, WI 54913.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1412.6980</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirchhoffer et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Heiner Kirchhoffer, Paul Haase, Wojciech Samek, Karsten Müller, Hamed Rezazadegan-Tavakoli, Francesco Cricri, Emre B. Aksu, Miska M. Hannuksela, Wei Jiang, Wei Wang, Shan Liu, Swayambhoo Jain, Shahab Hamidi-Rad, Fabien Racapé, and Werner Bailer. 2022.

</span>
<span class="ltx_bibblock">Overview of the Neural Network Compression and Representation (NNR) Standard.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology</em> 32, 5 (2022), 3203–3216.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TCSVT.2021.3095970" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TCSVT.2021.3095970</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
J. Konečný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. 2016.

</span>
<span class="ltx_bibblock">Federated Learning: Strategies for Improving Communication Efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1610.05492 (2016).

</span>
<span class="ltx_bibblock">arXiv:1610.05492

<a target="_blank" href="http://arxiv.org/abs/1610.05492" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1610.05492</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečný, Hugh Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016.

</span>
<span class="ltx_bibblock">Federated Optimization: Distributed Machine Learning for On-Device Intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1610.02527 (Oct. 2016).

</span>
<span class="ltx_bibblock">arXiv:1610.02527

<a target="_blank" href="http://arxiv.org/abs/1610.02527" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1610.02527</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren (2008)</span>
<span class="ltx_bibblock">
Yehuda Koren. 2008.

</span>
<span class="ltx_bibblock">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model. In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (Las Vegas, Nevada, USA) <em id="bib.bib60.2.2" class="ltx_emph ltx_font_italic">(KDD ’08)</em>. Association for Computing Machinery, New York, NY, USA, 426–434.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1401890.1401944" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1401890.1401944</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.

</span>
<span class="ltx_bibblock">Matrix Factorization Techniques for Recommender Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">Computer</em> 42, 8 (Aug. 2009), 30–37.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/MC.2009.263" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MC.2009.263</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kozyreva et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Anastasia Kozyreva, Philipp Lorenz-Spreen, Ralph Hertwig, Stephan Lewandowsky, and Stefan M Herzog. 2021.

</span>
<span class="ltx_bibblock">Public attitudes towards algorithmic personalization and use of personal data online: Evidence from Germany, Great Britain, and the United States.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Humanities and Social Sciences Communications</em> 8, 1 (2021), 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Shyong K. “Tony” Lam, Dan Frankowski, and John Riedl. 2006.

</span>
<span class="ltx_bibblock">Do You Trust Your Recommendations? An Exploration of Security and Privacy Issues in Recommender Systems. In <em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">Emerging Trends in Information and Communication Security</em>, Günter Müller (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 14–29.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang and Shlezinger (2022)</span>
<span class="ltx_bibblock">
Natalie Lang and Nir Shlezinger. 2022.

</span>
<span class="ltx_bibblock">Joint Privacy Enhancement and Quantization in Federated Learning. In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Symposium on Information Theory (ISIT)</em> (Aalto University, Espoo, Finland). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 2040–2045.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/ISIT50566.2022.9834551" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ISIT50566.2022.9834551</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lecun et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (1998)</span>
<span class="ltx_bibblock">
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em> 86, 11 (1998), 2278–2324.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/5.726791" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/5.726791</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (1990)</span>
<span class="ltx_bibblock">
Yann LeCun, John Denker, and Sara Solla. 1990.

</span>
<span class="ltx_bibblock">Optimal Brain Damage. In <em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, D. Touretzky (Ed.), Vol. 2. Morgan-Kaufmann, Denver, Colorado, USA.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. 2019.

</span>
<span class="ltx_bibblock">Federated Learning for Keyword Spotting. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (Brighton, United Kingdom). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 6341–6345.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP.2019.8683546" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP.2019.8683546</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2018.

</span>
<span class="ltx_bibblock">Federated Optimization in Heterogeneous Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.1812.06127" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.1812.06127</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock">Fair Resource Allocation in Federated Learning. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, Addis Ababa, Ethiopia.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=ByexElSYDr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=ByexElSYDr</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.

</span>
<span class="ltx_bibblock">On the Convergence of FedAvg on Non-IID Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.1907.02189" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.1907.02189</a>

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. 2021.

</span>
<span class="ltx_bibblock">FedBN: Federated learning on non-iid features via local batch normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.07623</em> abs/2102.07623 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Feng Liang, Weike Pan, and Zhong Ming. 2021.

</span>
<span class="ltx_bibblock">Fedrec++: Lossless federated recommendation with explicit feedback. In <em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 35. AAAI Press, Washington, DC, USA, 4224–4231.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr. 2022.

</span>
<span class="ltx_bibblock">FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. In <em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL 2022</em>. Association for Computational Linguistics, Seattle, United States, 157–175.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-naacl.13" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2022.findings-naacl.13</a>

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Darryl D. Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. 2016.

</span>
<span class="ltx_bibblock">Fixed Point Quantization of Deep Convolutional Networks. In <em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48</em> (New York, NY, USA) <em id="bib.bib74.4.2" class="ltx_emph ltx_font_italic">(ICML’16)</em>. JMLR.org, 1269 Law Street, San Diego, CA 92109, 2849–2858.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Guanyu Lin, Feng Liang, Weike Pan, and Zhong Ming. 2021a.

</span>
<span class="ltx_bibblock">FedRec: Federated Recommendation With Explicit Feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> 36, 5 (Sept. 2021), 21–30.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/MIS.2020.3017205" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MIS.2020.3017205</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Zhaohao Lin, Weike Pan, and Zhong Ming. 2021b.

</span>
<span class="ltx_bibblock">FR-FMSS: Federated Recommendation via Fake Marks and Secret Sharing. In <em id="bib.bib76.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th ACM Conference on Recommender Systems</em> (Amsterdam, Netherlands) <em id="bib.bib76.4.2" class="ltx_emph ltx_font_italic">(RecSys ’21)</em>. Association for Computing Machinery, New York, NY, USA, 668–673.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3460231.3478855" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3460231.3478855</a>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu. 2009.

</span>
<span class="ltx_bibblock">Learning to Rank for Information Retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends in Information Retrieval</em> 3, 3 (March 2009), 225–331.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1561/1500000016" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1561/1500000016</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang. 2020.

</span>
<span class="ltx_bibblock">A Secure Federated Transfer Learning Framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> 35, 4 (2020), 70–82.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/MIS.2020.2988525" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MIS.2020.2988525</a>

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015.

</span>
<span class="ltx_bibblock">Deep Learning Face Attributes in the Wild. In <em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision (ICCV)</em> (Santiago, Chile). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 3730–3738.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICCV.2015.425" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV.2015.425</a>

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lloyd (1982)</span>
<span class="ltx_bibblock">
S. Lloyd. 1982.

</span>
<span class="ltx_bibblock">Least squares quantization in PCM.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Theory</em> 28, 2 (1982), 129–137.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TIT.1982.1056489" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TIT.1982.1056489</a>

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper and Bird (2002)</span>
<span class="ltx_bibblock">
Edward Loper and Steven Bird. 2002.

</span>
<span class="ltx_bibblock">NLTK: The Natural Language Toolkit.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> cs/0205028 (May 2002).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.cs/0205028" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.cs/0205028</a>
arXiv:cs/0205028 [cs.CL]

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, and Qiang Yang. 2019.

</span>
<span class="ltx_bibblock">Real-World Image Datasets for Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1910.11089 (Oct. 2019).

</span>
<span class="ltx_bibblock">arXiv:1910.11089 [cs.CV]

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacKenzie et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Ian MacKenzie, Chris Meyer, and Steve Noble. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">How retailers can keep up with consumers</em>.

</span>
<span class="ltx_bibblock">McKinsey &amp; Company.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers</a>

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Hugh Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2016.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/1602.05629 (Feb. 2016).

</span>
<span class="ltx_bibblock">arXiv:1602.05629 [cs.LG]

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minto et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Lorenzo Minto, Moritz Haller, Benjamin Livshits, and Hamed Haddadi. 2021.

</span>
<span class="ltx_bibblock">Stronger privacy for federated collaborative filtering with implicit feedback. In <em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th ACM Conference on Recommender Systems</em>. ACM (Association for Computer Machinery), New York, NY, USA, 342–350.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moving Picture Experts Group  working group of ISO/IEC(2021) (MPEG)</span>
<span class="ltx_bibblock">Moving Picture Experts Group (MPEG) working group of ISO/IEC. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">MPEG-7: Compression of Neural Networks for Multimedia Content Description and analysis</em>.

</span>
<span class="ltx_bibblock">Standard. Moving Picture Experts Group (MPEG) working group of ISO/IEC, Hannover, DE.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Khalil Muhammad, Qinqin Wang, Diarmuid O’Reilly-Morgan, Elias Tragos, Barry Smyth, Neil Hurley, James Geraci, and Aonghus Lawlor. 2020.

</span>
<span class="ltx_bibblock">FedFast: Going Beyond Average for Faster Training of Federated Recommender Systems. In <em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">KDD ’20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (Virtual Event, CA, USA) <em id="bib.bib87.4.2" class="ltx_emph ltx_font_italic">(KDD ’20)</em>. Association for Computing Machinery, New York, New York, USA, 1234–1242.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3394486.3403176" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3394486.3403176</a>

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neumann et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
David Neumann, Felix Sattler, Heiner Kirchhoffer, Simon Wiedemann, Karsten Müller, Heiko Schwarz, Thomas Wiegand, Detlev Marpe, and Wojciech Samek. 2020.

</span>
<span class="ltx_bibblock">DeepCABAC: Plug&amp;Play Compression of Neural Network Weights and Weight Updates. In <em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Image Processing, ICIP 2020, October 25-28, 2020</em>. IEEE, Abu Dhabi, United Arab Emirates, 21–25.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICIP40778.2020.9190821" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICIP40778.2020.9190821</a>

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovi et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Pretom Roy Ovi, Emon Dey, Nirmalya Roy, and Aryya Gangopadhyay. 2022.

</span>
<span class="ltx_bibblock">Leveraging Mixed Precision Quantization to Tackle Gradient Leakage Attacks in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2210.13457 (Oct. 2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2210.13457" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2210.13457</a>
arXiv:2210.13457 [cs.LG]

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock">PyTorch: An Imperative Style, High-Performance Deep Learning Library.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32</em>, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., Vancouver, British Columbia, Canada, 8024–8035.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perifanis and Efraimidis (2022)</span>
<span class="ltx_bibblock">
Vasileios Perifanis and Pavlos S. Efraimidis. 2022.

</span>
<span class="ltx_bibblock">Federated Neural Collaborative Filtering.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Know.-Based Syst.</em> 242, C (April 2022), 16 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.knosys.2022.108441" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.knosys.2022.108441</a>

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phong et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. 2018.

</span>
<span class="ltx_bibblock">Privacy-Preserving Deep Learning via Additively Homomorphic Encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em> 13, 5 (2018), 1333–1345.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TIFS.2017.2787987" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TIFS.2017.2787987</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. 2019.

</span>
<span class="ltx_bibblock">FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.1909.13014" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.1909.13014</a>

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribero et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mónica Ribero, Jette Henderson, Sinead Williamson, and Haris Vikalo. 2022.

</span>
<span class="ltx_bibblock">Federating Recommendations Using Differentially Private Prototypes.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">Pattern Recogn.</em> 129, C (Sept. 2022), 14 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.patcog.2022.108746" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.patcog.2022.108746</a>

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robbins and Monro (1951)</span>
<span class="ltx_bibblock">
Herbert Robbins and Sutton Monro. 1951.

</span>
<span class="ltx_bibblock">A Stochastic Approximation Method.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">The Annals of Mathematical Statistics</em> 22, 3 (1951), 400–407.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://www.jstor.org/stable/2236626" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.jstor.org/stable/2236626</a>

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rønn Hansen et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Christian Rønn Hansen, Gareth Price, Matthew Field, Nis Sarup, Ruta Zukauskaite, Jørgen Johansen, Jesper Grau Eriksen, Farhannah Aly, Andrew McPartlin, Lois Holloway, David Thwaites, and Carsten Brink. 2022.

</span>
<span class="ltx_bibblock">Larynx cancer survival model developed through open-source federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">Radiotherapy and Oncology</em> 176 (2022), 179–186.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.radonc.2022.09.023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.radonc.2022.09.023</a>

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Felix Sattler, Simon Wiedemann, Klaus Robert Müller, and Wojciech Samek. 2019.

</span>
<span class="ltx_bibblock">Sparse Binary Compression: Towards Distributed Deep Learning with minimal Communication. In <em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks, IJCNN 2019</em> <em id="bib.bib97.4.2" class="ltx_emph ltx_font_italic">(Proceedings of the International Joint Conference on Neural Networks)</em>. Institute of Electrical and Electronics Engineers Inc., Budapest, Hungary.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/IJCNN.2019.8852172" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCNN.2019.8852172</a>

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. 2020.

</span>
<span class="ltx_bibblock">Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em> 31, 9 (2020), 3400–3413.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TNNLS.2019.2944481" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TNNLS.2019.2944481</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrage (2017)</span>
<span class="ltx_bibblock">
Michael Schrage. 2017.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Great Digital Companies Build Great Recommendation Engines</em>.

</span>
<span class="ltx_bibblock">Harvard Business Review.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://hbr.org/2017/08/great-digital-companies-build-great-recommendation-engines" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hbr.org/2017/08/great-digital-companies-build-great-recommendation-engines</a>

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz (2004)</span>
<span class="ltx_bibblock">
Barry Schwartz. 2004.

</span>
<span class="ltx_bibblock">The Tyranny of Choice.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Scientific American</em> 290, 4 (April 2004), 70–75.

</span>
<span class="ltx_bibblock">


<a target="_blank" href="https://doi.org/10.1038/scientificamerican0404-70" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/scientificamerican0404-70</a>

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sedhain et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.

</span>
<span class="ltx_bibblock">AutoRec: Autoencoders Meet Collaborative Filtering. In <em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th International Conference on World Wide Web</em> (Florence, Italy) <em id="bib.bib101.4.2" class="ltx_emph ltx_font_italic">(WWW ’15 Companion)</em>. Association for Computing Machinery, New York, NY, USA, 111–112.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2740908.2742726" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2740908.2742726</a>

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seol and Kim (2023)</span>
<span class="ltx_bibblock">
Mihye Seol and Taejoon Kim. 2023.

</span>
<span class="ltx_bibblock">Performance Enhancement in Federated Learning by Reducing Class Imbalance of Non-IID Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Sensors</em> 23, 3 (2023).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/s23031152" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/s23031152</a>

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shakespeare (1994)</span>
<span class="ltx_bibblock">
William Shakespeare. 1994.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">The Complete Works of William Shakespeare</em>. Project Gutenberg, Vol. 100.

</span>
<span class="ltx_bibblock">Project Gutenberg, P.O. Box 2782, Champaign, IL 61825-2782, USA.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.gutenberg.org/ebooks/100" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.gutenberg.org/ebooks/100</a>

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shamir (1979)</span>
<span class="ltx_bibblock">
Adi Shamir. 1979.

</span>
<span class="ltx_bibblock">How to share a secret.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 22, 11 (1979), 612–613.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sherstinsky (2020)</span>
<span class="ltx_bibblock">
Alex Sherstinsky. 2020.

</span>
<span class="ltx_bibblock">Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Physica D: Nonlinear Phenomena</em> 404 (March 2020), 132306.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.physd.2019.132306" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.physd.2019.132306</a>

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri and Shmatikov (2015)</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly Shmatikov. 2015.

</span>
<span class="ltx_bibblock">Privacy-Preserving Deep Learning. In <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</em> (Denver, Colorado, USA) <em id="bib.bib106.2.2" class="ltx_emph ltx_font_italic">(CCS ’15)</em>. Association for Computing Machinery, New York, NY, USA, 1310–1321.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2810103.2813687" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2810103.2813687</a>

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jessie J. Smith, Lucia Jayne, and Robin Burke. 2022.

</span>
<span class="ltx_bibblock">Recommender Systems and Algorithmic Hate. In <em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th ACM Conference on Recommender Systems</em> (Seattle, WA, USA) <em id="bib.bib107.4.2" class="ltx_emph ltx_font_italic">(RecSys ’22)</em>. Association for Computing Machinery, New York, NY, USA, 592–597.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3523227.3551480" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3523227.3551480</a>

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stoll (2022)</span>
<span class="ltx_bibblock">
Julia Stoll. 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Devices used to watch online video on demand (VOD) worldwide in 1st quarter 2022 and 2nd quarter 2022</em>.

</span>
<span class="ltx_bibblock">Statista.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.statista.com/statistics/1329449/vod-device-usage-share-worldwide/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.statista.com/statistics/1329449/vod-device-usage-share-worldwide/</a>

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tao Sun, Dongsheng Li, and Bao Wang. 2022.

</span>
<span class="ltx_bibblock">Adaptive Random Walk Gradient Descent for Decentralized Optimization. In <em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning</em> <em id="bib.bib109.4.2" class="ltx_emph ltx_font_italic">(Proceedings of Machine Learning Research, Vol. 162)</em>, Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, Baltimore, Maryland, USA, 20790–20809.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.mlr.press/v162/sun22b.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v162/sun22b.html</a>

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib110.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zehua Sun, Yonghui Xu, Yong Liu, Wei He, Lanju Kong, Fangzhao Wu, Yali Jiang, and Lizhen Cui. 2022.

</span>
<span class="ltx_bibblock">A Survey on Federated Recommendation Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> 2301.00767 (Dec. 2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2301.00767" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2301.00767</a>
arXiv:2301.00767 [cs.IR]

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wang (2018)</span>
<span class="ltx_bibblock">
Jiaxi Tang and Ke Wang. 2018.

</span>
<span class="ltx_bibblock">Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</em> (Marina Del Rey, CA, USA) <em id="bib.bib111.2.2" class="ltx_emph ltx_font_italic">(WSDM ’18)</em>. Association for Computing Machinery, New York, NY, USA, 565–573.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3159652.3159656" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3159652.3159656</a>

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Triastcyn et al<span id="bib.bib112.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Aleksei Triastcyn, Matthias Reisser, and Christos Louizos. 2022.

</span>
<span class="ltx_bibblock">Decentralized Learning with Random Walks and Communication-Efficient Adaptive Optimization. In <em id="bib.bib112.3.1" class="ltx_emph ltx_font_italic">Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)</em>. NeurIPS, New Orleans, LA, USA.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wainakh et al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Aidmar Wainakh, Tim Grube, Jörg Daubert, and Max Mühlhäuser. 2019.

</span>
<span class="ltx_bibblock">Efficient Privacy-Preserving Recommendations Based on Social Graphs. In <em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th ACM Conference on Recommender Systems</em> (Copenhagen, Denmark) <em id="bib.bib113.4.2" class="ltx_emph ltx_font_italic">(RecSys ’19)</em>. Association for Computing Machinery, New York, NY, USA, 78–86.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3298689.3347013" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3298689.3347013</a>

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib114.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jianyu Wang, Rudrajit Das, Gauri Joshi, Satyen Kale, Zheng Xu, and Tong Zhang. 2022.

</span>
<span class="ltx_bibblock">On the Unreasonable Effectiveness of Federated Averaging with Heterogeneous Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2206.04723" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2206.04723</a>

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuai Wang, Richard Cornelius Suwandi, and Tsung-Hui Chang. 2021.

</span>
<span class="ltx_bibblock">Demystifying Model Averaging for Communication-Efficient Federated Matrix Factorization. In <em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (Toronto, Ontario, Canada). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 3680–3684.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/ICASSP39728.2021.9413927" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP39728.2021.9413927</a>

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yanmeng Wang, Qingjiang Shi, and Tsung-Hui Chang. 2023.

</span>
<span class="ltx_bibblock">Batch Normalization Damages Federated Learning on NON-IID Data: Analysis and Remedy. In <em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 1–5.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP49357.2023.10095399" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP49357.2023.10095399</a>

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib117.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. 2020a.

</span>
<span class="ltx_bibblock">Federated Learning With Differential Privacy: Algorithms and Performance Analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.3.1" class="ltx_emph ltx_font_italic">Trans. Info. For. Sec.</em> 15 (Jan. 2020), 3454–3469.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TIFS.2020.2988575" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TIFS.2020.2988575</a>

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib118.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Mehmet Emre Gursoy, Stacey Truex, and Yanzhao Wu. 2020b.

</span>
<span class="ltx_bibblock">A Framework for Evaluating Client Privacy Leakages in Federated Learning. In <em id="bib.bib118.3.1" class="ltx_emph ltx_font_italic">Computer Security – ESORICS 2020</em>, Liqun Chen, Ninghui Li, Kaitai Liang, and Steve Schneider (Eds.). Springer International Publishing, Cham, 545–566.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weissenbacher et al<span id="bib.bib119.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Davy Weissenbacher, Abeed Sarker, Michael J. Paul, and Graciela Gonzalez-Hernandez. 2018.

</span>
<span class="ltx_bibblock">Overview of the Third Social Media Mining for Health (SMM4H) Shared Tasks at EMNLP 2018. In <em id="bib.bib119.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task</em>. Association for Computational Linguistics, Brussels, Belgium, 13–16.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/W18-5904" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/W18-5904</a>

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiedemann et al<span id="bib.bib120.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Simon Wiedemann, Heiner Kirchhoffer, Stefan Matlage, Paul Haase, Arturo Marban, Talmaj Marinč, David Neumann, Tung Nguyen, Heiko Schwarz, Thomas Wiegand, Detlev Marpe, and Wojciech Samek. 2020a.

</span>
<span class="ltx_bibblock">DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em> 14, 4 (2020), 700–714.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/JSTSP.2020.2969554" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/JSTSP.2020.2969554</a>

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiedemann et al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Simon Wiedemann, Heiner Kirchhoffer, Stefan Matlage, Paul Haase, Arturo Marban, Talmaj Marinč, David Neumann, Tung Nguyen, Heiko Schwarz, Thomas Wiegand, Detlev Marpe, and Wojciech Samek. 2020b.

</span>
<span class="ltx_bibblock">DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em> 14, 4 (2020), 700–714.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/JSTSP.2020.2969554" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/JSTSP.2020.2969554</a>

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib122.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. 2022.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning via knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.3.1" class="ltx_emph ltx_font_italic">Nature Communications</em> 13, 1 (April 2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1038/s41467-022-29763-x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s41467-022-29763-x</a>

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib123.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">MIND: A Large-scale Dataset for News Recommendation. In <em id="bib.bib123.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. Association for Computational Linguistics, Online, 3597–3606.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.331" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2020.acl-main.331</a>

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. 2016.

</span>
<span class="ltx_bibblock">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems. In <em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</em> (San Francisco, California, USA) <em id="bib.bib124.4.2" class="ltx_emph ltx_font_italic">(WSDM ’16)</em>. Association for Computing Machinery, New York, NY, USA, 153–162.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2835776.2835837" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2835776.2835837</a>

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and He (2020)</span>
<span class="ltx_bibblock">
Yuxin Wu and Kaiming He. 2020.

</span>
<span class="ltx_bibblock">Group Normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em> 128, 3 (01 Mar 2020), 742–755.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s11263-019-01198-w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11263-019-01198-w</a>

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib126.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Enyue Yang, Yunfeng Huang, Feng Liang, Weike Pan, and Zhong Ming. 2021.

</span>
<span class="ltx_bibblock">FCMF: Federated collective matrix factorization for heterogeneous collaborative filtering.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.3.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em> 220 (03 2021), 106946.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/j.knosys.2021.106946" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.knosys.2021.106946</a>

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yelp (2021)</span>
<span class="ltx_bibblock">
Yelp. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Yelp Dataset</em>.

</span>
<span class="ltx_bibblock">Yelp Inc.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.yelp.com/dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.yelp.com/dataset</a>

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018.

</span>
<span class="ltx_bibblock">Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In <em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (London, United Kingdom) <em id="bib.bib128.4.2" class="ltx_emph ltx_font_italic">(KDD ’18)</em>. Association for Computing Machinery, New York, NY, USA, 974–983.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3219819.3219890" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3219819.3219890</a>

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span id="bib.bib129.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Kai Yue, Richeng Jin, Chau-Wai Wong, Dror Baron, and Huaiyu Dai. 2022.

</span>
<span class="ltx_bibblock">Gradient Obfuscation Gives a False Sense of Security in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2206.04055 (June 2022).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2206.04055" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2206.04055</a>
arXiv:2206.04055 [cs.CR]

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaccone et al<span id="bib.bib130.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Riccardo Zaccone, Andrea Rizzardi, Debora Caldarola, Marco Ciccone, and Barbara Caputo. 2022.

</span>
<span class="ltx_bibblock">Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients. In <em id="bib.bib130.3.1" class="ltx_emph ltx_font_italic">2022 26th International Conference on Pattern Recognition (ICPR)</em> (Montréal, Québec, Canada). Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 3376–3382.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICPR56361.2022.9956084" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICPR56361.2022.9956084</a>

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2022.

</span>
<span class="ltx_bibblock">LightFR: Lightweight Federated Recommendation with Privacy-Preserving Matrix Factorization.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Inf. Syst.</em> 41, 2 (Dec. 2022).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3578361" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3578361</a>

</span>
<span class="ltx_bibblock">Just Accepted.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Jiang (2021)</span>
<span class="ltx_bibblock">
JianFei Zhang and YuChen Jiang. 2021.

</span>
<span class="ltx_bibblock">A vertical federation recommendation method based on clustering and latent factor model. In <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">2021 International Conference on Electronic Information Engineering and Computer Science (EIECS)</em>. Institute of Electrical and Electronics Engineers (IEEE), 3 Park Avenue, 17th Floor, New York, NY 10016-5997 USA, 362–366.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/EIECS53707.2021.9587935" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/EIECS53707.2021.9587935</a>

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020.

</span>
<span class="ltx_bibblock">iDLG: Improved Deep Leakage from Gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> abs/2001.02610 (Jan. 2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2001.02610" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2001.02610</a>
arXiv:2001.02610 [cs.LG]

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. 2021.

</span>
<span class="ltx_bibblock">Federated Learning on Non-IID Data: A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.3.1" class="ltx_emph ltx_font_italic">Neurocomput.</em> 465, C (Nov. 2021), 371–390.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.neucom.2021.07.098" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.neucom.2021.07.098</a>

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep Leakage from Gradients. In <em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc., Vancouver, British Columbia, Canada.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.neurips.cc/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf</a>

</span>
</li>
</ul>
</section>
<section id="id45" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="id45.45" class="ltx_glossarylist">
<dt id="id1.1.id1" class="ltx_glossaryentry">ANE</dt>
<dd>Apple Neural Engine</dd>
<dt id="id2.2.id2" class="ltx_glossaryentry">BatchNorm</dt>
<dd>batch normalization</dd>
<dt id="id3.3.id3" class="ltx_glossaryentry">BLSTM</dt>
<dd>bi-directional long short-term memory</dd>
<dt id="id4.4.id4" class="ltx_glossaryentry">BRNN</dt>
<dd>bi-directional recurrent neural network</dd>
<dt id="id5.5.id5" class="ltx_glossaryentry">CelebA</dt>
<dd>Large-scale CelebFaces Attributes Dataset</dd>
<dt id="id6.6.id6" class="ltx_glossaryentry">CNN</dt>
<dd>convolutional neural network</dd>
<dt id="id7.7.id7" class="ltx_glossaryentry">DeepCABAC</dt>
<dd>Deep Context-Adaptive Binary Arithmetic Coding</dd>
<dt id="id8.8.id8" class="ltx_glossaryentry">DL</dt>
<dd>deep learning</dd>
<dt id="id9.9.id9" class="ltx_glossaryentry">DNN</dt>
<dd>deep neural network</dd>
<dt id="id10.10.id10" class="ltx_glossaryentry">EMNIST</dt>
<dd>Extended MNIST</dd>
<dt id="id11.11.id11" class="ltx_glossaryentry">EU</dt>
<dd>European Union</dd>
<dt id="id12.12.id12" class="ltx_glossaryentry">FedAvg</dt>
<dd>federated averaging</dd>
<dt id="id13.13.id13" class="ltx_glossaryentry">FedCat</dt>
<dd>federated learning via device concatenation</dd>
<dt id="id14.14.id14" class="ltx_glossaryentry">FedDC</dt>
<dd>federated daisy-chaining</dd>
<dt id="id15.15.id15" class="ltx_glossaryentry">FedQ</dt>
<dd>federated learning with client queuing</dd>
<dt id="id16.16.id16" class="ltx_glossaryentry">FedRec</dt>
<dd>federated recommender system</dd>
<dt id="id17.17.id17" class="ltx_glossaryentry">FedSeq</dt>
<dd>federated learning via sequential superclients training</dd>
<dt id="id18.18.id18" class="ltx_glossaryentry">FedSGD</dt>
<dd>federated stochastic gradient descent</dd>
<dt id="id19.19.id19" class="ltx_glossaryentry">FEMNIST</dt>
<dd>Federated EMNIST</dd>
<dt id="id20.20.id20" class="ltx_glossaryentry">FL</dt>
<dd>federated learning</dd>
<dt id="id21.21.id21" class="ltx_glossaryentry">GCN</dt>
<dd>graph convolutional network</dd>
<dt id="id22.22.id22" class="ltx_glossaryentry">GDPR</dt>
<dd>General Data Protection Regulation</dd>
<dt id="id23.23.id23" class="ltx_glossaryentry">GloVe</dt>
<dd>Global Vectors for Word Representation</dd>
<dt id="id24.24.id24" class="ltx_glossaryentry">GroupNorm</dt>
<dd>group normalization</dd>
<dt id="id25.25.id25" class="ltx_glossaryentry">GRU</dt>
<dd>gated recurrent unit</dd>
<dt id="id26.26.id26" class="ltx_glossaryentry">i.i.d.</dt>
<dd>independent and identically distributed</dd>
<dt id="id27.27.id27" class="ltx_glossaryentry">LSTM</dt>
<dd>long short-term memory</dd>
<dt id="id28.28.id28" class="ltx_glossaryentry">MAML</dt>
<dd>model-agnostic meta-learning</dd>
<dt id="id29.29.id29" class="ltx_glossaryentry">ML</dt>
<dd>machine learning</dd>
<dt id="id30.30.id30" class="ltx_glossaryentry">MNIST</dt>
<dd>Modified NIST Database</dd>
<dt id="id31.31.id31" class="ltx_glossaryentry">MSE</dt>
<dd>mean squared error</dd>
<dt id="id32.32.id32" class="ltx_glossaryentry">NCF</dt>
<dd>neural collaborative filtering</dd>
<dt id="id33.33.id33" class="ltx_glossaryentry">NIST</dt>
<dd>National Institute of Standards and Technology</dd>
<dt id="id34.34.id34" class="ltx_glossaryentry">NN</dt>
<dd>neural network</dd>
<dt id="id35.35.id35" class="ltx_glossaryentry">NNC</dt>
<dd>neural network coding</dd>
<dt id="id36.36.id36" class="ltx_glossaryentry">PSNR</dt>
<dd>peak signal-to-noise ratio</dd>
<dt id="id37.37.id37" class="ltx_glossaryentry">QP</dt>
<dd>quantization parameter</dd>
<dt id="id38.38.id38" class="ltx_glossaryentry">RecSys</dt>
<dd>recommender system</dd>
<dt id="id39.39.id39" class="ltx_glossaryentry">ReLU</dt>
<dd>rectified linear unit</dd>
<dt id="id40.40.id40" class="ltx_glossaryentry">RNN</dt>
<dd>recurrent neural network</dd>
<dt id="id41.41.id41" class="ltx_glossaryentry">SGD</dt>
<dd>stochastic gradient descent</dd>
<dt id="id42.42.id42" class="ltx_glossaryentry">SoC</dt>
<dd>System on a Chip</dd>
<dt id="id43.43.id43" class="ltx_glossaryentry">TMDb</dt>
<dd>The Movie Database</dd>
<dt id="id44.44.id44" class="ltx_glossaryentry">UML</dt>
<dd>Unified Modeling Language</dd>
<dt id="id45.45.id45" class="ltx_glossaryentry">VoD</dt>
<dd>video-on-demand</dd>
</dl>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Federated Learning Simulator</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The unique requirements for the <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a> presented in this work, especially the considerable number of <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients involved, render its experimental evaluation exceedingly difficult. Performing the experiments under real-world conditions, i.e., deploying real devices that communicate with the central server via a network connection, was infeasible. Simulating the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> process is common in research, since most of the time the algorithmic and methodological underpinnings of the process are to be researched. Simulating an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> system with the required number of clients proved to be challenging, however, as simultaneously keeping all clients, their local datasets, and their local models in main memory is impractical. This meant that some concessions had to be made in order to be able to perform the simulations.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">The first and most obvious concession is that the clients must be strictly trained sequentially, which increases training times considerably, but allows for the training of the clients on limited computing resources. This enables the simulator to run on hardware whose processing capabilities allow for the training of at least one client.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">The second concession is that the clients cannot remain in main memory at the same time. In fact, because of the potential high amount of data involved in the training process, intermediate results can also not be stored on hard disks. <span id="A1.p3.1.1" class="ltx_ERROR undefined">\Ac</span>fl clients are usually comprised of the following data: a local dataset, a local model, and the data required by the optimizer. At the very least, an optimizer must store the gradient of the loss function with respect to the weights, whose size is equal to the size of the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model itself. Furthermore, some optimization algorithms require the storage of additional information. For example, the Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib56" title="" class="ltx_ref">2015</a>)</cite> also stores estimates of the first and second moments of the gradient, which are both equal in size to the gradient. Finally, the central server must store the global model, as well as all client updates, which are each equal in size to the global model. Although a single <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model is only tens of Megabytes in size, this can add up to an inhibitive amount of data. For example, in the federated training of the candidate generator <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model with more than 162.000 clients, the amount of storage required for all clients adds up to approximately 32 terabytes when using the regular <a href="#id41.41.id41"><abbr href="#id41.41.id41" title="stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGD</span></abbr></a> optimizer <cite class="ltx_cite ltx_citemacro_citep">(Robbins and Monro, <a href="#bib.bib95" title="" class="ltx_ref">1951</a>; Kiefer and Wolfowitz, <a href="#bib.bib54" title="" class="ltx_ref">1952</a>)</cite> and approximately 52 terabytes when using the Adam optimizer. Considering these memory requirements, it becomes obvious that this is not a viable option for the simulation.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">The <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> simulator employs multiple simple improvements to circumvent the need to keep all data in the main memory simultaneously. Since the clients are trained sequentially, they do not have to keep their local datasets and models in memory simultaneously. At the beginning of each training round, the central server sends the parameters of the global model to the current client, which will load its local dataset and instantiate a local model using the parameters of the global model before starting training. After the local training has finished, the client sends the updated parameters of its local model back to the central server and frees up the memory resources occupied by its local dataset and model.</p>
</div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.1" class="ltx_p">The second improvement is to not store all client updates on the central server and only aggregate them when all client updates have been received. Instead, a cumulative mean is kept in memory, which is updated each time the central server receives an update from a client. Since the client updates are weighted by the amount of data each client has trained on, this means that the central server must ask all participating clients at the beginning of each training round to reveal the size of their local dataset. In a real-world scenario, each client would send this information when it sends its training updates to the central server, but to be able to keep a cumulative mean, the server must know the local dataset sizes of all clients in advance. The central server then computes the percentage of training data every client contributes to the overall training, which it uses as the weights for the cumulative mean. When a client finishes its local training and sends the training updates back to the central server, the central server multiplies the received update by the weight of the respective client and adds it to the cumulative mean. After all clients have finished their training and the central server has aggregated all their contributions, the cumulative mean is equal to the actual mean of the client updates, which is then used as the updated parametrization of the global model.</p>
</div>
<div id="A1.p6" class="ltx_para">
<p id="A1.p6.1" class="ltx_p">Finally, the last improvement is to only use the <a href="#id41.41.id41"><abbr href="#id41.41.id41" title="stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGD</span></abbr></a> optimizer, as it is stateless and requires no memory at all beyond having to store the gradient. However, since the gradient is volatile and only needs to be stored until it has been applied to the weights of the local model, its memory footprint is as low as possible. Other optimizers, such as Adam, need to store further information, which cannot be discarded and must be kept in memory for the entire duration of the federated training, thus rendering these optimizers unusable. Depending on the model and the training objective, the choice of optimizer can influence convergence time, as well as the final performance of the model. In the present case <a href="#id41.41.id41"><abbr href="#id41.41.id41" title="stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGD</span></abbr></a> can train the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> models to the same level of performance in a similar amount of time as Adam.</p>
</div>
<div id="A1.p7" class="ltx_para">
<p id="A1.p7.1" class="ltx_p">Employed in unison, these improvements not only bring the computational requirements down to a manageable level, but they also reduce the memory footprint to a small fraction of the theoretical requirements. In fact, only the validation dataset and model of the central server, the cumulative mean of the client updates, the local training dataset and model of the current client, and the gradient calculated by the current client’s optimizer will ever be in memory at the same time. This reduces the memory requirements of the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> simulator from several tens of terabytes down to a few gigabytes.</p>
</div>
<div id="A1.p8" class="ltx_para">
<p id="A1.p8.1" class="ltx_p">The temporal complexity of the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> simulator is still relatively high, however. Although a round of local training of a single client only requires a couple of minutes, this adds up to a substantial amount of training time considering that the federated training procedure must be repeated dozens or even hundreds of times. This is a problem that is also faced by real-world <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> systems and is usually solved by client sub-sampling <cite class="ltx_cite ltx_citemacro_citep">(Fraboni et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, i.e., only selecting a small random subset of clients from the client population for each round of training. This is also the solution that we have chosen: For each round of federated training, we only select between 100 and 10,000 from the more than 162,000 clients. In conclusion, all these measures make it possible to train the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a> models using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> in a matter of a few days.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Candidate Generator Experiments</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this appendix we will perform various experiments to determine the optimal model architecture for the candidate generator model described in Section <a href="#S3.SS3" title="3.3. Candidate Generation ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, and then provide a detailed explanation of the chosen <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1. </span>Model Type Experiment</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">There are many possible architectures for candidate generator models based on <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">NNs</span></abbr></a> ranging from simple <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">DNNs</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> and <a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Choe et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> to more elaborate autoencoder architectures <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2016</a>)</cite>. Since the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model will be trained using <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the size of the model is a crucial factor. Mobile devices, such as smartphones, are likely candidates for training the <a href="#id38.38.id38"><abbr href="#id38.38.id38" title="recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RecSys</span></abbr></a>, as smartphones are the most used devices to watch online <a href="#id45.45.id45"><abbr href="#id45.45.id45" title="video-on-demand" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VoD</span></abbr></a> content <cite class="ltx_cite ltx_citemacro_citep">(Stoll, <a href="#bib.bib108" title="" class="ltx_ref">2022</a>)</cite>. Although some modern smartphones even have dedicated hardware for <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> training and inference<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>For example, the  <a href="#id1.1.id1"><span href="#id1.1.id1" title="Apple Neural Engine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Apple Neural Engine</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Apple Neural Engine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ANE</span></abbr></a>) introduced with the iPhone X’s A11  <a href="#id42.42.id42"><span href="#id42.42.id42" title="System on a Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">System on a Chip</span></span></a> (<a href="#id42.42.id42"><abbr href="#id42.42.id42" title="System on a Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SoC</span></abbr></a>) and Google’s Tensor <a href="#id42.42.id42"><abbr href="#id42.42.id42" title="System on a Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SoC</span></abbr></a> introduced with the Pixel 6 line of smartphones.</span></span></span>, they are still very resource-constrained as compared to contemporary <a href="#id29.29.id29"><abbr href="#id29.29.id29" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr></a> hardware. Therefore, only the simplest architectures can be considered for the candidate generator. The most basic <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture are feed-forward fully-connected <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">DNNs</span></abbr></a>. However, as the candidate generator will be trained on time-series data, <a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> would be a more appropriate choice. Therefore, an experiment with a simple feed-forward fully-connected architecture and multiple simple recurrent architectures, including plain <a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Sherstinsky, <a href="#bib.bib105" title="" class="ltx_ref">2020</a>)</cite>, <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a href="#bib.bib45" title="" class="ltx_ref">1997</a>)</cite> networks, and <a href="#id25.25.id25"><span href="#id25.25.id25" title="gated recurrent unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">gated recurrent units</span></span></a> <cite class="ltx_cite ltx_citemacro_citep">(Cho et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite>, was conducted. The recurrent architectures were all trained as both unidirectional and bidirectional models. The results of this experiment are shown in Figure <a href="#A2.F15" title="Figure 15 ‣ B.1. Model Type Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>.</p>
</div>
<figure id="A2.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x20.png" id="A2.F15.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="181" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>. </span><span id="A2.F15.3.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy results vs. number of epochs for different candidate generator model types.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A2.F15.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A2.F15.5" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy results vs. number of epochs for different candidate generator model types.</p>
</div>
</div>
</figure>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">The <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> and the <a href="#id25.25.id25"><abbr href="#id25.25.id25" title="gated recurrent unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GRU</span></abbr></a> have the worst average performance of the tested model architectures. They clearly show the pitfalls of recurrent architectures: Although the best-performing recurrent architectures reach best-in-class performances, they are tricky to train and show a large variance in training performance. Surprisingly, the <a href="#id40.40.id40"><abbr href="#id40.40.id40" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> are the highest performing among the recurrent architectures. Generally, the bi-directional versions of the recurrent architectures outperform their unidirectional counterpart. The feed-forward fully-connected model (denoted as <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a>) reaches an acceptable performance, which is almost as high as that of the  <a href="#id4.4.id4"><span href="#id4.4.id4" title="bi-directional recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">bi-directional recurrent neural network</span></span></a> (<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="bi-directional recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BRNN</span></abbr></a>) or the  <a href="#id3.3.id3"><span href="#id3.3.id3" title="bi-directional long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">bi-directional long short-term memory</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="bi-directional long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BLSTM</span></abbr></a>). Just considering the performance of the tested architectures, the <a href="#id4.4.id4"><abbr href="#id4.4.id4" title="bi-directional recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BRNN</span></abbr></a> should be favored, but it also has its downsides: (1) it is the slowest to converge with an average wall clock time of roughly 200 hours as compared to an average wall clock time of roughly 55 hours for the <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a>, which is almost 4 times as long, and (2) the complexity of the two architectures differs significantly, while the <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> only has 17,994,852 trainable parameters, the <a href="#id4.4.id4"><abbr href="#id4.4.id4" title="bi-directional recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BRNN</span></abbr></a> has 128,494,436 trainable parameters, which is more than 7 times as many. The same is true for the <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="bi-directional long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BLSTM</span></abbr></a>: It is much slower to converge in terms of wall clock time and is significantly larger. Especially considering that the model must be trained on resource-constrained devices, the simpler but also well-performing <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr></a> architecture was selected.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2. </span>Movie Embedding Layer Size Experiment</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">The size of the embedding vectors has a substantial impact on the classification result: they cannot fully capture the latent information from the data when they are too small. Additionally, there is a computational cost and a risk of overfitting when they are too large, which means that more data (or regularization) is needed to properly train the model. We determined the optimal size of the embedding vectors experimentally by testing different sizes, as shown in Figure <a href="#A2.F16" title="Figure 16 ‣ B.2. Movie Embedding Layer Size Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. The results demonstrate, that increasing the size of the movie embedding vectors directly results in a performance gain, but the return on investment falls off quickly: While doubling from a size of 32 to a size of 64 results in a sizable performance increase of roughly 0.83 percentage points on average, doubling it again to 128 only yields a rise of roughly 0.12 percentage points on average. This means that a 64-dimensional embedding vector provides the best trade-off between performance and model size.</p>
</div>
<figure id="A2.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x21.png" id="A2.F16.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="181" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>. </span><span id="A2.F16.3.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy results vs. number of epochs for different numbers of dimensions of the movie embedding layer in the candidate generator model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A2.F16.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A2.F16.5" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy results vs. number of epochs for different numbers of dimensions of the movie embedding layer in the candidate generator model.</p>
</div>
</div>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3. </span>Number of Hidden Layers Experiment</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">Likewise, the number of hidden layers in the candidate generator model also impacts both the performance, as well as the size of the resulting model. We performed an experiment with varying numbers of hidden layers. The results are shown in Figure <a href="#A2.F17" title="Figure 17 ‣ B.3. Number of Hidden Layers Experiment ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, giving an optimum of a 3-layer configuration, as both increasing and decreasing the number of hidden layers results in inferior performance.</p>
</div>
<figure id="A2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x22.png" id="A2.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="181" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F17.2.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>. </span><span id="A2.F17.3.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy results vs. number of epochs for different numbers of hidden layers of the candidate generator model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A2.F17.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A2.F17.5" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy results vs. number of epochs for different numbers of hidden layers of the candidate generator model.</p>
</div>
</div>
</figure>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4. </span>Candidate Generator Model Architecture</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p id="A2.SS4.p1.1" class="ltx_p">The final <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture that was chosen for the candidate generator has a 64-dimensional embedding layer for the movies in the watch history inputs, followed by three hidden fully-connected layers, which are each followed by a normalization layer and a <a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a> activation. The hidden layers with their normalization layers and <a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a> activations are then followed by an output fully-connected layer, which feeds its logits into a softmax.</p>
</div>
<div id="A2.SS4.p2" class="ltx_para">
<p id="A2.SS4.p2.1" class="ltx_p">Ever since its introduction, <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a href="#bib.bib48" title="" class="ltx_ref">2015</a>)</cite> has been a mainstay in deep learning. Today it is used in a wide variety of <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architectures. Unfortunately, <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> also comes with some drawbacks. First and foremost, <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> normalizes along the batch dimension, which causes problems with small batch sizes as the estimation of the statistics of a batch become more error prone the smaller the batch size becomes, which can make the training unstable. Especially in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> the clients tend to use small batch sizes because of the limited computing power. <span id="A2.SS4.p2.1.1" class="ltx_ERROR undefined">\Ac</span>groupnorm <cite class="ltx_cite ltx_citemacro_citep">(Wu and He, <a href="#bib.bib125" title="" class="ltx_ref">2020</a>)</cite> was introduced to deal with this problem. Instead of estimating the mean and variance of the data based on the batches, it divides the data into groups and measures the statistics within these groups. This makes  <a href="#id24.24.id24"><span href="#id24.24.id24" title="group normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">group normalization</span></span></a> (<a href="#id24.24.id24"><abbr href="#id24.24.id24" title="group normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GroupNorm</span></abbr></a>) independent of the batch size.</p>
</div>
<div id="A2.SS4.p3" class="ltx_para">
<p id="A2.SS4.p3.4" class="ltx_p">Secondly, <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> keeps, besides its two trainable parameters <math id="A2.SS4.p3.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="A2.SS4.p3.1.m1.1a"><mi id="A2.SS4.p3.1.m1.1.1" xref="A2.SS4.p3.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="A2.SS4.p3.1.m1.1b"><ci id="A2.SS4.p3.1.m1.1.1.cmml" xref="A2.SS4.p3.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p3.1.m1.1c">\gamma</annotation></semantics></math> and <math id="A2.SS4.p3.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.SS4.p3.2.m2.1a"><mi id="A2.SS4.p3.2.m2.1.1" xref="A2.SS4.p3.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.SS4.p3.2.m2.1b"><ci id="A2.SS4.p3.2.m2.1.1.cmml" xref="A2.SS4.p3.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p3.2.m2.1c">\beta</annotation></semantics></math>, a running average of the mean and the variance of the batches. This makes it complicated to use in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, as the running averages of the mean and the variance cannot be simply averaged. <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a href="#bib.bib71" title="" class="ltx_ref">2021</a>)</cite> propose to only communicate the trainable parameters of <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a>, i.e., <math id="A2.SS4.p3.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="A2.SS4.p3.3.m3.1a"><mi id="A2.SS4.p3.3.m3.1.1" xref="A2.SS4.p3.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="A2.SS4.p3.3.m3.1b"><ci id="A2.SS4.p3.3.m3.1.1.cmml" xref="A2.SS4.p3.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p3.3.m3.1c">\gamma</annotation></semantics></math> and <math id="A2.SS4.p3.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.SS4.p3.4.m4.1a"><mi id="A2.SS4.p3.4.m4.1.1" xref="A2.SS4.p3.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.SS4.p3.4.m4.1b"><ci id="A2.SS4.p3.4.m4.1.1.cmml" xref="A2.SS4.p3.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p3.4.m4.1c">\beta</annotation></semantics></math>, to the central server for aggregation but keep the running average of the batch statistics local. <span id="A2.SS4.p3.4.1" class="ltx_ERROR undefined">\Ac</span>groupnorm, however, has the edge over <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> in this case, as it does not keep a running average of the data statistics and instead always estimates them from the current input.</p>
</div>
<div id="A2.SS4.p4" class="ltx_para">
<p id="A2.SS4.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib116" title="" class="ltx_ref">Wang et al<span class="ltx_text">.</span></a></cite> have performed a convergence analysis and were able to show that, although several schemes have been proposed to remedy the problems of <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, most of them still suffer a loss in performance due to the fact that a mismatch between the local and global statistics, incurred by non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data distributions, causes a gradient deviation, which in turn leads the model to converge to a biased solution with a slower rate. To avoid all of the above-mentioned problems, we have decided to use <a href="#id24.24.id24"><abbr href="#id24.24.id24" title="group normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GroupNorm</span></abbr></a> for all <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments and <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> for all non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments. A detailed breakdown of the layers that comprise the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture of the candidate generator model is presented in Table <a href="#A2.T3" title="Table 3 ‣ B.4. Candidate Generator Model Architecture ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="A2.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A2.T3.21" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A2.T3.21.22" class="ltx_tr">
<td id="A2.T3.21.22.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t"><span id="A2.T3.21.22.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Type</span></td>
<td id="A2.T3.21.22.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A2.T3.21.22.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Shape</span></td>
<td id="A2.T3.21.22.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A2.T3.21.22.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Parameters</span></td>
</tr>
<tr id="A2.T3.1.1" class="ltx_tr">
<td id="A2.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t"><span id="A2.T3.1.1.2.1" class="ltx_text" style="font-size:90%;">Embedding Layer</span></td>
<td id="A2.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="A2.T3.1.1.1.m1.2" class="ltx_Math" alttext="53,797\times 64" display="inline"><semantics id="A2.T3.1.1.1.m1.2a"><mrow id="A2.T3.1.1.1.m1.2.2.1" xref="A2.T3.1.1.1.m1.2.2.2.cmml"><mn mathsize="90%" id="A2.T3.1.1.1.m1.1.1" xref="A2.T3.1.1.1.m1.1.1.cmml">53</mn><mo mathsize="90%" id="A2.T3.1.1.1.m1.2.2.1.2" xref="A2.T3.1.1.1.m1.2.2.2.cmml">,</mo><mrow id="A2.T3.1.1.1.m1.2.2.1.1" xref="A2.T3.1.1.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A2.T3.1.1.1.m1.2.2.1.1.2" xref="A2.T3.1.1.1.m1.2.2.1.1.2.cmml">797</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.T3.1.1.1.m1.2.2.1.1.1" xref="A2.T3.1.1.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A2.T3.1.1.1.m1.2.2.1.1.3" xref="A2.T3.1.1.1.m1.2.2.1.1.3.cmml">64</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.m1.2b"><list id="A2.T3.1.1.1.m1.2.2.2.cmml" xref="A2.T3.1.1.1.m1.2.2.1"><cn type="integer" id="A2.T3.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1">53</cn><apply id="A2.T3.1.1.1.m1.2.2.1.1.cmml" xref="A2.T3.1.1.1.m1.2.2.1.1"><times id="A2.T3.1.1.1.m1.2.2.1.1.1.cmml" xref="A2.T3.1.1.1.m1.2.2.1.1.1"></times><cn type="integer" id="A2.T3.1.1.1.m1.2.2.1.1.2.cmml" xref="A2.T3.1.1.1.m1.2.2.1.1.2">797</cn><cn type="integer" id="A2.T3.1.1.1.m1.2.2.1.1.3.cmml" xref="A2.T3.1.1.1.m1.2.2.1.1.3">64</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.m1.2c">53,797\times 64</annotation></semantics></math></td>
<td id="A2.T3.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A2.T3.1.1.3.1" class="ltx_text" style="font-size:90%;">3,443,008</span></td>
</tr>
<tr id="A2.T3.2.2" class="ltx_tr">
<td id="A2.T3.2.2.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.2.2.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.2.2.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A2.T3.2.2.1" class="ltx_td ltx_align_left">
<span id="A2.T3.2.2.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A2.T3.2.2.1.m1.2" class="ltx_Math" alttext="1,024\times 64" display="inline"><semantics id="A2.T3.2.2.1.m1.2a"><mrow id="A2.T3.2.2.1.m1.2.2.1" xref="A2.T3.2.2.1.m1.2.2.2.cmml"><mn mathsize="90%" id="A2.T3.2.2.1.m1.1.1" xref="A2.T3.2.2.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.2.2.1.m1.2.2.1.2" xref="A2.T3.2.2.1.m1.2.2.2.cmml">,</mo><mrow id="A2.T3.2.2.1.m1.2.2.1.1" xref="A2.T3.2.2.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A2.T3.2.2.1.m1.2.2.1.1.2" xref="A2.T3.2.2.1.m1.2.2.1.1.2.cmml">024</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.T3.2.2.1.m1.2.2.1.1.1" xref="A2.T3.2.2.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A2.T3.2.2.1.m1.2.2.1.1.3" xref="A2.T3.2.2.1.m1.2.2.1.1.3.cmml">64</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.1.m1.2b"><list id="A2.T3.2.2.1.m1.2.2.2.cmml" xref="A2.T3.2.2.1.m1.2.2.1"><cn type="integer" id="A2.T3.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.1.m1.1.1">1</cn><apply id="A2.T3.2.2.1.m1.2.2.1.1.cmml" xref="A2.T3.2.2.1.m1.2.2.1.1"><times id="A2.T3.2.2.1.m1.2.2.1.1.1.cmml" xref="A2.T3.2.2.1.m1.2.2.1.1.1"></times><cn type="integer" id="A2.T3.2.2.1.m1.2.2.1.1.2.cmml" xref="A2.T3.2.2.1.m1.2.2.1.1.2">024</cn><cn type="integer" id="A2.T3.2.2.1.m1.2.2.1.1.3.cmml" xref="A2.T3.2.2.1.m1.2.2.1.1.3">64</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.1.m1.2c">1,024\times 64</annotation></semantics></math>
</td>
<td id="A2.T3.2.2.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.2.2.3.1" class="ltx_text" style="font-size:90%;">66,560</span></td>
</tr>
<tr id="A2.T3.3.3" class="ltx_tr">
<td id="A2.T3.3.3.1" class="ltx_td ltx_align_left">
<span id="A2.T3.3.3.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A2.T3.3.3.1.m1.2" class="ltx_Math" alttext="1,024" display="inline"><semantics id="A2.T3.3.3.1.m1.2a"><mrow id="A2.T3.3.3.1.m1.2.3.2" xref="A2.T3.3.3.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.3.3.1.m1.1.1" xref="A2.T3.3.3.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.3.3.1.m1.2.3.2.1" xref="A2.T3.3.3.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.3.3.1.m1.2.2" xref="A2.T3.3.3.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.1.m1.2b"><list id="A2.T3.3.3.1.m1.2.3.1.cmml" xref="A2.T3.3.3.1.m1.2.3.2"><cn type="integer" id="A2.T3.3.3.1.m1.1.1.cmml" xref="A2.T3.3.3.1.m1.1.1">1</cn><cn type="integer" id="A2.T3.3.3.1.m1.2.2.cmml" xref="A2.T3.3.3.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.1.m1.2c">1,024</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.4.4" class="ltx_tr">
<td id="A2.T3.4.4.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.4.4.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.4.4.2.2" class="ltx_text" style="font-size:90%;"><a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> Layer</span>
</td>
<td id="A2.T3.4.4.1" class="ltx_td ltx_align_left">
<span id="A2.T3.4.4.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.4.4.1.m1.2" class="ltx_Math" alttext="1,024" display="inline"><semantics id="A2.T3.4.4.1.m1.2a"><mrow id="A2.T3.4.4.1.m1.2.3.2" xref="A2.T3.4.4.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.4.4.1.m1.1.1" xref="A2.T3.4.4.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.4.4.1.m1.2.3.2.1" xref="A2.T3.4.4.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.4.4.1.m1.2.2" xref="A2.T3.4.4.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.1.m1.2b"><list id="A2.T3.4.4.1.m1.2.3.1.cmml" xref="A2.T3.4.4.1.m1.2.3.2"><cn type="integer" id="A2.T3.4.4.1.m1.1.1.cmml" xref="A2.T3.4.4.1.m1.1.1">1</cn><cn type="integer" id="A2.T3.4.4.1.m1.2.2.cmml" xref="A2.T3.4.4.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.1.m1.2c">1,024</annotation></semantics></math>
</td>
<td id="A2.T3.4.4.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.4.4.3.1" class="ltx_text" style="font-size:90%;">2,048</span></td>
</tr>
<tr id="A2.T3.5.5" class="ltx_tr">
<td id="A2.T3.5.5.1" class="ltx_td ltx_align_left">
<span id="A2.T3.5.5.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.5.5.1.m1.2" class="ltx_Math" alttext="1,024" display="inline"><semantics id="A2.T3.5.5.1.m1.2a"><mrow id="A2.T3.5.5.1.m1.2.3.2" xref="A2.T3.5.5.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.5.5.1.m1.1.1" xref="A2.T3.5.5.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.5.5.1.m1.2.3.2.1" xref="A2.T3.5.5.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.5.5.1.m1.2.2" xref="A2.T3.5.5.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.1.m1.2b"><list id="A2.T3.5.5.1.m1.2.3.1.cmml" xref="A2.T3.5.5.1.m1.2.3.2"><cn type="integer" id="A2.T3.5.5.1.m1.1.1.cmml" xref="A2.T3.5.5.1.m1.1.1">1</cn><cn type="integer" id="A2.T3.5.5.1.m1.2.2.cmml" xref="A2.T3.5.5.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.1.m1.2c">1,024</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.23" class="ltx_tr">
<td id="A2.T3.21.23.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="3">
<span id="A2.T3.21.23.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A2.T3.21.23.1.2" class="ltx_text" style="font-size:90%;">or </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.23.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.6.6" class="ltx_tr">
<td id="A2.T3.6.6.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2"><span id="A2.T3.6.6.2.1" class="ltx_text" style="font-size:90%;">GroupNorm Layer (32 Groups)</span></td>
<td id="A2.T3.6.6.1" class="ltx_td ltx_align_left">
<span id="A2.T3.6.6.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.6.6.1.m1.2" class="ltx_Math" alttext="1,024" display="inline"><semantics id="A2.T3.6.6.1.m1.2a"><mrow id="A2.T3.6.6.1.m1.2.3.2" xref="A2.T3.6.6.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.6.6.1.m1.1.1" xref="A2.T3.6.6.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.6.6.1.m1.2.3.2.1" xref="A2.T3.6.6.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.6.6.1.m1.2.2" xref="A2.T3.6.6.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.1.m1.2b"><list id="A2.T3.6.6.1.m1.2.3.1.cmml" xref="A2.T3.6.6.1.m1.2.3.2"><cn type="integer" id="A2.T3.6.6.1.m1.1.1.cmml" xref="A2.T3.6.6.1.m1.1.1">1</cn><cn type="integer" id="A2.T3.6.6.1.m1.2.2.cmml" xref="A2.T3.6.6.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.1.m1.2c">1,024</annotation></semantics></math>
</td>
<td id="A2.T3.6.6.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.6.6.3.1" class="ltx_text" style="font-size:90%;">2,048</span></td>
</tr>
<tr id="A2.T3.7.7" class="ltx_tr">
<td id="A2.T3.7.7.1" class="ltx_td ltx_align_left">
<span id="A2.T3.7.7.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.7.7.1.m1.2" class="ltx_Math" alttext="1,024" display="inline"><semantics id="A2.T3.7.7.1.m1.2a"><mrow id="A2.T3.7.7.1.m1.2.3.2" xref="A2.T3.7.7.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.7.7.1.m1.1.1" xref="A2.T3.7.7.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A2.T3.7.7.1.m1.2.3.2.1" xref="A2.T3.7.7.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.7.7.1.m1.2.2" xref="A2.T3.7.7.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.7.7.1.m1.2b"><list id="A2.T3.7.7.1.m1.2.3.1.cmml" xref="A2.T3.7.7.1.m1.2.3.2"><cn type="integer" id="A2.T3.7.7.1.m1.1.1.cmml" xref="A2.T3.7.7.1.m1.1.1">1</cn><cn type="integer" id="A2.T3.7.7.1.m1.2.2.cmml" xref="A2.T3.7.7.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.7.1.m1.2c">1,024</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.24" class="ltx_tr">
<td id="A2.T3.21.24.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.21.24.1.1" class="ltx_text" style="font-size:90%;">        </span><a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a><span id="A2.T3.21.24.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.24.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.8.8" class="ltx_tr">
<td id="A2.T3.8.8.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.8.8.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.8.8.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A2.T3.8.8.1" class="ltx_td ltx_align_left">
<span id="A2.T3.8.8.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A2.T3.8.8.1.m1.2" class="ltx_Math" alttext="512\times 1,024" display="inline"><semantics id="A2.T3.8.8.1.m1.2a"><mrow id="A2.T3.8.8.1.m1.2.2.1" xref="A2.T3.8.8.1.m1.2.2.2.cmml"><mrow id="A2.T3.8.8.1.m1.2.2.1.1" xref="A2.T3.8.8.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A2.T3.8.8.1.m1.2.2.1.1.2" xref="A2.T3.8.8.1.m1.2.2.1.1.2.cmml">512</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.T3.8.8.1.m1.2.2.1.1.1" xref="A2.T3.8.8.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A2.T3.8.8.1.m1.2.2.1.1.3" xref="A2.T3.8.8.1.m1.2.2.1.1.3.cmml">1</mn></mrow><mo mathsize="90%" id="A2.T3.8.8.1.m1.2.2.1.2" xref="A2.T3.8.8.1.m1.2.2.2.cmml">,</mo><mn mathsize="90%" id="A2.T3.8.8.1.m1.1.1" xref="A2.T3.8.8.1.m1.1.1.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.8.8.1.m1.2b"><list id="A2.T3.8.8.1.m1.2.2.2.cmml" xref="A2.T3.8.8.1.m1.2.2.1"><apply id="A2.T3.8.8.1.m1.2.2.1.1.cmml" xref="A2.T3.8.8.1.m1.2.2.1.1"><times id="A2.T3.8.8.1.m1.2.2.1.1.1.cmml" xref="A2.T3.8.8.1.m1.2.2.1.1.1"></times><cn type="integer" id="A2.T3.8.8.1.m1.2.2.1.1.2.cmml" xref="A2.T3.8.8.1.m1.2.2.1.1.2">512</cn><cn type="integer" id="A2.T3.8.8.1.m1.2.2.1.1.3.cmml" xref="A2.T3.8.8.1.m1.2.2.1.1.3">1</cn></apply><cn type="integer" id="A2.T3.8.8.1.m1.1.1.cmml" xref="A2.T3.8.8.1.m1.1.1">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.8.8.1.m1.2c">512\times 1,024</annotation></semantics></math>
</td>
<td id="A2.T3.8.8.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.8.8.3.1" class="ltx_text" style="font-size:90%;">524,800</span></td>
</tr>
<tr id="A2.T3.9.9" class="ltx_tr">
<td id="A2.T3.9.9.1" class="ltx_td ltx_align_left">
<span id="A2.T3.9.9.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A2.T3.9.9.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A2.T3.9.9.1.m1.1a"><mn mathsize="90%" id="A2.T3.9.9.1.m1.1.1" xref="A2.T3.9.9.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A2.T3.9.9.1.m1.1b"><cn type="integer" id="A2.T3.9.9.1.m1.1.1.cmml" xref="A2.T3.9.9.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.9.9.1.m1.1c">512</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.10.10" class="ltx_tr">
<td id="A2.T3.10.10.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.10.10.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.10.10.2.2" class="ltx_text" style="font-size:90%;"><a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> Layer</span>
</td>
<td id="A2.T3.10.10.1" class="ltx_td ltx_align_left">
<span id="A2.T3.10.10.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.10.10.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A2.T3.10.10.1.m1.1a"><mn mathsize="90%" id="A2.T3.10.10.1.m1.1.1" xref="A2.T3.10.10.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A2.T3.10.10.1.m1.1b"><cn type="integer" id="A2.T3.10.10.1.m1.1.1.cmml" xref="A2.T3.10.10.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.10.10.1.m1.1c">512</annotation></semantics></math>
</td>
<td id="A2.T3.10.10.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.10.10.3.1" class="ltx_text" style="font-size:90%;">1,024</span></td>
</tr>
<tr id="A2.T3.11.11" class="ltx_tr">
<td id="A2.T3.11.11.1" class="ltx_td ltx_align_left">
<span id="A2.T3.11.11.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.11.11.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A2.T3.11.11.1.m1.1a"><mn mathsize="90%" id="A2.T3.11.11.1.m1.1.1" xref="A2.T3.11.11.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A2.T3.11.11.1.m1.1b"><cn type="integer" id="A2.T3.11.11.1.m1.1.1.cmml" xref="A2.T3.11.11.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.11.11.1.m1.1c">512</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.25" class="ltx_tr">
<td id="A2.T3.21.25.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="3">
<span id="A2.T3.21.25.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A2.T3.21.25.1.2" class="ltx_text" style="font-size:90%;">or </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.25.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.12.12" class="ltx_tr">
<td id="A2.T3.12.12.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2"><span id="A2.T3.12.12.2.1" class="ltx_text" style="font-size:90%;">GroupNorm Layer (32 Groups)</span></td>
<td id="A2.T3.12.12.1" class="ltx_td ltx_align_left">
<span id="A2.T3.12.12.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.12.12.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A2.T3.12.12.1.m1.1a"><mn mathsize="90%" id="A2.T3.12.12.1.m1.1.1" xref="A2.T3.12.12.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A2.T3.12.12.1.m1.1b"><cn type="integer" id="A2.T3.12.12.1.m1.1.1.cmml" xref="A2.T3.12.12.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.12.12.1.m1.1c">512</annotation></semantics></math>
</td>
<td id="A2.T3.12.12.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.12.12.3.1" class="ltx_text" style="font-size:90%;">1,024</span></td>
</tr>
<tr id="A2.T3.13.13" class="ltx_tr">
<td id="A2.T3.13.13.1" class="ltx_td ltx_align_left">
<span id="A2.T3.13.13.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.13.13.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A2.T3.13.13.1.m1.1a"><mn mathsize="90%" id="A2.T3.13.13.1.m1.1.1" xref="A2.T3.13.13.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A2.T3.13.13.1.m1.1b"><cn type="integer" id="A2.T3.13.13.1.m1.1.1.cmml" xref="A2.T3.13.13.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.13.13.1.m1.1c">512</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.26" class="ltx_tr">
<td id="A2.T3.21.26.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.21.26.1.1" class="ltx_text" style="font-size:90%;">        </span><a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a><span id="A2.T3.21.26.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.26.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.14.14" class="ltx_tr">
<td id="A2.T3.14.14.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.14.14.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.14.14.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A2.T3.14.14.1" class="ltx_td ltx_align_left">
<span id="A2.T3.14.14.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A2.T3.14.14.1.m1.1" class="ltx_Math" alttext="256\times 512" display="inline"><semantics id="A2.T3.14.14.1.m1.1a"><mrow id="A2.T3.14.14.1.m1.1.1" xref="A2.T3.14.14.1.m1.1.1.cmml"><mn mathsize="90%" id="A2.T3.14.14.1.m1.1.1.2" xref="A2.T3.14.14.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.T3.14.14.1.m1.1.1.1" xref="A2.T3.14.14.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="A2.T3.14.14.1.m1.1.1.3" xref="A2.T3.14.14.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.14.14.1.m1.1b"><apply id="A2.T3.14.14.1.m1.1.1.cmml" xref="A2.T3.14.14.1.m1.1.1"><times id="A2.T3.14.14.1.m1.1.1.1.cmml" xref="A2.T3.14.14.1.m1.1.1.1"></times><cn type="integer" id="A2.T3.14.14.1.m1.1.1.2.cmml" xref="A2.T3.14.14.1.m1.1.1.2">256</cn><cn type="integer" id="A2.T3.14.14.1.m1.1.1.3.cmml" xref="A2.T3.14.14.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.14.14.1.m1.1c">256\times 512</annotation></semantics></math>
</td>
<td id="A2.T3.14.14.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.14.14.3.1" class="ltx_text" style="font-size:90%;">131,328</span></td>
</tr>
<tr id="A2.T3.15.15" class="ltx_tr">
<td id="A2.T3.15.15.1" class="ltx_td ltx_align_left">
<span id="A2.T3.15.15.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A2.T3.15.15.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A2.T3.15.15.1.m1.1a"><mn mathsize="90%" id="A2.T3.15.15.1.m1.1.1" xref="A2.T3.15.15.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A2.T3.15.15.1.m1.1b"><cn type="integer" id="A2.T3.15.15.1.m1.1.1.cmml" xref="A2.T3.15.15.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.15.15.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.16.16" class="ltx_tr">
<td id="A2.T3.16.16.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.16.16.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.16.16.2.2" class="ltx_text" style="font-size:90%;"><a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> Layer</span>
</td>
<td id="A2.T3.16.16.1" class="ltx_td ltx_align_left">
<span id="A2.T3.16.16.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.16.16.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A2.T3.16.16.1.m1.1a"><mn mathsize="90%" id="A2.T3.16.16.1.m1.1.1" xref="A2.T3.16.16.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A2.T3.16.16.1.m1.1b"><cn type="integer" id="A2.T3.16.16.1.m1.1.1.cmml" xref="A2.T3.16.16.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.16.16.1.m1.1c">256</annotation></semantics></math>
</td>
<td id="A2.T3.16.16.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.16.16.3.1" class="ltx_text" style="font-size:90%;">512</span></td>
</tr>
<tr id="A2.T3.17.17" class="ltx_tr">
<td id="A2.T3.17.17.1" class="ltx_td ltx_align_left">
<span id="A2.T3.17.17.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.17.17.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A2.T3.17.17.1.m1.1a"><mn mathsize="90%" id="A2.T3.17.17.1.m1.1.1" xref="A2.T3.17.17.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A2.T3.17.17.1.m1.1b"><cn type="integer" id="A2.T3.17.17.1.m1.1.1.cmml" xref="A2.T3.17.17.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.17.17.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.27" class="ltx_tr">
<td id="A2.T3.21.27.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="3">
<span id="A2.T3.21.27.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A2.T3.21.27.1.2" class="ltx_text" style="font-size:90%;">or </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.27.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.18.18" class="ltx_tr">
<td id="A2.T3.18.18.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2"><span id="A2.T3.18.18.2.1" class="ltx_text" style="font-size:90%;">GroupNorm Layer (32 Groups)</span></td>
<td id="A2.T3.18.18.1" class="ltx_td ltx_align_left">
<span id="A2.T3.18.18.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A2.T3.18.18.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A2.T3.18.18.1.m1.1a"><mn mathsize="90%" id="A2.T3.18.18.1.m1.1.1" xref="A2.T3.18.18.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A2.T3.18.18.1.m1.1b"><cn type="integer" id="A2.T3.18.18.1.m1.1.1.cmml" xref="A2.T3.18.18.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.18.18.1.m1.1c">256</annotation></semantics></math>
</td>
<td id="A2.T3.18.18.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.18.18.3.1" class="ltx_text" style="font-size:90%;">512</span></td>
</tr>
<tr id="A2.T3.19.19" class="ltx_tr">
<td id="A2.T3.19.19.1" class="ltx_td ltx_align_left">
<span id="A2.T3.19.19.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A2.T3.19.19.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A2.T3.19.19.1.m1.1a"><mn mathsize="90%" id="A2.T3.19.19.1.m1.1.1" xref="A2.T3.19.19.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A2.T3.19.19.1.m1.1b"><cn type="integer" id="A2.T3.19.19.1.m1.1.1.cmml" xref="A2.T3.19.19.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.19.19.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.28" class="ltx_tr">
<td id="A2.T3.21.28.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.21.28.1.1" class="ltx_text" style="font-size:90%;">        </span><a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a><span id="A2.T3.21.28.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.28.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.20.20" class="ltx_tr">
<td id="A2.T3.20.20.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.20.20.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A2.T3.20.20.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A2.T3.20.20.1" class="ltx_td ltx_align_left">
<span id="A2.T3.20.20.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A2.T3.20.20.1.m1.2" class="ltx_Math" alttext="53,796\times 256" display="inline"><semantics id="A2.T3.20.20.1.m1.2a"><mrow id="A2.T3.20.20.1.m1.2.2.1" xref="A2.T3.20.20.1.m1.2.2.2.cmml"><mn mathsize="90%" id="A2.T3.20.20.1.m1.1.1" xref="A2.T3.20.20.1.m1.1.1.cmml">53</mn><mo mathsize="90%" id="A2.T3.20.20.1.m1.2.2.1.2" xref="A2.T3.20.20.1.m1.2.2.2.cmml">,</mo><mrow id="A2.T3.20.20.1.m1.2.2.1.1" xref="A2.T3.20.20.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A2.T3.20.20.1.m1.2.2.1.1.2" xref="A2.T3.20.20.1.m1.2.2.1.1.2.cmml">796</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.T3.20.20.1.m1.2.2.1.1.1" xref="A2.T3.20.20.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A2.T3.20.20.1.m1.2.2.1.1.3" xref="A2.T3.20.20.1.m1.2.2.1.1.3.cmml">256</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.20.20.1.m1.2b"><list id="A2.T3.20.20.1.m1.2.2.2.cmml" xref="A2.T3.20.20.1.m1.2.2.1"><cn type="integer" id="A2.T3.20.20.1.m1.1.1.cmml" xref="A2.T3.20.20.1.m1.1.1">53</cn><apply id="A2.T3.20.20.1.m1.2.2.1.1.cmml" xref="A2.T3.20.20.1.m1.2.2.1.1"><times id="A2.T3.20.20.1.m1.2.2.1.1.1.cmml" xref="A2.T3.20.20.1.m1.2.2.1.1.1"></times><cn type="integer" id="A2.T3.20.20.1.m1.2.2.1.1.2.cmml" xref="A2.T3.20.20.1.m1.2.2.1.1.2">796</cn><cn type="integer" id="A2.T3.20.20.1.m1.2.2.1.1.3.cmml" xref="A2.T3.20.20.1.m1.2.2.1.1.3">256</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.20.20.1.m1.2c">53,796\times 256</annotation></semantics></math>
</td>
<td id="A2.T3.20.20.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A2.T3.20.20.3.1" class="ltx_text" style="font-size:90%;">13,825,572</span></td>
</tr>
<tr id="A2.T3.21.21" class="ltx_tr">
<td id="A2.T3.21.21.1" class="ltx_td ltx_align_left">
<span id="A2.T3.21.21.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A2.T3.21.21.1.m1.2" class="ltx_Math" alttext="53,796" display="inline"><semantics id="A2.T3.21.21.1.m1.2a"><mrow id="A2.T3.21.21.1.m1.2.3.2" xref="A2.T3.21.21.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A2.T3.21.21.1.m1.1.1" xref="A2.T3.21.21.1.m1.1.1.cmml">53</mn><mo mathsize="90%" id="A2.T3.21.21.1.m1.2.3.2.1" xref="A2.T3.21.21.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A2.T3.21.21.1.m1.2.2" xref="A2.T3.21.21.1.m1.2.2.cmml">796</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.21.21.1.m1.2b"><list id="A2.T3.21.21.1.m1.2.3.1.cmml" xref="A2.T3.21.21.1.m1.2.3.2"><cn type="integer" id="A2.T3.21.21.1.m1.1.1.cmml" xref="A2.T3.21.21.1.m1.1.1">53</cn><cn type="integer" id="A2.T3.21.21.1.m1.2.2.cmml" xref="A2.T3.21.21.1.m1.2.2">796</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.21.21.1.m1.2c">53,796</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T3.21.29" class="ltx_tr">
<td id="A2.T3.21.29.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A2.T3.21.29.1.1" class="ltx_text" style="font-size:90%;">        Softmax </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A2.T3.21.29.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A2.T3.21.30" class="ltx_tr">
<td id="A2.T3.21.30.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_tt" colspan="2">
<span id="A2.T3.21.30.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A2.T3.21.30.1.2" class="ltx_text" style="font-size:90%;">Total</span>
</td>
<td id="A2.T3.21.30.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">
<span id="A2.T3.21.30.2.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A2.T3.21.30.2.2" class="ltx_text" style="font-size:90%;">17,994,852</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A2.T3.23.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="A2.T3.24.2" class="ltx_text" style="font-size:90%;">A detailed breakdown of the layers that make up the architecture of the candidate generator <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A2.T3.25" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A2.T3.26" class="ltx_p ltx_figure_panel">A detailed breakdown of the layers that make up the architecture of the candidate generator <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model.</p>
</div>
</div>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Ranker Experiments</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In this appendix we will perform various experiments to determine the optimal model architecture for the ranker model described in Section <a href="#S3.SS4" title="3.4. Ranking ‣ 3. Method ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, as well as the loss function that is used for its training. Furthermore, we will provide a detailed explanation of the chosen <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture.</p>
</div>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1. </span>Embedding Layer Sizes Experiment</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">Again, the embedding vector sizes for the three embedding layers for the users, the movies, and the movie genres, must be fine-tuned. Having too large embeddings may result in larger model sizes, overfitting, and longer convergence times. Therefore, we experimentally determined the optimal size of embedding vectors for each embedding layer. As can be seen in Figure <a href="#A3.F18.sf3" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(c)</span></a>, the optimal embedding sizes are 32 for users, 128 for movies, and 16 for genres. In the case of the user and the genre embedding vector sizes, the experiments are clearly determined, as 32 dimensions and 16 dimensions outperform all other embedding vector sizes both in terms of best final accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, as well as in best overall accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, respectively. The results of the movie embedding vector size are a bit ambiguous, as 16 dimensions outperform the other embedding vector sizes in terms of final accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, however, both 128 dimensions and 256 dimensions yield the highest overall accuracies and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">MSEs</span></abbr></a>. To balance accuracy and computational complexity, we selected 128 dimensions with a higher overall accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> than the 16-dimension-case and less computational complexity than the 256-dimension case.</p>
</div>
<figure id="A3.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F18.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x23.png" id="A3.F18.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F18.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A3.F18.sf1.3.2" class="ltx_text" style="font-size:90%;">User Embedding Layer</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F18.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x24.png" id="A3.F18.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F18.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A3.F18.sf2.3.2" class="ltx_text" style="font-size:90%;">Movie Embedding Layer</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F18.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x25.png" id="A3.F18.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F18.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A3.F18.sf3.3.2" class="ltx_text" style="font-size:90%;">Genre Embedding Layer</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F18.7.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>. </span><span id="A3.F18.8.2" class="ltx_text" style="font-size:90%;">Validation accuracy (<span id="A3.F18.8.2.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F18.8.2.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different embedding vector sizes for the three embedding layers in the ranker model: <span id="A3.F18.8.2.3" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf1" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(a)</span></a>)</span> user embedding layer, <span id="A3.F18.8.2.4" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf2" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(b)</span></a>)</span> movie embedding layer, and <span id="A3.F18.8.2.5" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf3" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(c)</span></a>)</span> genre embedding layer. Each graph shows the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="A3.F18.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.F18.10" class="ltx_p ltx_figure_panel ltx_align_center">Validation accuracy (<span id="A3.F18.10.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F18.10.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different embedding vector sizes for the three embedding layers in the ranker model: <span id="A3.F18.10.3" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf1" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(a)</span></a>)</span> user embedding layer, <span id="A3.F18.10.4" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf2" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(b)</span></a>)</span> movie embedding layer, and <span id="A3.F18.10.5" class="ltx_text ltx_font_bold">(<a href="#A3.F18.sf3" title="In Figure 18 ‣ C.1. Embedding Layer Sizes Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18(c)</span></a>)</span> genre embedding layer. Each graph shows the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</p>
</div>
</div>
</figure>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2. </span>Number of Hidden Layers Experiment</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">Similar to the candidate generator, we also determine the optimal number of hidden layers, as shown in Figure <a href="#A3.F19" title="Figure 19 ‣ C.2. Number of Hidden Layers Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>, resulting in an optimal ranker model with 1 hidden layer. The ranker model with 2 layers converges faster than the ranker model with 1 layer, however, at the expense of a lower final accuracy. Using 3 hidden layers already introduces overfitting and lowers the accuracy further. An optimal model with only 1 hidden layer also requires less computational complexity and is thus beneficial in the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> setting.</p>
</div>
<figure id="A3.F19" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x26.png" id="A3.F19.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="363" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F19.4.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>. </span><span id="A3.F19.5.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy (<span id="A3.F19.5.2.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F19.5.2.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different numbers of hidden layers for the ranker model. Each of the graphs show the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A3.F19.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.F19.7" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy (<span id="A3.F19.7.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F19.7.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different numbers of hidden layers for the ranker model. Each of the graphs show the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</p>
</div>
</div>
</figure>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3. </span>Loss Function Experiment</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">As the ranker model is trained to perform a classification task, the softmax cross-entropy loss function can be used. However, unlike in a typical classification problem, we want our prediction to be close to the correct value, even if it is wrong (predicting a rating of 3.5, when the actual ground-truth rating is 4.0 is still better than predicting a rating of 0.5, because the deviation from the true rating is smaller). Therefore, other loss functions such as <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, which penalize both incorrect predictions and the magnitude of the deviation, may be better suited. To determine this, we conducted experiments using softmax cross-entropy, <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>, and the sum of the two to combine the best of both approaches. The results are shown in Figure <a href="#A3.F20" title="Figure 20 ‣ C.3. Loss Function Experiment ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>. Against our expectations, the <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> loss function performs worse than the other two in terms of validation <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>. Here, one would assume that a model optimized on the <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> loss function should perform best when measuring its performance in terms of <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a>. Although using the <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> loss function causes the model to not overfit in terms of accuracy like the other loss functions and outperforms them when measuring the final accuracy of the model, the other loss functions converge faster and achieve a better overall accuracy. The softmax cross-entropy loss function and the sum of both yield a similar accuracy, however the softmax cross-entropy loss function is computationally less complex and was therefore selected.</p>
</div>
<figure id="A3.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2303.04689/assets/x27.png" id="A3.F20.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="363" height="121" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F20.4.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>. </span><span id="A3.F20.5.2" class="ltx_text" style="font-size:90%;">Validation top-100 accuracy (<span id="A3.F20.5.2.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F20.5.2.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different loss functions for training the ranker model. Each of the graphs shows the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A3.F20.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.F20.7" class="ltx_p ltx_figure_panel ltx_align_center">Validation top-100 accuracy (<span id="A3.F20.7.1" class="ltx_text ltx_font_italic">left</span>) and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> results (<span id="A3.F20.7.2" class="ltx_text ltx_font_italic">right</span>) vs. number of epochs for different loss functions for training the ranker model. Each of the graphs shows the minimum and maximum (given by the transparent region), as well as the mean (given by the solid line) of five repetitions of each experiment.</p>
</div>
</div>
</figure>
</section>
<section id="A3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4. </span>Ranker Model Architecture</h3>

<div id="A3.SS4.p1" class="ltx_para">
<p id="A3.SS4.p1.1" class="ltx_p">The final <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture that was chosen for the ranker has a 32-dimensional embedding layer for the user, a 128-dimensional embedding layer for the movie, and a 16-dimensional embedding layer for the genres. The genres are then averaged and all inputs, including the embeddings and the movie age, are concatenated. This is followed by a single hidden fully-connected layer. The output of the hidden layer is normalized using a normalization layer, which is followed by a <a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a> activation. The hidden layer with its normalization layer and <a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a> activation is then followed by an output fully-connected layer, which feeds its logits into a softmax. For the reasons described in Appendix <a href="#A2.SS4" title="B.4. Candidate Generator Model Architecture ‣ Appendix B Candidate Generator Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a>, a <a href="#id24.24.id24"><abbr href="#id24.24.id24" title="group normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GroupNorm</span></abbr></a> layer is used for the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments and a <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> layer is used for the non-<a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> experiments. A detailed breakdown of the layers that comprise the <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> architecture of the ranker model is presented in Table <a href="#A3.T4" title="Table 4 ‣ C.4. Ranker Model Architecture ‣ Appendix C Ranker Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A3.T4" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A3.T4.11" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A3.T4.11.12" class="ltx_tr">
<td id="A3.T4.11.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t"><span id="A3.T4.11.12.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Type</span></td>
<td id="A3.T4.11.12.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T4.11.12.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Shape</span></td>
<td id="A3.T4.11.12.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A3.T4.11.12.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Parameters</span></td>
</tr>
<tr id="A3.T4.1.1" class="ltx_tr">
<td id="A3.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t"><span id="A3.T4.1.1.2.1" class="ltx_text" style="font-size:90%;">User Embedding Layer</span></td>
<td id="A3.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="A3.T4.1.1.1.m1.2" class="ltx_Math" alttext="162,541\times 32" display="inline"><semantics id="A3.T4.1.1.1.m1.2a"><mrow id="A3.T4.1.1.1.m1.2.2.1" xref="A3.T4.1.1.1.m1.2.2.2.cmml"><mn mathsize="90%" id="A3.T4.1.1.1.m1.1.1" xref="A3.T4.1.1.1.m1.1.1.cmml">162</mn><mo mathsize="90%" id="A3.T4.1.1.1.m1.2.2.1.2" xref="A3.T4.1.1.1.m1.2.2.2.cmml">,</mo><mrow id="A3.T4.1.1.1.m1.2.2.1.1" xref="A3.T4.1.1.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A3.T4.1.1.1.m1.2.2.1.1.2" xref="A3.T4.1.1.1.m1.2.2.1.1.2.cmml">541</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A3.T4.1.1.1.m1.2.2.1.1.1" xref="A3.T4.1.1.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A3.T4.1.1.1.m1.2.2.1.1.3" xref="A3.T4.1.1.1.m1.2.2.1.1.3.cmml">32</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.T4.1.1.1.m1.2b"><list id="A3.T4.1.1.1.m1.2.2.2.cmml" xref="A3.T4.1.1.1.m1.2.2.1"><cn type="integer" id="A3.T4.1.1.1.m1.1.1.cmml" xref="A3.T4.1.1.1.m1.1.1">162</cn><apply id="A3.T4.1.1.1.m1.2.2.1.1.cmml" xref="A3.T4.1.1.1.m1.2.2.1.1"><times id="A3.T4.1.1.1.m1.2.2.1.1.1.cmml" xref="A3.T4.1.1.1.m1.2.2.1.1.1"></times><cn type="integer" id="A3.T4.1.1.1.m1.2.2.1.1.2.cmml" xref="A3.T4.1.1.1.m1.2.2.1.1.2">541</cn><cn type="integer" id="A3.T4.1.1.1.m1.2.2.1.1.3.cmml" xref="A3.T4.1.1.1.m1.2.2.1.1.3">32</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.1.1.1.m1.2c">162,541\times 32</annotation></semantics></math></td>
<td id="A3.T4.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A3.T4.1.1.3.1" class="ltx_text" style="font-size:90%;">5,201,312</span></td>
</tr>
<tr id="A3.T4.2.2" class="ltx_tr">
<td id="A3.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_l">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.2.2.2.1" class="ltx_text" style="font-size:90%;">
Movie Embedding Layer</span>
</td>
<td id="A3.T4.2.2.1" class="ltx_td ltx_align_left"><math id="A3.T4.2.2.1.m1.2" class="ltx_Math" alttext="53,796\times 128" display="inline"><semantics id="A3.T4.2.2.1.m1.2a"><mrow id="A3.T4.2.2.1.m1.2.2.1" xref="A3.T4.2.2.1.m1.2.2.2.cmml"><mn mathsize="90%" id="A3.T4.2.2.1.m1.1.1" xref="A3.T4.2.2.1.m1.1.1.cmml">53</mn><mo mathsize="90%" id="A3.T4.2.2.1.m1.2.2.1.2" xref="A3.T4.2.2.1.m1.2.2.2.cmml">,</mo><mrow id="A3.T4.2.2.1.m1.2.2.1.1" xref="A3.T4.2.2.1.m1.2.2.1.1.cmml"><mn mathsize="90%" id="A3.T4.2.2.1.m1.2.2.1.1.2" xref="A3.T4.2.2.1.m1.2.2.1.1.2.cmml">796</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A3.T4.2.2.1.m1.2.2.1.1.1" xref="A3.T4.2.2.1.m1.2.2.1.1.1.cmml">×</mo><mn mathsize="90%" id="A3.T4.2.2.1.m1.2.2.1.1.3" xref="A3.T4.2.2.1.m1.2.2.1.1.3.cmml">128</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.T4.2.2.1.m1.2b"><list id="A3.T4.2.2.1.m1.2.2.2.cmml" xref="A3.T4.2.2.1.m1.2.2.1"><cn type="integer" id="A3.T4.2.2.1.m1.1.1.cmml" xref="A3.T4.2.2.1.m1.1.1">53</cn><apply id="A3.T4.2.2.1.m1.2.2.1.1.cmml" xref="A3.T4.2.2.1.m1.2.2.1.1"><times id="A3.T4.2.2.1.m1.2.2.1.1.1.cmml" xref="A3.T4.2.2.1.m1.2.2.1.1.1"></times><cn type="integer" id="A3.T4.2.2.1.m1.2.2.1.1.2.cmml" xref="A3.T4.2.2.1.m1.2.2.1.1.2">796</cn><cn type="integer" id="A3.T4.2.2.1.m1.2.2.1.1.3.cmml" xref="A3.T4.2.2.1.m1.2.2.1.1.3">128</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.2.2.1.m1.2c">53,796\times 128</annotation></semantics></math></td>
<td id="A3.T4.2.2.3" class="ltx_td ltx_align_right ltx_border_r"><span id="A3.T4.2.2.3.1" class="ltx_text" style="font-size:90%;">6,885,888</span></td>
</tr>
<tr id="A3.T4.3.3" class="ltx_tr">
<td id="A3.T4.3.3.2" class="ltx_td ltx_align_left ltx_border_l">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.3.3.2.1" class="ltx_text" style="font-size:90%;">
Genre Embedding Layer</span>
</td>
<td id="A3.T4.3.3.1" class="ltx_td ltx_align_left"><math id="A3.T4.3.3.1.m1.1" class="ltx_Math" alttext="20\times 16" display="inline"><semantics id="A3.T4.3.3.1.m1.1a"><mrow id="A3.T4.3.3.1.m1.1.1" xref="A3.T4.3.3.1.m1.1.1.cmml"><mn mathsize="90%" id="A3.T4.3.3.1.m1.1.1.2" xref="A3.T4.3.3.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A3.T4.3.3.1.m1.1.1.1" xref="A3.T4.3.3.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="A3.T4.3.3.1.m1.1.1.3" xref="A3.T4.3.3.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T4.3.3.1.m1.1b"><apply id="A3.T4.3.3.1.m1.1.1.cmml" xref="A3.T4.3.3.1.m1.1.1"><times id="A3.T4.3.3.1.m1.1.1.1.cmml" xref="A3.T4.3.3.1.m1.1.1.1"></times><cn type="integer" id="A3.T4.3.3.1.m1.1.1.2.cmml" xref="A3.T4.3.3.1.m1.1.1.2">20</cn><cn type="integer" id="A3.T4.3.3.1.m1.1.1.3.cmml" xref="A3.T4.3.3.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.3.3.1.m1.1c">20\times 16</annotation></semantics></math></td>
<td id="A3.T4.3.3.3" class="ltx_td ltx_align_right ltx_border_r"><span id="A3.T4.3.3.3.1" class="ltx_text" style="font-size:90%;">320</span></td>
</tr>
<tr id="A3.T4.11.13" class="ltx_tr">
<td id="A3.T4.11.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.11.13.1.1" class="ltx_text" style="font-size:90%;">        Genre Embedding Average </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A3.T4.11.13.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A3.T4.11.14" class="ltx_tr">
<td id="A3.T4.11.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.11.14.1.1" class="ltx_text" style="font-size:90%;">        Input Concatenation </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A3.T4.11.14.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A3.T4.4.4" class="ltx_tr">
<td id="A3.T4.4.4.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.4.4.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A3.T4.4.4.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A3.T4.4.4.1" class="ltx_td ltx_align_left">
<span id="A3.T4.4.4.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A3.T4.4.4.1.m1.1" class="ltx_Math" alttext="256\times 177" display="inline"><semantics id="A3.T4.4.4.1.m1.1a"><mrow id="A3.T4.4.4.1.m1.1.1" xref="A3.T4.4.4.1.m1.1.1.cmml"><mn mathsize="90%" id="A3.T4.4.4.1.m1.1.1.2" xref="A3.T4.4.4.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A3.T4.4.4.1.m1.1.1.1" xref="A3.T4.4.4.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="A3.T4.4.4.1.m1.1.1.3" xref="A3.T4.4.4.1.m1.1.1.3.cmml">177</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T4.4.4.1.m1.1b"><apply id="A3.T4.4.4.1.m1.1.1.cmml" xref="A3.T4.4.4.1.m1.1.1"><times id="A3.T4.4.4.1.m1.1.1.1.cmml" xref="A3.T4.4.4.1.m1.1.1.1"></times><cn type="integer" id="A3.T4.4.4.1.m1.1.1.2.cmml" xref="A3.T4.4.4.1.m1.1.1.2">256</cn><cn type="integer" id="A3.T4.4.4.1.m1.1.1.3.cmml" xref="A3.T4.4.4.1.m1.1.1.3">177</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.4.4.1.m1.1c">256\times 177</annotation></semantics></math>
</td>
<td id="A3.T4.4.4.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A3.T4.4.4.3.1" class="ltx_text" style="font-size:90%;">45,568</span></td>
</tr>
<tr id="A3.T4.5.5" class="ltx_tr">
<td id="A3.T4.5.5.1" class="ltx_td ltx_align_left">
<span id="A3.T4.5.5.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A3.T4.5.5.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A3.T4.5.5.1.m1.1a"><mn mathsize="90%" id="A3.T4.5.5.1.m1.1.1" xref="A3.T4.5.5.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A3.T4.5.5.1.m1.1b"><cn type="integer" id="A3.T4.5.5.1.m1.1.1.cmml" xref="A3.T4.5.5.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.5.5.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A3.T4.6.6" class="ltx_tr">
<td id="A3.T4.6.6.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.6.6.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A3.T4.6.6.2.2" class="ltx_text" style="font-size:90%;"><a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> Layer</span>
</td>
<td id="A3.T4.6.6.1" class="ltx_td ltx_align_left">
<span id="A3.T4.6.6.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A3.T4.6.6.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A3.T4.6.6.1.m1.1a"><mn mathsize="90%" id="A3.T4.6.6.1.m1.1.1" xref="A3.T4.6.6.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A3.T4.6.6.1.m1.1b"><cn type="integer" id="A3.T4.6.6.1.m1.1.1.cmml" xref="A3.T4.6.6.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.6.6.1.m1.1c">256</annotation></semantics></math>
</td>
<td id="A3.T4.6.6.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A3.T4.6.6.3.1" class="ltx_text" style="font-size:90%;">512</span></td>
</tr>
<tr id="A3.T4.7.7" class="ltx_tr">
<td id="A3.T4.7.7.1" class="ltx_td ltx_align_left">
<span id="A3.T4.7.7.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A3.T4.7.7.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A3.T4.7.7.1.m1.1a"><mn mathsize="90%" id="A3.T4.7.7.1.m1.1.1" xref="A3.T4.7.7.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A3.T4.7.7.1.m1.1b"><cn type="integer" id="A3.T4.7.7.1.m1.1.1.cmml" xref="A3.T4.7.7.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.7.7.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A3.T4.11.15" class="ltx_tr">
<td id="A3.T4.11.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="3">
<span id="A3.T4.11.15.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A3.T4.11.15.1.2" class="ltx_text" style="font-size:90%;">  or </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A3.T4.11.15.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A3.T4.8.8" class="ltx_tr">
<td id="A3.T4.8.8.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2"><span id="A3.T4.8.8.2.1" class="ltx_text" style="font-size:90%;">GroupNorm Layer (32 Groups)</span></td>
<td id="A3.T4.8.8.1" class="ltx_td ltx_align_left">
<span id="A3.T4.8.8.1.1" class="ltx_text" style="font-size:90%;">Gamma: </span><math id="A3.T4.8.8.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A3.T4.8.8.1.m1.1a"><mn mathsize="90%" id="A3.T4.8.8.1.m1.1.1" xref="A3.T4.8.8.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A3.T4.8.8.1.m1.1b"><cn type="integer" id="A3.T4.8.8.1.m1.1.1.cmml" xref="A3.T4.8.8.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.8.8.1.m1.1c">256</annotation></semantics></math>
</td>
<td id="A3.T4.8.8.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A3.T4.8.8.3.1" class="ltx_text" style="font-size:90%;">512</span></td>
</tr>
<tr id="A3.T4.9.9" class="ltx_tr">
<td id="A3.T4.9.9.1" class="ltx_td ltx_align_left">
<span id="A3.T4.9.9.1.1" class="ltx_text" style="font-size:90%;">Beta: </span><math id="A3.T4.9.9.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="A3.T4.9.9.1.m1.1a"><mn mathsize="90%" id="A3.T4.9.9.1.m1.1.1" xref="A3.T4.9.9.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A3.T4.9.9.1.m1.1b"><cn type="integer" id="A3.T4.9.9.1.m1.1.1.cmml" xref="A3.T4.9.9.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.9.9.1.m1.1c">256</annotation></semantics></math>
</td>
</tr>
<tr id="A3.T4.11.16" class="ltx_tr">
<td id="A3.T4.11.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.11.16.1.1" class="ltx_text" style="font-size:90%;">        </span><a href="#id39.39.id39"><abbr href="#id39.39.id39" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ReLU</span></abbr></a><span id="A3.T4.11.16.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A3.T4.11.16.1.3" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A3.T4.10.10" class="ltx_tr">
<td id="A3.T4.10.10.2" class="ltx_td ltx_align_left ltx_border_l" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.10.10.2.1" class="ltx_text" style="font-size:90%;">
</span><span id="A3.T4.10.10.2.2" class="ltx_text" style="font-size:90%;">Fully-Connected Layer</span>
</td>
<td id="A3.T4.10.10.1" class="ltx_td ltx_align_left">
<span id="A3.T4.10.10.1.1" class="ltx_text" style="font-size:90%;">Weights: </span><math id="A3.T4.10.10.1.m1.1" class="ltx_Math" alttext="10\times 256" display="inline"><semantics id="A3.T4.10.10.1.m1.1a"><mrow id="A3.T4.10.10.1.m1.1.1" xref="A3.T4.10.10.1.m1.1.1.cmml"><mn mathsize="90%" id="A3.T4.10.10.1.m1.1.1.2" xref="A3.T4.10.10.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A3.T4.10.10.1.m1.1.1.1" xref="A3.T4.10.10.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="A3.T4.10.10.1.m1.1.1.3" xref="A3.T4.10.10.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T4.10.10.1.m1.1b"><apply id="A3.T4.10.10.1.m1.1.1.cmml" xref="A3.T4.10.10.1.m1.1.1"><times id="A3.T4.10.10.1.m1.1.1.1.cmml" xref="A3.T4.10.10.1.m1.1.1.1"></times><cn type="integer" id="A3.T4.10.10.1.m1.1.1.2.cmml" xref="A3.T4.10.10.1.m1.1.1.2">10</cn><cn type="integer" id="A3.T4.10.10.1.m1.1.1.3.cmml" xref="A3.T4.10.10.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.10.10.1.m1.1c">10\times 256</annotation></semantics></math>
</td>
<td id="A3.T4.10.10.3" class="ltx_td ltx_align_right ltx_border_r" rowspan="2"><span id="A3.T4.10.10.3.1" class="ltx_text" style="font-size:90%;">2,570</span></td>
</tr>
<tr id="A3.T4.11.11" class="ltx_tr">
<td id="A3.T4.11.11.1" class="ltx_td ltx_align_left">
<span id="A3.T4.11.11.1.1" class="ltx_text" style="font-size:90%;">Bias: </span><math id="A3.T4.11.11.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A3.T4.11.11.1.m1.1a"><mn mathsize="90%" id="A3.T4.11.11.1.m1.1.1" xref="A3.T4.11.11.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A3.T4.11.11.1.m1.1b"><cn type="integer" id="A3.T4.11.11.1.m1.1.1.cmml" xref="A3.T4.11.11.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.T4.11.11.1.m1.1c">10</annotation></semantics></math>
</td>
</tr>
<tr id="A3.T4.11.17" class="ltx_tr">
<td id="A3.T4.11.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" colspan="3">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A3.T4.11.17.1.1" class="ltx_text" style="font-size:90%;">        Softmax </span><span class="ltx_rule" style="width:0.0pt;height:11.6pt;background:black;display:inline-block;"></span><span id="A3.T4.11.17.1.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-5.8pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="A3.T4.11.18" class="ltx_tr">
<td id="A3.T4.11.18.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_tt" colspan="2">
<span id="A3.T4.11.18.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A3.T4.11.18.1.2" class="ltx_text" style="font-size:90%;">Total</span>
</td>
<td id="A3.T4.11.18.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">
<span id="A3.T4.11.18.2.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A3.T4.11.18.2.2" class="ltx_text" style="font-size:90%;">12,136,170</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A3.T4.13.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="A3.T4.14.2" class="ltx_text" style="font-size:90%;">A detailed breakdown of the layers that make up the architecture of the ranker <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A3.T4.15" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.T4.16" class="ltx_p ltx_figure_panel">A detailed breakdown of the layers that make up the architecture of the ranker <a href="#id34.34.id34"><abbr href="#id34.34.id34" title="neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NN</span></abbr></a> model.</p>
</div>
</div>
</figure>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Extended Federated Learning and FedQ Experiment Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Due to the broad range of different numbers of clients for the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments, the complete training graphs are poorly legible and were therefore omitted from Section <a href="#S4.SS3" title="4.3. Federated Learning Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> and Section <a href="#S4.SS4" title="4.4. FedQ Experiments ‣ 4. Experiments ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. Instead, only the final validation top-100 accuracy for the candidate generator experiments, as well as the final accuracy and <a href="#id31.31.id31"><abbr href="#id31.31.id31" title="mean squared error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr></a> for the ranker experiments were presented. For reference, the complete training graphs are included in this appendix as Figure <a href="#A4.F21.sf2" title="In Figure 21 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21(b)</span></a> and Figure <a href="#A4.F22.sf2" title="In Figure 22 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22(b)</span></a>.</p>
</div>
<figure id="A4.F21" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A4.F21.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x28.png" id="A4.F21.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A4.F21.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F21.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x29.png" id="A4.F21.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A4.F21.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.5.1.1" class="ltx_text" style="font-size:90%;">Figure 21</span>. </span><span id="A4.F21.6.2" class="ltx_ERROR undefined">\Ac</span><span id="A4.F21.7.3" class="ltx_text" style="font-size:90%;">fl experiment results for <span id="A4.F21.7.3.1" class="ltx_text ltx_font_bold">(<a href="#A4.F21.sf1" title="In Figure 21 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21(a)</span></a>)</span> the candidate generator and <span id="A4.F21.7.3.2" class="ltx_text ltx_font_bold">(<a href="#A4.F21.sf2" title="In Figure 21 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21(b)</span></a>)</span> the ranker.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><span id="A4.F21.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_3"><span id="A4.F21.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A4.F21.10" class="ltx_p ltx_figure_panel ltx_align_center">fl experiment results for <span id="A4.F21.10.1" class="ltx_text ltx_font_bold">(<a href="#A4.F21.sf1" title="In Figure 21 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21(a)</span></a>)</span> the candidate generator and <span id="A4.F21.10.2" class="ltx_text ltx_font_bold">(<a href="#A4.F21.sf2" title="In Figure 21 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21(b)</span></a>)</span> the ranker.</p>
</div>
</div>
</figure>
<figure id="A4.F22" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A4.F22.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x30.png" id="A4.F22.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A4.F22.sf1.3.2" class="ltx_text" style="font-size:90%;">Candidate generator</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F22.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04689/assets/x31.png" id="A4.F22.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A4.F22.sf2.3.2" class="ltx_text" style="font-size:90%;">Ranker</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.5.1.1" class="ltx_text" style="font-size:90%;">Figure 22</span>. </span><span id="A4.F22.6.2" class="ltx_ERROR undefined">\Ac</span><span id="A4.F22.7.3" class="ltx_text" style="font-size:90%;">fedq experiment results for <span id="A4.F22.7.3.1" class="ltx_text ltx_font_bold">(<a href="#A4.F22.sf1" title="In Figure 22 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22(a)</span></a>)</span> the candidate generator and <span id="A4.F22.7.3.2" class="ltx_text ltx_font_bold">(<a href="#A4.F22.sf2" title="In Figure 22 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22(b)</span></a>)</span> the ranker.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><span id="A4.F22.8" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_3"><span id="A4.F22.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A4.F22.10" class="ltx_p ltx_figure_panel ltx_align_center">fedq experiment results for <span id="A4.F22.10.1" class="ltx_text ltx_font_bold">(<a href="#A4.F22.sf1" title="In Figure 22 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22(a)</span></a>)</span> the candidate generator and <span id="A4.F22.10.2" class="ltx_text ltx_font_bold">(<a href="#A4.F22.sf2" title="In Figure 22 ‣ Appendix D Extended Federated Learning and FedQ Experiment Results ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22(b)</span></a>)</span> the ranker.</p>
</div>
</div>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Validation of FedQ on the LEAF Federated Learning Benchmark</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">Although <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> was developed in the context of a <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a>, it is a much more general algorithm that can be employed in other <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> pipelines that have to deal with small local datasets. To provide further evidence of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>’s efficacy, it was evaluated on LEAF, which is an open-source, modular benchmarking framework for federated settings <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite>. It consists of (1) multiple open-source datasets, (2) reference implementations for common <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> methods, and (3) several metrics that measure the statistical properties of the models that are being trained (e.g., accuracy), as well as metrics that measure properties of the <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> system (e.g., number of communicated bytes and local computation). The reference implementation currently includes scripts for preprocessing the data, the federated optimization algorithms <a href="#id18.18.id18"><abbr href="#id18.18.id18" title="federated stochastic gradient descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></a> and <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>, and one or more model architectures for each of the included datasets. We based the evaluation of <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> on the following datasets contained in LEAF:</p>
</div>
<div id="A5.p2" class="ltx_para">
<ul id="A5.I1" class="ltx_itemize">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<span id="A5.I1.i1.p1.1" class="ltx_ERROR undefined">\Ac</span>
<p id="A5.I1.i1.p1.2" class="ltx_p"><span id="A5.I1.i1.p1.2.1" class="ltx_text ltx_font_bold">femnist</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite> is a dataset that was created by the authors of the LEAF benchmark by partitioning the digit and character images of the  <a href="#id10.10.id10"><span href="#id10.10.id10" title="Extended MNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Extended MNIST</span></span></a> (<a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Extended MNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EMNIST</span></abbr></a>) <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> dataset by the person that wrote it. This partitioning makes the dataset more amenable to <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, since writers can be understood as clients. <span id="A5.I1.i1.p1.2.2" class="ltx_ERROR undefined">\Ac</span>emnist is a dataset that was created from the  <a href="#id33.33.id33"><span href="#id33.33.id33" title="National Institute of Standards and Technology" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">National Institute of Standards and Technology</span></span></a> (<a href="#id33.33.id33"><abbr href="#id33.33.id33" title="National Institute of Standards and Technology" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NIST</span></abbr></a>) Special Database 19 <cite class="ltx_cite ltx_citemacro_citep">(Grother and Hanaoka, <a href="#bib.bib36" title="" class="ltx_ref">1995</a>)</cite>, which is the same database that the popular <a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a> <cite class="ltx_cite ltx_citemacro_citep">(Lecun et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">1998</a>)</cite> is based on. The <a href="#id33.33.id33"><abbr href="#id33.33.id33" title="National Institute of Standards and Technology" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NIST</span></abbr></a> Special Database 19 contains handwritten digits, uppercase letters, and lowercase letters, which is much more data than what is exposed by <a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a>. <span id="A5.I1.i1.p1.2.3" class="ltx_ERROR undefined">\Ac</span>emnist was created in an effort to create a more challenging benchmark dataset by covering all data contained in the <a href="#id33.33.id33"><abbr href="#id33.33.id33" title="National Institute of Standards and Technology" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">NIST</span></abbr></a> Special Database 19, while employing the same conversion paradigm used for <a href="#id30.30.id30"><abbr href="#id30.30.id30" title="Modified NIST Database" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MNIST</span></abbr></a> to stay compatible.</p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<span id="A5.I1.i2.p1.1" class="ltx_ERROR undefined">\Ac</span>
<p id="A5.I1.i2.p1.2" class="ltx_p"><span id="A5.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">celeba</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2015</a>)</cite> is a dataset, which contains images of celebrities that were annotated with 40 attributes, including wearing eyeglasses, wearing a hat, wavy hair, and smiling. For the LEAF benchmark,  <a href="#id5.5.id5"><span href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Large-scale CelebFaces Attributes Dataset</span></span></a> (<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>) was adapted to the federated setting by partitioning it into client datasets based on the celebrity in the image. Furthermore, the classification task was simplified from a multi-label classification task to a binary classification task, which only distinguishes between smiling and not smiling celebrities.</p>
</div>
</li>
<li id="A5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i3.p1" class="ltx_para">
<p id="A5.I1.i3.p1.1" class="ltx_p"><span id="A5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Sentiment140</span> <cite class="ltx_cite ltx_citemacro_citep">(Go et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite> is an automatically generated dataset that contains Twitter messages that are classified as either positive or negative based on the emoticons contained in them. The dataset therefore presents a binary classification sentiment analysis task, where the input is a sequence of words. For the use in the LEAF benchmark, the messages are partitioned, such that each <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client is represented by a different Twitter user.</p>
</div>
</li>
<li id="A5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i4.p1" class="ltx_para">
<p id="A5.I1.i4.p1.1" class="ltx_p"><span id="A5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Reddit</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite> is a dataset that was created by the authors of the LEAF benchmark. They took comments posted on the social network Reddit in December 2017 and preprocessed them by (1) converting all named and numeric HTML character references to their corresponding unicode characters, (2) removing extraneous white spaces, (3) removing non-ASCII characters, (4) replacing URLs, Reddit user names and Subreddit names with special tokens, (5) converting the text to lowercase, and (6) tokenizing it using NLTK’s <cite class="ltx_cite ltx_citemacro_citep">(Loper and Bird, <a href="#bib.bib81" title="" class="ltx_ref">2002</a>)</cite> tweet tokenizer. Furthermore, users that were determined to be bots, or that had less than 5 or more than 1000 comments were removed, along with their comments. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> sub-sampled the dataset for their own experiments, as their reference implementation is not yet capable of training on the complete Reddit dataset. The training task of the dataset is next word prediction with a sequence of previous words as input. Each Reddit user is considered to be an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client.</p>
</div>
</li>
</ul>
</div>
<div id="A5.p3" class="ltx_para">
<p id="A5.p3.1" class="ltx_p">The LEAF benchmark provides two more datasets: <em id="A5.p3.1.1" class="ltx_emph ltx_font_italic">Shakespeare</em> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2016</a>)</cite>, which is a dataset that is based on <em id="A5.p3.1.2" class="ltx_emph ltx_font_italic">“The Complete Works of William Shakespeare”</em> <cite class="ltx_cite ltx_citemacro_citep">(Shakespeare, <a href="#bib.bib103" title="" class="ltx_ref">1994</a>)</cite>, where each speaking role represents an <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client, and a <em id="A5.p3.1.3" class="ltx_emph ltx_font_italic">synthetic</em> dataset that is based on the synthetic dataset proposed by <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a href="#bib.bib69" title="" class="ltx_ref">2020</a>)</cite>. The Shakespeare dataset comprises 4,226,158 samples across 1,129 <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> clients (i.e., speaker roles). On average, each client has 3,743.28 samples with a standard deviation of 6,212.26. <span id="A5.p3.1.4" class="ltx_ERROR undefined">\Ac</span>fedq is specifically tailored towards federated scenarios with small local datasets, therefore, the Shakespeare dataset is inadequate for evaluating <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>’s potency, as almost all clients have plenty of local data (only 8 clients have less than 10 and 114 clients have less than 100 samples in their local datasets). The synthetic dataset was specifically designed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> to create a more challenging task for meta-learning methods, which does not apply to <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>. For these reasons, we decided to only evaluate <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> on the four above-mentioned datasets.</p>
</div>
<div id="A5.p4" class="ltx_para">
<p id="A5.p4.1" class="ltx_p">To conduct the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> benchmark experiments, we used the LEAF reference implementation and integrated <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> as a new federated optimization algorithm. For the <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a> dataset, we use a simple two-layer <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a>, which consists of two convolution layers each followed by a maximum pooling layer, followed by two fully-connected layers. For the <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a> dataset, we utilize a <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a> with four convolution layers, each followed by a <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="batch normalization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BatchNorm</span></abbr></a> and a maximum pooling layer, followed by a single fully-connected layer. For the Sentiment140 dataset, we use a stacked <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> model with an embedding layer that is initialized with 300-dimensional, pre-trained  <a href="#id23.23.id23"><span href="#id23.23.id23" title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Global Vectors for Word Representation</span></span></a> (<a href="#id23.23.id23"><abbr href="#id23.23.id23" title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GloVe</span></abbr></a>) embeddings, followed by two <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> cells and two fully-connected layers. For the Reddit dataset, we rely on a stacked <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> model with an embedding layer that embeds the input words into an 8-dimensional vector space, followed by two <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> cells with dropout and a single fully-connected layer. All of these models are part of the reference implementation of the LEAF benchmark. The use of pre-trained <a href="#id23.23.id23"><abbr href="#id23.23.id23" title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GloVe</span></abbr></a> embeddings in the stacked <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a> model used for the Sentiment140 dataset is an adaptation that we incorporated. The embedding layer of the original reference implementation was randomly initialized and trained on the Sentiment140 dataset using the <a href="#id23.23.id23"><abbr href="#id23.23.id23" title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GloVe</span></abbr></a> vocabulary to embed its input words into a 100-dimensional vector space. Without this adaptation, the model fails to learn anything using the hyperparameters proposed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite>. In fact, the model usually settles in on an accuracy of around 50% after the first round of federated training and more or less keeps that accuracy for the entire duration of the training. For a binary classification task, an accuracy of 50% is not better than random chance. As a matter of fact, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib13" title="" class="ltx_ref">Chen et al<span class="ltx_text">.</span></a></cite> also made this adaptation in their LEAF benchmark experiments.</p>
</div>
<div id="A5.p5" class="ltx_para">
<p id="A5.p5.1" class="ltx_p">The preprocessed and sub-sampled version of the Reddit dataset used by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> was graciously made available for download. All other datasets were preprocessed using the tools provided in the reference implementation of the LEAF benchmark. We used the settings presented in Table <a href="#A5.T5" title="Table 5 ‣ Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The statistics of the resulting datasets can be seen in Table <a href="#A5.T6" title="Table 6 ‣ Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="A5.T5" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A5.T5.2" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A5.T5.2.1" class="ltx_tr">
<td id="A5.T5.2.1.1" class="ltx_td ltx_border_r"></td>
<td id="A5.T5.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T5.2.1.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T5.2.1.2.2" class="ltx_text ltx_font_bold" style="font-size:90%;">femnist</span>
</td>
<td id="A5.T5.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T5.2.1.3.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T5.2.1.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">celeba</span>
</td>
<td id="A5.T5.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T5.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sentiment140</span></td>
</tr>
<tr id="A5.T5.2.2" class="ltx_tr">
<td id="A5.T5.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.T5.2.2.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.2.1.2" class="ltx_text" style="font-size:90%;">Client Sample Distribution   </span><span id="A5.T5.2.2.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">-<span id="A5.T5.2.2.1.3.1" class="ltx_text ltx_lst_identifier">s</span></span>
</td>
<td id="A5.T5.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T5.2.2.2.1" class="ltx_text" style="font-size:90%;">non-</span><a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>
</td>
<td id="A5.T5.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T5.2.2.3.1" class="ltx_text" style="font-size:90%;">non-</span><a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>
</td>
<td id="A5.T5.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T5.2.2.4.1" class="ltx_text" style="font-size:90%;">non-</span><a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a>
</td>
</tr>
<tr id="A5.T5.2.3" class="ltx_tr">
<td id="A5.T5.2.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<span id="A5.T5.2.3.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.3.1.2" class="ltx_text" style="font-size:90%;">Fraction of Data to Sample   </span><span id="A5.T5.2.3.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">–<span id="A5.T5.2.3.1.3.1" class="ltx_text ltx_lst_identifier">sf</span></span>
</td>
<td id="A5.T5.2.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.3.2.1" class="ltx_text" style="font-size:90%;">100%</span></td>
<td id="A5.T5.2.3.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.3.3.1" class="ltx_text" style="font-size:90%;">100%</span></td>
<td id="A5.T5.2.3.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.3.4.1" class="ltx_text" style="font-size:90%;">15%</span></td>
</tr>
<tr id="A5.T5.2.4" class="ltx_tr">
<td id="A5.T5.2.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<span id="A5.T5.2.4.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.4.1.2" class="ltx_text" style="font-size:90%;">Minimum Number of Samples per Client   </span><span id="A5.T5.2.4.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">-<span id="A5.T5.2.4.1.3.1" class="ltx_text ltx_lst_identifier">k</span></span>
</td>
<td id="A5.T5.2.4.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.4.2.1" class="ltx_text" style="font-size:90%;">0</span></td>
<td id="A5.T5.2.4.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.4.3.1" class="ltx_text" style="font-size:90%;">0</span></td>
<td id="A5.T5.2.4.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.4.4.1" class="ltx_text" style="font-size:90%;">0</span></td>
</tr>
<tr id="A5.T5.2.5" class="ltx_tr">
<td id="A5.T5.2.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<span id="A5.T5.2.5.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.5.1.2" class="ltx_text" style="font-size:90%;">Training/Test Data Split Mode   </span><span id="A5.T5.2.5.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">-<span id="A5.T5.2.5.1.3.1" class="ltx_text ltx_lst_identifier">t</span></span>
</td>
<td id="A5.T5.2.5.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.5.2.1" class="ltx_text" style="font-size:90%;">Sample</span></td>
<td id="A5.T5.2.5.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.5.3.1" class="ltx_text" style="font-size:90%;">Sample</span></td>
<td id="A5.T5.2.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.5.4.1" class="ltx_text" style="font-size:90%;">Sample</span></td>
</tr>
<tr id="A5.T5.2.6" class="ltx_tr">
<td id="A5.T5.2.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<span id="A5.T5.2.6.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.6.1.2" class="ltx_text" style="font-size:90%;">Training Data Fraction   </span><span id="A5.T5.2.6.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">–<span id="A5.T5.2.6.1.3.1" class="ltx_text ltx_lst_identifier">tf</span></span>
</td>
<td id="A5.T5.2.6.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.6.2.1" class="ltx_text" style="font-size:90%;">90%</span></td>
<td id="A5.T5.2.6.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.6.3.1" class="ltx_text" style="font-size:90%;">90%</span></td>
<td id="A5.T5.2.6.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.6.4.1" class="ltx_text" style="font-size:90%;">90%</span></td>
</tr>
<tr id="A5.T5.2.7" class="ltx_tr">
<td id="A5.T5.2.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<span id="A5.T5.2.7.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.7.1.2" class="ltx_text" style="font-size:90%;">Sampling Seed   </span><span id="A5.T5.2.7.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">–<span id="A5.T5.2.7.1.3.1" class="ltx_text ltx_lst_identifier">smplseed</span></span>
</td>
<td id="A5.T5.2.7.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.7.2.1" class="ltx_text" style="font-size:90%;">1691607340</span></td>
<td id="A5.T5.2.7.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.7.3.1" class="ltx_text" style="font-size:90%;">1691605746</span></td>
<td id="A5.T5.2.7.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T5.2.7.4.1" class="ltx_text" style="font-size:90%;">1692132357</span></td>
</tr>
<tr id="A5.T5.2.8" class="ltx_tr">
<td id="A5.T5.2.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">
<span id="A5.T5.2.8.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T5.2.8.1.2" class="ltx_text" style="font-size:90%;">Split Seed   </span><span id="A5.T5.2.8.1.3" class="ltx_text ltx_lstlisting" style="font-size:90%;">–<span id="A5.T5.2.8.1.3.1" class="ltx_text ltx_lst_identifier">spltseed</span></span>
</td>
<td id="A5.T5.2.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="A5.T5.2.8.2.1" class="ltx_text" style="font-size:90%;">1691608842</span></td>
<td id="A5.T5.2.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="A5.T5.2.8.3.1" class="ltx_text" style="font-size:90%;">1691605747</span></td>
<td id="A5.T5.2.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="A5.T5.2.8.4.1" class="ltx_text" style="font-size:90%;">1692132372</span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A5.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="A5.T5.4.2" class="ltx_text" style="font-size:90%;">The settings used to preprocess the <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, and Sentiment140 datasets.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A5.T5.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A5.T5.6" class="ltx_p ltx_figure_panel">The settings used to preprocess the <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, and Sentiment140 datasets.</p>
</div>
</div>
</figure>
<figure id="A5.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A5.T6.2" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A5.T6.2.1" class="ltx_tr">
<td id="A5.T6.2.1.1" class="ltx_td ltx_border_r" colspan="2"></td>
<td id="A5.T6.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="A5.T6.2.1.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T6.2.1.2.2" class="ltx_text ltx_font_bold" style="font-size:90%;">femnist</span>
</td>
<td id="A5.T6.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="A5.T6.2.1.3.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T6.2.1.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">celeba</span>
</td>
<td id="A5.T6.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T6.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sentiment140</span></td>
<td id="A5.T6.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T6.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Reddit</span></td>
</tr>
<tr id="A5.T6.2.2" class="ltx_tr">
<td id="A5.T6.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" colspan="2" rowspan="2"><span id="A5.T6.2.2.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T6.2.2.1.1.1" class="ltx_ERROR undefined">\libertineSB</span>Number of Clients</span></td>
<td id="A5.T6.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T6.2.2.2.1" class="ltx_text" style="font-size:90%;">3,597</span></td>
<td id="A5.T6.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T6.2.2.3.1" class="ltx_text" style="font-size:90%;">9,343</span></td>
<td id="A5.T6.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;"><span id="A5.T6.2.2.4.1" class="ltx_text" style="font-size:90%;">99,149</span></td>
<td id="A5.T6.2.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;"><span id="A5.T6.2.2.5.1" class="ltx_text" style="font-size:90%;">817</span></td>
</tr>
<tr id="A5.T6.2.3" class="ltx_tr">
<td id="A5.T6.2.3.1" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:2.0pt;"><span id="A5.T6.2.3.1.1" class="ltx_text" style="font-size:80%;color:#404040;">(of 660,120)</span></td>
<td id="A5.T6.2.3.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:2.0pt;"><span id="A5.T6.2.3.2.1" class="ltx_text" style="font-size:80%;color:#404040;">(of 1,660,820)</span></td>
</tr>
<tr id="A5.T6.2.4" class="ltx_tr">
<td id="A5.T6.2.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:-3.0pt;" colspan="2" rowspan="2"><span id="A5.T6.2.4.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T6.2.4.1.1.1" class="ltx_ERROR undefined">\libertineSB</span>Number of Samples</span></td>
<td id="A5.T6.2.4.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T6.2.4.2.1" class="ltx_text" style="font-size:90%;">817,851</span></td>
<td id="A5.T6.2.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T6.2.4.3.1" class="ltx_text" style="font-size:90%;">200,288</span></td>
<td id="A5.T6.2.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;"><span id="A5.T6.2.4.4.1" class="ltx_text" style="font-size:90%;">240,074</span></td>
<td id="A5.T6.2.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;"><span id="A5.T6.2.4.5.1" class="ltx_text" style="font-size:90%;">55,556</span></td>
</tr>
<tr id="A5.T6.2.5" class="ltx_tr">
<td id="A5.T6.2.5.1" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:2.0pt;"><span id="A5.T6.2.5.1.1" class="ltx_text" style="font-size:80%;color:#404040;">(of 1,600,498)</span></td>
<td id="A5.T6.2.5.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:2.0pt;"><span id="A5.T6.2.5.2.1" class="ltx_text" style="font-size:80%;color:#404040;">(of 56,587,343)</span></td>
</tr>
<tr id="A5.T6.2.6" class="ltx_tr">
<td id="A5.T6.2.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" rowspan="4">
<span class="ltx_rule" style="width:100%;height:0.1pt;background:black;display:inline-block;"> </span><span id="A5.T6.2.6.1.1" class="ltx_text" style="font-size:90%;">
</span><span id="A5.T6.2.6.1.2" class="ltx_text" style="font-size:90%;"><span id="A5.T6.2.6.1.2.1" class="ltx_ERROR undefined">\libertineSB</span>Samples per Client</span>
</td>
<td id="A5.T6.2.6.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T6.2.6.2.1" class="ltx_text" style="font-size:90%;">Minimum</span></td>
<td id="A5.T6.2.6.3" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.6.3.1" class="ltx_text" style="font-size:90%;">19</span></td>
<td id="A5.T6.2.6.4" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.6.4.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="A5.T6.2.6.5" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.6.5.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="A5.T6.2.6.6" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.6.6.1" class="ltx_text" style="font-size:90%;">10</span></td>
</tr>
<tr id="A5.T6.2.7" class="ltx_tr">
<td id="A5.T6.2.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T6.2.7.1.1" class="ltx_text" style="font-size:90%;">Maximum</span></td>
<td id="A5.T6.2.7.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.7.2.1" class="ltx_text" style="font-size:90%;">584</span></td>
<td id="A5.T6.2.7.3" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.7.3.1" class="ltx_text" style="font-size:90%;">35</span></td>
<td id="A5.T6.2.7.4" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.7.4.1" class="ltx_text" style="font-size:90%;">236</span></td>
<td id="A5.T6.2.7.5" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.7.5.1" class="ltx_text" style="font-size:90%;">1,394</span></td>
</tr>
<tr id="A5.T6.2.8" class="ltx_tr">
<td id="A5.T6.2.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T6.2.8.1.1" class="ltx_text" style="font-size:90%;">Mean</span></td>
<td id="A5.T6.2.8.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.8.2.1" class="ltx_text" style="font-size:90%;">227.37</span></td>
<td id="A5.T6.2.8.3" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.8.3.1" class="ltx_text" style="font-size:90%;">21.44</span></td>
<td id="A5.T6.2.8.4" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.8.4.1" class="ltx_text" style="font-size:90%;">2.42</span></td>
<td id="A5.T6.2.8.5" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T6.2.8.5.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
</tr>
<tr id="A5.T6.2.9" class="ltx_tr">
<td id="A5.T6.2.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="A5.T6.2.9.1.1" class="ltx_text" style="font-size:90%;">Standard Deviation</span></td>
<td id="A5.T6.2.9.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A5.T6.2.9.2.1" class="ltx_text" style="font-size:90%;">88.84</span></td>
<td id="A5.T6.2.9.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A5.T6.2.9.3.1" class="ltx_text" style="font-size:90%;">7.63</span></td>
<td id="A5.T6.2.9.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A5.T6.2.9.4.1" class="ltx_text" style="font-size:90%;">4.63</span></td>
<td id="A5.T6.2.9.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A5.T6.2.9.5.1" class="ltx_text" style="font-size:90%;">120.27</span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A5.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>. </span><span id="A5.T6.4.2" class="ltx_text" style="font-size:90%;">The dataset statistics of the preprocessed <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, Sentiment140, and Reddit datasets.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A5.T6.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A5.T6.6" class="ltx_p ltx_figure_panel">The dataset statistics of the preprocessed <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, Sentiment140, and Reddit datasets.</p>
</div>
</div>
</figure>
<div id="A5.p6" class="ltx_para">
<p id="A5.p6.1" class="ltx_p">All models were trained using the default random seeds. Most of the remaining hyperparameters, however, deviate from the hyperparameters suggested by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite>. Especially the number of clients per communication round was increased to facilitate different queue lengths for <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>. The hyperparameters used for each dataset are specified in Table <a href="#A5.T7" title="Table 7 ‣ Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The experiments for each dataset were repeated three times, once with <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> as a baseline against which <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> can be compared, once with <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> and a queue length of 10, and once with <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> and a queue length of 100. The results of the experiments can be seen in Figure <a href="#A5.F23" title="Figure 23 ‣ Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>.</p>
</div>
<figure id="A5.T7" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A5.T7.2" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A5.T7.2.1" class="ltx_tr">
<td id="A5.T7.2.1.1" class="ltx_td ltx_border_r"></td>
<td id="A5.T7.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="A5.T7.2.1.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T7.2.1.2.2" class="ltx_text ltx_font_bold" style="font-size:90%;">femnist</span>
</td>
<td id="A5.T7.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="A5.T7.2.1.3.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T7.2.1.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">celeba</span>
</td>
<td id="A5.T7.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T7.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sentiment140</span></td>
<td id="A5.T7.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T7.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Reddit</span></td>
</tr>
<tr id="A5.T7.2.2" class="ltx_tr">
<td id="A5.T7.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;">
<span id="A5.T7.2.2.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T7.2.2.1.2" class="ltx_text" style="font-size:90%;">Communication Rounds</span>
</td>
<td id="A5.T7.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.2.2.1" class="ltx_text" style="font-size:90%;">400</span></td>
<td id="A5.T7.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.2.3.1" class="ltx_text" style="font-size:90%;">400</span></td>
<td id="A5.T7.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.2.4.1" class="ltx_text" style="font-size:90%;">400</span></td>
<td id="A5.T7.2.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.2.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="A5.T7.2.3" class="ltx_tr">
<td id="A5.T7.2.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:3.0pt;"><span id="A5.T7.2.3.1.1" class="ltx_text ltx_lstlisting" style="font-size:80%;">–<span id="A5.T7.2.3.1.1.1" class="ltx_text ltx_lst_identifier">num</span>-<span id="A5.T7.2.3.1.1.2" class="ltx_text ltx_lst_identifier">rounds</span></span></td>
</tr>
<tr id="A5.T7.2.4" class="ltx_tr">
<td id="A5.T7.2.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:-3.0pt;">
<span id="A5.T7.2.4.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T7.2.4.1.2" class="ltx_text" style="font-size:90%;">Clients per Communication Round</span>
</td>
<td id="A5.T7.2.4.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.4.2.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="A5.T7.2.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.4.3.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="A5.T7.2.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.4.4.1" class="ltx_text" style="font-size:90%;">1,000</span></td>
<td id="A5.T7.2.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.4.5.1" class="ltx_text" style="font-size:90%;">500</span></td>
</tr>
<tr id="A5.T7.2.5" class="ltx_tr">
<td id="A5.T7.2.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:3.0pt;"><span id="A5.T7.2.5.1.1" class="ltx_text ltx_lstlisting" style="font-size:80%;">–<span id="A5.T7.2.5.1.1.1" class="ltx_text ltx_lst_identifier">clients</span>-<span id="A5.T7.2.5.1.1.2" class="ltx_text ltx_lst_identifier">per</span>-<span id="A5.T7.2.5.1.1.3" class="ltx_text ltx_lst_identifier">round</span></span></td>
</tr>
<tr id="A5.T7.2.6" class="ltx_tr">
<td id="A5.T7.2.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:-3.0pt;">
<span id="A5.T7.2.6.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T7.2.6.1.2" class="ltx_text" style="font-size:90%;">Learning Rate</span>
</td>
<td id="A5.T7.2.6.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.6.2.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="A5.T7.2.6.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.6.3.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="A5.T7.2.6.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.6.4.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="A5.T7.2.6.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.6.5.1" class="ltx_text" style="font-size:90%;">8.0</span></td>
</tr>
<tr id="A5.T7.2.7" class="ltx_tr">
<td id="A5.T7.2.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:3.0pt;"><span id="A5.T7.2.7.1.1" class="ltx_text ltx_lstlisting" style="font-size:80%;">-<span id="A5.T7.2.7.1.1.1" class="ltx_text ltx_lst_identifier">lr</span></span></td>
</tr>
<tr id="A5.T7.2.8" class="ltx_tr">
<td id="A5.T7.2.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:-3.0pt;">
<span id="A5.T7.2.8.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T7.2.8.1.2" class="ltx_text" style="font-size:90%;">Batch Size</span>
</td>
<td id="A5.T7.2.8.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.8.2.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="A5.T7.2.8.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.8.3.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="A5.T7.2.8.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.8.4.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="A5.T7.2.8.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.8.5.1" class="ltx_text" style="font-size:90%;">5</span></td>
</tr>
<tr id="A5.T7.2.9" class="ltx_tr">
<td id="A5.T7.2.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:3.0pt;"><span id="A5.T7.2.9.1.1" class="ltx_text ltx_lstlisting" style="font-size:80%;">–<span id="A5.T7.2.9.1.1.1" class="ltx_text ltx_lst_identifier">batch</span>-<span id="A5.T7.2.9.1.1.2" class="ltx_text ltx_lst_identifier">size</span></span></td>
</tr>
<tr id="A5.T7.2.10" class="ltx_tr">
<td id="A5.T7.2.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-bottom:-3.0pt;">
<span id="A5.T7.2.10.1.1" class="ltx_ERROR undefined">\libertineSB</span><span id="A5.T7.2.10.1.2" class="ltx_text" style="font-size:90%;">Local Epochs</span>
</td>
<td id="A5.T7.2.10.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.10.2.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="A5.T7.2.10.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.10.3.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="A5.T7.2.10.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.10.4.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="A5.T7.2.10.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-bottom:-3.0pt;" rowspan="2"><span id="A5.T7.2.10.5.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="A5.T7.2.11" class="ltx_tr">
<td id="A5.T7.2.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" style="padding-bottom:3.0pt;"><span id="A5.T7.2.11.1.1" class="ltx_text ltx_lstlisting" style="font-size:80%;">–<span id="A5.T7.2.11.1.1.1" class="ltx_text ltx_lst_identifier">num</span>-<span id="A5.T7.2.11.1.1.2" class="ltx_text ltx_lst_identifier">epochs</span></span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A5.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>. </span><span id="A5.T7.4.2" class="ltx_text" style="font-size:90%;">The hyperparameters that were used for benchmarking <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> on LEAF.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A5.T7.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A5.T7.6" class="ltx_p ltx_figure_panel">The hyperparameters that were used for benchmarking <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> on LEAF.</p>
</div>
</div>
</figure>
<figure id="A5.F23" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2303.04689/assets/x32.png" id="A5.F23.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="371" height="247" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F23.3.1.1" class="ltx_text" style="font-size:90%;">Figure 23</span>. </span><span id="A5.F23.4.2" class="ltx_ERROR undefined">\Ac</span><span id="A5.F23.5.3" class="ltx_text" style="font-size:90%;">fedq LEAF benchmark experiment results.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="A5.F23.6" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="A5.F23.7" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Ac</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A5.F23.8" class="ltx_p ltx_figure_panel ltx_align_center">fedq LEAF benchmark experiment results.</p>
</div>
</div>
</figure>
<div id="A5.p7" class="ltx_para">
<p id="A5.p7.1" class="ltx_p">In all experiments, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 10 had a higher final accuracy than the other two experiments. In the cases of the Sentiment140 and the Reddit datasets, it even manages to clearly outperform <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 100. This is interesting in two ways: First of all, in the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments on our <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a>, there was always a benefit when using a larger queue length, albeit with diminishing returns. The LEAF benchmark experiments not only show that using a larger queue length does not always result in a significant increase in performance, but it may even make the training unstable and hinders convergence, as is the case for the Sentiment140 and the Reddit dataset. The second remarkable thing is, that in both cases where a larger queue length causes the training to become unstable, the model is an <a href="#id27.27.id27"><abbr href="#id27.27.id27" title="long short-term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LSTM</span></abbr></a>. Of course, no trend can be derived from just these experiments, but this interesting behavior could be explored in future work.</p>
</div>
<div id="A5.p8" class="ltx_para">
<p id="A5.p8.1" class="ltx_p">The margins with which <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> outperforms the <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> baseline are much smaller as compared to the results achieved with the models of our <a href="#id16.16.id16"><abbr href="#id16.16.id16" title="federated recommender system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedRec</span></abbr></a>. Nonetheless, it can be clearly seen that <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> has a much faster convergence rate. The <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> baseline reaches its highest accuracy in all cases at the very end of the training window (communication round 400/400 for <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, 390/400 for Sentiment140, 400/400 for <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, and 96/100 for Reddit). Both <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> experiments are able to reach or exceed the baseline’s highest accuracy in a much shorter time frame: For <a href="#id19.19.id19"><abbr href="#id19.19.id19" title="Federated EMNIST" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FEMNIST</span></abbr></a>, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 10 exceeded <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> at communication round 40, while <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 100 already outperformed <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> at communication round 10. For Sentiment140, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 10 surpasses <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> at communication round 180 and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 100 at communication round 50. For <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Large-scale CelebFaces Attributes Dataset" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CelebA</span></abbr></a>, the communication rounds were 50 and 20, while those for Reddit were 180 and 50 respectively. It should also be noted that, although its training was less stable, <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 100 outperformed the other two experiments in terms of highest accuracy for all datasets except for Reddit. It was also always significantly faster to exceed the highest accuracy of <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> than <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> with a queue length of 10. Table <a href="#A5.T8" title="Table 8 ‣ Appendix E Validation of FedQ on the LEAF Federated Learning Benchmark ‣ A Privacy Preserving System for Movie Recommendations Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> presents the results of our experiments in comparison to the results published by <cite class="ltx_cite ltx_citemacro_citet">Caldas et al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite>. Please be aware that our experiments used different hyperparameters for both the preprocessing of the datasets as well as the training of the models, which renders the results incomparable. Particularly notable is the difference in the Sentiment140 dataset, where <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> report on four experiments with varying minimum numbers of samples per client ranging from 3 to 100. In our experiments, we have set the minimum number of samples per client to 0 in the preprocessing of Sentiment140, which means that our experiments had a considerably lower number of samples per client on average. We have still included the results for your reference and as another baseline.</p>
</div>
<figure id="A5.T8" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A5.T8.4" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="A5.T8.4.5" class="ltx_tr">
<td id="A5.T8.4.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="A5.T8.4.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></td>
<td id="A5.T8.4.5.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2"><span id="A5.T8.4.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td id="A5.T8.4.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Result</span></td>
</tr>
<tr id="A5.T8.4.6" class="ltx_tr">
<td id="A5.T8.4.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span id="A5.T8.4.6.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.6.1.1.1" class="ltx_ERROR undefined">\Ac</span>femnist</span></td>
<td id="A5.T8.4.6.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.6.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.6.2.2" class="ltx_text" style="font-size:90%;">fedavg (LEAF)</span>
</td>
<td id="A5.T8.4.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.6.3.1" class="ltx_text" style="font-size:90%;">74.72%</span></td>
</tr>
<tr id="A5.T8.4.7" class="ltx_tr">
<td id="A5.T8.4.7.1" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.7.1.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.7.1.2" class="ltx_text" style="font-size:90%;">fedavg (ours)</span>
</td>
<td id="A5.T8.4.7.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.7.2.1" class="ltx_text" style="font-size:90%;">82.66%</span></td>
</tr>
<tr id="A5.T8.4.8" class="ltx_tr">
<td id="A5.T8.4.8.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A5.T8.4.8.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.8.1.1.1" class="ltx_ERROR undefined">\Ac</span>fedq (ours)</span></td>
<td id="A5.T8.4.8.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T8.4.8.2.1" class="ltx_text" style="font-size:90%;">Queue Length 10</span></td>
<td id="A5.T8.4.8.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.8.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">86.98%</span></td>
</tr>
<tr id="A5.T8.4.9" class="ltx_tr">
<td id="A5.T8.4.9.1" class="ltx_td ltx_align_left"><span id="A5.T8.4.9.1.1" class="ltx_text" style="font-size:90%;">Queue Length 100</span></td>
<td id="A5.T8.4.9.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.4.9.2.1" class="ltx_text" style="font-size:90%;">86.65%</span></td>
</tr>
<tr id="A5.T8.4.10" class="ltx_tr">
<td id="A5.T8.4.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span id="A5.T8.4.10.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.10.1.1.1" class="ltx_ERROR undefined">\Ac</span>celeba</span></td>
<td id="A5.T8.4.10.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.10.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.10.2.2" class="ltx_text" style="font-size:90%;">fedavg (LEAF)</span>
</td>
<td id="A5.T8.4.10.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.10.3.1" class="ltx_text" style="font-size:90%;">89.46%</span></td>
</tr>
<tr id="A5.T8.4.11" class="ltx_tr">
<td id="A5.T8.4.11.1" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.11.1.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.11.1.2" class="ltx_text" style="font-size:90%;">fedavg (ours)</span>
</td>
<td id="A5.T8.4.11.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.11.2.1" class="ltx_text" style="font-size:90%;">91.24%</span></td>
</tr>
<tr id="A5.T8.4.12" class="ltx_tr">
<td id="A5.T8.4.12.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A5.T8.4.12.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.12.1.1.1" class="ltx_ERROR undefined">\Ac</span>fedq (ours)</span></td>
<td id="A5.T8.4.12.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T8.4.12.2.1" class="ltx_text" style="font-size:90%;">Queue Length 10</span></td>
<td id="A5.T8.4.12.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.12.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">91.98%</span></td>
</tr>
<tr id="A5.T8.4.13" class="ltx_tr">
<td id="A5.T8.4.13.1" class="ltx_td ltx_align_left"><span id="A5.T8.4.13.1.1" class="ltx_text" style="font-size:90%;">Queue Length 100</span></td>
<td id="A5.T8.4.13.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.4.13.2.1" class="ltx_text" style="font-size:90%;">91.58%</span></td>
</tr>
<tr id="A5.T8.1.1" class="ltx_tr">
<td id="A5.T8.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="7"><span id="A5.T8.1.1.2.1" class="ltx_text" style="font-size:90%;">Sentiment140</span></td>
<td id="A5.T8.1.1.3" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="A5.T8.1.1.3.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.1.1.3.1.1" class="ltx_ERROR undefined">\Ac</span>fedavg (LEAF)</span></td>
<td id="A5.T8.1.1.1" class="ltx_td ltx_align_left ltx_border_t">
<math id="A5.T8.1.1.1.m1.1" class="ltx_Math" alttext="\geq 3" display="inline"><semantics id="A5.T8.1.1.1.m1.1a"><mrow id="A5.T8.1.1.1.m1.1.1" xref="A5.T8.1.1.1.m1.1.1.cmml"><mi id="A5.T8.1.1.1.m1.1.1.2" xref="A5.T8.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A5.T8.1.1.1.m1.1.1.1" xref="A5.T8.1.1.1.m1.1.1.1.cmml">≥</mo><mn mathsize="90%" id="A5.T8.1.1.1.m1.1.1.3" xref="A5.T8.1.1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.1.1.1.m1.1b"><apply id="A5.T8.1.1.1.m1.1.1.cmml" xref="A5.T8.1.1.1.m1.1.1"><geq id="A5.T8.1.1.1.m1.1.1.1.cmml" xref="A5.T8.1.1.1.m1.1.1.1"></geq><csymbol cd="latexml" id="A5.T8.1.1.1.m1.1.1.2.cmml" xref="A5.T8.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A5.T8.1.1.1.m1.1.1.3.cmml" xref="A5.T8.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.1.1.1.m1.1c">\geq 3</annotation></semantics></math><span id="A5.T8.1.1.1.1" class="ltx_text" style="font-size:90%;"> Samples per Client</span>
</td>
<td id="A5.T8.1.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.1.1.4.1" class="ltx_text" style="font-size:90%;">~50%*</span></td>
</tr>
<tr id="A5.T8.2.2" class="ltx_tr">
<td id="A5.T8.2.2.1" class="ltx_td ltx_align_left">
<math id="A5.T8.2.2.1.m1.1" class="ltx_Math" alttext="\geq 10" display="inline"><semantics id="A5.T8.2.2.1.m1.1a"><mrow id="A5.T8.2.2.1.m1.1.1" xref="A5.T8.2.2.1.m1.1.1.cmml"><mi id="A5.T8.2.2.1.m1.1.1.2" xref="A5.T8.2.2.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A5.T8.2.2.1.m1.1.1.1" xref="A5.T8.2.2.1.m1.1.1.1.cmml">≥</mo><mn mathsize="90%" id="A5.T8.2.2.1.m1.1.1.3" xref="A5.T8.2.2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.2.2.1.m1.1b"><apply id="A5.T8.2.2.1.m1.1.1.cmml" xref="A5.T8.2.2.1.m1.1.1"><geq id="A5.T8.2.2.1.m1.1.1.1.cmml" xref="A5.T8.2.2.1.m1.1.1.1"></geq><csymbol cd="latexml" id="A5.T8.2.2.1.m1.1.1.2.cmml" xref="A5.T8.2.2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A5.T8.2.2.1.m1.1.1.3.cmml" xref="A5.T8.2.2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.2.2.1.m1.1c">\geq 10</annotation></semantics></math><span id="A5.T8.2.2.1.1" class="ltx_text" style="font-size:90%;"> Samples per Client</span>
</td>
<td id="A5.T8.2.2.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.2.2.2.1" class="ltx_text" style="font-size:90%;">~50%*</span></td>
</tr>
<tr id="A5.T8.3.3" class="ltx_tr">
<td id="A5.T8.3.3.1" class="ltx_td ltx_align_left">
<math id="A5.T8.3.3.1.m1.1" class="ltx_Math" alttext="\geq 30" display="inline"><semantics id="A5.T8.3.3.1.m1.1a"><mrow id="A5.T8.3.3.1.m1.1.1" xref="A5.T8.3.3.1.m1.1.1.cmml"><mi id="A5.T8.3.3.1.m1.1.1.2" xref="A5.T8.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A5.T8.3.3.1.m1.1.1.1" xref="A5.T8.3.3.1.m1.1.1.1.cmml">≥</mo><mn mathsize="90%" id="A5.T8.3.3.1.m1.1.1.3" xref="A5.T8.3.3.1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.3.3.1.m1.1b"><apply id="A5.T8.3.3.1.m1.1.1.cmml" xref="A5.T8.3.3.1.m1.1.1"><geq id="A5.T8.3.3.1.m1.1.1.1.cmml" xref="A5.T8.3.3.1.m1.1.1.1"></geq><csymbol cd="latexml" id="A5.T8.3.3.1.m1.1.1.2.cmml" xref="A5.T8.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A5.T8.3.3.1.m1.1.1.3.cmml" xref="A5.T8.3.3.1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.3.3.1.m1.1c">\geq 30</annotation></semantics></math><span id="A5.T8.3.3.1.1" class="ltx_text" style="font-size:90%;"> Samples per Client</span>
</td>
<td id="A5.T8.3.3.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.3.3.2.1" class="ltx_text" style="font-size:90%;">~60%*</span></td>
</tr>
<tr id="A5.T8.4.4" class="ltx_tr">
<td id="A5.T8.4.4.1" class="ltx_td ltx_align_left">
<math id="A5.T8.4.4.1.m1.1" class="ltx_Math" alttext="\geq 100" display="inline"><semantics id="A5.T8.4.4.1.m1.1a"><mrow id="A5.T8.4.4.1.m1.1.1" xref="A5.T8.4.4.1.m1.1.1.cmml"><mi id="A5.T8.4.4.1.m1.1.1.2" xref="A5.T8.4.4.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A5.T8.4.4.1.m1.1.1.1" xref="A5.T8.4.4.1.m1.1.1.1.cmml">≥</mo><mn mathsize="90%" id="A5.T8.4.4.1.m1.1.1.3" xref="A5.T8.4.4.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.4.4.1.m1.1b"><apply id="A5.T8.4.4.1.m1.1.1.cmml" xref="A5.T8.4.4.1.m1.1.1"><geq id="A5.T8.4.4.1.m1.1.1.1.cmml" xref="A5.T8.4.4.1.m1.1.1.1"></geq><csymbol cd="latexml" id="A5.T8.4.4.1.m1.1.1.2.cmml" xref="A5.T8.4.4.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A5.T8.4.4.1.m1.1.1.3.cmml" xref="A5.T8.4.4.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.4.4.1.m1.1c">\geq 100</annotation></semantics></math><span id="A5.T8.4.4.1.1" class="ltx_text" style="font-size:90%;"> Samples per Client</span>
</td>
<td id="A5.T8.4.4.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.4.4.2.1" class="ltx_text" style="font-size:90%;">~69%*</span></td>
</tr>
<tr id="A5.T8.4.14" class="ltx_tr">
<td id="A5.T8.4.14.1" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.14.1.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.14.1.2" class="ltx_text" style="font-size:90%;">fedavg (ours)</span>
</td>
<td id="A5.T8.4.14.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.14.2.1" class="ltx_text" style="font-size:90%;">69.91%</span></td>
</tr>
<tr id="A5.T8.4.15" class="ltx_tr">
<td id="A5.T8.4.15.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A5.T8.4.15.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.15.1.1.1" class="ltx_ERROR undefined">\Ac</span>fedq (ours)</span></td>
<td id="A5.T8.4.15.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T8.4.15.2.1" class="ltx_text" style="font-size:90%;">Queue Length 10</span></td>
<td id="A5.T8.4.15.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.15.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.18%</span></td>
</tr>
<tr id="A5.T8.4.16" class="ltx_tr">
<td id="A5.T8.4.16.1" class="ltx_td ltx_align_left"><span id="A5.T8.4.16.1.1" class="ltx_text" style="font-size:90%;">Queue Length 100</span></td>
<td id="A5.T8.4.16.2" class="ltx_td ltx_align_right ltx_border_r"><span id="A5.T8.4.16.2.1" class="ltx_text" style="font-size:90%;">69.32%</span></td>
</tr>
<tr id="A5.T8.4.17" class="ltx_tr">
<td id="A5.T8.4.17.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="4"><span id="A5.T8.4.17.1.1" class="ltx_text" style="font-size:90%;">Reddit</span></td>
<td id="A5.T8.4.17.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.17.2.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.17.2.2" class="ltx_text" style="font-size:90%;">fedavg (LEAF)</span>
</td>
<td id="A5.T8.4.17.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.17.3.1" class="ltx_text" style="font-size:90%;">13.35%</span></td>
</tr>
<tr id="A5.T8.4.18" class="ltx_tr">
<td id="A5.T8.4.18.1" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="A5.T8.4.18.1.1" class="ltx_ERROR undefined">\Ac</span><span id="A5.T8.4.18.1.2" class="ltx_text" style="font-size:90%;">fedavg (ours)</span>
</td>
<td id="A5.T8.4.18.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.18.2.1" class="ltx_text" style="font-size:90%;">13.23%</span></td>
</tr>
<tr id="A5.T8.4.19" class="ltx_tr">
<td id="A5.T8.4.19.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" rowspan="2"><span id="A5.T8.4.19.1.1" class="ltx_text" style="font-size:90%;"><span id="A5.T8.4.19.1.1.1" class="ltx_ERROR undefined">\Ac</span>fedq (ours)</span></td>
<td id="A5.T8.4.19.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T8.4.19.2.1" class="ltx_text" style="font-size:90%;">Queue Length 10</span></td>
<td id="A5.T8.4.19.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="A5.T8.4.19.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">14.60%</span></td>
</tr>
<tr id="A5.T8.4.20" class="ltx_tr">
<td id="A5.T8.4.20.1" class="ltx_td ltx_align_left ltx_border_b"><span id="A5.T8.4.20.1.1" class="ltx_text" style="font-size:90%;">Queue Length 100</span></td>
<td id="A5.T8.4.20.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A5.T8.4.20.2.1" class="ltx_text" style="font-size:90%;">13.04%</span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A5.T8.6.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>. </span><span id="A5.T8.7.2" class="ltx_text" style="font-size:90%;">Comparison of the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> LEAF benchmark results against the results published by <cite class="ltx_cite ltx_citemacro_citet">Caldas et al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite>. *Please note that <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> do not publish final accuracies for the Sentiment140 dataset. The accuracies shown in the table were read from the graph in Figure 3 <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite> and are only approximations.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A5.T8.8" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A5.T8.9" class="ltx_p ltx_figure_panel">Comparison of the <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> LEAF benchmark results against the results published by <cite class="ltx_cite ltx_citemacro_citet">Caldas et al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite>. *Please note that <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Caldas et al<span class="ltx_text">.</span></a></cite> do not publish final accuracies for the Sentiment140 dataset. The accuracies shown in the table were read from the graph in Figure 3 <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018b</a>)</cite> and are only approximations.</p>
</div>
</div>
</figure>
<div id="A5.p9" class="ltx_para">
<p id="A5.p9.1" class="ltx_p">In conclusion, we think these experiments demonstrate that <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> is capable of outperforming <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a> on a wide variety of data modalities and training tasks. Although the margin with which <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> outperforms the baseline varies with dataset and model architecture, its ability to drastically improve convergence speed makes it particularly efficacious.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>FedQ and Other Client Chaining Techniques</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.3" class="ltx_p">This appendix section describes further techniques for <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> client chaining in comparison to <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, that have been developed in parallel to our method, and provides similarities and differences between them. <cite class="ltx_cite ltx_citemacro_citet">Kamp et al<span class="ltx_text">.</span> (<a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>, for example, aim to improve <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> in scenarios where each client only has a small local dataset. They propose a technique called  <a href="#id14.14.id14"><span href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated daisy-chaining</span></span></a> (<a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a>), where the central server, instead of aggregating the updated local models of the clients into a new global model, sends each updated local model <math id="A6.p1.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="A6.p1.1.m1.1a"><msub id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml"><mi id="A6.p1.1.m1.1.1.2" xref="A6.p1.1.m1.1.1.2.cmml">M</mi><mi id="A6.p1.1.m1.1.1.3" xref="A6.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><apply id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A6.p1.1.m1.1.1.1.cmml" xref="A6.p1.1.m1.1.1">subscript</csymbol><ci id="A6.p1.1.m1.1.1.2.cmml" xref="A6.p1.1.m1.1.1.2">𝑀</ci><ci id="A6.p1.1.m1.1.1.3.cmml" xref="A6.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">M_{i}</annotation></semantics></math> to a randomly selected client <math id="A6.p1.2.m2.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="A6.p1.2.m2.1a"><msub id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml"><mi id="A6.p1.2.m2.1.1.2" xref="A6.p1.2.m2.1.1.2.cmml">c</mi><mi id="A6.p1.2.m2.1.1.3" xref="A6.p1.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><apply id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A6.p1.2.m2.1.1.1.cmml" xref="A6.p1.2.m2.1.1">subscript</csymbol><ci id="A6.p1.2.m2.1.1.2.cmml" xref="A6.p1.2.m2.1.1.2">𝑐</ci><ci id="A6.p1.2.m2.1.1.3.cmml" xref="A6.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">c_{j}</annotation></semantics></math>, where <math id="A6.p1.3.m3.1" class="ltx_Math" alttext="i\neq j" display="inline"><semantics id="A6.p1.3.m3.1a"><mrow id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml"><mi id="A6.p1.3.m3.1.1.2" xref="A6.p1.3.m3.1.1.2.cmml">i</mi><mo id="A6.p1.3.m3.1.1.1" xref="A6.p1.3.m3.1.1.1.cmml">≠</mo><mi id="A6.p1.3.m3.1.1.3" xref="A6.p1.3.m3.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><apply id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1"><neq id="A6.p1.3.m3.1.1.1.cmml" xref="A6.p1.3.m3.1.1.1"></neq><ci id="A6.p1.3.m3.1.1.2.cmml" xref="A6.p1.3.m3.1.1.2">𝑖</ci><ci id="A6.p1.3.m3.1.1.3.cmml" xref="A6.p1.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">i\neq j</annotation></semantics></math>. After a few rounds of this daisy-chaining, the resulting models are aggregated analogously to <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>. <cite class="ltx_cite ltx_citemacro_citet">Hu et al<span class="ltx_text">.</span> (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> tackle the problem of non-<a href="#id26.26.id26"><abbr href="#id26.26.id26" title="independent and identically distributed" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">i.i.d.</span></abbr></a> data in <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and propose a technique called  <a href="#id13.13.id13"><span href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated learning via device concatenation</span></span></a> (<a href="#id13.13.id13"><abbr href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedCat</span></abbr></a>) that is essentially equivalent to <a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a>. The main difference between <a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a> and <a href="#id13.13.id13"><abbr href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedCat</span></abbr></a> is, that in <a href="#id13.13.id13"><abbr href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedCat</span></abbr></a> each model is trained by each client before they are aggregated to form a new global model and only the order of the client updates differs, while in <a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a>, depending on the daisy-chaining period, each model is only trained on a random subset of all clients. <cite class="ltx_cite ltx_citemacro_citet">Zaccone et al<span class="ltx_text">.</span> (<a href="#bib.bib130" title="" class="ltx_ref">2022</a>)</cite> also try to alleviate the problem of heterogeneous client datasets by proposing a technique called  <a href="#id17.17.id17"><span href="#id17.17.id17" title="federated learning via sequential superclients training" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">federated learning via sequential superclients training</span></span></a> (<a href="#id17.17.id17"><abbr href="#id17.17.id17" title="federated learning via sequential superclients training" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSeq</span></abbr></a>). They perform a pre-training phase, after which they use the resulting model to estimate the data generating distribution of each client. Using the estimated distributions, they generate groups of clients with different local distributions, which they denote as superclients. During <a href="#id20.20.id20"><abbr href="#id20.20.id20" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the clients within each superclient are trained sequentially, where the first client receives the global model and all consecutive clients receive the model of the previous client. The resulting local models of the superclients are then aggregated as in <a href="#id12.12.id12"><abbr href="#id12.12.id12" title="federated averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></a>.</p>
</div>
<div id="A6.p2" class="ltx_para">
<p id="A6.p2.1" class="ltx_p">All of the proposed techniques have similar goals and try to solve these problems by chaining the local training of multiple clients, but each of the techniques has variations in the training protocol that they follow. Both <a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a> and <a href="#id13.13.id13"><abbr href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedCat</span></abbr></a> train as many different models as there are clients in each communication round. <span id="A6.p2.1.1" class="ltx_ERROR undefined">\Ac</span>fedseq and our method <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a>, however, only train <math id="A6.p2.1.m1.1" class="ltx_Math" alttext="\frac{\#clients}{\#clients\,per\,superclient/queue}" display="inline"><semantics id="A6.p2.1.m1.1a"><mfrac id="A6.p2.1.m1.1.1" xref="A6.p2.1.m1.1.1.cmml"><mrow id="A6.p2.1.m1.1.1.2" xref="A6.p2.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="A6.p2.1.m1.1.1.2.2" xref="A6.p2.1.m1.1.1.2.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.3" xref="A6.p2.1.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1a" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.4" xref="A6.p2.1.m1.1.1.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1b" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.5" xref="A6.p2.1.m1.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1c" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.6" xref="A6.p2.1.m1.1.1.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1d" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.7" xref="A6.p2.1.m1.1.1.2.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1e" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.8" xref="A6.p2.1.m1.1.1.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.2.1f" xref="A6.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.2.9" xref="A6.p2.1.m1.1.1.2.9.cmml">s</mi></mrow><mrow id="A6.p2.1.m1.1.1.3" xref="A6.p2.1.m1.1.1.3.cmml"><mrow id="A6.p2.1.m1.1.1.3.2" xref="A6.p2.1.m1.1.1.3.2.cmml"><mrow id="A6.p2.1.m1.1.1.3.2.2" xref="A6.p2.1.m1.1.1.3.2.2.cmml"><mi mathvariant="normal" id="A6.p2.1.m1.1.1.3.2.2.2" xref="A6.p2.1.m1.1.1.3.2.2.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.3" xref="A6.p2.1.m1.1.1.3.2.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1a" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.4" xref="A6.p2.1.m1.1.1.3.2.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1b" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.5" xref="A6.p2.1.m1.1.1.3.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1c" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.6" xref="A6.p2.1.m1.1.1.3.2.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1d" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.7" xref="A6.p2.1.m1.1.1.3.2.2.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1e" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.8" xref="A6.p2.1.m1.1.1.3.2.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1f" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.9" xref="A6.p2.1.m1.1.1.3.2.2.9.cmml">s</mi><mo lspace="0.170em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1g" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.10" xref="A6.p2.1.m1.1.1.3.2.2.10.cmml">p</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1h" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.11" xref="A6.p2.1.m1.1.1.3.2.2.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1i" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.12" xref="A6.p2.1.m1.1.1.3.2.2.12.cmml">r</mi><mo lspace="0.170em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1j" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.13" xref="A6.p2.1.m1.1.1.3.2.2.13.cmml">s</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1k" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.14" xref="A6.p2.1.m1.1.1.3.2.2.14.cmml">u</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1l" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.15" xref="A6.p2.1.m1.1.1.3.2.2.15.cmml">p</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1m" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.16" xref="A6.p2.1.m1.1.1.3.2.2.16.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1n" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.17" xref="A6.p2.1.m1.1.1.3.2.2.17.cmml">r</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1o" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.18" xref="A6.p2.1.m1.1.1.3.2.2.18.cmml">c</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1p" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.19" xref="A6.p2.1.m1.1.1.3.2.2.19.cmml">l</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1q" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.20" xref="A6.p2.1.m1.1.1.3.2.2.20.cmml">i</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1r" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.21" xref="A6.p2.1.m1.1.1.3.2.2.21.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1s" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.22" xref="A6.p2.1.m1.1.1.3.2.2.22.cmml">n</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.2.2.1t" xref="A6.p2.1.m1.1.1.3.2.2.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.2.2.23" xref="A6.p2.1.m1.1.1.3.2.2.23.cmml">t</mi></mrow><mo id="A6.p2.1.m1.1.1.3.2.1" xref="A6.p2.1.m1.1.1.3.2.1.cmml">/</mo><mi id="A6.p2.1.m1.1.1.3.2.3" xref="A6.p2.1.m1.1.1.3.2.3.cmml">q</mi></mrow><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.1" xref="A6.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.3" xref="A6.p2.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.1a" xref="A6.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.4" xref="A6.p2.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.1b" xref="A6.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.5" xref="A6.p2.1.m1.1.1.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="A6.p2.1.m1.1.1.3.1c" xref="A6.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A6.p2.1.m1.1.1.3.6" xref="A6.p2.1.m1.1.1.3.6.cmml">e</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="A6.p2.1.m1.1b"><apply id="A6.p2.1.m1.1.1.cmml" xref="A6.p2.1.m1.1.1"><divide id="A6.p2.1.m1.1.1.1.cmml" xref="A6.p2.1.m1.1.1"></divide><apply id="A6.p2.1.m1.1.1.2.cmml" xref="A6.p2.1.m1.1.1.2"><times id="A6.p2.1.m1.1.1.2.1.cmml" xref="A6.p2.1.m1.1.1.2.1"></times><ci id="A6.p2.1.m1.1.1.2.2.cmml" xref="A6.p2.1.m1.1.1.2.2">#</ci><ci id="A6.p2.1.m1.1.1.2.3.cmml" xref="A6.p2.1.m1.1.1.2.3">𝑐</ci><ci id="A6.p2.1.m1.1.1.2.4.cmml" xref="A6.p2.1.m1.1.1.2.4">𝑙</ci><ci id="A6.p2.1.m1.1.1.2.5.cmml" xref="A6.p2.1.m1.1.1.2.5">𝑖</ci><ci id="A6.p2.1.m1.1.1.2.6.cmml" xref="A6.p2.1.m1.1.1.2.6">𝑒</ci><ci id="A6.p2.1.m1.1.1.2.7.cmml" xref="A6.p2.1.m1.1.1.2.7">𝑛</ci><ci id="A6.p2.1.m1.1.1.2.8.cmml" xref="A6.p2.1.m1.1.1.2.8">𝑡</ci><ci id="A6.p2.1.m1.1.1.2.9.cmml" xref="A6.p2.1.m1.1.1.2.9">𝑠</ci></apply><apply id="A6.p2.1.m1.1.1.3.cmml" xref="A6.p2.1.m1.1.1.3"><times id="A6.p2.1.m1.1.1.3.1.cmml" xref="A6.p2.1.m1.1.1.3.1"></times><apply id="A6.p2.1.m1.1.1.3.2.cmml" xref="A6.p2.1.m1.1.1.3.2"><divide id="A6.p2.1.m1.1.1.3.2.1.cmml" xref="A6.p2.1.m1.1.1.3.2.1"></divide><apply id="A6.p2.1.m1.1.1.3.2.2.cmml" xref="A6.p2.1.m1.1.1.3.2.2"><times id="A6.p2.1.m1.1.1.3.2.2.1.cmml" xref="A6.p2.1.m1.1.1.3.2.2.1"></times><ci id="A6.p2.1.m1.1.1.3.2.2.2.cmml" xref="A6.p2.1.m1.1.1.3.2.2.2">#</ci><ci id="A6.p2.1.m1.1.1.3.2.2.3.cmml" xref="A6.p2.1.m1.1.1.3.2.2.3">𝑐</ci><ci id="A6.p2.1.m1.1.1.3.2.2.4.cmml" xref="A6.p2.1.m1.1.1.3.2.2.4">𝑙</ci><ci id="A6.p2.1.m1.1.1.3.2.2.5.cmml" xref="A6.p2.1.m1.1.1.3.2.2.5">𝑖</ci><ci id="A6.p2.1.m1.1.1.3.2.2.6.cmml" xref="A6.p2.1.m1.1.1.3.2.2.6">𝑒</ci><ci id="A6.p2.1.m1.1.1.3.2.2.7.cmml" xref="A6.p2.1.m1.1.1.3.2.2.7">𝑛</ci><ci id="A6.p2.1.m1.1.1.3.2.2.8.cmml" xref="A6.p2.1.m1.1.1.3.2.2.8">𝑡</ci><ci id="A6.p2.1.m1.1.1.3.2.2.9.cmml" xref="A6.p2.1.m1.1.1.3.2.2.9">𝑠</ci><ci id="A6.p2.1.m1.1.1.3.2.2.10.cmml" xref="A6.p2.1.m1.1.1.3.2.2.10">𝑝</ci><ci id="A6.p2.1.m1.1.1.3.2.2.11.cmml" xref="A6.p2.1.m1.1.1.3.2.2.11">𝑒</ci><ci id="A6.p2.1.m1.1.1.3.2.2.12.cmml" xref="A6.p2.1.m1.1.1.3.2.2.12">𝑟</ci><ci id="A6.p2.1.m1.1.1.3.2.2.13.cmml" xref="A6.p2.1.m1.1.1.3.2.2.13">𝑠</ci><ci id="A6.p2.1.m1.1.1.3.2.2.14.cmml" xref="A6.p2.1.m1.1.1.3.2.2.14">𝑢</ci><ci id="A6.p2.1.m1.1.1.3.2.2.15.cmml" xref="A6.p2.1.m1.1.1.3.2.2.15">𝑝</ci><ci id="A6.p2.1.m1.1.1.3.2.2.16.cmml" xref="A6.p2.1.m1.1.1.3.2.2.16">𝑒</ci><ci id="A6.p2.1.m1.1.1.3.2.2.17.cmml" xref="A6.p2.1.m1.1.1.3.2.2.17">𝑟</ci><ci id="A6.p2.1.m1.1.1.3.2.2.18.cmml" xref="A6.p2.1.m1.1.1.3.2.2.18">𝑐</ci><ci id="A6.p2.1.m1.1.1.3.2.2.19.cmml" xref="A6.p2.1.m1.1.1.3.2.2.19">𝑙</ci><ci id="A6.p2.1.m1.1.1.3.2.2.20.cmml" xref="A6.p2.1.m1.1.1.3.2.2.20">𝑖</ci><ci id="A6.p2.1.m1.1.1.3.2.2.21.cmml" xref="A6.p2.1.m1.1.1.3.2.2.21">𝑒</ci><ci id="A6.p2.1.m1.1.1.3.2.2.22.cmml" xref="A6.p2.1.m1.1.1.3.2.2.22">𝑛</ci><ci id="A6.p2.1.m1.1.1.3.2.2.23.cmml" xref="A6.p2.1.m1.1.1.3.2.2.23">𝑡</ci></apply><ci id="A6.p2.1.m1.1.1.3.2.3.cmml" xref="A6.p2.1.m1.1.1.3.2.3">𝑞</ci></apply><ci id="A6.p2.1.m1.1.1.3.3.cmml" xref="A6.p2.1.m1.1.1.3.3">𝑢</ci><ci id="A6.p2.1.m1.1.1.3.4.cmml" xref="A6.p2.1.m1.1.1.3.4">𝑒</ci><ci id="A6.p2.1.m1.1.1.3.5.cmml" xref="A6.p2.1.m1.1.1.3.5">𝑢</ci><ci id="A6.p2.1.m1.1.1.3.6.cmml" xref="A6.p2.1.m1.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p2.1.m1.1c">\frac{\#clients}{\#clients\,per\,superclient/queue}</annotation></semantics></math> models per communication round. In <a href="#id14.14.id14"><abbr href="#id14.14.id14" title="federated daisy-chaining" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedDC</span></abbr></a>, <a href="#id13.13.id13"><abbr href="#id13.13.id13" title="federated learning via device concatenation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedCat</span></abbr></a>, and <a href="#id15.15.id15"><abbr href="#id15.15.id15" title="federated learning with client queuing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedQ</span></abbr></a> the clients for the sequential training are selected randomly, while in <a href="#id17.17.id17"><abbr href="#id17.17.id17" title="federated learning via sequential superclients training" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSeq</span></abbr></a> they are purposely selected in order to group clients together that have different data generating distributions.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.04688" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.04689" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.04689">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.04689" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.04690" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 21:14:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
