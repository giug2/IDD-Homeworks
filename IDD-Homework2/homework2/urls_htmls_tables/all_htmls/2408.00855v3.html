<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design</title>
<!--Generated on Mon Sep 30 09:37:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Human-AI Collaboration; Generative Artificial Intelligence; Personalized Fashion Design" lang="en" name="keywords"/>
<base href="/html/2408.00855v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S1" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S2" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S2.SS1" title="In 2. Related Work ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Generative AI in Fashion Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S2.SS2" title="In 2. Related Work ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Human-AI Collaboration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S2.SS3" title="In 2. Related Work ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Privacy Protection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>User Study and System Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.SS1" title="In 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>User Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.SS1.SSS1" title="In 3.1. User Study ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Questionnaires</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.SS1.SSS2" title="In 3.1. User Study ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Requirements Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.SS2" title="In 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>System Overview</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1" title="In 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Stable Diffusion Model-based Text-to-Image Cloud Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1.SSS1" title="In 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Stable Diffusion Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1.SSS2" title="In 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>SD Model Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1.SSS3" title="In 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Loss Function</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2" title="In 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Capture Personalized Designer-Style Image-to-Sketch Local Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2.SSS1" title="In 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>VGG Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2.SSS2" title="In 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Adaptive Personalized Style Normalization Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2.SSS3" title="In 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>DownSampling Multi-Feature Fusion Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2.SSS4" title="In 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Loss Function</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS3" title="In 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Cloud Image-based Local Personalized Sketch Recommendation Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS3.SSS1" title="In 4.3. Cloud Image-based Local Personalized Sketch Recommendation Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Sketch Recommendation Module</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4" title="In 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Style Transfer Module based on Local Personalized Sketch and Cloud Image</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4.SSS1" title="In 4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Denoising Diffusion Implicit Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4.SSS2" title="In 4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Channel Cross Attention Module</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS1" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS2" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS3" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS4" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Performance of Text-to-Image Cloud Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS4.SSS1" title="In 5.4. Performance of Text-to-Image Cloud Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>Optimization Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS4.SSS2" title="In 5.4. Performance of Text-to-Image Cloud Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>Qualitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS5" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Performance of Image-to-Sketch Local Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS5.SSS1" title="In 5.5. Performance of Image-to-Sketch Local Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.1 </span>Quantitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS5.SSS2" title="In 5.5. Performance of Image-to-Sketch Local Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5.2 </span>Qualitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS6" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Performance of Sketch Recommendation Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS6.SSS1" title="In 5.6. Performance of Sketch Recommendation Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Qualitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS7" title="In 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Performance of Style Transfer Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS7.SSS1" title="In 5.7. Performance of Style Transfer Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.1 </span>Quantitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.SS7.SSS2" title="In 5.7. Performance of Style Transfer Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.2 </span>Qualitative Results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>User survey on HAIGEN system</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6.SS1" title="In 6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Experimental Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6.SS2" title="In 6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Survey Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S7" title="In HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0003-3029-8146" title="">Jianan Jiang</a>, <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0001-8697-1817" title="">Di Wu</a>, <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0003-2091-5163" title="">Hanhui Deng</a>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Hunan University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Changsha</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Hunan</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><a class="ltx_ref ltx_href" href="https://orcid.org/0009-0001-7607-7106" title="">Yidan Long</a>, <a class="ltx_ref ltx_href" href="https://orcid.org/0009-0007-4530-2589" title="">Wenyi Tang</a>, <a class="ltx_ref ltx_href" href="https://orcid.org/0009-0005-8107-4590" title="">Xiang Li</a>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Hunan Normal University</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Changsha</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Hunan</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0003-3267-3317" title="">Can Liu</a>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">City University of Hong Kong</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2"></span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">Hong Kong</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0002-3020-3736" title="">Zhanpeng Jin</a>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">South China University of Technology</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">Guangdong</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><a class="ltx_ref ltx_href" href="https://orcid.org/0009-0007-9465-4273" title="">Wenlei Zhang</a>, <a class="ltx_ref ltx_href" href="https://orcid.org/0009-0007-9746-1557" title="">Tangquan Qi</a>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Wondershare Technology</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Changsha</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">Hunan</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">China</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id21.id1">The process of fashion design usually involves sketching, refining, and coloring, with designers drawing inspiration from various images to fuel their creative endeavors. However, conventional image search methods often yield irrelevant results, impeding the design process. Moreover, creating and coloring sketches can be time-consuming and demanding, acting as a bottleneck in the design workflow. In this work, we introduce HAIGEN (<span class="ltx_text ltx_font_bold" id="id21.id1.1">H</span>uman-<span class="ltx_text ltx_font_bold" id="id21.id1.2">AI</span> Collaboration for <span class="ltx_text ltx_font_bold" id="id21.id1.3">GEN</span>eration), an efficient fashion design system for Human-AI collaboration developed to aid designers. Specifically, HAIGEN consists of four modules. T2IM, located in the cloud, generates reference inspiration images directly from text prompts. With three other modules situated locally, the I2SM batch generates the image material library into a certain designer-style sketch material library. The SRM recommends similar sketches in the generated library to designers for further refinement, and the STM colors the refined sketch according to the styles of inspiration images. Through our system, any designer can perform local personalized fine-tuning and leverage the powerful generation capabilities of large models in the cloud, streamlining the entire design development process. Given that our approach integrates both cloud and local model deployment schemes, it effectively safeguards design privacy by avoiding the need to upload personalized data from local designers. We validated the effectiveness of each module through extensive qualitative and quantitative experiments. User surveys also confirmed that HAIGEN offers significant advantages in design efficiency, positioning it as a new generation of aid-tool for designers. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (ACM IMWUT/UbiComp 2024). Please refer to the formal version for more details: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3678518" title="">https://dl.acm.org/doi/10.1145/3678518</a>.</span></span></span></p>
</div>
<div class="ltx_keywords">Human-AI Collaboration; Generative Artificial Intelligence; Personalized Fashion Design
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Arts and humanities</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>INTRODUCTION</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the traditional fashion design process, when designers conceive new ideas, they often begin by using sketches to roughly outline the overall structure of their creations. Subsequently, they search for relevant sources of material inspiration, often turning to various online platforms, design publications, and other channels to refine and iterate upon their sketch designs. Notably, the initial sketching stage can be challenging, especially for junior designers. Accessing pre-existing sketch templates for customization and modification would thus serve as a valuable resource for many designers. Following the sketching stage, designers often seek inspiration to breathe life into their creations through a coloring process, enhancing the depth and realism of the sketches. However, this stage is laborious and time-consuming. Therefore, a quick sketch coloring tool would enable them to preview the sketch coloring effect in advance and speed up their design process. Throughout the entire design process, searching for design inspiration is crucial <cite class="ltx_cite ltx_citemacro_citep">(Jonson, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib25" title="">2005</a>)</cite>. However, designers often encounter difficulties due to the limited results provided by many search engine functions. This frequently results in designers forgetting key details of their original inspiration, significantly impacting their overall design efficiency.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="285" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">(a) Three distinct methods of deriving sketches from a provided clothing image are presented, highlighting noticeable stylistic differences among them. (b) Highlights three issues that emerge during the sketch coloring process, influencing the designer’s overall creative workflow. (c) Our approach distinguishes itself from previous methods used for searching inspirations. With our method, it can generate images that closely align with the designer’s inner thoughts based on detailed text descriptions.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent years have witnessed substantial advancements in Generative Artificial Intelligence (AI), particularly with the introduction of Generative Adversarial Networks (GAN) <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib11" title="">2014</a>)</cite> and Diffusion Models (DM) <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib16" title="">2020</a>)</cite>. These innovations have revolutionized the field of image generation by enabling the creation of highly realistic, diverse, and novel objects. This transformation is reshaping the landscape of the design field, leading an increasing number of designers to incorporate artificial intelligence tools into their creative processes.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the sketching stage, most methods are limited to extracting the outline and partial details of objects <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>; Xie and Tu, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib60" title="">2015</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib54" title="">2017</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib64" title="">2021</a>; Bhunia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib2" title="">2022</a>)</cite>, making it challenging to accurately represent complex details with reasonable sketch strokes. There are obvious stylistic differences in the sketching strokes of different people. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S1.F1" title="Figure 1 ‣ 1. INTRODUCTION ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">1</span></a> (a) shows the differences between synthetic edge maps <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>)</cite>, sketches without professional knowledge, and sketches with professional knowledge. In the sketch coloring stage, most research <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib59" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib63" title="">2023b</a>)</cite> relies on GAN to achieve style transfer from sketches to images. However, these methods often lead to problems such as uneven color distribution, coloring outside the sketch outline, and loss of sketch stroke details, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S1.F1" title="Figure 1 ‣ 1. INTRODUCTION ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">1</span></a> (b). These problems make it difficult for designers to achieve a perfect preview of sketch shading effects, thereby reducing design efficiency and affecting the overall design experience.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the most critical stage of inspiration search in the entire creation, traditional methods often require designers to use as concise descriptors as possible to search for relevant materials. This is very challenging for people with weak language skills and professional abilities. <cite class="ltx_cite ltx_citemacro_citep">(Schultheiß and Lewandowski, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib47" title="">2023</a>)</cite> demonstrates that users with limited familiarity with search engines tend to depend more on traditional methods like Google, compared to those with greater expertise. This reliance increases the likelihood of obtaining inaccurate results. In recent years, tools such as DALL-E <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib38" title="">2021</a>)</cite> and Midjourney <cite class="ltx_cite ltx_citemacro_citep">(Midjourney, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib33" title="">2022</a>)</cite> have emerged, allowing designers to directly enter comprehensive descriptions of their ideas to generate inspiration materials that are as consistent as possible. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S1.F1" title="Figure 1 ‣ 1. INTRODUCTION ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">1</span></a> (c) provides a visual comparison between these two inspiration search methods. However, these tools impose significant hardware requirements making it difficult for designers to customize the model in personalized styles. Additionally, their openness raises concerns about potential privacy breaches, particularly regarding designers’ confidential inspirations.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we present the HAIGEN (<span class="ltx_text ltx_font_bold" id="S1.p5.1.1">H</span>uman-<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">AI</span> Collaboration for <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">GEN</span>eration), a Human-AI collaboration system aimed at enhancing the creative efficiency of fashion designers. It comprises the <span class="ltx_text ltx_font_italic" id="S1.p5.1.4">Text-to-Image Cloud Module</span> (T2IM) deployed in the cloud, as well as the <span class="ltx_text ltx_font_italic" id="S1.p5.1.5">Image-to-Sketch Local Module</span> (I2SM), <span class="ltx_text ltx_font_italic" id="S1.p5.1.6">Sketch Recommendation Module</span> (SRM), and <span class="ltx_text ltx_font_italic" id="S1.p5.1.7">Style Transfer Module</span> (STM) situated locally. Designers can swiftly access desired inspiration materials by harnessing the robust cross-modal generation capabilities offered by cloud-based large models. These materials can then be seamlessly integrated with multiple local small models to facilitate the progression of the entire design process. Specifically, T2IM, based on SD 1.5 <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>)</cite>, generates images from text prompts, streamlining material search and providing inspiration for designers. We employ the LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib17" title="">2021</a>)</cite> for fine-tuning SD and combine it with ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib61" title="">2023a</a>)</cite> to further optimize the edge, structure, color, and other details of the text-to-image generation process. I2SM is a GAN-based model for converting images into sketches, featuring the APSN module to capture a designer’s personalized sketch style. The DSMFF module is introduced to fuse contour and detail features within multi-scale features, thereby generating numerous designer-style sketches as initial templates. SRM, based on the ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib8" title="">2020</a>)</cite>, recommends similar sketch templates to designers. STM is a DDIM-based <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib49" title="">2020</a>)</cite> model employed for sketch coloring. It incorporates the content feature of the sketch and the style feature of the reference image into the model’s diffusion process explicitly and implicitly. Utilizing our innovative CCAM, it can generate high-quality sketch coloring renderings based on the style of reference images. Notably, our approach, structured with a cloud and local separation architecture, effectively safeguards design privacy by exclusively downloading inspirations generated in the cloud without the need to upload local personalized data, distinguishing it from other existing methods <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib38" title="">2021</a>; Midjourney, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib33" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, our contributions can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a Human-AI collaborative auxiliary design system, allowing designers to combine cloud-based large models and local-based small models to achieve efficient design processes. All models can be personalized and fine-tuned, providing a user-friendly experience for designers to pursue personalized design. Moreover, the integration of cloud and local architecture ensures stable privacy protection for designers’ creativity.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce a comprehensive SD model deployed in the cloud as a text-to-image cross-modal solution. This innovation enables the model to rapidly learn new image styles and adjust details within the generated image. By inputting relevant text prompts, it can generate multi-style, multi-aspect ratio, and multi-resolution reference images. This feature empowers designers by efficiently providing rich design inspiration.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our image-to-sketch generation model offers a distinctive approach to capturing designer style. By leveraging the innovative APSN module, it learns design style features from designers’ historical data and seamlessly incorporates these style features into newly generated sketches. This capability empowers designers to create adequate sketch templates with personalized styles rapidly, thereby enhancing overall design efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Our local-based sketch-to-image style fusion framework represents a breakthrough approach to sketch coloring stylization. We create a supervised diffusion model by explicitly conditioning on locally refined personalized sketches and implicitly conditioning on cloud-generated reference images. The resultant image encapsulates the style of the reference image while preserving the unique design style and intricate details of the sketch.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Note:</span> Our HAIGEN system possesses the following key characteristics: (i) <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p8.1.2">Distributed System</span>: We introduce a cloud-local collaboration solution, harnessing the powerful generation capabilities of large cloud models to inspire design creativity. Complemented by a series of local small models, this approach facilitates the entire creative process. Designers can seamlessly integrate inspirational reference images generated in the cloud into the local development process, fostering efficient design and development. Furthermore, the solution effectively isolates local development data from the cloud, ensuring user privacy protection. (ii) <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p8.1.3">Ubiquitous Computing</span>: We present a Human-AI collaboration solution, facilitating rapid personalized fine-tuning of styles and multi-style fusion for cloud models. Additionally, our solution achieves lightweight design, as well as the capture and fusion of designers’ historical creative styles for local models. This enables any designer in any scenario to seamlessly integrate our system at a low cost for personalized fashion design development, thereby enhancing overall design efficiency. (iii) <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p8.1.4">Applicability to Different Fields</span>: With the trend of large-scale model development shifting towards vertical field applications, and Human-AI collaborative development emerging as an inevitable trend, our model cloud-local collaboration and personalized fine-tuning concepts are versatile and can be applied to various fields. These include medical image processing in the medical field, model structure design in the architectural field, teaching plan arrangement in the education field, and more.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The subsequent sections of this paper are structured as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S2" title="2. Related Work ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">2</span></a> provides the related works. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3" title="3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">3</span></a> encompasses a user study and our system overview. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4" title="4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4</span></a> conducts a detailed exposition of each module in our system. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5" title="5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">5</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6" title="6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">6</span></a> present the experimental results and user surveys. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S7" title="7. Conclusions ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">7</span></a> concludes our work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Generative AI in Fashion Design</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Generative AI in fashion design could be composed of inspiration searching stage, sketching stage, and sketch coloring stage. The inspiration searching stage corresponds to the Text-to-Image Generation task. Early methods, like Srivastava <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Srivastava and Salakhutdinov, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib50" title="">2012</a>)</cite>, attempted to employ deep Boltzmann machines for learning the modeling of images and text. However, these approaches were notably constrained in terms of complexity and realism. With the advent of Generative Adversarial Networks (GAN) <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib11" title="">2014</a>)</cite>, particularly Conditional GAN <cite class="ltx_cite ltx_citemacro_citep">(Mirza and Osindero, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib34" title="">2014</a>)</cite>, models <cite class="ltx_cite ltx_citemacro_citep">(Reed et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib39" title="">2016</a>; Sarafianos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib46" title="">2019</a>)</cite> have been developed that can effectively capture the intricate relationship between text and images. In recent years, the introduction of models such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>)</cite> and CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib37" title="">2021</a>)</cite> has ushered in a new era for text-to-image generation. Some of these newer models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib61" title="">2023a</a>; Ruiz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib43" title="">2023</a>; Kumari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib29" title="">2023</a>)</cite> are capable of generating more finely detailed images and offer greater control over the generated visuals.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The sketching stage corresponds to the Image-to-Sketch Generation task. Conventional edge extraction algorithms like Canny <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>)</cite> and HED <cite class="ltx_cite ltx_citemacro_citep">(Xie and Tu, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib60" title="">2015</a>)</cite> are inadequate in addressing these challenges. The images generated using these methods often exhibit poor quality and lack style consistency. Ha <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Ha and Eck, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib12" title="">2017</a>)</cite> employed recurrent neural networks, Ge <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib10" title="">2020</a>)</cite> introduced a part-based generative adversarial network. Bhunia <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Bhunia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib2" title="">2022</a>)</cite> proposed a two-stage framework. However, they ignore the unique design styles of different sketchers, and some of them are tailored for smaller objects. In the case of fashion clothing data, where objects are more detailed and complicated, it becomes crucial to accurately represent the diverse design styles and design skills of different designers.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The sketch coloring stage corresponds to the Sketch-based Style Transfer task. The introduction of AdaIN <cite class="ltx_cite ltx_citemacro_citep">(Huang and Belongie, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib18" title="">2017</a>)</cite> offers a practical solution. Li <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib30" title="">2020</a>)</cite> decompose the task into texture synthesis and shading enhancement. Subsequently, Liu <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>)</cite> proposed a two-stage style transfer framework to enhance the images generated in the first stage. StyleMe <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib59" title="">2023</a>)</cite> further elevates the style transfer quality with a deeper network and a consistent loss function. Zhang <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib63" title="">2023b</a>)</cite> employed a contrastive learning method to regulate the style of generated images. However, these methods still face challenges in handling complex strokes and rectifying synthetic structures to achieve realistic images. Therefore, synthesizing images that accurately depict the content of the sketch is of paramount importance, all the while ensuring a cohesive style that aligns with the reference image.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Human-AI Collaboration</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The rapid development of Artificial Intelligence (AI) technology has seen numerous algorithms transition from research laboratories to practical, real-world applications <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib53" title="">2020</a>)</cite>. This transformative progress has led various fields to evolve from a paradigm of individual human efforts to a collaborative framework where humans and AI work in cooperation. For instance, Prajwal <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib36" title="">2023</a>)</cite> proposed a Human-AI collaborative emotion self-report collection framework based on smartphone keyboards to sense user emotions. Sun <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib51" title="">2023</a>)</cite> introduced a customizable AI chatbot to explore innovative collaborative methods of illustration art and AI through chats with illustrators. Cho <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Cho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib5" title="">2023</a>)</cite> deployed AI in an actual home full of sensors and interactive devices to comprehensively study the experience of integrating AI-driven technology into daily home life. Moreover, Figueiredo <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.4">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Figueiredo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib9" title="">2024</a>)</cite> examined the impact of AI descriptions to aid health consumers in making personal health decisions based on user data and algorithmic output. Deng <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.5">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib6" title="">2024</a>)</cite> proposed a solution for multiple people to work collaboratively with AI tools on different devices and optimize network bandwidth to reduce latency through Lyaplov. All these studies confirm the effectiveness of Human-AI collaborative solutions.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Within the realm of design, traditional design methods require designers to autonomously navigate the entire design process, resulting in inefficiencies and even superfluous efforts. For instance, in the inspiration searching stage, designers aspire to swiftly locate the references that inspire them the most. During the sketching phase, most designers prefer to directly modify relevant templates. Furthermore, in the coloring stage, designers seek the ability to swiftly preview the suitable colors in advance and subsequently make relevant adjustments. Therefore, the development of an AI tool assumes paramount importance in minimizing the time and energy designers expend on non-innovative stages. This not only streamlines the design process but also enhances the overall efficiency of design endeavors.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Privacy Protection</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">As our world becomes increasingly technologically integrated <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib58" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib56" title="">2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib57" title="">2022b</a>; Ikeda et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib19" title="">2023</a>; Sailaja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib44" title="">2023</a>)</cite>, the generation and aggregation of substantial data, often encompassing sensitive personal information, has become ubiquitous. This proliferation has raised critical concerns regarding the privacy and security of personal data, particularly in the context of software applications, websites, and digital platforms <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib24" title="">2022</a>; Saisho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib45" title="">2023</a>)</cite>. In the realm of design, innovative concepts hold paramount importance. With the continuous advancement of image generation technology, designers naturally gravitate towards leveraging artificial intelligence tools like DALL-E <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib38" title="">2021</a>)</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>)</cite>, and Midjourney <cite class="ltx_cite ltx_citemacro_citep">(Midjourney, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib33" title="">2022</a>)</cite> to enhance their creative processes. However, this introduces the potential for data privacy breaches at every stage, from concept and development to utilization. In response to this challenge, we have devised a framework that seamlessly combines a cloud-based large generation model with local fine-tuning models. This integration effectively safeguards sensitive information, such as designers’ personalized sketches and final design projects, shielding user privacy and enhancing data security.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>User Study and System Overview</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>User Study</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Questionnaires</h4>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="144" id="S3.F2.g1" src="x2.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Basic information about the respondents.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We invited and collected responses from 51 participants by distributing online questionnaires to individuals in the fashion design industry, including design students, design teachers, and professional designers. The questionnaire comprised a total of 17 questions, covering the respondent’s basic information, opinions on traditional design tools, and perspectives on AI-assisted design tools, etc. Initially, our focus was on understanding user demographics, encompassing gender, age, occupation, and design experience. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.F2" title="Figure 2 ‣ 3.1.1. Questionnaires ‣ 3.1. User Study ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">2</span></a>, most of the respondents were female, constituting 76.67%, with the highest representation in the 18-25 age bracket at 66.67%. In terms of occupation, professional designers account for 23.53%. Notably, design students emerged as the predominant group, constituting the majority and representing the prospective driving force in the future fashion design landscape.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S3.F3.g1" src="x3.png" width="714"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Gain insights into the current landscape of fashion designers and unearth their specific needs for AI tools.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Subsequent inquiries delved into participants’ design practices and perceptions of current design efficiency. We present the main results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.F3" title="Figure 3 ‣ 3.1.1. Questionnaires ‣ 3.1. User Study ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">3</span></a>, a significant 80.39% indicated reliance on internet searches for design inspiration. And 47.06% and 41.18% favored collecting clothing renderings and clothing model try-on images. An overwhelming 80.39% of respondents indicated that traditional online search methods often yield irrelevant images, this may lead to designers spending significant time on ineffective inspiration retrieval and consequently affecting their overall design efficiency. Further questions probed respondents’ perspectives on AI-generated content. An overwhelming 96% expressed openness to employing an auxiliary design system generating images based on textual input, while 98% deemed this approach beneficial for obtaining design inspiration. Regarding sketching, 94.12% believed that automated sketch generation from images would enhance design efficiency, but 58.82% expressed concerns about the generated sketches lacking their distinct hand-drawn style. Significantly, all respondents expressed a desire to use AI tools for subsequent clothing rendering after generating sketches. Privacy concerns were also prevalent, with 88.24% worried about the potential leakage of design ideas and 90.2% expressing concerns about the privacy of their design works. These findings illuminate user perspectives, emphasizing a readiness to embrace AI-driven design tools while underscoring privacy apprehensions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Requirements Analysis</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Based on the results of the above questionnaire responses from fashion designers, we have classified and summarized their needs as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Inspiration Resource.</span> Generate a plethora of suitable inspiration images for designers using a text-to-image model. Input text can range from brief to detailed descriptions, producing images like clothing renderings and clothing model try-ons. Text prompts allow adjustments to style, background, and other attributes. A novel large model training strategy facilitates rapid learning of new styles.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Personalized Style.</span> Utilize a vast collection of fashion images from the internet as a foundational image material library. Designers upload hand-drawn sketches from their personal archives, transforming basic image materials into numerous sketch templates infused with their unique design styles, available for further refinement at any time. Furthermore, designers can continually update the image library over time.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Coloring Sketch.</span> Apply a style fusion algorithm based on reference images to showcase the sketch coloring effect, enabling designers to seamlessly blend the refined sketch with the appearance of the inspiration image. This allows designers to rapidly preview the colored sketch and choose the most suitable color style.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Privacy Protection.</span> The design processes related to personalized inspiration for designers are deployed on local small models, while the inspiration reference non-personalized design process is deployed on the cloud large model. This approach enables the use of large models to generate impressive inspirational references, while leveraging smaller models to swiftly develop and safeguard local design concepts from potential leaks.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>System Overview</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="455" id="S3.F4.g1" src="x4.png" width="564"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">An Overview of our HAIGEN system. The left side (orange) illustrates the designer’s design process. On the right side, the Text-to-Image Cloud Module (blue) is deployed in the cloud, while the Image-to-Sketch Local Module, Sketch Recommendation Module, and Style Transfer Module (green) are deployed locally.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The system framework of our HAIGEN is depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.F4" title="Figure 4 ‣ 3.2. System Overview ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4</span></a>. On the left side (orange) is the designer’s design process, which begins with the initial design idea, proceeds to the creation of the first draft, further refinement of the sketch, and ends with the coloring of the sketch. On the right side, the system comprises two main components: the cloud (blue) and the local side (green). The <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Text-to-Image Cloud Module</span>, situated in the cloud, generates images based on detailed sentence descriptions to inspire design. The local side includes the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Image-to-Sketch Local Module</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Sketch Recommendation Module</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">Style Transfer Module</span>, which serve to generate sketch templates, recommend templates to designers, and color refined sketches, respectively. Below, we will provide detailed descriptions of these four modules:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Stable Diffusion Model-based Text-to-Image Cloud Module.</span> During the design idea phase, designers often seek inspiration by entering keywords on various online platforms. This process can be time-consuming and arduous, as designers often struggle to find relevant materials. Our method deploys an enhanced Stable Diffusion model <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>)</cite> in the cloud. Designers can provide detailed descriptions of their ideas, which helps the generated image material closely align with their creative vision. Designers can continuously adjust and refine the generated images, such as altering character backgrounds, appearances, clothing colors, styles, and more. This significantly streamlines the inspiration-finding process, resulting in the creation of groundbreaking designs that push the boundaries of fashion.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Capture Personalized Designer-Style Image-to-Sketch Local Module.</span> In the sketch drawing phase, we employed web crawler technology to establish a foundational image library for designers, accumulating a vast collection of fashion clothing images. Building upon this foundation, we developed an Image-to-Sketch Local Module, utilizing GAN <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib11" title="">2014</a>)</cite> to transform images from the library into sketches reflecting the designer’s individual style. This was achieved by learning the overarching style from historical sketch data uploaded by the designer. As a result, designers have access to a comprehensive sketch library that serves as a repository of sketch templates. Furthermore, they have the flexibility to continually expand the library over time.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Cloud Image-based Local Personalized Sketch Recommendation Module.</span> When a designer generates a clothing image through the Text-to-Image Cloud Module that they are satisfied with, they can input this image into the Sketch Recommendation Module. This module will recommend similar sketch templates from the library that closely match the clothing image. Designers can then select their preferred sketch template from these recommendations and further refine it to obtain the final sketch. This process significantly simplifies the initial sketching phase, enhancing design efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Style Transfer Module based on Local Personalized Sketch and Cloud Image.</span> During the sketch coloring stage, we’ve introduced a Style Transfer Module based on DDIM <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib49" title="">2020</a>)</cite>. In this phase, designers input the Refined Sketch and the reference Clothing Image obtained from the previous modules. This module generates images that preserve the unique design style and intricate sketch details of the Refined Sketch while incorporating the color style of the Clothing Image. It’s important to note that we ensure privacy protection for designers’ sketch ideas by deploying all sketch-related models locally.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we will introduce the technical implementation details of our HAIGEN system. It encompasses the <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Text-to-Image Cloud Module</span> (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1" title="4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.1</span></a>), which visualizes design ideas, the <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">Image-to-Sketch Local Module</span> (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2" title="4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.2</span></a>), dedicated to generating personalized sketches, the <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">Sketch Recommendation Module</span> (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS3" title="4.3. Cloud Image-based Local Personalized Sketch Recommendation Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.3</span></a>), facilitating the recommendation of similar sketch templates, and finally, the <span class="ltx_text ltx_font_italic" id="S4.p1.1.4">Style Transfer Module</span> (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4" title="4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.4</span></a>), used for sketch coloring.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Stable Diffusion Model-based Text-to-Image Cloud Module</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In our <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Text-to-Image Cloud Module</span> (T2IM), we generate a variety of high-quality clothing renderings based on text prompts, offering designers an intuitive sense of the designed clothing’s quality. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F5" title="Figure 5 ‣ 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">5</span></a>, our T2IM primarily comprises two components: one is a model based on Stable Diffusion (SD) model <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>)</cite>, and the other involves fine-tuning the SD model using LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib17" title="">2021</a>)</cite> and ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib61" title="">2023a</a>)</cite>. This fine-tuning accelerates the model training process and enhances the detailed information of the generated images.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="369" id="S4.F5.g1" src="x5.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.4.2.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S4.F5.2.1" style="font-size:90%;">The illustration of the Stable Diffusion Model-based Text-to-Image Cloud Module. We initially utilize the VAE <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib28" title="">2013</a>)</cite> model to map the input sample <math alttext="x" class="ltx_Math" display="inline" id="S4.F5.2.1.m1.1"><semantics id="S4.F5.2.1.m1.1b"><mi id="S4.F5.2.1.m1.1.1" xref="S4.F5.2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.F5.2.1.m1.1c"><ci id="S4.F5.2.1.m1.1.1.cmml" xref="S4.F5.2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.2.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S4.F5.2.1.m1.1e">italic_x</annotation></semantics></math> into the latent space for diffusion training. To expedite training and exert control over the style and details of the denoising process, we freeze the initial parameters of the SD model and incorporate LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib17" title="">2021</a>)</cite> and ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib61" title="">2023a</a>)</cite>.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Stable Diffusion Model</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Due to the potent image generation capabilities of the Diffusion Model (DM) <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib16" title="">2020</a>)</cite>, it has exhibited remarkable results in image synthesis and various applications. However, operating in pixel space poses high hardware and time costs for models with extensive parameter quantities. In contrast to DM, the SD model employs a variational autoencoder (VAE) <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib28" title="">2013</a>)</cite> for encoding and decoding input images based on DM. This addresses the challenge by mapping the input image into a potential low-dimensional representation space for diffusion operations.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.11">In the DM model, image generation is simplified to the process of noise adding and denoising, where all noise follows a Gaussian distribution. Assuming a total of <math alttext="T" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.1"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mi id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><ci id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.1d">italic_T</annotation></semantics></math> noise increments are added to the input image <math alttext="x" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.2.m2.1"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mi id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><ci id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.2.m2.1d">italic_x</annotation></semantics></math>, with each noise value represented as <math alttext="\epsilon_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.3.m3.1"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><msub id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml">ϵ</mi><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">italic-ϵ</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">\epsilon_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.3.m3.1d">italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> (where <math alttext="t\in(1,\dots,T)" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.4.m4.3"><semantics id="S4.SS1.SSS1.p2.4.m4.3a"><mrow id="S4.SS1.SSS1.p2.4.m4.3.4" xref="S4.SS1.SSS1.p2.4.m4.3.4.cmml"><mi id="S4.SS1.SSS1.p2.4.m4.3.4.2" xref="S4.SS1.SSS1.p2.4.m4.3.4.2.cmml">t</mi><mo id="S4.SS1.SSS1.p2.4.m4.3.4.1" xref="S4.SS1.SSS1.p2.4.m4.3.4.1.cmml">∈</mo><mrow id="S4.SS1.SSS1.p2.4.m4.3.4.3.2" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml"><mo id="S4.SS1.SSS1.p2.4.m4.3.4.3.2.1" stretchy="false" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml">(</mo><mn id="S4.SS1.SSS1.p2.4.m4.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.cmml">1</mn><mo id="S4.SS1.SSS1.p2.4.m4.3.4.3.2.2" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS1.p2.4.m4.2.2" mathvariant="normal" xref="S4.SS1.SSS1.p2.4.m4.2.2.cmml">…</mi><mo id="S4.SS1.SSS1.p2.4.m4.3.4.3.2.3" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS1.p2.4.m4.3.3" xref="S4.SS1.SSS1.p2.4.m4.3.3.cmml">T</mi><mo id="S4.SS1.SSS1.p2.4.m4.3.4.3.2.4" stretchy="false" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.4.m4.3b"><apply id="S4.SS1.SSS1.p2.4.m4.3.4.cmml" xref="S4.SS1.SSS1.p2.4.m4.3.4"><in id="S4.SS1.SSS1.p2.4.m4.3.4.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.3.4.1"></in><ci id="S4.SS1.SSS1.p2.4.m4.3.4.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.3.4.2">𝑡</ci><vector id="S4.SS1.SSS1.p2.4.m4.3.4.3.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.3.4.3.2"><cn id="S4.SS1.SSS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p2.4.m4.1.1">1</cn><ci id="S4.SS1.SSS1.p2.4.m4.2.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.2.2">…</ci><ci id="S4.SS1.SSS1.p2.4.m4.3.3.cmml" xref="S4.SS1.SSS1.p2.4.m4.3.3">𝑇</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.4.m4.3c">t\in(1,\dots,T)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.4.m4.3d">italic_t ∈ ( 1 , … , italic_T )</annotation></semantics></math>), the model <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.5.m5.1"><semantics id="S4.SS1.SSS1.p2.5.m5.1a"><mi id="S4.SS1.SSS1.p2.5.m5.1.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.5.m5.1b"><ci id="S4.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.5.m5.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.5.m5.1d">italic_θ</annotation></semantics></math> is trained to predict the noise value <math alttext="\epsilon_{\theta}(x_{t},t)" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.6.m6.2"><semantics id="S4.SS1.SSS1.p2.6.m6.2a"><mrow id="S4.SS1.SSS1.p2.6.m6.2.2" xref="S4.SS1.SSS1.p2.6.m6.2.2.cmml"><msub id="S4.SS1.SSS1.p2.6.m6.2.2.3" xref="S4.SS1.SSS1.p2.6.m6.2.2.3.cmml"><mi id="S4.SS1.SSS1.p2.6.m6.2.2.3.2" xref="S4.SS1.SSS1.p2.6.m6.2.2.3.2.cmml">ϵ</mi><mi id="S4.SS1.SSS1.p2.6.m6.2.2.3.3" xref="S4.SS1.SSS1.p2.6.m6.2.2.3.3.cmml">θ</mi></msub><mo id="S4.SS1.SSS1.p2.6.m6.2.2.2" xref="S4.SS1.SSS1.p2.6.m6.2.2.2.cmml">⁢</mo><mrow id="S4.SS1.SSS1.p2.6.m6.2.2.1.1" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.2.cmml"><mo id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.2" stretchy="false" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.2.cmml">(</mo><msub id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.2" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.2.cmml">x</mi><mi id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.3" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.3" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.2.cmml">,</mo><mi id="S4.SS1.SSS1.p2.6.m6.1.1" xref="S4.SS1.SSS1.p2.6.m6.1.1.cmml">t</mi><mo id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.4" stretchy="false" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.6.m6.2b"><apply id="S4.SS1.SSS1.p2.6.m6.2.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2"><times id="S4.SS1.SSS1.p2.6.m6.2.2.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.2"></times><apply id="S4.SS1.SSS1.p2.6.m6.2.2.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.6.m6.2.2.3.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.3">subscript</csymbol><ci id="S4.SS1.SSS1.p2.6.m6.2.2.3.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.3.2">italic-ϵ</ci><ci id="S4.SS1.SSS1.p2.6.m6.2.2.3.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.3.3">𝜃</ci></apply><interval closure="open" id="S4.SS1.SSS1.p2.6.m6.2.2.1.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1"><apply id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.2">𝑥</ci><ci id="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.2.2.1.1.1.3">𝑡</ci></apply><ci id="S4.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1">𝑡</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.6.m6.2c">\epsilon_{\theta}(x_{t},t)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.6.m6.2d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )</annotation></semantics></math> that closely matches <math alttext="\epsilon_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.7.m7.1"><semantics id="S4.SS1.SSS1.p2.7.m7.1a"><msub id="S4.SS1.SSS1.p2.7.m7.1.1" xref="S4.SS1.SSS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.SSS1.p2.7.m7.1.1.2" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.cmml">ϵ</mi><mi id="S4.SS1.SSS1.p2.7.m7.1.1.3" xref="S4.SS1.SSS1.p2.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.7.m7.1b"><apply id="S4.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2">italic-ϵ</ci><ci id="S4.SS1.SSS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.7.m7.1c">\epsilon_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.7.m7.1d">italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> at each time <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.8.m8.1"><semantics id="S4.SS1.SSS1.p2.8.m8.1a"><mi id="S4.SS1.SSS1.p2.8.m8.1.1" xref="S4.SS1.SSS1.p2.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.8.m8.1b"><ci id="S4.SS1.SSS1.p2.8.m8.1.1.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.8.m8.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.8.m8.1d">italic_t</annotation></semantics></math>. Here, <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.9.m9.1"><semantics id="S4.SS1.SSS1.p2.9.m9.1a"><msub id="S4.SS1.SSS1.p2.9.m9.1.1" xref="S4.SS1.SSS1.p2.9.m9.1.1.cmml"><mi id="S4.SS1.SSS1.p2.9.m9.1.1.2" xref="S4.SS1.SSS1.p2.9.m9.1.1.2.cmml">x</mi><mi id="S4.SS1.SSS1.p2.9.m9.1.1.3" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.9.m9.1b"><apply id="S4.SS1.SSS1.p2.9.m9.1.1.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.2">𝑥</ci><ci id="S4.SS1.SSS1.p2.9.m9.1.1.3.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.9.m9.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.9.m9.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is represented by the result of iteratively adding noise to <math alttext="x" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.10.m10.1"><semantics id="S4.SS1.SSS1.p2.10.m10.1a"><mi id="S4.SS1.SSS1.p2.10.m10.1.1" xref="S4.SS1.SSS1.p2.10.m10.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.10.m10.1b"><ci id="S4.SS1.SSS1.p2.10.m10.1.1.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.10.m10.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.10.m10.1d">italic_x</annotation></semantics></math> over the <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.11.m11.1"><semantics id="S4.SS1.SSS1.p2.11.m11.1a"><mi id="S4.SS1.SSS1.p2.11.m11.1.1" xref="S4.SS1.SSS1.p2.11.m11.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.11.m11.1b"><ci id="S4.SS1.SSS1.p2.11.m11.1.1.cmml" xref="S4.SS1.SSS1.p2.11.m11.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.11.m11.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.11.m11.1d">italic_t</annotation></semantics></math>-th time. We have:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{DM}=\mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}\Big{[}\|\epsilon-\epsilon%
_{\theta}(x_{t},t)\|_{2}^{2}\Big{]}." class="ltx_Math" display="block" id="S4.E1.m1.8"><semantics id="S4.E1.m1.8a"><mrow id="S4.E1.m1.8.8.1" xref="S4.E1.m1.8.8.1.1.cmml"><mrow id="S4.E1.m1.8.8.1.1" xref="S4.E1.m1.8.8.1.1.cmml"><msub id="S4.E1.m1.8.8.1.1.3" xref="S4.E1.m1.8.8.1.1.3.cmml"><mi id="S4.E1.m1.8.8.1.1.3.2" xref="S4.E1.m1.8.8.1.1.3.2.cmml">L</mi><mrow id="S4.E1.m1.8.8.1.1.3.3" xref="S4.E1.m1.8.8.1.1.3.3.cmml"><mi id="S4.E1.m1.8.8.1.1.3.3.2" xref="S4.E1.m1.8.8.1.1.3.3.2.cmml">D</mi><mo id="S4.E1.m1.8.8.1.1.3.3.1" xref="S4.E1.m1.8.8.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.8.8.1.1.3.3.3" xref="S4.E1.m1.8.8.1.1.3.3.3.cmml">M</mi></mrow></msub><mo id="S4.E1.m1.8.8.1.1.2" xref="S4.E1.m1.8.8.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.8.8.1.1.1" xref="S4.E1.m1.8.8.1.1.1.cmml"><msub id="S4.E1.m1.8.8.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.3.cmml"><mi id="S4.E1.m1.8.8.1.1.1.3.2" xref="S4.E1.m1.8.8.1.1.1.3.2.cmml">𝔼</mi><mrow id="S4.E1.m1.6.6.6.6" xref="S4.E1.m1.6.6.6.7.cmml"><mrow id="S4.E1.m1.6.6.6.6.1" xref="S4.E1.m1.6.6.6.6.1.cmml"><mrow id="S4.E1.m1.6.6.6.6.1.2.2" xref="S4.E1.m1.6.6.6.6.1.2.1.cmml"><mi id="S4.E1.m1.3.3.3.3" xref="S4.E1.m1.3.3.3.3.cmml">x</mi><mo id="S4.E1.m1.6.6.6.6.1.2.2.1" xref="S4.E1.m1.6.6.6.6.1.2.1.cmml">,</mo><mi id="S4.E1.m1.4.4.4.4" xref="S4.E1.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S4.E1.m1.6.6.6.6.1.1" xref="S4.E1.m1.6.6.6.6.1.1.cmml">∼</mo><mrow id="S4.E1.m1.6.6.6.6.1.3" xref="S4.E1.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.6.6.6.6.1.3.2" xref="S4.E1.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo id="S4.E1.m1.6.6.6.6.1.3.1" xref="S4.E1.m1.6.6.6.6.1.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.6.6.6.6.1.3.3.2" xref="S4.E1.m1.6.6.6.6.1.3.3.1.cmml"><mo id="S4.E1.m1.6.6.6.6.1.3.3.2.1" stretchy="false" xref="S4.E1.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">0</mn><mo id="S4.E1.m1.6.6.6.6.1.3.3.2.2" xref="S4.E1.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mn id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml">1</mn><mo id="S4.E1.m1.6.6.6.6.1.3.3.2.3" stretchy="false" xref="S4.E1.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.6.6.6.6.2" xref="S4.E1.m1.6.6.6.7a.cmml">,</mo><mi id="S4.E1.m1.5.5.5.5" xref="S4.E1.m1.5.5.5.5.cmml">t</mi></mrow></msub><mo id="S4.E1.m1.8.8.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.8.8.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.2.cmml"><mo id="S4.E1.m1.8.8.1.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S4.E1.m1.8.8.1.1.1.1.2.1.cmml">[</mo><msubsup id="S4.E1.m1.8.8.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml">ϵ</mi><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ϵ</mi><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">θ</mi></msub><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S4.E1.m1.7.7" xref="S4.E1.m1.7.7.cmml">t</mi><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E1.m1.8.8.1.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S4.E1.m1.8.8.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S4.E1.m1.8.8.1.1.1.1.1.3" maxsize="160%" minsize="160%" xref="S4.E1.m1.8.8.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E1.m1.8.8.1.2" lspace="0em" xref="S4.E1.m1.8.8.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.8b"><apply id="S4.E1.m1.8.8.1.1.cmml" xref="S4.E1.m1.8.8.1"><eq id="S4.E1.m1.8.8.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.2"></eq><apply id="S4.E1.m1.8.8.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.3.1.cmml" xref="S4.E1.m1.8.8.1.1.3">subscript</csymbol><ci id="S4.E1.m1.8.8.1.1.3.2.cmml" xref="S4.E1.m1.8.8.1.1.3.2">𝐿</ci><apply id="S4.E1.m1.8.8.1.1.3.3.cmml" xref="S4.E1.m1.8.8.1.1.3.3"><times id="S4.E1.m1.8.8.1.1.3.3.1.cmml" xref="S4.E1.m1.8.8.1.1.3.3.1"></times><ci id="S4.E1.m1.8.8.1.1.3.3.2.cmml" xref="S4.E1.m1.8.8.1.1.3.3.2">𝐷</ci><ci id="S4.E1.m1.8.8.1.1.3.3.3.cmml" xref="S4.E1.m1.8.8.1.1.3.3.3">𝑀</ci></apply></apply><apply id="S4.E1.m1.8.8.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1"><times id="S4.E1.m1.8.8.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.2"></times><apply id="S4.E1.m1.8.8.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.3.1.cmml" xref="S4.E1.m1.8.8.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.8.8.1.1.1.3.2.cmml" xref="S4.E1.m1.8.8.1.1.1.3.2">𝔼</ci><apply id="S4.E1.m1.6.6.6.7.cmml" xref="S4.E1.m1.6.6.6.6"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.7a.cmml" xref="S4.E1.m1.6.6.6.6.2">formulae-sequence</csymbol><apply id="S4.E1.m1.6.6.6.6.1.cmml" xref="S4.E1.m1.6.6.6.6.1"><csymbol cd="latexml" id="S4.E1.m1.6.6.6.6.1.1.cmml" xref="S4.E1.m1.6.6.6.6.1.1">similar-to</csymbol><list id="S4.E1.m1.6.6.6.6.1.2.1.cmml" xref="S4.E1.m1.6.6.6.6.1.2.2"><ci id="S4.E1.m1.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3">𝑥</ci><ci id="S4.E1.m1.4.4.4.4.cmml" xref="S4.E1.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S4.E1.m1.6.6.6.6.1.3.cmml" xref="S4.E1.m1.6.6.6.6.1.3"><times id="S4.E1.m1.6.6.6.6.1.3.1.cmml" xref="S4.E1.m1.6.6.6.6.1.3.1"></times><ci id="S4.E1.m1.6.6.6.6.1.3.2.cmml" xref="S4.E1.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S4.E1.m1.6.6.6.6.1.3.3.1.cmml" xref="S4.E1.m1.6.6.6.6.1.3.3.2"><cn id="S4.E1.m1.1.1.1.1.cmml" type="integer" xref="S4.E1.m1.1.1.1.1">0</cn><cn id="S4.E1.m1.2.2.2.2.cmml" type="integer" xref="S4.E1.m1.2.2.2.2">1</cn></interval></apply></apply><ci id="S4.E1.m1.5.5.5.5.cmml" xref="S4.E1.m1.5.5.5.5">𝑡</ci></apply></apply><apply id="S4.E1.m1.8.8.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.8.8.1.1.1.1.2.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2"></minus><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3">italic-ϵ</ci><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.2">italic-ϵ</ci><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3.3">𝜃</ci></apply><interval closure="open" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S4.E1.m1.7.7.cmml" xref="S4.E1.m1.7.7">𝑡</ci></interval></apply></apply></apply><cn id="S4.E1.m1.8.8.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.8.8.1.1.1.1.1.1.1.3">2</cn></apply><cn id="S4.E1.m1.8.8.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.8.8.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.8c">L_{DM}=\mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}\Big{[}\|\epsilon-\epsilon%
_{\theta}(x_{t},t)\|_{2}^{2}\Big{]}.</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.8d">italic_L start_POSTSUBSCRIPT italic_D italic_M end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_x , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.4">In the SD model, besides the noise adding and denoising process similar to that in DM, it includes an additional step of encoding and decoding the input image <math alttext="x" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.1.m1.1"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mi id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><ci id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.1.m1.1d">italic_x</annotation></semantics></math> within the latent space through the VAE model <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.2.m2.1"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><ci id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.2.m2.1d">caligraphic_E</annotation></semantics></math>. And for conditional diffusion with text input, we feed the text prompt into the CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib37" title="">2021</a>)</cite> text model <math alttext="\mathcal{E}_{clip}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.3.m3.1"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><msub id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS1.p3.3.m3.1.1.2" xref="S4.SS1.SSS1.p3.3.m3.1.1.2.cmml">ℰ</mi><mrow id="S4.SS1.SSS1.p3.3.m3.1.1.3" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.cmml"><mi id="S4.SS1.SSS1.p3.3.m3.1.1.3.2" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.2.cmml">c</mi><mo id="S4.SS1.SSS1.p3.3.m3.1.1.3.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p3.3.m3.1.1.3.3" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.3.cmml">l</mi><mo id="S4.SS1.SSS1.p3.3.m3.1.1.3.1a" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p3.3.m3.1.1.3.4" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.4.cmml">i</mi><mo id="S4.SS1.SSS1.p3.3.m3.1.1.3.1b" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p3.3.m3.1.1.3.5" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><apply id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.2">ℰ</ci><apply id="S4.SS1.SSS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3"><times id="S4.SS1.SSS1.p3.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.1"></times><ci id="S4.SS1.SSS1.p3.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.2">𝑐</ci><ci id="S4.SS1.SSS1.p3.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.3">𝑙</ci><ci id="S4.SS1.SSS1.p3.3.m3.1.1.3.4.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.4">𝑖</ci><ci id="S4.SS1.SSS1.p3.3.m3.1.1.3.5.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">\mathcal{E}_{clip}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.3.m3.1d">caligraphic_E start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT</annotation></semantics></math> to generate feature vector <math alttext="\boldsymbol{p}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.4.m4.1"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><mi id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><ci id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">\boldsymbol{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.4.m4.1d">bold_italic_p</annotation></semantics></math>. Throughout the SD model training process, the feature vector is incorporated into the model’s diffusion process through the cross-attention module in U-Net, enabling control over the generated image:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{SD_{T}}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{p},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}(%
\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{p}))\|_{2}^{2}\bigg{]}," class="ltx_math_unparsed" display="block" id="S4.E2.m1.9"><semantics id="S4.E2.m1.9a"><mrow id="S4.E2.m1.9b"><msub id="S4.E2.m1.9.10"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.9.10.2">ℒ</mi><mrow id="S4.E2.m1.9.10.3"><mi id="S4.E2.m1.9.10.3.2">S</mi><mo id="S4.E2.m1.9.10.3.1">⁢</mo><msub id="S4.E2.m1.9.10.3.3"><mi id="S4.E2.m1.9.10.3.3.2">D</mi><mi id="S4.E2.m1.9.10.3.3.3">T</mi></msub></mrow></msub><mo id="S4.E2.m1.9.11">=</mo><msub id="S4.E2.m1.9.12"><mi id="S4.E2.m1.9.12.2">𝔼</mi><mrow id="S4.E2.m1.7.7.7.7"><mrow id="S4.E2.m1.7.7.7.7.1"><mrow id="S4.E2.m1.7.7.7.7.1.1.1"><mrow id="S4.E2.m1.7.7.7.7.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.7.7.7.7.1.1.1.1.2">ℰ</mi><mo id="S4.E2.m1.7.7.7.7.1.1.1.1.1">⁢</mo><mrow id="S4.E2.m1.7.7.7.7.1.1.1.1.3.2"><mo id="S4.E2.m1.7.7.7.7.1.1.1.1.3.2.1" stretchy="false">(</mo><mi id="S4.E2.m1.1.1.1.1">x</mi><mo id="S4.E2.m1.7.7.7.7.1.1.1.1.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.E2.m1.7.7.7.7.1.1.1.2">,</mo><mi id="S4.E2.m1.4.4.4.4">𝒑</mi><mo id="S4.E2.m1.7.7.7.7.1.1.1.3">,</mo><mi id="S4.E2.m1.5.5.5.5">ϵ</mi></mrow><mo id="S4.E2.m1.7.7.7.7.1.2">∼</mo><mrow id="S4.E2.m1.7.7.7.7.1.3"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.7.7.7.7.1.3.2">𝒩</mi><mo id="S4.E2.m1.7.7.7.7.1.3.1">⁢</mo><mrow id="S4.E2.m1.7.7.7.7.1.3.3.2"><mo id="S4.E2.m1.7.7.7.7.1.3.3.2.1" stretchy="false">(</mo><mn id="S4.E2.m1.2.2.2.2">0</mn><mo id="S4.E2.m1.7.7.7.7.1.3.3.2.2">,</mo><mn id="S4.E2.m1.3.3.3.3">1</mn><mo id="S4.E2.m1.7.7.7.7.1.3.3.2.3" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E2.m1.7.7.7.7.2">,</mo><mi id="S4.E2.m1.6.6.6.6">𝒕</mi></mrow></msub><mrow id="S4.E2.m1.9.13"><mo id="S4.E2.m1.9.13.1" maxsize="210%" minsize="210%">[</mo><mo id="S4.E2.m1.9.13.2" lspace="0em" rspace="0.167em">∥</mo><mi id="S4.E2.m1.9.13.3">ϵ</mi><mo id="S4.E2.m1.9.13.4">−</mo><msub id="S4.E2.m1.9.13.5"><mi id="S4.E2.m1.9.13.5.2">ϵ</mi><mi id="S4.E2.m1.9.13.5.3">θ</mi></msub><mrow id="S4.E2.m1.9.13.6"><mo id="S4.E2.m1.9.13.6.1" stretchy="false">(</mo><msub id="S4.E2.m1.9.13.6.2"><mi id="S4.E2.m1.9.13.6.2.2">𝒛</mi><mi id="S4.E2.m1.9.13.6.2.3">t</mi></msub><mo id="S4.E2.m1.9.13.6.3">,</mo><mi id="S4.E2.m1.8.8">𝒕</mi><mo id="S4.E2.m1.9.13.6.4">,</mo><mi id="S4.E2.m1.9.9">𝒑</mi><mo id="S4.E2.m1.9.13.6.5" stretchy="false">)</mo></mrow><mo id="S4.E2.m1.9.13.7" stretchy="false">)</mo></mrow><msubsup id="S4.E2.m1.9.14"><mo id="S4.E2.m1.9.14.2.2" lspace="0em" rspace="0.167em">∥</mo><mn id="S4.E2.m1.9.14.2.3">2</mn><mn id="S4.E2.m1.9.14.3">2</mn></msubsup><mo id="S4.E2.m1.9.15" maxsize="210%" minsize="210%">]</mo><mo id="S4.E2.m1.9.16">,</mo></mrow><annotation encoding="application/x-tex" id="S4.E2.m1.9c">\mathcal{L}_{SD_{T}}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{p},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}(%
\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{p}))\|_{2}^{2}\bigg{]},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.9d">caligraphic_L start_POSTSUBSCRIPT italic_S italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , bold_italic_p , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , bold_italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_t , bold_italic_p ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.3">where <math alttext="z_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p4.1.m1.1"><semantics id="S4.SS1.SSS1.p4.1.m1.1a"><msub id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p4.1.m1.1.1.2" xref="S4.SS1.SSS1.p4.1.m1.1.1.2.cmml">z</mi><mi id="S4.SS1.SSS1.p4.1.m1.1.1.3" xref="S4.SS1.SSS1.p4.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.1b"><apply id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1.2">𝑧</ci><ci id="S4.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.1c">z_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p4.1.m1.1d">italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents the outcome after adding noise to <math alttext="z" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p4.2.m2.1"><semantics id="S4.SS1.SSS1.p4.2.m2.1a"><mi id="S4.SS1.SSS1.p4.2.m2.1.1" xref="S4.SS1.SSS1.p4.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.2.m2.1b"><ci id="S4.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p4.2.m2.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.2.m2.1c">z</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p4.2.m2.1d">italic_z</annotation></semantics></math> for the <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p4.3.m3.1"><semantics id="S4.SS1.SSS1.p4.3.m3.1a"><mi id="S4.SS1.SSS1.p4.3.m3.1.1" xref="S4.SS1.SSS1.p4.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.3.m3.1b"><ci id="S4.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p4.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p4.3.m3.1d">italic_t</annotation></semantics></math>-th iteration.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>SD Model Fine-tuning</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.5">A crucial paradigm in natural language processing involves large-scale pre-training on general domain data, followed by adaptation to specific tasks or domains. However, when dealing with extremely large pre-trained models, complete fine-tuning of all model parameters through retraining becomes infeasible. To address this challenge, Hu <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.5.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib17" title="">2021</a>)</cite> introduced the low-rank adaptation (LoRA) by preserving the pre-trained model weights and introducing a trainable rank decomposition matrix into each layer of the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib52" title="">2017</a>)</cite> architecture. This approach significantly reduces the number of trainable parameters for task-specific adaptation. Inspired by LoRA, our fine-tuning of the SD model involves training only the model residuals, utilizing a low-rank matrix to represent the learned content. This not only accelerates model training but also enables diverse transformations of images in specific styles through a combination of various pre-trained LoRA models and SD weights during the inference process, providing a plug-and-play solution. Specifically, we conducted retraining of the weights associated with <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><ci id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">italic_Q</annotation></semantics></math>, <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mi id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><ci id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">italic_K</annotation></semantics></math>, and <math alttext="V" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mi id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><ci id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.3.m3.1d">italic_V</annotation></semantics></math> within the cross-attention block of the SD model. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F5" title="Figure 5 ‣ 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">5</span></a> [LoRA], during the model training process, we freeze the original weight values <math alttext="W_{0}\in\mathbb{R}^{d\times k}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.4.m4.1"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><mrow id="S4.SS1.SSS2.p1.4.m4.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.cmml"><msub id="S4.SS1.SSS2.p1.4.m4.1.1.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.2.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.2.cmml">W</mi><mn id="S4.SS1.SSS2.p1.4.m4.1.1.2.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS1.SSS2.p1.4.m4.1.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS2.p1.4.m4.1.1.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.3.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS2.p1.4.m4.1.1.3.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.2.cmml">d</mi><mo id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.1b"><apply id="S4.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1"><in id="S4.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.1"></in><apply id="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.1.1.2.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.1.1.2.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.2">𝑊</ci><cn id="S4.SS1.SSS2.p1.4.m4.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.3">0</cn></apply><apply id="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3"><times id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.1"></times><ci id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.2">𝑑</ci><ci id="S4.SS1.SSS2.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.1c">W_{0}\in\mathbb{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.4.m4.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> while incorporating the concept of residual to introduce a new branch utilizing a low-rank matrix. By utilizing low-rank matrices, we were able to capture new style features and derive a new set of weight values <math alttext="\Delta W" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.5.m5.1"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mrow id="S4.SS1.SSS2.p1.5.m5.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS2.p1.5.m5.1.1.2" mathvariant="normal" xref="S4.SS1.SSS2.p1.5.m5.1.1.2.cmml">Δ</mi><mo id="S4.SS1.SSS2.p1.5.m5.1.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS1.SSS2.p1.5.m5.1.1.3" xref="S4.SS1.SSS2.p1.5.m5.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.m5.1b"><apply id="S4.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1"><times id="S4.SS1.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.1"></times><ci id="S4.SS1.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.2">Δ</ci><ci id="S4.SS1.SSS2.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.m5.1c">\Delta W</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.5.m5.1d">roman_Δ italic_W</annotation></semantics></math>. Subsequently, during the model inference process, we effectively managed to control the generated image style by integrating the feature outputs from these two sets of weights:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y=W_{0}f+\Delta Wf=W_{0}f+BAf," class="ltx_Math" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml">y</mi><mo id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.4.cmml"><mrow id="S4.E3.m1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.4.2.cmml"><msub id="S4.E3.m1.1.1.1.1.4.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.4.2.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.4.2.2.3" xref="S4.E3.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.4.2.1" xref="S4.E3.m1.1.1.1.1.4.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.4.2.3" xref="S4.E3.m1.1.1.1.1.4.2.3.cmml">f</mi></mrow><mo id="S4.E3.m1.1.1.1.1.4.1" xref="S4.E3.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.4.3.cmml"><mi id="S4.E3.m1.1.1.1.1.4.3.2" mathvariant="normal" xref="S4.E3.m1.1.1.1.1.4.3.2.cmml">Δ</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.4.3.3" xref="S4.E3.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1a" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.4.3.4" xref="S4.E3.m1.1.1.1.1.4.3.4.cmml">f</mi></mrow></mrow><mo id="S4.E3.m1.1.1.1.1.5" xref="S4.E3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.6" xref="S4.E3.m1.1.1.1.1.6.cmml"><mrow id="S4.E3.m1.1.1.1.1.6.2" xref="S4.E3.m1.1.1.1.1.6.2.cmml"><msub id="S4.E3.m1.1.1.1.1.6.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.6.2.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.6.2.2.3" xref="S4.E3.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.6.2.1" xref="S4.E3.m1.1.1.1.1.6.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.6.2.3" xref="S4.E3.m1.1.1.1.1.6.2.3.cmml">f</mi></mrow><mo id="S4.E3.m1.1.1.1.1.6.1" xref="S4.E3.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.6.3" xref="S4.E3.m1.1.1.1.1.6.3.cmml"><mi id="S4.E3.m1.1.1.1.1.6.3.2" xref="S4.E3.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.6.3.3" xref="S4.E3.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1a" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.6.3.4" xref="S4.E3.m1.1.1.1.1.6.3.4.cmml">f</mi></mrow></mrow></mrow><mo id="S4.E3.m1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><and id="S4.E3.m1.1.1.1.1a.cmml" xref="S4.E3.m1.1.1.1"></and><apply id="S4.E3.m1.1.1.1.1b.cmml" xref="S4.E3.m1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"></eq><ci id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2">𝑦</ci><apply id="S4.E3.m1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.4"><plus id="S4.E3.m1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.4.1"></plus><apply id="S4.E3.m1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2"><times id="S4.E3.m1.1.1.1.1.4.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.1"></times><apply id="S4.E3.m1.1.1.1.1.4.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.4.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.4.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.2">𝑊</ci><cn id="S4.E3.m1.1.1.1.1.4.2.2.3.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.4.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.3">𝑓</ci></apply><apply id="S4.E3.m1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3"><times id="S4.E3.m1.1.1.1.1.4.3.1.cmml" xref="S4.E3.m1.1.1.1.1.4.3.1"></times><ci id="S4.E3.m1.1.1.1.1.4.3.2.cmml" xref="S4.E3.m1.1.1.1.1.4.3.2">Δ</ci><ci id="S4.E3.m1.1.1.1.1.4.3.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S4.E3.m1.1.1.1.1.4.3.4.cmml" xref="S4.E3.m1.1.1.1.1.4.3.4">𝑓</ci></apply></apply></apply><apply id="S4.E3.m1.1.1.1.1c.cmml" xref="S4.E3.m1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.5.cmml" xref="S4.E3.m1.1.1.1.1.5"></eq><share href="https://arxiv.org/html/2408.00855v3#S4.E3.m1.1.1.1.1.4.cmml" id="S4.E3.m1.1.1.1.1d.cmml" xref="S4.E3.m1.1.1.1"></share><apply id="S4.E3.m1.1.1.1.1.6.cmml" xref="S4.E3.m1.1.1.1.1.6"><plus id="S4.E3.m1.1.1.1.1.6.1.cmml" xref="S4.E3.m1.1.1.1.1.6.1"></plus><apply id="S4.E3.m1.1.1.1.1.6.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2"><times id="S4.E3.m1.1.1.1.1.6.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.1"></times><apply id="S4.E3.m1.1.1.1.1.6.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.6.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.6.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.2">𝑊</ci><cn id="S4.E3.m1.1.1.1.1.6.2.2.3.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.6.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.3">𝑓</ci></apply><apply id="S4.E3.m1.1.1.1.1.6.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3"><times id="S4.E3.m1.1.1.1.1.6.3.1.cmml" xref="S4.E3.m1.1.1.1.1.6.3.1"></times><ci id="S4.E3.m1.1.1.1.1.6.3.2.cmml" xref="S4.E3.m1.1.1.1.1.6.3.2">𝐵</ci><ci id="S4.E3.m1.1.1.1.1.6.3.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3.3">𝐴</ci><ci id="S4.E3.m1.1.1.1.1.6.3.4.cmml" xref="S4.E3.m1.1.1.1.1.6.3.4">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">y=W_{0}f+\Delta Wf=W_{0}f+BAf,</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">italic_y = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_f + roman_Δ italic_W italic_f = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_f + italic_B italic_A italic_f ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.5">where <math alttext="A" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.1.m1.1"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mi id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><ci id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.1.m1.1d">italic_A</annotation></semantics></math>, <math alttext="B" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.2.m2.1"><semantics id="S4.SS1.SSS2.p2.2.m2.1a"><mi id="S4.SS1.SSS2.p2.2.m2.1.1" xref="S4.SS1.SSS2.p2.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.2.m2.1b"><ci id="S4.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.2.m2.1d">italic_B</annotation></semantics></math> are a set of low-rank matrices, and <math alttext="A\in\mathbb{R}^{d\times r}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.3.m3.1"><semantics id="S4.SS1.SSS2.p2.3.m3.1a"><mrow id="S4.SS1.SSS2.p2.3.m3.1.1" xref="S4.SS1.SSS2.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS2.p2.3.m3.1.1.2" xref="S4.SS1.SSS2.p2.3.m3.1.1.2.cmml">A</mi><mo id="S4.SS1.SSS2.p2.3.m3.1.1.1" xref="S4.SS1.SSS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS2.p2.3.m3.1.1.3" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS1.SSS2.p2.3.m3.1.1.3.2" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS2.p2.3.m3.1.1.3.3" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.cmml"><mi id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.2" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.2.cmml">d</mi><mo id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.3" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.3.m3.1b"><apply id="S4.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1"><in id="S4.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.1"></in><ci id="S4.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.2">𝐴</ci><apply id="S4.SS1.SSS2.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3"><times id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.1"></times><ci id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.2">𝑑</ci><ci id="S4.SS1.SSS2.p2.3.m3.1.1.3.3.3.cmml" xref="S4.SS1.SSS2.p2.3.m3.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.3.m3.1c">A\in\mathbb{R}^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.3.m3.1d">italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="B\in\mathbb{R}^{r\times k}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.4.m4.1"><semantics id="S4.SS1.SSS2.p2.4.m4.1a"><mrow id="S4.SS1.SSS2.p2.4.m4.1.1" xref="S4.SS1.SSS2.p2.4.m4.1.1.cmml"><mi id="S4.SS1.SSS2.p2.4.m4.1.1.2" xref="S4.SS1.SSS2.p2.4.m4.1.1.2.cmml">B</mi><mo id="S4.SS1.SSS2.p2.4.m4.1.1.1" xref="S4.SS1.SSS2.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS2.p2.4.m4.1.1.3" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.cmml"><mi id="S4.SS1.SSS2.p2.4.m4.1.1.3.2" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS2.p2.4.m4.1.1.3.3" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.cmml"><mi id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.2" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.2.cmml">r</mi><mo id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.3" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.4.m4.1b"><apply id="S4.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1"><in id="S4.SS1.SSS2.p2.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.1"></in><ci id="S4.SS1.SSS2.p2.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.2">𝐵</ci><apply id="S4.SS1.SSS2.p2.4.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3"><times id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.1"></times><ci id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.2">𝑟</ci><ci id="S4.SS1.SSS2.p2.4.m4.1.1.3.3.3.cmml" xref="S4.SS1.SSS2.p2.4.m4.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.4.m4.1c">B\in\mathbb{R}^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.4.m4.1d">italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> with the rank <math alttext="r\ll\min(d,k)" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.5.m5.3"><semantics id="S4.SS1.SSS2.p2.5.m5.3a"><mrow id="S4.SS1.SSS2.p2.5.m5.3.4" xref="S4.SS1.SSS2.p2.5.m5.3.4.cmml"><mi id="S4.SS1.SSS2.p2.5.m5.3.4.2" xref="S4.SS1.SSS2.p2.5.m5.3.4.2.cmml">r</mi><mo id="S4.SS1.SSS2.p2.5.m5.3.4.1" xref="S4.SS1.SSS2.p2.5.m5.3.4.1.cmml">≪</mo><mrow id="S4.SS1.SSS2.p2.5.m5.3.4.3.2" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml"><mi id="S4.SS1.SSS2.p2.5.m5.1.1" xref="S4.SS1.SSS2.p2.5.m5.1.1.cmml">min</mi><mo id="S4.SS1.SSS2.p2.5.m5.3.4.3.2a" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml">⁡</mo><mrow id="S4.SS1.SSS2.p2.5.m5.3.4.3.2.1" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml"><mo id="S4.SS1.SSS2.p2.5.m5.3.4.3.2.1.1" stretchy="false" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml">(</mo><mi id="S4.SS1.SSS2.p2.5.m5.2.2" xref="S4.SS1.SSS2.p2.5.m5.2.2.cmml">d</mi><mo id="S4.SS1.SSS2.p2.5.m5.3.4.3.2.1.2" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS2.p2.5.m5.3.3" xref="S4.SS1.SSS2.p2.5.m5.3.3.cmml">k</mi><mo id="S4.SS1.SSS2.p2.5.m5.3.4.3.2.1.3" stretchy="false" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.5.m5.3b"><apply id="S4.SS1.SSS2.p2.5.m5.3.4.cmml" xref="S4.SS1.SSS2.p2.5.m5.3.4"><csymbol cd="latexml" id="S4.SS1.SSS2.p2.5.m5.3.4.1.cmml" xref="S4.SS1.SSS2.p2.5.m5.3.4.1">much-less-than</csymbol><ci id="S4.SS1.SSS2.p2.5.m5.3.4.2.cmml" xref="S4.SS1.SSS2.p2.5.m5.3.4.2">𝑟</ci><apply id="S4.SS1.SSS2.p2.5.m5.3.4.3.1.cmml" xref="S4.SS1.SSS2.p2.5.m5.3.4.3.2"><min id="S4.SS1.SSS2.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p2.5.m5.1.1"></min><ci id="S4.SS1.SSS2.p2.5.m5.2.2.cmml" xref="S4.SS1.SSS2.p2.5.m5.2.2">𝑑</ci><ci id="S4.SS1.SSS2.p2.5.m5.3.3.cmml" xref="S4.SS1.SSS2.p2.5.m5.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.5.m5.3c">r\ll\min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.5.m5.3d">italic_r ≪ roman_min ( italic_d , italic_k )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">In the realm of GAN models, StyleGAN <cite class="ltx_cite ltx_citemacro_citep">(Karras et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib26" title="">2020</a>)</cite> stands out. It allows for precise control over specific details of the generated image by manipulating corresponding portions of the input latent vector. Each dimension of the latent vector corresponds to a distinct feature of the generated image, such as facial expression, hairstyle, age, and more. More recently, ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib61" title="">2023a</a>)</cite> has presented a similar concept, extending the ability to fine-tune the SD model. It introduces spatially localized input conditions to the pre-trained text-to-image diffusion model, enabling the generation of various image types with detailed feature control. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F5" title="Figure 5 ‣ 4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">5</span></a> [ControlNet], our approach involves the freezing of all weights within the initial SD model. Initially, we duplicate the Encoder from the U-Net architecture <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib41" title="">2015a</a>)</cite> within the SD model and subsequently freeze it. Each layer’s output from the Encoder is connected to a 1×1 convolution layer initialized with a weight of 0. We then add the output of each layer to the input of the corresponding layer in the Decoder. Throughout the model training process, we only need to manage a single input perturbation while training the 1×1 convolutional layer. This approach fine-tunes the generated image and allows for precise control over the designated features <math alttext="c" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.1.m1.1"><semantics id="S4.SS1.SSS2.p3.1.m1.1a"><mi id="S4.SS1.SSS2.p3.1.m1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.1.m1.1b"><ci id="S4.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.1.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.1.m1.1d">italic_c</annotation></semantics></math>. The objective function can be succinctly expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{Control}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{c},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}(%
\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{c}))\|_{2}^{2}\bigg{]}." class="ltx_math_unparsed" display="block" id="S4.E4.m1.9"><semantics id="S4.E4.m1.9a"><mrow id="S4.E4.m1.9b"><msub id="S4.E4.m1.9.10"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.9.10.2">ℒ</mi><mrow id="S4.E4.m1.9.10.3"><mi id="S4.E4.m1.9.10.3.2">C</mi><mo id="S4.E4.m1.9.10.3.1">⁢</mo><mi id="S4.E4.m1.9.10.3.3">o</mi><mo id="S4.E4.m1.9.10.3.1a">⁢</mo><mi id="S4.E4.m1.9.10.3.4">n</mi><mo id="S4.E4.m1.9.10.3.1b">⁢</mo><mi id="S4.E4.m1.9.10.3.5">t</mi><mo id="S4.E4.m1.9.10.3.1c">⁢</mo><mi id="S4.E4.m1.9.10.3.6">r</mi><mo id="S4.E4.m1.9.10.3.1d">⁢</mo><mi id="S4.E4.m1.9.10.3.7">o</mi><mo id="S4.E4.m1.9.10.3.1e">⁢</mo><mi id="S4.E4.m1.9.10.3.8">l</mi></mrow></msub><mo id="S4.E4.m1.9.11">=</mo><msub id="S4.E4.m1.9.12"><mi id="S4.E4.m1.9.12.2">𝔼</mi><mrow id="S4.E4.m1.7.7.7.7"><mrow id="S4.E4.m1.7.7.7.7.1"><mrow id="S4.E4.m1.7.7.7.7.1.1.1"><mrow id="S4.E4.m1.7.7.7.7.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.7.7.7.7.1.1.1.1.2">ℰ</mi><mo id="S4.E4.m1.7.7.7.7.1.1.1.1.1">⁢</mo><mrow id="S4.E4.m1.7.7.7.7.1.1.1.1.3.2"><mo id="S4.E4.m1.7.7.7.7.1.1.1.1.3.2.1" stretchy="false">(</mo><mi id="S4.E4.m1.1.1.1.1">x</mi><mo id="S4.E4.m1.7.7.7.7.1.1.1.1.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.E4.m1.7.7.7.7.1.1.1.2">,</mo><mi id="S4.E4.m1.4.4.4.4">𝒄</mi><mo id="S4.E4.m1.7.7.7.7.1.1.1.3">,</mo><mi id="S4.E4.m1.5.5.5.5">ϵ</mi></mrow><mo id="S4.E4.m1.7.7.7.7.1.2">∼</mo><mrow id="S4.E4.m1.7.7.7.7.1.3"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.7.7.7.7.1.3.2">𝒩</mi><mo id="S4.E4.m1.7.7.7.7.1.3.1">⁢</mo><mrow id="S4.E4.m1.7.7.7.7.1.3.3.2"><mo id="S4.E4.m1.7.7.7.7.1.3.3.2.1" stretchy="false">(</mo><mn id="S4.E4.m1.2.2.2.2">0</mn><mo id="S4.E4.m1.7.7.7.7.1.3.3.2.2">,</mo><mn id="S4.E4.m1.3.3.3.3">1</mn><mo id="S4.E4.m1.7.7.7.7.1.3.3.2.3" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E4.m1.7.7.7.7.2">,</mo><mi id="S4.E4.m1.6.6.6.6">𝒕</mi></mrow></msub><mrow id="S4.E4.m1.9.13"><mo id="S4.E4.m1.9.13.1" maxsize="210%" minsize="210%">[</mo><mo id="S4.E4.m1.9.13.2" lspace="0em" rspace="0.167em">∥</mo><mi id="S4.E4.m1.9.13.3">ϵ</mi><mo id="S4.E4.m1.9.13.4">−</mo><msub id="S4.E4.m1.9.13.5"><mi id="S4.E4.m1.9.13.5.2">ϵ</mi><mi id="S4.E4.m1.9.13.5.3">θ</mi></msub><mrow id="S4.E4.m1.9.13.6"><mo id="S4.E4.m1.9.13.6.1" stretchy="false">(</mo><msub id="S4.E4.m1.9.13.6.2"><mi id="S4.E4.m1.9.13.6.2.2">𝒛</mi><mi id="S4.E4.m1.9.13.6.2.3">t</mi></msub><mo id="S4.E4.m1.9.13.6.3">,</mo><mi id="S4.E4.m1.8.8">𝒕</mi><mo id="S4.E4.m1.9.13.6.4">,</mo><mi id="S4.E4.m1.9.9">𝒄</mi><mo id="S4.E4.m1.9.13.6.5" stretchy="false">)</mo></mrow><mo id="S4.E4.m1.9.13.7" stretchy="false">)</mo></mrow><msubsup id="S4.E4.m1.9.14"><mo id="S4.E4.m1.9.14.2.2" lspace="0em" rspace="0.167em">∥</mo><mn id="S4.E4.m1.9.14.2.3">2</mn><mn id="S4.E4.m1.9.14.3">2</mn></msubsup><mo id="S4.E4.m1.9.15" maxsize="210%" minsize="210%">]</mo><mo id="S4.E4.m1.9.16" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S4.E4.m1.9c">\mathcal{L}_{Control}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{c},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}(%
\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{c}))\|_{2}^{2}\bigg{]}.</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.9d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_o italic_n italic_t italic_r italic_o italic_l end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , bold_italic_c , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , bold_italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_t , bold_italic_c ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">To summarize, during model training, we utilize the LoRA to govern the overall style and the intended object within the generated image. Meanwhile, ControlNet regulates other detail features of the resulting image. Through training on diverse style datasets, we obtain a range of LoRA and ControlNet models. Subsequently, during model inference, we ensure stable control over production images and generate a diverse set of high-quality inspiration images by combining various LoRA and ControlNet models.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Loss Function</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Therefore, the loss function during the overall model training process can be represented as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{T2IM}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{p},\boldsymbol{c},%
\epsilon\sim\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{%
\theta}(\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{p},\boldsymbol{c}))\|_{2%
}^{2}\bigg{]}." class="ltx_math_unparsed" display="block" id="S4.E5.m1.11"><semantics id="S4.E5.m1.11a"><mrow id="S4.E5.m1.11b"><msub id="S4.E5.m1.11.12"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.11.12.2">ℒ</mi><mrow id="S4.E5.m1.11.12.3"><mi id="S4.E5.m1.11.12.3.2">T</mi><mo id="S4.E5.m1.11.12.3.1">⁢</mo><mn id="S4.E5.m1.11.12.3.3">2</mn><mo id="S4.E5.m1.11.12.3.1a">⁢</mo><mi id="S4.E5.m1.11.12.3.4">I</mi><mo id="S4.E5.m1.11.12.3.1b">⁢</mo><mi id="S4.E5.m1.11.12.3.5">M</mi></mrow></msub><mo id="S4.E5.m1.11.13">=</mo><msub id="S4.E5.m1.11.14"><mi id="S4.E5.m1.11.14.2">𝔼</mi><mrow id="S4.E5.m1.8.8.8.8"><mrow id="S4.E5.m1.8.8.8.8.1"><mrow id="S4.E5.m1.8.8.8.8.1.1.1"><mrow id="S4.E5.m1.8.8.8.8.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.8.8.8.8.1.1.1.1.2">ℰ</mi><mo id="S4.E5.m1.8.8.8.8.1.1.1.1.1">⁢</mo><mrow id="S4.E5.m1.8.8.8.8.1.1.1.1.3.2"><mo id="S4.E5.m1.8.8.8.8.1.1.1.1.3.2.1" stretchy="false">(</mo><mi id="S4.E5.m1.1.1.1.1">x</mi><mo id="S4.E5.m1.8.8.8.8.1.1.1.1.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.E5.m1.8.8.8.8.1.1.1.2">,</mo><mi id="S4.E5.m1.4.4.4.4">𝒑</mi><mo id="S4.E5.m1.8.8.8.8.1.1.1.3">,</mo><mi id="S4.E5.m1.5.5.5.5">𝒄</mi><mo id="S4.E5.m1.8.8.8.8.1.1.1.4">,</mo><mi id="S4.E5.m1.6.6.6.6">ϵ</mi></mrow><mo id="S4.E5.m1.8.8.8.8.1.2">∼</mo><mrow id="S4.E5.m1.8.8.8.8.1.3"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.8.8.8.8.1.3.2">𝒩</mi><mo id="S4.E5.m1.8.8.8.8.1.3.1">⁢</mo><mrow id="S4.E5.m1.8.8.8.8.1.3.3.2"><mo id="S4.E5.m1.8.8.8.8.1.3.3.2.1" stretchy="false">(</mo><mn id="S4.E5.m1.2.2.2.2">0</mn><mo id="S4.E5.m1.8.8.8.8.1.3.3.2.2">,</mo><mn id="S4.E5.m1.3.3.3.3">1</mn><mo id="S4.E5.m1.8.8.8.8.1.3.3.2.3" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E5.m1.8.8.8.8.2">,</mo><mi id="S4.E5.m1.7.7.7.7">𝒕</mi></mrow></msub><mrow id="S4.E5.m1.11.15"><mo id="S4.E5.m1.11.15.1" maxsize="210%" minsize="210%">[</mo><mo id="S4.E5.m1.11.15.2" lspace="0em" rspace="0.167em">∥</mo><mi id="S4.E5.m1.11.15.3">ϵ</mi><mo id="S4.E5.m1.11.15.4">−</mo><msub id="S4.E5.m1.11.15.5"><mi id="S4.E5.m1.11.15.5.2">ϵ</mi><mi id="S4.E5.m1.11.15.5.3">θ</mi></msub><mrow id="S4.E5.m1.11.15.6"><mo id="S4.E5.m1.11.15.6.1" stretchy="false">(</mo><msub id="S4.E5.m1.11.15.6.2"><mi id="S4.E5.m1.11.15.6.2.2">𝒛</mi><mi id="S4.E5.m1.11.15.6.2.3">t</mi></msub><mo id="S4.E5.m1.11.15.6.3">,</mo><mi id="S4.E5.m1.9.9">𝒕</mi><mo id="S4.E5.m1.11.15.6.4">,</mo><mi id="S4.E5.m1.10.10">𝒑</mi><mo id="S4.E5.m1.11.15.6.5">,</mo><mi id="S4.E5.m1.11.11">𝒄</mi><mo id="S4.E5.m1.11.15.6.6" stretchy="false">)</mo></mrow><mo id="S4.E5.m1.11.15.7" stretchy="false">)</mo></mrow><msubsup id="S4.E5.m1.11.16"><mo id="S4.E5.m1.11.16.2.2" lspace="0em" rspace="0.167em">∥</mo><mn id="S4.E5.m1.11.16.2.3">2</mn><mn id="S4.E5.m1.11.16.3">2</mn></msubsup><mo id="S4.E5.m1.11.17" maxsize="210%" minsize="210%">]</mo><mo id="S4.E5.m1.11.18" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S4.E5.m1.11c">\mathcal{L}_{T2IM}=\mathbb{E}_{\mathcal{E}(x),\boldsymbol{p},\boldsymbol{c},%
\epsilon\sim\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{%
\theta}(\boldsymbol{z}_{t},\boldsymbol{t},\boldsymbol{p},\boldsymbol{c}))\|_{2%
}^{2}\bigg{]}.</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.11d">caligraphic_L start_POSTSUBSCRIPT italic_T 2 italic_I italic_M end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , bold_italic_p , bold_italic_c , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , bold_italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_t , bold_italic_p , bold_italic_c ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Capture Personalized Designer-Style Image-to-Sketch Local Module</h3>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.8.4.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S4.F6.6.3" style="font-size:90%;">The illustration of the Capture Personalized Designer-Style Image-to-Sketch Local Module. We begin by inputting the sketch-image pair (<math alttext="RGB" class="ltx_Math" display="inline" id="S4.F6.4.1.m1.1"><semantics id="S4.F6.4.1.m1.1b"><mrow id="S4.F6.4.1.m1.1.1" xref="S4.F6.4.1.m1.1.1.cmml"><mi id="S4.F6.4.1.m1.1.1.2" xref="S4.F6.4.1.m1.1.1.2.cmml">R</mi><mo id="S4.F6.4.1.m1.1.1.1" xref="S4.F6.4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F6.4.1.m1.1.1.3" xref="S4.F6.4.1.m1.1.1.3.cmml">G</mi><mo id="S4.F6.4.1.m1.1.1.1b" xref="S4.F6.4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F6.4.1.m1.1.1.4" xref="S4.F6.4.1.m1.1.1.4.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.4.1.m1.1c"><apply id="S4.F6.4.1.m1.1.1.cmml" xref="S4.F6.4.1.m1.1.1"><times id="S4.F6.4.1.m1.1.1.1.cmml" xref="S4.F6.4.1.m1.1.1.1"></times><ci id="S4.F6.4.1.m1.1.1.2.cmml" xref="S4.F6.4.1.m1.1.1.2">𝑅</ci><ci id="S4.F6.4.1.m1.1.1.3.cmml" xref="S4.F6.4.1.m1.1.1.3">𝐺</ci><ci id="S4.F6.4.1.m1.1.1.4.cmml" xref="S4.F6.4.1.m1.1.1.4">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.1.m1.1d">RGB</annotation><annotation encoding="application/x-llamapun" id="S4.F6.4.1.m1.1e">italic_R italic_G italic_B</annotation></semantics></math> and <math alttext="SKT_{o}" class="ltx_Math" display="inline" id="S4.F6.5.2.m2.1"><semantics id="S4.F6.5.2.m2.1b"><mrow id="S4.F6.5.2.m2.1.1" xref="S4.F6.5.2.m2.1.1.cmml"><mi id="S4.F6.5.2.m2.1.1.2" xref="S4.F6.5.2.m2.1.1.2.cmml">S</mi><mo id="S4.F6.5.2.m2.1.1.1" xref="S4.F6.5.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.F6.5.2.m2.1.1.3" xref="S4.F6.5.2.m2.1.1.3.cmml">K</mi><mo id="S4.F6.5.2.m2.1.1.1b" xref="S4.F6.5.2.m2.1.1.1.cmml">⁢</mo><msub id="S4.F6.5.2.m2.1.1.4" xref="S4.F6.5.2.m2.1.1.4.cmml"><mi id="S4.F6.5.2.m2.1.1.4.2" xref="S4.F6.5.2.m2.1.1.4.2.cmml">T</mi><mi id="S4.F6.5.2.m2.1.1.4.3" xref="S4.F6.5.2.m2.1.1.4.3.cmml">o</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.5.2.m2.1c"><apply id="S4.F6.5.2.m2.1.1.cmml" xref="S4.F6.5.2.m2.1.1"><times id="S4.F6.5.2.m2.1.1.1.cmml" xref="S4.F6.5.2.m2.1.1.1"></times><ci id="S4.F6.5.2.m2.1.1.2.cmml" xref="S4.F6.5.2.m2.1.1.2">𝑆</ci><ci id="S4.F6.5.2.m2.1.1.3.cmml" xref="S4.F6.5.2.m2.1.1.3">𝐾</ci><apply id="S4.F6.5.2.m2.1.1.4.cmml" xref="S4.F6.5.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.F6.5.2.m2.1.1.4.1.cmml" xref="S4.F6.5.2.m2.1.1.4">subscript</csymbol><ci id="S4.F6.5.2.m2.1.1.4.2.cmml" xref="S4.F6.5.2.m2.1.1.4.2">𝑇</ci><ci id="S4.F6.5.2.m2.1.1.4.3.cmml" xref="S4.F6.5.2.m2.1.1.4.3">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.5.2.m2.1d">SKT_{o}</annotation><annotation encoding="application/x-llamapun" id="S4.F6.5.2.m2.1e">italic_S italic_K italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>) into the system, and multi-level features are extracted using the standard pre-trained VGG-16 <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib48" title="">2014</a>)</cite> network. Subsequently, the APSN module and DSMFF module are employed to capture personalized features at multiple levels from the designer’s sketch and perform feature fusion to generate the sketch (<math alttext="SKT_{g}" class="ltx_Math" display="inline" id="S4.F6.6.3.m3.1"><semantics id="S4.F6.6.3.m3.1b"><mrow id="S4.F6.6.3.m3.1.1" xref="S4.F6.6.3.m3.1.1.cmml"><mi id="S4.F6.6.3.m3.1.1.2" xref="S4.F6.6.3.m3.1.1.2.cmml">S</mi><mo id="S4.F6.6.3.m3.1.1.1" xref="S4.F6.6.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.F6.6.3.m3.1.1.3" xref="S4.F6.6.3.m3.1.1.3.cmml">K</mi><mo id="S4.F6.6.3.m3.1.1.1b" xref="S4.F6.6.3.m3.1.1.1.cmml">⁢</mo><msub id="S4.F6.6.3.m3.1.1.4" xref="S4.F6.6.3.m3.1.1.4.cmml"><mi id="S4.F6.6.3.m3.1.1.4.2" xref="S4.F6.6.3.m3.1.1.4.2.cmml">T</mi><mi id="S4.F6.6.3.m3.1.1.4.3" xref="S4.F6.6.3.m3.1.1.4.3.cmml">g</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.6.3.m3.1c"><apply id="S4.F6.6.3.m3.1.1.cmml" xref="S4.F6.6.3.m3.1.1"><times id="S4.F6.6.3.m3.1.1.1.cmml" xref="S4.F6.6.3.m3.1.1.1"></times><ci id="S4.F6.6.3.m3.1.1.2.cmml" xref="S4.F6.6.3.m3.1.1.2">𝑆</ci><ci id="S4.F6.6.3.m3.1.1.3.cmml" xref="S4.F6.6.3.m3.1.1.3">𝐾</ci><apply id="S4.F6.6.3.m3.1.1.4.cmml" xref="S4.F6.6.3.m3.1.1.4"><csymbol cd="ambiguous" id="S4.F6.6.3.m3.1.1.4.1.cmml" xref="S4.F6.6.3.m3.1.1.4">subscript</csymbol><ci id="S4.F6.6.3.m3.1.1.4.2.cmml" xref="S4.F6.6.3.m3.1.1.4.2">𝑇</ci><ci id="S4.F6.6.3.m3.1.1.4.3.cmml" xref="S4.F6.6.3.m3.1.1.4.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.6.3.m3.1d">SKT_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.F6.6.3.m3.1e">italic_S italic_K italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>) with the designer’s style.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In previous works <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib27" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>)</cite>, the task of image-to-sketch generation can be summarized as extracting contour and detail features from the input image. However, our proposed sketch generation model aims to assist designers in quickly completing hand-drawn sketch works, ensuring that the generated sketches possess the unique artistic style of the designer. This is because each designer has their own unique artistic style and conveys their perception of important elements in the sketches from various perspectives through their personal drawing techniques. To illustrate this, we present the framework of our <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Image-to-Sketch Local Module</span> (I2SM) in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F6" title="Figure 6 ‣ 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">6</span></a>, which primarily consists of three components. These components include the standard pre-trained VGG-16 <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib48" title="">2014</a>)</cite> for extracting multi-level features, the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Adaptive Personalized Style Normalization</span> (APSN) Module for capturing and blending different designer styles, and the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">DownSampling Multi-Feature Fusion</span> (DSMFF) Module in Decoder for multi-level feature reconstruction.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>VGG Encoder</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.5">The VGG network possesses the advantage of providing rich multi-level feature representation and enhanced feature expression capabilities. It excels in capturing low-level image features, such as edges and textures, in shallower layers, while deeper layers are capable of capturing higher-level semantic features, such as overall shape and structure. We leverage the multi-level feature extraction capability of the VGG-16 to acquire the first four levels of features as input for subsequent sketch reconstruction, namely <math alttext="f_{e1}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><msub id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">f</mi><mrow id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">e</mi><mo id="S4.SS2.SSS1.p1.1.m1.1.1.3.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">𝑓</ci><apply id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3"><times id="S4.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2">𝑒</ci><cn id="S4.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">f_{e1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_e 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="f_{e2}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.2.m2.1"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><msub id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.1.1.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml">f</mi><mrow id="S4.SS2.SSS1.p1.2.m2.1.1.3" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">e</mi><mo id="S4.SS2.SSS1.p1.2.m2.1.1.3.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mn id="S4.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><apply id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.2">𝑓</ci><apply id="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3"><times id="S4.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S4.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.2">𝑒</ci><cn id="S4.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">f_{e2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_e 2 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="f_{e3}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.3.m3.1"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><msub id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.1.1.2" xref="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml">f</mi><mrow id="S4.SS2.SSS1.p1.3.m3.1.1.3" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.2.cmml">e</mi><mo id="S4.SS2.SSS1.p1.3.m3.1.1.3.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mn id="S4.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><apply id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.2">𝑓</ci><apply id="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.3"><times id="S4.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.1"></times><ci id="S4.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.2">𝑒</ci><cn id="S4.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">f_{e3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_e 3 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{e4}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.4.m4.1"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><msub id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml">f</mi><mrow id="S4.SS2.SSS1.p1.4.m4.1.1.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">e</mi><mo id="S4.SS2.SSS1.p1.4.m4.1.1.3.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">⁢</mo><mn id="S4.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">4</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><apply id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2">𝑓</ci><apply id="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3"><times id="S4.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.1"></times><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.2">𝑒</ci><cn id="S4.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">f_{e4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.4.m4.1d">italic_f start_POSTSUBSCRIPT italic_e 4 end_POSTSUBSCRIPT</annotation></semantics></math>. The final output linear layer features <math alttext="f_{e5}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.5.m5.1"><semantics id="S4.SS2.SSS1.p1.5.m5.1a"><msub id="S4.SS2.SSS1.p1.5.m5.1.1" xref="S4.SS2.SSS1.p1.5.m5.1.1.cmml"><mi id="S4.SS2.SSS1.p1.5.m5.1.1.2" xref="S4.SS2.SSS1.p1.5.m5.1.1.2.cmml">f</mi><mrow id="S4.SS2.SSS1.p1.5.m5.1.1.3" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.5.m5.1.1.3.2" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.2.cmml">e</mi><mo id="S4.SS2.SSS1.p1.5.m5.1.1.3.1" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.1.cmml">⁢</mo><mn id="S4.SS2.SSS1.p1.5.m5.1.1.3.3" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.3.cmml">5</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.5.m5.1b"><apply id="S4.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1.2">𝑓</ci><apply id="S4.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1.3"><times id="S4.SS2.SSS1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.1"></times><ci id="S4.SS2.SSS1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.2">𝑒</ci><cn id="S4.SS2.SSS1.p1.5.m5.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.5.m5.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.5.m5.1c">f_{e5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_e 5 end_POSTSUBSCRIPT</annotation></semantics></math> are utilized to preserve the semantic consistency of the generated output. It should be noted that we select features before the max-pooling layer within the selected first four levels, as the max-pooling operation may lead to the loss of certain local detail features. This ensures that most designer design needs are met as much as possible.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Adaptive Personalized Style Normalization Module</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">The fundamental concept behind the traditional Adaptive Instance Normalization (AdaIN) module is to dynamically adjust the normalization parameters, enabling the model to adapt to distribution variances among different samples and layers. This adaptation enhances the model’s generalization capability and adaptability. Building upon this idea, we introduce the <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">Adaptive Personalized Style Normalization</span> (APSN) module. It normalizes both the input image and sketch features, followed by mapping the image features utilized for sketch generation to the input sketch feature space. We believe that each designer possesses their own distinctive design style, and as a result, hand-drawn sketches created by the same designer should exhibit similar feature means and variances. This is attributed to the fact that the mean and variance of an image can effectively represent its fundamental characteristics, such as brightness and contrast. Similarly, the mean and variance of sketch features extracted by the VGG encoder can aptly describe the similarity of features, particularly in terms of style-related features.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.2">To be more specific, our objective is to enhance the model’s capacity to capture the designer’s unique design style by aligning generated sketch features with the characteristics of hand-drawn sketch features. Thus, before model training, we compute the mean <math alttext="\mu(f_{e})" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.1.m1.1"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mrow id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.1.m1.1.1.3" xref="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml">μ</mi><mo id="S4.SS2.SSS2.p2.1.m1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.3" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml">e</mi></msub><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><apply id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1"><times id="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.2"></times><ci id="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.3">𝜇</ci><apply id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.2">𝑓</ci><ci id="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.1.1.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">\mu(f_{e})</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.1.m1.1d">italic_μ ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT )</annotation></semantics></math> and variance <math alttext="\sigma(f_{e})" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.2.m2.1"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mrow id="S4.SS2.SSS2.p2.2.m2.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p2.2.m2.1.1.3" xref="S4.SS2.SSS2.p2.2.m2.1.1.3.cmml">σ</mi><mo id="S4.SS2.SSS2.p2.2.m2.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.SSS2.p2.2.m2.1.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml"><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.3" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.3.cmml">e</mi></msub><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><apply id="S4.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1"><times id="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.2"></times><ci id="S4.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.3">𝜎</ci><apply id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.2">𝑓</ci><ci id="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.1.1.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">\sigma(f_{e})</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.2.m2.1d">italic_σ ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT )</annotation></semantics></math> across all sketch samples from a specific designer. We calculate these statistics for the 0th, 2nd, and 3rd dimensions of the data, corresponding to batch size, height, and width, respectively. During training, for each minibatch of input image samples, we extract their multi-level features using the VGG encoder. These obtained image features are then normalized using the APSN module. Subsequently, the normalized features are fed into the Decoder network, which is responsible for sketch generation, and the normalized features are as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}APSN(x,\mu_{s},\sigma_{s})=\sigma_{s}(\frac{x-\mu(x)}{\sigma(x)+%
eps})+\mu_{s},\quad f^{i}_{e_{n}}=APSN(f^{i}_{e_{n}},\mu(f^{s}_{e_{n}}),\sigma%
(f^{s}_{e_{n}})),\end{split}" class="ltx_math_unparsed" display="block" id="S4.E6.m1.52"><semantics id="S4.E6.m1.52a"><mtable displaystyle="true" id="S4.E6.m1.52.52.1"><mtr id="S4.E6.m1.52.52.1a"><mtd class="ltx_align_right" columnalign="right" id="S4.E6.m1.52.52.1b"><mrow id="S4.E6.m1.52.52.1.52.52.52.52"><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1"><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1"><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2"><mi id="S4.E6.m1.1.1.1.1.1.1">A</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.3">⁢</mo><mi id="S4.E6.m1.2.2.2.2.2.2">P</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.3a">⁢</mo><mi id="S4.E6.m1.3.3.3.3.3.3">S</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.3b">⁢</mo><mi id="S4.E6.m1.4.4.4.4.4.4">N</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.3c">⁢</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.2.2"><mo id="S4.E6.m1.5.5.5.5.5.5" stretchy="false">(</mo><mi id="S4.E6.m1.6.6.6.6.6.6">x</mi><mo id="S4.E6.m1.7.7.7.7.7.7">,</mo><msub id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.1.1.1.1"><mi id="S4.E6.m1.8.8.8.8.8.8">μ</mi><mi id="S4.E6.m1.9.9.9.9.9.9.1">s</mi></msub><mo id="S4.E6.m1.10.10.10.10.10.10">,</mo><msub id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.2.2.2.2"><mi id="S4.E6.m1.11.11.11.11.11.11">σ</mi><mi id="S4.E6.m1.12.12.12.12.12.12.1">s</mi></msub><mo id="S4.E6.m1.13.13.13.13.13.13" stretchy="false">)</mo></mrow></mrow><mo id="S4.E6.m1.14.14.14.14.14.14">=</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3"><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3.1"><msub id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3.1.2"><mi id="S4.E6.m1.15.15.15.15.15.15">σ</mi><mi id="S4.E6.m1.16.16.16.16.16.16.1">s</mi></msub><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3.1.1">⁢</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3.1.3"><mo id="S4.E6.m1.17.17.17.17.17.17" stretchy="false">(</mo><mfrac id="S4.E6.m1.18.18.18.18.18.18"><mrow id="S4.E6.m1.18.18.18.18.18.18.1"><mi id="S4.E6.m1.18.18.18.18.18.18.1.3">x</mi><mo id="S4.E6.m1.18.18.18.18.18.18.1.2">−</mo><mrow id="S4.E6.m1.18.18.18.18.18.18.1.4"><mi id="S4.E6.m1.18.18.18.18.18.18.1.4.2">μ</mi><mo id="S4.E6.m1.18.18.18.18.18.18.1.4.1">⁢</mo><mrow id="S4.E6.m1.18.18.18.18.18.18.1.4.3.2"><mo id="S4.E6.m1.18.18.18.18.18.18.1.4.3.2.1" stretchy="false">(</mo><mi id="S4.E6.m1.18.18.18.18.18.18.1.1">x</mi><mo id="S4.E6.m1.18.18.18.18.18.18.1.4.3.2.2" stretchy="false">)</mo></mrow></mrow></mrow><mrow id="S4.E6.m1.18.18.18.18.18.18.2"><mrow id="S4.E6.m1.18.18.18.18.18.18.2.3"><mi id="S4.E6.m1.18.18.18.18.18.18.2.3.2">σ</mi><mo id="S4.E6.m1.18.18.18.18.18.18.2.3.1">⁢</mo><mrow id="S4.E6.m1.18.18.18.18.18.18.2.3.3.2"><mo id="S4.E6.m1.18.18.18.18.18.18.2.3.3.2.1" stretchy="false">(</mo><mi id="S4.E6.m1.18.18.18.18.18.18.2.1">x</mi><mo id="S4.E6.m1.18.18.18.18.18.18.2.3.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.E6.m1.18.18.18.18.18.18.2.2">+</mo><mrow id="S4.E6.m1.18.18.18.18.18.18.2.4"><mi id="S4.E6.m1.18.18.18.18.18.18.2.4.2">e</mi><mo id="S4.E6.m1.18.18.18.18.18.18.2.4.1">⁢</mo><mi id="S4.E6.m1.18.18.18.18.18.18.2.4.3">p</mi><mo id="S4.E6.m1.18.18.18.18.18.18.2.4.1a">⁢</mo><mi id="S4.E6.m1.18.18.18.18.18.18.2.4.4">s</mi></mrow></mrow></mfrac><mo id="S4.E6.m1.19.19.19.19.19.19" stretchy="false">)</mo></mrow></mrow><mo id="S4.E6.m1.20.20.20.20.20.20">+</mo><msub id="S4.E6.m1.52.52.1.52.52.52.52.1.1.1.3.2"><mi id="S4.E6.m1.21.21.21.21.21.21">μ</mi><mi id="S4.E6.m1.22.22.22.22.22.22.1">s</mi></msub></mrow></mrow><mo id="S4.E6.m1.23.23.23.23.23.23" rspace="1.167em">,</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2"><msubsup id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.4"><mi id="S4.E6.m1.24.24.24.24.24.24">f</mi><msub id="S4.E6.m1.26.26.26.26.26.26.1"><mi id="S4.E6.m1.26.26.26.26.26.26.1.2">e</mi><mi id="S4.E6.m1.26.26.26.26.26.26.1.3">n</mi></msub><mi id="S4.E6.m1.25.25.25.25.25.25.1">i</mi></msubsup><mo id="S4.E6.m1.27.27.27.27.27.27">=</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3"><mi id="S4.E6.m1.28.28.28.28.28.28">A</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.4">⁢</mo><mi id="S4.E6.m1.29.29.29.29.29.29">P</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.4a">⁢</mo><mi id="S4.E6.m1.30.30.30.30.30.30">S</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.4b">⁢</mo><mi id="S4.E6.m1.31.31.31.31.31.31">N</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.4c">⁢</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.3.3"><mo id="S4.E6.m1.32.32.32.32.32.32" stretchy="false">(</mo><msubsup id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.1.1.1.1"><mi id="S4.E6.m1.33.33.33.33.33.33">f</mi><msub id="S4.E6.m1.35.35.35.35.35.35.1"><mi id="S4.E6.m1.35.35.35.35.35.35.1.2">e</mi><mi id="S4.E6.m1.35.35.35.35.35.35.1.3">n</mi></msub><mi id="S4.E6.m1.34.34.34.34.34.34.1">i</mi></msubsup><mo id="S4.E6.m1.36.36.36.36.36.36">,</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.2.2.2.2"><mi id="S4.E6.m1.37.37.37.37.37.37">μ</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.2.2.2.2.2">⁢</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.2.2.2.2.1.1"><mo id="S4.E6.m1.38.38.38.38.38.38" stretchy="false">(</mo><msubsup id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.2.2.2.2.1.1.1"><mi id="S4.E6.m1.39.39.39.39.39.39">f</mi><msub id="S4.E6.m1.41.41.41.41.41.41.1"><mi id="S4.E6.m1.41.41.41.41.41.41.1.2">e</mi><mi id="S4.E6.m1.41.41.41.41.41.41.1.3">n</mi></msub><mi id="S4.E6.m1.40.40.40.40.40.40.1">s</mi></msubsup><mo id="S4.E6.m1.42.42.42.42.42.42" stretchy="false">)</mo></mrow></mrow><mo id="S4.E6.m1.43.43.43.43.43.43">,</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.3.3.3"><mi id="S4.E6.m1.44.44.44.44.44.44">σ</mi><mo id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.3.3.3.2">⁢</mo><mrow id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.3.3.3.1.1"><mo id="S4.E6.m1.45.45.45.45.45.45" stretchy="false">(</mo><msubsup id="S4.E6.m1.52.52.1.52.52.52.52.1.2.2.3.3.3.3.1.1.1"><mi id="S4.E6.m1.46.46.46.46.46.46">f</mi><msub id="S4.E6.m1.48.48.48.48.48.48.1"><mi id="S4.E6.m1.48.48.48.48.48.48.1.2">e</mi><mi id="S4.E6.m1.48.48.48.48.48.48.1.3">n</mi></msub><mi id="S4.E6.m1.47.47.47.47.47.47.1">s</mi></msubsup><mo id="S4.E6.m1.49.49.49.49.49.49" stretchy="false">)</mo></mrow></mrow><mo id="S4.E6.m1.50.50.50.50.50.50" stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E6.m1.51.51.51.51.51.51">,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S4.E6.m1.52b">\begin{split}APSN(x,\mu_{s},\sigma_{s})=\sigma_{s}(\frac{x-\mu(x)}{\sigma(x)+%
eps})+\mu_{s},\quad f^{i}_{e_{n}}=APSN(f^{i}_{e_{n}},\mu(f^{s}_{e_{n}}),\sigma%
(f^{s}_{e_{n}})),\end{split}</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.52c">start_ROW start_CELL italic_A italic_P italic_S italic_N ( italic_x , italic_μ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = italic_σ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( divide start_ARG italic_x - italic_μ ( italic_x ) end_ARG start_ARG italic_σ ( italic_x ) + italic_e italic_p italic_s end_ARG ) + italic_μ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_f start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_A italic_P italic_S italic_N ( italic_f start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_μ ( italic_f start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , italic_σ ( italic_f start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ) , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.7">where <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.1.m1.1"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><mi id="S4.SS2.SSS2.p3.1.m1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.1b"><ci id="S4.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.1.m1.1d">italic_n</annotation></semantics></math> = 1,2,3,4, <math alttext="eps={10}^{-6}" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.2.m2.1"><semantics id="S4.SS2.SSS2.p3.2.m2.1a"><mrow id="S4.SS2.SSS2.p3.2.m2.1.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.cmml"><mrow id="S4.SS2.SSS2.p3.2.m2.1.1.2" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.cmml"><mi id="S4.SS2.SSS2.p3.2.m2.1.1.2.2" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.2.cmml">e</mi><mo id="S4.SS2.SSS2.p3.2.m2.1.1.2.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p3.2.m2.1.1.2.3" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.3.cmml">p</mi><mo id="S4.SS2.SSS2.p3.2.m2.1.1.2.1a" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p3.2.m2.1.1.2.4" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.4.cmml">s</mi></mrow><mo id="S4.SS2.SSS2.p3.2.m2.1.1.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.1.cmml">=</mo><msup id="S4.SS2.SSS2.p3.2.m2.1.1.3" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.cmml"><mn id="S4.SS2.SSS2.p3.2.m2.1.1.3.2" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS2.p3.2.m2.1.1.3.3" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.SSS2.p3.2.m2.1.1.3.3a" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS2.p3.2.m2.1.1.3.3.2" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.2.m2.1b"><apply id="S4.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1"><eq id="S4.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.1"></eq><apply id="S4.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2"><times id="S4.SS2.SSS2.p3.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.1"></times><ci id="S4.SS2.SSS2.p3.2.m2.1.1.2.2.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.2">𝑒</ci><ci id="S4.SS2.SSS2.p3.2.m2.1.1.2.3.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.3">𝑝</ci><ci id="S4.SS2.SSS2.p3.2.m2.1.1.2.4.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.2.4">𝑠</ci></apply><apply id="S4.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.3">superscript</csymbol><cn id="S4.SS2.SSS2.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.2">10</cn><apply id="S4.SS2.SSS2.p3.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3"><minus id="S4.SS2.SSS2.p3.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3"></minus><cn id="S4.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS2.SSS2.p3.2.m2.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.2.m2.1c">eps={10}^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.2.m2.1d">italic_e italic_p italic_s = 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math> for numerical stability to prevent <math alttext="\sigma(x)=0" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.3.m3.1"><semantics id="S4.SS2.SSS2.p3.3.m3.1a"><mrow id="S4.SS2.SSS2.p3.3.m3.1.2" xref="S4.SS2.SSS2.p3.3.m3.1.2.cmml"><mrow id="S4.SS2.SSS2.p3.3.m3.1.2.2" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.cmml"><mi id="S4.SS2.SSS2.p3.3.m3.1.2.2.2" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.2.cmml">σ</mi><mo id="S4.SS2.SSS2.p3.3.m3.1.2.2.1" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.1.cmml">⁢</mo><mrow id="S4.SS2.SSS2.p3.3.m3.1.2.2.3.2" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.cmml"><mo id="S4.SS2.SSS2.p3.3.m3.1.2.2.3.2.1" stretchy="false" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.cmml">(</mo><mi id="S4.SS2.SSS2.p3.3.m3.1.1" xref="S4.SS2.SSS2.p3.3.m3.1.1.cmml">x</mi><mo id="S4.SS2.SSS2.p3.3.m3.1.2.2.3.2.2" stretchy="false" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS2.p3.3.m3.1.2.1" xref="S4.SS2.SSS2.p3.3.m3.1.2.1.cmml">=</mo><mn id="S4.SS2.SSS2.p3.3.m3.1.2.3" xref="S4.SS2.SSS2.p3.3.m3.1.2.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.3.m3.1b"><apply id="S4.SS2.SSS2.p3.3.m3.1.2.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.2"><eq id="S4.SS2.SSS2.p3.3.m3.1.2.1.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.2.1"></eq><apply id="S4.SS2.SSS2.p3.3.m3.1.2.2.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.2.2"><times id="S4.SS2.SSS2.p3.3.m3.1.2.2.1.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.1"></times><ci id="S4.SS2.SSS2.p3.3.m3.1.2.2.2.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.2.2.2">𝜎</ci><ci id="S4.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.1">𝑥</ci></apply><cn id="S4.SS2.SSS2.p3.3.m3.1.2.3.cmml" type="integer" xref="S4.SS2.SSS2.p3.3.m3.1.2.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.3.m3.1c">\sigma(x)=0</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.3.m3.1d">italic_σ ( italic_x ) = 0</annotation></semantics></math>, <math alttext="f^{i}_{e_{n}}" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.4.m4.1"><semantics id="S4.SS2.SSS2.p3.4.m4.1a"><msubsup id="S4.SS2.SSS2.p3.4.m4.1.1" xref="S4.SS2.SSS2.p3.4.m4.1.1.cmml"><mi id="S4.SS2.SSS2.p3.4.m4.1.1.2.2" xref="S4.SS2.SSS2.p3.4.m4.1.1.2.2.cmml">f</mi><msub id="S4.SS2.SSS2.p3.4.m4.1.1.3" xref="S4.SS2.SSS2.p3.4.m4.1.1.3.cmml"><mi id="S4.SS2.SSS2.p3.4.m4.1.1.3.2" xref="S4.SS2.SSS2.p3.4.m4.1.1.3.2.cmml">e</mi><mi id="S4.SS2.SSS2.p3.4.m4.1.1.3.3" xref="S4.SS2.SSS2.p3.4.m4.1.1.3.3.cmml">n</mi></msub><mi id="S4.SS2.SSS2.p3.4.m4.1.1.2.3" xref="S4.SS2.SSS2.p3.4.m4.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.4.m4.1b"><apply id="S4.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1">subscript</csymbol><apply id="S4.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.4.m4.1.1.2.1.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p3.4.m4.1.1.2.2.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.2.2">𝑓</ci><ci id="S4.SS2.SSS2.p3.4.m4.1.1.2.3.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.SSS2.p3.4.m4.1.1.3.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS2.p3.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.3.2">𝑒</ci><ci id="S4.SS2.SSS2.p3.4.m4.1.1.3.3.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.4.m4.1c">f^{i}_{e_{n}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.4.m4.1d">italic_f start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> represents the multi-level features of the input image samples, <math alttext="f^{s}_{e_{n}}" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.5.m5.1"><semantics id="S4.SS2.SSS2.p3.5.m5.1a"><msubsup id="S4.SS2.SSS2.p3.5.m5.1.1" xref="S4.SS2.SSS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.SSS2.p3.5.m5.1.1.2.2" xref="S4.SS2.SSS2.p3.5.m5.1.1.2.2.cmml">f</mi><msub id="S4.SS2.SSS2.p3.5.m5.1.1.3" xref="S4.SS2.SSS2.p3.5.m5.1.1.3.cmml"><mi id="S4.SS2.SSS2.p3.5.m5.1.1.3.2" xref="S4.SS2.SSS2.p3.5.m5.1.1.3.2.cmml">e</mi><mi id="S4.SS2.SSS2.p3.5.m5.1.1.3.3" xref="S4.SS2.SSS2.p3.5.m5.1.1.3.3.cmml">n</mi></msub><mi id="S4.SS2.SSS2.p3.5.m5.1.1.2.3" xref="S4.SS2.SSS2.p3.5.m5.1.1.2.3.cmml">s</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.5.m5.1b"><apply id="S4.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1">subscript</csymbol><apply id="S4.SS2.SSS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.5.m5.1.1.2.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p3.5.m5.1.1.2.2.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.2.2">𝑓</ci><ci id="S4.SS2.SSS2.p3.5.m5.1.1.2.3.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.2.3">𝑠</ci></apply><apply id="S4.SS2.SSS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.5.m5.1.1.3.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS2.p3.5.m5.1.1.3.2.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3.2">𝑒</ci><ci id="S4.SS2.SSS2.p3.5.m5.1.1.3.3.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.5.m5.1c">f^{s}_{e_{n}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.5.m5.1d">italic_f start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> represents the multi-level features of all sketch samples. It is worth noting that the normalization operation (denoted as ”<math alttext="\mu(x)" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.6.m6.1"><semantics id="S4.SS2.SSS2.p3.6.m6.1a"><mrow id="S4.SS2.SSS2.p3.6.m6.1.2" xref="S4.SS2.SSS2.p3.6.m6.1.2.cmml"><mi id="S4.SS2.SSS2.p3.6.m6.1.2.2" xref="S4.SS2.SSS2.p3.6.m6.1.2.2.cmml">μ</mi><mo id="S4.SS2.SSS2.p3.6.m6.1.2.1" xref="S4.SS2.SSS2.p3.6.m6.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.SSS2.p3.6.m6.1.2.3.2" xref="S4.SS2.SSS2.p3.6.m6.1.2.cmml"><mo id="S4.SS2.SSS2.p3.6.m6.1.2.3.2.1" stretchy="false" xref="S4.SS2.SSS2.p3.6.m6.1.2.cmml">(</mo><mi id="S4.SS2.SSS2.p3.6.m6.1.1" xref="S4.SS2.SSS2.p3.6.m6.1.1.cmml">x</mi><mo id="S4.SS2.SSS2.p3.6.m6.1.2.3.2.2" stretchy="false" xref="S4.SS2.SSS2.p3.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.6.m6.1b"><apply id="S4.SS2.SSS2.p3.6.m6.1.2.cmml" xref="S4.SS2.SSS2.p3.6.m6.1.2"><times id="S4.SS2.SSS2.p3.6.m6.1.2.1.cmml" xref="S4.SS2.SSS2.p3.6.m6.1.2.1"></times><ci id="S4.SS2.SSS2.p3.6.m6.1.2.2.cmml" xref="S4.SS2.SSS2.p3.6.m6.1.2.2">𝜇</ci><ci id="S4.SS2.SSS2.p3.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p3.6.m6.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.6.m6.1c">\mu(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.6.m6.1d">italic_μ ( italic_x )</annotation></semantics></math>” and ”<math alttext="\sigma(x)" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p3.7.m7.1"><semantics id="S4.SS2.SSS2.p3.7.m7.1a"><mrow id="S4.SS2.SSS2.p3.7.m7.1.2" xref="S4.SS2.SSS2.p3.7.m7.1.2.cmml"><mi id="S4.SS2.SSS2.p3.7.m7.1.2.2" xref="S4.SS2.SSS2.p3.7.m7.1.2.2.cmml">σ</mi><mo id="S4.SS2.SSS2.p3.7.m7.1.2.1" xref="S4.SS2.SSS2.p3.7.m7.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.SSS2.p3.7.m7.1.2.3.2" xref="S4.SS2.SSS2.p3.7.m7.1.2.cmml"><mo id="S4.SS2.SSS2.p3.7.m7.1.2.3.2.1" stretchy="false" xref="S4.SS2.SSS2.p3.7.m7.1.2.cmml">(</mo><mi id="S4.SS2.SSS2.p3.7.m7.1.1" xref="S4.SS2.SSS2.p3.7.m7.1.1.cmml">x</mi><mo id="S4.SS2.SSS2.p3.7.m7.1.2.3.2.2" stretchy="false" xref="S4.SS2.SSS2.p3.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.7.m7.1b"><apply id="S4.SS2.SSS2.p3.7.m7.1.2.cmml" xref="S4.SS2.SSS2.p3.7.m7.1.2"><times id="S4.SS2.SSS2.p3.7.m7.1.2.1.cmml" xref="S4.SS2.SSS2.p3.7.m7.1.2.1"></times><ci id="S4.SS2.SSS2.p3.7.m7.1.2.2.cmml" xref="S4.SS2.SSS2.p3.7.m7.1.2.2">𝜎</ci><ci id="S4.SS2.SSS2.p3.7.m7.1.1.cmml" xref="S4.SS2.SSS2.p3.7.m7.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.7.m7.1c">\sigma(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.7.m7.1d">italic_σ ( italic_x )</annotation></semantics></math>” here) is specifically applied to the 2nd and 3rd dimensions of the input feature, which correspond to the height and width dimensions. The purpose of this normalization is to normalize each individual image feature within each minibatch, rather than calculating the mean and variance of all samples collectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>DownSampling Multi-Feature Fusion Module</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">In the decoder part, we aim to utilize the previously acquired normalized multi-level features for feature decoupling, ensuring that fine-grained information is retained as much as possible. This approach facilitates the generation of high-quality sketches in the designer’s style. To achieve this, we introduce the <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">DownSample Multi-Feature Fusion</span> (DSMFF) module, illustrated as part (iii) and (iv) in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F6" title="Figure 6 ‣ 4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.18">For the input features <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.1.m1.1"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><msub id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.1.m1.1.1.3" xref="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><apply id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{d}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.2.m2.1"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><msub id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p2.2.m2.1.1.2" xref="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.2.m2.1.1.3" xref="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><apply id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">f_{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, their respective dimensions are determined as <math alttext="f_{e}\in\mathbb{R}^{B\times C\times W\times W}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.3.m3.1"><semantics id="S4.SS2.SSS3.p2.3.m3.1a"><mrow id="S4.SS2.SSS3.p2.3.m3.1.1" xref="S4.SS2.SSS3.p2.3.m3.1.1.cmml"><msub id="S4.SS2.SSS3.p2.3.m3.1.1.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.cmml"><mi id="S4.SS2.SSS3.p2.3.m3.1.1.2.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.3.m3.1.1.2.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.3.cmml">e</mi></msub><mo id="S4.SS2.SSS3.p2.3.m3.1.1.1" xref="S4.SS2.SSS3.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S4.SS2.SSS3.p2.3.m3.1.1.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.SSS3.p2.3.m3.1.1.3.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.cmml"><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.2.cmml">B</mi><mo id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.3.cmml">C</mi><mo id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.4" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.4.cmml">W</mi><mo id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.5" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.5.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.3.m3.1b"><apply id="S4.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1"><in id="S4.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.1"></in><apply id="S4.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS3.p2.3.m3.1.1.2.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.3.m3.1.1.2.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.3">𝑒</ci></apply><apply id="S4.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3"><times id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.1"></times><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.2">𝐵</ci><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.3">𝐶</ci><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.4.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.4">𝑊</ci><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.5.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.5">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.3.m3.1c">f_{e}\in\mathbb{R}^{B\times C\times W\times W}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_B × italic_C × italic_W × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="f_{d}\in\mathbb{R}^{B\times C\times W/2\times W/2}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.4.m4.1"><semantics id="S4.SS2.SSS3.p2.4.m4.1a"><mrow id="S4.SS2.SSS3.p2.4.m4.1.1" xref="S4.SS2.SSS3.p2.4.m4.1.1.cmml"><msub id="S4.SS2.SSS3.p2.4.m4.1.1.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.2.cmml"><mi id="S4.SS2.SSS3.p2.4.m4.1.1.2.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.2.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.4.m4.1.1.2.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.2.3.cmml">d</mi></msub><mo id="S4.SS2.SSS3.p2.4.m4.1.1.1" xref="S4.SS2.SSS3.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S4.SS2.SSS3.p2.4.m4.1.1.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.cmml"><mi id="S4.SS2.SSS3.p2.4.m4.1.1.3.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.SSS3.p2.4.m4.1.1.3.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.cmml"><mrow id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.cmml"><mrow id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.cmml"><mrow id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.2" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.2.cmml">B</mi><mo id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.3.cmml">C</mi><mo id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.4" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.4.cmml">W</mi></mrow><mo id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.1" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.1.cmml">/</mo><mn id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.3.cmml">2</mn></mrow><mo id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.1.cmml">×</mo><mi id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.3.cmml">W</mi></mrow><mo id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.1" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.1.cmml">/</mo><mn id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.3" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.4.m4.1b"><apply id="S4.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1"><in id="S4.SS2.SSS3.p2.4.m4.1.1.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.1"></in><apply id="S4.SS2.SSS3.p2.4.m4.1.1.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.4.m4.1.1.2.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS3.p2.4.m4.1.1.2.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.2.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.4.m4.1.1.2.3.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.2.3">𝑑</ci></apply><apply id="S4.SS2.SSS3.p2.4.m4.1.1.3.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS3.p2.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3"><divide id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.1"></divide><apply id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2"><times id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.1"></times><apply id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2"><divide id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.1"></divide><apply id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2"><times id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.1"></times><ci id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.2">𝐵</ci><ci id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.3.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.3">𝐶</ci><ci id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.4.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.2.4">𝑊</ci></apply><cn id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.2.3">2</cn></apply><ci id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.3.cmml" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.2.3">𝑊</ci></apply><cn id="S4.SS2.SSS3.p2.4.m4.1.1.3.3.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.4.m4.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.4.m4.1c">f_{d}\in\mathbb{R}^{B\times C\times W/2\times W/2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.4.m4.1d">italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_B × italic_C × italic_W / 2 × italic_W / 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. Here, <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.5.m5.1"><semantics id="S4.SS2.SSS3.p2.5.m5.1a"><msub id="S4.SS2.SSS3.p2.5.m5.1.1" xref="S4.SS2.SSS3.p2.5.m5.1.1.cmml"><mi id="S4.SS2.SSS3.p2.5.m5.1.1.2" xref="S4.SS2.SSS3.p2.5.m5.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.5.m5.1.1.3" xref="S4.SS2.SSS3.p2.5.m5.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.5.m5.1b"><apply id="S4.SS2.SSS3.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.5.m5.1.1.1.cmml" xref="S4.SS2.SSS3.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.5.m5.1.1.2.cmml" xref="S4.SS2.SSS3.p2.5.m5.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.5.m5.1.1.3.cmml" xref="S4.SS2.SSS3.p2.5.m5.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.5.m5.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> represents the feature normalized by the APSN module, <math alttext="f_{d}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.6.m6.1"><semantics id="S4.SS2.SSS3.p2.6.m6.1a"><msub id="S4.SS2.SSS3.p2.6.m6.1.1" xref="S4.SS2.SSS3.p2.6.m6.1.1.cmml"><mi id="S4.SS2.SSS3.p2.6.m6.1.1.2" xref="S4.SS2.SSS3.p2.6.m6.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.6.m6.1.1.3" xref="S4.SS2.SSS3.p2.6.m6.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.6.m6.1b"><apply id="S4.SS2.SSS3.p2.6.m6.1.1.cmml" xref="S4.SS2.SSS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.6.m6.1.1.1.cmml" xref="S4.SS2.SSS3.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.6.m6.1.1.2.cmml" xref="S4.SS2.SSS3.p2.6.m6.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.6.m6.1.1.3.cmml" xref="S4.SS2.SSS3.p2.6.m6.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.6.m6.1c">f_{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> represents the feature decoded by the previous level in the Decoder, <math alttext="B" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.7.m7.1"><semantics id="S4.SS2.SSS3.p2.7.m7.1a"><mi id="S4.SS2.SSS3.p2.7.m7.1.1" xref="S4.SS2.SSS3.p2.7.m7.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.7.m7.1b"><ci id="S4.SS2.SSS3.p2.7.m7.1.1.cmml" xref="S4.SS2.SSS3.p2.7.m7.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.7.m7.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.7.m7.1d">italic_B</annotation></semantics></math> represents the batch size of the feature, <math alttext="C" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.8.m8.1"><semantics id="S4.SS2.SSS3.p2.8.m8.1a"><mi id="S4.SS2.SSS3.p2.8.m8.1.1" xref="S4.SS2.SSS3.p2.8.m8.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.8.m8.1b"><ci id="S4.SS2.SSS3.p2.8.m8.1.1.cmml" xref="S4.SS2.SSS3.p2.8.m8.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.8.m8.1c">C</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.8.m8.1d">italic_C</annotation></semantics></math> represents the number of channels, <math alttext="W" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.9.m9.1"><semantics id="S4.SS2.SSS3.p2.9.m9.1a"><mi id="S4.SS2.SSS3.p2.9.m9.1.1" xref="S4.SS2.SSS3.p2.9.m9.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.9.m9.1b"><ci id="S4.SS2.SSS3.p2.9.m9.1.1.cmml" xref="S4.SS2.SSS3.p2.9.m9.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.9.m9.1c">W</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.9.m9.1d">italic_W</annotation></semantics></math> represents the size of the feature map. In the beginning, we downsample (DS) the input feature <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.10.m10.1"><semantics id="S4.SS2.SSS3.p2.10.m10.1a"><msub id="S4.SS2.SSS3.p2.10.m10.1.1" xref="S4.SS2.SSS3.p2.10.m10.1.1.cmml"><mi id="S4.SS2.SSS3.p2.10.m10.1.1.2" xref="S4.SS2.SSS3.p2.10.m10.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.10.m10.1.1.3" xref="S4.SS2.SSS3.p2.10.m10.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.10.m10.1b"><apply id="S4.SS2.SSS3.p2.10.m10.1.1.cmml" xref="S4.SS2.SSS3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.10.m10.1.1.1.cmml" xref="S4.SS2.SSS3.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.10.m10.1.1.2.cmml" xref="S4.SS2.SSS3.p2.10.m10.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.10.m10.1.1.3.cmml" xref="S4.SS2.SSS3.p2.10.m10.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.10.m10.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.10.m10.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> by using max-pooling. The DS allows us to obtain a feature map that captures more global semantic information, thereby capturing the overall structure of the input feature. Following this, we utilize a convolution layer, which is composed of a 3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.11.m11.1"><semantics id="S4.SS2.SSS3.p2.11.m11.1a"><mo id="S4.SS2.SSS3.p2.11.m11.1.1" xref="S4.SS2.SSS3.p2.11.m11.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.11.m11.1b"><times id="S4.SS2.SSS3.p2.11.m11.1.1.cmml" xref="S4.SS2.SSS3.p2.11.m11.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.11.m11.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.11.m11.1d">×</annotation></semantics></math>3 convolution kernel. Additionally, we apply 1-pixel padding convolution to maintain the spatial dimensions of the feature map. To enhance the stability and performance of the network, we incorporate spectral normalization (SN) <cite class="ltx_cite ltx_citemacro_citep">(Miyato et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib35" title="">2018</a>)</cite> and batch normalization (BN) <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib20" title="">2015</a>)</cite>. And employ the LeakyReLU activation function to introduce non-linearity and avoid vanishing gradient, so the convolution layer <math alttext="Conv(x)=LeakyReLU(BN(SN(Conv_{3\times 3}(x))))" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.12.m12.3"><semantics id="S4.SS2.SSS3.p2.12.m12.3a"><mrow id="S4.SS2.SSS3.p2.12.m12.3.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.cmml"><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.3.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.2.cmml">C</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.3.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.3.cmml">o</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.1a" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.3.4" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.4.cmml">n</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.1b" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.3.5" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.5.cmml">v</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.1c" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.1.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.3.6.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.cmml"><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.6.2.1" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.cmml">(</mo><mi id="S4.SS2.SSS3.p2.12.m12.1.1" xref="S4.SS2.SSS3.p2.12.m12.1.1.cmml">x</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.3.6.2.2" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS3.p2.12.m12.3.3.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.2.cmml">=</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.3.cmml">L</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.4" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.4.cmml">e</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2a" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.5" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.5.cmml">a</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2b" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.6" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.6.cmml">k</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2c" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.7" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.7.cmml">y</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2d" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.8" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.8.cmml">R</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2e" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.9" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.9.cmml">e</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2f" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.10" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.10.cmml">L</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2g" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.11" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.11.cmml">U</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.2h" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.cmml"><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.3.cmml">B</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.4" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.4.cmml">N</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2a" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.cmml"><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.4" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.4.cmml">N</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2a" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">C</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml">o</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1a" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.4" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.4.cmml">n</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1b" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.2.cmml">v</mi><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.cmml"><mn id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.2.cmml">3</mn><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.1.cmml">×</mo><mn id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.3" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.3.cmml">3</mn></mrow></msub><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1c" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.6.2" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.6.2.1" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S4.SS2.SSS3.p2.12.m12.2.2" xref="S4.SS2.SSS3.p2.12.m12.2.2.cmml">x</mi><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.6.2.2" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.3" stretchy="false" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.12.m12.3b"><apply id="S4.SS2.SSS3.p2.12.m12.3.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3"><eq id="S4.SS2.SSS3.p2.12.m12.3.3.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.2"></eq><apply id="S4.SS2.SSS3.p2.12.m12.3.3.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3"><times id="S4.SS2.SSS3.p2.12.m12.3.3.3.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.1"></times><ci id="S4.SS2.SSS3.p2.12.m12.3.3.3.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.2">𝐶</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.3.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.3">𝑜</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.3.4.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.4">𝑛</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.3.5.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.3.5">𝑣</ci><ci id="S4.SS2.SSS3.p2.12.m12.1.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.1.1">𝑥</ci></apply><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1"><times id="S4.SS2.SSS3.p2.12.m12.3.3.1.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.2"></times><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.3">𝐿</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.4.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.4">𝑒</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.5.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.5">𝑎</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.6.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.6">𝑘</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.7.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.7">𝑦</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.8.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.8">𝑅</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.9.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.9">𝑒</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.10.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.10">𝐿</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.11.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.11">𝑈</ci><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1"><times id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.2"></times><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.3">𝐵</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.4.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.4">𝑁</ci><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1"><times id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.2"></times><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.3">𝑆</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.4.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.4">𝑁</ci><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1"><times id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.2">𝐶</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.3">𝑜</ci><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.4">𝑛</ci><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.2">𝑣</ci><apply id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3"><times id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.1.cmml" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.1"></times><cn id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.2.cmml" type="integer" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.2">3</cn><cn id="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.12.m12.3.3.1.1.1.1.1.1.1.1.1.1.5.3.3">3</cn></apply></apply><ci id="S4.SS2.SSS3.p2.12.m12.2.2.cmml" xref="S4.SS2.SSS3.p2.12.m12.2.2">𝑥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.12.m12.3c">Conv(x)=LeakyReLU(BN(SN(Conv_{3\times 3}(x))))</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.12.m12.3d">italic_C italic_o italic_n italic_v ( italic_x ) = italic_L italic_e italic_a italic_k italic_y italic_R italic_e italic_L italic_U ( italic_B italic_N ( italic_S italic_N ( italic_C italic_o italic_n italic_v start_POSTSUBSCRIPT 3 × 3 end_POSTSUBSCRIPT ( italic_x ) ) ) )</annotation></semantics></math>. Next, we concatenate this downscaled feature with the decoded feature <math alttext="f_{d}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.13.m13.1"><semantics id="S4.SS2.SSS3.p2.13.m13.1a"><msub id="S4.SS2.SSS3.p2.13.m13.1.1" xref="S4.SS2.SSS3.p2.13.m13.1.1.cmml"><mi id="S4.SS2.SSS3.p2.13.m13.1.1.2" xref="S4.SS2.SSS3.p2.13.m13.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.13.m13.1.1.3" xref="S4.SS2.SSS3.p2.13.m13.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.13.m13.1b"><apply id="S4.SS2.SSS3.p2.13.m13.1.1.cmml" xref="S4.SS2.SSS3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.13.m13.1.1.1.cmml" xref="S4.SS2.SSS3.p2.13.m13.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.13.m13.1.1.2.cmml" xref="S4.SS2.SSS3.p2.13.m13.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.13.m13.1.1.3.cmml" xref="S4.SS2.SSS3.p2.13.m13.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.13.m13.1c">f_{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.13.m13.1d">italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>. Subsequently, we perform upsampling (US) and convolution operations <math alttext="Conv(x)" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.14.m14.1"><semantics id="S4.SS2.SSS3.p2.14.m14.1a"><mrow id="S4.SS2.SSS3.p2.14.m14.1.2" xref="S4.SS2.SSS3.p2.14.m14.1.2.cmml"><mi id="S4.SS2.SSS3.p2.14.m14.1.2.2" xref="S4.SS2.SSS3.p2.14.m14.1.2.2.cmml">C</mi><mo id="S4.SS2.SSS3.p2.14.m14.1.2.1" xref="S4.SS2.SSS3.p2.14.m14.1.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.14.m14.1.2.3" xref="S4.SS2.SSS3.p2.14.m14.1.2.3.cmml">o</mi><mo id="S4.SS2.SSS3.p2.14.m14.1.2.1a" xref="S4.SS2.SSS3.p2.14.m14.1.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.14.m14.1.2.4" xref="S4.SS2.SSS3.p2.14.m14.1.2.4.cmml">n</mi><mo id="S4.SS2.SSS3.p2.14.m14.1.2.1b" xref="S4.SS2.SSS3.p2.14.m14.1.2.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.14.m14.1.2.5" xref="S4.SS2.SSS3.p2.14.m14.1.2.5.cmml">v</mi><mo id="S4.SS2.SSS3.p2.14.m14.1.2.1c" xref="S4.SS2.SSS3.p2.14.m14.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.SSS3.p2.14.m14.1.2.6.2" xref="S4.SS2.SSS3.p2.14.m14.1.2.cmml"><mo id="S4.SS2.SSS3.p2.14.m14.1.2.6.2.1" stretchy="false" xref="S4.SS2.SSS3.p2.14.m14.1.2.cmml">(</mo><mi id="S4.SS2.SSS3.p2.14.m14.1.1" xref="S4.SS2.SSS3.p2.14.m14.1.1.cmml">x</mi><mo id="S4.SS2.SSS3.p2.14.m14.1.2.6.2.2" stretchy="false" xref="S4.SS2.SSS3.p2.14.m14.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.14.m14.1b"><apply id="S4.SS2.SSS3.p2.14.m14.1.2.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2"><times id="S4.SS2.SSS3.p2.14.m14.1.2.1.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2.1"></times><ci id="S4.SS2.SSS3.p2.14.m14.1.2.2.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2.2">𝐶</ci><ci id="S4.SS2.SSS3.p2.14.m14.1.2.3.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2.3">𝑜</ci><ci id="S4.SS2.SSS3.p2.14.m14.1.2.4.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2.4">𝑛</ci><ci id="S4.SS2.SSS3.p2.14.m14.1.2.5.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.2.5">𝑣</ci><ci id="S4.SS2.SSS3.p2.14.m14.1.1.cmml" xref="S4.SS2.SSS3.p2.14.m14.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.14.m14.1c">Conv(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.14.m14.1d">italic_C italic_o italic_n italic_v ( italic_x )</annotation></semantics></math> to generate a feature that incorporates both the encoding information from the image layer and the decoding information from the upper layer. Finally, we add this combined feature to the input feature <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.15.m15.1"><semantics id="S4.SS2.SSS3.p2.15.m15.1a"><msub id="S4.SS2.SSS3.p2.15.m15.1.1" xref="S4.SS2.SSS3.p2.15.m15.1.1.cmml"><mi id="S4.SS2.SSS3.p2.15.m15.1.1.2" xref="S4.SS2.SSS3.p2.15.m15.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.15.m15.1.1.3" xref="S4.SS2.SSS3.p2.15.m15.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.15.m15.1b"><apply id="S4.SS2.SSS3.p2.15.m15.1.1.cmml" xref="S4.SS2.SSS3.p2.15.m15.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.15.m15.1.1.1.cmml" xref="S4.SS2.SSS3.p2.15.m15.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.15.m15.1.1.2.cmml" xref="S4.SS2.SSS3.p2.15.m15.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.15.m15.1.1.3.cmml" xref="S4.SS2.SSS3.p2.15.m15.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.15.m15.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.15.m15.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math>. To ensure that the resulting feature contains both global information from the combined feature and fine-grained information from <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.16.m16.1"><semantics id="S4.SS2.SSS3.p2.16.m16.1a"><msub id="S4.SS2.SSS3.p2.16.m16.1.1" xref="S4.SS2.SSS3.p2.16.m16.1.1.cmml"><mi id="S4.SS2.SSS3.p2.16.m16.1.1.2" xref="S4.SS2.SSS3.p2.16.m16.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.16.m16.1.1.3" xref="S4.SS2.SSS3.p2.16.m16.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.16.m16.1b"><apply id="S4.SS2.SSS3.p2.16.m16.1.1.cmml" xref="S4.SS2.SSS3.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.16.m16.1.1.1.cmml" xref="S4.SS2.SSS3.p2.16.m16.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.16.m16.1.1.2.cmml" xref="S4.SS2.SSS3.p2.16.m16.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.16.m16.1.1.3.cmml" xref="S4.SS2.SSS3.p2.16.m16.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.16.m16.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.16.m16.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math>. Furthermore, we introduce a differentiable parameter <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.17.m17.1"><semantics id="S4.SS2.SSS3.p2.17.m17.1a"><mi id="S4.SS2.SSS3.p2.17.m17.1.1" xref="S4.SS2.SSS3.p2.17.m17.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.17.m17.1b"><ci id="S4.SS2.SSS3.p2.17.m17.1.1.cmml" xref="S4.SS2.SSS3.p2.17.m17.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.17.m17.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.17.m17.1d">italic_γ</annotation></semantics></math> as the weight of the <math alttext="f_{e}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.18.m18.1"><semantics id="S4.SS2.SSS3.p2.18.m18.1a"><msub id="S4.SS2.SSS3.p2.18.m18.1.1" xref="S4.SS2.SSS3.p2.18.m18.1.1.cmml"><mi id="S4.SS2.SSS3.p2.18.m18.1.1.2" xref="S4.SS2.SSS3.p2.18.m18.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS3.p2.18.m18.1.1.3" xref="S4.SS2.SSS3.p2.18.m18.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.18.m18.1b"><apply id="S4.SS2.SSS3.p2.18.m18.1.1.cmml" xref="S4.SS2.SSS3.p2.18.m18.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.18.m18.1.1.1.cmml" xref="S4.SS2.SSS3.p2.18.m18.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.18.m18.1.1.2.cmml" xref="S4.SS2.SSS3.p2.18.m18.1.1.2">𝑓</ci><ci id="S4.SS2.SSS3.p2.18.m18.1.1.3.cmml" xref="S4.SS2.SSS3.p2.18.m18.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.18.m18.1c">f_{e}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.18.m18.1d">italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> feature to minimally affect the final decoded features. In summary, the described process can be mathematically expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}D1(f_{e})&amp;=Conv(DS(f_{e})),\quad D2(f_{e},f_{d})=Conv(US(Cat(D1(f%
_{e}),f_{d})))\\
DSMFF(f_{e},f_{d})&amp;=Conv(D2(f_{e},f_{d})+\gamma\cdot f_{e}),\quad f_{d_{n-1}}=%
DSMFF(f_{e_{n}},f_{d_{n}}),n=3,2,1.\end{split}" class="ltx_math_unparsed" display="block" id="S4.E7.m1.118"><semantics id="S4.E7.m1.118a"><mtable columnspacing="0pt" displaystyle="true" id="S4.E7.m1.118.118.6" rowspacing="0pt"><mtr id="S4.E7.m1.118.118.6a"><mtd class="ltx_align_right" columnalign="right" id="S4.E7.m1.118.118.6b"><mrow id="S4.E7.m1.113.113.1.113.55.7"><mi id="S4.E7.m1.1.1.1.1.1.1">D</mi><mo id="S4.E7.m1.113.113.1.113.55.7.8">⁢</mo><mn id="S4.E7.m1.2.2.2.2.2.2">1</mn><mo id="S4.E7.m1.113.113.1.113.55.7.8a">⁢</mo><mrow id="S4.E7.m1.113.113.1.113.55.7.7.1"><mo id="S4.E7.m1.3.3.3.3.3.3" stretchy="false">(</mo><msub id="S4.E7.m1.113.113.1.113.55.7.7.1.1"><mi id="S4.E7.m1.4.4.4.4.4.4">f</mi><mi id="S4.E7.m1.5.5.5.5.5.5.1">e</mi></msub><mo id="S4.E7.m1.6.6.6.6.6.6" stretchy="false">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.118.118.6c"><mrow id="S4.E7.m1.115.115.3.115.57.50.50"><mrow id="S4.E7.m1.114.114.2.114.56.49.49.1"><mi id="S4.E7.m1.114.114.2.114.56.49.49.1.2"></mi><mo id="S4.E7.m1.7.7.7.7.1.1">=</mo><mrow id="S4.E7.m1.114.114.2.114.56.49.49.1.1"><mi id="S4.E7.m1.8.8.8.8.2.2">C</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.2">⁢</mo><mi id="S4.E7.m1.9.9.9.9.3.3">o</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.2a">⁢</mo><mi id="S4.E7.m1.10.10.10.10.4.4">n</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.2b">⁢</mo><mi id="S4.E7.m1.11.11.11.11.5.5">v</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.2c">⁢</mo><mrow id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1"><mo id="S4.E7.m1.12.12.12.12.6.6" stretchy="false">(</mo><mrow id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1.1"><mi id="S4.E7.m1.13.13.13.13.7.7">D</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1.1.2">⁢</mo><mi id="S4.E7.m1.14.14.14.14.8.8">S</mi><mo id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1.1.2a">⁢</mo><mrow id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1.1.1.1"><mo id="S4.E7.m1.15.15.15.15.9.9" stretchy="false">(</mo><msub id="S4.E7.m1.114.114.2.114.56.49.49.1.1.1.1.1.1.1.1"><mi id="S4.E7.m1.16.16.16.16.10.10">f</mi><mi id="S4.E7.m1.17.17.17.17.11.11.1">e</mi></msub><mo id="S4.E7.m1.18.18.18.18.12.12" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.19.19.19.19.13.13" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E7.m1.20.20.20.20.14.14" rspace="1.167em">,</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2"><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.2"><mi id="S4.E7.m1.21.21.21.21.15.15">D</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.2.3">⁢</mo><mn id="S4.E7.m1.22.22.22.22.16.16">2</mn><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.2.3a">⁢</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.2.2.2"><mo id="S4.E7.m1.23.23.23.23.17.17" stretchy="false">(</mo><msub id="S4.E7.m1.115.115.3.115.57.50.50.2.1.1.1.1"><mi id="S4.E7.m1.24.24.24.24.18.18">f</mi><mi id="S4.E7.m1.25.25.25.25.19.19.1">e</mi></msub><mo id="S4.E7.m1.26.26.26.26.20.20">,</mo><msub id="S4.E7.m1.115.115.3.115.57.50.50.2.2.2.2.2"><mi id="S4.E7.m1.27.27.27.27.21.21">f</mi><mi id="S4.E7.m1.28.28.28.28.22.22.1">d</mi></msub><mo id="S4.E7.m1.29.29.29.29.23.23" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.30.30.30.30.24.24">=</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3"><mi id="S4.E7.m1.31.31.31.31.25.25">C</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.2">⁢</mo><mi id="S4.E7.m1.32.32.32.32.26.26">o</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.2a">⁢</mo><mi id="S4.E7.m1.33.33.33.33.27.27">n</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.2b">⁢</mo><mi id="S4.E7.m1.34.34.34.34.28.28">v</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.2c">⁢</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1"><mo id="S4.E7.m1.35.35.35.35.29.29" stretchy="false">(</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1"><mi id="S4.E7.m1.36.36.36.36.30.30">U</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.2">⁢</mo><mi id="S4.E7.m1.37.37.37.37.31.31">S</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.2a">⁢</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1"><mo id="S4.E7.m1.38.38.38.38.32.32" stretchy="false">(</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1"><mi id="S4.E7.m1.39.39.39.39.33.33">C</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.3">⁢</mo><mi id="S4.E7.m1.40.40.40.40.34.34">a</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.3a">⁢</mo><mi id="S4.E7.m1.41.41.41.41.35.35">t</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.3b">⁢</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.2.2"><mo id="S4.E7.m1.42.42.42.42.36.36" stretchy="false">(</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.1.1.1"><mi id="S4.E7.m1.43.43.43.43.37.37">D</mi><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.1.1.1.2">⁢</mo><mn id="S4.E7.m1.44.44.44.44.38.38">1</mn><mo id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.1.1.1.2a">⁢</mo><mrow id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.1.1.1.1.1"><mo id="S4.E7.m1.45.45.45.45.39.39" stretchy="false">(</mo><msub id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.1.1.1.1.1.1"><mi id="S4.E7.m1.46.46.46.46.40.40">f</mi><mi id="S4.E7.m1.47.47.47.47.41.41.1">e</mi></msub><mo id="S4.E7.m1.48.48.48.48.42.42" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.49.49.49.49.43.43">,</mo><msub id="S4.E7.m1.115.115.3.115.57.50.50.2.3.1.1.1.1.1.1.2.2.2"><mi id="S4.E7.m1.50.50.50.50.44.44">f</mi><mi id="S4.E7.m1.51.51.51.51.45.45.1">d</mi></msub><mo id="S4.E7.m1.52.52.52.52.46.46" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.53.53.53.53.47.47" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.54.54.54.54.48.48" stretchy="false">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S4.E7.m1.118.118.6d"><mtd class="ltx_align_right" columnalign="right" id="S4.E7.m1.118.118.6e"><mrow id="S4.E7.m1.117.117.5.117.60.14"><mi id="S4.E7.m1.55.55.55.1.1.1">D</mi><mo id="S4.E7.m1.117.117.5.117.60.14.15">⁢</mo><mi id="S4.E7.m1.56.56.56.2.2.2">S</mi><mo id="S4.E7.m1.117.117.5.117.60.14.15a">⁢</mo><mi id="S4.E7.m1.57.57.57.3.3.3">M</mi><mo id="S4.E7.m1.117.117.5.117.60.14.15b">⁢</mo><mi id="S4.E7.m1.58.58.58.4.4.4">F</mi><mo id="S4.E7.m1.117.117.5.117.60.14.15c">⁢</mo><mi id="S4.E7.m1.59.59.59.5.5.5">F</mi><mo id="S4.E7.m1.117.117.5.117.60.14.15d">⁢</mo><mrow id="S4.E7.m1.117.117.5.117.60.14.14.2"><mo id="S4.E7.m1.60.60.60.6.6.6" stretchy="false">(</mo><msub id="S4.E7.m1.116.116.4.116.59.13.13.1.1"><mi id="S4.E7.m1.61.61.61.7.7.7">f</mi><mi id="S4.E7.m1.62.62.62.8.8.8.1">e</mi></msub><mo id="S4.E7.m1.63.63.63.9.9.9">,</mo><msub id="S4.E7.m1.117.117.5.117.60.14.14.2.2"><mi id="S4.E7.m1.64.64.64.10.10.10">f</mi><mi id="S4.E7.m1.65.65.65.11.11.11.1">d</mi></msub><mo id="S4.E7.m1.66.66.66.12.12.12" stretchy="false">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.118.118.6f"><mrow id="S4.E7.m1.118.118.6.118.61.47.47"><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1"><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1"><mi id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.2"></mi><mo id="S4.E7.m1.67.67.67.13.1.1">=</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1"><mi id="S4.E7.m1.68.68.68.14.2.2">C</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.2">⁢</mo><mi id="S4.E7.m1.69.69.69.15.3.3">o</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.2a">⁢</mo><mi id="S4.E7.m1.70.70.70.16.4.4">n</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.2b">⁢</mo><mi id="S4.E7.m1.71.71.71.17.5.5">v</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.2c">⁢</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1"><mo id="S4.E7.m1.72.72.72.18.6.6" stretchy="false">(</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1"><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.2"><mi id="S4.E7.m1.73.73.73.19.7.7">D</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.2.3">⁢</mo><mn id="S4.E7.m1.74.74.74.20.8.8">2</mn><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.2.3a">⁢</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.2.2.2"><mo id="S4.E7.m1.75.75.75.21.9.9" stretchy="false">(</mo><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.1.1.1.1"><mi id="S4.E7.m1.76.76.76.22.10.10">f</mi><mi id="S4.E7.m1.77.77.77.23.11.11.1">e</mi></msub><mo id="S4.E7.m1.78.78.78.24.12.12">,</mo><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.2.2.2.2"><mi id="S4.E7.m1.79.79.79.25.13.13">f</mi><mi id="S4.E7.m1.80.80.80.26.14.14.1">d</mi></msub><mo id="S4.E7.m1.81.81.81.27.15.15" stretchy="false">)</mo></mrow></mrow><mo id="S4.E7.m1.82.82.82.28.16.16">+</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.3"><mi id="S4.E7.m1.83.83.83.29.17.17">γ</mi><mo id="S4.E7.m1.84.84.84.30.18.18" lspace="0.222em" rspace="0.222em">⋅</mo><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.1.1.1.1.1.1.3.1"><mi id="S4.E7.m1.85.85.85.31.19.19">f</mi><mi id="S4.E7.m1.86.86.86.32.20.20.1">e</mi></msub></mrow></mrow><mo id="S4.E7.m1.87.87.87.33.21.21" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E7.m1.88.88.88.34.22.22" rspace="1.167em">,</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2"><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1"><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.3"><mi id="S4.E7.m1.89.89.89.35.23.23">f</mi><msub id="S4.E7.m1.90.90.90.36.24.24.1"><mi id="S4.E7.m1.90.90.90.36.24.24.1.2">d</mi><mrow id="S4.E7.m1.90.90.90.36.24.24.1.3"><mi id="S4.E7.m1.90.90.90.36.24.24.1.3.2">n</mi><mo id="S4.E7.m1.90.90.90.36.24.24.1.3.1">−</mo><mn id="S4.E7.m1.90.90.90.36.24.24.1.3.3">1</mn></mrow></msub></msub><mo id="S4.E7.m1.91.91.91.37.25.25">=</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2"><mi id="S4.E7.m1.92.92.92.38.26.26">D</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.3">⁢</mo><mi id="S4.E7.m1.93.93.93.39.27.27">S</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.3a">⁢</mo><mi id="S4.E7.m1.94.94.94.40.28.28">M</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.3b">⁢</mo><mi id="S4.E7.m1.95.95.95.41.29.29">F</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.3c">⁢</mo><mi id="S4.E7.m1.96.96.96.42.30.30">F</mi><mo id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.3d">⁢</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.2.2"><mo id="S4.E7.m1.97.97.97.43.31.31" stretchy="false">(</mo><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.1.1.1.1"><mi id="S4.E7.m1.98.98.98.44.32.32">f</mi><msub id="S4.E7.m1.99.99.99.45.33.33.1"><mi id="S4.E7.m1.99.99.99.45.33.33.1.2">e</mi><mi id="S4.E7.m1.99.99.99.45.33.33.1.3">n</mi></msub></msub><mo id="S4.E7.m1.100.100.100.46.34.34">,</mo><msub id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.1.1.2.2.2.2"><mi id="S4.E7.m1.101.101.101.47.35.35">f</mi><msub id="S4.E7.m1.102.102.102.48.36.36.1"><mi id="S4.E7.m1.102.102.102.48.36.36.1.2">d</mi><mi id="S4.E7.m1.102.102.102.48.36.36.1.3">n</mi></msub></msub><mo id="S4.E7.m1.103.103.103.49.37.37" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E7.m1.104.104.104.50.38.38">,</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.2.2"><mi id="S4.E7.m1.105.105.105.51.39.39">n</mi><mo id="S4.E7.m1.106.106.106.52.40.40">=</mo><mrow id="S4.E7.m1.118.118.6.118.61.47.47.1.2.2.2.2.1"><mn id="S4.E7.m1.107.107.107.53.41.41">3</mn><mo id="S4.E7.m1.108.108.108.54.42.42">,</mo><mn id="S4.E7.m1.109.109.109.55.43.43">2</mn><mo id="S4.E7.m1.110.110.110.56.44.44">,</mo><mn id="S4.E7.m1.111.111.111.57.45.45">1</mn></mrow></mrow></mrow></mrow><mo id="S4.E7.m1.112.112.112.58.46.46" lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S4.E7.m1.118b">\begin{split}D1(f_{e})&amp;=Conv(DS(f_{e})),\quad D2(f_{e},f_{d})=Conv(US(Cat(D1(f%
_{e}),f_{d})))\\
DSMFF(f_{e},f_{d})&amp;=Conv(D2(f_{e},f_{d})+\gamma\cdot f_{e}),\quad f_{d_{n-1}}=%
DSMFF(f_{e_{n}},f_{d_{n}}),n=3,2,1.\end{split}</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.118c">start_ROW start_CELL italic_D 1 ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) end_CELL start_CELL = italic_C italic_o italic_n italic_v ( italic_D italic_S ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) ) , italic_D 2 ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = italic_C italic_o italic_n italic_v ( italic_U italic_S ( italic_C italic_a italic_t ( italic_D 1 ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) , italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ) ) end_CELL end_ROW start_ROW start_CELL italic_D italic_S italic_M italic_F italic_F ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) end_CELL start_CELL = italic_C italic_o italic_n italic_v ( italic_D 2 ( italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) + italic_γ ⋅ italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) , italic_f start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_D italic_S italic_M italic_F italic_F ( italic_f start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , italic_n = 3 , 2 , 1 . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>Loss Function</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.12">Our loss function comprises four components: <math alttext="\mathcal{L}_{gram}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.1.m1.1"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><msub id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.1.m1.1.1.2" xref="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.SS2.SSS4.p1.1.m1.1.1.3" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.2.cmml">g</mi><mo id="S4.SS2.SSS4.p1.1.m1.1.1.3.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.3.cmml">r</mi><mo id="S4.SS2.SSS4.p1.1.m1.1.1.3.1a" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.1.m1.1.1.3.4" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.4.cmml">a</mi><mo id="S4.SS2.SSS4.p1.1.m1.1.1.3.1b" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.1.m1.1.1.3.5" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.5.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><apply id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.2">ℒ</ci><apply id="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3"><times id="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.2">𝑔</ci><ci id="S4.SS2.SSS4.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.3">𝑟</ci><ci id="S4.SS2.SSS4.p1.1.m1.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.4">𝑎</ci><ci id="S4.SS2.SSS4.p1.1.m1.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">\mathcal{L}_{gram}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_r italic_a italic_m end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathcal{L}_{g}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.2.m2.1"><semantics id="S4.SS2.SSS4.p1.2.m2.1a"><msub id="S4.SS2.SSS4.p1.2.m2.1.1" xref="S4.SS2.SSS4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.2.m2.1.1.2" xref="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml">ℒ</mi><mi id="S4.SS2.SSS4.p1.2.m2.1.1.3" xref="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.2.m2.1b"><apply id="S4.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.2">ℒ</ci><ci id="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.2.m2.1c">\mathcal{L}_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathcal{L}_{cos}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.3.m3.1"><semantics id="S4.SS2.SSS4.p1.3.m3.1a"><msub id="S4.SS2.SSS4.p1.3.m3.1.1" xref="S4.SS2.SSS4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.3.m3.1.1.2" xref="S4.SS2.SSS4.p1.3.m3.1.1.2.cmml">ℒ</mi><mrow id="S4.SS2.SSS4.p1.3.m3.1.1.3" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.3.m3.1.1.3.1" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.3.m3.1.1.3.3" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.3.cmml">o</mi><mo id="S4.SS2.SSS4.p1.3.m3.1.1.3.1a" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.3.m3.1.1.3.4" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.3.m3.1b"><apply id="S4.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.2">ℒ</ci><apply id="S4.SS2.SSS4.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.3"><times id="S4.SS2.SSS4.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.3">𝑜</ci><ci id="S4.SS2.SSS4.p1.3.m3.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.3.m3.1c">\mathcal{L}_{cos}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="{L}_{clip}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.4.m4.1"><semantics id="S4.SS2.SSS4.p1.4.m4.1a"><msub id="S4.SS2.SSS4.p1.4.m4.1.1" xref="S4.SS2.SSS4.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS4.p1.4.m4.1.1.2" xref="S4.SS2.SSS4.p1.4.m4.1.1.2.cmml">L</mi><mrow id="S4.SS2.SSS4.p1.4.m4.1.1.3" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.4.m4.1.1.3.1" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.4.m4.1.1.3.3" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.3.cmml">l</mi><mo id="S4.SS2.SSS4.p1.4.m4.1.1.3.1a" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.4.m4.1.1.3.4" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.4.cmml">i</mi><mo id="S4.SS2.SSS4.p1.4.m4.1.1.3.1b" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.4.m4.1.1.3.5" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.4.m4.1b"><apply id="S4.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.2">𝐿</ci><apply id="S4.SS2.SSS4.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3"><times id="S4.SS2.SSS4.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.4.m4.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.3">𝑙</ci><ci id="S4.SS2.SSS4.p1.4.m4.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.4">𝑖</ci><ci id="S4.SS2.SSS4.p1.4.m4.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.4.m4.1c">{L}_{clip}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.4.m4.1d">italic_L start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. Among them, <math alttext="\mathcal{L}_{gram}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.5.m5.1"><semantics id="S4.SS2.SSS4.p1.5.m5.1a"><msub id="S4.SS2.SSS4.p1.5.m5.1.1" xref="S4.SS2.SSS4.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.5.m5.1.1.2" xref="S4.SS2.SSS4.p1.5.m5.1.1.2.cmml">ℒ</mi><mrow id="S4.SS2.SSS4.p1.5.m5.1.1.3" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.5.m5.1.1.3.2" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.2.cmml">g</mi><mo id="S4.SS2.SSS4.p1.5.m5.1.1.3.1" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.5.m5.1.1.3.3" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.3.cmml">r</mi><mo id="S4.SS2.SSS4.p1.5.m5.1.1.3.1a" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.5.m5.1.1.3.4" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.4.cmml">a</mi><mo id="S4.SS2.SSS4.p1.5.m5.1.1.3.1b" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.5.m5.1.1.3.5" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.5.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.5.m5.1b"><apply id="S4.SS2.SSS4.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.5.m5.1.1.1.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.5.m5.1.1.2.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.2">ℒ</ci><apply id="S4.SS2.SSS4.p1.5.m5.1.1.3.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3"><times id="S4.SS2.SSS4.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.2">𝑔</ci><ci id="S4.SS2.SSS4.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.3">𝑟</ci><ci id="S4.SS2.SSS4.p1.5.m5.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.4">𝑎</ci><ci id="S4.SS2.SSS4.p1.5.m5.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.5.m5.1c">\mathcal{L}_{gram}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_r italic_a italic_m end_POSTSUBSCRIPT</annotation></semantics></math> uses the Gram matrix and Mean Square Error loss to control the consistency of detailed features in the generated sketches. <math alttext="\mathcal{L}_{g}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.6.m6.1"><semantics id="S4.SS2.SSS4.p1.6.m6.1a"><msub id="S4.SS2.SSS4.p1.6.m6.1.1" xref="S4.SS2.SSS4.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.6.m6.1.1.2" xref="S4.SS2.SSS4.p1.6.m6.1.1.2.cmml">ℒ</mi><mi id="S4.SS2.SSS4.p1.6.m6.1.1.3" xref="S4.SS2.SSS4.p1.6.m6.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.6.m6.1b"><apply id="S4.SS2.SSS4.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.6.m6.1.1.1.cmml" xref="S4.SS2.SSS4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.6.m6.1.1.2.cmml" xref="S4.SS2.SSS4.p1.6.m6.1.1.2">ℒ</ci><ci id="S4.SS2.SSS4.p1.6.m6.1.1.3.cmml" xref="S4.SS2.SSS4.p1.6.m6.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.6.m6.1c">\mathcal{L}_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.6.m6.1d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> is the discriminator loss, evaluating the quality of the generated sketches and facilitating model convergence. <math alttext="\mathcal{L}_{cos}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.7.m7.1"><semantics id="S4.SS2.SSS4.p1.7.m7.1a"><msub id="S4.SS2.SSS4.p1.7.m7.1.1" xref="S4.SS2.SSS4.p1.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS4.p1.7.m7.1.1.2" xref="S4.SS2.SSS4.p1.7.m7.1.1.2.cmml">ℒ</mi><mrow id="S4.SS2.SSS4.p1.7.m7.1.1.3" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.7.m7.1.1.3.2" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.7.m7.1.1.3.1" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.7.m7.1.1.3.3" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.3.cmml">o</mi><mo id="S4.SS2.SSS4.p1.7.m7.1.1.3.1a" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.7.m7.1.1.3.4" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.7.m7.1b"><apply id="S4.SS2.SSS4.p1.7.m7.1.1.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.7.m7.1.1.1.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.7.m7.1.1.2.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.2">ℒ</ci><apply id="S4.SS2.SSS4.p1.7.m7.1.1.3.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.3"><times id="S4.SS2.SSS4.p1.7.m7.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.7.m7.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.7.m7.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.3">𝑜</ci><ci id="S4.SS2.SSS4.p1.7.m7.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.7.m7.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.7.m7.1c">\mathcal{L}_{cos}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.7.m7.1d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and CLIP-based <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib37" title="">2021</a>)</cite> <math alttext="{L}_{clip}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.8.m8.1"><semantics id="S4.SS2.SSS4.p1.8.m8.1a"><msub id="S4.SS2.SSS4.p1.8.m8.1.1" xref="S4.SS2.SSS4.p1.8.m8.1.1.cmml"><mi id="S4.SS2.SSS4.p1.8.m8.1.1.2" xref="S4.SS2.SSS4.p1.8.m8.1.1.2.cmml">L</mi><mrow id="S4.SS2.SSS4.p1.8.m8.1.1.3" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.8.m8.1.1.3.2" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.8.m8.1.1.3.1" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.8.m8.1.1.3.3" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.3.cmml">l</mi><mo id="S4.SS2.SSS4.p1.8.m8.1.1.3.1a" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.8.m8.1.1.3.4" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.4.cmml">i</mi><mo id="S4.SS2.SSS4.p1.8.m8.1.1.3.1b" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.8.m8.1.1.3.5" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.8.m8.1b"><apply id="S4.SS2.SSS4.p1.8.m8.1.1.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.8.m8.1.1.1.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.8.m8.1.1.2.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.2">𝐿</ci><apply id="S4.SS2.SSS4.p1.8.m8.1.1.3.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3"><times id="S4.SS2.SSS4.p1.8.m8.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.8.m8.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.8.m8.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.3">𝑙</ci><ci id="S4.SS2.SSS4.p1.8.m8.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.4">𝑖</ci><ci id="S4.SS2.SSS4.p1.8.m8.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.8.m8.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.8.m8.1c">{L}_{clip}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.8.m8.1d">italic_L start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT</annotation></semantics></math>, control the semantic feature consistency and global feature consistency of the reference sketch and the generated sketch, respectively. The weight values assigned to these losses are as follows: <math alttext="\lambda_{gram}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.9.m9.1"><semantics id="S4.SS2.SSS4.p1.9.m9.1a"><msub id="S4.SS2.SSS4.p1.9.m9.1.1" xref="S4.SS2.SSS4.p1.9.m9.1.1.cmml"><mi id="S4.SS2.SSS4.p1.9.m9.1.1.2" xref="S4.SS2.SSS4.p1.9.m9.1.1.2.cmml">λ</mi><mrow id="S4.SS2.SSS4.p1.9.m9.1.1.3" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.9.m9.1.1.3.2" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.2.cmml">g</mi><mo id="S4.SS2.SSS4.p1.9.m9.1.1.3.1" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.9.m9.1.1.3.3" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.3.cmml">r</mi><mo id="S4.SS2.SSS4.p1.9.m9.1.1.3.1a" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.9.m9.1.1.3.4" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.4.cmml">a</mi><mo id="S4.SS2.SSS4.p1.9.m9.1.1.3.1b" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.9.m9.1.1.3.5" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.5.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.9.m9.1b"><apply id="S4.SS2.SSS4.p1.9.m9.1.1.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.9.m9.1.1.1.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.9.m9.1.1.2.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.2">𝜆</ci><apply id="S4.SS2.SSS4.p1.9.m9.1.1.3.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3"><times id="S4.SS2.SSS4.p1.9.m9.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.9.m9.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.2">𝑔</ci><ci id="S4.SS2.SSS4.p1.9.m9.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.3">𝑟</ci><ci id="S4.SS2.SSS4.p1.9.m9.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.4">𝑎</ci><ci id="S4.SS2.SSS4.p1.9.m9.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.9.m9.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.9.m9.1c">\lambda_{gram}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.9.m9.1d">italic_λ start_POSTSUBSCRIPT italic_g italic_r italic_a italic_m end_POSTSUBSCRIPT</annotation></semantics></math>=100.0, <math alttext="\lambda_{g}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.10.m10.1"><semantics id="S4.SS2.SSS4.p1.10.m10.1a"><msub id="S4.SS2.SSS4.p1.10.m10.1.1" xref="S4.SS2.SSS4.p1.10.m10.1.1.cmml"><mi id="S4.SS2.SSS4.p1.10.m10.1.1.2" xref="S4.SS2.SSS4.p1.10.m10.1.1.2.cmml">λ</mi><mi id="S4.SS2.SSS4.p1.10.m10.1.1.3" xref="S4.SS2.SSS4.p1.10.m10.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.10.m10.1b"><apply id="S4.SS2.SSS4.p1.10.m10.1.1.cmml" xref="S4.SS2.SSS4.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.10.m10.1.1.1.cmml" xref="S4.SS2.SSS4.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.10.m10.1.1.2.cmml" xref="S4.SS2.SSS4.p1.10.m10.1.1.2">𝜆</ci><ci id="S4.SS2.SSS4.p1.10.m10.1.1.3.cmml" xref="S4.SS2.SSS4.p1.10.m10.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.10.m10.1c">\lambda_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.10.m10.1d">italic_λ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>=1.0, <math alttext="\lambda_{cos}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.11.m11.1"><semantics id="S4.SS2.SSS4.p1.11.m11.1a"><msub id="S4.SS2.SSS4.p1.11.m11.1.1" xref="S4.SS2.SSS4.p1.11.m11.1.1.cmml"><mi id="S4.SS2.SSS4.p1.11.m11.1.1.2" xref="S4.SS2.SSS4.p1.11.m11.1.1.2.cmml">λ</mi><mrow id="S4.SS2.SSS4.p1.11.m11.1.1.3" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.11.m11.1.1.3.2" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.11.m11.1.1.3.1" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.11.m11.1.1.3.3" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.3.cmml">o</mi><mo id="S4.SS2.SSS4.p1.11.m11.1.1.3.1a" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.11.m11.1.1.3.4" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.11.m11.1b"><apply id="S4.SS2.SSS4.p1.11.m11.1.1.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.11.m11.1.1.1.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.11.m11.1.1.2.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.2">𝜆</ci><apply id="S4.SS2.SSS4.p1.11.m11.1.1.3.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.3"><times id="S4.SS2.SSS4.p1.11.m11.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.11.m11.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.11.m11.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.3">𝑜</ci><ci id="S4.SS2.SSS4.p1.11.m11.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.11.m11.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.11.m11.1c">\lambda_{cos}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.11.m11.1d">italic_λ start_POSTSUBSCRIPT italic_c italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math>=0.5, <math alttext="\lambda_{clip}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.12.m12.1"><semantics id="S4.SS2.SSS4.p1.12.m12.1a"><msub id="S4.SS2.SSS4.p1.12.m12.1.1" xref="S4.SS2.SSS4.p1.12.m12.1.1.cmml"><mi id="S4.SS2.SSS4.p1.12.m12.1.1.2" xref="S4.SS2.SSS4.p1.12.m12.1.1.2.cmml">λ</mi><mrow id="S4.SS2.SSS4.p1.12.m12.1.1.3" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.cmml"><mi id="S4.SS2.SSS4.p1.12.m12.1.1.3.2" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.2.cmml">c</mi><mo id="S4.SS2.SSS4.p1.12.m12.1.1.3.1" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.12.m12.1.1.3.3" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.3.cmml">l</mi><mo id="S4.SS2.SSS4.p1.12.m12.1.1.3.1a" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.12.m12.1.1.3.4" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.4.cmml">i</mi><mo id="S4.SS2.SSS4.p1.12.m12.1.1.3.1b" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.SSS4.p1.12.m12.1.1.3.5" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.12.m12.1b"><apply id="S4.SS2.SSS4.p1.12.m12.1.1.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.12.m12.1.1.1.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p1.12.m12.1.1.2.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.2">𝜆</ci><apply id="S4.SS2.SSS4.p1.12.m12.1.1.3.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3"><times id="S4.SS2.SSS4.p1.12.m12.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.1"></times><ci id="S4.SS2.SSS4.p1.12.m12.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.2">𝑐</ci><ci id="S4.SS2.SSS4.p1.12.m12.1.1.3.3.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.3">𝑙</ci><ci id="S4.SS2.SSS4.p1.12.m12.1.1.3.4.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.4">𝑖</ci><ci id="S4.SS2.SSS4.p1.12.m12.1.1.3.5.cmml" xref="S4.SS2.SSS4.p1.12.m12.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.12.m12.1c">\lambda_{clip}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.12.m12.1d">italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT</annotation></semantics></math>=100.0. These weight values are set to achieve a balanced and stable training process, ensuring that each loss function contributes effectively to the overall model optimization. Therefore, the complete loss function of our <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p1.12.1">Image-to-Sketch Generation</span> model is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{I2SM}=\lambda_{gram}\mathcal{L}_{gram}+\lambda_{g}\mathcal{L}_{g}%
+\lambda_{cos}\mathcal{L}_{cos}+\lambda_{clip}\mathcal{L}_{clip}." class="ltx_Math" display="block" id="S4.E8.m1.1"><semantics id="S4.E8.m1.1a"><mrow id="S4.E8.m1.1.1.1" xref="S4.E8.m1.1.1.1.1.cmml"><mrow id="S4.E8.m1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.cmml"><msub id="S4.E8.m1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E8.m1.1.1.1.1.2.2" xref="S4.E8.m1.1.1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.E8.m1.1.1.1.1.2.3" xref="S4.E8.m1.1.1.1.1.2.3.cmml"><mi id="S4.E8.m1.1.1.1.1.2.3.2" xref="S4.E8.m1.1.1.1.1.2.3.2.cmml">I</mi><mo id="S4.E8.m1.1.1.1.1.2.3.1" xref="S4.E8.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mn id="S4.E8.m1.1.1.1.1.2.3.3" xref="S4.E8.m1.1.1.1.1.2.3.3.cmml">2</mn><mo id="S4.E8.m1.1.1.1.1.2.3.1a" xref="S4.E8.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.2.3.4" xref="S4.E8.m1.1.1.1.1.2.3.4.cmml">S</mi><mo id="S4.E8.m1.1.1.1.1.2.3.1b" xref="S4.E8.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.2.3.5" xref="S4.E8.m1.1.1.1.1.2.3.5.cmml">M</mi></mrow></msub><mo id="S4.E8.m1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E8.m1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.3.2.cmml"><msub id="S4.E8.m1.1.1.1.1.3.2.2" xref="S4.E8.m1.1.1.1.1.3.2.2.cmml"><mi id="S4.E8.m1.1.1.1.1.3.2.2.2" xref="S4.E8.m1.1.1.1.1.3.2.2.2.cmml">λ</mi><mrow id="S4.E8.m1.1.1.1.1.3.2.2.3" xref="S4.E8.m1.1.1.1.1.3.2.2.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.2.2.3.2" xref="S4.E8.m1.1.1.1.1.3.2.2.3.2.cmml">g</mi><mo id="S4.E8.m1.1.1.1.1.3.2.2.3.1" xref="S4.E8.m1.1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.2.3.3" xref="S4.E8.m1.1.1.1.1.3.2.2.3.3.cmml">r</mi><mo id="S4.E8.m1.1.1.1.1.3.2.2.3.1a" xref="S4.E8.m1.1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.2.3.4" xref="S4.E8.m1.1.1.1.1.3.2.2.3.4.cmml">a</mi><mo id="S4.E8.m1.1.1.1.1.3.2.2.3.1b" xref="S4.E8.m1.1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.2.3.5" xref="S4.E8.m1.1.1.1.1.3.2.2.3.5.cmml">m</mi></mrow></msub><mo id="S4.E8.m1.1.1.1.1.3.2.1" xref="S4.E8.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="S4.E8.m1.1.1.1.1.3.2.3" xref="S4.E8.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E8.m1.1.1.1.1.3.2.3.2" xref="S4.E8.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mrow id="S4.E8.m1.1.1.1.1.3.2.3.3" xref="S4.E8.m1.1.1.1.1.3.2.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.2.3.3.2" xref="S4.E8.m1.1.1.1.1.3.2.3.3.2.cmml">g</mi><mo id="S4.E8.m1.1.1.1.1.3.2.3.3.1" xref="S4.E8.m1.1.1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.3.3.3" xref="S4.E8.m1.1.1.1.1.3.2.3.3.3.cmml">r</mi><mo id="S4.E8.m1.1.1.1.1.3.2.3.3.1a" xref="S4.E8.m1.1.1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.3.3.4" xref="S4.E8.m1.1.1.1.1.3.2.3.3.4.cmml">a</mi><mo id="S4.E8.m1.1.1.1.1.3.2.3.3.1b" xref="S4.E8.m1.1.1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.2.3.3.5" xref="S4.E8.m1.1.1.1.1.3.2.3.3.5.cmml">m</mi></mrow></msub></mrow><mo id="S4.E8.m1.1.1.1.1.3.1" xref="S4.E8.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E8.m1.1.1.1.1.3.3" xref="S4.E8.m1.1.1.1.1.3.3.cmml"><msub id="S4.E8.m1.1.1.1.1.3.3.2" xref="S4.E8.m1.1.1.1.1.3.3.2.cmml"><mi id="S4.E8.m1.1.1.1.1.3.3.2.2" xref="S4.E8.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mi id="S4.E8.m1.1.1.1.1.3.3.2.3" xref="S4.E8.m1.1.1.1.1.3.3.2.3.cmml">g</mi></msub><mo id="S4.E8.m1.1.1.1.1.3.3.1" xref="S4.E8.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="S4.E8.m1.1.1.1.1.3.3.3" xref="S4.E8.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E8.m1.1.1.1.1.3.3.3.2" xref="S4.E8.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mi id="S4.E8.m1.1.1.1.1.3.3.3.3" xref="S4.E8.m1.1.1.1.1.3.3.3.3.cmml">g</mi></msub></mrow><mo id="S4.E8.m1.1.1.1.1.3.1a" xref="S4.E8.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E8.m1.1.1.1.1.3.4" xref="S4.E8.m1.1.1.1.1.3.4.cmml"><msub id="S4.E8.m1.1.1.1.1.3.4.2" xref="S4.E8.m1.1.1.1.1.3.4.2.cmml"><mi id="S4.E8.m1.1.1.1.1.3.4.2.2" xref="S4.E8.m1.1.1.1.1.3.4.2.2.cmml">λ</mi><mrow id="S4.E8.m1.1.1.1.1.3.4.2.3" xref="S4.E8.m1.1.1.1.1.3.4.2.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.4.2.3.2" xref="S4.E8.m1.1.1.1.1.3.4.2.3.2.cmml">c</mi><mo id="S4.E8.m1.1.1.1.1.3.4.2.3.1" xref="S4.E8.m1.1.1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.4.2.3.3" xref="S4.E8.m1.1.1.1.1.3.4.2.3.3.cmml">o</mi><mo id="S4.E8.m1.1.1.1.1.3.4.2.3.1a" xref="S4.E8.m1.1.1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.4.2.3.4" xref="S4.E8.m1.1.1.1.1.3.4.2.3.4.cmml">s</mi></mrow></msub><mo id="S4.E8.m1.1.1.1.1.3.4.1" xref="S4.E8.m1.1.1.1.1.3.4.1.cmml">⁢</mo><msub id="S4.E8.m1.1.1.1.1.3.4.3" xref="S4.E8.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E8.m1.1.1.1.1.3.4.3.2" xref="S4.E8.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mrow id="S4.E8.m1.1.1.1.1.3.4.3.3" xref="S4.E8.m1.1.1.1.1.3.4.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.4.3.3.2" xref="S4.E8.m1.1.1.1.1.3.4.3.3.2.cmml">c</mi><mo id="S4.E8.m1.1.1.1.1.3.4.3.3.1" xref="S4.E8.m1.1.1.1.1.3.4.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.4.3.3.3" xref="S4.E8.m1.1.1.1.1.3.4.3.3.3.cmml">o</mi><mo id="S4.E8.m1.1.1.1.1.3.4.3.3.1a" xref="S4.E8.m1.1.1.1.1.3.4.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.4.3.3.4" xref="S4.E8.m1.1.1.1.1.3.4.3.3.4.cmml">s</mi></mrow></msub></mrow><mo id="S4.E8.m1.1.1.1.1.3.1b" xref="S4.E8.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E8.m1.1.1.1.1.3.5" xref="S4.E8.m1.1.1.1.1.3.5.cmml"><msub id="S4.E8.m1.1.1.1.1.3.5.2" xref="S4.E8.m1.1.1.1.1.3.5.2.cmml"><mi id="S4.E8.m1.1.1.1.1.3.5.2.2" xref="S4.E8.m1.1.1.1.1.3.5.2.2.cmml">λ</mi><mrow id="S4.E8.m1.1.1.1.1.3.5.2.3" xref="S4.E8.m1.1.1.1.1.3.5.2.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.5.2.3.2" xref="S4.E8.m1.1.1.1.1.3.5.2.3.2.cmml">c</mi><mo id="S4.E8.m1.1.1.1.1.3.5.2.3.1" xref="S4.E8.m1.1.1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.2.3.3" xref="S4.E8.m1.1.1.1.1.3.5.2.3.3.cmml">l</mi><mo id="S4.E8.m1.1.1.1.1.3.5.2.3.1a" xref="S4.E8.m1.1.1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.2.3.4" xref="S4.E8.m1.1.1.1.1.3.5.2.3.4.cmml">i</mi><mo id="S4.E8.m1.1.1.1.1.3.5.2.3.1b" xref="S4.E8.m1.1.1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.2.3.5" xref="S4.E8.m1.1.1.1.1.3.5.2.3.5.cmml">p</mi></mrow></msub><mo id="S4.E8.m1.1.1.1.1.3.5.1" xref="S4.E8.m1.1.1.1.1.3.5.1.cmml">⁢</mo><msub id="S4.E8.m1.1.1.1.1.3.5.3" xref="S4.E8.m1.1.1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E8.m1.1.1.1.1.3.5.3.2" xref="S4.E8.m1.1.1.1.1.3.5.3.2.cmml">ℒ</mi><mrow id="S4.E8.m1.1.1.1.1.3.5.3.3" xref="S4.E8.m1.1.1.1.1.3.5.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.5.3.3.2" xref="S4.E8.m1.1.1.1.1.3.5.3.3.2.cmml">c</mi><mo id="S4.E8.m1.1.1.1.1.3.5.3.3.1" xref="S4.E8.m1.1.1.1.1.3.5.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.3.3.3" xref="S4.E8.m1.1.1.1.1.3.5.3.3.3.cmml">l</mi><mo id="S4.E8.m1.1.1.1.1.3.5.3.3.1a" xref="S4.E8.m1.1.1.1.1.3.5.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.3.3.4" xref="S4.E8.m1.1.1.1.1.3.5.3.3.4.cmml">i</mi><mo id="S4.E8.m1.1.1.1.1.3.5.3.3.1b" xref="S4.E8.m1.1.1.1.1.3.5.3.3.1.cmml">⁢</mo><mi id="S4.E8.m1.1.1.1.1.3.5.3.3.5" xref="S4.E8.m1.1.1.1.1.3.5.3.3.5.cmml">p</mi></mrow></msub></mrow></mrow></mrow><mo id="S4.E8.m1.1.1.1.2" lspace="0em" xref="S4.E8.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.1b"><apply id="S4.E8.m1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1"><eq id="S4.E8.m1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1"></eq><apply id="S4.E8.m1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.2.1.cmml" xref="S4.E8.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.2.2.cmml" xref="S4.E8.m1.1.1.1.1.2.2">ℒ</ci><apply id="S4.E8.m1.1.1.1.1.2.3.cmml" xref="S4.E8.m1.1.1.1.1.2.3"><times id="S4.E8.m1.1.1.1.1.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.2.3.1"></times><ci id="S4.E8.m1.1.1.1.1.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.2.3.2">𝐼</ci><cn id="S4.E8.m1.1.1.1.1.2.3.3.cmml" type="integer" xref="S4.E8.m1.1.1.1.1.2.3.3">2</cn><ci id="S4.E8.m1.1.1.1.1.2.3.4.cmml" xref="S4.E8.m1.1.1.1.1.2.3.4">𝑆</ci><ci id="S4.E8.m1.1.1.1.1.2.3.5.cmml" xref="S4.E8.m1.1.1.1.1.2.3.5">𝑀</ci></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.3"><plus id="S4.E8.m1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.1"></plus><apply id="S4.E8.m1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2"><times id="S4.E8.m1.1.1.1.1.3.2.1.cmml" xref="S4.E8.m1.1.1.1.1.3.2.1"></times><apply id="S4.E8.m1.1.1.1.1.3.2.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.2">𝜆</ci><apply id="S4.E8.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3"><times id="S4.E8.m1.1.1.1.1.3.2.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.2.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3.2">𝑔</ci><ci id="S4.E8.m1.1.1.1.1.3.2.2.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3.3">𝑟</ci><ci id="S4.E8.m1.1.1.1.1.3.2.2.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3.4">𝑎</ci><ci id="S4.E8.m1.1.1.1.1.3.2.2.3.5.cmml" xref="S4.E8.m1.1.1.1.1.3.2.2.3.5">𝑚</ci></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.2.3.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.2">ℒ</ci><apply id="S4.E8.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3"><times id="S4.E8.m1.1.1.1.1.3.2.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.2.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3.2">𝑔</ci><ci id="S4.E8.m1.1.1.1.1.3.2.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3.3">𝑟</ci><ci id="S4.E8.m1.1.1.1.1.3.2.3.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3.4">𝑎</ci><ci id="S4.E8.m1.1.1.1.1.3.2.3.3.5.cmml" xref="S4.E8.m1.1.1.1.1.3.2.3.3.5">𝑚</ci></apply></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.3"><times id="S4.E8.m1.1.1.1.1.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.3.1"></times><apply id="S4.E8.m1.1.1.1.1.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E8.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E8.m1.1.1.1.1.3.3.2.2">𝜆</ci><ci id="S4.E8.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.E8.m1.1.1.1.1.3.3.2.3">𝑔</ci></apply><apply id="S4.E8.m1.1.1.1.1.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S4.E8.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.3.3.3">𝑔</ci></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.4"><times id="S4.E8.m1.1.1.1.1.3.4.1.cmml" xref="S4.E8.m1.1.1.1.1.3.4.1"></times><apply id="S4.E8.m1.1.1.1.1.3.4.2.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.4.2.1.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.4.2.2.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.2">𝜆</ci><apply id="S4.E8.m1.1.1.1.1.3.4.2.3.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.3"><times id="S4.E8.m1.1.1.1.1.3.4.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.4.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.3.2">𝑐</ci><ci id="S4.E8.m1.1.1.1.1.3.4.2.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.3.3">𝑜</ci><ci id="S4.E8.m1.1.1.1.1.3.4.2.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.4.2.3.4">𝑠</ci></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.4.3.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.4.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.4.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.2">ℒ</ci><apply id="S4.E8.m1.1.1.1.1.3.4.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.3"><times id="S4.E8.m1.1.1.1.1.3.4.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.4.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.3.2">𝑐</ci><ci id="S4.E8.m1.1.1.1.1.3.4.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.3.3">𝑜</ci><ci id="S4.E8.m1.1.1.1.1.3.4.3.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.4.3.3.4">𝑠</ci></apply></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.5.cmml" xref="S4.E8.m1.1.1.1.1.3.5"><times id="S4.E8.m1.1.1.1.1.3.5.1.cmml" xref="S4.E8.m1.1.1.1.1.3.5.1"></times><apply id="S4.E8.m1.1.1.1.1.3.5.2.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.5.2.1.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.5.2.2.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.2">𝜆</ci><apply id="S4.E8.m1.1.1.1.1.3.5.2.3.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3"><times id="S4.E8.m1.1.1.1.1.3.5.2.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.5.2.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3.2">𝑐</ci><ci id="S4.E8.m1.1.1.1.1.3.5.2.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3.3">𝑙</ci><ci id="S4.E8.m1.1.1.1.1.3.5.2.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3.4">𝑖</ci><ci id="S4.E8.m1.1.1.1.1.3.5.2.3.5.cmml" xref="S4.E8.m1.1.1.1.1.3.5.2.3.5">𝑝</ci></apply></apply><apply id="S4.E8.m1.1.1.1.1.3.5.3.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.5.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.5.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.2">ℒ</ci><apply id="S4.E8.m1.1.1.1.1.3.5.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3"><times id="S4.E8.m1.1.1.1.1.3.5.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3.1"></times><ci id="S4.E8.m1.1.1.1.1.3.5.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3.2">𝑐</ci><ci id="S4.E8.m1.1.1.1.1.3.5.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3.3">𝑙</ci><ci id="S4.E8.m1.1.1.1.1.3.5.3.3.4.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3.4">𝑖</ci><ci id="S4.E8.m1.1.1.1.1.3.5.3.3.5.cmml" xref="S4.E8.m1.1.1.1.1.3.5.3.3.5">𝑝</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.1c">\mathcal{L}_{I2SM}=\lambda_{gram}\mathcal{L}_{gram}+\lambda_{g}\mathcal{L}_{g}%
+\lambda_{cos}\mathcal{L}_{cos}+\lambda_{clip}\mathcal{L}_{clip}.</annotation><annotation encoding="application/x-llamapun" id="S4.E8.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_I 2 italic_S italic_M end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT italic_g italic_r italic_a italic_m end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g italic_r italic_a italic_m end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_c italic_o italic_s end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_o italic_s end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_i italic_p end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Cloud Image-based Local Personalized Sketch Recommendation Module</h3>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F7.g1" src="x7.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.4.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S4.F7.5.2" style="font-size:90%;">The illustration of the Cloud Image-based Local Personalized Sketch Recommendation Module. We first acquire the inspiration image and personalized sketch material library through the <span class="ltx_text ltx_font_italic" id="S4.F7.5.2.1">Text-to-Image Cloud Module</span> and <span class="ltx_text ltx_font_italic" id="S4.F7.5.2.2">Image-to-Sketch Local Module</span>. Subsequently, we leverage the ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib8" title="">2020</a>)</cite> to assess their similarity and output recommendations based on the resulting rankings.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this module, our goal is to identify sketches within the local sketch material library that closely match the input cloud image, thereby further improving the existing materials and finally realizing sketch-based image style transfer for coloring. Illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F7" title="Figure 7 ‣ 4.3. Cloud Image-based Local Personalized Sketch Recommendation Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">7</span></a>, we present the workflow of the personalized sketch recommendation system. It involves obtaining a substantial collection of personalized sketch material libraries through a locally pre-trained sketch-to-image generation model and image material library (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2" title="4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.2</span></a>). We generate numerous images of the outfit worn in various scenes through a cloud-based pre-trained text-to-image generation model and the input reference text prompt (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1" title="4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.1</span></a>). Then, we retrieve the corresponding clothing image and recommend the most similar sketch materials from the personalized sketch material library based on the clothing image. Designers can further refine the recommended sketch materials and input them into our Style Transfer Module (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4" title="4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Sketch Recommendation Module</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.6">The task of image-based sketch recommendation can be categorized as cross-modal data retrieval since sketches and images represent the same objects but exhibit distinct modal characteristics. Numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib8" title="">2020</a>; Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib13" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib32" title="">2021a</a>)</cite> have demonstrated the superior feature extraction capabilities of Vision Transformer (ViT) models, especially concerning global feature extraction from image data when compared to Convolutional Neural Networks (CNN) models. In the context of the image-based sketch recommendation task, there is a need to distinguish between various clothing styles, such as shirts, coats, skirts, etc., followed by capturing fine-grained details within the sketches. For this reason, we opted for the standard ViT-B/16 model <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib8" title="">2020</a>)</cite> as the feature extraction model for our sketch recommendation module. The workflow begins with the acquisition of extensive personalized sketch materials and reference clothing data, which are obtained through the Image-to-Sketch Generation Module and Text-to-Image Cloud Module. These data are then input into the pre-trained Vision Transformer (ViT) model, followed by a connected linear layer for feature embedding of both sketches and the reference image. The central part of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F7" title="Figure 7 ‣ 4.3. Cloud Image-based Local Personalized Sketch Recommendation Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the distribution of embedded sketches and the reference image within the embedding space. Based on the relative distances in this embedding space, we recommend a Top-K ranking of sketches within the personalized sketch material library. Designers can then use this ranking to select suitable materials for further refinements based on the recommended sketches. Specifically, we adopt cosine similarity to measure the similarity between the input image and the sketch template, which is expressed mathematically as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Cosine Similarity}(A,B)=\frac{A\cdot B}{\|A\|\cdot\|B\|}," class="ltx_Math" display="block" id="S4.E9.m1.5"><semantics id="S4.E9.m1.5a"><mrow id="S4.E9.m1.5.5.1" xref="S4.E9.m1.5.5.1.1.cmml"><mrow id="S4.E9.m1.5.5.1.1" xref="S4.E9.m1.5.5.1.1.cmml"><mrow id="S4.E9.m1.5.5.1.1.2" xref="S4.E9.m1.5.5.1.1.2.cmml"><mtext id="S4.E9.m1.5.5.1.1.2.2" xref="S4.E9.m1.5.5.1.1.2.2a.cmml">Cosine Similarity</mtext><mo id="S4.E9.m1.5.5.1.1.2.1" xref="S4.E9.m1.5.5.1.1.2.1.cmml">⁢</mo><mrow id="S4.E9.m1.5.5.1.1.2.3.2" xref="S4.E9.m1.5.5.1.1.2.3.1.cmml"><mo id="S4.E9.m1.5.5.1.1.2.3.2.1" stretchy="false" xref="S4.E9.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S4.E9.m1.3.3" xref="S4.E9.m1.3.3.cmml">A</mi><mo id="S4.E9.m1.5.5.1.1.2.3.2.2" xref="S4.E9.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S4.E9.m1.4.4" xref="S4.E9.m1.4.4.cmml">B</mi><mo id="S4.E9.m1.5.5.1.1.2.3.2.3" stretchy="false" xref="S4.E9.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E9.m1.5.5.1.1.1" xref="S4.E9.m1.5.5.1.1.1.cmml">=</mo><mfrac id="S4.E9.m1.2.2" xref="S4.E9.m1.2.2.cmml"><mrow id="S4.E9.m1.2.2.4" xref="S4.E9.m1.2.2.4.cmml"><mi id="S4.E9.m1.2.2.4.2" xref="S4.E9.m1.2.2.4.2.cmml">A</mi><mo id="S4.E9.m1.2.2.4.1" lspace="0.222em" rspace="0.222em" xref="S4.E9.m1.2.2.4.1.cmml">⋅</mo><mi id="S4.E9.m1.2.2.4.3" xref="S4.E9.m1.2.2.4.3.cmml">B</mi></mrow><mrow id="S4.E9.m1.2.2.2" xref="S4.E9.m1.2.2.2.cmml"><mrow id="S4.E9.m1.2.2.2.4.2" xref="S4.E9.m1.2.2.2.4.1.cmml"><mo id="S4.E9.m1.2.2.2.4.2.1" stretchy="false" xref="S4.E9.m1.2.2.2.4.1.1.cmml">‖</mo><mi id="S4.E9.m1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml">A</mi><mo id="S4.E9.m1.2.2.2.4.2.2" rspace="0.055em" stretchy="false" xref="S4.E9.m1.2.2.2.4.1.1.cmml">‖</mo></mrow><mo id="S4.E9.m1.2.2.2.3" rspace="0.222em" xref="S4.E9.m1.2.2.2.3.cmml">⋅</mo><mrow id="S4.E9.m1.2.2.2.5.2" xref="S4.E9.m1.2.2.2.5.1.cmml"><mo id="S4.E9.m1.2.2.2.5.2.1" stretchy="false" xref="S4.E9.m1.2.2.2.5.1.1.cmml">‖</mo><mi id="S4.E9.m1.2.2.2.2" xref="S4.E9.m1.2.2.2.2.cmml">B</mi><mo id="S4.E9.m1.2.2.2.5.2.2" stretchy="false" xref="S4.E9.m1.2.2.2.5.1.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><mo id="S4.E9.m1.5.5.1.2" xref="S4.E9.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.5b"><apply id="S4.E9.m1.5.5.1.1.cmml" xref="S4.E9.m1.5.5.1"><eq id="S4.E9.m1.5.5.1.1.1.cmml" xref="S4.E9.m1.5.5.1.1.1"></eq><apply id="S4.E9.m1.5.5.1.1.2.cmml" xref="S4.E9.m1.5.5.1.1.2"><times id="S4.E9.m1.5.5.1.1.2.1.cmml" xref="S4.E9.m1.5.5.1.1.2.1"></times><ci id="S4.E9.m1.5.5.1.1.2.2a.cmml" xref="S4.E9.m1.5.5.1.1.2.2"><mtext id="S4.E9.m1.5.5.1.1.2.2.cmml" xref="S4.E9.m1.5.5.1.1.2.2">Cosine Similarity</mtext></ci><interval closure="open" id="S4.E9.m1.5.5.1.1.2.3.1.cmml" xref="S4.E9.m1.5.5.1.1.2.3.2"><ci id="S4.E9.m1.3.3.cmml" xref="S4.E9.m1.3.3">𝐴</ci><ci id="S4.E9.m1.4.4.cmml" xref="S4.E9.m1.4.4">𝐵</ci></interval></apply><apply id="S4.E9.m1.2.2.cmml" xref="S4.E9.m1.2.2"><divide id="S4.E9.m1.2.2.3.cmml" xref="S4.E9.m1.2.2"></divide><apply id="S4.E9.m1.2.2.4.cmml" xref="S4.E9.m1.2.2.4"><ci id="S4.E9.m1.2.2.4.1.cmml" xref="S4.E9.m1.2.2.4.1">⋅</ci><ci id="S4.E9.m1.2.2.4.2.cmml" xref="S4.E9.m1.2.2.4.2">𝐴</ci><ci id="S4.E9.m1.2.2.4.3.cmml" xref="S4.E9.m1.2.2.4.3">𝐵</ci></apply><apply id="S4.E9.m1.2.2.2.cmml" xref="S4.E9.m1.2.2.2"><ci id="S4.E9.m1.2.2.2.3.cmml" xref="S4.E9.m1.2.2.2.3">⋅</ci><apply id="S4.E9.m1.2.2.2.4.1.cmml" xref="S4.E9.m1.2.2.2.4.2"><csymbol cd="latexml" id="S4.E9.m1.2.2.2.4.1.1.cmml" xref="S4.E9.m1.2.2.2.4.2.1">norm</csymbol><ci id="S4.E9.m1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1">𝐴</ci></apply><apply id="S4.E9.m1.2.2.2.5.1.cmml" xref="S4.E9.m1.2.2.2.5.2"><csymbol cd="latexml" id="S4.E9.m1.2.2.2.5.1.1.cmml" xref="S4.E9.m1.2.2.2.5.2.1">norm</csymbol><ci id="S4.E9.m1.2.2.2.2.cmml" xref="S4.E9.m1.2.2.2.2">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.5c">\text{Cosine Similarity}(A,B)=\frac{A\cdot B}{\|A\|\cdot\|B\|},</annotation><annotation encoding="application/x-llamapun" id="S4.E9.m1.5d">Cosine Similarity ( italic_A , italic_B ) = divide start_ARG italic_A ⋅ italic_B end_ARG start_ARG ∥ italic_A ∥ ⋅ ∥ italic_B ∥ end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS1.p1.5">where <math alttext="A" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mi id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><ci id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.m2.1"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mi id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><ci id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.m2.1d">italic_B</annotation></semantics></math> represent the feature vectors of the sketches and the reference image, <math alttext="\cdot" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.3.m3.1"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><mo id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><ci id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.3.m3.1d">⋅</annotation></semantics></math> represents the inner product between the vectors, and <math alttext="\|A\|" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.4.m4.1"><semantics id="S4.SS3.SSS1.p1.4.m4.1a"><mrow id="S4.SS3.SSS1.p1.4.m4.1.2.2" xref="S4.SS3.SSS1.p1.4.m4.1.2.1.cmml"><mo id="S4.SS3.SSS1.p1.4.m4.1.2.2.1" stretchy="false" xref="S4.SS3.SSS1.p1.4.m4.1.2.1.1.cmml">‖</mo><mi id="S4.SS3.SSS1.p1.4.m4.1.1" xref="S4.SS3.SSS1.p1.4.m4.1.1.cmml">A</mi><mo id="S4.SS3.SSS1.p1.4.m4.1.2.2.2" stretchy="false" xref="S4.SS3.SSS1.p1.4.m4.1.2.1.1.cmml">‖</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.4.m4.1b"><apply id="S4.SS3.SSS1.p1.4.m4.1.2.1.cmml" xref="S4.SS3.SSS1.p1.4.m4.1.2.2"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.4.m4.1.2.1.1.cmml" xref="S4.SS3.SSS1.p1.4.m4.1.2.2.1">norm</csymbol><ci id="S4.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p1.4.m4.1.1">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.4.m4.1c">\|A\|</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.4.m4.1d">∥ italic_A ∥</annotation></semantics></math> and <math alttext="\|B\|" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.5.m5.1"><semantics id="S4.SS3.SSS1.p1.5.m5.1a"><mrow id="S4.SS3.SSS1.p1.5.m5.1.2.2" xref="S4.SS3.SSS1.p1.5.m5.1.2.1.cmml"><mo id="S4.SS3.SSS1.p1.5.m5.1.2.2.1" stretchy="false" xref="S4.SS3.SSS1.p1.5.m5.1.2.1.1.cmml">‖</mo><mi id="S4.SS3.SSS1.p1.5.m5.1.1" xref="S4.SS3.SSS1.p1.5.m5.1.1.cmml">B</mi><mo id="S4.SS3.SSS1.p1.5.m5.1.2.2.2" stretchy="false" xref="S4.SS3.SSS1.p1.5.m5.1.2.1.1.cmml">‖</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.5.m5.1b"><apply id="S4.SS3.SSS1.p1.5.m5.1.2.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.2.2"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.5.m5.1.2.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.2.2.1">norm</csymbol><ci id="S4.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.5.m5.1c">\|B\|</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.5.m5.1d">∥ italic_B ∥</annotation></semantics></math> represent their Euclidean norms.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Style Transfer Module based on Local Personalized Sketch and Cloud Image</h3>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S4.F8.g1" src="x8.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">The illustration of the Style Transfer Module based on Local Personalized Sketch and Cloud Image. We incorporate the sketch and reference image into the DDIM <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib49" title="">2020</a>)</cite> diffusion process through both explicit and implicit ways to supervise model generation. This integration, coupled with our proposed CCAM, enhances the overall quality of the generated image.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">At this stage, our proposed Style Transfer Module (STM) merges the sketch obtained locally refined with the image acquired from the cloud model, generating a customized sketch image imbued with the stylistic elements of the cloud image, as present in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F8" title="Figure 8 ‣ 4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">8</span></a>. This approach empowers designers to quickly apply personalized color to their sketches, facilitating the selection of the most fitting color style for their work and thereby enhancing designer efficiency. The use of a private, locally located sketch ensures that the designer’s creative ideas remain secure, effectively safeguarding their privacy. Our STM is built around the core component of the Denoising Diffusion Implicit Model (DDIM) <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib49" title="">2020</a>)</cite>, with the generation of DDIM guided by the addition of sketches and images as content and style conditions. Furthermore, we enhance the conditions for the diffusion process through our proposed <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">Channel Cross Attention Module</span> (CCAM), ensuring the production of high-quality, realistic images.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Denoising Diffusion Implicit Model</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.7">Differing from traditional Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib11" title="">2014</a>)</cite>, the Denoising Diffusion Probability Model (DDPM) <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib16" title="">2020</a>)</cite> transforms the image generation problem as a process of noise adding and denoising. It generates realistic images by continually denoising pairs of random Gaussian noise. In the forward noise-adding process, at time <math alttext="t" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.1.m1.1"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><mi id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><ci id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.1.m1.1d">italic_t</annotation></semantics></math> <math alttext="(1\leq t\leq T)" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.2.m2.1"><semantics id="S4.SS4.SSS1.p1.2.m2.1a"><mrow id="S4.SS4.SSS1.p1.2.m2.1.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.cmml"><mo id="S4.SS4.SSS1.p1.2.m2.1.1.1.2" stretchy="false" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.SSS1.p1.2.m2.1.1.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.cmml"><mn id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.2" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.2.cmml">1</mn><mo id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.3" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.3.cmml">≤</mo><mi id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.4" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.4.cmml">t</mi><mo id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.5" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.5.cmml">≤</mo><mi id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.6" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.6.cmml">T</mi></mrow><mo id="S4.SS4.SSS1.p1.2.m2.1.1.1.3" stretchy="false" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.2.m2.1b"><apply id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"><and id="S4.SS4.SSS1.p1.2.m2.1.1.1.1a.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"></and><apply id="S4.SS4.SSS1.p1.2.m2.1.1.1.1b.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"><leq id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.3.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.3"></leq><cn id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.2.cmml" type="integer" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.2">1</cn><ci id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.4.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.4">𝑡</ci></apply><apply id="S4.SS4.SSS1.p1.2.m2.1.1.1.1c.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"><leq id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.5.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.5"></leq><share href="https://arxiv.org/html/2408.00855v3#S4.SS4.SSS1.p1.2.m2.1.1.1.1.4.cmml" id="S4.SS4.SSS1.p1.2.m2.1.1.1.1d.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"></share><ci id="S4.SS4.SSS1.p1.2.m2.1.1.1.1.6.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.1.6">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.2.m2.1c">(1\leq t\leq T)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.2.m2.1d">( 1 ≤ italic_t ≤ italic_T )</annotation></semantics></math>, the image <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.3.m3.1"><semantics id="S4.SS4.SSS1.p1.3.m3.1a"><msub id="S4.SS4.SSS1.p1.3.m3.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS4.SSS1.p1.3.m3.1.1.2" xref="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS1.p1.3.m3.1.1.3" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.3.m3.1b"><apply id="S4.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.2">𝑥</ci><ci id="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.3.m3.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is defined as <math alttext="x_{t}=\alpha_{t}x_{0}+\sqrt{1-\alpha_{t}^{2}}\epsilon_{t}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.4.m4.1"><semantics id="S4.SS4.SSS1.p1.4.m4.1a"><mrow id="S4.SS4.SSS1.p1.4.m4.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.cmml"><msub id="S4.SS4.SSS1.p1.4.m4.1.1.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.2.cmml">x</mi><mi id="S4.SS4.SSS1.p1.4.m4.1.1.2.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.3.cmml">t</mi></msub><mo id="S4.SS4.SSS1.p1.4.m4.1.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS4.SSS1.p1.4.m4.1.1.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.cmml"><mrow id="S4.SS4.SSS1.p1.4.m4.1.1.3.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.cmml"><msub id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.2.cmml">α</mi><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.3.cmml">t</mi></msub><mo id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.1.cmml">⁢</mo><msub id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.2.cmml">x</mi><mn id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.3.cmml">0</mn></msub></mrow><mo id="S4.SS4.SSS1.p1.4.m4.1.1.3.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.1.cmml">+</mo><mrow id="S4.SS4.SSS1.p1.4.m4.1.1.3.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.cmml"><msqrt id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.cmml"><mrow id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.cmml"><mn id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.2.cmml">1</mn><mo id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.1.cmml">−</mo><msubsup id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.2.cmml">α</mi><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.3.cmml">t</mi><mn id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.3.cmml">2</mn></msubsup></mrow></msqrt><mo id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.1.cmml">⁢</mo><msub id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.2.cmml">ϵ</mi><mi id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.4.m4.1b"><apply id="S4.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1"><eq id="S4.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.1"></eq><apply id="S4.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS4.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.2">𝑥</ci><ci id="S4.SS4.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.3">𝑡</ci></apply><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3"><plus id="S4.SS4.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.1"></plus><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2"><times id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.1"></times><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2">subscript</csymbol><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.2">𝛼</ci><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.2.3">𝑡</ci></apply><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3">subscript</csymbol><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.2">𝑥</ci><cn id="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.3.cmml" type="integer" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.2.3.3">0</cn></apply></apply><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3"><times id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.1"></times><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2"><root id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2a.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2"></root><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2"><minus id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.1"></minus><cn id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.2.cmml" type="integer" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.2">1</cn><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3">superscript</csymbol><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3">subscript</csymbol><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.2">𝛼</ci><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.2.3">𝑡</ci></apply><cn id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.3.cmml" type="integer" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.2.2.3.3">2</cn></apply></apply></apply><apply id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.2">italic-ϵ</ci><ci id="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.4.m4.1c">x_{t}=\alpha_{t}x_{0}+\sqrt{1-\alpha_{t}^{2}}\epsilon_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_ϵ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\alpha_{t}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.5.m5.1"><semantics id="S4.SS4.SSS1.p1.5.m5.1a"><msub id="S4.SS4.SSS1.p1.5.m5.1.1" xref="S4.SS4.SSS1.p1.5.m5.1.1.cmml"><mi id="S4.SS4.SSS1.p1.5.m5.1.1.2" xref="S4.SS4.SSS1.p1.5.m5.1.1.2.cmml">α</mi><mi id="S4.SS4.SSS1.p1.5.m5.1.1.3" xref="S4.SS4.SSS1.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.5.m5.1b"><apply id="S4.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1.2">𝛼</ci><ci id="S4.SS4.SSS1.p1.5.m5.1.1.3.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.5.m5.1c">\alpha_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.5.m5.1d">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is a gradually decreasing hyperparameter ensuring that <math alttext="x_{T}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.6.m6.1"><semantics id="S4.SS4.SSS1.p1.6.m6.1a"><msub id="S4.SS4.SSS1.p1.6.m6.1.1" xref="S4.SS4.SSS1.p1.6.m6.1.1.cmml"><mi id="S4.SS4.SSS1.p1.6.m6.1.1.2" xref="S4.SS4.SSS1.p1.6.m6.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS1.p1.6.m6.1.1.3" xref="S4.SS4.SSS1.p1.6.m6.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.6.m6.1b"><apply id="S4.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS4.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.6.m6.1.1.1.cmml" xref="S4.SS4.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.6.m6.1.1.2.cmml" xref="S4.SS4.SSS1.p1.6.m6.1.1.2">𝑥</ci><ci id="S4.SS4.SSS1.p1.6.m6.1.1.3.cmml" xref="S4.SS4.SSS1.p1.6.m6.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.6.m6.1c">x_{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.6.m6.1d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> approximately equal to Gaussian distribution as <math alttext="T" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.7.m7.1"><semantics id="S4.SS4.SSS1.p1.7.m7.1a"><mi id="S4.SS4.SSS1.p1.7.m7.1.1" xref="S4.SS4.SSS1.p1.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.7.m7.1b"><ci id="S4.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS4.SSS1.p1.7.m7.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.7.m7.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.7.m7.1d">italic_T</annotation></semantics></math> becomes sufficiently large. During the model inference process, noise added during the forward process is predicted and gradually removed to obtain the generated image. In our method, we adopt DDIM <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib49" title="">2020</a>)</cite> as our sampling method because it overcomes the longer generation times issue associated with the Markov chain in the DDPM reverse denoising process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.5">Taking inspiration from DiffusionRig <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib7" title="">2023</a>)</cite>, we introduce sketches and images as conditional supervision to guide the model’s generation process. This conditional supervision is divided into explicit and implicit conditions. For explicit conditions, we directly concatenate the sketch <math alttext="\boldsymbol{c}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.1.m1.1"><semantics id="S4.SS4.SSS1.p2.1.m1.1a"><mi id="S4.SS4.SSS1.p2.1.m1.1.1" xref="S4.SS4.SSS1.p2.1.m1.1.1.cmml">𝒄</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.1.m1.1b"><ci id="S4.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1">𝒄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.1.m1.1c">\boldsymbol{c}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.1.m1.1d">bold_italic_c</annotation></semantics></math> and noise image <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.2.m2.1"><semantics id="S4.SS4.SSS1.p2.2.m2.1a"><msub id="S4.SS4.SSS1.p2.2.m2.1.1" xref="S4.SS4.SSS1.p2.2.m2.1.1.cmml"><mi id="S4.SS4.SSS1.p2.2.m2.1.1.2" xref="S4.SS4.SSS1.p2.2.m2.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS1.p2.2.m2.1.1.3" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.2.m2.1b"><apply id="S4.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.2">𝑥</ci><ci id="S4.SS4.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.2.m2.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> as the input to the U-Net <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib42" title="">2015b</a>)</cite> denoising model at time <math alttext="t" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.3.m3.1"><semantics id="S4.SS4.SSS1.p2.3.m3.1a"><mi id="S4.SS4.SSS1.p2.3.m3.1.1" xref="S4.SS4.SSS1.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.3.m3.1b"><ci id="S4.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.3.m3.1d">italic_t</annotation></semantics></math>. This ensures that the final generated image seamlessly fits the input sketch outline and preserves as much sketch detail as possible. For implicit conditions, we encode the style of the reference image <math alttext="\boldsymbol{s}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.4.m4.1"><semantics id="S4.SS4.SSS1.p2.4.m4.1a"><mi id="S4.SS4.SSS1.p2.4.m4.1.1" xref="S4.SS4.SSS1.p2.4.m4.1.1.cmml">𝒔</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.4.m4.1b"><ci id="S4.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p2.4.m4.1.1">𝒔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.4.m4.1c">\boldsymbol{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.4.m4.1d">bold_italic_s</annotation></semantics></math> and the time <math alttext="t" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.5.m5.1"><semantics id="S4.SS4.SSS1.p2.5.m5.1a"><mi id="S4.SS4.SSS1.p2.5.m5.1.1" xref="S4.SS4.SSS1.p2.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.5.m5.1b"><ci id="S4.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS4.SSS1.p2.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.5.m5.1d">italic_t</annotation></semantics></math> of the current diffusion step into the model’s denoising process using the cross-attention mechanism inspired by CrossViT <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib4" title="">2021</a>)</cite>. This ensures that the generated image possesses the same style characteristics as the reference image, and leverages time encoding to enhance the model’s ability to predict noise during each denoising pass. For encoding the style of the reference image, we use features obtained through ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib14" title="">2016</a>)</cite> with a linear layer, and for encoding the diffusion time, features are acquired through two linear layers. Consequently, the optimization objective of our model can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(10)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{STM}=\mathbb{E}_{\boldsymbol{c},\boldsymbol{s},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}((%
\boldsymbol{x}_{t},\boldsymbol{c}),\boldsymbol{s},\boldsymbol{t}))\|_{2}^{2}%
\bigg{]}," class="ltx_math_unparsed" display="block" id="S4.E10.m1.10"><semantics id="S4.E10.m1.10a"><mrow id="S4.E10.m1.10b"><msub id="S4.E10.m1.10.11"><mi class="ltx_font_mathcaligraphic" id="S4.E10.m1.10.11.2">ℒ</mi><mrow id="S4.E10.m1.10.11.3"><mi id="S4.E10.m1.10.11.3.2">S</mi><mo id="S4.E10.m1.10.11.3.1">⁢</mo><mi id="S4.E10.m1.10.11.3.3">T</mi><mo id="S4.E10.m1.10.11.3.1a">⁢</mo><mi id="S4.E10.m1.10.11.3.4">M</mi></mrow></msub><mo id="S4.E10.m1.10.12">=</mo><msub id="S4.E10.m1.10.13"><mi id="S4.E10.m1.10.13.2">𝔼</mi><mrow id="S4.E10.m1.7.7.7.7"><mrow id="S4.E10.m1.7.7.7.7.1"><mrow id="S4.E10.m1.7.7.7.7.1.2.2"><mi id="S4.E10.m1.3.3.3.3">𝒄</mi><mo id="S4.E10.m1.7.7.7.7.1.2.2.1">,</mo><mi id="S4.E10.m1.4.4.4.4">𝒔</mi><mo id="S4.E10.m1.7.7.7.7.1.2.2.2">,</mo><mi id="S4.E10.m1.5.5.5.5">ϵ</mi></mrow><mo id="S4.E10.m1.7.7.7.7.1.1">∼</mo><mrow id="S4.E10.m1.7.7.7.7.1.3"><mi class="ltx_font_mathcaligraphic" id="S4.E10.m1.7.7.7.7.1.3.2">𝒩</mi><mo id="S4.E10.m1.7.7.7.7.1.3.1">⁢</mo><mrow id="S4.E10.m1.7.7.7.7.1.3.3.2"><mo id="S4.E10.m1.7.7.7.7.1.3.3.2.1" stretchy="false">(</mo><mn id="S4.E10.m1.1.1.1.1">0</mn><mo id="S4.E10.m1.7.7.7.7.1.3.3.2.2">,</mo><mn id="S4.E10.m1.2.2.2.2">1</mn><mo id="S4.E10.m1.7.7.7.7.1.3.3.2.3" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S4.E10.m1.7.7.7.7.2">,</mo><mi id="S4.E10.m1.6.6.6.6">𝒕</mi></mrow></msub><mrow id="S4.E10.m1.10.14"><mo id="S4.E10.m1.10.14.1" maxsize="210%" minsize="210%">[</mo><mo id="S4.E10.m1.10.14.2" lspace="0em" rspace="0.167em">∥</mo><mi id="S4.E10.m1.10.14.3">ϵ</mi><mo id="S4.E10.m1.10.14.4">−</mo><msub id="S4.E10.m1.10.14.5"><mi id="S4.E10.m1.10.14.5.2">ϵ</mi><mi id="S4.E10.m1.10.14.5.3">θ</mi></msub><mrow id="S4.E10.m1.10.14.6"><mo id="S4.E10.m1.10.14.6.1" stretchy="false">(</mo><mrow id="S4.E10.m1.10.14.6.2"><mo id="S4.E10.m1.10.14.6.2.1" stretchy="false">(</mo><msub id="S4.E10.m1.10.14.6.2.2"><mi id="S4.E10.m1.10.14.6.2.2.2">𝒙</mi><mi id="S4.E10.m1.10.14.6.2.2.3">t</mi></msub><mo id="S4.E10.m1.10.14.6.2.3">,</mo><mi id="S4.E10.m1.8.8">𝒄</mi><mo id="S4.E10.m1.10.14.6.2.4" stretchy="false">)</mo></mrow><mo id="S4.E10.m1.10.14.6.3">,</mo><mi id="S4.E10.m1.9.9">𝒔</mi><mo id="S4.E10.m1.10.14.6.4">,</mo><mi id="S4.E10.m1.10.10">𝒕</mi><mo id="S4.E10.m1.10.14.6.5" stretchy="false">)</mo></mrow><mo id="S4.E10.m1.10.14.7" stretchy="false">)</mo></mrow><msubsup id="S4.E10.m1.10.15"><mo id="S4.E10.m1.10.15.2.2" lspace="0em" rspace="0.167em">∥</mo><mn id="S4.E10.m1.10.15.2.3">2</mn><mn id="S4.E10.m1.10.15.3">2</mn></msubsup><mo id="S4.E10.m1.10.16" maxsize="210%" minsize="210%">]</mo><mo id="S4.E10.m1.10.17">,</mo></mrow><annotation encoding="application/x-tex" id="S4.E10.m1.10c">\mathcal{L}_{STM}=\mathbb{E}_{\boldsymbol{c},\boldsymbol{s},\epsilon\sim%
\mathcal{N}(0,1),\boldsymbol{t}}\bigg{[}\|\epsilon-\epsilon_{\theta}((%
\boldsymbol{x}_{t},\boldsymbol{c}),\boldsymbol{s},\boldsymbol{t}))\|_{2}^{2}%
\bigg{]},</annotation><annotation encoding="application/x-llamapun" id="S4.E10.m1.10d">caligraphic_L start_POSTSUBSCRIPT italic_S italic_T italic_M end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_c , bold_italic_s , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , bold_italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_c ) , bold_italic_s , bold_italic_t ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.SSS1.p2.7">where <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.6.m1.1"><semantics id="S4.SS4.SSS1.p2.6.m1.1a"><msub id="S4.SS4.SSS1.p2.6.m1.1.1" xref="S4.SS4.SSS1.p2.6.m1.1.1.cmml"><mi id="S4.SS4.SSS1.p2.6.m1.1.1.2" xref="S4.SS4.SSS1.p2.6.m1.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.SSS1.p2.6.m1.1.1.3" xref="S4.SS4.SSS1.p2.6.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.6.m1.1b"><apply id="S4.SS4.SSS1.p2.6.m1.1.1.cmml" xref="S4.SS4.SSS1.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p2.6.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p2.6.m1.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p2.6.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p2.6.m1.1.1.2">italic-ϵ</ci><ci id="S4.SS4.SSS1.p2.6.m1.1.1.3.cmml" xref="S4.SS4.SSS1.p2.6.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.6.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.6.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is the denoising model to predict the noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.7.m2.1"><semantics id="S4.SS4.SSS1.p2.7.m2.1a"><mi id="S4.SS4.SSS1.p2.7.m2.1.1" xref="S4.SS4.SSS1.p2.7.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.7.m2.1b"><ci id="S4.SS4.SSS1.p2.7.m2.1.1.cmml" xref="S4.SS4.SSS1.p2.7.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.7.m2.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.7.m2.1d">italic_ϵ</annotation></semantics></math> added during the forward diffusion process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Channel Cross Attention Module</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">In the noise prediction U-Net model <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p1.1.m1.1"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><msub id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS2.p1.1.m1.1.1.2" xref="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.SSS2.p1.1.m1.1.1.3" xref="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><apply id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.2">italic-ϵ</ci><ci id="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p1.1.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we introduce the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.1">Channel Cross Attention Module</span> (CCAM) to maintain stable control over the denoising process through conditional embedding, as illustrated in the right part of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.F8" title="Figure 8 ‣ 4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">8</span></a>. Differing from previous methods <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib40" title="">2022</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib7" title="">2023</a>)</cite>, we sequentially add time embedding and style embedding to the U-Net, with our cross-attention module primarily focusing on channel information within the feature map. Specifically, we split the input conditional embedding and feature embedding into two branches. One achieves rapid feature fusion through matrix multiplication and addition, efficiently retaining more features from both inputs. The other utilizes channel cross attention, obtaining channel information maps through feature dimensionality reduction and concentration. Subsequently, we employ feature fusion of their channel information to facilitate the learning of channel information. This methodology of CCAM is applied to both time embedding and style embedding, enhancing feature fusion capabilities, and eventually yielding fused features as output. It is noteworthy that we selectively added the CCAM to certain layers to improve model training and inference efficiency, while ensuring optimal performance in model generation.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we will discuss the experimental results and ablation studies to demonstrate the contribution of different components. Additionally, we will compare our proposed system, HAIGEN, with different state-of-the-art approaches through quantitative and qualitative evaluations.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Implementation Details</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We implemented our model using PyTorch, and our experiments were conducted on the open-source clothing dataset HAIFashion <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib22" title="">2024a</a>)</cite>, which comprises 3,100 fashion clothing images. For the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">Text-to-Image Cloud Module</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS1" title="4.1. Stable Diffusion Model-based Text-to-Image Cloud Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.1</span></a>) experiments, we utilized the NVIDIA Tesla A100 40G GPUs. The optimizer used was AdamW with a learning rate of 1e-4, a batch size of 8, and the scheduler was set to Constant with Warmup. The model was trained for 100 epochs with 8-bit quantization. Local module experiments were performed on a single NVIDIA Tesla V100 32G GPU. In the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">Image-to-Sketch Local Module</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS2" title="4.2. Capture Personalized Designer-Style Image-to-Sketch Local Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we used the SGD optimizer with an initial learning rate of 2e-4 and a momentum of 0.9. The batch size was set to 8, and each training session used 100 pairs of sketch-image data from three different designers in Clothes-V1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib23" title="">2024b</a>)</cite>, totaling 50 epochs. For the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">Sketch Recommendation Module</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS3" title="4.3. Cloud Image-based Local Personalized Sketch Recommendation Module ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.3</span></a>), we employed the pre-trained ViT-B/16 model <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib8" title="">2020</a>)</cite>. In the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">Style Transfer Module</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S4.SS4" title="4.4. Style Transfer Module based on Local Personalized Sketch and Cloud Image ‣ 4. HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4.4</span></a>), we used the Adam optimizer with an initial learning rate of 1e-4 and a batch size set to 8, and the model was trained for 500,000 iterations. The dataset is partitioned into 2,500 pairs for the train and 600 pairs for the test.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Baselines</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We conducted comparative experiments using our proposed method and other baselines. We Use the same preprocessing steps and parameters provided by the original authors for each model to ensure a fair and consistent comparison.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Canny</span> <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>)</cite> generates edge maps of images through thresholding and edge pixel concatenation.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.2">pix2pix</span> <cite class="ltx_cite ltx_citemacro_citep">(Isola et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib21" title="">2017</a>)</cite> pioneered a comprehensive solution for image translation using conditional generative adversarial networks.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.3">UGATIT</span> <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib27" title="">2020</a>)</cite> combines a novel attention module and a learnable normalization function.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.4">Self-Sup</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>)</cite> introduces an unsupervised image-to-sketch generation method and proposes a multi-stage style transfer method.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.5">StyleMe</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib59" title="">2023</a>)</cite> guides the edge feature generation of the image through class activation mapping and ensures the consistency of style transfer by separating the styles in the generated images.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.6">AdaIN</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang and Belongie, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib18" title="">2017</a>)</cite> achieves the style transfer by applying instance normalization to each channel pixel.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.7">UCAST</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib63" title="">2023b</a>)</cite> learns style representations directly from massive images through contrastive learning.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.8">DiffusionRig</span> <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib7" title="">2023</a>)</cite> proposes an image editing method based on the diffusion model and 3D prior.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We employ five commonly used quantitative perceptual evaluation metrics to assess the objective quality of generated images:
<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">PSNR</span> (Peak Signal-to-Noise Ratio) measures the relationship between signal and noise in the reconstructed and original images based on the mean square error.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.2">SSIM</span> (Structural Similarity Index) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib55" title="">2004</a>)</cite> evaluates the similarity of the original image and the reconstructed image by considering their brightness, contrast, and structural information.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.3">MSE</span> (Mean Squared Error) compares the differences between images by calculating the average of the squared differences of each pixel between the original image and the reconstructed image.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.4">LPIPS</span> (Learned Perceptual Image Patch Similarity) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib62" title="">2018</a>)</cite> utilizes deep convolutional neural networks to learn perceptual differences, providing a metric that better simulates human visual perception.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.5">FID</span> (Fréchet Inception Distance) <cite class="ltx_cite ltx_citemacro_citep">(Heusel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib15" title="">2017</a>)</cite> measures the difference in feature distribution and statistics between generated samples and real samples, offering a reliable measure consistent with human subjective perception.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Performance of Text-to-Image Cloud Module</h3>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1. </span>Optimization Results</h4>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S5.F9.1.g1" src="extracted/5889475/pic/sd_com.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.3.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S5.F9.4.2" style="font-size:90%;">The comparison of various demands brought by different SD training methods.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">We compare the performance of four approaches, namely Stable Diffusion (SD) model training, SD Fine-tune, SD+LoRA, and SD+ControlNet, from six perspectives: GPU memory consumption, RAM usage, training duration, dataset size, batch size, and training iterations during the training process, which as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.F9" title="Figure 9 ‣ 5.4.1. Optimization Results ‣ 5.4. Performance of Text-to-Image Cloud Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">9</span></a>. Our findings reveal that the traditional methods of direct SD training or utilizing the SD model for fine-tuning significantly burden computational resources, creating an unfavorable scenario for designers that may hinder their design process. In contrast, our approach, incorporating LoRA and ControlNet, minimizes data, hardware, and time requirements for SD model learning of specific styles. This method not only efficiently satisfies these requirements but also allows designers to swiftly access multiple different LoRA and ControlNet models, offering diverse style effects through random combinations during usage.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S5.F10.1.g1" src="x9.png" width="764"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.3.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S5.F10.4.2" style="font-size:90%;">Given a consistent text prompt as input, a diverse set of satisfying inspiration images are generated by selecting different styles (each column represents images in the same style).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2. </span>Qualitative Results</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.F10" title="Figure 10 ‣ 5.4.1. Optimization Results ‣ 5.4. Performance of Text-to-Image Cloud Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates the performance of our system for generating images from text prompts. Using a consistent input text, ”A girl wears a khaki coat and posed with her hands in her pockets,” we combine the generated inspiration images with LoRA models featuring different styles. Regardless of the chosen style, the results closely align with the description of ”girl,” ”khaki coat,” and ”hands in pockets” from the input text. The generated images also exhibit a general consistency with the desired style. The last two columns show a discrepancy because the requested style conflicts with the khaki color characteristics, which hinders the accurate representation of the coat’s khaki color. Despite this, other characteristics are well-displayed. Upon closer examination of the generated images, one can observe rich details, including facial features, clothing, and the surrounding environment. These exceptional results are attributed to LoRA’s control over the overall image style and ControlNet’s further optimization of image details.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Performance of Image-to-Sketch Local Module</h3>
<section class="ltx_subsubsection" id="S5.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.1. </span>Quantitative Results</h4>
<div class="ltx_para" id="S5.SS5.SSS1.p1">
<p class="ltx_p" id="S5.SS5.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.T1" title="Table 1 ‣ 5.5.1. Quantitative Results ‣ 5.5. Performance of Image-to-Sketch Local Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">1</span></a> presents the quantitative analysis of our image-to-sketch generation results, comparing our proposed model with several baseline models and showcasing the results of ablation experiments on relevant modules. We chose the FID index as our measurement standard because it closely aligns with human perception and effectively reflects the quality of sketch generation. HAIGEN demonstrates excellent performance across the three professional designer datasets and outperforms the four baseline models. Moreover, Our model boasts smaller sizes, enabling faster training and inference speeds compared to UGATIT, Self-Sup, and StyleMe. This compact size provides advantages in terms of flexibility, efficiency, and ease of deployment. It sets our model apart from the Stable Diffusion model, making it more practical and scalable for local devices. In the ablation study, we primarily focus on validating the effectiveness of the APSN module and the downsample plus concatenation operation (referred to as ”Down” in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.T1" title="Table 1 ‣ 5.5.1. Quantitative Results ‣ 5.5. Performance of Image-to-Sketch Local Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">1</span></a>) in the DSMFF module. The presence of the APSN module and the Down module significantly enhances the overall performance of our model. This improvement can be attributed to the capability of the APSN module to learn the distinctive style of the input hand-drawn sketches by capturing their overall feature distribution. Additionally, the Down module effectively utilizes the features extracted by the VGG encoder to fuse detailed information from the input features with contour features from the downsampling process. This fusion enables a more refined feature representation, resulting in enhanced output quality with intricate details.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.13.2.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S5.T1.2.1" style="font-size:90%;">Quantitative experimental results on image-to-sketch generation. (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.2.1.m1.1"><semantics id="S5.T1.2.1.m1.1b"><mo id="S5.T1.2.1.m1.1.1" stretchy="false" xref="S5.T1.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.1.m1.1c"><ci id="S5.T1.2.1.m1.1.1.cmml" xref="S5.T1.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.1.m1.1e">↓</annotation></semantics></math> indicates the lower value, the better effect.)</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.3.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T1.3.1.2" rowspan="2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.3.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.3.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.4.1">Model Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.5.1">Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.1.6"><span class="ltx_text ltx_font_bold" id="S5.T1.3.1.6.1">Inference</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T1.3.1.1">
<span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.1">Clothes-V1</span> (<span class="ltx_text ltx_font_bold" id="S5.T1.3.1.1.2">FID</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.3.1.1.m1.1"><semantics id="S5.T1.3.1.1.m1.1a"><mo id="S5.T1.3.1.1.m1.1.1" stretchy="false" xref="S5.T1.3.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.1.1.m1.1b"><ci id="S5.T1.3.1.1.m1.1.1.cmml" xref="S5.T1.3.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.1.1.m1.1d">↓</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr" id="S5.T1.11.10.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.1">(MB)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.2">(hours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.3">(items/s)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.11.10.1.4.1">Designer1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.11.10.1.5.1">Designer2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.11.10.1.6"><span class="ltx_text ltx_font_bold" id="S5.T1.11.10.1.6.1">Designer3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.11.11.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S5.T1.11.11.1.1" rowspan="5"><span class="ltx_text" id="S5.T1.11.11.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.11.11.1.1.1.1" style="width:6.9pt;height:40pt;vertical-align:-16.5pt;"><span class="ltx_transformed_inner" style="width:40.0pt;transform:translate(-16.51pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T1.11.11.1.1.1.1.1">Baselines</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.11.11.1.2">Canny <cite class="ltx_cite ltx_citemacro_citep">(Canny, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib3" title="">1986</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.6">245.8723</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.7">239.1300</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.11.11.1.8">226.3630</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.2.2">pix2pix <cite class="ltx_cite ltx_citemacro_citep">(Isola et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib21" title="">2017</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.3">43.52</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.4">0.76</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.1">115.93 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.4.2.1.m1.1"><semantics id="S5.T1.4.2.1.m1.1a"><mo id="S5.T1.4.2.1.m1.1.1" xref="S5.T1.4.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.2.1.m1.1.1.cmml" xref="S5.T1.4.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.2.1.m1.1d">±</annotation></semantics></math> 0.03</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.5">135.8482</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.2.6.1">83.0723</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.2.7">106.9900</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.5.3.2">UGATIT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib27" title="">2020</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.3">1054.72</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.4">1.08</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.1">19.46 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.5.3.1.m1.1"><semantics id="S5.T1.5.3.1.m1.1a"><mo id="S5.T1.5.3.1.m1.1.1" xref="S5.T1.5.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.5.3.1.m1.1.1.cmml" xref="S5.T1.5.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.3.1.m1.1d">±</annotation></semantics></math> 0.01</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.5">154.7540</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.6">126.8223</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.3.7">130.4817</td>
</tr>
<tr class="ltx_tr" id="S5.T1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.6.4.2">Self-Sup <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.3">21.33</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.4">0.05</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.1">68.67 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.6.4.1.m1.1"><semantics id="S5.T1.6.4.1.m1.1a"><mo id="S5.T1.6.4.1.m1.1.1" xref="S5.T1.6.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.6.4.1.m1.1.1.cmml" xref="S5.T1.6.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.4.1.m1.1d">±</annotation></semantics></math> 0.02</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.5">84.8238</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.6">98.9730</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.4.7">97.1567</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.7.5.2">StyleMe <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib59" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.3">1087.98</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.4">0.11</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.1">36.28 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.7.5.1.m1.1"><semantics id="S5.T1.7.5.1.m1.1a"><mo id="S5.T1.7.5.1.m1.1.1" xref="S5.T1.7.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.5.1.m1.1b"><csymbol cd="latexml" id="S5.T1.7.5.1.m1.1.1.cmml" xref="S5.T1.7.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.5.1.m1.1d">±</annotation></semantics></math> 0.05</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.7.5.5.1">80.5416</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.6">83.8772</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.5.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.7.5.7.1">88.9189</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.8.6.2" rowspan="4"><span class="ltx_text" id="S5.T1.8.6.2.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.8.6.2.1.1" style="width:6.9pt;height:38.1pt;vertical-align:-15.6pt;"><span class="ltx_transformed_inner" style="width:38.1pt;transform:translate(-15.56pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T1.8.6.2.1.1.1">Ablation</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.8.6.3">w/o APSN and Down</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.4">14.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.8.6.5.1">0.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.1">89.28 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.8.6.1.m1.1"><semantics id="S5.T1.8.6.1.m1.1a"><mo id="S5.T1.8.6.1.m1.1.1" xref="S5.T1.8.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.6.1.m1.1b"><csymbol cd="latexml" id="S5.T1.8.6.1.m1.1.1.cmml" xref="S5.T1.8.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.6.1.m1.1d">±</annotation></semantics></math> 0.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.6">95.9745</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.7">86.6144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.6.8">90.6861</td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.9.7.2">w/o APSN</th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.7.3.1">13.66</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.7.4.1">0.04</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.1"><span class="ltx_text ltx_font_bold" id="S5.T1.9.7.1.1">147.79 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.9.7.1.1.m1.1"><semantics id="S5.T1.9.7.1.1.m1.1a"><mo id="S5.T1.9.7.1.1.m1.1.1" xref="S5.T1.9.7.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.7.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.9.7.1.1.m1.1.1.cmml" xref="S5.T1.9.7.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.7.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.7.1.1.m1.1d">±</annotation></semantics></math> 0.02</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.5">98.2104</td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.6">98.7075</td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.7.7">106.3330</td>
</tr>
<tr class="ltx_tr" id="S5.T1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.10.8.2">w/o Down</th>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.3">14.34</td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.10.8.4.1">0.04</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.1">83.85 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.10.8.1.m1.1"><semantics id="S5.T1.10.8.1.m1.1a"><mo id="S5.T1.10.8.1.m1.1.1" xref="S5.T1.10.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.8.1.m1.1b"><csymbol cd="latexml" id="S5.T1.10.8.1.m1.1.1.cmml" xref="S5.T1.10.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.8.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.10.8.1.m1.1d">±</annotation></semantics></math> 0.01</td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.5">84.4137</td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.6">85.4105</td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.8.7">89.1427</td>
</tr>
<tr class="ltx_tr" id="S5.T1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.11.9.2"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.2.1">HAIGEN (Ours Full)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.3"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.3.1">13.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.4"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.4.1">0.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.11.9.1.1">135.25 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.11.9.1.1.m1.1"><semantics id="S5.T1.11.9.1.1.m1.1a"><mo id="S5.T1.11.9.1.1.m1.1.1" xref="S5.T1.11.9.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.11.9.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.11.9.1.1.m1.1.1.cmml" xref="S5.T1.11.9.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.9.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.11.9.1.1.m1.1d">±</annotation></semantics></math> 0.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.5"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.5.1">76.9561</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.6"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.6.1">78.6799</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.9.7"><span class="ltx_text ltx_font_bold" id="S5.T1.11.9.7.1">86.8245</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.2. </span>Qualitative Results</h4>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="288" id="S5.F11.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.2.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text" id="S5.F11.3.2" style="font-size:90%;">Comparison of generated sketches from the manuscripts of three professional designers.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.SSS2.p1">
<p class="ltx_p" id="S5.SS5.SSS2.p1.1">Furthermore, we showcase the qualitative results in the image-to-sketch generation of our proposed model compared with four baselines along with the ablation study of the APSN module and Down module as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.F11" title="Figure 11 ‣ 5.5.2. Qualitative Results ‣ 5.5. Performance of Image-to-Sketch Local Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">11</span></a>. The observed differences in the sketch styles generated by other baseline models indicate certain issues, such as the lines in Canny exhibit severe pixelation, UGATIT’s outline edges appear blurred, and while Self-Sup and StyleMe perform better, they still face challenges with excessive details (Designer2) or pixelated lines (Designer1). Similarly, in the model ablation experiments, the absence of both the APSN and Down modules clearly leads to noticeable style discrepancies between the generated sketches and hand-drawn sketches. Without the APSN module, there are evident problems with sketch lines and the handling of intricate details. Likewise, the absence of the Down module results in a loss of partial detail in the generated sketches. Overall, the results highlight the superiority of HAIGEN compared to the baselines and the contributions of individual modules in our model, showcasing its prowess in image-to-sketch generation. This makes a great contribution to generating a personalized sketch material library.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>Performance of Sketch Recommendation Module</h3>
<section class="ltx_subsubsection" id="S5.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.1. </span>Qualitative Results</h4>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S5.F12.1.g1" src="x11.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F12.3.1.1" style="font-size:90%;">Figure 12</span>. </span><span class="ltx_text" id="S5.F12.4.2" style="font-size:90%;">The performance of our Sketch Recommendation Module on recommending similar sketch templates.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.SSS1.p1">
<p class="ltx_p" id="S5.SS6.SSS1.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.F12" title="Figure 12 ‣ 5.6.1. Qualitative Results ‣ 5.6. Performance of Sketch Recommendation Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">12</span></a> illustrates the impressive performance of HAIGEN in recommending sketch templates. The first column represents the input reference image, followed by ten columns showing the top-10 recommended sketch templates from our built sketch library, most closely resembling the reference image. Notably, the recommended sketches closely resemble the real sketches, appearing prominently in the lineup. Furthermore, other sketch templates also exhibit striking similarities with the reference images. For instance, similarities in clothing styles can be observed in the second and third rows, and details such as the neckline of the clothes and pockets in the first row are replicated in the recommended templates. Although the first-ranked recommended sketch template does not precisely match the reference image, the resemblance between this recommendation and both the reference image and the real sketch template is evident. These results convincingly highlight the exceptional performance of our Sketch Recommendation Module in recommending sketch templates</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7. </span>Performance of Style Transfer Module</h3>
<section class="ltx_subsubsection" id="S5.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.1. </span>Quantitative Results</h4>
<div class="ltx_para" id="S5.SS7.SSS1.p1">
<p class="ltx_p" id="S5.SS7.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.T2" title="Table 2 ‣ 5.7.1. Quantitative Results ‣ 5.7. Performance of Style Transfer Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the quantitative evaluation of our method using five metrics to compare the performance with other approaches. It is worth noting that our method exhibits clear advantages across all indicators, particularly excelling in LPIPS and FID, aligning with human eye perception and reaching new state-of-the-art levels. Specifically, our HAIGEN employs the DDIM algorithm with 100 samples, demonstrating substantial advantages. While the results with 20 and 50 samples exhibit slightly reduced accuracy, the benefits still stand out. For the 200 samples result, the increased time cost for interfacing doesn’t provide a proportional improvement in image quality, making it less cost-effective. Furthermore, the comparison indicates that our proposed CCAM enhances the overall performance in all metrics. This improvement is attributed to its nature as an attention module, utilizing cross-attention for enhanced feature fusion between feature channels. The matrix operations in the other branch of CCAM facilitate simple aggregation of feature details. These quantitative results effectively demonstrate the effectiveness of our approach in sketch-to-image style transfer.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.15.3.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Quantitative experimental results on sketch-to-image style transfer. (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.3.1.m1.1"><semantics id="S5.T2.3.1.m1.1b"><mo id="S5.T2.3.1.m1.1.1" stretchy="false" xref="S5.T2.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.m1.1c"><ci id="S5.T2.3.1.m1.1.1.cmml" xref="S5.T2.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.1.m1.1e">↑</annotation></semantics></math> indicates the higher value, the better effect. <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.4.2.m2.1"><semantics id="S5.T2.4.2.m2.1b"><mo id="S5.T2.4.2.m2.1.1" stretchy="false" xref="S5.T2.4.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.m2.1c"><ci id="S5.T2.4.2.m2.1.1.cmml" xref="S5.T2.4.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.2.m2.1e">↓</annotation></semantics></math> indicates the lower value, the better effect.)</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.13">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.13.10.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T2.13.10.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.13.10.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.13.10.1.2.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S5.T2.13.10.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.13.10.1.3.1">HAIFashion</span></th>
</tr>
<tr class="ltx_tr" id="S5.T2.9.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.5.1.1">
<span class="ltx_text ltx_font_bold" id="S5.T2.5.1.1.1">PSNR</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.5.1.1.m1.1"><semantics id="S5.T2.5.1.1.m1.1a"><mo id="S5.T2.5.1.1.m1.1.1" stretchy="false" xref="S5.T2.5.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.1.1.m1.1b"><ci id="S5.T2.5.1.1.m1.1.1.cmml" xref="S5.T2.5.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.6.2.2">
<span class="ltx_text ltx_font_bold" id="S5.T2.6.2.2.1">SSIM</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.6.2.2.m1.1"><semantics id="S5.T2.6.2.2.m1.1a"><mo id="S5.T2.6.2.2.m1.1.1" stretchy="false" xref="S5.T2.6.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.2.2.m1.1b"><ci id="S5.T2.6.2.2.m1.1.1.cmml" xref="S5.T2.6.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.7.3.3">
<span class="ltx_text ltx_font_bold" id="S5.T2.7.3.3.1">MSE</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.7.3.3.m1.1"><semantics id="S5.T2.7.3.3.m1.1a"><mo id="S5.T2.7.3.3.m1.1.1" stretchy="false" xref="S5.T2.7.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.3.3.m1.1b"><ci id="S5.T2.7.3.3.m1.1.1.cmml" xref="S5.T2.7.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.8.4.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.4.4.1">LPIPS</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.8.4.4.m1.1"><semantics id="S5.T2.8.4.4.m1.1a"><mo id="S5.T2.8.4.4.m1.1.1" stretchy="false" xref="S5.T2.8.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.4.4.m1.1b"><ci id="S5.T2.8.4.4.m1.1.1.cmml" xref="S5.T2.8.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.4.4.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.9.5.5">
<span class="ltx_text ltx_font_bold" id="S5.T2.9.5.5.1">FID</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.9.5.5.m1.1"><semantics id="S5.T2.9.5.5.m1.1a"><mo id="S5.T2.9.5.5.m1.1.1" stretchy="false" xref="S5.T2.9.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.5.5.m1.1b"><ci id="S5.T2.9.5.5.m1.1.1.cmml" xref="S5.T2.9.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.9.5.5.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.13.11.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S5.T2.13.11.1.1" rowspan="5"><span class="ltx_text" id="S5.T2.13.11.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T2.13.11.1.1.1.1" style="width:6.9pt;height:40pt;vertical-align:-16.5pt;"><span class="ltx_transformed_inner" style="width:40.0pt;transform:translate(-16.51pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T2.13.11.1.1.1.1.1">Baselines</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.13.11.1.2">AdaIN <cite class="ltx_cite ltx_citemacro_citep">(Huang and Belongie, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib18" title="">2017</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.13.11.1.3">17.5611</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.13.11.1.4">0.3168</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.13.11.1.5">0.0302</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.13.11.1.6">0.1016</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.13.11.1.7">67.3568</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.12.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.12.2.1">Self-Sup <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib31" title="">2021b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.12.2.2">15.0941</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.12.2.3">0.3787</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.12.2.4">0.0446</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.12.2.5">0.1732</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.12.2.6">61.4071</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.13.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.13.3.1">StyleMe <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib59" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.3.2">22.8712</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.3.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.13.13.3.3.1">0.4576</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.3.4">0.0099</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.3.5">0.0651</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.3.6">30.7081</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.14.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.14.4.1">UCAST <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib63" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.14.4.2">18.8464</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.14.4.3">0.1144</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.14.4.4">0.0230</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.14.4.5">0.1275</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.14.4.6">43.2199</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.15.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.15.5.1">DiffusionRig <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#bib.bib7" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.15.5.2">15.0146</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.15.5.3">0.1857</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.15.5.4">0.0359</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.15.5.5">0.1119</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.15.5.6">40.1118</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.16.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.13.16.6.1" rowspan="5"><span class="ltx_text" id="S5.T2.13.16.6.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T2.13.16.6.1.1.1" style="width:6.9pt;height:38.1pt;vertical-align:-15.6pt;"><span class="ltx_transformed_inner" style="width:38.1pt;transform:translate(-15.56pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T2.13.16.6.1.1.1.1">Ablation</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.13.16.6.2">w/o CCAM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.16.6.3">22.1428</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.16.6.4">0.3625</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.16.6.5">0.0076</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.16.6.6">0.0363</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.16.6.7">19.8801</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.17.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.17.7.1">20 samples</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.17.7.2">23.1550</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.17.7.3">0.2425</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.17.7.4">0.0058</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.17.7.5">0.0591</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.17.7.6">30.9403</td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.18.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.13.18.8.1">50 samples</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.18.8.2">22.9912</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.18.8.3">0.2748</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.18.8.4">0.0060</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.18.8.5">0.0350</td>
<td class="ltx_td ltx_align_center" id="S5.T2.13.18.8.6">19.9451</td>
</tr>
<tr class="ltx_tr" id="S5.T2.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.11.7.3">200 samples</th>
<td class="ltx_td ltx_align_center" id="S5.T2.11.7.4"><span class="ltx_text ltx_font_bold" id="S5.T2.11.7.4.1">23.9765</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.11.7.5"><span class="ltx_text ltx_font_bold" id="S5.T2.11.7.5.1">0.5000</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.10.6.1"><math alttext="\textbf{0.0048}_{7}" class="ltx_Math" display="inline" id="S5.T2.10.6.1.m1.1"><semantics id="S5.T2.10.6.1.m1.1a"><msub id="S5.T2.10.6.1.m1.1.1" xref="S5.T2.10.6.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.T2.10.6.1.m1.1.1.2" xref="S5.T2.10.6.1.m1.1.1.2a.cmml">0.0048</mtext><mn id="S5.T2.10.6.1.m1.1.1.3" xref="S5.T2.10.6.1.m1.1.1.3.cmml">7</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T2.10.6.1.m1.1b"><apply id="S5.T2.10.6.1.m1.1.1.cmml" xref="S5.T2.10.6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.10.6.1.m1.1.1.1.cmml" xref="S5.T2.10.6.1.m1.1.1">subscript</csymbol><ci id="S5.T2.10.6.1.m1.1.1.2a.cmml" xref="S5.T2.10.6.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.T2.10.6.1.m1.1.1.2.cmml" xref="S5.T2.10.6.1.m1.1.1.2">0.0048</mtext></ci><cn id="S5.T2.10.6.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.10.6.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.6.1.m1.1c">\textbf{0.0048}_{7}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.10.6.1.m1.1d">0.0048 start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T2.11.7.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.11.7.2.1"><math alttext="0.0334_{4}" class="ltx_Math" display="inline" id="S5.T2.11.7.2.1.m1.1"><semantics id="S5.T2.11.7.2.1.m1.1a"><msub id="S5.T2.11.7.2.1.m1.1.1" xref="S5.T2.11.7.2.1.m1.1.1.cmml"><mn id="S5.T2.11.7.2.1.m1.1.1.2" xref="S5.T2.11.7.2.1.m1.1.1.2.cmml">0.0334</mn><mn id="S5.T2.11.7.2.1.m1.1.1.3" xref="S5.T2.11.7.2.1.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T2.11.7.2.1.m1.1b"><apply id="S5.T2.11.7.2.1.m1.1.1.cmml" xref="S5.T2.11.7.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.11.7.2.1.m1.1.1.1.cmml" xref="S5.T2.11.7.2.1.m1.1.1">subscript</csymbol><cn id="S5.T2.11.7.2.1.m1.1.1.2.cmml" type="float" xref="S5.T2.11.7.2.1.m1.1.1.2">0.0334</cn><cn id="S5.T2.11.7.2.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.11.7.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.7.2.1.m1.1c">0.0334_{4}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.11.7.2.1.m1.1d">0.0334 start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.11.7.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.11.7.6.1">18.1717</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.13.9.3"><span class="ltx_text ltx_font_bold" id="S5.T2.13.9.3.1">HAIGEN (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.13.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.13.9.4.1">23.9430</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.13.9.5">0.3973</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.12.8.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.12.8.1.1"><math alttext="0.0049_{3}" class="ltx_Math" display="inline" id="S5.T2.12.8.1.1.m1.1"><semantics id="S5.T2.12.8.1.1.m1.1a"><msub id="S5.T2.12.8.1.1.m1.1.1" xref="S5.T2.12.8.1.1.m1.1.1.cmml"><mn id="S5.T2.12.8.1.1.m1.1.1.2" xref="S5.T2.12.8.1.1.m1.1.1.2.cmml">0.0049</mn><mn id="S5.T2.12.8.1.1.m1.1.1.3" xref="S5.T2.12.8.1.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T2.12.8.1.1.m1.1b"><apply id="S5.T2.12.8.1.1.m1.1.1.cmml" xref="S5.T2.12.8.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.12.8.1.1.m1.1.1.1.cmml" xref="S5.T2.12.8.1.1.m1.1.1">subscript</csymbol><cn id="S5.T2.12.8.1.1.m1.1.1.2.cmml" type="float" xref="S5.T2.12.8.1.1.m1.1.1.2">0.0049</cn><cn id="S5.T2.12.8.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.12.8.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.8.1.1.m1.1c">0.0049_{3}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.12.8.1.1.m1.1d">0.0049 start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.13.9.2"><math alttext="\textbf{0.0333}_{7}" class="ltx_Math" display="inline" id="S5.T2.13.9.2.m1.1"><semantics id="S5.T2.13.9.2.m1.1a"><msub id="S5.T2.13.9.2.m1.1.1" xref="S5.T2.13.9.2.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.T2.13.9.2.m1.1.1.2" xref="S5.T2.13.9.2.m1.1.1.2a.cmml">0.0333</mtext><mn id="S5.T2.13.9.2.m1.1.1.3" xref="S5.T2.13.9.2.m1.1.1.3.cmml">7</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T2.13.9.2.m1.1b"><apply id="S5.T2.13.9.2.m1.1.1.cmml" xref="S5.T2.13.9.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.13.9.2.m1.1.1.1.cmml" xref="S5.T2.13.9.2.m1.1.1">subscript</csymbol><ci id="S5.T2.13.9.2.m1.1.1.2a.cmml" xref="S5.T2.13.9.2.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.T2.13.9.2.m1.1.1.2.cmml" xref="S5.T2.13.9.2.m1.1.1.2">0.0333</mtext></ci><cn id="S5.T2.13.9.2.m1.1.1.3.cmml" type="integer" xref="S5.T2.13.9.2.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.9.2.m1.1c">\textbf{0.0333}_{7}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.13.9.2.m1.1d">0.0333 start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.13.9.6"><span class="ltx_text ltx_font_bold" id="S5.T2.13.9.6.1">18.1367</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.2. </span>Qualitative Results</h4>
<figure class="ltx_figure" id="S5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S5.F13.1.g1" src="x12.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F13.3.1.1" style="font-size:90%;">Figure 13</span>. </span><span class="ltx_text" id="S5.F13.4.2" style="font-size:90%;">The comparison of sketch-to-image synthesis from the baselines and our model.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS7.SSS2.p1">
<p class="ltx_p" id="S5.SS7.SSS2.p1.1">We demonstrate the image generation performance of our method compared with other baselines in sketch-to-image style transfer as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S5.F13" title="Figure 13 ‣ 5.7.2. Qualitative Results ‣ 5.7. Performance of Style Transfer Module ‣ 5. Experiments ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">13</span></a>. It’s evident that the image quality generated by the diffusion model-based methods surpasses that of the GAN-based methods. For instance, DiffusionRig exhibits clear advantages in terms of generated image details, color uniformity, and color distribution, despite the obvious background color present in its images. The background color contributes to lower quantitative evaluation indicators. AdaIN faces challenges related to clothing integrity, color uniformity, and image detail. Self-Sup suffers from inconsistencies in color between the generated image and the reference image, along with a loss of details (first row) and an exaggeration of details (fourth row). StyleMe and UCAST, although exhibiting better generation effects than the former two, still struggle with retaining details. In comparison, our HAIGEN method demonstrates substantial advantages in pattern processing, detail retention, color distribution, and more. Notable examples include the patterns in the first row, clothing labels in the second row, texture details of the skirt in the third row, and patch color distribution in the fourth row. A further comparison with the CCAM removed reveals some lost details, darker colors, and inconsistent color matching with the reference. In summary, our method excels in maintaining a consistent color style while effectively handling the complete structure and intricate details of input clothing sketches. This robust performance underscores the effectiveness of our method in sketch coloring.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>User survey on HAIGEN system</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We further conducted user surveys to comprehensively assess the overall performance of our HAIGEN system and validate its effectiveness in inspiring design creativity and enhancing design efficiency.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Experimental Settings</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We designed a set of controlled experiments, wherein one group utilized traditional clothing design methods. Designers searched for inspiration references through the Internet and progressed through the design process from ideas to sketches to images through hand drawing. In contrast, another group employed the HAIGEN system as an auxiliary tool. Designers use HAIGEN to generate inspiration references and facilitate the entire design process, as detailed in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S3.F4" title="Figure 4 ‣ 3.2. System Overview ‣ 3. User Study and System Overview ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">4</span></a>. Each group comprises three designers, all of whom are graduate students majoring in design and possess three years or more of design experience.
To avoid any potential skill disparities resulting from equipment replacement, all participating designers utilized their familiar equipment, including iPads and computers, during the experiments. And for the group utilizing the HAIGEN system, we configured the experimental environment on their existing equipment, provided preliminary training on the system’s usage procedures, and ensured that all experiments were conducted on the same local area network to maintain consistency.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">During the experiments, both groups of designers were assigned a specific design theme and tasked with creating clothing designs based on that theme. We conducted three separate experiments over three weeks, with each experiment assigned design themes of varying difficulty, including modern, retro, and tech. This approach was adopted to minimize experimental interference. Following the experiment, we gathered the six participating designers to share their feelings and experiences during the design process. Subsequently, we conducted in-depth interviews to gain further insights into their perspectives.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">To assess the efficacy of the HAIGEN system in terms of the quality of completed works, we created a questionnaire regarding the design themes and the works they designed, which was distributed to the designer community. We collected 115 valid responses from the participants, collecting 115 valid responses. The questionnaire categorized participants’ designs into three themes and further divided each theme into works created with or without the HAIGEN system. Participants used a Likert scale for evaluation, consisting of five ratings: ”very good,” ”good,” ”average,” ”poor,” and ”very poor.” This comprehensive approach allowed us to gather both qualitative and quantitative data on the impact of the HAIGEN system on design quality.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Survey Results</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In the in-depth interviews conducted with the six participating designers, we gleaned the following insights:
(i) Overall Performance: After using the HAIGEN system, designers obviously feel that designing is more convenient than before.
(ii) Inspiration Stage: Designers can generate the desired inspiration images through detailed text descriptions rather than through complicated and repetitive searches. Moreover, it can switch between different scenes and styles while showing the clothing try-on renderings. In addition, the model can also generate many unexpected results for them, which makes them very excited.
(iii) Sketching Stage: The most satisfying thing for designers here is that they can modify the sketch directly on the recommended template instead of starting from scratch.
(iv) Coloring Stage: Designers find joy in the fact that they no longer need to visualize the coloring effects in their minds as required by traditional methods. Instead, they can directly employ the model to produce the corresponding renderings. This speeds up their coloring process and makes the coloring more accurate.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S6.F14.1.g1" src="extracted/5889475/pic/users.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F14.3.1.1" style="font-size:90%;">Figure 14</span>. </span><span class="ltx_text" id="S6.F14.4.2" style="font-size:90%;">The user survey results on time cost and work quality.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6.F14" title="Figure 14 ‣ 6.2. Survey Results ‣ 6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">14</span></a>, we present a comparison of the time spent in the design process when using the HAIGEN system (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6.F14" title="Figure 14 ‣ 6.2. Survey Results ‣ 6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">14</span></a> (a)), along with the results of the questionnaire survey regarding the obtained design works (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.00855v3#S6.F14" title="Figure 14 ‣ 6.2. Survey Results ‣ 6. User survey on HAIGEN system ‣ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design"><span class="ltx_text ltx_ref_tag">14</span></a> (b)). The analysis reveals a consistent improvement in designers’ efficiency across the three experimental groups after incorporating HAIGEN, with an impressive increase of approximately 250%. This substantial enhancement is highly advantageous for their design processes. Notably, the traditional method requires around four hours for inspiration reference, whereas our approach achieves the same task in just about half an hour. Furthermore, in the sketching and sketch coloring stages, HAIGEN accelerates the design process by offering a large number of preset sketch templates and enabling designers to preview sketches in advance. The results of the questionnaire indicated a positive impact of HAIGEN on the overall quality of the final design work. Notably, it demonstrated commendable performance in terms of image quality and theme consistency, with the proportion of scores at ”average” and above increasing by 3% and 11%, respectively. Additionally, positive reviews (”very good” and ”good”) increased from 44% and 46% to 50% and 53%, respectively. This success can be attributed to our system’s capability to generate matching inspiration images from detailed text descriptions, providing rich background and clothing model try-on references, thereby effectively inspiring designers and preserving their initial ideas. Furthermore, the sketch coloring module enables designers to swiftly experiment with various styles, facilitating the identification of the most suitable color style for the final solution. Despite some negative feedback regarding sketch quality, attributed to limitations of the image-to-sketch generation algorithm in accurately generating line outlines outside of outlines like humans, the algorithm’s fast generation speed and minimal impact of the sketches on the final image generation underscore its efficacy as a solution. Overall, these user experiments reaffirm the effectiveness of our system.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduce HAIGEN, a Human-AI collaboration fashion design system that seamlessly integrates designers with cloud large models and local small models. This integration facilitates the entire design creation process, addressing the specific needs identified through user study. First and foremost, we developed an enhanced cloud-based Stable Diffusion large model that can generate a wealth of relevant inspirational images, serving as a wellspring of creative ideas for designers. Moreover, we’ve integrated a sketch generation model renowned for capturing designers’ unique styles. Coupled with our self-collected fashion clothing image material library, this model can generate a profusion of personalized sketch templates. To further streamline the sketching process, we’ve introduced a sketch recommend module that recommends similar sketch templates to designers. This approach simplifies and expedites the sketch creation process. In the final phase, our style transfer module, equipped with impressive feature fusion capabilities, harmoniously blends refined sketches with inspirational images. The outcome is a collection of high-quality, realistic, and expertly-colored sketch images. To validate the performance of our system, we conducted extensive qualitative and quantitative experiments on our self-collected clothing datasets. The results underscore the efficacy and utility of each module within our system. Furthermore, through user usage surveys, we emphasize the applicability and effectiveness of HAIGEN, establishing it as an efficient generative AI system for fashion design with human-in-the-loop.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhunia et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jorma Laaksonen, and Michael Felsberg. 2022.

</span>
<span class="ltx_bibblock">Doodleformer: Creative sketch drawing with transformers. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVII</em>. Springer, 338–355.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canny (1986)</span>
<span class="ltx_bibblock">
John Canny. 1986.

</span>
<span class="ltx_bibblock">A computational approach to edge detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on pattern analysis and machine intelligence</em> 6 (1986), 679–698.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. 2021.

</span>
<span class="ltx_bibblock">Crossvit: Cross-attention multi-scale vision transformer for image classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the IEEE/CVF international conference on computer vision</em>. 357–366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sungjae Cho, Yoonsu Kim, Jaewoong Jang, and Inseok Hwang. 2023.

</span>
<span class="ltx_bibblock">AI-to-Human Actuation: Boosting Unmodified AI’s Robustness by Proactively Inducing Favorable Human Sensing Conditions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 1 (2023), 1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hanhui Deng, Jianan Jiang, Zhiwang Yu, Jinhui Ouyang, and Di Wu. 2024.

</span>
<span class="ltx_bibblock">CrossGAI: A Cross-Device Generative AI Framework for Collaborative Fashion Design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 8, 1 (2024), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen Tu, and Xiuming Zhang. 2023.

</span>
<span class="ltx_bibblock">DiffusionRig: Learning Personalized Priors for Facial Appearance Editing. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 12736–12746.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al<span class="ltx_text" id="bib.bib8.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.4.1">arXiv preprint arXiv:2010.11929</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Figueiredo et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mayara Costa Figueiredo, Elizabeth Ankrah, Jacquelyn E Powell, Daniel A Epstein, and Yunan Chen. 2024.

</span>
<span class="ltx_bibblock">Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 4 (2024), 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, and Devi Parikh. 2020.

</span>
<span class="ltx_bibblock">Creative Sketch Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2011.10039 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Generative Adversarial Nets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Advances in Neural Information Processing Systems</em> 27 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha and Eck (2017)</span>
<span class="ltx_bibblock">
David Ha and Douglas Eck. 2017.

</span>
<span class="ltx_bibblock">A neural representation of sketch drawings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:1704.03477</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. 2021.

</span>
<span class="ltx_bibblock">Transformer in transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 770–778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Advances in neural information processing systems</em> 33 (2020), 6840–6851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2106.09685</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Belongie (2017)</span>
<span class="ltx_bibblock">
Xun Huang and Serge Belongie. 2017.

</span>
<span class="ltx_bibblock">Arbitrary style transfer in real-time with adaptive instance normalization. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE international conference on computer vision</em>. 1501–1510.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ikeda et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Markus Ikeda, Fabian Widmoser, Gernot Stübl, Setareh Zafari, Andreas Sackl, and Andreas Pichler. 2023.

</span>
<span class="ltx_bibblock">An Interactive Workplace for Improving Human Robot Collaboration: Sketch Workpiece Interface for Fast Teaching (SWIFT).. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing</em>. 191–194.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy (2015)</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy. 2015.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing internal covariate shift. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International conference on machine learning</em>. pmlr, 448–456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isola et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017.

</span>
<span class="ltx_bibblock">Image-to-image translation with conditional adversarial networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 1125–1134.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jianan Jiang, Xinglin Li, Weiren Yu, and Di Wu. 2024a.

</span>
<span class="ltx_bibblock">HAIFIT: Human-Centered AI for Fashion Image Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2403.08651</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jianan Jiang, Di Wu, Zhilin Jiang, and Weiren Yu. 2024b.

</span>
<span class="ltx_bibblock">Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2406.11551</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Haojian Jin, Boyuan Guo, Rituparna Roychoudhury, Yaxing Yao, Swarun Kumar, Yuvraj Agarwal, and Jason I Hong. 2022.

</span>
<span class="ltx_bibblock">Exploring the needs of users for supporting privacy-protective behaviors in smart homes. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jonson (2005)</span>
<span class="ltx_bibblock">
Ben Jonson. 2005.

</span>
<span class="ltx_bibblock">Design ideation: the conceptual sketch in the digital age.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Design studies</em> 26, 6 (2005), 613–624.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020.

</span>
<span class="ltx_bibblock">Analyzing and improving the image quality of stylegan. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 8110–8119.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Junho Kim, Minjae Kim, Hyeonwoo Kang, and Hee Kwang Lee. 2020.

</span>
<span class="ltx_bibblock">U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2013)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Max Welling. 2013.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:1312.6114</em> (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumari et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023.

</span>
<span class="ltx_bibblock">Multi-concept customization of text-to-image diffusion. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1931–1941.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yao Li, Xianggang Yu, Xiaoguang Han, Nianjuan Jiang, Kui Jia, and Jiangbo Lu. 2020.

</span>
<span class="ltx_bibblock">A deep learning based interactive sketching system for fashion images design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2010.04413</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. 2021b.

</span>
<span class="ltx_bibblock">Self-supervised sketch-to-image synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 35. 2073–2081.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021a.

</span>
<span class="ltx_bibblock">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Midjourney (2022)</span>
<span class="ltx_bibblock">
Midjourney. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Midjourney</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.midjourney.com/home" title="">https://www.midjourney.com/home</a>
</span>
<span class="ltx_bibblock">[Online; accessed August-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirza and Osindero (2014)</span>
<span class="ltx_bibblock">
Mehdi Mirza and Simon Osindero. 2014.

</span>
<span class="ltx_bibblock">Conditional generative adversarial nets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1411.1784</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miyato et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018.

</span>
<span class="ltx_bibblock">Spectral normalization for generative adversarial networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:1802.05957</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
M Prajwal, Ayush Raj, Sougata Sen, Snehanshu Saha, and Surjya Ghosh. 2023.

</span>
<span class="ltx_bibblock">Towards Efficient Emotion Self-report Collection Using Human-AI Collaboration: A Case Study on Smartphone Keyboard Interaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 2 (2023), 1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span class="ltx_text" id="bib.bib37.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.4.1">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">International Conference on Machine Learning</em>. PMLR, 8821–8831.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016.

</span>
<span class="ltx_bibblock">Generative adversarial text to image synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">International conference on machine learning</em>. PMLR, 1060–1069.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2015a)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015a.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>. Springer, 234–241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2015b)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015b.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>. Springer, 234–241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023.

</span>
<span class="ltx_bibblock">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 22500–22510.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sailaja et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Neelima Sailaja, Teresa Castle-Green, Paul Coulton, Michael Stead, Joseph Lindley, Lachlan Urquhart, and Dimitrios Paris Darzentas. 2023.

</span>
<span class="ltx_bibblock">UbiFix: Tackling Repairability Challenges in Smart Devices. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing</em>. 802–806.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saisho et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Osamu Saisho, Keiichiro Kashiwagi, Sakiko Kawai, Kazuki Iwahana, and Koki Mitani. 2023.

</span>
<span class="ltx_bibblock">Sandbox AI: We Don’t Trust Each Other but Want to Create New Value Efficiently Through Collaboration Using Sensitive Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing</em>. 68–72.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarafianos et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris. 2019.

</span>
<span class="ltx_bibblock">Adversarial representation learning for text-to-image matching. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of the IEEE/CVF international conference on computer vision</em>. 5814–5824.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schultheiß and Lewandowski (2023)</span>
<span class="ltx_bibblock">
Sebastian Schultheiß and Dirk Lewandowski. 2023.

</span>
<span class="ltx_bibblock">Misplaced trust? The relationship between trust, ability to identify commercially influenced results and search engine preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Journal of Information Science</em> 49, 3 (2023), 609–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1409.1556</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion implicit models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">arXiv preprint arXiv:2010.02502</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava and Salakhutdinov (2012)</span>
<span class="ltx_bibblock">
Nitish Srivastava and Russ R Salakhutdinov. 2012.

</span>
<span class="ltx_bibblock">Multimodal learning with deep boltzmann machines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in neural information processing systems</em> 25 (2012).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuqian Sun, Xingyu Li, Jun Peng, and Ze Gao. 2023.

</span>
<span class="ltx_bibblock">Inspire creativity with ORIBA: Transform Artists’ Original Characters into Chatbots through Large Language Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing</em>. 78–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and Qianying Wang. 2020.

</span>
<span class="ltx_bibblock">From human-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Extended abstracts of the 2020 CHI conference on human factors in computing systems</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Nannan Wang, Xinbo Gao, Leiyu Sun, and Jie Li. 2017.

</span>
<span class="ltx_bibblock">Bayesian face sketch synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">IEEE transactions on image processing</em> 26, 3 (2017), 1264–1274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to structural similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">IEEE transactions on image processing</em> 13, 4 (2004), 600–612.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Di Wu, Qinghua Guan, Zhe Fan, Hanhui Deng, and Tao Wu. 2022a.

</span>
<span class="ltx_bibblock">Automl with parallel genetic algorithm for fast hyperparameters optimization in efficient iot time series prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">IEEE Transactions on Industrial Informatics</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Di Wu, Jinhui Ouyang, Ningyi Dai, Mingzhu Wu, Haodan Tan, Hanhui Deng, Yongmei Fan, Dakuo Wang, and Zhanpeng Jin. 2022b.

</span>
<span class="ltx_bibblock">DeepBrain: Enabling Fine-Grained Brain-Robot Interaction through Human-Centered Learning of Coarse EEG Signals from Low-Cost Devices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 6, 3 (2022), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Di Wu, He Xu, Zhongkai Jiang, Weiren Yu, Xuetao Wei, and Jiwu Lu. 2021.

</span>
<span class="ltx_bibblock">EdgeLSTM: Towards deep and sequential edge computing for IoT applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">IEEE/ACM Transactions on Networking</em> 29, 4 (2021), 1895–1908.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Di Wu, Zhiwang Yu, Nan Ma, Jianan Jiang, Yuetian Wang, Guixiang Zhou, Hanhui Deng, and Yi Li. 2023.

</span>
<span class="ltx_bibblock">StyleMe: Towards Intelligent Fashion Generation with Designer Style. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie and Tu (2015)</span>
<span class="ltx_bibblock">
Saining Xie and Zhuowen Tu. 2015.

</span>
<span class="ltx_bibblock">Holistically-nested edge detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE international conference on computer vision</em>. 1395–1403.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023a.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 3836–3847.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 586–595.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng Xu. 2023b.

</span>
<span class="ltx_bibblock">A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">ACM Transactions on Graphics</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mingrui Zhu, Changcheng Liang, Nannan Wang, Xiaoyu Wang, Zhifeng Li, and Xinbo Gao. 2021.

</span>
<span class="ltx_bibblock">A Sketch-Transformer Network for Face Photo-Sketch Synthesis.. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">IJCAI</em>. 1352–1358.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 09:37:20 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
