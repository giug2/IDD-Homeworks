<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>domain adaptation for Arabic machine translation: the case of financial texts</title>
<!--Generated on Fri Sep 22 13:24:59 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="https://browse.arxiv.org/latexml/ar5iv_0.7.4.min.css" type="text/css">
<link rel="stylesheet" href="https://browse.arxiv.org/latexml/styles.css" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://browse.arxiv.org/latexml/addons.js"></script>
<script src="https://browse.arxiv.org/latexml/feedbackOverlay.js"></script>
</head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S0.SS1" title="0.1 Introduction â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S1" title="1 Background â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS1" title="1.1 Neural machine translation â€£ 1 Background â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Neural machine translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS2" title="1.2 Domain-specific MT â€£ 1 Background â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Domain-specific MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS3" title="1.3 Domain-adaptation in Arabic MT â€£ 1 Background â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Domain-adaptation in Arabic MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S2.SS1" title="2.1 Approach â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS1.SSS1" title="2.1.1 Synthetic data generation â€£ 2.1 Approach â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Synthetic data generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS1.SSS2" title="2.1.2 Back-translation â€£ 2.1 Approach â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Back-translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S2.SS2" title="2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Experiment setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS2.SSS1" title="2.2.1 Datasets â€£ 2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS2.SSS2" title="2.2.2 NMT pre-trained models â€£ 2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>NMT pre-trained models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="2.3 Metrics â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="3.1 Automatic evaluation â€£ 3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Automatic evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="3.2 Human evaluation â€£ 3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Human evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S4" title="4 Conclusion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS1" title="4.1 Acknowledgements â€£ 4 Conclusion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_ERROR undefined">\useunder</span>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.1.2" class="ltx_ERROR undefined">\ul</span>




</p>
</div>
<h1 class="ltx_title ltx_title_document">domain adaptation for Arabic machine translation: the case of financial texts</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Emad A. Alghamdi
<br class="ltx_break">King Abdulaziz University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">eaalghamdi@kau.edu.sa</span> 
<br class="ltx_break">ASAS AI Lab 
<br class="ltx_break">&amp;Jezia Zakraoui 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">j.zakraoui@gmail.com</span> 
<br class="ltx_break">ASAS AI Lab 
<br class="ltx_break">&amp;Fares A. Abanmy
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">abanmyfares@gmail.com</span> 
<br class="ltx_break">ASAS AI Lab 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-of-domain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaption in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed carefully a parallel corpus for Arabic-English (AR-EN) translation in the financial domain for benchmarking different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that the fine-tuning is successful using just a few well-aligned in-domain AR-EN segments. The quality of ChatGPT translation was superior than other models based on automatic and human evaluations. To the best of our knowledge, this is the first work on fine-tuning ChatGPT towards financial domain transfer learning. To contribute to research in domain translation, we made our datasets and fine-tuned models available at <a href="https://huggingface.co/asas-ai/" title="" class="ltx_ref ltx_href">https://huggingface.co/asas-ai/</a>.</p>
</div>
<div id="p2" class="ltx_para ltx_noindent">
<p id="p2.3" class="ltx_p"><em id="p2.3.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p2.3.2" class="ltx_text ltx_font_bold">eywords</span>â€‚Machine Translation Â <math id="p2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p2.1.m1.1a"><mo id="p2.1.m1.1.1" xref="p2.1.m1.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p2.1.m1.1b"><ci id="p2.1.m1.1.1.cmml" xref="p2.1.m1.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p2.1.m1.1d">â‹…</annotation></semantics></math>
Arabic MT Â <math id="p2.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p2.2.m2.1a"><mo id="p2.2.m2.1.1" xref="p2.2.m2.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p2.2.m2.1b"><ci id="p2.2.m2.1.1.cmml" xref="p2.2.m2.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p2.2.m2.1d">â‹…</annotation></semantics></math>
Domain Adaptation, Â <math id="p2.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p2.3.m3.1a"><mo id="p2.3.m3.1.1" xref="p2.3.m3.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p2.3.m3.1b"><ci id="p2.3.m3.1.1.cmml" xref="p2.3.m3.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p2.3.m3.1d">â‹…</annotation></semantics></math>
Financial Domain</p>
</div>
<section id="S0.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.1 </span>Introduction</h3>

<div id="S0.SS1.p1" class="ltx_para ltx_noindent">
<p id="S0.SS1.p1.1" class="ltx_p">In the recent years, the rapid advancement of deep learning techniques and their adaptation in machine translation has made a great stride in many translation tasks. Neural Machine Translation (NMT) systems, trained on a large-scale corpora, have demonstrated impressive performance in translating generic language. However, NMT models tend to perform poorly on out-of-domain data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, especially if the target domain has a distinctive style and vocabulary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>. A NMT model trained on exclusively medical texts is unlikely to achieve accurate performance on financial or news data. To address this problem, researchers have proposed different domain adaptation approaches and techniques which seem to improve the quality of NMT systems on out-of-domain data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>, <a href="#bib.bibx4" title="" class="ltx_ref">4</a>, <a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S0.SS1.p2" class="ltx_para ltx_noindent">
<p id="S0.SS1.p2.1" class="ltx_p">While there are many MT models, systems and tools for translating Arabic texts in the literature; however, the quality of the translation is poor, especially for out-of-domain texts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>. A key technical challenge related to AMT arises from the lack of available bilingual datasets for out-of-domain texts that can be used as standard benchmark to conduct unified experiments. In fact, researchers tend to collect datasets according to their specific domains and try to resolve the linguistic issues for Arabic, based on custom datasets such as in the domain of news <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite> ignoring hereby many other domains. Other technical issues such as out-of-vocabulary (OOV) and very long sentences also make MT more challenging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. To address these challenges, researchers have proposed different techniques, including for example, BPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>, character-level BPE variant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, hybrid techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>, and mixed fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>. However, domain robustness remains an unsolved problem and there is a need for further research in this area <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>. This is specially true for Arabic language. Existing domain adaptation research has only focused on news <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> and medical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> domains, no prior study, to the best of our knowledge, has been conducted on financial domain.</p>
</div>
<div id="S0.SS1.p3" class="ltx_para ltx_noindent">
<p id="S0.SS1.p3.1" class="ltx_p">To alleviate the issue with translation mismatch related to out-of-domain texts, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> studied the performance of NMT systems under morphology-based and frequency-based tokenization schemes and BPE on in-domain data. They evaluated their best performing models on out-of-domain data yielding significant improvements of 37.96% in BLEU score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>. The latter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> proposed a method for domain-specific data augmentation for MT to tackle the issue with a small bilingual dataset. They employed mixed fine-tuning to train models that significantly improve translation of in-domain texts. Their method achieved improvements of approximately 5-6 BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic language pairs.
</p>
</div>
<div id="S0.SS1.p4" class="ltx_para ltx_noindent">
<p id="S0.SS1.p4.1" class="ltx_p">While a lot of research in domain adaptation in MT for other language pairs like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite> exist which focus on synthetic data generation and multiple other techniques like checkpoint averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, only one work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> investigated the same for AMT, but only for a medical domain. This research aims to fill the gap in creating different MT settings and investigate in the domain of financial texts and potentially to extend to other domains.</p>
</div>
<div id="S0.SS1.p5" class="ltx_para ltx_noindent">
<p id="S0.SS1.p5.1" class="ltx_p">Our contributions are the following:</p>
<ul id="S0.I1" class="ltx_itemize">
<li id="S0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S0.I1.i1.p1" class="ltx_para">
<p id="S0.I1.i1.p1.1" class="ltx_p">We introduce the first AR-EN parallel corpus in the financial domain.</p>
</div>
</li>
<li id="S0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S0.I1.i2.p1" class="ltx_para">
<p id="S0.I1.i2.p1.1" class="ltx_p">We compare the effectiveness of different adaption methods and data augmentation approaches for limited domain data.</p>
</div>
</li>
<li id="S0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S0.I1.i3.p1" class="ltx_para">
<p id="S0.I1.i3.p1.1" class="ltx_p">We fine-tuned several models and made them publicly available to the research community.</p>
</div>
</li>
<li id="S0.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S0.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S0.I1.i4.p1.1" class="ltx_p">Our work is the first to fine-tune GPT3.5 model and evaluate its capability for domain adaption.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Background</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Neural machine translation</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.7" class="ltx_p">NMT models based on deep neural networks (DNN) have been proposed in early NMT research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>. A DNN based NMT model employs a neural network system to perform the required machine translation tasks using an encoder-decoder network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>. The encoder neural network inputs and encodes a source language sentence into a fixed-length vector in each hidden state. Then, given the final hidden state of the encoder, the decoder does the reverse work by transforming the hidden state vector to the target sentence word by word. A translation probability of a source sentence is modeled into the target sentence. Given a source sentences
<math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><mi id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><ci id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.1.m1.1d">italic_S</annotation></semantics></math>= <math id="S1.SS1.p1.2.m2.1" class="ltx_math_unparsed" alttext="\bigl{\{}s_{1},s_{2},..s_{n}\bigl{\}}" display="inline"><semantics id="S1.SS1.p1.2.m2.1a"><mrow id="S1.SS1.p1.2.m2.1b"><mo maxsize="120%" minsize="120%" id="S1.SS1.p1.2.m2.1.1">{</mo><msub id="S1.SS1.p1.2.m2.1.2"><mi id="S1.SS1.p1.2.m2.1.2.2">s</mi><mn id="S1.SS1.p1.2.m2.1.2.3">1</mn></msub><mo id="S1.SS1.p1.2.m2.1.3">,</mo><msub id="S1.SS1.p1.2.m2.1.4"><mi id="S1.SS1.p1.2.m2.1.4.2">s</mi><mn id="S1.SS1.p1.2.m2.1.4.3">2</mn></msub><mo id="S1.SS1.p1.2.m2.1.5">,</mo><mo lspace="0em" rspace="0.0835em" id="S1.SS1.p1.2.m2.1.6">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S1.SS1.p1.2.m2.1.7">.</mo><msub id="S1.SS1.p1.2.m2.1.8"><mi id="S1.SS1.p1.2.m2.1.8.2">s</mi><mi id="S1.SS1.p1.2.m2.1.8.3">n</mi></msub><mo lspace="0em" mathsize="120%" id="S1.SS1.p1.2.m2.1.9">}</mo></mrow><annotation encoding="application/x-tex" id="S1.SS1.p1.2.m2.1c">\bigl{\{}s_{1},s_{2},..s_{n}\bigl{\}}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.2.m2.1d">{ italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , . . italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>
and a target sentence
<math id="S1.SS1.p1.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S1.SS1.p1.3.m3.1a"><mi id="S1.SS1.p1.3.m3.1.1" xref="S1.SS1.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.3.m3.1b"><ci id="S1.SS1.p1.3.m3.1.1.cmml" xref="S1.SS1.p1.3.m3.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.3.m3.1d">italic_T</annotation></semantics></math>= <math id="S1.SS1.p1.4.m4.1" class="ltx_math_unparsed" alttext="\bigl{\{}t_{1},t_{2},..t_{n}\bigl{\}}" display="inline"><semantics id="S1.SS1.p1.4.m4.1a"><mrow id="S1.SS1.p1.4.m4.1b"><mo maxsize="120%" minsize="120%" id="S1.SS1.p1.4.m4.1.1">{</mo><msub id="S1.SS1.p1.4.m4.1.2"><mi id="S1.SS1.p1.4.m4.1.2.2">t</mi><mn id="S1.SS1.p1.4.m4.1.2.3">1</mn></msub><mo id="S1.SS1.p1.4.m4.1.3">,</mo><msub id="S1.SS1.p1.4.m4.1.4"><mi id="S1.SS1.p1.4.m4.1.4.2">t</mi><mn id="S1.SS1.p1.4.m4.1.4.3">2</mn></msub><mo id="S1.SS1.p1.4.m4.1.5">,</mo><mo lspace="0em" rspace="0.0835em" id="S1.SS1.p1.4.m4.1.6">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S1.SS1.p1.4.m4.1.7">.</mo><msub id="S1.SS1.p1.4.m4.1.8"><mi id="S1.SS1.p1.4.m4.1.8.2">t</mi><mi id="S1.SS1.p1.4.m4.1.8.3">n</mi></msub><mo lspace="0em" mathsize="120%" id="S1.SS1.p1.4.m4.1.9">}</mo></mrow><annotation encoding="application/x-tex" id="S1.SS1.p1.4.m4.1c">\bigl{\{}t_{1},t_{2},..t_{n}\bigl{\}}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.4.m4.1d">{ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , . . italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>
, the encoder encodes all the words from the source sentence <math id="S1.SS1.p1.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S1.SS1.p1.5.m5.1a"><mi id="S1.SS1.p1.5.m5.1.1" xref="S1.SS1.p1.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.5.m5.1b"><ci id="S1.SS1.p1.5.m5.1.1.cmml" xref="S1.SS1.p1.5.m5.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.5.m5.1d">italic_S</annotation></semantics></math>
into a set of hidden states <math id="S1.SS1.p1.6.m6.1" class="ltx_math_unparsed" alttext="\bigl{(}h_{1},h_{2},..h_{n}\bigl{)}" display="inline"><semantics id="S1.SS1.p1.6.m6.1a"><mrow id="S1.SS1.p1.6.m6.1b"><mo maxsize="120%" minsize="120%" id="S1.SS1.p1.6.m6.1.1">(</mo><msub id="S1.SS1.p1.6.m6.1.2"><mi id="S1.SS1.p1.6.m6.1.2.2">h</mi><mn id="S1.SS1.p1.6.m6.1.2.3">1</mn></msub><mo id="S1.SS1.p1.6.m6.1.3">,</mo><msub id="S1.SS1.p1.6.m6.1.4"><mi id="S1.SS1.p1.6.m6.1.4.2">h</mi><mn id="S1.SS1.p1.6.m6.1.4.3">2</mn></msub><mo id="S1.SS1.p1.6.m6.1.5">,</mo><mo lspace="0em" rspace="0.0835em" id="S1.SS1.p1.6.m6.1.6">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S1.SS1.p1.6.m6.1.7">.</mo><msub id="S1.SS1.p1.6.m6.1.8"><mi id="S1.SS1.p1.6.m6.1.8.2">h</mi><mi id="S1.SS1.p1.6.m6.1.8.3">n</mi></msub><mo lspace="0em" mathsize="120%" id="S1.SS1.p1.6.m6.1.9">)</mo></mrow><annotation encoding="application/x-tex" id="S1.SS1.p1.6.m6.1c">\bigl{(}h_{1},h_{2},..h_{n}\bigl{)}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.6.m6.1d">( italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , . . italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>
and passes the fixed-size vector <math id="S1.SS1.p1.7.m7.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S1.SS1.p1.7.m7.1a"><mi id="S1.SS1.p1.7.m7.1.1" xref="S1.SS1.p1.7.m7.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.7.m7.1b"><ci id="S1.SS1.p1.7.m7.1.1.cmml" xref="S1.SS1.p1.7.m7.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.7.m7.1c">v</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p1.7.m7.1d">italic_v</annotation></semantics></math>, which represents the source sentence, to the decoder. The translation probability with a single neural network is given by following formula <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>:</p>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<table id="S1.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E1.m1.3" class="ltx_Math" alttext="P(S)=\prod_{i=1}^{n}P(t_{&lt;i},S)" display="block"><semantics id="S1.E1.m1.3a"><mrow id="S1.E1.m1.3.3" xref="S1.E1.m1.3.3.cmml"><mrow id="S1.E1.m1.3.3.3" xref="S1.E1.m1.3.3.3.cmml"><mi id="S1.E1.m1.3.3.3.2" xref="S1.E1.m1.3.3.3.2.cmml">P</mi><mo id="S1.E1.m1.3.3.3.1" xref="S1.E1.m1.3.3.3.1.cmml">â¢</mo><mrow id="S1.E1.m1.3.3.3.3.2" xref="S1.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S1.E1.m1.3.3.3.3.2.1" xref="S1.E1.m1.3.3.3.cmml">(</mo><mi id="S1.E1.m1.1.1" xref="S1.E1.m1.1.1.cmml">S</mi><mo stretchy="false" id="S1.E1.m1.3.3.3.3.2.2" xref="S1.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S1.E1.m1.3.3.2" xref="S1.E1.m1.3.3.2.cmml">=</mo><mrow id="S1.E1.m1.3.3.1" xref="S1.E1.m1.3.3.1.cmml"><munderover id="S1.E1.m1.3.3.1.2" xref="S1.E1.m1.3.3.1.2.cmml"><mo movablelimits="false" id="S1.E1.m1.3.3.1.2.2.2" xref="S1.E1.m1.3.3.1.2.2.2.cmml">âˆ</mo><mrow id="S1.E1.m1.3.3.1.2.2.3" xref="S1.E1.m1.3.3.1.2.2.3.cmml"><mi id="S1.E1.m1.3.3.1.2.2.3.2" xref="S1.E1.m1.3.3.1.2.2.3.2.cmml">i</mi><mo id="S1.E1.m1.3.3.1.2.2.3.1" xref="S1.E1.m1.3.3.1.2.2.3.1.cmml">=</mo><mn id="S1.E1.m1.3.3.1.2.2.3.3" xref="S1.E1.m1.3.3.1.2.2.3.3.cmml">1</mn></mrow><mi id="S1.E1.m1.3.3.1.2.3" xref="S1.E1.m1.3.3.1.2.3.cmml">n</mi></munderover><mrow id="S1.E1.m1.3.3.1.1" xref="S1.E1.m1.3.3.1.1.cmml"><mi id="S1.E1.m1.3.3.1.1.3" xref="S1.E1.m1.3.3.1.1.3.cmml">P</mi><mo id="S1.E1.m1.3.3.1.1.2" xref="S1.E1.m1.3.3.1.1.2.cmml">â¢</mo><mrow id="S1.E1.m1.3.3.1.1.1.1" xref="S1.E1.m1.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S1.E1.m1.3.3.1.1.1.1.2" xref="S1.E1.m1.3.3.1.1.1.2.cmml">(</mo><msub id="S1.E1.m1.3.3.1.1.1.1.1" xref="S1.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S1.E1.m1.3.3.1.1.1.1.1.2" xref="S1.E1.m1.3.3.1.1.1.1.1.2.cmml">t</mi><mrow id="S1.E1.m1.3.3.1.1.1.1.1.3" xref="S1.E1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S1.E1.m1.3.3.1.1.1.1.1.3.2" xref="S1.E1.m1.3.3.1.1.1.1.1.3.2.cmml"></mi><mo id="S1.E1.m1.3.3.1.1.1.1.1.3.1" xref="S1.E1.m1.3.3.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S1.E1.m1.3.3.1.1.1.1.1.3.3" xref="S1.E1.m1.3.3.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S1.E1.m1.3.3.1.1.1.1.3" xref="S1.E1.m1.3.3.1.1.1.2.cmml">,</mo><mi id="S1.E1.m1.2.2" xref="S1.E1.m1.2.2.cmml">S</mi><mo stretchy="false" id="S1.E1.m1.3.3.1.1.1.1.4" xref="S1.E1.m1.3.3.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.E1.m1.3b"><apply id="S1.E1.m1.3.3.cmml" xref="S1.E1.m1.3.3"><eq id="S1.E1.m1.3.3.2.cmml" xref="S1.E1.m1.3.3.2"></eq><apply id="S1.E1.m1.3.3.3.cmml" xref="S1.E1.m1.3.3.3"><times id="S1.E1.m1.3.3.3.1.cmml" xref="S1.E1.m1.3.3.3.1"></times><ci id="S1.E1.m1.3.3.3.2.cmml" xref="S1.E1.m1.3.3.3.2">ğ‘ƒ</ci><ci id="S1.E1.m1.1.1.cmml" xref="S1.E1.m1.1.1">ğ‘†</ci></apply><apply id="S1.E1.m1.3.3.1.cmml" xref="S1.E1.m1.3.3.1"><apply id="S1.E1.m1.3.3.1.2.cmml" xref="S1.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S1.E1.m1.3.3.1.2.1.cmml" xref="S1.E1.m1.3.3.1.2">superscript</csymbol><apply id="S1.E1.m1.3.3.1.2.2.cmml" xref="S1.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S1.E1.m1.3.3.1.2.2.1.cmml" xref="S1.E1.m1.3.3.1.2">subscript</csymbol><csymbol cd="latexml" id="S1.E1.m1.3.3.1.2.2.2.cmml" xref="S1.E1.m1.3.3.1.2.2.2">product</csymbol><apply id="S1.E1.m1.3.3.1.2.2.3.cmml" xref="S1.E1.m1.3.3.1.2.2.3"><eq id="S1.E1.m1.3.3.1.2.2.3.1.cmml" xref="S1.E1.m1.3.3.1.2.2.3.1"></eq><ci id="S1.E1.m1.3.3.1.2.2.3.2.cmml" xref="S1.E1.m1.3.3.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S1.E1.m1.3.3.1.2.2.3.3.cmml" xref="S1.E1.m1.3.3.1.2.2.3.3">1</cn></apply></apply><ci id="S1.E1.m1.3.3.1.2.3.cmml" xref="S1.E1.m1.3.3.1.2.3">ğ‘›</ci></apply><apply id="S1.E1.m1.3.3.1.1.cmml" xref="S1.E1.m1.3.3.1.1"><times id="S1.E1.m1.3.3.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.2"></times><ci id="S1.E1.m1.3.3.1.1.3.cmml" xref="S1.E1.m1.3.3.1.1.3">ğ‘ƒ</ci><interval closure="open" id="S1.E1.m1.3.3.1.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.1.1"><apply id="S1.E1.m1.3.3.1.1.1.1.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S1.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.2">ğ‘¡</ci><apply id="S1.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.3"><lt id="S1.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S1.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.3.2">absent</csymbol><ci id="S1.E1.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S1.E1.m1.2.2.cmml" xref="S1.E1.m1.2.2">ğ‘†</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E1.m1.3c">P(S)=\prod_{i=1}^{n}P(t_{&lt;i},S)</annotation><annotation encoding="application/x-llamapun" id="S1.E1.m1.3d">italic_P ( italic_S ) = âˆ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_P ( italic_t start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT , italic_S )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S1.SS1.p3" class="ltx_para ltx_noindent">
<p id="S1.SS1.p3.5" class="ltx_p">where <math id="S1.SS1.p3.1.m1.1" class="ltx_Math" alttext="t&lt;i" display="inline"><semantics id="S1.SS1.p3.1.m1.1a"><mrow id="S1.SS1.p3.1.m1.1.1" xref="S1.SS1.p3.1.m1.1.1.cmml"><mi id="S1.SS1.p3.1.m1.1.1.2" xref="S1.SS1.p3.1.m1.1.1.2.cmml">t</mi><mo id="S1.SS1.p3.1.m1.1.1.1" xref="S1.SS1.p3.1.m1.1.1.1.cmml">&lt;</mo><mi id="S1.SS1.p3.1.m1.1.1.3" xref="S1.SS1.p3.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.1.m1.1b"><apply id="S1.SS1.p3.1.m1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1"><lt id="S1.SS1.p3.1.m1.1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1.1"></lt><ci id="S1.SS1.p3.1.m1.1.1.2.cmml" xref="S1.SS1.p3.1.m1.1.1.2">ğ‘¡</ci><ci id="S1.SS1.p3.1.m1.1.1.3.cmml" xref="S1.SS1.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.1.m1.1c">t&lt;i</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p3.1.m1.1d">italic_t &lt; italic_i</annotation></semantics></math> stands for the sequence preceding the <math id="S1.SS1.p3.2.m2.1" class="ltx_Math" alttext="ith" display="inline"><semantics id="S1.SS1.p3.2.m2.1a"><mrow id="S1.SS1.p3.2.m2.1.1" xref="S1.SS1.p3.2.m2.1.1.cmml"><mi id="S1.SS1.p3.2.m2.1.1.2" xref="S1.SS1.p3.2.m2.1.1.2.cmml">i</mi><mo id="S1.SS1.p3.2.m2.1.1.1" xref="S1.SS1.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S1.SS1.p3.2.m2.1.1.3" xref="S1.SS1.p3.2.m2.1.1.3.cmml">t</mi><mo id="S1.SS1.p3.2.m2.1.1.1a" xref="S1.SS1.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S1.SS1.p3.2.m2.1.1.4" xref="S1.SS1.p3.2.m2.1.1.4.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.2.m2.1b"><apply id="S1.SS1.p3.2.m2.1.1.cmml" xref="S1.SS1.p3.2.m2.1.1"><times id="S1.SS1.p3.2.m2.1.1.1.cmml" xref="S1.SS1.p3.2.m2.1.1.1"></times><ci id="S1.SS1.p3.2.m2.1.1.2.cmml" xref="S1.SS1.p3.2.m2.1.1.2">ğ‘–</ci><ci id="S1.SS1.p3.2.m2.1.1.3.cmml" xref="S1.SS1.p3.2.m2.1.1.3">ğ‘¡</ci><ci id="S1.SS1.p3.2.m2.1.1.4.cmml" xref="S1.SS1.p3.2.m2.1.1.4">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.2.m2.1c">ith</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p3.2.m2.1d">italic_i italic_t italic_h</annotation></semantics></math> target word.
Hence each predicted word <math id="S1.SS1.p3.3.m3.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="S1.SS1.p3.3.m3.1a"><msub id="S1.SS1.p3.3.m3.1.1" xref="S1.SS1.p3.3.m3.1.1.cmml"><mi id="S1.SS1.p3.3.m3.1.1.2" xref="S1.SS1.p3.3.m3.1.1.2.cmml">t</mi><mi id="S1.SS1.p3.3.m3.1.1.3" xref="S1.SS1.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.3.m3.1b"><apply id="S1.SS1.p3.3.m3.1.1.cmml" xref="S1.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S1.SS1.p3.3.m3.1.1.1.cmml" xref="S1.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S1.SS1.p3.3.m3.1.1.2.cmml" xref="S1.SS1.p3.3.m3.1.1.2">ğ‘¡</ci><ci id="S1.SS1.p3.3.m3.1.1.3.cmml" xref="S1.SS1.p3.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.3.m3.1c">t_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p3.3.m3.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is based on the previously predicted word <math id="S1.SS1.p3.4.m4.1" class="ltx_Math" alttext="t_{i-1}" display="inline"><semantics id="S1.SS1.p3.4.m4.1a"><msub id="S1.SS1.p3.4.m4.1.1" xref="S1.SS1.p3.4.m4.1.1.cmml"><mi id="S1.SS1.p3.4.m4.1.1.2" xref="S1.SS1.p3.4.m4.1.1.2.cmml">t</mi><mrow id="S1.SS1.p3.4.m4.1.1.3" xref="S1.SS1.p3.4.m4.1.1.3.cmml"><mi id="S1.SS1.p3.4.m4.1.1.3.2" xref="S1.SS1.p3.4.m4.1.1.3.2.cmml">i</mi><mo id="S1.SS1.p3.4.m4.1.1.3.1" xref="S1.SS1.p3.4.m4.1.1.3.1.cmml">âˆ’</mo><mn id="S1.SS1.p3.4.m4.1.1.3.3" xref="S1.SS1.p3.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.4.m4.1b"><apply id="S1.SS1.p3.4.m4.1.1.cmml" xref="S1.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S1.SS1.p3.4.m4.1.1.1.cmml" xref="S1.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S1.SS1.p3.4.m4.1.1.2.cmml" xref="S1.SS1.p3.4.m4.1.1.2">ğ‘¡</ci><apply id="S1.SS1.p3.4.m4.1.1.3.cmml" xref="S1.SS1.p3.4.m4.1.1.3"><minus id="S1.SS1.p3.4.m4.1.1.3.1.cmml" xref="S1.SS1.p3.4.m4.1.1.3.1"></minus><ci id="S1.SS1.p3.4.m4.1.1.3.2.cmml" xref="S1.SS1.p3.4.m4.1.1.3.2">ğ‘–</ci><cn type="integer" id="S1.SS1.p3.4.m4.1.1.3.3.cmml" xref="S1.SS1.p3.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.4.m4.1c">t_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p3.4.m4.1d">italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> and the previous hidden states <math id="S1.SS1.p3.5.m5.1" class="ltx_Math" alttext="h_{i-1}" display="inline"><semantics id="S1.SS1.p3.5.m5.1a"><msub id="S1.SS1.p3.5.m5.1.1" xref="S1.SS1.p3.5.m5.1.1.cmml"><mi id="S1.SS1.p3.5.m5.1.1.2" xref="S1.SS1.p3.5.m5.1.1.2.cmml">h</mi><mrow id="S1.SS1.p3.5.m5.1.1.3" xref="S1.SS1.p3.5.m5.1.1.3.cmml"><mi id="S1.SS1.p3.5.m5.1.1.3.2" xref="S1.SS1.p3.5.m5.1.1.3.2.cmml">i</mi><mo id="S1.SS1.p3.5.m5.1.1.3.1" xref="S1.SS1.p3.5.m5.1.1.3.1.cmml">âˆ’</mo><mn id="S1.SS1.p3.5.m5.1.1.3.3" xref="S1.SS1.p3.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.5.m5.1b"><apply id="S1.SS1.p3.5.m5.1.1.cmml" xref="S1.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S1.SS1.p3.5.m5.1.1.1.cmml" xref="S1.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S1.SS1.p3.5.m5.1.1.2.cmml" xref="S1.SS1.p3.5.m5.1.1.2">â„</ci><apply id="S1.SS1.p3.5.m5.1.1.3.cmml" xref="S1.SS1.p3.5.m5.1.1.3"><minus id="S1.SS1.p3.5.m5.1.1.3.1.cmml" xref="S1.SS1.p3.5.m5.1.1.3.1"></minus><ci id="S1.SS1.p3.5.m5.1.1.3.2.cmml" xref="S1.SS1.p3.5.m5.1.1.3.2">ğ‘–</ci><cn type="integer" id="S1.SS1.p3.5.m5.1.1.3.3.cmml" xref="S1.SS1.p3.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.5.m5.1c">h_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p3.5.m5.1d">italic_h start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. However, when the sentences become long the performance deteriorates. This limitation is due to the limited feature representation ability in a fixed-length vector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>. To overcome this issue and to provide additional word alignment information in translating long sentences, Bahdanau et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite> introduced the idea of the attention mechanism. Concretely, attention mechanism is an intermediate component between encoder and decoder, which can help to determine the word alignment dynamically. The decoder pays attention to input or to any part of the input sentence. Attention is calculated using each encoder output and the current hidden state, resulting in a vector of the same size as the input sequences using score functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. There are mainly three different architectures for constructing NMT, namely Recurrent neural network (RNN), Convolution neural network (CNN), and Self-attention-based Transformer.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para ltx_noindent">
<p id="S1.SS1.p4.1" class="ltx_p">The use of RNN-based models has demonstrated good quality translation results. This type of network is composed of encoder and decoder with similar working of sequence-to-sequence learning. Multiple variant of RNN architectures include i.e., LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>, BiLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite> and GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para ltx_noindent">
<p id="S1.SS1.p5.1" class="ltx_p">The second approach of developing NMT systems based in convolution neural network (CNN) architecture. Work using CNN has generally reported good results, specially for word-based MT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. This work applied a convolution layer on the bottom of the recurrent layer which hinders the performance. The bottleneck was handled by implementing the fully convolutional model as suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>. The performance and accuracy were improved with a number of models; word-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>, character-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, and recently with attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para ltx_noindent">
<p id="S1.SS1.p6.1" class="ltx_p">Recently, the use of transformers has resulted in well-performing machine translation systems. This type of it is a sequence-to-sequence model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>, which consists of a stack of layers. Each layer first utilizes the self-attention to extract information from the whole sentence, then follows a point-wise feed-forward network to provide non-linearity. The novel idea of self-attention is to extend the mechanism to the processing of input sequences and output sentences as well. In general form, the Transformer attention function uses three vectors: queries(Q), keys (K) and values (V).</p>
</div>
<div id="S1.SS1.p7" class="ltx_para ltx_noindent">
<p id="S1.SS1.p7.2" class="ltx_p">The output is a weighted sum of values, where weights are computed by a similarity score between <math id="S1.SS1.p7.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.SS1.p7.1.m1.1a"><mi id="S1.SS1.p7.1.m1.1.1" xref="S1.SS1.p7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p7.1.m1.1b"><ci id="S1.SS1.p7.1.m1.1.1.cmml" xref="S1.SS1.p7.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p7.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p7.1.m1.1d">italic_n</annotation></semantics></math> query vectors and <math id="S1.SS1.p7.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S1.SS1.p7.2.m2.1a"><mi id="S1.SS1.p7.2.m2.1.1" xref="S1.SS1.p7.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p7.2.m2.1b"><ci id="S1.SS1.p7.2.m2.1.1.cmml" xref="S1.SS1.p7.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p7.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="S1.SS1.p7.2.m2.1d">italic_m</annotation></semantics></math> keys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>. The attention is defined as follows:</p>
</div>
<div id="S1.SS1.p8" class="ltx_para ltx_noindent">
<table id="S1.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E2.m1.6" class="ltx_Math" alttext="Attention(Q,K,V)=softmax(score(Q,K))V" display="block"><semantics id="S1.E2.m1.6a"><mrow id="S1.E2.m1.6.6" xref="S1.E2.m1.6.6.cmml"><mrow id="S1.E2.m1.6.6.3" xref="S1.E2.m1.6.6.3.cmml"><mi id="S1.E2.m1.6.6.3.2" xref="S1.E2.m1.6.6.3.2.cmml">A</mi><mo id="S1.E2.m1.6.6.3.1" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.3" xref="S1.E2.m1.6.6.3.3.cmml">t</mi><mo id="S1.E2.m1.6.6.3.1a" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.4" xref="S1.E2.m1.6.6.3.4.cmml">t</mi><mo id="S1.E2.m1.6.6.3.1b" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.5" xref="S1.E2.m1.6.6.3.5.cmml">e</mi><mo id="S1.E2.m1.6.6.3.1c" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.6" xref="S1.E2.m1.6.6.3.6.cmml">n</mi><mo id="S1.E2.m1.6.6.3.1d" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.7" xref="S1.E2.m1.6.6.3.7.cmml">t</mi><mo id="S1.E2.m1.6.6.3.1e" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.8" xref="S1.E2.m1.6.6.3.8.cmml">i</mi><mo id="S1.E2.m1.6.6.3.1f" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.9" xref="S1.E2.m1.6.6.3.9.cmml">o</mi><mo id="S1.E2.m1.6.6.3.1g" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.3.10" xref="S1.E2.m1.6.6.3.10.cmml">n</mi><mo id="S1.E2.m1.6.6.3.1h" xref="S1.E2.m1.6.6.3.1.cmml">â¢</mo><mrow id="S1.E2.m1.6.6.3.11.2" xref="S1.E2.m1.6.6.3.11.1.cmml"><mo stretchy="false" id="S1.E2.m1.6.6.3.11.2.1" xref="S1.E2.m1.6.6.3.11.1.cmml">(</mo><mi id="S1.E2.m1.1.1" xref="S1.E2.m1.1.1.cmml">Q</mi><mo id="S1.E2.m1.6.6.3.11.2.2" xref="S1.E2.m1.6.6.3.11.1.cmml">,</mo><mi id="S1.E2.m1.2.2" xref="S1.E2.m1.2.2.cmml">K</mi><mo id="S1.E2.m1.6.6.3.11.2.3" xref="S1.E2.m1.6.6.3.11.1.cmml">,</mo><mi id="S1.E2.m1.3.3" xref="S1.E2.m1.3.3.cmml">V</mi><mo stretchy="false" id="S1.E2.m1.6.6.3.11.2.4" xref="S1.E2.m1.6.6.3.11.1.cmml">)</mo></mrow></mrow><mo id="S1.E2.m1.6.6.2" xref="S1.E2.m1.6.6.2.cmml">=</mo><mrow id="S1.E2.m1.6.6.1" xref="S1.E2.m1.6.6.1.cmml"><mi id="S1.E2.m1.6.6.1.3" xref="S1.E2.m1.6.6.1.3.cmml">s</mi><mo id="S1.E2.m1.6.6.1.2" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.4" xref="S1.E2.m1.6.6.1.4.cmml">o</mi><mo id="S1.E2.m1.6.6.1.2a" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.5" xref="S1.E2.m1.6.6.1.5.cmml">f</mi><mo id="S1.E2.m1.6.6.1.2b" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.6" xref="S1.E2.m1.6.6.1.6.cmml">t</mi><mo id="S1.E2.m1.6.6.1.2c" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.7" xref="S1.E2.m1.6.6.1.7.cmml">m</mi><mo id="S1.E2.m1.6.6.1.2d" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.8" xref="S1.E2.m1.6.6.1.8.cmml">a</mi><mo id="S1.E2.m1.6.6.1.2e" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.9" xref="S1.E2.m1.6.6.1.9.cmml">x</mi><mo id="S1.E2.m1.6.6.1.2f" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mrow id="S1.E2.m1.6.6.1.1.1" xref="S1.E2.m1.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S1.E2.m1.6.6.1.1.1.2" xref="S1.E2.m1.6.6.1.1.1.1.cmml">(</mo><mrow id="S1.E2.m1.6.6.1.1.1.1" xref="S1.E2.m1.6.6.1.1.1.1.cmml"><mi id="S1.E2.m1.6.6.1.1.1.1.2" xref="S1.E2.m1.6.6.1.1.1.1.2.cmml">s</mi><mo id="S1.E2.m1.6.6.1.1.1.1.1" xref="S1.E2.m1.6.6.1.1.1.1.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.1.1.1.3" xref="S1.E2.m1.6.6.1.1.1.1.3.cmml">c</mi><mo id="S1.E2.m1.6.6.1.1.1.1.1a" xref="S1.E2.m1.6.6.1.1.1.1.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.1.1.1.4" xref="S1.E2.m1.6.6.1.1.1.1.4.cmml">o</mi><mo id="S1.E2.m1.6.6.1.1.1.1.1b" xref="S1.E2.m1.6.6.1.1.1.1.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.1.1.1.5" xref="S1.E2.m1.6.6.1.1.1.1.5.cmml">r</mi><mo id="S1.E2.m1.6.6.1.1.1.1.1c" xref="S1.E2.m1.6.6.1.1.1.1.1.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.1.1.1.6" xref="S1.E2.m1.6.6.1.1.1.1.6.cmml">e</mi><mo id="S1.E2.m1.6.6.1.1.1.1.1d" xref="S1.E2.m1.6.6.1.1.1.1.1.cmml">â¢</mo><mrow id="S1.E2.m1.6.6.1.1.1.1.7.2" xref="S1.E2.m1.6.6.1.1.1.1.7.1.cmml"><mo stretchy="false" id="S1.E2.m1.6.6.1.1.1.1.7.2.1" xref="S1.E2.m1.6.6.1.1.1.1.7.1.cmml">(</mo><mi id="S1.E2.m1.4.4" xref="S1.E2.m1.4.4.cmml">Q</mi><mo id="S1.E2.m1.6.6.1.1.1.1.7.2.2" xref="S1.E2.m1.6.6.1.1.1.1.7.1.cmml">,</mo><mi id="S1.E2.m1.5.5" xref="S1.E2.m1.5.5.cmml">K</mi><mo stretchy="false" id="S1.E2.m1.6.6.1.1.1.1.7.2.3" xref="S1.E2.m1.6.6.1.1.1.1.7.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S1.E2.m1.6.6.1.1.1.3" xref="S1.E2.m1.6.6.1.1.1.1.cmml">)</mo></mrow><mo id="S1.E2.m1.6.6.1.2g" xref="S1.E2.m1.6.6.1.2.cmml">â¢</mo><mi id="S1.E2.m1.6.6.1.10" xref="S1.E2.m1.6.6.1.10.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.E2.m1.6b"><apply id="S1.E2.m1.6.6.cmml" xref="S1.E2.m1.6.6"><eq id="S1.E2.m1.6.6.2.cmml" xref="S1.E2.m1.6.6.2"></eq><apply id="S1.E2.m1.6.6.3.cmml" xref="S1.E2.m1.6.6.3"><times id="S1.E2.m1.6.6.3.1.cmml" xref="S1.E2.m1.6.6.3.1"></times><ci id="S1.E2.m1.6.6.3.2.cmml" xref="S1.E2.m1.6.6.3.2">ğ´</ci><ci id="S1.E2.m1.6.6.3.3.cmml" xref="S1.E2.m1.6.6.3.3">ğ‘¡</ci><ci id="S1.E2.m1.6.6.3.4.cmml" xref="S1.E2.m1.6.6.3.4">ğ‘¡</ci><ci id="S1.E2.m1.6.6.3.5.cmml" xref="S1.E2.m1.6.6.3.5">ğ‘’</ci><ci id="S1.E2.m1.6.6.3.6.cmml" xref="S1.E2.m1.6.6.3.6">ğ‘›</ci><ci id="S1.E2.m1.6.6.3.7.cmml" xref="S1.E2.m1.6.6.3.7">ğ‘¡</ci><ci id="S1.E2.m1.6.6.3.8.cmml" xref="S1.E2.m1.6.6.3.8">ğ‘–</ci><ci id="S1.E2.m1.6.6.3.9.cmml" xref="S1.E2.m1.6.6.3.9">ğ‘œ</ci><ci id="S1.E2.m1.6.6.3.10.cmml" xref="S1.E2.m1.6.6.3.10">ğ‘›</ci><vector id="S1.E2.m1.6.6.3.11.1.cmml" xref="S1.E2.m1.6.6.3.11.2"><ci id="S1.E2.m1.1.1.cmml" xref="S1.E2.m1.1.1">ğ‘„</ci><ci id="S1.E2.m1.2.2.cmml" xref="S1.E2.m1.2.2">ğ¾</ci><ci id="S1.E2.m1.3.3.cmml" xref="S1.E2.m1.3.3">ğ‘‰</ci></vector></apply><apply id="S1.E2.m1.6.6.1.cmml" xref="S1.E2.m1.6.6.1"><times id="S1.E2.m1.6.6.1.2.cmml" xref="S1.E2.m1.6.6.1.2"></times><ci id="S1.E2.m1.6.6.1.3.cmml" xref="S1.E2.m1.6.6.1.3">ğ‘ </ci><ci id="S1.E2.m1.6.6.1.4.cmml" xref="S1.E2.m1.6.6.1.4">ğ‘œ</ci><ci id="S1.E2.m1.6.6.1.5.cmml" xref="S1.E2.m1.6.6.1.5">ğ‘“</ci><ci id="S1.E2.m1.6.6.1.6.cmml" xref="S1.E2.m1.6.6.1.6">ğ‘¡</ci><ci id="S1.E2.m1.6.6.1.7.cmml" xref="S1.E2.m1.6.6.1.7">ğ‘š</ci><ci id="S1.E2.m1.6.6.1.8.cmml" xref="S1.E2.m1.6.6.1.8">ğ‘</ci><ci id="S1.E2.m1.6.6.1.9.cmml" xref="S1.E2.m1.6.6.1.9">ğ‘¥</ci><apply id="S1.E2.m1.6.6.1.1.1.1.cmml" xref="S1.E2.m1.6.6.1.1.1"><times id="S1.E2.m1.6.6.1.1.1.1.1.cmml" xref="S1.E2.m1.6.6.1.1.1.1.1"></times><ci id="S1.E2.m1.6.6.1.1.1.1.2.cmml" xref="S1.E2.m1.6.6.1.1.1.1.2">ğ‘ </ci><ci id="S1.E2.m1.6.6.1.1.1.1.3.cmml" xref="S1.E2.m1.6.6.1.1.1.1.3">ğ‘</ci><ci id="S1.E2.m1.6.6.1.1.1.1.4.cmml" xref="S1.E2.m1.6.6.1.1.1.1.4">ğ‘œ</ci><ci id="S1.E2.m1.6.6.1.1.1.1.5.cmml" xref="S1.E2.m1.6.6.1.1.1.1.5">ğ‘Ÿ</ci><ci id="S1.E2.m1.6.6.1.1.1.1.6.cmml" xref="S1.E2.m1.6.6.1.1.1.1.6">ğ‘’</ci><interval closure="open" id="S1.E2.m1.6.6.1.1.1.1.7.1.cmml" xref="S1.E2.m1.6.6.1.1.1.1.7.2"><ci id="S1.E2.m1.4.4.cmml" xref="S1.E2.m1.4.4">ğ‘„</ci><ci id="S1.E2.m1.5.5.cmml" xref="S1.E2.m1.5.5">ğ¾</ci></interval></apply><ci id="S1.E2.m1.6.6.1.10.cmml" xref="S1.E2.m1.6.6.1.10">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E2.m1.6c">Attention(Q,K,V)=softmax(score(Q,K))V</annotation><annotation encoding="application/x-llamapun" id="S1.E2.m1.6d">italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n ( italic_Q , italic_K , italic_V ) = italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( italic_s italic_c italic_o italic_r italic_e ( italic_Q , italic_K ) ) italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S1.SS1.p9" class="ltx_para ltx_noindent">
<p id="S1.SS1.p9.1" class="ltx_p">where score Q,K is an nÃ—m matrix of similarity scores. A straightforward choice for scoreQ,K proposed by Luong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite> is the dot product i.e. score(Q,K) =QK. The softmax function normalizes over the columns of that matrix so that the weights for each query vector sum up to one. There are many variants in the implementation of attention-based models which are classified into two broad categories, global and local attention discussed in detail in this survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.SS1.p10" class="ltx_para ltx_noindent">
<p id="S1.SS1.p10.1" class="ltx_p">Current state-of-the-art NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite> rely on the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite> and multiple attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. However, the transformer-based language models such as Bidirectional Encoder Representation from Transformers (BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite> expands the function of attention to encompass the main task. It uses self-attention, which is applied to two states within the same sequence, as the foundation for sequence representations rather than an RNN. For Arabic language, two transformer-based language models have been developed so far; notably AraBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite> and GigaBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. Both models aim at solving a masked language-modelling task in order to correctly predict a masked word from its context. Besides, these models aim at resolving a next sentence prediction task especially to decide whether two sentences are consecutive or not.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Domain-specific MT</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p id="S1.SS2.p1.1" class="ltx_p">Domain translation is a challenging task due to the fact that language varies across different domains, genres, and styles. For example, texts in a financial domain often contain specific terminologies and jargon that may not be extensively used in legal or health domains. Therefore, researchers have proposed different methods to improve the quality of translations in domains such as medical and biomedical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>, <a href="#bib.bibx33" title="" class="ltx_ref">33</a>, <a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite>, legal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>]</cite>, and financial texts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">36</a>]</cite>. Several domain adaptation approaches have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">[</span>, for more comprehensive survey see]</cite>] saunders2022domain. Domain adaptation methods can intervene in various stages of NMT system design, training and use and can be classified into three main categories: data centeric methods, architecture-centric adaptation methods, and inference schemes for adaptation
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>.
In data centeric methods, the objective is to select or generate appropriate in-domain data. A large generic monolingual data can be filtered to select domain-representative dataset based on some unique characteristics of the target domain. However, selecting a small in-domain dataset may be more domain relevant, but the impact of any deviation from the target domain will be magnified <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite>. Another approach is to construct partially synthetic bilingual training corpora by forward- or back translation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">38</a>]</cite> observed that models trained exclusively on back translations can perform similarly to models trained on natural data. Recently, the use of pre-trained large language models (LLMs) to generate large amounts of synthetic data at very low cost has emerged to be an effective approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p id="S1.SS2.p2.1" class="ltx_p">Architecture-centric adaptation typically involves adding trainable parameters to pre-trained models to avoid train models from scratch. A common approach is to fine-tune an existing well-performing NMT model on small in-domain data. Extensive fine-tuning can lead to catastrophic forgetting. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">39</a>]</cite> proposed mixed-fine tuning which involves two steps: (1) training an NMT model on out-of-domain data until convergence and then (2) fine-tuning the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) until convergence. Mixed-fine tuning approaches can be helpful to prevent two major issues notably overlooking the specificity of each domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> and forgetting previously learned knowledge when exposed to the new training examples as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p id="S1.SS2.p3.1" class="ltx_p">Lastly, the inference schemes for adaptation develop a separate NMT model to each domain and combine them at inference time.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Domain-adaptation in Arabic MT</h3>

<div id="S1.SS3.p1" class="ltx_para ltx_noindent">
<p id="S1.SS3.p1.1" class="ltx_p">The development of Arabic MT systems has gone through different stages, including rule-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">41</a>, <a href="#bib.bibx42" title="" class="ltx_ref">42</a>]</cite>, statistical MT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">43</a>]</cite>, and more recently neural MT systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">44</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">45</a>]</cite> conducted a comprehensive survey of Arabic MT systems and the unique challenges in Arabic MT.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para ltx_noindent">
<p id="S1.SS3.p2.1" class="ltx_p">Arabic is one of the official six languages adopted by United Nations and it is spoken by 400 million people in the middle east, north Africa, and many other parts of the world. Arabic is a Semitic language and it is notoriously difficult for MT due to its linguistic characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">46</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">45</a>]</cite>. First, Arabic has a rich and complex morphology which is substantially different from English or other western languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx47" title="" class="ltx_ref">47</a>]</cite>. Second, Arabic has long and short vowels. While the long vowels are represented by letters, the short vowels are marked by diacritic signs placed above or below the letters. However, the use of diacritic signs is not compulsatory in Arabic and hence they are rarely used in informal writing. Therefore, it is hard to identify the correct sense of a word, especially when sufficient context is not provided. Third, variation among different Arabic dialects has always been problematic for AMT. Furthermore, the Arabic language used in social medial varies considerably from Modern Standard Arabic (MSA). These aspects of the Arabic language pose series challenges for Arabic MT.</p>
</div>
<div id="S1.SS3.p3" class="ltx_para ltx_noindent">
<p id="S1.SS3.p3.1" class="ltx_p">In addition to the aforementioned issues, there is a lack of high quality parallel corpora of sufficient size for training or fine-tuning Arabic MT for different domains. It is commonly known that NMT systems do not perform well in domain specific translation, specially in low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. Addressing these challenges, some researchers have turned to domain adaption methods to develop domain-specific Arabic MT systems. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> proposed the use of pre-trained LMs and back-translation for domain-specific data augmentation for MT.</p>
</div>
<div id="S1.SS3.p4" class="ltx_para ltx_noindent">
<p id="S1.SS3.p4.1" class="ltx_p">Furthermore, current Arabic MT research has primary focused on the translation of limited domains such as news and official texts, whilst few attempts focus on domain specific translation such as medical domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">48</a>]</cite>. Specifically, most of the used parallel data available to the researcher was limited to texts produced by international organizations, parliamentary debates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>. Unfortunately, existing single-domain AMT methods do not work well for multiple domains. Thus, multi-domain NMT approaches are more in demand to tackle this limitation.</p>
</div>
<div id="S1.SS3.p5" class="ltx_para ltx_noindent">
<p id="S1.SS3.p5.1" class="ltx_p">To recap, previous research has shown that domain adaptation leads to better translation quality than general NMT. Since there is relatively little work on Arabic domain adaptation, the primary objective of this research is to explore different effectiveness of domain translation methods, a yet unexplored domain, financial domain. To this end, this work aims to fine-tune several Transformer NMT models and LLM and perform cross-domain testing and evaluation to gain some insights into model robustness against domain changes.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">This section gives an overview of the methods and algorithms for AMT domain adaptation using LLMs models. First, information about the collected bilingual dataset used is given which we refer as authentic dataset, then our approach is presented, and lastly, the metrics we used for evaluation are described.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Approach</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In this work, we investigate mainly two methods to augment our in-domain data for the domain of financial news and propose approaches to leverage pre-trained LLMs for domain-specific data generation for this MT task. Concerning domain-specific data generation, we start with synthetic data generation to augment our authentic sentences for Arabic. Then, to obtain the parallel data in English, we apply back-translation from the Arabic synthetic sentences.
<br class="ltx_break">In our case, we leverage a pipeline of different models. We start by AraGPT2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">49</a>]</cite> and gpt2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">50</a>]</cite> as text generation models for Arabic and English to create synthetic pairs for (AR-EN) and (EN-AR), respectively. For Arabic, we use titles only from the collected authentic dataset as text prompts to generate corresponding long-form text using AraGTP2. Then, we use a summarization model to summarize the generated bunches of texts to obtain short summaries that will serve as generated titles. In the end, we back-translate the long-form text as well as the generated summaries to serve as title and article. The same pipeline applies to English as the target language.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Figure 1 shows the case of augmenting the authentic dataset with AR-EN pairs using this method.
</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="extracted/5128776/images/text_generation.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="431" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data augmentation pipeline</figcaption>
<br class="ltx_break ltx_centering">
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Synthetic data generation</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Data augmentation has been used in domain translation due to the scarcity of domain-specific datasets that are suitable for training large models. A common approach to augment domain data is the use of back-translation when there is abundant data in the target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">38</a>, <a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> proposed the use of state-of-the-art large language models to general unlimited new sentences in the source language and then back-translating in the target language. Recent studies explored the use of ChatGPT for generating new parallel sentences. However, in this study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx51" title="" class="ltx_ref">51</a>]</cite>, the authors showed that the performance of ChatGPT for Arabic shows inferior performance compared to the finetuned AraT5. 
<br class="ltx_break"></p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Authentic dataset statistics</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Articles</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Titles</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Sentences</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Tadawul</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">569</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">569</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">2544</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Capital Markets Authority</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_right">2320</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_right">2320</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_right">8351</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Eye of Riyadh</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_right">891</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_right">891</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_right">1877</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Total</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_right ltx_border_bb">3780</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb">3780</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb">15771</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Back-translation</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">We use a pre-trained machine translation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">52</a>]</cite> for back-translation. The back-translation is applied on both sides of generated summaries and titles, namely on the long-form text (which serves as an article) as well as on the summarized form (which serves as a title) into the respective target language.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Experiment setup</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Datasets</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">For fine-tuning in domain-specific MT models, we collected a dataset from different online resources for the pair AR-EN. As shown in table <a href="#S2.T1" title="Table 1 â€£ 2.1.1 Synthetic data generation â€£ 2.1 Approach â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> most of the data is collected from Capital Markets Authority (CMA) yielding a total of 7560 AR-EN pairs. Note that we consider titles (3780 AR-EN pairs) and articles (3780 AR-EN pairs). Additionally, we augmented our dataset with synthetic data as well as back-translated data. This step augmented the authentic dataset by 12,318 and 12,000 AR-EN sentence pairs as synthetic and back-translated data, respectively.

<br class="ltx_break">This table <a href="#S2.T2" title="Table 2 â€£ 2.2.1 Datasets â€£ 2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the breakdown of the segments in our dataset. We randomly sampled 1000 segments from the authentic dataset to serve as test data for all models. Additionally, we randomly sampled 1000 segments for building the development for both models notably for OPUS (bt-big) and NLLB. However, for fine-tuning chatGPT we randomly sampled 2000 pairs each for each setup.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Authentic dataset split and augmented data count</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Language pair</span></th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Fine-tuning</span></th>
<th id="S2.T2.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Dev</span></th>
<th id="S2.T2.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<th id="S2.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AR-EN</th>
<th id="S2.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Authentic</th>
<td id="S2.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">5560</td>
<td id="S2.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">1000</td>
<td id="S2.T2.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">1000</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<th id="S2.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AR-EN</th>
<th id="S2.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Synthetic</th>
<td id="S2.T2.1.3.2.3" class="ltx_td ltx_align_right">11318</td>
<td id="S2.T2.1.3.2.4" class="ltx_td ltx_align_right">1000</td>
<td id="S2.T2.1.3.2.5" class="ltx_td ltx_align_right">-</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<th id="S2.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">EN-AR</th>
<th id="S2.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Back-translated</th>
<td id="S2.T2.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb">11000</td>
<td id="S2.T2.1.4.3.4" class="ltx_td ltx_align_right ltx_border_bb">1000</td>
<td id="S2.T2.1.4.3.5" class="ltx_td ltx_align_right ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>NMT pre-trained models</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Our generic NMT pre-trained models use different Transformer architectures, however, we have implemented the fine-tuning objective using the huggingface NMT transformer (a sequence-to-sequence version in the Transformers library) procedure.
For inference, we use beam size 4 and batch size 16, on a GPU T4-15GB (Google Colab). Further, we use chatGPT as a baseline with zero-shot learning.
OPUS (bt-big):
We use OPUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">53</a>]</cite> models from the Tatoeba-Challenge, specifically
the models augmented with back-translated data of Wikimedia content and trained with Transformer-Big architecture. Here we picked the <span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Helsinki-NLP/opus-mt-ar-en</span> checkpoint. For tokenization, we instantiate our tokenizer which is based on SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx54" title="" class="ltx_ref">54</a>]</cite>
with the <span id="S2.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">AutoTokenizer.from_pretrained</span> method. This ensures that the tokenizer corresponds to the model architecture we want to use.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">NLLB:
No-Language-Left-Behind (NLLB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">55</a>]</cite> is a multilingual model which supports 200 languages with a massive size Transformer. Fine-tuning is carried out on NLLB using its distilled version <span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">facebook/nllb-200-distilled-600M</span> checkpoint. For tokenization, we instantiate a multilingual model provided by NLLB for tokenization with the <span id="S2.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">NllbTokenizerFast.from_pretrained</span> method. This ensures that the tokenizer corresponds to the model architecture we are using.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">ChatGPT3.5:
We use the chatGPT-3.5-turbo model via its official API <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://chat.openai.com</span></span></span> which power the ChatGPT. Here we prepare our dataset in the format that is acceptabted by the API functions. In particular we convert the AR-EN pairs into the Prompt template for sentence-level translation as recommended in the OpenAI playground for sentence-level translation task. In order to avoid error, we truncate all the sentence pairs to a max size of 4,290 characters before sending the request. Moreover, we set the size of the total tokens to about 378,460 tokens due to limit rate costs. For this model, we formatted the requests with as the system message first <span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_italic">â€™You are a professional translator in the financial domain. Translate the following Arabic sentence: </span> ar_en <span id="S2.SS2.SSS2.p3.1.2" class="ltx_text ltx_font_italic">into Englishâ€™</span> followed by user content messages, where ar_en represent the AR-EN pairs.</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p">Before, starting with the experiments, we considered the following three setups for fine-tuning the models on the domain-specific dataset. Next section 3 will discuss the results and findings.
Setup #1: Baseline models
<br class="ltx_break ltx_align_left">We consider pre-trained NMT models evaluated on our cleaned authentic test split containing 1000 AR-EN sentence pairs. Our baseline NMT models use the OPUS(bt-big) <span id="footnote2" class="ltx_note ltx_align_left ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/ara-eng</span></span></span> <cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">52</a>]</cite>, NLLB 600M <span id="footnote3" class="ltx_note ltx_align_left ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/facebook/nllb-200-distilled-600M</span></span></span> <cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">55</a>]</cite> and ChatGPT-3.5 <span id="footnote4" class="ltx_note ltx_align_left ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://platform.openai.com/docs/guides/gpt/chat-completions-api</span></span></span>.
Setup #2: Fine-tuning with authentic data
<br class="ltx_break ltx_align_left">For fine-tuning, we have initialized the transformer models with the trained weights of the baselines. We use our authentic dataset with the splits shown in table <a href="#S2.T2" title="Table 2 â€£ 2.2.1 Datasets â€£ 2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref ltx_align_left"><span class="ltx_text ltx_ref_tag">2</span></a>. We have kept all hyperparameters identical. The models have been fine-tuned until convergence over the validation set. At test time, the respective testset from the authentic dataset is used for this setup as well. Again, all metrics are reported.
Setup #3: Fine-tuning with augmented data
<br class="ltx_break">Similar to previous setup, we have initialized the transformer models with the trained weights of the baselines. However, here we use our authentic dataset augmented with the respective data with the splits shown in table <a href="#S2.T2" title="Table 2 â€£ 2.2.1 Datasets â€£ 2.2 Experiment setup â€£ 2 Methodology â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Basically, we augment the authentic dataset with back-translated data and shuffle it. The same applies for synthetic data. This step yield two version of fine-tuning, one using the former and one using the latter.
The models have been fine-tuned until convergence over the validation set. At test time, the testset from the authentic dataset is used for this setup as well while also reporting all metrics.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Metrics</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">As performance measures, we report the spBLEU score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite> which uses a SentencePiece tokenizer, chrF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx56" title="" class="ltx_ref">56</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx57" title="" class="ltx_ref">57</a>]</cite>, which are implemented in sacrebleu <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/mjpost/sacreBLEU</span></span></span>. Additionally, we compute COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">58</a>]</cite> that was proposed recently by taking advantage of cross-lingual pre-trained LMs using knowledge from both source and target languages. COMET make prediction score that correlates with human judgement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">58</a>]</cite>. For our experiments, we adapt the official COMET implementation <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/Unbabel/COMET</span></span></span>. For COMET, we use the reference-based Estimation model <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">wmt20-comet-da</span>, trained based on (Direct Assessment) DA and used Quality Estimation (QE). Another score that correlates with human evaluation BERTScore
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">59</a>]</cite> is also computed. Including different metrics in the evaluation allows us to test the models on metrics different from those used for training.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">This section elaborates on our automatic and human evaluations and discusses the results. We also provide a preliminary comparison of the modelsâ€™ performance on domain-specific MT as baseline models and as fine-tuned models. Therefore, we report if they can perform robustly well on domain-specific or even noisy sentences from our collected dataset. Specifically, we focus on the translation robustness of the models on the translation of Arabic financial news.
Table <a href="#S3.T3" title="Table 3 â€£ 3.1 Automatic evaluation â€£ 3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the main results over the respective testset. The â†‘ and â†“ symbols in the tables indicate which values are better. We analyze the translation outputs by comparing the MT evaluation metrics in each setup.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Automatic evaluation</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In setup #1, OPUS and NLLB perform equally with inferior performances of around 14 and 42 for BLEU and chrF points, respectively. The TER score which is expressed as the ratio of the number of edits to the average number of words in the reference is high for the two models. Thus it indicates that the translation is of poor quality.
In terms of COMET score, both models have very poor results which means reference-based COMET may lose information from source, translation output, or reference embeddings, except for ChatGPT-3.5. But, BERTScores for all three models are high which means that they donâ€™t correlate with COMET score. In comparison, BERTScore and COMET have a significant difference in their scores.
In contrast, ChatGPT-3.5 performs competitively better (BLEU 26.13) than OPUS and NLLB models. Indeed, we are not surprised by this fact which is in-line with related research works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">60</a>, <a href="#bib.bibx51" title="" class="ltx_ref">51</a>, <a href="#bib.bibx61" title="" class="ltx_ref">61</a>]</cite>. However, these findings are not consistent with a previous finding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx62" title="" class="ltx_ref">62</a>]</cite> where the authors evaluated ChatGPT and GPT on 4,000 Arabic-English pairs and found out that SoTA models like araT5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">63</a>]</cite> outperforms ChatGPT by 19 BLEU Points.
As for the translation robustness, results from this setup#1 suggest that ChatGPT-3.5 outperforms these models on financial news by a significant margin. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">When, we analyze setup #2, as expected, fine-tuning all models on authentic data has generally helped improve the BLEU scores and other metrics as well. This finding is also in-line with othersâ€™ research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx54" title="" class="ltx_ref">54</a>, <a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>. However, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">60</a>]</cite> notice that for domain-specific translation (e.g., in the biomedical filed), ChatGPTâ€™s performance degrades considerably. We attribute this behaviour to the observation that ChatGPT is capable to translate our sentences better than terminologies in sentences from the biomedical domain, a very specific domain. Furthermore, we clearly, see that BLEU scores increase from 14.58 to 48.83, from 14.38 to 43.43 and from 26.13 to 51.15 for OPUS, NLLB and chatGPT-3.5, respectively. In terms of COMET and BERTScore, both metrics correlate. This indicates acceptable translation outputs.
<br class="ltx_break">Concerning ChatGPT, even though it only used 2000 pairs of AR-EN sentences for fine-tuning, it outperforms all other models which means the MT quality of chatGPT can easily be improved with little additional data from the language pair, a fact that has not been previously confirmed for related approaches, since this is the first work that assesses the performance of ChatGPT fine-tuned models for AR-EN MT task. Nevertheless, for English, this work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx64" title="" class="ltx_ref">64</a>]</cite> has shown that ChatGPT has great robust translation capabilities over related SoTA MT models. Our experimental result confirms the latter finding and shows that with a carefully prepared certain amount of fine-tuning data, this model is capable to create acceptable translations. As for the translation robustness, results from this setup#2 suggest that ChatGPT-3.5 performs competitively well on financial news. Regarding the human evaluation, all models in this setup reached possible and acceptable translations. We conclude, our experiment shows that providing in-domain examples to ChatGPT achieves comparable results to a SoTA model in terms of automatic and human evaluation.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">In setup #3 we fine-tune the baseline models with the augmented data in two versions, one using back-translated data and the other using synthetic data. We observe both lexical metrics (BLEU and chrF) show consistent degradation with all models. The same applies to TER score.
For instance, for ChatGPT, BLEU score decreased dramatically from 51.15 to 34.67 when fine-tuned on synthetic data while maintaining an acceptable score (BLEU 45.38) when fine-tuned on back-translated data. We observe that COMET score degraded massively for ChatGPT more than for OPUS and NLLB. One explanation could be that the synthetic data may have a lot of generated tokens that are grammatically correct, but they have nonsense meaning, as we know from the current state of the generative text. This could indicate that the translation results are not close in embedding space with the source and reference. In contrast, BERTScore maintained a good score over the two versions for all models.
In this setup, OPUS (bt-big) FT (back MT) has made it the best model that provides reasonably good scores translations, however still lags behind the OPUS model fine-tuned on authentic data by at least 1.3 BLEU points.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">Generally, the drop in performance for all models in this setup is not consistent with othersâ€™ research. For instance, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> used synthetic data in the healthcare domain and achieved improvements on the in-domain test set. In comparison, with this work, the authors applied synthetic data generation using mGPT <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://huggingface.co/sberbank-ai/mGPT</span></span></span> a multilingual language model. We argue that this model might have better perplexity in generated tokens compared to araGPT2. To the best of our knowledge, we did not find any research work investigating the performance of both models in regard to Arabic. We will further investigate this issue in future work. However, there are many general reasons explaining OPUS, NLLB and ChatGPT behaviour in domain-specific MT, especially in the case of augmenting the dataset with synthetic data. One explanation is that the use of synthetic data may cause incorrect token choices, grammatical errors, or unnatural sentence structures to propagate into the translation outputs which make suboptimal translation outputs.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p">Indeed, the results of this study demonstrate the modelsâ€™ robust translation capabilities for in-domain adaptation. They perform well when fine-tuned on authentic data. However, we observe a discrepancy between COMET and BERTScore. For instance, ChatGPT-3.5 perform worse on augmented data yielding a lower COMET score (23.03) but still having high BERTScore (0.91). This behaviour seems uncommon. One possible explanation is that COMET with reference-based translation is failing to find closeness in all three resource embeddings, whereas BERTScore is able the find closeness in the similarity between an MT output and a reference translation.
This behaviour encourages us to drive human evaluation a much-needed score for trustworthiness.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p">Regarding the human scoring, we observe that in setup #1 and #2, where ChatGPT-3.5 made it the best model in terms of lexical and semantic metrics, the human evaluation supports this result with the highest score of 3.1.

<br class="ltx_break">However, in setup #3, even though the automatic metrics are degraded for all models, except BERTScore, the human evaluation shows that the translation quality of all models is comparable.
Thus, we find that BERTScore correlates with human judgment more than COMET which has become recently a new state-of-the-art level of correlation with human judgment. This finding opens a great investigation for the future into whether semantic metrics correlate with human judgment and to what extent, in particular when ChatGPT-3.5 is applied. Figure <a href="#S3.F2" title="Figure 2 â€£ 3.1 Automatic evaluation â€£ 3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> elaborates on all the automatic evaluation and human results.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>MT evaluation scores and human evaluation for AR-EN Test dataset (1000 pairs). The best scores are in <span id="S3.T3.2.1" class="ltx_text ltx_font_bold">bold</span>.</figcaption>
<table id="S3.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.3.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S3.T3.3.1.1.1.1" class="ltx_text ltx_font_bold">Setups</span></th>
<td id="S3.T3.3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T3.3.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.3.1" class="ltx_text ltx_font_bold">spBLEU â†‘</span></td>
<td id="S3.T3.3.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.4.1" class="ltx_text ltx_font_bold">chrF â†‘</span></td>
<td id="S3.T3.3.1.1.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.5.1" class="ltx_text ltx_font_bold">TER â†“</span></td>
<td id="S3.T3.3.1.1.6" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.6.1" class="ltx_text ltx_font_bold">COMET â†‘</span></td>
<td id="S3.T3.3.1.1.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.7.1" class="ltx_text ltx_font_bold">BERTScore â†‘</span></td>
<td id="S3.T3.3.1.1.8" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.3.1.1.8.1" class="ltx_text ltx_font_bold">Human â†‘</span></td>
</tr>
<tr id="S3.T3.3.2.2" class="ltx_tr">
<th id="S3.T3.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S3.T3.3.2.2.1.1" class="ltx_text">1</span></th>
<td id="S3.T3.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t">OPUS (bt-big <span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://huggingface.co/Helsinki-NLP/opus-mt-ar-en</span></span></span>)</td>
<td id="S3.T3.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t">14.58</td>
<td id="S3.T3.3.2.2.4" class="ltx_td ltx_align_left ltx_border_t">43.93</td>
<td id="S3.T3.3.2.2.5" class="ltx_td ltx_align_left ltx_border_t">79.59</td>
<td id="S3.T3.3.2.2.6" class="ltx_td ltx_align_left ltx_border_t">3.89</td>
<td id="S3.T3.3.2.2.7" class="ltx_td ltx_align_left ltx_border_t">0.89</td>
<td id="S3.T3.3.2.2.8" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S3.T3.3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.3.1" class="ltx_td ltx_align_left">NLLB 600M <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://huggingface.co/facebook/nllb-200-distilled-600M</span></span></span>
</td>
<td id="S3.T3.3.3.3.2" class="ltx_td ltx_align_left">14.38</td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_left">42.17</td>
<td id="S3.T3.3.3.3.4" class="ltx_td ltx_align_left">77.58</td>
<td id="S3.T3.3.3.3.5" class="ltx_td ltx_align_left">2.98</td>
<td id="S3.T3.3.3.3.6" class="ltx_td ltx_align_left">0.89</td>
<td id="S3.T3.3.3.3.7" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S3.T3.3.4.4" class="ltx_tr">
<td id="S3.T3.3.4.4.1" class="ltx_td ltx_align_left">ChatGPT-3.5</td>
<td id="S3.T3.3.4.4.2" class="ltx_td ltx_align_left"><span id="S3.T3.3.4.4.2.1" class="ltx_text ltx_font_bold">26.13</span></td>
<td id="S3.T3.3.4.4.3" class="ltx_td ltx_align_left"><span id="S3.T3.3.4.4.3.1" class="ltx_text ltx_font_bold">60.98</span></td>
<td id="S3.T3.3.4.4.4" class="ltx_td ltx_align_left"><span id="S3.T3.3.4.4.4.1" class="ltx_text ltx_font_bold">66.83</span></td>
<td id="S3.T3.3.4.4.5" class="ltx_td ltx_align_left"><span id="S3.T3.3.4.4.5.1" class="ltx_text ltx_font_bold">33.7</span></td>
<td id="S3.T3.3.4.4.6" class="ltx_td ltx_align_left"><span id="S3.T3.3.4.4.6.1" class="ltx_text ltx_font_bold">0.91</span></td>
<td id="S3.T3.3.4.4.7" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S3.T3.3.5.5" class="ltx_tr">
<th id="S3.T3.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S3.T3.3.5.5.1.1" class="ltx_text">2</span></th>
<td id="S3.T3.3.5.5.2" class="ltx_td ltx_align_left ltx_border_t">OPUS (bt-big) FT <span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://huggingface.co/asas-ai/opus-mt-ar-en-finetuned-ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.5.5.3" class="ltx_td ltx_align_left ltx_border_t">48.83</td>
<td id="S3.T3.3.5.5.4" class="ltx_td ltx_align_left ltx_border_t">65.11</td>
<td id="S3.T3.3.5.5.5" class="ltx_td ltx_align_left ltx_border_t">53.18</td>
<td id="S3.T3.3.5.5.6" class="ltx_td ltx_align_left ltx_border_t">51.12</td>
<td id="S3.T3.3.5.5.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.5.5.7.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S3.T3.3.5.5.8" class="ltx_td ltx_align_left ltx_border_t">2.7</td>
</tr>
<tr id="S3.T3.3.6.6" class="ltx_tr">
<td id="S3.T3.3.6.6.1" class="ltx_td ltx_align_left">NLLB 600M FT <span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://huggingface.co/asas-ai/nllb-200-distilled-600M-finetuned-ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.6.6.2" class="ltx_td ltx_align_left">43.43</td>
<td id="S3.T3.3.6.6.3" class="ltx_td ltx_align_left">61.01</td>
<td id="S3.T3.3.6.6.4" class="ltx_td ltx_align_left">54.65</td>
<td id="S3.T3.3.6.6.5" class="ltx_td ltx_align_left"><span id="S3.T3.3.6.6.5.1" class="ltx_text ltx_font_bold">52.10</span></td>
<td id="S3.T3.3.6.6.6" class="ltx_td ltx_align_left">0.94</td>
<td id="S3.T3.3.6.6.7" class="ltx_td ltx_align_left">2.81</td>
</tr>
<tr id="S3.T3.3.7.7" class="ltx_tr">
<td id="S3.T3.3.7.7.1" class="ltx_td ltx_align_left">ChatGPT-3.5 FT</td>
<td id="S3.T3.3.7.7.2" class="ltx_td ltx_align_left"><span id="S3.T3.3.7.7.2.1" class="ltx_text ltx_font_bold">51.15</span></td>
<td id="S3.T3.3.7.7.3" class="ltx_td ltx_align_left"><span id="S3.T3.3.7.7.3.1" class="ltx_text ltx_font_bold">71.28</span></td>
<td id="S3.T3.3.7.7.4" class="ltx_td ltx_align_left"><span id="S3.T3.3.7.7.4.1" class="ltx_text ltx_font_bold">46.47</span></td>
<td id="S3.T3.3.7.7.5" class="ltx_td ltx_align_left">42.90</td>
<td id="S3.T3.3.7.7.6" class="ltx_td ltx_align_left">0.94</td>
<td id="S3.T3.3.7.7.7" class="ltx_td ltx_align_left">3.1</td>
</tr>
<tr id="S3.T3.3.8.8" class="ltx_tr">
<th id="S3.T3.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="6"><span id="S3.T3.3.8.8.1.1" class="ltx_text">3</span></th>
<td id="S3.T3.3.8.8.2" class="ltx_td ltx_align_left ltx_border_t">OPUS (bt-big) FT (back MT)<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://huggingface.co/asas-ai/opus-mt-ar-en-finetuned_augmented_MT-ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.8.8.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.8.8.3.1" class="ltx_text ltx_font_bold">47.56</span></td>
<td id="S3.T3.3.8.8.4" class="ltx_td ltx_align_left ltx_border_t">64.53</td>
<td id="S3.T3.3.8.8.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.8.8.5.1" class="ltx_text ltx_font_bold">54.30</span></td>
<td id="S3.T3.3.8.8.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.8.8.6.1" class="ltx_text ltx_font_bold">57.21</span></td>
<td id="S3.T3.3.8.8.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.8.8.7.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S3.T3.3.8.8.8" class="ltx_td ltx_align_left ltx_border_t">2.94</td>
</tr>
<tr id="S3.T3.3.9.9" class="ltx_tr">
<td id="S3.T3.3.9.9.1" class="ltx_td ltx_align_left">OPUS (bt-big) FT (synthetic) <span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://huggingface.co/asas-ai/opus-mt-ar-en-finetuned_augmented_synthetic-ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.9.9.2" class="ltx_td ltx_align_left">40.67</td>
<td id="S3.T3.3.9.9.3" class="ltx_td ltx_align_left">57.87</td>
<td id="S3.T3.3.9.9.4" class="ltx_td ltx_align_left">60.46</td>
<td id="S3.T3.3.9.9.5" class="ltx_td ltx_align_left">49.71</td>
<td id="S3.T3.3.9.9.6" class="ltx_td ltx_align_left">0.94</td>
<td id="S3.T3.3.9.9.7" class="ltx_td ltx_align_left">2.67</td>
</tr>
<tr id="S3.T3.3.10.10" class="ltx_tr">
<td id="S3.T3.3.10.10.1" class="ltx_td ltx_align_left ltx_border_t">NLLB 600M FT (back MT)<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>https://huggingface.co/asas-ai/nllb-200-distilled-600M-finetuned_augmented_MT_ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.10.10.2" class="ltx_td ltx_align_left ltx_border_t">43.38</td>
<td id="S3.T3.3.10.10.3" class="ltx_td ltx_align_left ltx_border_t">60.92</td>
<td id="S3.T3.3.10.10.4" class="ltx_td ltx_align_left ltx_border_t">54.63</td>
<td id="S3.T3.3.10.10.5" class="ltx_td ltx_align_left ltx_border_t">52.77</td>
<td id="S3.T3.3.10.10.6" class="ltx_td ltx_align_left ltx_border_t">0.94</td>
<td id="S3.T3.3.10.10.7" class="ltx_td ltx_align_left ltx_border_t">2.67</td>
</tr>
<tr id="S3.T3.3.11.11" class="ltx_tr">
<td id="S3.T3.3.11.11.1" class="ltx_td ltx_align_left">NLLB 600M FT (synthetic) <span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>https://huggingface.co/asas-ai/nllb-200-distilled-600M-finetuned_augmented_synthetic_ar-to-en</span></span></span>
</td>
<td id="S3.T3.3.11.11.2" class="ltx_td ltx_align_left">40.77</td>
<td id="S3.T3.3.11.11.3" class="ltx_td ltx_align_left">58.26</td>
<td id="S3.T3.3.11.11.4" class="ltx_td ltx_align_left">57.48</td>
<td id="S3.T3.3.11.11.5" class="ltx_td ltx_align_left">49.44</td>
<td id="S3.T3.3.11.11.6" class="ltx_td ltx_align_left">0.94</td>
<td id="S3.T3.3.11.11.7" class="ltx_td ltx_align_left">2.85</td>
</tr>
<tr id="S3.T3.3.12.12" class="ltx_tr">
<td id="S3.T3.3.12.12.1" class="ltx_td ltx_align_left ltx_border_t">ChatGPT-3.5 FT (back MT)</td>
<td id="S3.T3.3.12.12.2" class="ltx_td ltx_align_left ltx_border_t">45.07</td>
<td id="S3.T3.3.12.12.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.3.12.12.3.1" class="ltx_text ltx_font_bold">67.64</span></td>
<td id="S3.T3.3.12.12.4" class="ltx_td ltx_align_left ltx_border_t">55.07</td>
<td id="S3.T3.3.12.12.5" class="ltx_td ltx_align_left ltx_border_t">33.55</td>
<td id="S3.T3.3.12.12.6" class="ltx_td ltx_align_left ltx_border_t">0.93</td>
<td id="S3.T3.3.12.12.7" class="ltx_td ltx_align_left ltx_border_t">2.93</td>
</tr>
<tr id="S3.T3.3.13.13" class="ltx_tr">
<td id="S3.T3.3.13.13.1" class="ltx_td ltx_align_left">ChatGPT-3.5 FT (synthetic)</td>
<td id="S3.T3.3.13.13.2" class="ltx_td ltx_align_left">34.67</td>
<td id="S3.T3.3.13.13.3" class="ltx_td ltx_align_left">62.93</td>
<td id="S3.T3.3.13.13.4" class="ltx_td ltx_align_left">70.29</td>
<td id="S3.T3.3.13.13.5" class="ltx_td ltx_align_left">23.03</td>
<td id="S3.T3.3.13.13.6" class="ltx_td ltx_align_left">0.91</td>
<td id="S3.T3.3.13.13.7" class="ltx_td ltx_align_left">2.77</td>
</tr>
<tr id="S3.T3.3.14.14" class="ltx_tr">
<th id="S3.T3.3.14.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T3.3.14.14.2" class="ltx_td ltx_align_left ltx_border_tt">FT = Fine-tuned</td>
<td id="S3.T3.3.14.14.3" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.3.14.14.4" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.3.14.14.5" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.3.14.14.6" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.3.14.14.7" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.3.14.14.8" class="ltx_td ltx_border_tt"></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<figure id="S3.F1.sf1" class="ltx_figure ltx_flex_size_1 ltx_align_center"><img src="extracted/5128776/images/Setup_1.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Setup #1</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5128776/images/Setup_2.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Setup #2</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F1.sf3" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="extracted/5128776/images/Setup_3.png" id="S3.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Setup #3</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Plotting the performance of the three models across different setups</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Human evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In addition to the automatic evaluations reported above, we decided to assess the quality of our modelsâ€™ translations using human evaluation. To this end, we recruited three native speakers and domain experts (post-graduate students in finance) to rate the acceptability of 50 randomly selected sentences from the test set. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>, we conducted a bilingual evaluation, whereby the evaluators rated both the original source sentences and translations generated by the MT models. The human evaluators were asked to rate each of the sentence based on the scale proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx65" title="" class="ltx_ref">65</a>]</cite>, ranging from 1 to 4, and outlined as follows:
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">4 = Ideal: Not necessarily a perfect translation, but grammatically correct, with all information
accurately transferred.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">3 = Acceptable: Not perfect (stylistically or grammatically odd), but definitely comprehensible,
AND with accurate transfer of all important information.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">2 = Possibly Acceptable: Possibly comprehensible (given enough context and/or time to work it
out); some information transferred accurately.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i4.p1.1" class="ltx_p">1 = Unacceptable: Absolutely not comprehensible and/or little or no information is accurately
transferred.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">We first asked the three human evaluators to rate one modelâ€™s output and then we conducted an inter-rater reliability analysis on their ratings. The result of weighted Cohenâ€™s Kappa is X. Then, we asked each rater rate there modelsâ€™ outputs and provide justification for their responses were "Ideal" or "Unacceptable." The mean value of the ratersâ€™ scores was averaged for each system, as shown in table <a href="#S3.T3" title="Table 3 â€£ 3.1 Automatic evaluation â€£ 3 Results and Discussion â€£ domain adaptation for Arabic machine translation: the case of financial texts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this paper, we conducted several experiments to
assess the performance of pre-trained NMT and LLM like GPT-3.5 using data augmentation in the domain of Arabic financial news articles. Generally, the results obtained from these experiments
are very promising. While ChatGPT shows good
results using few pairs, other models need more examples and still have lower performance.
We explored the effectiveness of all models using data augmentation in the financial domain and found that the MT quality decreased for all models adequately. Here ChatGPT shows inferior performance, while OPUS still performs well on back-translated data than on synthetic data.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">There are many future works that can be carried out based on the findings from this study. Firstly, we would like to explore new techniques and methods to enhance translation outputs rather than the approach of data augmentation. Secondly, we think it is valuable
to integrate more high-performance automatic
metrics into the comparison that take semantics into consideration in a better way than in COMET and BERTScore.
Finally, we will explore novel approaches to integrate additional models or even incorporate domain-specific models for improved translation performance.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Acknowledgements</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">This work is supported by a research grant from the Saudi Ministry of Culture.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Philipp Koehn and Rebecca Knowles
</span>
<span class="ltx_bibblock">â€œSix challenges for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03872</em>, 2017
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Hal DaumÃ© Iii and Jagadeesh Jagarlamudi
</span>
<span class="ltx_bibblock">â€œDomain adaptation for machine translation by mining unseen wordsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, 2011, pp. 407â€“412
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Danielle Saunders
</span>
<span class="ltx_bibblock">â€œDomain adaptation and multi-domain adaptation for neural machine translation: A surveyâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Journal of Artificial Intelligence Research</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">75</span>, 2022, pp. 351â€“424
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Chenhui Chu and Rui Wang
</span>
<span class="ltx_bibblock">â€œA survey of domain adaptation for machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">Journal of information processing</em> <span id="bib.bibx4.2.2" class="ltx_text ltx_font_bold">28</span>
</span>
<span class="ltx_bibblock">Information Processing Society of Japan, 2020, pp. 413â€“426
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Yasmin Moslem, Rejwanul Haque and Andy Way
</span>
<span class="ltx_bibblock">â€œAdaptive machine translation with large language modelsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.13294</em>, 2023
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Martin Popel et al.
</span>
<span class="ltx_bibblock">â€œTransforming machine translation: a deep learning system reaches news translation quality comparable to human professionalsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Nature communications</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">11.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 2020, pp. 1â€“15
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Arwa Hatem and Nazlia Omar
</span>
<span class="ltx_bibblock">â€œSyntactic reordering for Arabic-English phrase-based machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Database theory and application, bio-science and bio-technology</em>
</span>
<span class="ltx_bibblock">Springer, 2010, pp. 198â€“206
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Zakaryia Mustafa Almahasees
</span>
<span class="ltx_bibblock">â€œAssessment of Google and Microsoft Bing translation of journalistic textsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">International Journal of Languages, Literature and Linguistics</em> <span id="bib.bibx8.2.2" class="ltx_text ltx_font_bold">4.3</span>, 2018, pp. 231â€“235
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Rico Sennrich, Barry Haddow and Alexandra Birch
</span>
<span class="ltx_bibblock">â€œNeural machine translation of rare words with subword unitsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.07909</em>, 2015
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Marta R Costa-Jussa and JosÃ© AR Fonollosa
</span>
<span class="ltx_bibblock">â€œCharacter-based neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1603.00810</em>, 2016
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Minh-Thang Luong and Christopher D Manning
</span>
<span class="ltx_bibblock">â€œAchieving open vocabulary neural machine translation with hybrid word-character modelsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.00788</em>, 2016
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Mathias MÃ¼ller, Annette Rios and Rico Sennrich
</span>
<span class="ltx_bibblock">â€œDomain robustness in neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.03109</em>, 2019
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Mai Oudah, Amjad Almahairi and Nizar Habash
</span>
<span class="ltx_bibblock">â€œThe impact of preprocessing on Arabic-English statistical and neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.11751</em>, 2019
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Yasmin Moslem, Rejwanul Haque, John D Kelleher and Andy Way
</span>
<span class="ltx_bibblock">â€œDomain-Specific Text Generation for Machine Translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.05909</em>, 2022
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu
</span>
<span class="ltx_bibblock">â€œBLEU: a method for automatic evaluation of machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting on association for computational linguistics</em>, 2002, pp. 311â€“318
</span>
<span class="ltx_bibblock">Association for Computational Linguistics
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Ankur Bapna, Naveen Arivazhagan and Orhan Firat
</span>
<span class="ltx_bibblock">â€œSimple, scalable adaptation for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08478</em>, 2019
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Shuoheng Yang, Yuxin Wang and Xiaowen Chu
</span>
<span class="ltx_bibblock">â€œA survey of deep learning techniques for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.07526</em>, 2020
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Ilya Sutskever, Oriol Vinyals and Quoc V Le
</span>
<span class="ltx_bibblock">â€œSequence to sequence learning with neural networksâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> <span id="bib.bibx18.2.2" class="ltx_text ltx_font_bold">27</span>, 2014
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Melvin Johnson et al.
</span>
<span class="ltx_bibblock">â€œGoogleâ€™s multilingual neural machine translation system: Enabling zero-shot translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em> <span id="bib.bibx19.2.2" class="ltx_text ltx_font_bold">5</span>
</span>
<span class="ltx_bibblock">MIT Press, 2017, pp. 339â€“351
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio
</span>
<span class="ltx_bibblock">â€œNeural machine translation by jointly learning to align and translateâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.0473</em>, 2014
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Jie Zhou et al.
</span>
<span class="ltx_bibblock">â€œDeep recurrent models with fast-forward connections for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em> <span id="bib.bibx21.2.2" class="ltx_text ltx_font_bold">4</span>
</span>
<span class="ltx_bibblock">MIT Press, 2016, pp. 371â€“383
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Duygu Ataman, Wilker Aziz and Alexandra Birch
</span>
<span class="ltx_bibblock">â€œA latent morphology model for open-vocabulary neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.13890</em>, 2019
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Fandong Meng et al.
</span>
<span class="ltx_bibblock">â€œEncoding source language with convolutional neural network for machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.01838</em>, 2015
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Jonas Gehring, Michael Auli, David Grangier and Yann N Dauphin
</span>
<span class="ltx_bibblock">â€œA convolutional encoder model for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.02344</em>, 2016
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Nal Kalchbrenner et al.
</span>
<span class="ltx_bibblock">â€œNeural machine translation in linear timeâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.10099</em>, 2016
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Jonas Gehring et al.
</span>
<span class="ltx_bibblock">â€œConvolutional sequence to sequence learningâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, 2017, pp. 1243â€“1252
</span>
<span class="ltx_bibblock">PMLR
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Ashish Vaswani et al.
</span>
<span class="ltx_bibblock">â€œAttention is all you needâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> <span id="bib.bibx27.2.2" class="ltx_text ltx_font_bold">30</span>, 2017
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Minh-Thang Luong, Hieu Pham and Christopher D Manning
</span>
<span class="ltx_bibblock">â€œEffective approaches to attention-based neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.04025</em>, 2015
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Felix Stahlberg
</span>
<span class="ltx_bibblock">â€œNeural machine translation: A reviewâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Journal of Artificial Intelligence Research</em> <span id="bib.bibx29.2.2" class="ltx_text ltx_font_bold">69</span>, 2020, pp. 343â€“418
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova
</span>
<span class="ltx_bibblock">â€œBert: Pre-training of deep bidirectional transformers for language understandingâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Wissam Antoun, Fady Baly and Hazem Hajj
</span>
<span class="ltx_bibblock">â€œArabert: Transformer-based model for Arabic language understandingâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.00104</em>, 2020
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Wuwei Lan, Yang Chen, Wei Xu and Alan Ritter
</span>
<span class="ltx_bibblock">â€œGigabert: Zero-shot transfer learning from english to arabicâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)</em>, 2020
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Sadaf Abdul-Rauf, Kiran Kiani, Ammara Zafar and Raheel Nawaz
</span>
<span class="ltx_bibblock">â€œExploring transfer learning and domain data selection for the biomedical translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</em>, 2019, pp. 156â€“163
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Boxiang Liu and Liang Huang
</span>
<span class="ltx_bibblock">â€œParaMed: a parallel corpus for Englishâ€“Chinese translation in the biomedical domainâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">BMC Medical Informatics and Decision Making</em> <span id="bib.bibx34.2.2" class="ltx_text ltx_font_bold">21.1</span>
</span>
<span class="ltx_bibblock">Springer, 2021, pp. 258
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">RubÃ©n MartÃ­nez-DomÃ­nguez et al.
</span>
<span class="ltx_bibblock">â€œCustomized Neural Machine Translation Systems for the Swiss Legal Domainâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)</em>, 2020, pp. 217â€“223
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Samuel LÃ¤ubli et al.
</span>
<span class="ltx_bibblock">â€œPost-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domainâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.01685</em>, 2019
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">David Grangier and Dan Iter
</span>
<span class="ltx_bibblock">â€œThe trade-offs of domain adaptation for neural language modelsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.10274</em>, 2021
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Alberto Poncelas et al.
</span>
<span class="ltx_bibblock">â€œInvestigating Backtranslation in Neural Machine Translation. 2018â€
</span>
<span class="ltx_bibblock">Arxiv, 1804
</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Chenhui Chu, Raj Dabre and Sadao Kurohashi
</span>
<span class="ltx_bibblock">â€œAn empirical comparison of domain adaptation methods for neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 2017, pp. 385â€“391
</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Yongchao Deng et al.
</span>
<span class="ltx_bibblock">â€œFactorized transformer for multi-domain neural machine translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, 2020, pp. 4221â€“4230
</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Hitham Abo Bakr, Khaled Shaalan and Ibrahim Ziedan
</span>
<span class="ltx_bibblock">â€œA hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabicâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx41.1.1" class="ltx_emph ltx_font_italic">The 6th international conference on informatics and systems, infos2008. cairo university</em>, 2008
</span>
<span class="ltx_bibblock">Citeseer
</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Emad Mohamed, Behrang Mohit and Kemal Oflazer
</span>
<span class="ltx_bibblock">â€œTransforming standard Arabic to colloquial Arabicâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 2012, pp. 176â€“180
</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Nizar Habash and Jun Hu
</span>
<span class="ltx_bibblock">â€œImproving Arabic-Chinese statistical machine translation using English as pivot languageâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Workshop on Statistical Machine Translation</em>, 2009, pp. 173â€“181
</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">El Moatez Billah Nagoudi, AbdelRahim Elmadany and Muhammad Abdul-Mageed
</span>
<span class="ltx_bibblock">â€œAraT5: Text-to-text transformers for Arabic language generationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.12068</em>, 2021
</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Jezia Zakraoui, Moutaz Saleh, Somaya Al-Maadeed and Jihad Mohamed Aljaâ€™am
</span>
<span class="ltx_bibblock">â€œArabic Machine Translation: A Survey With Challenges and Future Directionsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx45.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em> <span id="bib.bibx45.2.2" class="ltx_text ltx_font_bold">9</span>
</span>
<span class="ltx_bibblock">IEEE, 2021, pp. 161445â€“161468
</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Mohamed Seghir Hadj Ameur, Farid Meziane and Ahmed Guessoum
</span>
<span class="ltx_bibblock">â€œArabic machine translation: A survey of the latest trends and challengesâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx46.1.1" class="ltx_emph ltx_font_italic">Computer Science Review</em> <span id="bib.bibx46.2.2" class="ltx_text ltx_font_bold">38</span>
</span>
<span class="ltx_bibblock">Elsevier, 2020, pp. 100305
</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Nizar Y Habash
</span>
<span class="ltx_bibblock">â€œIntroduction to Arabic natural language processingâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx47.1.1" class="ltx_emph ltx_font_italic">Synthesis lectures on human language technologies</em> <span id="bib.bibx47.2.2" class="ltx_text ltx_font_bold">3.1</span>
</span>
<span class="ltx_bibblock">Morgan &amp; Claypool Publishers, 2010, pp. 1â€“187
</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">Rana Ehab, Eslam Amer and Mahmoud Gadallah
</span>
<span class="ltx_bibblock">â€œEnglish-Arabic hybrid machine translation system using EBMT and translation memoryâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx48.1.1" class="ltx_emph ltx_font_italic">Int. J. Adv. Comput. Sci. Appl.</em> <span id="bib.bibx48.2.2" class="ltx_text ltx_font_bold">10.1</span>, 2019, pp. 195â€“203
</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Wissam Antoun, Fady Baly and Hazem Hajj
</span>
<span class="ltx_bibblock">â€œAraGPT2: Pre-Trained Transformer for Arabic Language Generationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Arabic Natural Language Processing Workshop</em>
</span>
<span class="ltx_bibblock">Kyiv, Ukraine (Virtual): Association for Computational Linguistics, 2021, pp. 196â€“207
</span>
<span class="ltx_bibblock">URL: <a href="https://aclanthology.org/2021.wanlp-1.21" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.wanlp-1.21</a>
</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Alec Radford et al.
</span>
<span class="ltx_bibblock">â€œLanguage Models are Unsupervised Multitask Learnersâ€, 2019
</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi and Muhammad Abdul-Mageed
</span>
<span class="ltx_bibblock">â€œGPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLPâ€, 2023
</span>
<span class="ltx_bibblock">arXiv:<a href="https://arxiv.org/abs/2305.14976" title="" class="ltx_ref ltx_href">2305.14976 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">JÃ¶rg Tiedemann and Santhosh Thottingal
</span>
<span class="ltx_bibblock">â€œOPUS-MT â€“ Building open translation services for the Worldâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx52.1.1" class="ltx_emph ltx_font_italic">European Association for Machine Translation Conferences/Workshops</em>, 2020
</span>
<span class="ltx_bibblock">URL: <a href="https://api.semanticscholar.org/CorpusID:221097277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:221097277</a>
</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">JÃ¶rg Tiedemann
</span>
<span class="ltx_bibblock">â€œThe Tatoeba Translation Challenge â€“ Realistic Data Sets for Low Resource and Multilingual MTâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Conference on Machine Translation</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2020, pp. 1174â€“1182
</span>
<span class="ltx_bibblock">URL: <a href="https://www.aclweb.org/anthology/2020.wmt-1.139" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.wmt-1.139</a>
</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">Taku Kudo and John Richardson
</span>
<span class="ltx_bibblock">â€œSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processingâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018</em>, 2018, pp. 66â€“71
</span>
<span class="ltx_bibblock">DOI: <a href="https://dx.doi.org/10.18653/v1/d18-2012" title="" class="ltx_ref ltx_href">10.18653/v1/d18-2012</a>
</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">NLLB Team et al.
</span>
<span class="ltx_bibblock">â€œNo Language Left Behind: Scaling Human-Centered Machine Translationâ€, 2022
</span>
<span class="ltx_bibblock">arXiv:<a href="https://arxiv.org/abs/2207.04672" title="" class="ltx_ref ltx_href">2207.04672 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">Maja PopoviÄ‡
</span>
<span class="ltx_bibblock">â€œchrF: character n-gram F-score for automatic MT evaluationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>
</span>
<span class="ltx_bibblock">Lisbon, Portugal: Association for Computational Linguistics, 2015, pp. 392â€“395
</span>
<span class="ltx_bibblock">DOI: <a href="https://dx.doi.org/10.18653/v1/W15-3049" title="" class="ltx_ref ltx_href">10.18653/v1/W15-3049</a>
</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">Matthew Snover et al.
</span>
<span class="ltx_bibblock">â€œA study of translation edit rate with targeted human annotationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</em>, 2006, pp. 223â€“231
</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">Ricardo Rei, Craig Stewart, Ana C Farinha and Alon Lavie
</span>
<span class="ltx_bibblock">â€œCOMET: A Neural Framework for MT Evaluationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
</span>
<span class="ltx_bibblock">Online: Association for Computational Linguistics, 2020, pp. 2685â€“2702
</span>
<span class="ltx_bibblock">DOI: <a href="https://dx.doi.org/10.18653/v1/2020.emnlp-main.213" title="" class="ltx_ref ltx_href">10.18653/v1/2020.emnlp-main.213</a>
</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">Tianyi Zhang et al.
</span>
<span class="ltx_bibblock">â€œBERTScore: Evaluating Text Generation with BERTâ€, 2020
</span>
<span class="ltx_bibblock">arXiv:<a href="https://arxiv.org/abs/1904.09675" title="" class="ltx_ref ltx_href">1904.09675 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">Keqin Peng et al.
</span>
<span class="ltx_bibblock">â€œTowards Making the Most of ChatGPT for Machine Translationâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx60.1.1" class="ltx_emph ltx_font_italic">ArXiv</em> <span id="bib.bibx60.2.2" class="ltx_text ltx_font_bold">abs/2303.13780</span>, 2023
</span>
<span class="ltx_bibblock">URL: <a href="https://api.semanticscholar.org/CorpusID:257704711" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:257704711</a>
</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">Faten Khoshafah
</span>
<span class="ltx_bibblock">â€œChatGPT for Arabic-English Translation: Evaluating the Accuracyâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx61.1.1" class="ltx_emph ltx_font_italic">Research Square</em>
</span>
<span class="ltx_bibblock">Research Square, 2023, pp. https://doi.org/10.21203/rs.3.rsâ€“2814154/v1
</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">Zaid Alyafeai et al.
</span>
<span class="ltx_bibblock">â€œTaqyim: Evaluating Arabic NLP Tasks Using ChatGPT Modelsâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.16322</em>, 2023
</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">El Moatez Billah Nagoudi, AbdelRahim Elmadany and Muhammad Abdul-Mageed
</span>
<span class="ltx_bibblock">â€œAraT5: Text-to-Text Transformers for Arabic Language Generationâ€, 2022
</span>
<span class="ltx_bibblock">arXiv:<a href="https://arxiv.org/abs/2109.12068" title="" class="ltx_ref ltx_href">2109.12068 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">Amr Hendy et al.
</span>
<span class="ltx_bibblock">â€œHow Good Are GPT Models at Machine Translation? A Comprehensive Evaluationâ€, 2023
</span>
<span class="ltx_bibblock">arXiv:<a href="https://arxiv.org/abs/2302.09210" title="" class="ltx_ref ltx_href">2302.09210 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">Deborah Coughlin
</span>
<span class="ltx_bibblock">â€œCorrelating automated and human assessments of machine translation qualityâ€
</span>
<span class="ltx_bibblock">In <em id="bib.bibx65.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit IX: Papers</em>, 2003
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 22 13:24:59 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
