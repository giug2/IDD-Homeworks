<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Quantifying and Enabling the Interpretability of CLIP-like Models</title>
<!--Generated on Tue Sep 10 15:17:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.06579v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S1" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S2" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S3" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Quantifying interpretability in CLIP models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.SS1" title="In 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Entanglement score</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.SS2" title="In 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Association score</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>CLIP-InterpreT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.SS1" title="In 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Property-based nearest neighbors search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.SS2" title="In 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Per head topic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.SS3" title="In 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Contrastive Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.SS4" title="In 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Per-head Nearest Neighbors of an Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.SS5" title="In 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Per-head Nearest neighbors of a Text Input</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S6" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#A1" title="In Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix / supplemental material</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Quantifying and Enabling the Interpretability of CLIP-like Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Avinash Madasu 
<br class="ltx_break"/>Intel Labs 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">avinash.madasu@intel.com</span> &amp;Yossi Gandelsman 
<br class="ltx_break"/>UC Berkeley 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">yossi@gandelsman.com</span> <span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Vasudev Lal 
<br class="ltx_break"/>Intel Labs 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">vasudev.lal@intel.com</span> &amp;Phillip Howard 
<br class="ltx_break"/>Intel Labs 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">phillip.r.howard@intel.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">CLIP is one of the most popular foundational models and is heavily used for many vision-language tasks. However, little is known about the inner workings of CLIP. To bridge this gap we propose a study to quantify the interpretability in CLIP like models. We conduct this study on six different CLIP models from OpenAI and OpenCLIP which vary by size, type of pre-training data and patch size. Our approach begins with using the TEXTSPAN algorithm and in-context learning to break down individual attention heads into specific properties. We then evaluate how easily these heads can be interpreted using new metrics which measure property consistency within heads and property disentanglement across heads. Our findings reveal that larger CLIP models are generally more interpretable than their smaller counterparts. To further assist users in understanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a tool designed for interpretability analysis. CLIP-InterpreT offers five types of analyses: property-based nearest neighbor search, per-head topic segmentation, contrastive segmentation, per-head nearest neighbors of an image, and per-head nearest neighbors of text.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">CLIP  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib24" title="">24</a>]</cite>, a large-scale vision-language (VL) model, is extensively used as a foundational model for tasks such as video retrieval, image generation, and segmentation  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib18" title="">18</a>]</cite>. Given its widespread use, it is imperative to understand the inner workings of CLIP. Towards this end, we introduce a systematic methodology for quantifying the interpretability in CLIP like models. We perform this study on six types of CLIP models: ViT-B-16, ViT-B-32, and ViT-L-14 from OpenAI, as well as ViT-B-16, ViT-B-32, and ViT-L-14 from OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">First, we identify interpretable structures within the individual heads of the last four layers of the model, using a set of text descriptions. To accomplish this, we employ the <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.1">TextSpan</span> algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib13" title="">13</a>]</cite>, which helps us find the most appropriate text descriptions for each head.
After identifying these text descriptions, we assign labels to each head, representing the common property shared by the descriptions. This labeling process is carried out using in-context learning with ChatGPT. We begin by manually labeling five pairs of text descriptions and their corresponding property labels, which serve as examples. These examples are then used to prompt ChatGPT to assign labels for the remaining heads. This approach systematically connects the attention heads to the properties they learn during large-scale pre-training, offering insights into the roles of individual heads.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We introduce two metrics, the entanglement score and the association score, to quantify interpretability in CLIP models. These metrics are specifically designed to assess how easily properties can be linked to each attention head within the model. The entanglement score highlights that in larger CLIP models, attention heads tend to exhibit greater independence, meaning they seldom share properties with other heads. This indicates a clearer distinction in the roles of individual heads. Similarly, the association score shows that larger CLIP models consistently focus on a single property within the associated text descriptions, reinforcing the idea that these models are more interpretable because their heads are more specialized and less likely to overlap in the properties they learn.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Finally, we propose a new interpretability user application: CLIP-InterpreT (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F1" title="Figure 1 ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">1</span></a>) designed specifically to help users understand the inner workings of CLIP. This tool is engineered to adapt and integrate five distinct interpretability analysis methods, allowing for comprehensive insights into six different variants of CLIP models.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Early research on interpretability primarily concentrated on convolutional neural networks (CNNs) due to their intricate and opaque decision-making processes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib16" title="">16</a>]</cite>.
More recently, the interpretability of Vision Transformers (ViT) has garnered significant attention as these models, unlike CNNs, rely on self-attention mechanisms rather than convolutions. Researchers have focused on task-specific analyses in areas such as image classification, captioning, and object detection to understand how ViTs process and interpret visual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib9" title="">9</a>]</cite>. One of the key metrics used to measure interpretability in ViTs is the attention mechanism itself, which provides insights into how the model distributes focus across different parts of an image when making decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib4" title="">4</a>]</cite>. This has led to the development of techniques that leverage attention maps to explain ViT predictions. Early work on multimodal interpretability, which involves models that handle both visual and textual inputs, probed tasks such as how different modalities influence model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib22" title="">22</a>]</cite> and how visual semantics are represented within the model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib19" title="">19</a>]</cite>. Aflalo et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib1" title="">1</a>]</cite> explored interpretability methods for vision-language transformers, examining how these models combine visual and textual information to make joint decisions. Similarly, Stan et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib27" title="">27</a>]</cite> proposed new approaches for interpreting vision-language models, focusing on the interactions between modalities and how these influence model predictions. Our work builds upon and leverages the methods introduced by <cite class="ltx_cite ltx_citemacro_citet">Gandelsman et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib14" title="">14</a>]</cite> to interpret attention heads, neurons, and layers in vision-language models, providing deeper insights into their decision-making processes.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we outline the methodology used in our analysis, beginning with an explanation of the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">TextSpan</span> algorithm. We then describe how we extend this algorithm to apply it across all attention heads in multiple CLIP models using in-context learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.6">The <span class="ltx_text ltx_font_smallcaps" id="S3.p2.6.1">TextSpan</span> algorithm  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib13" title="">13</a>]</cite> is designed to decompose individual attention heads by associating them with corresponding text descriptions. It requires an initial set of text descriptions broadly capturing the concepts in an image. The algorithm starts by generating two matrices: <math alttext="C" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_C</annotation></semantics></math>, which contains the outputs for a specific head (denoted as <math alttext="(l,h)" class="ltx_Math" display="inline" id="S3.p2.2.m2.2"><semantics id="S3.p2.2.m2.2a"><mrow id="S3.p2.2.m2.2.3.2" xref="S3.p2.2.m2.2.3.1.cmml"><mo id="S3.p2.2.m2.2.3.2.1" stretchy="false" xref="S3.p2.2.m2.2.3.1.cmml">(</mo><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">l</mi><mo id="S3.p2.2.m2.2.3.2.2" xref="S3.p2.2.m2.2.3.1.cmml">,</mo><mi id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">h</mi><mo id="S3.p2.2.m2.2.3.2.3" stretchy="false" xref="S3.p2.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2b"><interval closure="open" id="S3.p2.2.m2.2.3.1.cmml" xref="S3.p2.2.m2.2.3.2"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑙</ci><ci id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">ℎ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2c">(l,h)</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.2d">( italic_l , italic_h )</annotation></semantics></math>), and <math alttext="R" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_R</annotation></semantics></math>, which includes representations of candidate text descriptions projected onto the span of <math alttext="C" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_C</annotation></semantics></math>.
In each iteration, <span class="ltx_text ltx_font_smallcaps" id="S3.p2.6.2">TextSpan</span> calculates the dot product between each row of <math alttext="R" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_R</annotation></semantics></math> and the outputs in <math alttext="C" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_C</annotation></semantics></math> to identify the row with the highest variance, known as the "first principal component." Once identified, this component is projected away from all rows, and the process is repeated to find additional components. The projection step ensures that each new component adds variance that is orthogonal to the earlier components, thereby isolating distinct aspects of the text descriptions relevant to each head.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p3.1.1">TextSpan</span> is effective at identifying text descriptions that are most similar to a given head. To label the common property shared by these text descriptions, we employ in-context learning with ChatGPT. We begin by manually labeling properties for five heads, which serve as examples. These examples are then used to prompt ChatGPT to generate labels for the remaining heads. This approach allows us to systematically label the properties associated with each head.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Quantifying interpretability in CLIP models</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Given the numerous CLIP-like models which are available, a key question arises: can we quantify how interpretable these models are?
To answer this, we introduce a set of metrics specifically designed to assess how easily properties can be linked to each layer and head within the models. These metrics provide a way to measure the interpretability of the models, helping us understand how clearly different properties are represented.
</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Entanglement score</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We define the entanglement score as the mean frequency at which heads exhibit the same <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.1">TextSpan</span> label (property). A higher score suggests that the property is shared among multiple heads or layers, making it more challenging to distinctly associate the property with a particular head. If the score is zero, then all the heads are disentangled from each other.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.T1" title="Table 1 ‣ 4.1 Entanglement score ‣ 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">1</span></a> presents the entanglement scores across various CLIP models, revealing several key insights. Notably, larger CLIP models exhibit significantly lower entanglement scores compared to their base model counterparts, suggesting a more distinct association of properties to specific heads and layers in these larger models. This reduced entanglement is an important result as it highlights the improved interpretability and clarity in larger models.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Additionally, the table highlights another key observation: OpenAI’s smaller models are less entangled than the models from OpenCLIP, indicating a potential difference in how these models manage property associations at a smaller scale. Conversely, the entanglement scores for OpenAI’s larger models are higher than expected, showing more entanglement than their smaller versions, which could suggest a trade-off in complexity and property association within these models. These findings emphasize the importance of model size and architecture in determining the level of entanglement,</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1.1" style="width:325.2pt;height:106.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.0pt,9.5pt) scale(0.848693164503311,0.848693164503311) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.2.1">Model<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.3.1">Model size<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.4.1">Patch size<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.5.1">Pre-training data<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1">Entanglement Score (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math>) <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2.1.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2.1.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2.1.3">32<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2.1.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2.1.5">0.437<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.3.2.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.3.2.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.3.2.3">32<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.3.2.4">OpenCLIP-datacomp<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.3.2.5">0.52<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.4.3.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.4.3.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.4.3.3">16<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.4.3.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.4.3.5">0.5<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.5.4.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.5.4.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.5.4.3">16<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.5.4.4">OpenCLIP-LAION2B<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.5.4.5">0.541<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.6.5.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.6.5.2">L <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.6.5.3">14<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.6.5.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1.6.5.5">0.359<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.1.7.6.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.1.7.6.2">L <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.1.7.6.3">14<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.1.7.6.4">OpenCLIP-LAION2B<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.1.7.6.5">0.343<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S4.T1.3.1">Entanglement scores for CLIP models</span>. Larger CLIP models are less entangled.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Association score</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Previously, we explored how the <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.1.1">TextSpan</span> labels assigned to different attention heads in a model can reflect the model’s interpretability. This led to the question of whether all the text descriptions linked to a given head genuinely correspond to the assigned property label. To investigate this, we manually evaluated how frequently the text descriptions align with the property label. We introduced the "association score" to quantify this, which is defined as the average frequency of heads with at least three text descriptions matching the property label.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.T2" title="Table 2 ‣ 4.2 Association score ‣ 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">2</span></a> presents the association scores for various CLIP models. The data clearly shows that larger CLIP models have more heads consistently focusing on a single property, making them more interpretable. This observation is consistent with the results from the entanglement scores, which lead to a similar conclusion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">From Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.T1" title="Table 1 ‣ 4.1 Entanglement score ‣ 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S4.T2" title="Table 2 ‣ 4.2 Association score ‣ 4 Quantifying interpretability in CLIP models ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">2</span></a>, it is evident that larger models have heads that learn properties independently of other heads while consistently focusing on a single property. This independence allows for the easy isolation of head-property pairs, enhancing the interpretability of the model’s individual heads.
Additionally, OpenCLIP’s smaller models have lower association scores compared to their OpenAI counterparts, while the opposite is true for larger models. This pattern mirrors the findings seen in the entanglement scores, reinforcing the insights gained from both metrics.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1.1" style="width:325.2pt;height:110pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.7pt,8.0pt) scale(0.872798755157666,0.872798755157666) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1">Model<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1">Model size<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.4.1">Patch size<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.5.1">Pre-training data<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">Association Score (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.m1.1d">↑</annotation></semantics></math>) <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span><span class="ltx_rule" style="width:0.0pt;height:0.0pt;position:relative; bottom:-3.9pt;background:black;display:inline-block;"></span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1.3">32<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1.5">0.437<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3.2.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3.2.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3.2.3">32<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3.2.4">OpenCLIP-datacomp<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3.2.5">0.145<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4.3.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4.3.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4.3.3">16<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4.3.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4.3.5">0.354<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5.4.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5.4.2">B <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5.4.3">16<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5.4.4">OpenCLIP-LAION2B<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5.4.5">0.166<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6.5.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6.5.2">L <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6.5.3">14<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6.5.4">OpenAI-400M<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6.5.5">0.453<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.1.7.6.1">CLIP<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.1.7.6.2">L <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.1.7.6.3">14<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.1.7.6.4">OpenCLIP-LAION2B<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.1.7.6.5">0.562<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.3.1">Association scores for CLIP models</span>. Larger models have greater property consistency.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CLIP-InterpreT</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the previous sections, we outlined a systematic approach to quantifying interpretability in CLIP models. However, the true value of interpretability lies in its accessibility to users. To address this, we introduce a new application called CLIP-InterpreT, a comprehensive tool designed to empower users with insights into the inner workings of CLIP-like models. This application provides an intuitive interface, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F1" title="Figure 1 ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">1</span></a>. In the user interface, users can easily upload an image of their choice and select one of six CLIP models for analysis. These models include ViT-B-16, ViT-B-32, and ViT-L-14 from OpenAI, as well as ViT-B-16, ViT-B-32, and ViT-L-14 from OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib17" title="">17</a>]</cite>. The application offers five distinct types of interpretability analyses, which we will discuss next.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="S5.F1.g1" src="extracted/5845727/Intro-figure.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Interface of CLIP-InterpreT application. Users can upload an image and select a model to analyze. There are five tabs for different decomposition analyses.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="87" id="S5.F2.g1" src="extracted/5845727/Tiger_colors_ViT-B-32-datacomp_combined.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S5.F2.2.1">Top-4 nearest neighbors for "colors" property</span>. The model used is ViT-B-32 (Data comp). The input is the image of tiger which is on the left of the dotted lines. The outputs are the four images, right of the dotted lines. In this example, we see that both the input and retrieved output images have common orange, black, and green colors.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Property-based nearest neighbors search</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this analysis, we demonstrate that the layers and heads of CLIP models can be characterized by specific attributes such as colors, locations, animals, and more. Since multiple heads can learn the same attribute, we combine the representations of these heads, which have been labeled using in-context learning, to create unified image representations. To evaluate the similarity between an input image and a set of candidate images from the ImageNet validation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib7" title="">7</a>]</cite>, we calculate the cosine similarity between the unified representation of the input image and each candidate image. The top four images with the highest similarity scores are then retrieved, showing which images are most similar to the input image based on the learned attributes.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F2" title="Figure 2 ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">2</span></a> provides an example focusing on the color property which was obtained from CLIP-InterpreT. The model used in this example is ViT-B-32 pretrained on datacomp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib17" title="">17</a>]</cite>. The input image uploaded to CLIP-InterpreT, located to the left of the dotted line, features a tiger. On the right side of the dotted line are the four images retrieved by the model. In this case, we observe that both the input image and the retrieved images share common colors, specifically orange, black, and green.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="69" id="S5.F3.g1" src="extracted/5845727/vit-b-16-openai-environment-l11-h3-topic-seg.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S5.F3.2.1">Topic Segmentation results for Layer 11, Head 3 (an "environment/weather" head).</span>. The model used is ViT-B-16 (LAION-2B). In the first image (left), the heatmap (blue) is focused on "flowers" which matches the text description. In the second image (middle), the heatmap (blue) is concentrated on the "tornado" matching the text description. In the last image, the heatmap (blue) is focused on "sun" matching the description "Hot Summer".</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Per head topic Segmentation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In contrast to <cite class="ltx_cite ltx_citemacro_citet">Gandelsman et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#bib.bib13" title="">13</a>]</cite>, we study topic segmentation for each individual head in various CLIP-like models using CLIP-InterpreT. This analysis focuses on projecting a segmentation map corresponding to an input text onto the reference image. The segmentation map is computed using various heads of the model to highlight the specific properties that each head captures, which allows us to visualize how different heads focus on different elements within the image based on the given text description.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F3" title="Figure 3 ‣ 5.1 Property-based nearest neighbors search ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">3</span></a> provides examples obtained using CLIP-InterpreT for layer 11, head 3 of the ViT-B-16 model, which has been pretrained on the LAION-2B dataset. In the first image on the left, the heatmap (represented in blue) focuses on the "flowers," aligning well with the text description. In the second image, positioned in the middle, the heatmap (blue) concentrates on the "tornado," again corresponding to the text. Finally, in the third image, the heatmap (blue) is centered on the "sun," accurately reflecting the description "Hot Summer." These examples illustrate how the model’s attention is directed towards specific features in the image that match the input text.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S5.F4.g1" src="extracted/5845727/thunder_contrastive_seg.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S5.F4.2.1">Image shows the contrastive Segmentation between portions of the image containing "tornado" and "thunderstorm."</span> The model used is ViT-L-14 pretrained on LAION-2B dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Contrastive Segmentation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The contrastive segmentation analysis provided in CLIP-InterpreT contrasts the visual interpretations of a single image when described by two different text inputs. By projecting the segmentation maps corresponding to each input onto the original image, we reveal how the model visually interprets and differentiates the input texts. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F4" title="Figure 4 ‣ 5.2 Per head topic Segmentation ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">4</span></a> provides an example using the ViT-L-14 model, pretrained on the LAION-2B dataset. The image illustrates the contrast between segments corresponding to "tornado" and "thunderstorm", in the image showcasing how the model distinguishes between these two concepts.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S5.F5.g1" src="x1.png" width="956"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S5.F5.2.1">Top-8 nearest neighbors per head and image</span>. The input image is provided on the left, with the head-specific nearest neighbors shown on the right. The model used is OpenAI-400M.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Per-head Nearest Neighbors of an Image</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">CLIP-InterpreT can also visualize the nearest neighbors retrieved for an input image based on similarity scores computed using a single attention head. Certain heads are specialized in capturing specific image properties, allowing us to leverage their intermediate representations for better interpretability. By calculating the similarity of direct contributions from individual heads, we can identify images that closely match the input image in terms of specific aspects, such as colors or objects.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F5" title="Figure 5 ‣ 5.3 Contrastive Segmentation ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates this concept using the ViT-B-16 model from OpenAI. For an image of pandas which was uploaded to CLIP-InterpreT, the eight nearest neighbors retrieved from layer 11, head 7 (which focuses on ’colors’), display similar color patterns. Meanwhile, layer 10, head 5 (which specializes in ’animals’), retrieves images featuring animals. Similarly, for an input image of the Taj Mahal, layer 11, head 4 (the ’style’ head) retrieves images with a similar architectural style, while layer 8, head 3 returns images that share a common concept, such as structures set against open skies.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S5.F6.g1" src="extracted/5845727/Vit-b-16-openai-nearest-text.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S5.F6.2.1">Nearest neighbors retrieved for the top <span class="ltx_text ltx_font_smallcaps" id="S5.F6.2.1.1">TextSpan</span> outputs of a given layer and head</span>. The model used is ViT-B-16 pretrained on OpenAI-400M.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Per-head Nearest neighbors of a Text Input</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To determine whether CLIP-like models can link image representations to a given text input, CLIP-InterpreT provides the ability to retrieve the nearest neighbors for a given input text using different attention heads. We utilize the top <span class="ltx_text ltx_font_smallcaps" id="S5.SS5.p1.1.1">TextSpan</span> outputs identified for each head. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06579v1#S5.F6" title="Figure 6 ‣ 5.4 Per-head Nearest Neighbors of an Image ‣ 5 CLIP-InterpreT ‣ Quantifying and Enabling the Interpretability of CLIP-like Models"><span class="ltx_text ltx_ref_tag">6</span></a> provides an example of this analysis. In the figure, we observe that layer 11, head 6 (the ’location’ head) retrieves images that match the text caption. We also see similar behavior from layer 11, head 8 (the ’environment’ head), and layer 21, head 8 (the ’colors’ head).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we conducted an analysis to quantify the interpretability of six different CLIP-like models from OpenAI and OpenCLIP, varying in size, pre-training data, and patch size. We used the <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">TextSpan</span> algorithm and in-context learning to decompose individual attention heads into specific properties and then evaluated interpretability using newly developed metrics. Our findings show that larger CLIP models are generally easier to interpret than smaller ones. To help users explore these insights, we introduced CLIP-InterpreT, a tool offering five types of interpretability analyses.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aflalo et al. [2022]</span>
<span class="ltx_bibblock">
Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, Chenfei Wu, Nan Duan, and Vasudev Lal.

</span>
<span class="ltx_bibblock">Vl-interpret: An interactive visualization tool for interpreting vision-language transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</em>, pages 21406–21415, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al. [2023]</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 18392–18402, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2020]</span>
<span class="ltx_bibblock">
Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Behind the scene: Revealing the secrets of pre-trained vision-and-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16</em>, pages 565–580. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer et al. [2021]</span>
<span class="ltx_bibblock">
Hila Chefer, Shir Gur, and Lior Wolf.

</span>
<span class="ltx_bibblock">Transformer interpretability beyond attention visualization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 782–791, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cordonnier et al. [2019]</span>
<span class="ltx_bibblock">
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.

</span>
<span class="ltx_bibblock">On the relationship between self-attention and convolutional layers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1911.03584</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cornia et al. [2022]</span>
<span class="ltx_bibblock">
Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.

</span>
<span class="ltx_bibblock">Explaining transformer-based image captioning models: An empirical analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">AI Communications</em>, 35(2):111–129, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2009 IEEE conference on computer vision and pattern recognition</em>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2022]</span>
<span class="ltx_bibblock">
Bowen Dong, Pan Zhou, Shuicheng Yan, and Wangmeng Zuo.

</span>
<span class="ltx_bibblock">Towards class interpretable vision transformer with multi-class-tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</em>, pages 609–622. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dravid et al. [2023]</span>
<span class="ltx_bibblock">
Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher.

</span>
<span class="ltx_bibblock">Rosetta neurons: Mining the common units in a model zoo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 1934–1943, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elguendouze et al. [2023]</span>
<span class="ltx_bibblock">
Sofiane Elguendouze, Adel Hafiane, Marcilio CP de Souto, and Anaïs Halftermeyer.

</span>
<span class="ltx_bibblock">Explainability in image captioning based on the latent space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Neurocomputing</em>, 546:126319, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. [2024]</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fong and Vedaldi [2017]</span>
<span class="ltx_bibblock">
Ruth C Fong and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Interpretable explanations of black boxes by meaningful perturbation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 3429–3437, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpreting clip’s image representation via text-based decomposition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandelsman et al. [2024]</span>
<span class="ltx_bibblock">
Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpreting the second-order effects of neurons in clip, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks and Nematzadeh [2021]</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks and Aida Nematzadeh.

</span>
<span class="ltx_bibblock">Probing image-language transformers for verb understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2106.09141</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks et al. [2016]</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Generating visual explanations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14</em>, pages 3–19. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et al. [2021]</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.5143773" title="">https://doi.org/10.5281/zenodo.5143773</a>.

</span>
<span class="ltx_bibblock">If you use this software, please cite it as below.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. [2023]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4015–4026, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindström et al. [2021]</span>
<span class="ltx_bibblock">
Adam Dahlgren Lindström, Suna Bensch, Johanna Björklund, and Frank Drewes.

</span>
<span class="ltx_bibblock">Probing multimodal embeddings for linguistic properties: the visual-semantic case.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2102.11115</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 26296–26306, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2022]</span>
<span class="ltx_bibblock">
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.

</span>
<span class="ltx_bibblock">Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Neurocomputing</em>, 508:293–304, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madasu and Lal [2023]</span>
<span class="ltx_bibblock">
Avinash Madasu and Vasudev Lal.

</span>
<span class="ltx_bibblock">Is multimodal vision supervision beneficial to language?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 2637–2642, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mannix and Bondell [2024]</span>
<span class="ltx_bibblock">
Evelyn Mannix and Howard Bondell.

</span>
<span class="ltx_bibblock">Scalable and robust transformer decoders for interpretable image classification with foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2403.04125</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International conference on machine learning</em>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. [2017]</span>
<span class="ltx_bibblock">
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks via gradient-based localization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 618–626, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan et al. [2014]</span>
<span class="ltx_bibblock">
K Simonyan, A Vedaldi, and A Zisserman.

</span>
<span class="ltx_bibblock">Deep inside convolutional networks: visualising image classification models and saliency maps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>. ICLR, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stan et al. [2024]</span>
<span class="ltx_bibblock">
Gabriela Ben Melech Stan, Raanan Yehezkel Rohekar, Yaniv Gurwicz, Matthew Lyle Olson, Anahita Bhiwandiwalla, Estelle Aflalo, Chenfei Wu, Nan Duan, Shao-Yen Tseng, and Vasudev Lal.

</span>
<span class="ltx_bibblock">Lvlm-intrepret: An interpretability tool for large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2404.03118</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. [2022]</span>
<span class="ltx_bibblock">
Mengqi Xue, Qihan Huang, Haofei Zhang, Lechao Cheng, Jie Song, Minghui Wu, and Mingli Song.

</span>
<span class="ltx_bibblock">Protopformer: Concentrating on prototypical parts in vision transformers for interpretable image recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2208.10431</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeiler and Fergus [2014]</span>
<span class="ltx_bibblock">
Matthew D Zeiler and Rob Fergus.

</span>
<span class="ltx_bibblock">Visualizing and understanding convolutional networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13</em>, pages 818–833. Springer, 2014.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix / supplemental material</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we present additional figures from our CLIP-InterpreT application.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="117" id="A1.F7.g1" src="extracted/5845727/appendix/France_location_vit-b-32-openai_combined.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="A1.F7.2.1">Top-4 nearest neighbors for "location" property.</span> The model used is ViT-B-32 (OpenAI). The input image is a picture of the "Eiffel tower" in Paris, France. The top-4 images are related to popular location landmarks.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="A1.F8.g1" src="extracted/5845727/appendix/Statue_of_liberty_location_ViT-L-14-laion_combined.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="A1.F8.2.1">Top-4 nearest neighbors for "location" property.</span> Top-4 nearest neighbors for "location" property. The model used is ViT-L-14 (LAION2B).</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="109" id="A1.F9.g1" src="extracted/5845727/appendix/Animals_zebra_vit-b-16-openai-combined.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="A1.F9.2.1">Top-4 nearest neighbors for "animals" property.</span> The model used is ViT-B-16 (OpenAI).</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="A1.F10.g1" src="extracted/5845727/appendix/Tiger_pattern_vit-b-32-openai-combined.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold" id="A1.F10.2.1">Top-4 nearest neighbors for "pattern" property.</span> The model used is ViT-B-32 (OpenAI). In this example, the input image shows an animal laying on the grass. The top retrieved images also show this common pattern of an animal laying on the grass.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="77" id="A1.F11.g1" src="extracted/5845727/appendix/Vit-l-14-laoin-location-l22-h13-topic-seg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold" id="A1.F11.2.1">Topic Segmentation results for Layer 22, Head 13 (a "geolocation" head).</span> The model utilized is ViT-L-14 (trained on LAION-2B). The blue highlight in the segmentation map focuses on landmarks such as the "Eiffel Tower," "Christ the Redeemer," "Statue of Liberty," and "Taj Mahal," which are located in France, New York, Brazil, and India, respectively, as indicated in the text input. Notably, the text does not explicitly state that the Eiffel Tower is in Paris, France. Instead, Layer 22, Head 13 of the model possesses geolocation properties that implicitly identify these locations.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="A1.F12.g1" src="extracted/5845727/appendix/vit-b-32-openai-emotion-l10-h6-topic-seg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="A1.F12.2.1">Topic Segmentation results for Layer 10, Head 6 (an "emotion" head).</span> The model employed is ViT-B-32 (OpenAI-400M). In the first image (on the left), the heatmap (in blue) prominently highlights the "smile" emotion on a child’s face, aligning well with the text description. The middle image shows the heatmap concentrated on the "fear" emotion, which is associated with the Conjuring movie. Notably, there is no explicit indication in the text that the image is meant to convey "fear." In the final image, the heatmap centers on the "sad" emotion displayed by "Thanos" from Marvel.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="A1.F13.g1" src="extracted/5845727/appendix/Vit-b-16-laoin-france-contrastive-seg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The image illustrates a contrastive segmentation between areas of the image associated with "tower" and "wine." The model utilized for this analysis is ViT-L-16 (trained on LAION-2B).</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="A1.F14.g1" src="extracted/5845727/appendix/vit-l-14-openai-nearest-image.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span class="ltx_text ltx_font_bold" id="A1.F14.2.1">Top-8 nearest neighbors per head and image.</span> The model used is ViT-L-14 pretrained on OpenAI-400M.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="264" id="A1.F15.g1" src="extracted/5845727/appendix/Vit-l-14-laoin-nearest-neighbours-text.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Nearest neighbors retrieved for the top <span class="ltx_text ltx_font_smallcaps" id="A1.F15.2.1">TextSpan</span> outputs of a given layer and head. The model used is ViT-L-14 pretrained on LAION-2B.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 15:17:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
