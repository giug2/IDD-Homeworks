<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.01021] Synthetic Data for Face Recognition: Current State and Future Prospects</title><meta property="og:description" content="Over the past years, deep learning capabilities and the availability of large-scale training datasets advanced rapidly, leading to breakthroughs in face recognition accuracy. However, these technologies are foreseen to…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data for Face Recognition: Current State and Future Prospects">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data for Face Recognition: Current State and Future Prospects">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.01021">

<!--Generated on Thu Feb 29 10:20:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data for Face Recognition: Current State and Future Prospects</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fadi Boutros<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Vitomir Struc<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">2</span></sup>,Julian Fierrez<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">3</span></sup>, Naser Damer<sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">1,4</span></sup> 
<br class="ltx_break"><sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">1</span></sup>Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic">2</span></sup> Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, Slovenia 
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup"><span id="id15.15.id7.1" class="ltx_text ltx_font_italic">3</span></sup> School of Engineering, Universidad Autonoma de Madrid, Madrid, Spain 
<br class="ltx_break"><sup id="id16.16.id8" class="ltx_sup"><span id="id16.16.id8.1" class="ltx_text ltx_font_italic">4</span></sup>Department of Computer Science, TU Darmstadt,
Darmstadt, Germany
<br class="ltx_break">Email: fadi.boutros@igd.fraunhofer.de
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Over the past years, deep learning capabilities and the availability of large-scale training datasets advanced rapidly, leading to breakthroughs in face recognition accuracy. However, these technologies are foreseen to face a major challenge in the next years due to the legal and ethical concerns about using authentic biometric data in AI model training and evaluation along with increasingly utilizing data-hungry state-of-the-art deep learning models.
With the recent advances in deep generative models and their success in generating realistic and high-resolution synthetic image data, privacy-friendly synthetic data has been recently proposed as an alternative to privacy-sensitive authentic data to overcome the challenges of using authentic data in face recognition development.
This work aims at providing a clear and structured picture of the use-cases taxonomy of synthetic face data in face recognition along with the recent emerging advances of face recognition models developed on the bases of synthetic data. We also discuss the challenges facing the use of synthetic data in face recognition development and several future prospects of synthetic data in the domain of face recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The breakthroughs of deep neural networks and their training optimizations as well as the availability of large-scale identity-labeled face datasets have reshaped the research landscape of face recognition (FR) over the past years. These emerging technologies have dramatically improved FR performances leading to the wider integration of FR in a variety of applications from logical access control and consumer low-end devices to automated border control.
State-of-the-Art (SOTA) FR models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> utilized large-scale face datasets e.g. CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, or VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to train deep neural networks (DNN) with millions of trainable parameters, where the goal is to optimize the empirical risk minimization function given input training samples, their corresponding labels, and DNN trainable parameters. Achieving such a goal without being over-optimized, i.e. overfitted, requires that training datasets are of large scale (massive number of images of many identities) and representative of various variations that exist in the real world.
Large and representative data is also required to evaluate FR accuracies against different variations that present in real operation scenarios e.g. pose, aging, occlusion, or lighting. Data is required to evaluate the vulnerability of FR against different types of attacks such as morphing, presentation, master-face, and deep fake attacks.
FR components, face processing models, attack detectors, and face image quality estimation models are not different as they require face data for training and evaluation.
Besides the technical limitation of collecting large-scale data with realistic variations, there are increased concerns about collecting, maintaining, redistributing, and using biometric data due to legal, ethical, and privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Consequently, many widely used datasets for FR development such as VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> have been retracted by their creator. Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the most widely used datasets to train FR models. Even though many of these datasets have been publically released, there are not any more accessible.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Processing biometric data is governed by a set of legal restrictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Taking the General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as an example, it categories biometric data as a special category of personal data subjected to rigorous data protection rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, requiring high protection in connection with fundamental rights and freedoms of individuals.
Dealing with such data requires adherence to one of the exemptions of biometric data processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the related national laws <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, maintaining processing records <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and the preparation of data protection impact assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, among other restrictions.
Depending on the purpose of the biometric data processing, this set of restrictions can be rigorously extended <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
Besides the legal complications of using and sharing biometric data, ethical requirements are commonly necessary, such as the approval of an ethics committee or competent authorities.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The increased concerns about the legal and ethical use of authentic data in biometrics along with the technical limitation in collecting large and diverse face datasets motivate recent works to propose the use of synthetic data as an alternative to privacy-sensitive authentic data in FR training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
In an attempt to provide a clear understanding of the feasibility of utilizing synthetic face data to train, evaluate, attack, or privacy enhancement, this work is the first to analyze the properties needed of the synthetic data for FR, the use-cases taxonomy of synthetic data in FR, the current state of synthetic-based FR, the limitations and challenges facing the use of current synthetic face data in FR, and possible future research directions that might give a larger space for synthetic data in different aspects of FR development.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.3.2" class="ltx_text" style="font-size:90%;">Overview of the most widely used authentic and synthetic facial datasets commonly used to train FR models, along with the number of images, identities, images per identity, and the fact that each database is public and/or still accessible. Note that many of the public databases are not accessible (raising a practical problem for researchers and developers) anymore based on legal and ethical concerns and even those that are available are ethically questioned as the individual consent of the data subjects is not always insured.</span></figcaption>
<div id="S1.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:308.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.7pt,35.3pt) scale(0.813459908537672,0.813459908537672) ;">
<table id="S1.T1.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.4.1.1.1" class="ltx_tr">
<td id="S1.T1.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></td>
<td id="S1.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S1.T1.4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.3.1" class="ltx_text ltx_font_bold"># Images (m)</span></td>
<td id="S1.T1.4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.4.1" class="ltx_text ltx_font_bold"># Identities (k)</span></td>
<td id="S1.T1.4.1.1.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.5.1" class="ltx_text ltx_font_bold">Avg.</span></td>
<td id="S1.T1.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.6.1" class="ltx_text ltx_font_bold">Public</span></td>
<td id="S1.T1.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.7.1" class="ltx_text ltx_font_bold">Accessible</span></td>
<td id="S1.T1.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.4.1.1.1.8.1" class="ltx_text ltx_font_bold">Authentic</span></td>
</tr>
<tr id="S1.T1.4.1.2.2" class="ltx_tr">
<td id="S1.T1.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S1.T1.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S1.T1.4.1.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.5</td>
<td id="S1.T1.4.1.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10.6</td>
<td id="S1.T1.4.1.2.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">47</td>
<td id="S1.T1.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.3.3" class="ltx_tr">
<td id="S1.T1.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DeepFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S1.T1.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S1.T1.4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.4</td>
<td id="S1.T1.4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.0</td>
<td id="S1.T1.4.1.3.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1092</td>
<td id="S1.T1.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.4.4" class="ltx_tr">
<td id="S1.T1.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S1.T1.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S1.T1.4.1.4.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">200.0</td>
<td id="S1.T1.4.1.4.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8,000.0</td>
<td id="S1.T1.4.1.4.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">25</td>
<td id="S1.T1.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.5.5" class="ltx_tr">
<td id="S1.T1.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Facebook <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S1.T1.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S1.T1.4.1.5.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">500.0</td>
<td id="S1.T1.4.1.5.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10,000.0</td>
<td id="S1.T1.4.1.5.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">50</td>
<td id="S1.T1.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.6.6" class="ltx_tr">
<td id="S1.T1.4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">VGGFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S1.T1.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S1.T1.4.1.6.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.6</td>
<td id="S1.T1.4.1.6.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.6</td>
<td id="S1.T1.4.1.6.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">992</td>
<td id="S1.T1.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.7.7" class="ltx_tr">
<td id="S1.T1.4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CelebFaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S1.T1.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2016</td>
<td id="S1.T1.4.1.7.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.09</td>
<td id="S1.T1.4.1.7.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.4</td>
<td id="S1.T1.4.1.7.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16</td>
<td id="S1.T1.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.8.8" class="ltx_tr">
<td id="S1.T1.4.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S1.T1.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2016</td>
<td id="S1.T1.4.1.8.8.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10</td>
<td id="S1.T1.4.1.8.8.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">100.0</td>
<td id="S1.T1.4.1.8.8.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">100</td>
<td id="S1.T1.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.9.9" class="ltx_tr">
<td id="S1.T1.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MegaFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S1.T1.4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S1.T1.4.1.9.9.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.7</td>
<td id="S1.T1.4.1.9.9.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">672.0</td>
<td id="S1.T1.4.1.9.9.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7</td>
<td id="S1.T1.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.10.10" class="ltx_tr">
<td id="S1.T1.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">UMDFaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S1.T1.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S1.T1.4.1.10.10.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.4</td>
<td id="S1.T1.4.1.10.10.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.3</td>
<td id="S1.T1.4.1.10.10.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">46</td>
<td id="S1.T1.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.11.11" class="ltx_tr">
<td id="S1.T1.4.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S1.T1.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S1.T1.4.1.11.11.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">3.3</td>
<td id="S1.T1.4.1.11.11.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.1</td>
<td id="S1.T1.4.1.11.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">363</td>
<td id="S1.T1.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.12.12" class="ltx_tr">
<td id="S1.T1.4.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">IMDbFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S1.T1.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S1.T1.4.1.12.12.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.7</td>
<td id="S1.T1.4.1.12.12.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">59.0</td>
<td id="S1.T1.4.1.12.12.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">29</td>
<td id="S1.T1.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.13.13" class="ltx_tr">
<td id="S1.T1.4.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MS1MV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S1.T1.4.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S1.T1.4.1.13.13.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.8</td>
<td id="S1.T1.4.1.13.13.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">85.0</td>
<td id="S1.T1.4.1.13.13.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">68</td>
<td id="S1.T1.4.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.13.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.14.14" class="ltx_tr">
<td id="S1.T1.4.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MillionCelebs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S1.T1.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S1.T1.4.1.14.14.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">18.8</td>
<td id="S1.T1.4.1.14.14.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">636.0</td>
<td id="S1.T1.4.1.14.14.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">30</td>
<td id="S1.T1.4.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S1.T1.4.1.14.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.15.15" class="ltx_tr">
<td id="S1.T1.4.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">WebFace260M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S1.T1.4.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S1.T1.4.1.15.15.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">260</td>
<td id="S1.T1.4.1.15.15.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4,000.0</td>
<td id="S1.T1.4.1.15.15.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">65</td>
<td id="S1.T1.4.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.15.15.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.16.16" class="ltx_tr">
<td id="S1.T1.4.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">WebFace42M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S1.T1.4.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S1.T1.4.1.16.16.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">42</td>
<td id="S1.T1.4.1.16.16.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2,000.0</td>
<td id="S1.T1.4.1.16.16.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21</td>
<td id="S1.T1.4.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.16.16.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
</tr>
<tr id="S1.T1.4.1.17.17" class="ltx_tr">
<td id="S1.T1.4.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S1.T1.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2021</td>
<td id="S1.T1.4.1.17.17.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.5</td>
<td id="S1.T1.4.1.17.17.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">10</td>
<td id="S1.T1.4.1.17.17.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">50</td>
<td id="S1.T1.4.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✔</td>
<td id="S1.T1.4.1.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✔</td>
<td id="S1.T1.4.1.17.17.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✘</td>
</tr>
<tr id="S1.T1.4.1.18.18" class="ltx_tr">
<td id="S1.T1.4.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DigiFace-1M-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S1.T1.4.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.4.1.18.18.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.72</td>
<td id="S1.T1.4.1.18.18.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10</td>
<td id="S1.T1.4.1.18.18.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">72</td>
<td id="S1.T1.4.1.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.18.18.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
</tr>
<tr id="S1.T1.4.1.19.19" class="ltx_tr">
<td id="S1.T1.4.1.19.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DigiFace-1M-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S1.T1.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.4.1.19.19.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.5</td>
<td id="S1.T1.4.1.19.19.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">100</td>
<td id="S1.T1.4.1.19.19.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5</td>
<td id="S1.T1.4.1.19.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.19.19.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
</tr>
<tr id="S1.T1.4.1.20.20" class="ltx_tr">
<td id="S1.T1.4.1.20.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S1.T1.4.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.4.1.20.20.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.63</td>
<td id="S1.T1.4.1.20.20.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10.6</td>
<td id="S1.T1.4.1.20.20.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">60</td>
<td id="S1.T1.4.1.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.20.20.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
</tr>
<tr id="S1.T1.4.1.21.21" class="ltx_tr">
<td id="S1.T1.4.1.21.21.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">USynthFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S1.T1.4.1.21.21.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.4.1.21.21.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.4</td>
<td id="S1.T1.4.1.21.21.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.4</td>
<td id="S1.T1.4.1.21.21.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">1</td>
<td id="S1.T1.4.1.21.21.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.21.21.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✔</td>
<td id="S1.T1.4.1.21.21.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✘</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Where is the synthetic data used?</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To analyse the properties of the needed synthetic data, one should start by building a clear taxonomy of the different possible uses-cases of synthetic data in its interaction with FR.
This taxonomy here will consider the operations where the synthetic data is used to interact with the recognition part of FR systems, i.e. the feature extraction.
Therefore, synthetic data that is meant to interact with other system components, as defined in ISO ISO/IEC 19795-1:2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, are out of scope, e.g. synthetic data used to train or evaluate face detection or segmentation solutions.
Additionally, synthesizing faces as a means of domain transformation, e.g. from thermal to visible face appearance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is also out of scope as it just transfers the appearance of the image.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the use-case taxonomy of the synthetic face data interaction with FR.
These use-cases are categorised under 4 groups, along with the properties of the possibly needed data under each category (the latter will be discussed in detail in the next section).
The four use-case categories are discussed in the following.</p>
</div>
<div id="S2.p3" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Training FR:</span>
Modern FR solutions are based on deep learning models that are either trained directly to generate identity-discriminant feature representations (e.g. triplet loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>) or to classify the identity classes in the training data (e.g. ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, ElasticFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, etc.).
In the latter approach, embeddings proceeding the classification layer of the network are then used to extract the identity-discriminant representations. This family of approaches is currently predominantly leading to SOTA FR performances.
In both cases, training face data that represents the high inter and intra-class diversity of real applications is needed to train the models.
As mentioned in the introduction, the diversity of such data, if authentic, is limited by practical data collection constraints, and its collection and handling are hedged by privacy, legal, and ethical concerns.
Synthetic data can come in handy to train such FR models in different manners based on the training requirements.
If the model is trained in one of the two approaches mentioned above, then the synthetic data has to contain a large number of identities and multiple samples of each identity.
If the model is trained on partially authentic data, however, the intra-class variation of this data is low, then the synthetic data needs to contain multiple samples for each of the authentic identities, i.e. act as an augmentation strategy.
Finally, if the FR model is trained in an unsupervised manner, then the synthetic training data is not largely concerned with the identity grouping, but rather just requires a set of faces of random identities.
This data has also been shown to be successful in training processes during the training-aware quantization of models based on full precision parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Although it is out of the scope of this work, synthetic faces of this kind can also be used to train face detectors, face segmentation, and attack detection methods (e.g. morphing attack detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Evaluating FR:</span>
FR algorithmic evaluation, following the ISO ISO/IEC 19795-1:2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, requires the existence of a large set of genuine (same identity) and imposter (different identity) face image pairs that represent the real operational scenario.
The need for a large number of these pairs is intensified by the ever-more accurate performance of FR algorithms.
FR algorithms can produce two main algorithmic errors, genuine pairs classified wrongly as imposters (false non-match (FNM)) or imposter pairs classified wrongly as genuine (false match FM).
As the algorithms produce lower and lower rates of decision errors, the FM rates (FMR) and FNM rates (FNMR), the number of evaluated pairs required to produce statistically significant evaluation results become higher.
This need for large-scale evaluation data is one of the main motivations behind requiring synthetic data for the evaluation.
Another reason is that some authorities that require in-house testing on their own data when purchasing FR solutions do only possess a single image per identity in their databases (think of visa systems) and thus it is impossible to have genuine pairs to evaluate FR algorithms.
Such situations would require synthetic data to be generated so it belongs to a certain authentic identity, but with realistic variations.
In a third scenario where the operation scenario would require a very low FMR, the need for a huge number of imposter pairs is required to evaluate, with statistical significance, the FMR.
In such cases, random synthetic faces with random identities can be used to create such imposter pairs.
Again, although it is out of the scope of this work, these synthetic faces, regardless of their identity information, can be used to evaluate face detectors, face segmentation, and presentation/morphing attack detection.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Attacking FR:</span>
Commonly, developers would use technology to enhance the convenience and security of individuals and societies.
However, technology can also be used maliciously to create attacks on individuals, systems, and societies.
This is the case also with synthetic face data, which can also be used as an attack.
Synthetic data can be created so that a certain face can be matched with two or more faces.
This can target automatic FR comparison or human image verification, or both.
Such attacks can be face morphing attacks, where an image is generated to match two or more identities, then used on an identity or travel document with the alphanumeric data of when the targeted matches.
Later such a document can be used by the other targeted identities illegally, leading to a serious security threat.
Another attack in the same category is the MasterFace attack, where the synthetic face is created to match a wider proportion of the population, raising many security threats.
The second type of attack by generated face images might focus on generating a face image of a specific identity.
Such attacks are commonly referred to as Deep-Fakes and they are commonly used to fool the viewer into wrongly believing that a certain person has said or done an action in an image or a video.
A third attack can use synthetic faces that maintain a certain identity but excludes a specific pattern with the aim of attacking a biometric-based system that ensures a legal operation of a process.
Such an attack can be by presenting the attacker’s real identity, but excluding the information that points out that the user is underage, in a service that requires age verification.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Enhancing the privacy for FR users:</span>
Although excluding certain patterns from generated images of specific identities can be seen as an attack on biometric systems, in different use-cases, they can be seen as a privacy-enhancing tool when they are used to avoid the illegal or unconsented processing of the data.
Such generation of the data aims at maintaining a certain set of visual patterns but removing the clues of a specific pattern.
Depending on the use-case, this excluded pattern can be related to the identity in what is widely known as image-level face de-identification, which is defined under the standard ISO/IEC 20889:2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
The excluded pattern can be related to certain soft biometric attributes like age or gender, which is commonly referred to as soft-biometric privacy enhancement.
Although it is out of the scope of this work, the generated faces can exclude patterns that makes them detectable to face detection tool, i.e. excluding the information that makes the face a face in the view of automatic face detection.</p>
</div>
</li>
</ol>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">So far, we presented a discussion on the possible use-cases of synthetic face data in FR. Each of these use-cases has different needs when it comes to synthetic data. These needs are discussed in the next section.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2305.01021/assets/taxonomy2.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="527" height="333" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text" style="font-size:90%;">A taxonomy of the synthetic data use-cases (on the top of the figure) directly interacting with FR models, either by training them, evaluating them, attacking them, or enhancing the privacy of the information extracted by them. This taxonomy lists the existing and foreseen synthetic data types that are needed by these use-cases (under each use-case). These data needs are grouped by their main properties by color and discussed, along with the use-cases in this paper. </span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>What data is needed and what properties make it good?</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The properties of the needed synthetic data under the different use-cases (discussed in the previous section) are grouped by their required properties under different colors in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and are discussed in detail in the following:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Single faces of random identities:</span>
As detailed in the previous section, and illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, synthetic face images of random identities without the requirement of multiple images to belong to one identity can be used for training FR models in an unsupervised manner.
They additionally can be used to evaluate FR models, specifically evaluate the FMR, especially when the targeted operational point is at a very low FMR, requiring an extremely large number of diverse imposter pairs to make the evaluation result statistically significant.
Here, such data should be realistic, i.e. act like authentic data when processed by the FR model.
A successful way to measure that was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and it is based on comparing the activation function value ranges in the FR model when processing authentic data versus when processing the synthetic data.
Additionally, the distribution of the comparison scores between pairs of these single images of random identities should theoretically be similar to those of imposter comparisons of authentic data to ensure the similarity to the authentic inter-identity variation, which was explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Multiple faces per random identity:</span>
This kind of data represents what one would typically expect from FR training or evaluation data.
That is, multiple identities, with multiple images per identity.
This, given a sufficient inter and intra-class (identity variation), can be used to train an FR model in a supervised manner.
This also would contain both imposter and genuine pairs to evaluate the performance of FR by calculating both possible errors, FMR and FNMR.
Such data should also interact with the FR model similarly to authentic data, this as mentioned earlier can be measured by monitoring the value range of the model’s activation functions.
Here, the data should possess an inter and intra-class variability of the targeted authentic data scenario.
We specify “targeted” here as different evaluation and training goals of FR might occur, e.g. a model is evaluated specifically for cases with an extreme pose or extreme age differences between the comparison pairs (intra-class variations), or for cases of pairs of twins or siblings (inter-class variation). This goes for training as well, as an FR model can be trained to specifically be tolerant to mask occlusions, and thus the training data inter and intra-class diversity should represent that.
The suitability of such data can be measured by comparing its genuine and imposter comparison scores distributions with that of the targeted authentic data (which can be much smaller in size) as performed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
For specifically targeted attribute variations, such as age and pose, attribute predictors can be used to ensure the existence of such attribute variations in the synthetic data to the same degree as the authentic data.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multiple faces of an existing identity:</span>
Authentic face data with insufficient intra-class variation is problematic for the training and evaluation of FR.
In terms of training an FR model, such data will lead to models that are not trained to tolerate intra-class variation (e.g. pose, expressions, age, illumination, etc.) and thus are expected to lead to high FNMR in practical operations.
When evaluating FR, evaluation data in some practical cases such as an authority that possesses only a single (or few) images per identity (e.g. visa applicant database) would not be sufficient to evaluate the expected FNMR as no (or few) genuine pairs exist in the data.
Both cases require acquiring more samples of each of the existing identities.
These samples have to be of realistic variation that matches the targeted scenario.
Such samples might be created synthetically and would act as an augmentation approach when training an FR model, or as additional samples to create genuine pairs when evaluating FR models (or training FR in a triplet loss-like strategy).
Such synthetic data should interact with the FR model similarly to authentic data, as previously discussed.
It should also result in genuine comparison score distribution that matches the targeted authentic data scenario.
One must take notice that this should be the case when the pairs are between the existing authentic sample is compared to the synthetic images of the same identity, but also, if needed, between the synthetically generated samples of the same identity themselves.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">A face of multiple identities:</span>
A synthetic face can also be used as an attack, the fact that a face can be generated synthetically with properties that enables an attack on identity systems pursues researchers to foresee such attacks.
A face can be synthesized in a way that it matches two more specific (known) identities to create what is referred to as a morphing attack.
A morphing attack image is designed to match with a number of specific identities and can be created on the image level by interpolating the images of the targeted identities, or generated synthetically to possess the identity information of the targets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
Such an image, if used in association with a passport or an identity document can enable multiple persons to be verified to the alphanumeric information on the card.
A wider attack that surfaced lately in the literature is the MasteFace attack, where the attack image is synthesized to match a wide range of the population without the need to know the targeted identities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
As these attacks might be used to attack visual inspection, automatic verification, or both, they first have to have a natural appearance.
This natural appearance is best measured by user studies, where individuals are asked if an image appears realistic or not.
The vulnerability of automatic FR to such attacks, and thus the measure of how good is the synthetic data for its purpose, can be measured using the Mated Morph Presentation Match Rate (MMPMR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The MMPMR refers to the fraction of morphs whose similarity to both identities used to morph, are below the selected FR comparison score threshold relative to all morphs.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">A face of specific authentic identity:</span>
Synthesizing a face of a specific authentic identity is usually related to the need to synthesize this face with also a specific expression or domain, unlike generating such faces of an authentic identity where a realistic variation is needed.
This is commonly related to what is referred to as DeepFake faces but also includes other face manipulation techniques such as expression and attribute manipulations.
As such attacks aim at manipulating human viewers, their success is best measured by how realistic they are to these viewers and how well they succeeded in the targeted manipulation in the view of the viewers through user studies related to the exact goal of the manipulation.
However, more within the scope of this work is the ability of these attacks to fool automatic FR and attack detection algorithms.
A comprehensive survey on the issue of DeepFakes and facial image manipulation is presented by Tolosana et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">A face that excludes a specific pattern:</span>
A face synthesizing process can maintain a subset of patterns from a specific face and excludes other subsets of these patterns.
Such patterns can be identity information, age, gender, ethnicity, or even the patterns that make a face detectable as a face, among other attribute patterns.
Such a process can be seen as an attack if it is aimed at avoiding a consented required process, such as automatic age verification to receive a service or make an online purchase.
However, such a process can also be seen as a privacy enhancement mechanism.
Excluding the identity, while maintaining the image appearance and other attributes to some degree is commonly referred to as image-level face de-identification and it aims at avoiding the unconsented identification of face images, whether in the public or private space.
A subset of this is to exclude the patterns of the face that makes it detectable and thus avoid further processing.
Removing other patterns like gender or age falls within the image-level soft-biometric privacy enhancement techniques that aim at maintaining the identification possibilities without allowing unconsented estimation of soft-biometric attributes.
Evaluating the ability to synthesize these face images is based on evaluating the degree to which the patterns that need to be excluded and the ones that need to be maintained are detectable, where the first need to be as undetectable as possible and the latter needs to be as detectable as possible.
A comprehensive survey and discussion on these technologies are presented by Meden et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">Verification accuracies (%) on five different FR benchmarks achieved by the supervised and unsupervised FR models trained on the synthetic training databases with the numbers of real and synthetic training samples.
The result in the first row is reported using the FR model trained on the authentic dataset to give an indication of the performance of an FR model trained on the authentic CASIA-WebFace dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
To provide a fair comparison, all model results are obtained from the original published works using the same network architecture (ResNet50) trained on relatively same training dataset size. KT refers to knowledge transfer from the pretrained FR model. LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, AgeDB-30 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, CA-LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, CP-LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> are widely used FR evaluation benchmarks.</span></figcaption>
<div id="S3.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:110.5pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-208.8pt,53.0pt) scale(0.509391252096859,0.509391252096859) ;">
<table id="S3.T2.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.4.1.1.1" class="ltx_tr">
<td id="S3.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Unsupervised</span></td>
<td id="S3.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Data augmentation</span></td>
<td id="S3.T2.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.4.1" class="ltx_text ltx_font_bold"># Synthetic Images</span></td>
<td id="S3.T2.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.5.1" class="ltx_text ltx_font_bold"># Authentic Images</span></td>
<td id="S3.T2.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.6.1" class="ltx_text ltx_font_bold">KT</span></td>
<td id="S3.T2.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.7.1" class="ltx_text ltx_font_bold">LFW</span></td>
<td id="S3.T2.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.8.1" class="ltx_text ltx_font_bold">AgeDB-30</span></td>
<td id="S3.T2.4.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.9.1" class="ltx_text ltx_font_bold">CFP-FP</span></td>
<td id="S3.T2.4.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.10.1" class="ltx_text ltx_font_bold">CA-LFW</span></td>
<td id="S3.T2.4.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.1.1.1.11.1" class="ltx_text ltx_font_bold">CP-LFW</span></td>
</tr>
<tr id="S3.T2.4.1.2.2" class="ltx_tr">
<td id="S3.T2.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CosFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S3.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500K</td>
<td id="S3.T2.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.55</td>
<td id="S3.T2.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.55</td>
<td id="S3.T2.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.31</td>
<td id="S3.T2.4.1.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.78</td>
<td id="S3.T2.4.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.95</td>
</tr>
<tr id="S3.T2.4.1.3.3" class="ltx_tr">
<td id="S3.T2.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S3.T2.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✘</td>
<td id="S3.T2.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">GAN-based</td>
<td id="S3.T2.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">500K</td>
<td id="S3.T2.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0</td>
<td id="S3.T2.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✘</td>
<td id="S3.T2.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">91.93</td>
<td id="S3.T2.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">61.63</td>
<td id="S3.T2.4.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">75.03</td>
<td id="S3.T2.4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">74.73</td>
<td id="S3.T2.4.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">70.43</td>
</tr>
<tr id="S3.T2.4.1.4.4" class="ltx_tr">
<td id="S3.T2.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T2.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500K</td>
<td id="S3.T2.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.07</td>
<td id="S3.T2.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.92</td>
<td id="S3.T2.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.99</td>
<td id="S3.T2.4.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.23</td>
<td id="S3.T2.4.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.73</td>
</tr>
<tr id="S3.T2.4.1.5.5" class="ltx_tr">
<td id="S3.T2.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T2.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Accessory + Geometric and color</td>
<td id="S3.T2.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500K</td>
<td id="S3.T2.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.40</td>
<td id="S3.T2.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.97</td>
<td id="S3.T2.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.40</td>
<td id="S3.T2.4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.62</td>
<td id="S3.T2.4.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.87</td>
</tr>
<tr id="S3.T2.4.1.6.6" class="ltx_tr">
<td id="S3.T2.4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">634K</td>
<td id="S3.T2.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.87</td>
<td id="S3.T2.4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.68</td>
<td id="S3.T2.4.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.86</td>
<td id="S3.T2.4.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.93</td>
<td id="S3.T2.4.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.20</td>
</tr>
<tr id="S3.T2.4.1.7.7" class="ltx_tr">
<td id="S3.T2.4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">USynthFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T2.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✔</td>
<td id="S3.T2.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GAN-based + Geometric and color</td>
<td id="S3.T2.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400K</td>
<td id="S3.T2.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.23</td>
<td id="S3.T2.4.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.62</td>
<td id="S3.T2.4.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.56</td>
<td id="S3.T2.4.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.05</td>
<td id="S3.T2.4.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.03</td>
</tr>
<tr id="S3.T2.4.1.8.8" class="ltx_tr">
<td id="S3.T2.4.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">IDnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S3.T2.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">528K</td>
<td id="S3.T2.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.83</td>
<td id="S3.T2.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.58</td>
<td id="S3.T2.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.43</td>
<td id="S3.T2.4.1.8.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.50</td>
<td id="S3.T2.4.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.35</td>
</tr>
<tr id="S3.T2.4.1.9.9" class="ltx_tr">
<td id="S3.T2.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">IDnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S3.T2.4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Geometric and color</td>
<td id="S3.T2.4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">528K</td>
<td id="S3.T2.4.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.58</td>
<td id="S3.T2.4.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.53</td>
<td id="S3.T2.4.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.40</td>
<td id="S3.T2.4.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.90 (3)</td>
<td id="S3.T2.4.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.25</td>
</tr>
<tr id="S3.T2.4.1.10.10" class="ltx_tr">
<td id="S3.T2.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S3.T2.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✘</td>
<td id="S3.T2.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">GAN-based</td>
<td id="S3.T2.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">500K</td>
<td id="S3.T2.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">40K</td>
<td id="S3.T2.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✘</td>
<td id="S3.T2.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">97.23</td>
<td id="S3.T2.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">81.32</td>
<td id="S3.T2.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">87.68</td>
<td id="S3.T2.4.1.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">85.08</td>
<td id="S3.T2.4.1.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.32</td>
</tr>
<tr id="S3.T2.4.1.11.11" class="ltx_tr">
<td id="S3.T2.4.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T2.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Accessory + Geometric and color</td>
<td id="S3.T2.4.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500K</td>
<td id="S3.T2.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40K</td>
<td id="S3.T2.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.05</td>
<td id="S3.T2.4.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.77</td>
<td id="S3.T2.4.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.01</td>
<td id="S3.T2.4.1.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.08</td>
<td id="S3.T2.4.1.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.27</td>
</tr>
<tr id="S3.T2.4.1.12.12" class="ltx_tr">
<td id="S3.T2.4.1.12.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✘</td>
<td id="S3.T2.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">634K</td>
<td id="S3.T2.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S3.T2.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✔</td>
<td id="S3.T2.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.13</td>
<td id="S3.T2.4.1.12.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">91.03</td>
<td id="S3.T2.4.1.12.12.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">91.14</td>
<td id="S3.T2.4.1.12.12.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.47</td>
<td id="S3.T2.4.1.12.12.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">87.03</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Where are we now?</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Face image generation:</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">A deep generative model (DGM) is a deep neural network that is trained to interpret and model a probability distribution of the authentic training data.
Specifically, a deep generative model takes random points from e.g. Gaussian distribution and maps them through a neural network such as the generated distribution closely matches the authentic data distribution.
The main DGM approaches that are proposed in the literature are Variational Auto-Encoder (VAE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Generative Adversarial Network (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, Autoregressive model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and Normalizing Flows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and Diffusion Models (DiffModel) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, in addition to a large number of hybrid models that combined two of previous approaches such as GAN with VAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. A comprehensive review of deep generative modelings is presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
Each of these approaches presented contributions towards providing a better trade-off between generated sample quality i.e. producing samples of high perceived quality and fidelity that resemble the DGM training data, inference time i.e. enabling fast sampling mechanism, architecture restrictions i.e. some of the DGMs are limited to underlying network architecture and sample appearance variations.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>How do the DGM approaches match the needed synthetic face data properties?</h3>

<div id="S4.SS2.p1" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Single faces of random identities: DGM approaches such as StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> presented very promising results in generating single faces of random synthetic identities with high visual fidelity. However, the generated faces could share the identity information, to a small degree, with DGM’s original training (as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Multiple faces per random identities: Approaches such as Face-ID-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, DiscoFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, GAN-Control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, InterFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, and CONFIG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> proposed GAN models based on disentangled representation learning to conditionally generate face images from synthetic identities with predefined attributes e.g. age, pose, illumination, or expression. As generated images are explicitly controlled by a predefined set of attributes, such images might lake the intra-class diversity that exists in real-world face data and it is needed to train and evaluate FR.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Multiple faces of an existing identity: DGM approaches such as CONFIG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> are able to regenerate multiple faces of an existing identity by reconstructing input faces with a predefined set of attributes such as changing expression, wearing sunglasses, adding makeup, or changing hair color. However, such attribute manipulation approaches might induce some artifacts in reconstructed faces, which might affect identity preservation between the input and the reconstructed faces. Also, as such approaches are explicitly manipulating the attributes of their input faces, the generated faces might not contain large appearance variations, which are needed to train and evaluate FR models. More importantly, identity preservation in reconstructed samples is rarely evaluated and reported.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">A face of multiple identities: DGM approaches were not explicitly designed and trained to generate a face of multiple identities. However, recent works such as MorGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, MIPGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, and MorDIFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, make use of generative models to generate a face of multiple identities by interpolating two or more latent vectors of synthetic or real faces and then generating a new face of multiple identities. In a similar manner, however, with latent vector optimization rather than optimization, MasterFaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> are generated to match unknown identities.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">A face of specific authentic identity: DGM approaches that targeted image-to-image modeling achieved impressive results in generating a face of specific authentic identity. This has been commonly achieved by manipulating the input source face to match specific attributes or a target domain while maintaining the identity information of the source image. Although such approaches did not target generating Deep-Fake attacks, they have been widely used in generating such kinds of attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">A face that excludes a specific pattern: None of the SOTA DGM approaches explicitly target generating a face that excludes a specific pattern. A number of works make use of DGM approaches to exclude a specific pattern e.g. identity, age, or gender of authentic input faces, especially when such models include attribute disentanglement. However, to the best of our knowledge, none of the previous works present solutions to generate a face of synthetic identity that excludes a specific pattern, rather this is done for faces of authentic identities. An overview of the current state of this issue can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2305.01021/assets/x1.png" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="415" height="533" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.4.2" class="ltx_text" style="font-size:90%;"> Sample of synthetic data used in SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, UsynthFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and IDnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. It can be clearly noticed the high variations in SFace images in comparison to other synthetic datasets. Although SynFace and UsynthFace utilized the same DGM (DiscoFaceGAN), it can be also observed the appearance variations in USynthFace using geometric and color transformations. </span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>What is the current state of the defined use-cases?</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Very recently a few works build on existing DGM approaches to propose FR based on synthetic data. The following discussion presents the use of synthetic data in FR grouped by the use-cases (discussed earlier in this paper and presented in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Training FR</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Recently, synthetically generated face data has been proposed as an alternative to privacy-sensitive authentic data to train FR models mitigating the technical, ethical, and legal concerns of using authentic biometric data in training FR models.
The currently proposed approaches in the literature utilized synthetically generated data to train unsupervised (UsynthFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>) or supervised FR models (SFace<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, SynFace<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, DigiFace-1M<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and IDnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>).
Training the unsupervised FR model as in UsynthFace requires that the training data maintain the property 1 (Section <a href="#S2" title="2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) i.e. single face of random identities, while supervised approaches, SFace, SynFace, IDnet, and DigiFace-1M, require that the training data maintain the property 2 i.e. multiple faces per random identities (Section <a href="#S2" title="2 Where is the synthetic data used? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Some of these approaches, SynFace and DigiFace-1M, proposed combining authentic with synthetic data during the training or transferring the knowledge from the pretrained FR model to improve the recognition accuracies. Others (USynthFace) utilized only synthetic data for FR training. Most synthetic FR approaches utilized GAN-based (UsynthFace, SynFace) and/or geometric and color transformation data augmentation (UsynthFace, IDnet, and DigiFace-1M) methods to create more challenging training samples improving the model recognition accuracies. Table <a href="#S3.T2" title="Table 2 ‣ 3 What data is needed and what properties make it good? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the achieved accuracies on five FR benchmarks by recent FR models trained on synthetic data.
It can be observed from the reported results in Table <a href="#S3.T2" title="Table 2 ‣ 3 What data is needed and what properties make it good? ‣ Synthetic Data for Face Recognition: Current State and Future Prospects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> that including data augmentation in FR model training significantly improved the recognition accuracies. Also, the unsupervised FR model (UsynthFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>) obtained very competitive results using unlabeled data to supervised synthetic-based FR models.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Evaluating FR</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">A few works proposed the use of synthetic data for evaluating FR.
SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> presented a synthetic version of the Labeled Faces in the Wild (LFW) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and evaluated two FR models trained on authentic and synthetic data, respectively on the synthetic version of the LFW.
The model trained on real data achieved an accuracy of 98.85% and the one trained on synthetic data achieved an accuracy of 99.98%.
The work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> also suggested that the degradation in the verification performance between the two models is due to the domain gap between synthetic and real training images.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Attacking FR</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">DGM approaches have been widely and successfully utilized to generate morphing, MasterFace, deep-fake, and manipulation attacks on FR.
Researchers generally attempt to foresee such attacks and evaluate their potential.
Deep-fake and face manipulation attacks are already a serious problem facing modern societies and their generation is becoming more available and realistic with time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Morphing attacks based on synthesized faces are a serious threat and FR recognition vulnerability to them is getting close to that of image-level morphing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. MasterFace attacks are relatively new, their initial proposed form is based on optimization on a relatively weak FR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> with other works arguing their feasibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.
However, on the other hand, synthetic data has helped create privacy-friendly databases for the detection of such attacks, specifically, the morphing attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and face presentation attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Huber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> organized a competition on face morphing attack detection (MAD) based on privacy-aware synthetic training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The competition aimed at promoting the use of synthetic data to develop MAD solutions and attracted 12 solutions from both academia and industry.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Privacy enhancement</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Main advances in this respect are presented under one of two categories, de-identification or soft-biometric privacy. De-identification can be achieved by adding adversarial noise to the image, image obfuscation, and image synthesis, the latter being the core focus of this work.
Many solutions have been proposed in the literature, with a recent overview of these solutions presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
The main challenge so far in this domain is the cross-FR model performance as most works showed very good performances on the FR models that were used to optimize the solution, however, this performance drops when using other unknown FR models.
Syntheses-based soft-biometric privacy followed a similar trend as de-identification, however, with much less dominance in the literature.
In this aspect, many works rather focused on soft-biometric privacy on the template level rather than the image. Image and template level techniques are surveyed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. An example of image-based techniques is the FlowSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> aimed at minimizing gender information in the resulting images. Here, as the target is the soft-biometrics and not the identity, the main challenge is to achieve generalized performance across soft-biometric estimators while maintaining FR performance across FR models.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Where can we do better?</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Here, based on the discussed use-cases taxonomy, the synthetic data requirements, and their current state along with the generation process, we discuss the main issues where further improvement in future research can have a strong effect on the use of synthetic data in FR. The following discussion will touch on the generation process, the defined use-cases, as well as the general lack of well-defined suitability evaluation protocols.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Face image generation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Generating realistic and high-quality samples along with enabling high sampling speed and high-resolution scaling have derived the main contributions of recent generative models proposed in the literature.
In addition, some DGM approaches targeted specific applications such as image in-painting, attribute manipulation, face aging, image super-resolution, and image-to-image and text-to-image translations.
Such applications mainly require that the generated samples are of high visual fidelity with less focus on the identity information, which might be less optimal for biometric applications.
When developing DGM for FR use-cases, the solution should focus on the utility of the generated images for the given tasks rather than only focusing on the human-perceived quality.
The emerging works on training FR solutions, presented earlier, are considered the first step in this regard.
This focus on utility, rather than only the perceived quality, should be the main drive in future research when synthesizing images for FR.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training FR</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Recent works that proposed the use of synthetic face data for FR utilized deep neural network architectures with hyper-parameters that are optimized on authentic data.
Such training paradigms might be sub-optimal for learning face representations from synthetic data. Future research works might target proposing network architectures or training paradigms designed specifically to learn from synthetic data.
In general, training FR solutions of synthetic data still fails behind those trained on authentic data in terms of accuracy, which is the main practical shortcoming that hinders placing such solutions in practical use currently.
However, one must keep in mind that training FR on synthetic data is a very recently emerging research direction and it is already achieving higher recognition accuracies than solutions trained on synthetic data less than a decade ago <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluating FR</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The need for large-scale FR evaluation datasets that represent real scenario variations is the main motivation for future research directions on synthetic data for FR evaluation. Although DGMs can generate arbitrary realistic face images, the utility of the generated images for FR remains challenging. Future research works include but are not limited to, DGMs for generating multiple faces of existing authentic identities, which might target specific variations such as age and pose, and generating complete evaluation datasets of multiple images of multiple identities.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Attacking FR</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Even though creating novel attacks on identity management systems and society in general sounds is a serious malicious action, it is essential to foresee attacks created by real attackers to better enable their detection.
As the attackers would ask, the researchers should also ask “What is the strongest attack I can create to serve the attack goals given the current state of basic technology?”
This follows the never-ending game of cat and mouse between attacks and attack mitigation.
Therefore, the constant struggle here is to always try to foresee new attacks and attack generation methodologies and analyze their strengths and weaknesses, leading to better mitigation strategies.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Privacy enhancement</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">The main challenge to generative face privacy enhancement is the generalizability and robustness as it must possess to maintain operation in real-world applications.
This generalization must ensure that the de-identification properties are strongly maintained even with unknown FR solutions.
The same goes for soft-biometric privacy, where the privacy-enhanced images should maintain their privacy properties when processed by diverse soft-biometric estimators with different levels of knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
Other open issues that still require increasing attention are the lack of clear quantifiability and provability privacy enhancement, the limited public benchmarks, and the need for controllable privacy where the user can have a choice of the privatised information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Evaluation protocols</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">We provided in this work an initial discussion on what synthetic data is needed for different FR use-cases and what properties are needed from such data based on the way it is used.
However, this initial discussion should evolve into a much-needed set of evaluation metrics and protocols that can precisely and comparably answer the question of “How well does the created data fit its targeted properties within its use-case?”
Besides, and based on, the needed academic efforts in this regard, given that the synthetic data is foreseen to be a commodity, there is a need for such protocols and metric standards on the industrial level.
A clear candidate to develop such a standard would be the ISO SC37 work group 5 on Biometric testing and reporting.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>CONCLUSION</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The use of authentic data in FR poses technical, legal, and ethical concerns.
However, such data plays a major role in training, evaluating, enhancing the FR user privacy, and even attacking FR.
This work provided initial discussions on the use of synthetic data in FR as an alternative to authentic data. We started by analysing and defining taxonomies for different possible FR use-cases in which synthetic data can be used. Then, we discussed the needed properties of synthetic data under each FR use-case. This has been followed by presenting the current state of synthetic FR. Finally, we provided several interesting directions of work that can be investigated in the future.
As a concluding remark, the use of synthetic data in different FR uses-cases is still in the early research stage and this work provides a base discussion on this research direction and aims at motivating and promoting further research works toward responsible FR development.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE and the ARRS research program P2-0250 (B) Metrology and Biometrics Systems. This work has been partially funded by the German Federal Ministry of Education and Research (BMBF) through the Software Campus Project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, F. Kirchbuchner, A. Kuijper,
<a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00164" title="" class="ltx_ref ltx_href">Elasticface: Elastic
margin loss for deep face recognition</a>, in: IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans,
LA, USA, June 19-20, 2022, IEEE, 2022, pp. 1577–1586.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00164" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPRW56347.2022.00164</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00164" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPRW56347.2022.00164</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, N. Xue, S. Zafeiriou,
<a target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">Arcface:
Additive angular margin loss for deep face recognition</a>, in: IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, June 16-20, 2019, Computer Vision Foundation / IEEE, 2019,
pp. 4690–4699.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00482" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2019.00482</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. Yi, Z. Lei, S. Liao, S. Z. Li,
<a target="_blank" href="http://arxiv.org/abs/1411.7923" title="" class="ltx_ref ltx_href">Learning face representation from
scratch</a>, CoRR abs/1411.7923 (2014).

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1411.7923" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1411.7923</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://arxiv.org/abs/1411.7923" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1411.7923</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Guo, L. Zhang, Y. Hu, X. He, J. Gao,
<a target="_blank" href="https://doi.org/10.1007/978-3-319-46487-9_6" title="" class="ltx_ref ltx_href">Ms-celeb-1m: A dataset
and benchmark for large-scale face recognition</a>, in: B. Leibe, J. Matas,
N. Sebe, M. Welling (Eds.), Computer Vision - ECCV 2016 - 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,
Part III, Vol. 9907 of Lecture Notes in Computer Science, Springer, 2016,
pp. 87–102.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-46487-9_6" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1007/978-3-319-46487-9\_6</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1007/978-3-319-46487-9_6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-319-46487-9_6</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman,
<a target="_blank" href="https://doi.org/10.1109/FG.2018.00020" title="" class="ltx_ref ltx_href">Vggface2: A dataset for
recognising faces across pose and age</a>, in: 13th IEEE International
Conference on Automatic Face &amp; Gesture Recognition, FG 2018, Xi’an,
China, May 15-19, 2018, IEEE Computer Society, 2018, pp. 67–74.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/FG.2018.00020" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/FG.2018.00020</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/FG.2018.00020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/FG.2018.00020</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Regulation
(eu) 2016/679 of the european parliament and of the council of 27 april 2016
on the protection of natural persons with regard to the processing of
personal data and on the free movement of such data, and repealing directive
95/46/ec (General Data Protection Regulation) (2016).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 9 of
the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 9(2)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 9(4)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 30 of
the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 35(3)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 37(1)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 22(4)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 27(2)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
The European Parliament and the Council of the European Union, Article 6(4)
of the general data protection regulation (2016).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
F. Boutros, M. Huber, P. Siebke, T. Rieber, N. Damer,
<a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007961" title="" class="ltx_ref ltx_href">Sface:
Privacy-friendly and accurate face recognition using synthetic data</a>, in:
IEEE International Joint Conference on Biometrics, IJCB 2022, Abu Dhabi,
United Arab Emirates, October 10-13, 2022, IEEE, 2022, pp. 1–11.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007961" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IJCB54206.2022.10007961</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007961" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCB54206.2022.10007961</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, D. Tao,
<a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01070" title="" class="ltx_ref ltx_href">Synface: Face recognition
with synthetic data</a>, in: 2021 IEEE/CVF International Conference on
Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021,
IEEE, 2021, pp. 10860–10870.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01070" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCV48922.2021.01070</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01070" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV48922.2021.01070</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Boutros, M. Klemt, M. Fang, A. Kuijper, N. Damer,
<a target="_blank" href="https://doi.org/10.1109/FG57933.2023.10042627" title="" class="ltx_ref ltx_href">Unsupervised face
recognition using unlabeled synthetic data</a>, in: 17th IEEE International
Conference on Automatic Face and Gesture Recognition, FG 2023, Waikoloa
Beach, HI, USA, January 5-8, 2023, IEEE, 2023, pp. 1–8.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/FG57933.2023.10042627" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/FG57933.2023.10042627</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/FG57933.2023.10042627" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/FG57933.2023.10042627</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Taigman, M. Yang, M. Ranzato, L. Wolf,
<a target="_blank" href="https://doi.org/10.1109/CVPR.2014.220" title="" class="ltx_ref ltx_href">Deepface: Closing the gap to
human-level performance in face verification</a>, in: 2014 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June
23-28, 2014, IEEE Computer Society, 2014, pp. 1701–1708.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2014.220" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2014.220</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2014.220" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2014.220</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
F. Schroff, D. Kalenichenko, J. Philbin,
<a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298682" title="" class="ltx_ref ltx_href">Facenet: A unified
embedding for face recognition and clustering</a>, in: IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June
7-12, 2015, IEEE Computer Society, 2015, pp. 815–823.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298682" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2015.7298682</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298682" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2015.7298682</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Taigman, M. Yang, M. Ranzato, L. Wolf,
<a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298891" title="" class="ltx_ref ltx_href">Web-scale training for face
identification</a>, in: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, IEEE Computer
Society, 2015, pp. 2746–2754.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298891" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2015.7298891</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2015.7298891" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2015.7298891</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O. M. Parkhi, A. Vedaldi, A. Zisserman,
<a target="_blank" href="https://doi.org/10.5244/C.29.41" title="" class="ltx_ref ltx_href">Deep face recognition</a>, in: X. Xie,
M. W. Jones, G. K. L. Tam (Eds.), Proceedings of the British Machine Vision
Conference 2015, BMVC 2015, Swansea, UK, September 7-10, 2015, BMVA
Press, 2015, pp. 41.1–41.12.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5244/C.29.41" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.5244/C.29.41</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.5244/C.29.41" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5244/C.29.41</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Sun, X. Wang, X. Tang,
<a target="_blank" href="https://doi.org/10.1109/TPAMI.2015.2505293" title="" class="ltx_ref ltx_href">Hybrid deep learning for
face verification</a>, IEEE Trans. Pattern Anal. Mach. Intell. 38 (10) (2016)
1997–2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2015.2505293" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2015.2505293</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/TPAMI.2015.2505293" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TPAMI.2015.2505293</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Nech, I. Kemelmacher-Shlizerman,
<a target="_blank" href="https://doi.org/10.1109/CVPR.2017.363" title="" class="ltx_ref ltx_href">Level playing field for million
scale face recognition</a>, in: 2017 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, IEEE
Computer Society, 2017, pp. 3406–3415.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.363" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2017.363</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2017.363" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2017.363</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Bansal, A. Nanduri, C. D. Castillo, R. Ranjan, R. Chellappa,
<a target="_blank" href="https://doi.org/10.1109/BTAS.2017.8272731" title="" class="ltx_ref ltx_href">Umdfaces: An annotated face
dataset for training deep networks</a>, in: 2017 IEEE International Joint
Conference on Biometrics, IJCB 2017, Denver, CO, USA, October 1-4, 2017,
IEEE, 2017, pp. 464–473.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/BTAS.2017.8272731" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/BTAS.2017.8272731</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/BTAS.2017.8272731" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/BTAS.2017.8272731</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
F. Wang, L. Chen, C. Li, S. Huang, Y. Chen, C. Qian, C. C. Loy,
<a target="_blank" href="https://doi.org/10.1007/978-3-030-01240-3_47" title="" class="ltx_ref ltx_href">The devil of face
recognition is in the noise</a>, in: V. Ferrari, M. Hebert, C. Sminchisescu,
Y. Weiss (Eds.), Computer Vision - ECCV 2018 - 15th European Conference,
Munich, Germany, September 8-14, 2018, Proceedings, Part IX, Vol. 11213 of
Lecture Notes in Computer Science, Springer, 2018, pp. 780–795.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-01240-3_47" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1007/978-3-030-01240-3\_47</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1007/978-3-030-01240-3_47" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-01240-3_47</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Zhang, W. Deng, M. Wang, J. Hu, X. Li, D. Zhao, D. Wen,
<a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.html" title="" class="ltx_ref ltx_href">Global-local
GCN: large-scale label noise cleansing for face recognition</a>, in: 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
2020, Seattle, WA, USA, June 13-19, 2020, Computer Vision Foundation /
IEEE, 2020, pp. 7728–7737.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.00775" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.00775</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.html</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Zhu, G. Huang, J. Deng, Y. Ye, J. Huang, X. Chen, J. Zhu, T. Yang, J. Lu,
D. Du, J. Zhou,
<a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_WebFace260M_A_Benchmark_Unveiling_the_Power_of_Million-Scale_Deep_Face_CVPR_2021_paper.html" title="" class="ltx_ref ltx_href">Webface260m:
A benchmark unveiling the power of million-scale deep face recognition</a>,
in: IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2021, virtual, June 19-25, 2021, Computer Vision Foundation / IEEE, 2021,
pp. 10492–10502.

<br class="ltx_break">URL <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_WebFace260M_A_Benchmark_Unveiling_the_Power_of_Million-Scale_Deep_Face_CVPR_2021_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_WebFace260M_A_Benchmark_Unveiling_the_Power_of_Million-Scale_Deep_Face_CVPR_2021_paper.html</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
G. Bae, M. de La Gorce, T. Baltrusaitis, C. Hewitt, D. Chen, J. P. C. Valentin,
R. Cipolla, J. Shen,
<a target="_blank" href="https://doi.org/10.1109/WACV56688.2023.00352" title="" class="ltx_ref ltx_href">Digiface-1m: 1 million
digital face images for face recognition</a>, in: IEEE/CVF Winter Conference
on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January
2-7, 2023, IEEE, 2023, pp. 3515–3524.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV56688.2023.00352" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/WACV56688.2023.00352</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/WACV56688.2023.00352" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WACV56688.2023.00352</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
ISO/IEC JTC1 SC37 Biometrics, ISO/IEC 19795-1:2021 Information technology
— Biometric performance testing and reporting — Part 1: Principles and
framework (2021).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. Mallat, N. Damer, F. Boutros, A. Kuijper, J. Dugelay,
<a target="_blank" href="https://doi.org/10.1109/ICB45273.2019.8987347" title="" class="ltx_ref ltx_href">Cross-spectrum thermal
to visible face recognition based on cascaded image synthesis</a>, in: 2019
International Conference on Biometrics, ICB 2019, Crete, Greece, June 4-7,
2019, IEEE, 2019, pp. 1–8.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICB45273.2019.8987347" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICB45273.2019.8987347</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/ICB45273.2019.8987347" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICB45273.2019.8987347</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, A. Kuijper,
<a target="_blank" href="https://doi.org/10.1109/ICPR56361.2022.9955645" title="" class="ltx_ref ltx_href">Quantface: Towards
lightweight face recognition by synthetic data low-bit quantization</a>, in:
26th International Conference on Pattern Recognition, ICPR 2022, Montreal,
QC, Canada, August 21-25, 2022, IEEE, 2022, pp. 855–862.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICPR56361.2022.9955645" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICPR56361.2022.9955645</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/ICPR56361.2022.9955645" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICPR56361.2022.9955645</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Damer, C. A. F. López, M. Fang, N. Spiller, M. V. Pham, F. Boutros,
<a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00167" title="" class="ltx_ref ltx_href">Privacy-friendly
synthetic data for the development of face morphing attack detectors</a>, in:
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
CVPR Workshops 2022, New Orleans, LA, USA, June 19-20, 2022, IEEE, 2022,
pp. 1605–1616.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00167" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPRW56347.2022.00167</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00167" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPRW56347.2022.00167</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
ISO/IEC JTC 1/SC 27 Information security, cybersecurity and privacy
protection, <a target="_blank" href="https://www.iso.org/standard/69373.html" title="" class="ltx_ref ltx_href">ISO/IEC
20889:2018 Privacy enhancing data de-identification terminology and
classification of techniques </a> (2018).

<br class="ltx_break">URL <a target="_blank" href="https://www.iso.org/standard/69373.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.iso.org/standard/69373.html</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
N. Damer, A. M. Saladie, A. Braun, A. Kuijper,
<a target="_blank" href="https://doi.org/10.1109/BTAS.2018.8698563" title="" class="ltx_ref ltx_href">Morgan: Recognition
vulnerability and attack detectability of face morphing attacks created by
generative adversarial network</a>, in: 9th IEEE International Conference on
Biometrics Theory, Applications and Systems, BTAS 2018, Redondo Beach, CA,
USA, October 22-25, 2018, IEEE, 2018, pp. 1–10.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/BTAS.2018.8698563" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/BTAS.2018.8698563</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/BTAS.2018.8698563" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/BTAS.2018.8698563</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
H. H. Nguyen, J. Yamagishi, I. Echizen, S. Marcel,
<a target="_blank" href="https://doi.org/10.1109/IJCB48548.2020.9304893" title="" class="ltx_ref ltx_href">Generating master faces
for use in performing wolf attacks on face recognition systems</a>, in: 2020
IEEE International Joint Conference on Biometrics, IJCB 2020, Houston,
TX, USA, September 28 - October 1, 2020, IEEE, 2020, pp. 1–10.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCB48548.2020.9304893" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IJCB48548.2020.9304893</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/IJCB48548.2020.9304893" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCB48548.2020.9304893</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
U. Scherhag, A. Nautsch, C. Rathgeb, M. Gomez-Barrero, R. N. J. Veldhuis,
L. J. Spreeuwers, M. Schils, D. Maltoni, P. Grother, S. Marcel,
R. Breithaupt, R. Raghavendra, C. Busch,
<a target="_blank" href="https://doi.org/10.23919/BIOSIG.2017.8053499" title="" class="ltx_ref ltx_href">Biometric systems under
morphing attacks: Assessment of morphing techniques and vulnerability
reporting</a>, in: A. Brömme, C. Busch, A. Dantcheva, C. Rathgeb, A. Uhl
(Eds.), International Conference of the Biometrics Special Interest Group,
BIOSIG 2017, Darmstadt, Germany, September 20-22, 2017, Vol. P-270 of
LNI, GI / IEEE, 2017, pp. 149–159.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.23919/BIOSIG.2017.8053499" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.23919/BIOSIG.2017.8053499</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.23919/BIOSIG.2017.8053499" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.23919/BIOSIG.2017.8053499</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. Tolosana, R. Vera-Rodríguez, J. Fiérrez, A. Morales,
J. Ortega-Garcia,
<a target="_blank" href="https://doi.org/10.1016/j.inffus.2020.06.014" title="" class="ltx_ref ltx_href">Deepfakes and beyond: A
survey of face manipulation and fake detection</a>, Inf. Fusion 64 (2020)
131–148.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.inffus.2020.06.014" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1016/j.inffus.2020.06.014</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1016/j.inffus.2020.06.014" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.inffus.2020.06.014</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
B. Meden, P. Rot, P. Terhörst, N. Damer, A. Kuijper, W. J. Scheirer,
A. Ross, P. Peer, V. Struc,
<a target="_blank" href="https://doi.org/10.1109/TIFS.2021.3096024" title="" class="ltx_ref ltx_href">Privacy-enhancing face
biometrics: A comprehensive survey</a>, IEEE Trans. Inf. Forensics Secur. 16
(2021) 4147–4183.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TIFS.2021.3096024" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TIFS.2021.3096024</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/TIFS.2021.3096024" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TIFS.2021.3096024</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
G. B. Huang, M. Ramesh, T. Berg, E. Learned-Miller, Labeled faces in the wild:
A database for studying face recognition in unconstrained environments, Tech.
Rep. 07-49, University of Massachusetts, Amherst (11 2007).

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, S. Zafeiriou,
<a target="_blank" href="https://doi.org/10.1109/CVPRW.2017.250" title="" class="ltx_ref ltx_href">Agedb: The first manually
collected, in-the-wild age database</a>, in: 2017 IEEE CVPRW, CVPR Workshops
2017, Honolulu, HI, USA, July 21-26, 2017, IEEE Computer Society, 2017, pp.
1997–2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPRW.2017.250" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPRW.2017.250</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/CVPRW.2017.250" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPRW.2017.250</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Sengupta, J. Chen, C. D. Castillo, V. M. Patel, R. Chellappa, D. W. Jacobs,
<a target="_blank" href="https://doi.org/10.1109/WACV.2016.7477558" title="" class="ltx_ref ltx_href">Frontal to profile face
verification in the wild</a>, in: 2016 IEEE Winter Conference on Applications
of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016,
IEEE Computer Society, 2016, pp. 1–9.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV.2016.7477558" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/WACV.2016.7477558</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/WACV.2016.7477558" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WACV.2016.7477558</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
T. Zheng, W. Deng, J. Hu, <a target="_blank" href="http://arxiv.org/abs/1708.08197" title="" class="ltx_ref ltx_href">Cross-age
LFW: A database for studying cross-age face recognition in unconstrained
environments</a>, CoRR abs/1708.08197 (2017).

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1708.08197" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1708.08197</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://arxiv.org/abs/1708.08197" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1708.08197</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
T. Zheng, W. Deng, Cross-pose lfw: A database for studying cross-pose face
recognition in unconstrained environments, Tech. Rep. 18-01, Beijing
University of Posts and Telecommunications (February 2018).

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, W. Liu,
<a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html" title="" class="ltx_ref ltx_href">Cosface:
Large margin cosine loss for deep face recognition</a>, in: 2018 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake
City, UT, USA, June 18-22, 2018, Computer Vision Foundation / IEEE Computer
Society, 2018, pp. 5265–5274.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00552" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2018.00552</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J. N. Kolf, T. Rieber, J. Elliesen, F. B. A. Kuijper, N. Damer, Identity-driven
three-player generative adversarial network for synthetic-based face
recognition, in: IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, CVPR Workshops 2023, Canada, June 19-20, 2023, IEEE,
2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
D. P. Kingma, M. Welling, <a target="_blank" href="http://arxiv.org/abs/1312.6114" title="" class="ltx_ref ltx_href">Auto-encoding
variational bayes</a>, in: Y. Bengio, Y. LeCun (Eds.), 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings, 2014.

<br class="ltx_break">URL <a target="_blank" href="http://arxiv.org/abs/1312.6114" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1312.6114</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. C. Courville, Y. Bengio,
<a target="_blank" href="https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html" title="" class="ltx_ref ltx_href">Generative
adversarial nets</a>, in: Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems
27: Annual Conference on Neural Information Processing Systems 2014, December
8-13 2014, Montreal, Quebec, Canada, 2014, pp. 2672–2680.

<br class="ltx_break">URL <a target="_blank" href="https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. van den Oord, N. Kalchbrenner, K. Kavukcuoglu,
<a target="_blank" href="http://proceedings.mlr.press/v48/oord16.html" title="" class="ltx_ref ltx_href">Pixel recurrent neural
networks</a>, in: M. Balcan, K. Q. Weinberger (Eds.), Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016, Vol. 48 of JMLR Workshop and Conference Proceedings,
JMLR.org, 2016, pp. 1747–1756.

<br class="ltx_break">URL <a target="_blank" href="http://proceedings.mlr.press/v48/oord16.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v48/oord16.html</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
I. Kobyzev, S. J. D. Prince, M. A. Brubaker,
<a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.2992934" title="" class="ltx_ref ltx_href">Normalizing flows: An
introduction and review of current methods</a>, IEEE Trans. Pattern Anal.
Mach. Intell. 43 (11) (2021) 3964–3979.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.2992934" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2020.2992934</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.2992934" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TPAMI.2020.2992934</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, P. Abbeel,
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="" class="ltx_ref ltx_href">Denoising
diffusion probabilistic models</a>, in: H. Larochelle, M. Ranzato, R. Hadsell,
M. Balcan, H. Lin (Eds.), Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020.

<br class="ltx_break">URL <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
M. Kowalski, S. J. Garbin, V. Estellers, T. Baltrusaitis, M. Johnson,
J. Shotton, <a target="_blank" href="https://doi.org/10.1007/978-3-030-58621-8_18" title="" class="ltx_ref ltx_href">CONFIG:
controllable neural face image generation</a>, in: A. Vedaldi, H. Bischof,
T. Brox, J. Frahm (Eds.), Computer Vision - ECCV 2020 - 16th European
Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, Vol.
12356 of Lecture Notes in Computer Science, Springer, 2020, pp. 299–315.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58621-8_18" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1007/978-3-030-58621-8\_18</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1007/978-3-030-58621-8_18" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-58621-8_18</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S. Bond-Taylor, A. Leach, Y. Long, C. G. Willcocks,
<a target="_blank" href="https://doi.org/10.1109/TPAMI.2021.3116668" title="" class="ltx_ref ltx_href">Deep generative modelling:
A comparative review of vaes, gans, normalizing flows, energy-based and
autoregressive models</a>, IEEE Trans. Pattern Anal. Mach. Intell. 44 (11)
(2022) 7327–7347.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2021.3116668" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2021.3116668</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/TPAMI.2021.3116668" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TPAMI.2021.3116668</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, T. Aila,
<a target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">A
style-based generator architecture for generative adversarial networks</a>, in:
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,
Long Beach, CA, USA, June 16-20, 2019, Computer Vision Foundation / IEEE,
2019, pp. 4401–4410.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00453" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2019.00453</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
P. J. Tinsley, A. Czajka, P. J. Flynn,
<a target="_blank" href="https://doi.org/10.1109/WACV48630.2021.00136" title="" class="ltx_ref ltx_href">This face does not
exist… but it might be yours! identity leakage in generative models</a>, in:
IEEE Winter Conference on Applications of Computer Vision, WACV 2021,
Waikoloa, HI, USA, January 3-8, 2021, IEEE, 2021, pp. 1319–1327.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV48630.2021.00136" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/WACV48630.2021.00136</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/WACV48630.2021.00136" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WACV48630.2021.00136</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Y. Shen, P. Luo, J. Yan, X. Wang, X. Tang,
<a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.html" title="" class="ltx_ref ltx_href">Faceid-gan:
Learning a symmetry three-player GAN for identity-preserving face
synthesis</a>, in: 2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, Computer
Vision Foundation / IEEE Computer Society, 2018, pp. 821–830.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00092" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2018.00092</span></a>.

<br class="ltx_break">URL <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.html</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Y. Deng, J. Yang, D. Chen, F. Wen, X. Tong,
<a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.html" title="" class="ltx_ref ltx_href">Disentangled
and controllable face image generation via 3d imitative-contrastive
learning</a>, in: 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, Computer Vision
Foundation / IEEE, 2020, pp. 5153–5162.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.00520" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR42600.2020.00520</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openaccess.thecvf.com/content_CVPR_2020/html/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.html</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A. Shoshan, N. Bhonker, I. Kviatkovsky, G. G. Medioni,
<a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01382" title="" class="ltx_ref ltx_href">Gan-control: Explicitly
controllable gans</a>, in: 2021 IEEE/CVF International Conference on Computer
Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, IEEE, 2021,
pp. 14063–14073.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01382" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCV48922.2021.01382</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.01382" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV48922.2021.01382</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Y. Shen, C. Yang, X. Tang, B. Zhou,
<a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.3034267" title="" class="ltx_ref ltx_href">Interfacegan: Interpreting
the disentangled face representation learned by gans</a>, IEEE Trans. Pattern
Anal. Mach. Intell. 44 (4) (2022) 2004–2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.3034267" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/TPAMI.2020.3034267</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/TPAMI.2020.3034267" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TPAMI.2020.3034267</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
H. Zhang, S. Venkatesh, R. Ramachandra, K. B. Raja, N. Damer, C. Busch,
MIPGAN - generating strong and high quality morphing attacks using identity
prior driven GAN, IEEE Trans. Biom. Behav. Identity Sci. 3 (3) (2021)
365–383.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
N. Damer, M. Fang, P. Siebke, J. N. Kolf, M. Huber, F. Boutros, Mordiff:
Recognition vulnerability and attack detectability of face morphing attacks
created by diffusion autoencoders, in: 11th IEEE International Workshop on
Biometrics and Forensics, IWBF 2023, Barcelona, Spain, April 19-20, 2023,
IEEE, 2023, pp. 1–6.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
P. Terhörst, F. Bierbaum, M. Huber, N. Damer, F. Kirchbuchner, K. B.
Raja, A. Kuijper, <a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007976" title="" class="ltx_ref ltx_href">On
the (limited) generalization of masterface attacks and its relation to the
capacity of face representations</a>, in: IEEE International Joint Conference
on Biometrics, IJCB 2022, Abu Dhabi, United Arab Emirates, October 10-13,
2022, IEEE, 2022, pp. 1–9.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007976" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IJCB54206.2022.10007976</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007976" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCB54206.2022.10007976</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
M. Huber, F. Boutros, A. T. Luu, K. B. Raja, R. Ramachandra, N. Damer, P. C.
Neto, T. Gonçalves, A. F. Sequeira, J. S. Cardoso, J. Tremoço,
M. Lourenço, S. Serra, E. Cermeño, M. Ivanovska, B. Batagelj,
A. Kronovsek, P. Peer, V. Struc,
<a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007950" title="" class="ltx_ref ltx_href">SYN-MAD 2022:
Competition on face morphing attack detection based on privacy-aware
synthetic training data</a>, in: IEEE International Joint Conference on
Biometrics, IJCB 2022, Abu Dhabi, United Arab Emirates, October 10-13,
2022, IEEE, 2022, pp. 1–10.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007950" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IJCB54206.2022.10007950</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/IJCB54206.2022.10007950" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IJCB54206.2022.10007950</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
M. Fang, M. Huber, N. Damer, Synthaspoof: Developing face presentation attack
detection based on privacy-friendly synthetic data, in: IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2023,
Canada, June 19-20, 2023, IEEE, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
V. Mirjalili, S. Raschka, A. Ross,
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2019.2924619" title="" class="ltx_ref ltx_href">Flowsan: Privacy-enhancing
semi-adversarial networks to confound arbitrary face-based gender
classifiers</a>, IEEE Access 7 (2019) 99735–99745.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ACCESS.2019.2924619" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ACCESS.2019.2924619</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://doi.org/10.1109/ACCESS.2019.2924619" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2019.2924619</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
P. Terhörst, M. Huber, N. Damer, P. Rot, F. Kirchbuchner, V. Struc,
A. Kuijper, <a target="_blank" href="https://dl.gi.de/20.500.12116/34330" title="" class="ltx_ref ltx_href">Privacy evaluation
protocols for the evaluation of soft-biometric privacy-enhancing
technologies</a>, in: A. Brömme, C. Busch, A. Dantcheva, K. B. Raja,
C. Rathgeb, A. Uhl (Eds.), BIOSIG 2020 - Proceedings of the 19th
International Conference of the Biometrics Special Interest Group, online,
16.-18. September 2020, Vol. P-306 of LNI, Gesellschaft für
Informatik e.V., 2020, pp. 215–222.

<br class="ltx_break">URL <a target="_blank" href="https://dl.gi.de/20.500.12116/34330" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.gi.de/20.500.12116/34330</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.01020" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.01021" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.01021">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.01021" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.01023" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 10:20:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
