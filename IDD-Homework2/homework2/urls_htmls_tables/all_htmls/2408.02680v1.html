<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Recording First-person Experiences to Build a New Type of Foundation Model</title>
<!--Generated on Wed Jul 31 11:48:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.02680v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S1" title="In Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2" title="In Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2.SS1" title="In 2 Background ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2.SS2" title="In 2 Background ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Personality Modelling with Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2.SS3" title="In 2 Background ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Emotions, Physiology and Decision-making</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2.SS4" title="In 2 Background ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Lifelogging and Google StreetView</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3" title="In Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>First-person Recorder</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.SS1" title="In 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Hardware and Software</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.SS2" title="In 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Preliminary Experiments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4" title="In Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.SS1" title="In 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>First-person Foundation Models (FPFMs)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.SS2" title="In 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Applications of First-person Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.SS3" title="In 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Next Steps</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S5" title="In Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Recording First-person Experiences to Build a New Type of Foundation Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dionis Barcari
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Gamez
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aliya Grig
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Department of Computer Science, Middlesex University, London, UK
</span>
<span class="ltx_contact ltx_role_address">Evolwe, 251 Little Falls Drive, Wilmington, New Castle County, Delaware 19808, United States
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Chat-GPT, are trained on large amounts of Internet data. However, it is becoming apparent that this data is likely to be exhausted soon, and technology companies are looking for new sources of data to train the next generation of foundation models.</p>
<p class="ltx_p" id="id2.id2">Reinforcement learning, RAG, prompt engineering and cognitive modelling are often used to fine-tune and augment the behaviour of foundation models. These techniques have been used to replicate people, such as Caryn Marjorie. These chatbots are not based on people’s actual emotional and physiological responses to their environment, so they are, at best, a surface-level approximation to the characters they are imitating.</p>
<p class="ltx_p" id="id3.id3">To address these issues, we have developed a recording rig that captures what the wearer is seeing and hearing as well as their skin conductance (GSR), facial expression and brain state (14 channel EEG). AI algorithms are used to process this data into a rich picture of the environment and internal states of the subject. Foundation models trained on this data could replicate human behaviour much more accurately than the personality models that have been developed so far. This type of model has many potential applications, including recommendation, personal assistance, GAN systems, dating and recruitment.</p>
<p class="ltx_p" id="id4.id4">This paper gives some background to this work and describes the recording rig and preliminary tests of its functionality. It then suggests how a new type of foundation model could be created from the data captured by the rig and outlines some applications. Data gathering and model training are expensive, so we are currently working on the launch of a start-up that could raise funds for the next stage of the project.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\paperid</span>
<p class="ltx_p" id="p1.2">0</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Llama, Chat-GPT and Dall-E, can generate plausible text and images in response to text and image prompts. These models are trained on large amounts of text and image data scraped from the Internet - often accessed through the Common Crawl repository. Researchers and technology companies are starting to realize that this data source could become exhausted soon, so they are looking for new sources of data to train the next generation of foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">After initial training the behaviour of foundation models can be fine-tuned through a combination of reinforcement learning, retrieval augmented generation (RAG) and prompt engineering. These techniques have been used to create fairly plausible imitations of individual people, such as Marilyn Monroe (https://www.soulmachines.com/) and Caryn Marjorie (https://caryn.ai). However, these chatbots are not based on people’s actual physiological and emotional responses, so they are, at best, a surface level approximation to the characters that they are imitating.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these issues this paper describes a new type of recording rig that captures what the wearer is seeing and hearing and their emotional and physiological reactions to these stimuli. Data recorded by this rig could be used to build a new type of foundation model, a first person-foundation model (FPFM), that maps environmental stimuli to a person’s emotional and physiological reactions, and maps a person’s physiological and emotional states to their behaviour. First-person foundation models could provide much more realistic personality modelling, which could be used for personal assistants, generative adversarial networks (GAN), dating, recruitment and to develop a new type of recommendation engine. This recording rig could also help to address the predicted shortage of training data for the next generation of foundation models.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The first part of this paper gives background information about foundation models, personality modelling with foundation models, the role of emotional and physiological states in decision-making, and similar hardware to our recording rig. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3" title="3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">3</span></a> describes the hardware and software of the first-person recorder that we have developed along with preliminary tests of its functionality. The final section discusses how data recorded by this rig could be used to train a FPFM, outlines some applications of FPFMs and suggests improvements that could be made to the recording rig in the future.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Foundation Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Foundation models often use a transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib14" title="">14</a>]</cite> with hundreds of billions of parameters. They are trained on large amounts of data using considerable computer resources. For example, GPT-1 was trained on 5GB, GPT-2 on 40GB, GPT-3 on 45TB and MusicGen on 20,000 hours of audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib2" title="">2</a>]</cite>. After training, foundation models are given a prompt, such as a text input, and they generate an output, such as text, code or images. Some examples of foundation models are given in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S2.T1" title="Table 1 ‣ 2.1 Foundation Models ‣ 2 Background ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of mappings carried out by some of the current foundation models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:65.4pt;">Input</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:28.5pt;">Output</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:85.4pt;">Foundation Models</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.2.1.1.1.1" style="width:65.4pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1" style="width:28.5pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.1.2.1.3.1.1" style="width:85.4pt;">GPT 3.5 Claude 3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.1.1">
<span class="ltx_p" id="S2.T1.1.3.2.1.1.1" style="width:65.4pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.2.1">
<span class="ltx_p" id="S2.T1.1.3.2.2.1.1" style="width:28.5pt;">Images</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.3.1">
<span class="ltx_p" id="S2.T1.1.3.2.3.1.1" style="width:85.4pt;">DALL-E, Stable Diffusion, GPT 4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.1.1">
<span class="ltx_p" id="S2.T1.1.4.3.1.1.1" style="width:65.4pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1" style="width:28.5pt;">Code</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.3.1">
<span class="ltx_p" id="S2.T1.1.4.3.3.1.1" style="width:85.4pt;">CoPilot, CodeWhisperer</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.1.1">
<span class="ltx_p" id="S2.T1.1.5.4.1.1.1" style="width:65.4pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.2.1">
<span class="ltx_p" id="S2.T1.1.5.4.2.1.1" style="width:28.5pt;">Music</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.3.1">
<span class="ltx_p" id="S2.T1.1.5.4.3.1.1" style="width:85.4pt;">MusicGen</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.1.1">
<span class="ltx_p" id="S2.T1.1.6.5.1.1.1" style="width:65.4pt;">Text, images</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.2.1">
<span class="ltx_p" id="S2.T1.1.6.5.2.1.1" style="width:28.5pt;">Robot actions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.3.1">
<span class="ltx_p" id="S2.T1.1.6.5.3.1.1" style="width:85.4pt;">RT-2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.1.1">
<span class="ltx_p" id="S2.T1.1.7.6.1.1.1" style="width:65.4pt;">Text, images, DNA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.2.1">
<span class="ltx_p" id="S2.T1.1.7.6.2.1.1" style="width:28.5pt;">Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.3.1">
<span class="ltx_p" id="S2.T1.1.7.6.3.1.1" style="width:85.4pt;">Med-PaLM M</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.1.1">
<span class="ltx_p" id="S2.T1.1.8.7.1.1.1" style="width:65.4pt;">DNA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.2.1">
<span class="ltx_p" id="S2.T1.1.8.7.2.1.1" style="width:28.5pt;">Cellular function</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.3.1">
<span class="ltx_p" id="S2.T1.1.8.7.3.1.1" style="width:85.4pt;">Geneformer, scGPT</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The behaviour of foundation models is often fine-tuned through a combination of reinforcement learning, retrieval augmented generation (RAG) and prompt engineering. In reinforcement learning the model generates multiple different outputs, which are ranked, typically by a human, and then it is trained to generate the desired output more frequently in the future. In retrieval augmented generation a vector database is created with a set of documents that the foundation model is required to use. When the user enters a query, a vector search identifies documents that are most relevant to the user’s input and these are combined with the user’s query in the final prompt that is sent to the model. Prompt engineering is a variety of techniques that are used to structure prompts to get desired outputs. These include intents, roles, chains of thought and output constraints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Personality Modelling with Foundation Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Foundation models can imitate the conversational styles of individual people. With simple prompt engineering, Chat-GPT can generate text in the style of personalities, such as Donald Trump, whose speeches and tweets were included in the original training data. More sophisticated models have been created by companies like Facebook (imitations of Snoop Dogg, Tom Brady and other celebrities) and UneeQ, who constructs digital avatars with different personalities (https://www.digitalhumans.com/). For the most part, these chatbots appear to be generated by a combination of reinforcement learning, RAG and prompt engineering. The only potential exception that we are aware of is Soul Machines (https://www.soulmachines.com/), whose website claims that their AI characters are based on a combination of LLMs and multimodal cognitive models. Details are lacking, but our best guess is that cognitive models are used to simulate the agent’s state, which is combined with previous knowledge (retrieved using RAG) to build the LLM prompt. Similar work has been carried out by Park et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib11" title="">11</a>]</cite> and Kirk et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Emotions, Physiology and Decision-making</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">It is becoming increasingly recognized that our emotions and physiological states play a central role in our decision-making and behaviour. Consider a person who is sitting in front of a beef burger in a restaurant. Suppose they are experiencing hunger, and they predict that eating the burger will cause them to experience pleasurable sensations and a feeling of satiety. In this case their current and predicted physiological state explain their action of eating the burger. On the other hand, suppose that the person is feeling sick or cares deeply about animals. In this case they will not eat the burger and this behaviour will, again, be explainable in terms of their current and predicted emotional and physiological states. This relationship between emotions, physiology and decision-making is nicely described by Damasio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib4" title="">4</a>]</cite>, whose theory of somatic markers explains how we associate positive and negative emotions with objects in our environment. Goel’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib6" title="">6</a>]</cite> theory of tethered rationality is based on the idea that emotional and physiological states play a central role in the selection and initiation of behaviour. A close relationship between emotions and decision making has also been demonstrated in many psychological studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Within AI research, the important role that emotion plays in cognition is coming to be more widely recognized. As Pessoa puts it "Emotion is not an ’add on’ that endows a robot with ’feelings’ (for instance, reporting or expressing its internal state). It allows the significance of percepts, plans, and actions to be an integral part of all its computations." <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib12" title="">12</a>, p. 168]</cite>. An AI model of a person that does not include their emotional and physiological reactions will, at best, approximate the surface level. It will not include the motivation behind their behaviours or the diversity of people’s behaviours (the different burger-eating outcomes in our example).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Lifelogging and Google StreetView</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">A variety of life logging devices have been created to store continuous records of people’s lives. For example, the Narrative Clip, worn in front of the user, takes a picture every 30 seconds, and many phone apps have been developed to track different aspects of users’ lives. As far as we are aware, no-one has developed a life logging device that captures both the environment and the emotional and physiological states of the wearer. Our recorder is also similar to Google StreetView, which uses cameras mounted on cars and people to capture exterior and interior environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>First-person Recorder</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our hypothesis is that a foundation model that is trained from scratch on the stimuli and emotional and physiological states of a person will replicate human behaviour more effectively than surface-level approximations built with LLMs, RAG, cognitive models and prompt engineering. First-person recorders could also help to plug the estimated shortfall of data that is required to train the next generation of foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib13" title="">13</a>]</cite>. To test this hypothesis, we have built a recording rig that captures high quality data from the wearer and stores it in a format that is suitable for training a foundation model.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Hardware and Software</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The recorder is based on a Raspberry Pi, worn around the user’s neck, which is connected to a camera, microphone, GSR sensor and speaker. Data recorded by the Raspberry Pi is sent to a web service running on a laptop carried by the user, which has a WebSocket connection to the Epoc X EEG headset. Cloud services, such as AWS Rekognition, and the Emotiv Cortex API<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://emotiv.gitbook.io/cortex-api</span></span></span>, are used to analyse the raw data for higher level properties, such as text contents, sentiment, cognition, facial expression, and object labels. A website hosted on the laptop enables the recorder to be configured and supports playback of recorded data (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.F3" title="Figure 3 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">3</span></a>). To reduce fraud a blockchain architecture is implemented that sends a hash of the data to the cloud and receives a hash of the data plus a random number known only to the cloud service, which is added to the next file in the sequence. To ensure the privacy of other people, all faces are automatically blurred during the recording process. The data is stored in JSON files; schema definitions for these files are available on the project website. Version 1.0 of the recorder is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.F1" title="Figure 1 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">1</span></a>. The architecture is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.F2" title="Figure 2 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">2</span></a> and the data stored by the recorder is summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.T2" title="Table 2 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="424" id="S3.F1.g1" src="extracted/5766196/RecordingRig.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>First-person recorder.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F2.g1" src="extracted/5766196/Architecture.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture. The Raspberry Pi worn around the user’s neck collects data from the camera, microphone and GSR sensor and sends it to a web service running on a laptop worn on the user’s back. The Emotiv Epoc X EEG headset sends data to the laptop using the Emotiv Cortex API. A speaker attached to the Raspberry Pi emits a tone at random intervals for descriptive experience sampling (DES) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib7" title="">7</a>]</cite>. A data processing program running on the laptop collects the data and uses Amazon Web Services (AWS) to identify text, sentiment and image labels. When the data for the file is complete, a hash of the file contents is sent to another cloud web service, which stores the original hash and sends back a new hash that combines the original hash with a random number. A web interface is provided to configure the recorder and play back recorded data.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Data recorded by the rig. Schemas for the output files and an example recording are available at the project’s GitHub page. Sample rates, such as image frequency, are configurable through the web interface. The data rates are based on the test recording on the project’s GitHub repository, which contains one image per second. The data rates can vary, depending on the behaviour and environment of the user.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1" style="width:42.7pt;">Data</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1" style="width:42.7pt;">Source</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.1.3.1.1" style="width:85.4pt;">Description</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.4.1">
<span class="ltx_p" id="S3.T2.1.1.1.4.1.1" style="width:28.5pt;">Rate (Kbps)</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1" style="width:42.7pt;">EEG</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1" style="width:42.7pt;">Emotiv Epoc X headset</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.2.1.3.1.1" style="width:85.4pt;">Raw EEG data from 14 channels</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.4.1">
<span class="ltx_p" id="S3.T2.1.2.1.4.1.1" style="width:28.5pt;">30</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1" style="width:42.7pt;">Audio</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1" style="width:42.7pt;">USB microphone</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.3.1">
<span class="ltx_p" id="S3.T2.1.3.2.3.1.1" style="width:85.4pt;">Audio from microphone mounted on front of user.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.3.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.4.1">
<span class="ltx_p" id="S3.T2.1.3.2.4.1.1" style="width:28.5pt;">20(mp3)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1.1" style="width:42.7pt;">Images</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.2.1">
<span class="ltx_p" id="S3.T2.1.4.3.2.1.1" style="width:42.7pt;">ZeroCam</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.3.1">
<span class="ltx_p" id="S3.T2.1.4.3.3.1.1" style="width:85.4pt;">Pictures from camera mounted in front of user.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.4.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.4.1">
<span class="ltx_p" id="S3.T2.1.4.3.4.1.1" style="width:28.5pt;">600(jpg)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.1.1">
<span class="ltx_p" id="S3.T2.1.5.4.1.1.1" style="width:42.7pt;">GSR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.2.1">
<span class="ltx_p" id="S3.T2.1.5.4.2.1.1" style="width:42.7pt;">Grove GSR sensor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.3.1">
<span class="ltx_p" id="S3.T2.1.5.4.3.1.1" style="width:85.4pt;">Galvanic skin response (GSR) of user.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.5.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.4.1">
<span class="ltx_p" id="S3.T2.1.5.4.4.1.1" style="width:28.5pt;">0.01(1 Hz)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.1.1">
<span class="ltx_p" id="S3.T2.1.6.5.1.1.1" style="width:42.7pt;">EEG band power</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.2.1">
<span class="ltx_p" id="S3.T2.1.6.5.2.1.1" style="width:42.7pt;">EmotivCortex API</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.3.1">
<span class="ltx_p" id="S3.T2.1.6.5.3.1.1" style="width:85.4pt;">EEG power in the theta, alpha, beta L, beta H, and gamma bands.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.6.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.4.1">
<span class="ltx_p" id="S3.T2.1.6.5.4.1.1" style="width:28.5pt;">8</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.1.1">
<span class="ltx_p" id="S3.T2.1.7.6.1.1.1" style="width:42.7pt;">Facial expression</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.2.1">
<span class="ltx_p" id="S3.T2.1.7.6.2.1.1" style="width:42.7pt;">EmotivCortex API</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.3.1">
<span class="ltx_p" id="S3.T2.1.7.6.3.1.1" style="width:85.4pt;">Eye action and expression on upper and lower face.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.7.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.4.1">
<span class="ltx_p" id="S3.T2.1.7.6.4.1.1" style="width:28.5pt;">4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.7.1.1">
<span class="ltx_p" id="S3.T2.1.8.7.1.1.1" style="width:42.7pt;">Cognition</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.7.2.1">
<span class="ltx_p" id="S3.T2.1.8.7.2.1.1" style="width:42.7pt;">EmotivCortex API</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.7.3.1">
<span class="ltx_p" id="S3.T2.1.8.7.3.1.1" style="width:85.4pt;">Cognitive states, including engagement, excitement, stress, relaxation, interest and focus.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.8.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.7.4.1">
<span class="ltx_p" id="S3.T2.1.8.7.4.1.1" style="width:28.5pt;">0.02</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.9.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.8.1.1">
<span class="ltx_p" id="S3.T2.1.9.8.1.1.1" style="width:42.7pt;">Audio text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.9.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.8.2.1">
<span class="ltx_p" id="S3.T2.1.9.8.2.1.1" style="width:42.7pt;">AWSTranscribe</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.9.8.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.8.3.1">
<span class="ltx_p" id="S3.T2.1.9.8.3.1.1" style="width:85.4pt;">Recorded audio is converted to text using AWS Transcribe. Speech of wearer is automatically separated out from other people’s speech using an audio sample generated by the user.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.9.8.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.8.4.1">
<span class="ltx_p" id="S3.T2.1.9.8.4.1.1" style="width:28.5pt;">0.003</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.10.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.10.9.1.1">
<span class="ltx_p" id="S3.T2.1.10.9.1.1.1" style="width:42.7pt;">Speech sentiment</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.10.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.10.9.2.1">
<span class="ltx_p" id="S3.T2.1.10.9.2.1.1" style="width:42.7pt;">AWSComprehend</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.10.9.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.10.9.3.1">
<span class="ltx_p" id="S3.T2.1.10.9.3.1.1" style="width:85.4pt;">Text generated by user is analysed for sentiment (positive, negative, mixed, and neutral) using AWS Comprehend.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.10.9.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.10.9.4.1">
<span class="ltx_p" id="S3.T2.1.10.9.4.1.1" style="width:28.5pt;">0.002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.11.10.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.11.10.1.1">
<span class="ltx_p" id="S3.T2.1.11.10.1.1.1" style="width:42.7pt;">DES</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.11.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.11.10.2.1">
<span class="ltx_p" id="S3.T2.1.11.10.2.1.1" style="width:42.7pt;">User</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.11.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.11.10.3.1">
<span class="ltx_p" id="S3.T2.1.11.10.3.1.1" style="width:85.4pt;">Descriptive Experience Sampling (DES) is a technique in which a person describes the contents of their consciousness when prompted by a tone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib7" title="">7</a>]</cite>. Key phrases, such as “Start Ziggy” and “End Ziggy”, identify the start and end of a DES report.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.11.10.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.11.10.4.1">
<span class="ltx_p" id="S3.T2.1.11.10.4.1.1" style="width:28.5pt;">0.001(per report)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.12.11.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.12.11.1.1">
<span class="ltx_p" id="S3.T2.1.12.11.1.1.1" style="width:42.7pt;">Image text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.12.11.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.12.11.2.1">
<span class="ltx_p" id="S3.T2.1.12.11.2.1.1" style="width:42.7pt;">AWSRekognition</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.12.11.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.12.11.3.1">
<span class="ltx_p" id="S3.T2.1.12.11.3.1.1" style="width:85.4pt;">Text in recorded images is identified using AWS Rekognition.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.12.11.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.12.11.4.1">
<span class="ltx_p" id="S3.T2.1.12.11.4.1.1" style="width:28.5pt;">0.001</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.1.13.12.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.13.12.1.1">
<span class="ltx_p" id="S3.T2.1.13.12.1.1.1" style="width:42.7pt;">Image labels</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.1.13.12.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.13.12.2.1">
<span class="ltx_p" id="S3.T2.1.13.12.2.1.1" style="width:42.7pt;">AWSRekognition</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.1.13.12.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.13.12.3.1">
<span class="ltx_p" id="S3.T2.1.13.12.3.1.1" style="width:85.4pt;">Labels for objects in recorded images are generated using AWS Rekognition.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.1.13.12.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.13.12.4.1">
<span class="ltx_p" id="S3.T2.1.13.12.4.1.1" style="width:28.5pt;">2</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="445" id="S3.F3.g1" src="extracted/5766196/WebInterface.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Web interface for recorder.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Code, 3D print files, JSON schemas for the data files, user manual and sample recordings are available at the project’s GitHub repository: https://github.com/ancara22/Data-Recording-Rig.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Preliminary Experiments</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Initial tests were carried out in a live environment on campus at Middlesex University, London. These tests established that the hardware could capture, process and store the data in real time. A sample of data recorded during these tests can be downloaded from the project’s GitHub repository.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">A second experiment was carried using images from the Socio-Moral Image Database (SMID), which were selected by Crone et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib3" title="">3</a>]</cite> for their ability to elicit specific emotions in the viewer. Crone et al. also measured the valence and arousal that were elicited by these images in 2,716 participants. To compare the data captured by our recorder with the results from Crone et al.’s experiments, a proxy for arousal was calculated from the combination of excitement and stress (see Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.T2" title="Table 2 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">2</span></a>) scaled to a value between -2.5 and 2.5. A selection of SMID images were then presented for 20 seconds each to a person wearing the recorder. The results are plotted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.F4" title="Figure 4 ‣ 3.2 Preliminary Experiments ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="S3.F4.g1" src="extracted/5766196/ArousalGraph.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mean arousal levels in experiments by Crone et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib3" title="">3</a>]</cite> and arousal levels recorded from a subject while viewing a sample of images from the SMID data set.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The results presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.F4" title="Figure 4 ‣ 3.2 Preliminary Experiments ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">4</span></a> are only given to illustrate how data recorded by the rig could be connected to the emotional and physiological states of a subject. Much more work needs to be done to combine the different data streams into a single picture of the subject’s emotional and physiological state (see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.SS3" title="4.3 Next Steps ‣ 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>First-person Foundation Models (FPFMs)</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The primary aim of this work is to record data from individuals and train a first-person foundation model that can carry out the following mappings:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">Image/audio/text -&gt; Emotional/physiological state</span>. How people’s emotional and physiological state changes in response to different stimuli. For example, I feel sad when I look at a picture of my deceased grandmother.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Emotional/physiological state -&gt; External behaviour</span>. What people do or say when they are feeling a particular way. For example, I engage in food-seeking behaviour when I am hungry.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Image/audio/text + Emotional/physiological state -&gt; External behaviour</span>. What people do or say when they are feeling a certain way and experience a particular stimuli. For example, I eat a burger when I perceive a burger in front of me and I am hungry.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The data in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S3.T2" title="Table 2 ‣ 3.1 Hardware and Software ‣ 3 First-person Recorder ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the recorder can capture ~40 GB of data (images, audio and text) in a 16 hour day. If the images, audio, raw EEG and raw GSR are excluded, this figure drops to ~1 GB per 16 hour day. Based on these figures, Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.T3" title="Table 3 ‣ 4.1 First-person Foundation Models (FPFMs) ‣ 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">3</span></a> gives some rough estimates of how long it would take to store enough data to train foundation models on the scale of GPT-1, GPT-2 and GPT-3.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Time required to record enough full data or text data (without raw GSR or EEG) to train GPT-1, GPT-2 and GPT-3. The recording times are based on 16 hour days.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.1.1">
<span class="ltx_p" id="S4.T3.1.1.1.1.1.1" style="width:28.5pt;">Model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.1.2.1.1" style="width:34.1pt;">Training Data (GB)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.3.1">
<span class="ltx_p" id="S4.T3.1.1.1.3.1.1" style="width:56.9pt;">Recording Time(days, full)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.4.1">
<span class="ltx_p" id="S4.T3.1.1.1.4.1.1" style="width:71.1pt;">Recording time(days, selected text)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.1.1">
<span class="ltx_p" id="S4.T3.1.2.2.1.1.1" style="width:28.5pt;">GPT-1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.2.1">
<span class="ltx_p" id="S4.T3.1.2.2.2.1.1" style="width:34.1pt;">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.3.1">
<span class="ltx_p" id="S4.T3.1.2.2.3.1.1" style="width:56.9pt;">0.14</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.4.1">
<span class="ltx_p" id="S4.T3.1.2.2.4.1.1" style="width:71.1pt;">6.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.1.1">
<span class="ltx_p" id="S4.T3.1.3.3.1.1.1" style="width:28.5pt;">GPT-2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.2.1">
<span class="ltx_p" id="S4.T3.1.3.3.2.1.1" style="width:34.1pt;">40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.3.1">
<span class="ltx_p" id="S4.T3.1.3.3.3.1.1" style="width:56.9pt;">1.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.4.1">
<span class="ltx_p" id="S4.T3.1.3.3.4.1.1" style="width:71.1pt;">52</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.4.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.1.1">
<span class="ltx_p" id="S4.T3.1.4.4.1.1.1" style="width:28.5pt;">GPT-3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.4.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.2.1">
<span class="ltx_p" id="S4.T3.1.4.4.2.1.1" style="width:34.1pt;">46080</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.4.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.3.1">
<span class="ltx_p" id="S4.T3.1.4.4.3.1.1" style="width:56.9pt;">1300</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.4.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.4.1">
<span class="ltx_p" id="S4.T3.1.4.4.4.1.1" style="width:71.1pt;">60000</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">According to Table <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.T3" title="Table 3 ‣ 4.1 First-person Foundation Models (FPFMs) ‣ 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">3</span></a>, a purely text based version of GPT-2 could be created from ~50 days of data from a single individual. Larger models are likely to require data recorded from several individuals.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">To obtain this data one or more people could be paid to wear the recorder for an extended period of time. This could be accomplished through companies, such as Surge AI (https://www.surgehq.ai/), who recruit people to generate training data and fine tune AI models. Another option would be to release a cheap version of the recorder and launch a marketplace where people could be paid for their recordings. People could also be motivated to use the recorder in exchange for benefits, such as life logging, personal assistance, enhanced recommendation, or generation of media content based on their lives.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This was dramatized in the Joan is Awful episode of <span class="ltx_text ltx_font_italic" id="footnote2.1">Black Mirror</span>.</span></span></span> This would be similar to the way in which big technology companies give us free and useful services, such as email, calendars and social networking, in exchange for access to our personal data.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">When enough data has been collected, it could be used to train one or more first-person foundation models. Data gathering and model training are expensive, so we are currently working on the launch of a start-up that could raise funds for the next stage of the project.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Applications of First-person Foundation Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">FPFMs have many potential applications:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">Recommendation</span>. Modern recommendation systems are often based on keywords that are associated with films, music, or products. The keywords of previous products that the customer has consumed are mapped in a high dimensional space. Clustering and other techniques are then used to identify similar products to the ones that the customer has already consumed. FPFMs could be used to create a new form of recommendation engine that measures the customer’s actual preferences for each product and recommends the ones with the highest net valence to the customer. For example, a FPFM based on <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.2">my</span> emotional reactions could watch every single film and TV series on Netflix and recommend the ones that generate the most positive emotional states.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">Focus groups</span>. FPFMs based on target audiences could be used to evaluate films, products, political policies, etc. prior to their release.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.1">Dialog in novels and scripts</span>. Current foundation models, such as GPT-4, are already being used to generate novels and scripts. FPFMs could model characters more effectively, leading to more realistic dialog. It would also be possible to use FPFMs based on recordings of individual actors to generate scripts tailored to these actors. For example, a FPFM of Tom Cruise could be used to write the script for the next <span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.2">Mission Impossible</span> movie.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i4.p1.1.1">Personal assistants</span>. A FPFM that understands the users preferences could book holidays, restaurants, etc. based on their preferences.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i5.p1.1.1">GAN systems</span>. A FPFM could be used as the discriminator in a generative adversarial network, providing feedback about whether text, music, images, etc. generated by a foundation model are likely to produce positive emotional responses in a specific consumer. This could be a powerful way of improving the output quality of foundation models without human feedback.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i6.p1.1.1">Dating and recruitment</span>. Volar (https://www.volardating.com) uses the interactions between chatbots based on two people to reduce the awkwardness of first dates. Interactions between FPFMs of prospective partners would be a much more accurate way of evaluating the suitability of a match<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This was dramatized in the Hang the DJ episode of <span class="ltx_text ltx_font_italic" id="footnote3.1">Black Mirror</span>.</span></span></span>. A similar method could be used to evaluate whether job candidates are compatible with a company’s current team.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Other applications include bereavement support, assisting dementia patients with a model of their former selves, patient models for psychologist training and phobia treatment.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Next Steps</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The backend of Version 1.0 of the recorder was implemented in a laptop carried in a backpack to reduce development costs. In the future this backend could be migrated to the cloud. While the Raspberry Pi could be replaced with a smartphone app, the battery drain is likely to be prohibitively high and people’s frequent use of their phones would compromise the quality of the recordings. Something like the Humane AI pin (https://humane.com) would be a more suitable upgrade. Future versions of the recorder could also pull data from physiological trackers, such as the Fitbit.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Further work is required to integrate the recorded signals into a single picture of the emotional and physiological state of the wearer. This could be done through a machine learning approach, in which the DES data is used as the labels and a deep network is trained to automatically generate the labels from the recorded data. Since people’s emotional and physiological reactions vary widely, it is likely to be necessary to calibrate the recorder for each user.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The camera mounted on the user’s chest captures their field of view, but not what they are actually looking at. One solution would be to use eye-tracking technology - for example, the Tobii Pro<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>See: https://www.tobii.com/.</span></span></span> - to record what the wearer is seeing. High quality eye-tracking hardware is expensive, so this could be approximated by increasing the resolution of the camera and using an attention model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#bib.bib8" title="">8</a>]</cite> to make a best guess about the contents of the wearer’s vision. The attention model could be combined with top-down knowledge of which objects are likely to be salient to the user. For example, if the image contained a spider and the user had strong negative reactions to spiders, then it is likely that their eyes would saccade to the spider.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">The first-person recorder captures everything that the wearer is exposed to, including copyrighted books, music, and films. To reduce this problem, a FPFM based on a single individual could be used <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">privately</span> by that individual (for recommendation, etc. - see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02680v1#S4.SS2" title="4.2 Applications of First-person Foundation Models ‣ 4 Discussion ‣ Recording First-person Experiences to Build a New Type of Foundation Model"><span class="ltx_text ltx_ref_tag">4.2</span></a>) without being made available to third parties. This would also ensure the privacy of the user’s data. Another option would be to use GPS to switch the recorder off automatically in situations in which copyright is likely to be an issue, such as concerts and cinemas. Recordings could also be automatically screened for copyrighted content. The elimination of copyrighted content from the model would reduce the power of the FPFM as a recommendation engine, since it would no longer be storing the user’s reactions to books, films, and music.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">The automatic blurring of faces that was introduced to protect privacy would prevent a FPFM trained on the data from learning the strong emotional reactions that we have to familiar faces, such as friends, family, colleagues and celebrities. To address this issue people could be asked to give their consent to have their faces recorded by the device. This could be at an individual level (I give my consent for my wife to record my face on her device) or globally (celebrities could give their consent to be recorded by anyone). Face recognition could then be used to record the faces of consenting people in an unblurred state.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The central role that emotions and physiology play in the selection and initiation of behaviour is widely acknowledged in neuroscience and psychology, and it is starting to be recognised in artificial intelligence research. If this interpretation of the importance of emotion is correct, foundation models that are trained on text and image data from the Internet will only be able to create surface-level approximations to the behaviour of individual people. Technology companies are also starting to understand the limits of data scraped from the Internet. Many are now searching for new sources of data that could be used to train the next generation of foundation models.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">To address these issues, we have developed a recording rig that stores what the wearer is seeing and hearing as well as their emotional and physiological reactions to their environment. A first-person foundation model trained on this data would be able to model human minds much more realistically than existing systems. There are many other exciting applications of FPFMs, including recommendation, focus groups, dialog writing, GAN systems, dating and recruitment. We are currently working on the launch of a start-up that could raise the funds to record a substantial amount of first-person data and train a FPFM model.</p>
</div>
<div class="ltx_para" id="S5.p3">
<span class="ltx_ERROR undefined" id="S5.p3.1">{ack}</span>
<p class="ltx_p" id="S5.p3.2">We would like to express our thanks to Evolwe (https://evolwe.ai) for financially supporting this work. We are particularly grateful to Alexander Morozov and Anastasia Rizzo from Evolwe, for their helpful feedback throughout the project. This research was approved by the ethics committee at Middlesex University, London.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
B. Chen, Z. Zhang, N. Langrené, and S. Zhu.

</span>
<span class="ltx_bibblock">Unleashing the potential of prompt engineering in large language
models: A comprehensive review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv:cs.CL/2310.14735</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Copet et al. [2024]</span>
<span class="ltx_bibblock">
J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and
A. Défossez.

</span>
<span class="ltx_bibblock">Simple and controllable music generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crone et al. [2018]</span>
<span class="ltx_bibblock">
D. L. Crone, S. Bode, C. Murawski, and S. M. Laham.

</span>
<span class="ltx_bibblock">The socio-moral image database (smid): A novel stimulus set for the
study of social, moral and affective processes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">PLOS ONE</em>, 13(1):1–34, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damasio [1994]</span>
<span class="ltx_bibblock">
A. R. Damasio.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Descartes’ Error: Emotion, Reason, and the Human Brain</em>.

</span>
<span class="ltx_bibblock">G.P. Putnam, New York, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">George and Dane [2016]</span>
<span class="ltx_bibblock">
J. M. George and E. Dane.

</span>
<span class="ltx_bibblock">Affect, emotion, and decision making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Organizational Behavior and Human Decision Processes</em>,
136:47–55, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goel [2022]</span>
<span class="ltx_bibblock">
V. Goel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Reason and Less: Pursuing Food, Sex, and Politics</em>.

</span>
<span class="ltx_bibblock">The MIT Press, Cambridge, Massachusetts, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hurlburt and Akhter [2006]</span>
<span class="ltx_bibblock">
R. T. Hurlburt and S. A. Akhter.

</span>
<span class="ltx_bibblock">The descriptive experience sampling method.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Phenomenology and the Cognitive Sciences</em>, 5:271–301, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itti and Koch [2001]</span>
<span class="ltx_bibblock">
L. Itti and C. Koch.

</span>
<span class="ltx_bibblock">Computational modelling of visual attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Nature Reviews Neuroscience</em>, 2(3):194–203, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirk et al. [2023]</span>
<span class="ltx_bibblock">
J. R. Kirk, R. E. Wray, and J. E. Laird.

</span>
<span class="ltx_bibblock">Exploiting language models as a source of knowledge for cognitive
agents.

</span>
<span class="ltx_bibblock">In C. Geib and R. Petrick, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2023
AAAI Fall Symposia</em>, volume 2, pages 286–94. The AAAI Press, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lerner et al. [2015]</span>
<span class="ltx_bibblock">
J. S. Lerner, Y. Li, P. Valdesolo, and K. S. Kassam.

</span>
<span class="ltx_bibblock">Emotion and decision making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Annual Review of Psychology</em>, 66:799–823, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2023]</span>
<span class="ltx_bibblock">
J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">UIST ’23: Proceedings of the 36th Annual ACM Symposium on
User Interface Software and Technology</em>, pages 1–22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pessoa [2019]</span>
<span class="ltx_bibblock">
L. Pessoa.

</span>
<span class="ltx_bibblock">Intelligent architectures for robotics: The merging of cognition and
emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Physics of Life Reviews</em>, 31:157–170, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seetharaman [2024]</span>
<span class="ltx_bibblock">
D. Seetharaman.

</span>
<span class="ltx_bibblock">For data-guzzling ai companies, the internet is too small.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Wall Street Journal</em>, 2024.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.wsj.com/tech/ai/ai-training-data-synthetic-openai-anthropic-9230f8d8</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.
Kaiser, and I. Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural
Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et al. [2022]</span>
<span class="ltx_bibblock">
P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv:cs.LG/2211.04325</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, and Y. Tang.

</span>
<span class="ltx_bibblock">A brief overview of chatgpt: The history, status quo and potential
future development.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE/CAA Journal of Automatica Sinica</em>, 10(5):1122–1136, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 31 11:48:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
