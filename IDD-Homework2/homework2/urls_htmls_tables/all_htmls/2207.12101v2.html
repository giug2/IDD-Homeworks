<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.12101] Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?</title><meta property="og:description" content="The use of Deep Learning and Computer Vision in the Cultural Heritage domain is becoming highly relevant in the last few years with lots of applications about audio smart guides, interactive museums and augmented reali‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.12101">

<!--Generated on Tue Feb 27 03:00:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Visual Question Answering,  GPT-3,  Image captioning,  natural language processing,  computer vision">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Florence, MICC, Italy
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>name.surname@unifi.it</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pietro Bongini 
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-8217-6266" title="ORCID identifier" class="ltx_ref">0000-0002-8217-6266</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Federico Becattini
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-2537-2700" title="ORCID identifier" class="ltx_ref">0000-0003-2537-2700</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alberto Del Bimbo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-1052-8322" title="ORCID identifier" class="ltx_ref">0000-0002-1052-8322</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The use of Deep Learning and Computer Vision in the Cultural Heritage domain is becoming highly relevant in the last few years with lots of applications about audio smart guides, interactive museums and augmented reality. All these technologies require lots of data to work effectively and be useful for the user. In the context of artworks, such data is annotated by experts in an expensive and time consuming process. In particular, for each artwork, an image of the artwork and a description sheet have to be collected in order to perform common tasks like Visual Question Answering. In this paper we propose a method for Visual Question Answering that allows to generate at runtime a description sheet that can be used for answering both visual and contextual questions about the artwork, avoiding completely the image and the annotation process. For this purpose, we investigate on the use of GPT-3 for generating descriptions for artworks analyzing the quality of generated descriptions through captioning metrics. Finally we evaluate the performance for Visual Question Answering and captioning tasks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Visual Question Answering, GPT-3, Image captioning, natural language processing, computer vision
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Cultural Heritage often relies on digital resources to engage and attract visitors. From audio-guides to smartphone applications, museum visits are becoming increasingly more interactive, allowing users to deepen concepts without the need of a human assistant or after the visit is concluded.
Forms of gamification are also important, favoring engagement especially for young visitors and instructional purposes.
Artificial Intelligence and Computer Vision are playing a large part in the development of such smart visits and applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. A notable machine learning application that has recently found usage in cultural heritage is Visual Question Answering (VQA), which exploits both Computer Vision and Natural Language Processing to allow users to ask questions on the content of an image¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The advantage of VQA is that it allows museums to develop smart guides and interactive gamification approaches. However, for pictorial art, most questions posed by users concern contextual information rather than what is actually depicted in a painting.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address this limitation, an evolution of VQA known as Contextual Question Answering (CQA) was proposed¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The authors explicitly focused on cultural heritage applications, combining visual and contextual cues to answer questions.
The contextual information is derived from a textual meta-data, which is fed to the model along with the question and the image. In this way the VQA/CQA model has to learn to attend either relevant parts of an image or relevant sections of the text to provide an adequate answer.
The need of a textual data nonetheless opens a new issue, namely where to obtain such description. Information sheets for artworks may already be available to museum curators yet extending this kind of application to new data becomes time-consuming and requires a domain expert.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we explore the usage of a generative natural language processing model to automatically create contextual information to be fed to a CQA model.
In fact, recently, generative text models have been finding large diffusion with groundbreaking results. Among these we find GPT-3, a generative model trained on a massive corpus of textual data regarding several domains, including art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
GPT-3 is capable of generating a description starting from a textual query and it has been demonstrated that the model includes knowledge of the entities described in the training data, for example paintings and artworks.
We therefore investigate the possibilities and the limitations of GPT-3 in applications for cultural heritage, with a specific focus on question answering.
In particular, we explore the quality of the textual description of artworks that the model is able to generate and we evaluate their applicability for visual and contextual question answering.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The main contributions of our work are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose an automatic approach to generate textual information sheets of artworks exploiting GPT-3. We find that the model has excellent knowledge of art concepts and event details of specific paintings.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a method to answer both visual and contextual questions which is artwork agnostic, i.e. it does not require any additional data or training to be adapted to a new set of images.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We explore the applicability of GPT-3 in cultural heritage applications. To the best of our knowledge we are the first to apply GPT-3 to the art domain.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Natural Language Processing (NLP) in recent years has evolved at an extremely fast pace, converging to a set of well defined application
paradigms¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Such paradigms include text classification, matching, machine reading comprehension, sequence to sequence translation, sequence tagging and language modeling.
Despite the wide variety of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, some recent noticeable approaches have been shown to perform well as generic pre-training for NLP models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In particular, this can be attributed to the introduction of attention models, based on the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The effectiveness of models such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> stems from the capability of processing text bidirectionally exploiting the self-attention mechanism of transformers to obtain word level representations that are informed of their surrounding context within the sentence. Whereas BERT is built exploiting the encoder part of the transformers, another state of the art approach for NLP, Generative Pre-trained Transformer (GPT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, is built stacking transformer decoder blocks and is trained to predict the next word in a sentence.
The model has then been improved in subsequent versions, GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, yielding larger and more effective models.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Interestingly, GPT-3 has been trained using a large quantity of internet data, meaning that the training process has distilled into the model common sense knowledge making it able to generate essays and even poetry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
In this paper we exploit GPT-3 as a generator of textual content describing artworks, showing that it can be used for interactive applications for cultural heritage such as captioning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
VQA is a recent trend in machine learning that bridges the Natural Language Processing and Computer Vision domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The goal is to answer questions regarding the content of an image through artificial intelligence. This involves several sub-tasks such as object detection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and recognition¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, question reasoning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Typical VQA approaches use Convolutional Neural Networks (CNNs) to interpret images and Recurrent Neural Networks (RNNs) to process questions. The authors of ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> proposed a bottom-up attention mechanism looking at salient objects in images. Differently from previous approaches that considered regularly spaced image portions ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, they use object Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> features as attention candidates. In the past few years multiple Transformer-based approaches reached impressive performances on this task ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Recently, a few approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> have addressed VQA in the cultural heritage domain.
A dataset of questions and answers for art related questions has been recently proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, exploiting an ontology based framework to extract data with question templates.
The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> found that to make the best out of VQA for museum applications, a model must be able to integrate some source of external knowledge in order to address contextual questions, i.e. questions concerning non-visual cues such as name of the author, year and artistic style. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> used a question classifier to understand if visual of contextual knowledge is required. Depending on the output of the classifier a VQA model is used, otherwise a purely textual based question answering model is used discarding the image content.
In this work we explore the effectiveness of using GPT-3 to generate artwork captions, suitable for such a visual and contextual question answering model.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Other approaches have been used to answer questions relying on captions, yet only regarding visual content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The most similar approach to ours is instead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, which used GPT-3 for VQA. However, differently from us, the authors feed GPT-3 with questions and descriptions generated by an image captioner directly to obtain an answer. We, instead, aim at extracting the domain specific knowledge from GPT-3 which is requested to correctly answer a question.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>GPT-3</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To provide to the reader a better understanding of our work, here we present a brief background context about GPT-3, the third version of Generative Pre-Trained Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This is an autoregressive language model with 175 billion parameters that can be used for different tasks without any finetuning, achieving strong performances.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The architecture of the GPT-3 Transfomer model is made of 96 attention layers. While language models like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> use the Encoder to generate embeddings from the raw text which can be used in other machine learning applications, GPT-3 use the Decoder half, so it takes embeddings as inputs and produces text. In particular the GPT-3 language model has the ability to generate natural language text that can be hard to distinguish from human-written text, to the point that research has been carried out to asses whether GPT-3 could pass a written Turing test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.8" class="ltx_p">Concretely, during inference, the target of the new task <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">y</annotation></semantics></math> is directly predicted conditioned on the given context <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">C</annotation></semantics></math> and the new task‚Äôs input <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">x</annotation></semantics></math>, as a text sequence generation task. Note that all <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">C</annotation></semantics></math>, <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">x</annotation></semantics></math> and <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.p3.6.m6.1a"><mi id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><ci id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">y</annotation></semantics></math> are text sequences. For example, <math id="S3.p3.7.m7.3" class="ltx_Math" alttext="y=(y^{1},...,y^{T})" display="inline"><semantics id="S3.p3.7.m7.3a"><mrow id="S3.p3.7.m7.3.3" xref="S3.p3.7.m7.3.3.cmml"><mi id="S3.p3.7.m7.3.3.4" xref="S3.p3.7.m7.3.3.4.cmml">y</mi><mo id="S3.p3.7.m7.3.3.3" xref="S3.p3.7.m7.3.3.3.cmml">=</mo><mrow id="S3.p3.7.m7.3.3.2.2" xref="S3.p3.7.m7.3.3.2.3.cmml"><mo stretchy="false" id="S3.p3.7.m7.3.3.2.2.3" xref="S3.p3.7.m7.3.3.2.3.cmml">(</mo><msup id="S3.p3.7.m7.2.2.1.1.1" xref="S3.p3.7.m7.2.2.1.1.1.cmml"><mi id="S3.p3.7.m7.2.2.1.1.1.2" xref="S3.p3.7.m7.2.2.1.1.1.2.cmml">y</mi><mn id="S3.p3.7.m7.2.2.1.1.1.3" xref="S3.p3.7.m7.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S3.p3.7.m7.3.3.2.2.4" xref="S3.p3.7.m7.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml">‚Ä¶</mi><mo id="S3.p3.7.m7.3.3.2.2.5" xref="S3.p3.7.m7.3.3.2.3.cmml">,</mo><msup id="S3.p3.7.m7.3.3.2.2.2" xref="S3.p3.7.m7.3.3.2.2.2.cmml"><mi id="S3.p3.7.m7.3.3.2.2.2.2" xref="S3.p3.7.m7.3.3.2.2.2.2.cmml">y</mi><mi id="S3.p3.7.m7.3.3.2.2.2.3" xref="S3.p3.7.m7.3.3.2.2.2.3.cmml">T</mi></msup><mo stretchy="false" id="S3.p3.7.m7.3.3.2.2.6" xref="S3.p3.7.m7.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.3b"><apply id="S3.p3.7.m7.3.3.cmml" xref="S3.p3.7.m7.3.3"><eq id="S3.p3.7.m7.3.3.3.cmml" xref="S3.p3.7.m7.3.3.3"></eq><ci id="S3.p3.7.m7.3.3.4.cmml" xref="S3.p3.7.m7.3.3.4">ùë¶</ci><vector id="S3.p3.7.m7.3.3.2.3.cmml" xref="S3.p3.7.m7.3.3.2.2"><apply id="S3.p3.7.m7.2.2.1.1.1.cmml" xref="S3.p3.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p3.7.m7.2.2.1.1.1.1.cmml" xref="S3.p3.7.m7.2.2.1.1.1">superscript</csymbol><ci id="S3.p3.7.m7.2.2.1.1.1.2.cmml" xref="S3.p3.7.m7.2.2.1.1.1.2">ùë¶</ci><cn type="integer" id="S3.p3.7.m7.2.2.1.1.1.3.cmml" xref="S3.p3.7.m7.2.2.1.1.1.3">1</cn></apply><ci id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1">‚Ä¶</ci><apply id="S3.p3.7.m7.3.3.2.2.2.cmml" xref="S3.p3.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p3.7.m7.3.3.2.2.2.1.cmml" xref="S3.p3.7.m7.3.3.2.2.2">superscript</csymbol><ci id="S3.p3.7.m7.3.3.2.2.2.2.cmml" xref="S3.p3.7.m7.3.3.2.2.2.2">ùë¶</ci><ci id="S3.p3.7.m7.3.3.2.2.2.3.cmml" xref="S3.p3.7.m7.3.3.2.2.2.3">ùëá</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.3c">y=(y^{1},...,y^{T})</annotation></semantics></math>.
Therefore, at each decoding step <math id="S3.p3.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p3.8.m8.1a"><mi id="S3.p3.8.m8.1.1" xref="S3.p3.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p3.8.m8.1b"><ci id="S3.p3.8.m8.1.1.cmml" xref="S3.p3.8.m8.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.8.m8.1c">t</annotation></semantics></math> we have</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="y^{t}=\arg\max_{y^{t}}{p_{W}}(y^{t}|C,x,y&lt;t)" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><msup id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">y</mi><mi id="S3.E1.m1.4.4.3.3" xref="S3.E1.m1.4.4.3.3.cmml">t</mi></msup><mo id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml"><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.4.4.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.1" xref="S3.E1.m1.4.4.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E1.m1.4.4.1.3a" xref="S3.E1.m1.4.4.1.3.cmml">‚Å°</mo><mrow id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.4.4.1.3.2.cmml"><munder id="S3.E1.m1.4.4.1.3.2.1" xref="S3.E1.m1.4.4.1.3.2.1.cmml"><mi id="S3.E1.m1.4.4.1.3.2.1.2" xref="S3.E1.m1.4.4.1.3.2.1.2.cmml">max</mi><msup id="S3.E1.m1.4.4.1.3.2.1.3" xref="S3.E1.m1.4.4.1.3.2.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.2.1.3.2" xref="S3.E1.m1.4.4.1.3.2.1.3.2.cmml">y</mi><mi id="S3.E1.m1.4.4.1.3.2.1.3.3" xref="S3.E1.m1.4.4.1.3.2.1.3.3.cmml">t</mi></msup></munder><mo lspace="0.167em" id="S3.E1.m1.4.4.1.3.2a" xref="S3.E1.m1.4.4.1.3.2.cmml">‚Å°</mo><msub id="S3.E1.m1.4.4.1.3.2.2" xref="S3.E1.m1.4.4.1.3.2.2.cmml"><mi id="S3.E1.m1.4.4.1.3.2.2.2" xref="S3.E1.m1.4.4.1.3.2.2.2.cmml">p</mi><mi id="S3.E1.m1.4.4.1.3.2.2.3" xref="S3.E1.m1.4.4.1.3.2.2.3.cmml">W</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml"><msup id="S3.E1.m1.4.4.1.1.1.1.2.2" xref="S3.E1.m1.4.4.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.2.2.2.cmml">y</mi><mi id="S3.E1.m1.4.4.1.1.1.1.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.2.2.3.cmml">t</mi></msup><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.2.1" xref="S3.E1.m1.4.4.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.2.3.2" xref="S3.E1.m1.4.4.1.1.1.1.2.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">C</mi><mo id="S3.E1.m1.4.4.1.1.1.1.2.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.2.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">x</mi><mo id="S3.E1.m1.4.4.1.1.1.1.2.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.2.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">y</mi></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">&lt;</mo><mi id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"></eq><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3">superscript</csymbol><ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">ùë¶</ci><ci id="S3.E1.m1.4.4.3.3.cmml" xref="S3.E1.m1.4.4.3.3">ùë°</ci></apply><apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1"><times id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2"></times><apply id="S3.E1.m1.4.4.1.3.cmml" xref="S3.E1.m1.4.4.1.3"><arg id="S3.E1.m1.4.4.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.1"></arg><apply id="S3.E1.m1.4.4.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2"><apply id="S3.E1.m1.4.4.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.2.1.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1">subscript</csymbol><max id="S3.E1.m1.4.4.1.3.2.1.2.cmml" xref="S3.E1.m1.4.4.1.3.2.1.2"></max><apply id="S3.E1.m1.4.4.1.3.2.1.3.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.2.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3">superscript</csymbol><ci id="S3.E1.m1.4.4.1.3.2.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3.2">ùë¶</ci><ci id="S3.E1.m1.4.4.1.3.2.1.3.3.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3.3">ùë°</ci></apply></apply><apply id="S3.E1.m1.4.4.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.2.2.1.cmml" xref="S3.E1.m1.4.4.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.3.2.2.2.cmml" xref="S3.E1.m1.4.4.1.3.2.2.2">ùëù</ci><ci id="S3.E1.m1.4.4.1.3.2.2.3.cmml" xref="S3.E1.m1.4.4.1.3.2.2.3">ùëä</ci></apply></apply></apply><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><lt id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"></lt><apply id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.1">conditional</csymbol><apply id="S3.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.2">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.2.2">ùë¶</ci><ci id="S3.E1.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.2.3">ùë°</ci></apply><list id="S3.E1.m1.4.4.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùê∂</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ùë•</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ùë¶</ci></list></apply><ci id="S3.E1.m1.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">y^{t}=\arg\max_{y^{t}}{p_{W}}(y^{t}|C,x,y&lt;t)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p3.13" class="ltx_p">where <math id="S3.p3.9.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.p3.9.m1.1a"><mi id="S3.p3.9.m1.1.1" xref="S3.p3.9.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.p3.9.m1.1b"><ci id="S3.p3.9.m1.1.1.cmml" xref="S3.p3.9.m1.1.1">ùëä</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.9.m1.1c">W</annotation></semantics></math> are the weights of the pretrained language model, which are frozen for all new tasks. The context
<math id="S3.p3.10.m2.6" class="ltx_Math" alttext="C={h,x_{1},y_{1},...,x_{n},y_{n}}" display="inline"><semantics id="S3.p3.10.m2.6a"><mrow id="S3.p3.10.m2.6.6" xref="S3.p3.10.m2.6.6.cmml"><mi id="S3.p3.10.m2.6.6.6" xref="S3.p3.10.m2.6.6.6.cmml">C</mi><mo id="S3.p3.10.m2.6.6.5" xref="S3.p3.10.m2.6.6.5.cmml">=</mo><mrow id="S3.p3.10.m2.6.6.4.4" xref="S3.p3.10.m2.6.6.4.5.cmml"><mi id="S3.p3.10.m2.1.1" xref="S3.p3.10.m2.1.1.cmml">h</mi><mo id="S3.p3.10.m2.6.6.4.4.5" xref="S3.p3.10.m2.6.6.4.5.cmml">,</mo><msub id="S3.p3.10.m2.3.3.1.1.1" xref="S3.p3.10.m2.3.3.1.1.1.cmml"><mi id="S3.p3.10.m2.3.3.1.1.1.2" xref="S3.p3.10.m2.3.3.1.1.1.2.cmml">x</mi><mn id="S3.p3.10.m2.3.3.1.1.1.3" xref="S3.p3.10.m2.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.p3.10.m2.6.6.4.4.6" xref="S3.p3.10.m2.6.6.4.5.cmml">,</mo><msub id="S3.p3.10.m2.4.4.2.2.2" xref="S3.p3.10.m2.4.4.2.2.2.cmml"><mi id="S3.p3.10.m2.4.4.2.2.2.2" xref="S3.p3.10.m2.4.4.2.2.2.2.cmml">y</mi><mn id="S3.p3.10.m2.4.4.2.2.2.3" xref="S3.p3.10.m2.4.4.2.2.2.3.cmml">1</mn></msub><mo id="S3.p3.10.m2.6.6.4.4.7" xref="S3.p3.10.m2.6.6.4.5.cmml">,</mo><mi mathvariant="normal" id="S3.p3.10.m2.2.2" xref="S3.p3.10.m2.2.2.cmml">‚Ä¶</mi><mo id="S3.p3.10.m2.6.6.4.4.8" xref="S3.p3.10.m2.6.6.4.5.cmml">,</mo><msub id="S3.p3.10.m2.5.5.3.3.3" xref="S3.p3.10.m2.5.5.3.3.3.cmml"><mi id="S3.p3.10.m2.5.5.3.3.3.2" xref="S3.p3.10.m2.5.5.3.3.3.2.cmml">x</mi><mi id="S3.p3.10.m2.5.5.3.3.3.3" xref="S3.p3.10.m2.5.5.3.3.3.3.cmml">n</mi></msub><mo id="S3.p3.10.m2.6.6.4.4.9" xref="S3.p3.10.m2.6.6.4.5.cmml">,</mo><msub id="S3.p3.10.m2.6.6.4.4.4" xref="S3.p3.10.m2.6.6.4.4.4.cmml"><mi id="S3.p3.10.m2.6.6.4.4.4.2" xref="S3.p3.10.m2.6.6.4.4.4.2.cmml">y</mi><mi id="S3.p3.10.m2.6.6.4.4.4.3" xref="S3.p3.10.m2.6.6.4.4.4.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.10.m2.6b"><apply id="S3.p3.10.m2.6.6.cmml" xref="S3.p3.10.m2.6.6"><eq id="S3.p3.10.m2.6.6.5.cmml" xref="S3.p3.10.m2.6.6.5"></eq><ci id="S3.p3.10.m2.6.6.6.cmml" xref="S3.p3.10.m2.6.6.6">ùê∂</ci><list id="S3.p3.10.m2.6.6.4.5.cmml" xref="S3.p3.10.m2.6.6.4.4"><ci id="S3.p3.10.m2.1.1.cmml" xref="S3.p3.10.m2.1.1">‚Ñé</ci><apply id="S3.p3.10.m2.3.3.1.1.1.cmml" xref="S3.p3.10.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.p3.10.m2.3.3.1.1.1.1.cmml" xref="S3.p3.10.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.p3.10.m2.3.3.1.1.1.2.cmml" xref="S3.p3.10.m2.3.3.1.1.1.2">ùë•</ci><cn type="integer" id="S3.p3.10.m2.3.3.1.1.1.3.cmml" xref="S3.p3.10.m2.3.3.1.1.1.3">1</cn></apply><apply id="S3.p3.10.m2.4.4.2.2.2.cmml" xref="S3.p3.10.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.p3.10.m2.4.4.2.2.2.1.cmml" xref="S3.p3.10.m2.4.4.2.2.2">subscript</csymbol><ci id="S3.p3.10.m2.4.4.2.2.2.2.cmml" xref="S3.p3.10.m2.4.4.2.2.2.2">ùë¶</ci><cn type="integer" id="S3.p3.10.m2.4.4.2.2.2.3.cmml" xref="S3.p3.10.m2.4.4.2.2.2.3">1</cn></apply><ci id="S3.p3.10.m2.2.2.cmml" xref="S3.p3.10.m2.2.2">‚Ä¶</ci><apply id="S3.p3.10.m2.5.5.3.3.3.cmml" xref="S3.p3.10.m2.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.p3.10.m2.5.5.3.3.3.1.cmml" xref="S3.p3.10.m2.5.5.3.3.3">subscript</csymbol><ci id="S3.p3.10.m2.5.5.3.3.3.2.cmml" xref="S3.p3.10.m2.5.5.3.3.3.2">ùë•</ci><ci id="S3.p3.10.m2.5.5.3.3.3.3.cmml" xref="S3.p3.10.m2.5.5.3.3.3.3">ùëõ</ci></apply><apply id="S3.p3.10.m2.6.6.4.4.4.cmml" xref="S3.p3.10.m2.6.6.4.4.4"><csymbol cd="ambiguous" id="S3.p3.10.m2.6.6.4.4.4.1.cmml" xref="S3.p3.10.m2.6.6.4.4.4">subscript</csymbol><ci id="S3.p3.10.m2.6.6.4.4.4.2.cmml" xref="S3.p3.10.m2.6.6.4.4.4.2">ùë¶</ci><ci id="S3.p3.10.m2.6.6.4.4.4.3.cmml" xref="S3.p3.10.m2.6.6.4.4.4.3">ùëõ</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.10.m2.6c">C={h,x_{1},y_{1},...,x_{n},y_{n}}</annotation></semantics></math> consists of an optional prompt head <math id="S3.p3.11.m3.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.p3.11.m3.1a"><mi id="S3.p3.11.m3.1.1" xref="S3.p3.11.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.p3.11.m3.1b"><ci id="S3.p3.11.m3.1.1.cmml" xref="S3.p3.11.m3.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.11.m3.1c">h</annotation></semantics></math> and <math id="S3.p3.12.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p3.12.m4.1a"><mi id="S3.p3.12.m4.1.1" xref="S3.p3.12.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p3.12.m4.1b"><ci id="S3.p3.12.m4.1.1.cmml" xref="S3.p3.12.m4.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.12.m4.1c">n</annotation></semantics></math> in-context examples <math id="S3.p3.13.m5.1" class="ltx_math_unparsed" alttext="(\{x_{i},y_{i}\}{{}^{n}}_{i=1})" display="inline"><semantics id="S3.p3.13.m5.1a"><mrow id="S3.p3.13.m5.1b"><mo stretchy="false" id="S3.p3.13.m5.1.1">(</mo><mrow id="S3.p3.13.m5.1.2"><mo stretchy="false" id="S3.p3.13.m5.1.2.1">{</mo><msub id="S3.p3.13.m5.1.2.2"><mi id="S3.p3.13.m5.1.2.2.2">x</mi><mi id="S3.p3.13.m5.1.2.2.3">i</mi></msub><mo id="S3.p3.13.m5.1.2.3">,</mo><msub id="S3.p3.13.m5.1.2.4"><mi id="S3.p3.13.m5.1.2.4.2">y</mi><mi id="S3.p3.13.m5.1.2.4.3">i</mi></msub><mo stretchy="false" id="S3.p3.13.m5.1.2.5">}</mo></mrow><mmultiscripts id="S3.p3.13.m5.1.3"><mo stretchy="false" id="S3.p3.13.m5.1.3.2.2">)</mo><mprescripts id="S3.p3.13.m5.1.3a"></mprescripts><mrow id="S3.p3.13.m5.1.3b"></mrow><mi id="S3.p3.13.m5.1.3.3">n</mi><mrow id="S3.p3.13.m5.1.3.2.3"><mi id="S3.p3.13.m5.1.3.2.3.2">i</mi><mo id="S3.p3.13.m5.1.3.2.3.1">=</mo><mn id="S3.p3.13.m5.1.3.2.3.3">1</mn></mrow><mrow id="S3.p3.13.m5.1.3c"></mrow></mmultiscripts></mrow><annotation encoding="application/x-tex" id="S3.p3.13.m5.1c">(\{x_{i},y_{i}\}{{}^{n}}_{i=1})</annotation></semantics></math> from the new task.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In a Cultural Heritage context, the information useful to answer questions about a specific artwork is contained in the artwork image and in its contextual description.
Finding such a description might not be trivial, since it might require a domain expert to write it down. At the same time, it is quite costly to train a Visual Question Answering model that takes in input both the image and the description. This is also not straightforward, since the two modalities need to be blended and matched together.
Consequently, the main idea of this work is to generate new descriptions for artworks based on a specific prompt or a specific question and directly use these descriptions to answer visual and contextual questions.
The overall pipeline of our proposed work is as follows:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">GPT-3 caption generation.</span> We use GPT-3 to generate descriptions of artworks, leveraging its memorization capabilities that allowed the model retain relevant information about training instances.
An important aspect in this phase in to feed the correct prompt in input to GPT-3 in order to obtain realistic and correct descriptions. We consider two different types of input prompt: 
<br class="ltx_break"></p>
</div>
<div id="S4.I1.i1.p2" class="ltx_para">
<ul id="S4.I1.i1.I1" class="ltx_itemize">
<li id="S4.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">General</span> - A general prompt where the expected output is a general description of the artwork. The input text follows the structure:</p>
</div>
<div id="S4.I1.i1.I1.i1.p2" class="ltx_para">
<p id="S4.I1.i1.I1.i1.p2.2" class="ltx_p"><span id="S4.I1.i1.I1.i1.p2.2.2" class="ltx_text ltx_font_typewriter">"Describe and Contextualize the painting <math id="S4.I1.i1.I1.i1.p2.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.I1.i1.I1.i1.p2.1.1.m1.1a"><mo id="S4.I1.i1.I1.i1.p2.1.1.m1.1.1" xref="S4.I1.i1.I1.i1.p2.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i1.p2.1.1.m1.1b"><lt id="S4.I1.i1.I1.i1.p2.1.1.m1.1.1.cmml" xref="S4.I1.i1.I1.i1.p2.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i1.p2.1.1.m1.1c">&lt;</annotation></semantics></math> painting_name <math id="S4.I1.i1.I1.i1.p2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.I1.i1.I1.i1.p2.2.2.m2.1a"><mo id="S4.I1.i1.I1.i1.p2.2.2.m2.1.1" xref="S4.I1.i1.I1.i1.p2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i1.p2.2.2.m2.1b"><gt id="S4.I1.i1.I1.i1.p2.2.2.m2.1.1.cmml" xref="S4.I1.i1.I1.i1.p2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i1.p2.2.2.m2.1c">&gt;</annotation></semantics></math>"
<br class="ltx_break"></span></p>
</div>
</li>
<li id="S4.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Question-based</span> - A specific question based prompt. The input text follows the structure:</p>
</div>
<div id="S4.I1.i1.I1.i2.p2" class="ltx_para">
<p id="S4.I1.i1.I1.i2.p2.4" class="ltx_p"><span id="S4.I1.i1.I1.i2.p2.4.4" class="ltx_text ltx_font_typewriter">"Painting <math id="S4.I1.i1.I1.i2.p2.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.I1.i1.I1.i2.p2.1.1.m1.1a"><mo id="S4.I1.i1.I1.i2.p2.1.1.m1.1.1" xref="S4.I1.i1.I1.i2.p2.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i2.p2.1.1.m1.1b"><lt id="S4.I1.i1.I1.i2.p2.1.1.m1.1.1.cmml" xref="S4.I1.i1.I1.i2.p2.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i2.p2.1.1.m1.1c">&lt;</annotation></semantics></math> painting_name <math id="S4.I1.i1.I1.i2.p2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.I1.i1.I1.i2.p2.2.2.m2.1a"><mo id="S4.I1.i1.I1.i2.p2.2.2.m2.1.1" xref="S4.I1.i1.I1.i2.p2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i2.p2.2.2.m2.1b"><gt id="S4.I1.i1.I1.i2.p2.2.2.m2.1.1.cmml" xref="S4.I1.i1.I1.i2.p2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i2.p2.2.2.m2.1c">&gt;</annotation></semantics></math> <math id="S4.I1.i1.I1.i2.p2.3.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.I1.i1.I1.i2.p2.3.3.m3.1a"><mo id="S4.I1.i1.I1.i2.p2.3.3.m3.1.1" xref="S4.I1.i1.I1.i2.p2.3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i2.p2.3.3.m3.1b"><lt id="S4.I1.i1.I1.i2.p2.3.3.m3.1.1.cmml" xref="S4.I1.i1.I1.i2.p2.3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i2.p2.3.3.m3.1c">&lt;</annotation></semantics></math> question <math id="S4.I1.i1.I1.i2.p2.4.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.I1.i1.I1.i2.p2.4.4.m4.1a"><mo id="S4.I1.i1.I1.i2.p2.4.4.m4.1.1" xref="S4.I1.i1.I1.i2.p2.4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.I1.i2.p2.4.4.m4.1b"><gt id="S4.I1.i1.I1.i2.p2.4.4.m4.1.1.cmml" xref="S4.I1.i1.I1.i2.p2.4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.I1.i2.p2.4.4.m4.1c">&gt;</annotation></semantics></math>"</span>.</p>
</div>
<div id="S4.I1.i1.I1.i2.p3" class="ltx_para">
<p id="S4.I1.i1.I1.i2.p3.1" class="ltx_p">The expected generated text by GPT-3 is a small text snippet that consists in a couple of sentences, focused on the topic of the question. 
<br class="ltx_break"></p>
</div>
</li>
</ul>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Question answering.</span> Once the description has been generated in the previous step, we can exploit it to answer both visual and contextual questions through a Question Answering language model. For this purpose we use a pretrained version of DistilBert¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> fine-tuned on the SQUAD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> dataset. We feed in input to the DistilBert model the generated text from the previous step together with the question. The answer given as output will be the final answer of our method.</p>
</div>
</li>
</ol>
<p id="S4.p1.2" class="ltx_p">Fig.¬†<a href="#S4.F1" title="Figure 1 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig.¬†<a href="#S4.F2" title="Figure 2 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show a scheme of the two variants of our method. More precisely, in Fig.¬†<a href="#S4.F1" title="Figure 1 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> the general input prompt for GPT-3 yields the generation of a long description of the artwork (similar to a museum information sheet).
On the other hand, the question-based prompt in Fig.¬†<a href="#S4.F2" title="Figure 2 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> yields only the generation of a brief output text, which we find suitable for answering the question.
In conclusion, these two schemes follow roughly the same structure. The difference is in the input prompt that in the case of Fig.¬†<a href="#S4.F1" title="Figure 1 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is more general and in Fig.¬†<a href="#S4.F2" title="Figure 2 ‚Ä£ 4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is more task oriented.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2207.12101/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Scheme of our method for answering questions using a general generated description. A prompt with a specific structure is given in input to GPT-3. Subsequently the generated text is fed together with the question to a Question Answering model that outputs the answer.</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2207.12101/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Scheme of our method for answering questions using a question-based generated description. A prompt containing the name of the painting and the question is given in input to GPT-3. Subsequently the generated text is fed together with the question to a Question Answering model that outputs the answer.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section we first outline the experimental setting for the experiments carried out in this paper, presenting dataset and experimental protocol and we then move on to a discussion of the results.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">For our experiments, we use the Artpedia dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Artpedia contains a collection of 2,930 artworks, associated to a variable number of textual descriptions gathered from WikiPedia.
Sentences are labelled as a visual descriptions or as a contextual descriptions. Contextual descriptions regard information about the artwork that does not directly describe its visual content.
For instance, contextual descriptions can describe the historical context of the artwork, its author, the artistic influence or the museum where a painting is exhibited.
The dataset contains 28,212 descriptions, 9,173 of which are labelled as visual and the remaining 19,039 as contextual.
The Artpedia dataset has been extended with Question-Answer annotations in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. In fact, a subset of the images have been associated with visual and contextual questions, derived from the corresponding captions. In this work we follow the dataset split of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental protocol</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Following prior work such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we evaluate visual questions and contextual questions with different metrics. In fact, visual question answering and traditional text-based question answering are often treated in two different ways. Visual Question Answering is considered as a classification problem, meaning that a model has to pick an answer from a predefined dictionary of possible candidates containing a few words each. This stems from the fact that questions in most datasets are a way of guiding attention towards specific objects or attributes in the image, without requiring any complex form of language reasoning. Question Answering on the other hand is based on a set of sentences, which may contain rare or out-of-dictionary words. The task is in fact defined as identifying a subset of the textual description that contains the answer.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.3" class="ltx_p">In light of this, to evaluate visual questions we rely on accuracy:</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="Accuracy=\frac{N_{c}}{N_{a}}" display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mrow id="S5.E2.m1.1.1.2" xref="S5.E2.m1.1.1.2.cmml"><mi id="S5.E2.m1.1.1.2.2" xref="S5.E2.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.3" xref="S5.E2.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1a" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.4" xref="S5.E2.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1b" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.5" xref="S5.E2.m1.1.1.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1c" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.6" xref="S5.E2.m1.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1d" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.7" xref="S5.E2.m1.1.1.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1e" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.8" xref="S5.E2.m1.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1f" xref="S5.E2.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S5.E2.m1.1.1.2.9" xref="S5.E2.m1.1.1.2.9.cmml">y</mi></mrow><mo id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1.cmml">=</mo><mfrac id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3.cmml"><msub id="S5.E2.m1.1.1.3.2" xref="S5.E2.m1.1.1.3.2.cmml"><mi id="S5.E2.m1.1.1.3.2.2" xref="S5.E2.m1.1.1.3.2.2.cmml">N</mi><mi id="S5.E2.m1.1.1.3.2.3" xref="S5.E2.m1.1.1.3.2.3.cmml">c</mi></msub><msub id="S5.E2.m1.1.1.3.3" xref="S5.E2.m1.1.1.3.3.cmml"><mi id="S5.E2.m1.1.1.3.3.2" xref="S5.E2.m1.1.1.3.3.2.cmml">N</mi><mi id="S5.E2.m1.1.1.3.3.3" xref="S5.E2.m1.1.1.3.3.3.cmml">a</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><eq id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"></eq><apply id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1.2"><times id="S5.E2.m1.1.1.2.1.cmml" xref="S5.E2.m1.1.1.2.1"></times><ci id="S5.E2.m1.1.1.2.2.cmml" xref="S5.E2.m1.1.1.2.2">ùê¥</ci><ci id="S5.E2.m1.1.1.2.3.cmml" xref="S5.E2.m1.1.1.2.3">ùëê</ci><ci id="S5.E2.m1.1.1.2.4.cmml" xref="S5.E2.m1.1.1.2.4">ùëê</ci><ci id="S5.E2.m1.1.1.2.5.cmml" xref="S5.E2.m1.1.1.2.5">ùë¢</ci><ci id="S5.E2.m1.1.1.2.6.cmml" xref="S5.E2.m1.1.1.2.6">ùëü</ci><ci id="S5.E2.m1.1.1.2.7.cmml" xref="S5.E2.m1.1.1.2.7">ùëé</ci><ci id="S5.E2.m1.1.1.2.8.cmml" xref="S5.E2.m1.1.1.2.8">ùëê</ci><ci id="S5.E2.m1.1.1.2.9.cmml" xref="S5.E2.m1.1.1.2.9">ùë¶</ci></apply><apply id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3"><divide id="S5.E2.m1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.3"></divide><apply id="S5.E2.m1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.3.2.1.cmml" xref="S5.E2.m1.1.1.3.2">subscript</csymbol><ci id="S5.E2.m1.1.1.3.2.2.cmml" xref="S5.E2.m1.1.1.3.2.2">ùëÅ</ci><ci id="S5.E2.m1.1.1.3.2.3.cmml" xref="S5.E2.m1.1.1.3.2.3">ùëê</ci></apply><apply id="S5.E2.m1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.3.3">subscript</csymbol><ci id="S5.E2.m1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.3.3.2">ùëÅ</ci><ci id="S5.E2.m1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.3.3.3">ùëé</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">Accuracy=\frac{N_{c}}{N_{a}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.2" class="ltx_p">where <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">N</mi><mi id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">ùëÅ</ci><ci id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">N_{c}</annotation></semantics></math> is the number of correct answers and <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="N_{a}" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><msub id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">N</mi><mi id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">ùëÅ</ci><ci id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">N_{a}</annotation></semantics></math> the number of total answers.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">For text-based question answering, instead, we use both accuracy and F1-measure, a metric that takes into account the global correctness of the answer:</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.1" class="ltx_Math" alttext="F1=2\times\frac{Precision\times Recall}{Precision+Recall}" display="block"><semantics id="S5.E3.m1.1a"><mrow id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml"><mrow id="S5.E3.m1.1.1.2" xref="S5.E3.m1.1.1.2.cmml"><mi id="S5.E3.m1.1.1.2.2" xref="S5.E3.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.2.1" xref="S5.E3.m1.1.1.2.1.cmml">‚Äã</mo><mn id="S5.E3.m1.1.1.2.3" xref="S5.E3.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S5.E3.m1.1.1.1" xref="S5.E3.m1.1.1.1.cmml">=</mo><mrow id="S5.E3.m1.1.1.3" xref="S5.E3.m1.1.1.3.cmml"><mn id="S5.E3.m1.1.1.3.2" xref="S5.E3.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S5.E3.m1.1.1.3.1" xref="S5.E3.m1.1.1.3.1.cmml">√ó</mo><mfrac id="S5.E3.m1.1.1.3.3" xref="S5.E3.m1.1.1.3.3.cmml"><mrow id="S5.E3.m1.1.1.3.3.2" xref="S5.E3.m1.1.1.3.3.2.cmml"><mrow id="S5.E3.m1.1.1.3.3.2.2" xref="S5.E3.m1.1.1.3.3.2.2.cmml"><mrow id="S5.E3.m1.1.1.3.3.2.2.2" xref="S5.E3.m1.1.1.3.3.2.2.2.cmml"><mi id="S5.E3.m1.1.1.3.3.2.2.2.2" xref="S5.E3.m1.1.1.3.3.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.3" xref="S5.E3.m1.1.1.3.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1a" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.4" xref="S5.E3.m1.1.1.3.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1b" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.5" xref="S5.E3.m1.1.1.3.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1c" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.6" xref="S5.E3.m1.1.1.3.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1d" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.7" xref="S5.E3.m1.1.1.3.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1e" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.8" xref="S5.E3.m1.1.1.3.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1f" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.9" xref="S5.E3.m1.1.1.3.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.2.2.1g" xref="S5.E3.m1.1.1.3.3.2.2.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.2.2.10" xref="S5.E3.m1.1.1.3.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.E3.m1.1.1.3.3.2.2.1" xref="S5.E3.m1.1.1.3.3.2.2.1.cmml">√ó</mo><mi id="S5.E3.m1.1.1.3.3.2.2.3" xref="S5.E3.m1.1.1.3.3.2.2.3.cmml">R</mi></mrow><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.1" xref="S5.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.3" xref="S5.E3.m1.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.1a" xref="S5.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.4" xref="S5.E3.m1.1.1.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.1b" xref="S5.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.5" xref="S5.E3.m1.1.1.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.1c" xref="S5.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.6" xref="S5.E3.m1.1.1.3.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.2.1d" xref="S5.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.2.7" xref="S5.E3.m1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S5.E3.m1.1.1.3.3.3" xref="S5.E3.m1.1.1.3.3.3.cmml"><mrow id="S5.E3.m1.1.1.3.3.3.2" xref="S5.E3.m1.1.1.3.3.3.2.cmml"><mi id="S5.E3.m1.1.1.3.3.3.2.2" xref="S5.E3.m1.1.1.3.3.3.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.3" xref="S5.E3.m1.1.1.3.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1a" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.4" xref="S5.E3.m1.1.1.3.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1b" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.5" xref="S5.E3.m1.1.1.3.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1c" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.6" xref="S5.E3.m1.1.1.3.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1d" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.7" xref="S5.E3.m1.1.1.3.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1e" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.8" xref="S5.E3.m1.1.1.3.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1f" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.9" xref="S5.E3.m1.1.1.3.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.2.1g" xref="S5.E3.m1.1.1.3.3.3.2.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.2.10" xref="S5.E3.m1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S5.E3.m1.1.1.3.3.3.1" xref="S5.E3.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S5.E3.m1.1.1.3.3.3.3" xref="S5.E3.m1.1.1.3.3.3.3.cmml"><mi id="S5.E3.m1.1.1.3.3.3.3.2" xref="S5.E3.m1.1.1.3.3.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.3.1" xref="S5.E3.m1.1.1.3.3.3.3.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.3.3" xref="S5.E3.m1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.3.1a" xref="S5.E3.m1.1.1.3.3.3.3.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.3.4" xref="S5.E3.m1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.3.1b" xref="S5.E3.m1.1.1.3.3.3.3.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.3.5" xref="S5.E3.m1.1.1.3.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.3.1c" xref="S5.E3.m1.1.1.3.3.3.3.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.3.6" xref="S5.E3.m1.1.1.3.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.3.3.3.1d" xref="S5.E3.m1.1.1.3.3.3.3.1.cmml">‚Äã</mo><mi id="S5.E3.m1.1.1.3.3.3.3.7" xref="S5.E3.m1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"><eq id="S5.E3.m1.1.1.1.cmml" xref="S5.E3.m1.1.1.1"></eq><apply id="S5.E3.m1.1.1.2.cmml" xref="S5.E3.m1.1.1.2"><times id="S5.E3.m1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.2.1"></times><ci id="S5.E3.m1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.2.2">ùêπ</ci><cn type="integer" id="S5.E3.m1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.2.3">1</cn></apply><apply id="S5.E3.m1.1.1.3.cmml" xref="S5.E3.m1.1.1.3"><times id="S5.E3.m1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.3.1"></times><cn type="integer" id="S5.E3.m1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.3.2">2</cn><apply id="S5.E3.m1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.3.3"><divide id="S5.E3.m1.1.1.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3"></divide><apply id="S5.E3.m1.1.1.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.2"><times id="S5.E3.m1.1.1.3.3.2.1.cmml" xref="S5.E3.m1.1.1.3.3.2.1"></times><apply id="S5.E3.m1.1.1.3.3.2.2.cmml" xref="S5.E3.m1.1.1.3.3.2.2"><times id="S5.E3.m1.1.1.3.3.2.2.1.cmml" xref="S5.E3.m1.1.1.3.3.2.2.1"></times><apply id="S5.E3.m1.1.1.3.3.2.2.2.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2"><times id="S5.E3.m1.1.1.3.3.2.2.2.1.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.1"></times><ci id="S5.E3.m1.1.1.3.3.2.2.2.2.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.2">ùëÉ</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.3.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.3">ùëü</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.4.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.4">ùëí</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.5.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.5">ùëê</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.6.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.6">ùëñ</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.7.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.7">ùë†</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.8.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.8">ùëñ</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.9.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.9">ùëú</ci><ci id="S5.E3.m1.1.1.3.3.2.2.2.10.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2.10">ùëõ</ci></apply><ci id="S5.E3.m1.1.1.3.3.2.2.3.cmml" xref="S5.E3.m1.1.1.3.3.2.2.3">ùëÖ</ci></apply><ci id="S5.E3.m1.1.1.3.3.2.3.cmml" xref="S5.E3.m1.1.1.3.3.2.3">ùëí</ci><ci id="S5.E3.m1.1.1.3.3.2.4.cmml" xref="S5.E3.m1.1.1.3.3.2.4">ùëê</ci><ci id="S5.E3.m1.1.1.3.3.2.5.cmml" xref="S5.E3.m1.1.1.3.3.2.5">ùëé</ci><ci id="S5.E3.m1.1.1.3.3.2.6.cmml" xref="S5.E3.m1.1.1.3.3.2.6">ùëô</ci><ci id="S5.E3.m1.1.1.3.3.2.7.cmml" xref="S5.E3.m1.1.1.3.3.2.7">ùëô</ci></apply><apply id="S5.E3.m1.1.1.3.3.3.cmml" xref="S5.E3.m1.1.1.3.3.3"><plus id="S5.E3.m1.1.1.3.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3.3.1"></plus><apply id="S5.E3.m1.1.1.3.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.3.2"><times id="S5.E3.m1.1.1.3.3.3.2.1.cmml" xref="S5.E3.m1.1.1.3.3.3.2.1"></times><ci id="S5.E3.m1.1.1.3.3.3.2.2.cmml" xref="S5.E3.m1.1.1.3.3.3.2.2">ùëÉ</ci><ci id="S5.E3.m1.1.1.3.3.3.2.3.cmml" xref="S5.E3.m1.1.1.3.3.3.2.3">ùëü</ci><ci id="S5.E3.m1.1.1.3.3.3.2.4.cmml" xref="S5.E3.m1.1.1.3.3.3.2.4">ùëí</ci><ci id="S5.E3.m1.1.1.3.3.3.2.5.cmml" xref="S5.E3.m1.1.1.3.3.3.2.5">ùëê</ci><ci id="S5.E3.m1.1.1.3.3.3.2.6.cmml" xref="S5.E3.m1.1.1.3.3.3.2.6">ùëñ</ci><ci id="S5.E3.m1.1.1.3.3.3.2.7.cmml" xref="S5.E3.m1.1.1.3.3.3.2.7">ùë†</ci><ci id="S5.E3.m1.1.1.3.3.3.2.8.cmml" xref="S5.E3.m1.1.1.3.3.3.2.8">ùëñ</ci><ci id="S5.E3.m1.1.1.3.3.3.2.9.cmml" xref="S5.E3.m1.1.1.3.3.3.2.9">ùëú</ci><ci id="S5.E3.m1.1.1.3.3.3.2.10.cmml" xref="S5.E3.m1.1.1.3.3.3.2.10">ùëõ</ci></apply><apply id="S5.E3.m1.1.1.3.3.3.3.cmml" xref="S5.E3.m1.1.1.3.3.3.3"><times id="S5.E3.m1.1.1.3.3.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3.3.3.1"></times><ci id="S5.E3.m1.1.1.3.3.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.3.3.2">ùëÖ</ci><ci id="S5.E3.m1.1.1.3.3.3.3.3.cmml" xref="S5.E3.m1.1.1.3.3.3.3.3">ùëí</ci><ci id="S5.E3.m1.1.1.3.3.3.3.4.cmml" xref="S5.E3.m1.1.1.3.3.3.3.4">ùëê</ci><ci id="S5.E3.m1.1.1.3.3.3.3.5.cmml" xref="S5.E3.m1.1.1.3.3.3.3.5">ùëé</ci><ci id="S5.E3.m1.1.1.3.3.3.3.6.cmml" xref="S5.E3.m1.1.1.3.3.3.3.6">ùëô</ci><ci id="S5.E3.m1.1.1.3.3.3.3.7.cmml" xref="S5.E3.m1.1.1.3.3.3.3.7">ùëô</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">F1=2\times\frac{Precision\times Recall}{Precision+Recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Where <math id="S5.SS2.p5.1.m1.1" class="ltx_Math" alttext="Precision" display="inline"><semantics id="S5.SS2.p5.1.m1.1a"><mrow id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml"><mi id="S5.SS2.p5.1.m1.1.1.2" xref="S5.SS2.p5.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.3" xref="S5.SS2.p5.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1a" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.4" xref="S5.SS2.p5.1.m1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1b" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.5" xref="S5.SS2.p5.1.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1c" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.6" xref="S5.SS2.p5.1.m1.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1d" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.7" xref="S5.SS2.p5.1.m1.1.1.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1e" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.8" xref="S5.SS2.p5.1.m1.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1f" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.9" xref="S5.SS2.p5.1.m1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p5.1.m1.1.1.1g" xref="S5.SS2.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p5.1.m1.1.1.10" xref="S5.SS2.p5.1.m1.1.1.10.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><apply id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1"><times id="S5.SS2.p5.1.m1.1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1.1"></times><ci id="S5.SS2.p5.1.m1.1.1.2.cmml" xref="S5.SS2.p5.1.m1.1.1.2">ùëÉ</ci><ci id="S5.SS2.p5.1.m1.1.1.3.cmml" xref="S5.SS2.p5.1.m1.1.1.3">ùëü</ci><ci id="S5.SS2.p5.1.m1.1.1.4.cmml" xref="S5.SS2.p5.1.m1.1.1.4">ùëí</ci><ci id="S5.SS2.p5.1.m1.1.1.5.cmml" xref="S5.SS2.p5.1.m1.1.1.5">ùëê</ci><ci id="S5.SS2.p5.1.m1.1.1.6.cmml" xref="S5.SS2.p5.1.m1.1.1.6">ùëñ</ci><ci id="S5.SS2.p5.1.m1.1.1.7.cmml" xref="S5.SS2.p5.1.m1.1.1.7">ùë†</ci><ci id="S5.SS2.p5.1.m1.1.1.8.cmml" xref="S5.SS2.p5.1.m1.1.1.8">ùëñ</ci><ci id="S5.SS2.p5.1.m1.1.1.9.cmml" xref="S5.SS2.p5.1.m1.1.1.9">ùëú</ci><ci id="S5.SS2.p5.1.m1.1.1.10.cmml" xref="S5.SS2.p5.1.m1.1.1.10">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">Precision</annotation></semantics></math> is defined as:</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.1" class="ltx_Math" alttext="Precision=\frac{N_{Cw}}{|ans|}" display="block"><semantics id="S5.E4.m1.1a"><mrow id="S5.E4.m1.1.2" xref="S5.E4.m1.1.2.cmml"><mrow id="S5.E4.m1.1.2.2" xref="S5.E4.m1.1.2.2.cmml"><mi id="S5.E4.m1.1.2.2.2" xref="S5.E4.m1.1.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.3" xref="S5.E4.m1.1.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1a" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.4" xref="S5.E4.m1.1.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1b" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.5" xref="S5.E4.m1.1.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1c" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.6" xref="S5.E4.m1.1.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1d" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.7" xref="S5.E4.m1.1.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1e" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.8" xref="S5.E4.m1.1.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1f" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.9" xref="S5.E4.m1.1.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1g" xref="S5.E4.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.2.2.10" xref="S5.E4.m1.1.2.2.10.cmml">n</mi></mrow><mo id="S5.E4.m1.1.2.1" xref="S5.E4.m1.1.2.1.cmml">=</mo><mfrac id="S5.E4.m1.1.1" xref="S5.E4.m1.1.1.cmml"><msub id="S5.E4.m1.1.1.3" xref="S5.E4.m1.1.1.3.cmml"><mi id="S5.E4.m1.1.1.3.2" xref="S5.E4.m1.1.1.3.2.cmml">N</mi><mrow id="S5.E4.m1.1.1.3.3" xref="S5.E4.m1.1.1.3.3.cmml"><mi id="S5.E4.m1.1.1.3.3.2" xref="S5.E4.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.3.3.1" xref="S5.E4.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.1.3.3.3" xref="S5.E4.m1.1.1.3.3.3.cmml">w</mi></mrow></msub><mrow id="S5.E4.m1.1.1.1.1" xref="S5.E4.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E4.m1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.2.1.cmml">|</mo><mrow id="S5.E4.m1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.cmml"><mi id="S5.E4.m1.1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.1.1.1a" xref="S5.E4.m1.1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S5.E4.m1.1.1.1.1.1.4" xref="S5.E4.m1.1.1.1.1.1.4.cmml">s</mi></mrow><mo stretchy="false" id="S5.E4.m1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.1b"><apply id="S5.E4.m1.1.2.cmml" xref="S5.E4.m1.1.2"><eq id="S5.E4.m1.1.2.1.cmml" xref="S5.E4.m1.1.2.1"></eq><apply id="S5.E4.m1.1.2.2.cmml" xref="S5.E4.m1.1.2.2"><times id="S5.E4.m1.1.2.2.1.cmml" xref="S5.E4.m1.1.2.2.1"></times><ci id="S5.E4.m1.1.2.2.2.cmml" xref="S5.E4.m1.1.2.2.2">ùëÉ</ci><ci id="S5.E4.m1.1.2.2.3.cmml" xref="S5.E4.m1.1.2.2.3">ùëü</ci><ci id="S5.E4.m1.1.2.2.4.cmml" xref="S5.E4.m1.1.2.2.4">ùëí</ci><ci id="S5.E4.m1.1.2.2.5.cmml" xref="S5.E4.m1.1.2.2.5">ùëê</ci><ci id="S5.E4.m1.1.2.2.6.cmml" xref="S5.E4.m1.1.2.2.6">ùëñ</ci><ci id="S5.E4.m1.1.2.2.7.cmml" xref="S5.E4.m1.1.2.2.7">ùë†</ci><ci id="S5.E4.m1.1.2.2.8.cmml" xref="S5.E4.m1.1.2.2.8">ùëñ</ci><ci id="S5.E4.m1.1.2.2.9.cmml" xref="S5.E4.m1.1.2.2.9">ùëú</ci><ci id="S5.E4.m1.1.2.2.10.cmml" xref="S5.E4.m1.1.2.2.10">ùëõ</ci></apply><apply id="S5.E4.m1.1.1.cmml" xref="S5.E4.m1.1.1"><divide id="S5.E4.m1.1.1.2.cmml" xref="S5.E4.m1.1.1"></divide><apply id="S5.E4.m1.1.1.3.cmml" xref="S5.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.3.1.cmml" xref="S5.E4.m1.1.1.3">subscript</csymbol><ci id="S5.E4.m1.1.1.3.2.cmml" xref="S5.E4.m1.1.1.3.2">ùëÅ</ci><apply id="S5.E4.m1.1.1.3.3.cmml" xref="S5.E4.m1.1.1.3.3"><times id="S5.E4.m1.1.1.3.3.1.cmml" xref="S5.E4.m1.1.1.3.3.1"></times><ci id="S5.E4.m1.1.1.3.3.2.cmml" xref="S5.E4.m1.1.1.3.3.2">ùê∂</ci><ci id="S5.E4.m1.1.1.3.3.3.cmml" xref="S5.E4.m1.1.1.3.3.3">ùë§</ci></apply></apply><apply id="S5.E4.m1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1"><abs id="S5.E4.m1.1.1.1.2.1.cmml" xref="S5.E4.m1.1.1.1.1.2"></abs><apply id="S5.E4.m1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1"><times id="S5.E4.m1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1"></times><ci id="S5.E4.m1.1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1.1.2">ùëé</ci><ci id="S5.E4.m1.1.1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.1.1.3">ùëõ</ci><ci id="S5.E4.m1.1.1.1.1.1.4.cmml" xref="S5.E4.m1.1.1.1.1.1.4">ùë†</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.1c">Precision=\frac{N_{Cw}}{|ans|}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.2" class="ltx_p">with <math id="S5.SS2.p7.1.m1.1" class="ltx_Math" alttext="N_{Cw}" display="inline"><semantics id="S5.SS2.p7.1.m1.1a"><msub id="S5.SS2.p7.1.m1.1.1" xref="S5.SS2.p7.1.m1.1.1.cmml"><mi id="S5.SS2.p7.1.m1.1.1.2" xref="S5.SS2.p7.1.m1.1.1.2.cmml">N</mi><mrow id="S5.SS2.p7.1.m1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.3.cmml"><mi id="S5.SS2.p7.1.m1.1.1.3.2" xref="S5.SS2.p7.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.1.m1.1.1.3.1" xref="S5.SS2.p7.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S5.SS2.p7.1.m1.1.1.3.3" xref="S5.SS2.p7.1.m1.1.1.3.3.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><apply id="S5.SS2.p7.1.m1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.1.m1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.2">ùëÅ</ci><apply id="S5.SS2.p7.1.m1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3"><times id="S5.SS2.p7.1.m1.1.1.3.1.cmml" xref="S5.SS2.p7.1.m1.1.1.3.1"></times><ci id="S5.SS2.p7.1.m1.1.1.3.2.cmml" xref="S5.SS2.p7.1.m1.1.1.3.2">ùê∂</ci><ci id="S5.SS2.p7.1.m1.1.1.3.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3.3">ùë§</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">N_{Cw}</annotation></semantics></math> is the number of common words between the output answer and the ground truth answer and <math id="S5.SS2.p7.2.m2.1" class="ltx_Math" alttext="ans" display="inline"><semantics id="S5.SS2.p7.2.m2.1a"><mrow id="S5.SS2.p7.2.m2.1.1" xref="S5.SS2.p7.2.m2.1.1.cmml"><mi id="S5.SS2.p7.2.m2.1.1.2" xref="S5.SS2.p7.2.m2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.2.m2.1.1.1" xref="S5.SS2.p7.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p7.2.m2.1.1.3" xref="S5.SS2.p7.2.m2.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.2.m2.1.1.1a" xref="S5.SS2.p7.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p7.2.m2.1.1.4" xref="S5.SS2.p7.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.2.m2.1b"><apply id="S5.SS2.p7.2.m2.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1"><times id="S5.SS2.p7.2.m2.1.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1.1"></times><ci id="S5.SS2.p7.2.m2.1.1.2.cmml" xref="S5.SS2.p7.2.m2.1.1.2">ùëé</ci><ci id="S5.SS2.p7.2.m2.1.1.3.cmml" xref="S5.SS2.p7.2.m2.1.1.3">ùëõ</ci><ci id="S5.SS2.p7.2.m2.1.1.4.cmml" xref="S5.SS2.p7.2.m2.1.1.4">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.2.m2.1c">ans</annotation></semantics></math> the number of words in the generated answer.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.1" class="ltx_p"><math id="S5.SS2.p8.1.m1.1" class="ltx_Math" alttext="Recall" display="inline"><semantics id="S5.SS2.p8.1.m1.1a"><mrow id="S5.SS2.p8.1.m1.1.1" xref="S5.SS2.p8.1.m1.1.1.cmml"><mi id="S5.SS2.p8.1.m1.1.1.2" xref="S5.SS2.p8.1.m1.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p8.1.m1.1.1.1" xref="S5.SS2.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p8.1.m1.1.1.3" xref="S5.SS2.p8.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p8.1.m1.1.1.1a" xref="S5.SS2.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p8.1.m1.1.1.4" xref="S5.SS2.p8.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p8.1.m1.1.1.1b" xref="S5.SS2.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p8.1.m1.1.1.5" xref="S5.SS2.p8.1.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p8.1.m1.1.1.1c" xref="S5.SS2.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p8.1.m1.1.1.6" xref="S5.SS2.p8.1.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p8.1.m1.1.1.1d" xref="S5.SS2.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p8.1.m1.1.1.7" xref="S5.SS2.p8.1.m1.1.1.7.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.1.m1.1b"><apply id="S5.SS2.p8.1.m1.1.1.cmml" xref="S5.SS2.p8.1.m1.1.1"><times id="S5.SS2.p8.1.m1.1.1.1.cmml" xref="S5.SS2.p8.1.m1.1.1.1"></times><ci id="S5.SS2.p8.1.m1.1.1.2.cmml" xref="S5.SS2.p8.1.m1.1.1.2">ùëÖ</ci><ci id="S5.SS2.p8.1.m1.1.1.3.cmml" xref="S5.SS2.p8.1.m1.1.1.3">ùëí</ci><ci id="S5.SS2.p8.1.m1.1.1.4.cmml" xref="S5.SS2.p8.1.m1.1.1.4">ùëê</ci><ci id="S5.SS2.p8.1.m1.1.1.5.cmml" xref="S5.SS2.p8.1.m1.1.1.5">ùëé</ci><ci id="S5.SS2.p8.1.m1.1.1.6.cmml" xref="S5.SS2.p8.1.m1.1.1.6">ùëô</ci><ci id="S5.SS2.p8.1.m1.1.1.7.cmml" xref="S5.SS2.p8.1.m1.1.1.7">ùëô</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.1.m1.1c">Recall</annotation></semantics></math> instead is defined as:</p>
</div>
<div id="S5.SS2.p9" class="ltx_para">
<table id="S5.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E5.m1.1" class="ltx_Math" alttext="Recall=\frac{N_{Cw}}{|gt|}" display="block"><semantics id="S5.E5.m1.1a"><mrow id="S5.E5.m1.1.2" xref="S5.E5.m1.1.2.cmml"><mrow id="S5.E5.m1.1.2.2" xref="S5.E5.m1.1.2.2.cmml"><mi id="S5.E5.m1.1.2.2.2" xref="S5.E5.m1.1.2.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.2.2.1" xref="S5.E5.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.2.2.3" xref="S5.E5.m1.1.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.2.2.1a" xref="S5.E5.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.2.2.4" xref="S5.E5.m1.1.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.2.2.1b" xref="S5.E5.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.2.2.5" xref="S5.E5.m1.1.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.2.2.1c" xref="S5.E5.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.2.2.6" xref="S5.E5.m1.1.2.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.2.2.1d" xref="S5.E5.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.2.2.7" xref="S5.E5.m1.1.2.2.7.cmml">l</mi></mrow><mo id="S5.E5.m1.1.2.1" xref="S5.E5.m1.1.2.1.cmml">=</mo><mfrac id="S5.E5.m1.1.1" xref="S5.E5.m1.1.1.cmml"><msub id="S5.E5.m1.1.1.3" xref="S5.E5.m1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.3.2" xref="S5.E5.m1.1.1.3.2.cmml">N</mi><mrow id="S5.E5.m1.1.1.3.3" xref="S5.E5.m1.1.1.3.3.cmml"><mi id="S5.E5.m1.1.1.3.3.2" xref="S5.E5.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.1.3.3.1" xref="S5.E5.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.1.3.3.3" xref="S5.E5.m1.1.1.3.3.3.cmml">w</mi></mrow></msub><mrow id="S5.E5.m1.1.1.1.1" xref="S5.E5.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E5.m1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.2.1.cmml">|</mo><mrow id="S5.E5.m1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.cmml"><mi id="S5.E5.m1.1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.1.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S5.E5.m1.1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S5.E5.m1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E5.m1.1b"><apply id="S5.E5.m1.1.2.cmml" xref="S5.E5.m1.1.2"><eq id="S5.E5.m1.1.2.1.cmml" xref="S5.E5.m1.1.2.1"></eq><apply id="S5.E5.m1.1.2.2.cmml" xref="S5.E5.m1.1.2.2"><times id="S5.E5.m1.1.2.2.1.cmml" xref="S5.E5.m1.1.2.2.1"></times><ci id="S5.E5.m1.1.2.2.2.cmml" xref="S5.E5.m1.1.2.2.2">ùëÖ</ci><ci id="S5.E5.m1.1.2.2.3.cmml" xref="S5.E5.m1.1.2.2.3">ùëí</ci><ci id="S5.E5.m1.1.2.2.4.cmml" xref="S5.E5.m1.1.2.2.4">ùëê</ci><ci id="S5.E5.m1.1.2.2.5.cmml" xref="S5.E5.m1.1.2.2.5">ùëé</ci><ci id="S5.E5.m1.1.2.2.6.cmml" xref="S5.E5.m1.1.2.2.6">ùëô</ci><ci id="S5.E5.m1.1.2.2.7.cmml" xref="S5.E5.m1.1.2.2.7">ùëô</ci></apply><apply id="S5.E5.m1.1.1.cmml" xref="S5.E5.m1.1.1"><divide id="S5.E5.m1.1.1.2.cmml" xref="S5.E5.m1.1.1"></divide><apply id="S5.E5.m1.1.1.3.cmml" xref="S5.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S5.E5.m1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.3">subscript</csymbol><ci id="S5.E5.m1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.3.2">ùëÅ</ci><apply id="S5.E5.m1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.3.3"><times id="S5.E5.m1.1.1.3.3.1.cmml" xref="S5.E5.m1.1.1.3.3.1"></times><ci id="S5.E5.m1.1.1.3.3.2.cmml" xref="S5.E5.m1.1.1.3.3.2">ùê∂</ci><ci id="S5.E5.m1.1.1.3.3.3.cmml" xref="S5.E5.m1.1.1.3.3.3">ùë§</ci></apply></apply><apply id="S5.E5.m1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1"><abs id="S5.E5.m1.1.1.1.2.1.cmml" xref="S5.E5.m1.1.1.1.1.2"></abs><apply id="S5.E5.m1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1"><times id="S5.E5.m1.1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1.1"></times><ci id="S5.E5.m1.1.1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1.1.2">ùëî</ci><ci id="S5.E5.m1.1.1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.1.1.3">ùë°</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E5.m1.1c">Recall=\frac{N_{Cw}}{|gt|}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS2.p10" class="ltx_para">
<p id="S5.SS2.p10.1" class="ltx_p">where <math id="S5.SS2.p10.1.m1.1" class="ltx_Math" alttext="|gt|" display="inline"><semantics id="S5.SS2.p10.1.m1.1a"><mrow id="S5.SS2.p10.1.m1.1.1.1" xref="S5.SS2.p10.1.m1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p10.1.m1.1.1.1.2" xref="S5.SS2.p10.1.m1.1.1.2.1.cmml">|</mo><mrow id="S5.SS2.p10.1.m1.1.1.1.1" xref="S5.SS2.p10.1.m1.1.1.1.1.cmml"><mi id="S5.SS2.p10.1.m1.1.1.1.1.2" xref="S5.SS2.p10.1.m1.1.1.1.1.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p10.1.m1.1.1.1.1.1" xref="S5.SS2.p10.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S5.SS2.p10.1.m1.1.1.1.1.3" xref="S5.SS2.p10.1.m1.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S5.SS2.p10.1.m1.1.1.1.3" xref="S5.SS2.p10.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p10.1.m1.1b"><apply id="S5.SS2.p10.1.m1.1.1.2.cmml" xref="S5.SS2.p10.1.m1.1.1.1"><abs id="S5.SS2.p10.1.m1.1.1.2.1.cmml" xref="S5.SS2.p10.1.m1.1.1.1.2"></abs><apply id="S5.SS2.p10.1.m1.1.1.1.1.cmml" xref="S5.SS2.p10.1.m1.1.1.1.1"><times id="S5.SS2.p10.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p10.1.m1.1.1.1.1.1"></times><ci id="S5.SS2.p10.1.m1.1.1.1.1.2.cmml" xref="S5.SS2.p10.1.m1.1.1.1.1.2">ùëî</ci><ci id="S5.SS2.p10.1.m1.1.1.1.1.3.cmml" xref="S5.SS2.p10.1.m1.1.1.1.1.3">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p10.1.m1.1c">|gt|</annotation></semantics></math> is the number of words in the ground truth.</p>
</div>
<div id="S5.SS2.p11" class="ltx_para">
<p id="S5.SS2.p11.1" class="ltx_p">We also evaluate the quality of the descriptions generated by GPT-3, considering it as a standalone image captioning model. We use the following standard metrics for captioning:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">BLEU1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>: BiLingual Evaluation Understudy (BLEU) is the most commonly used metric for machine translation and image captioning. BLEU scores are based on how similar a generated caption is to a reference caption, computing the precision of the generated words. The downside of BLEU is that it is very sensitive to small changes, such as synonyms or different word order.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">ROUGE</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>: differently from BLEU, which measures the precision of the caption, Recall Oriented Understudy of Gisting Evaluation (ROUGE) focuses on quantifying the amount of correct words with respect to the reference. Thus, this metric is recall-based and tends to reward long sentences.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">CIDEr</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>: Consensus-based Image Description Evaluation (CIDEr) is an automatic consensus metric that measures the similarity of captions against a set of ground truth sentences written by humans. This metric has been shown to yield a higher agreement with humans generated text since it captures notions of grammar, importance and precision and recall.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Cosine Similarity</span>: we compute the cosine similarity between feature vectors for the generated caption and the reference caption. Features are extracted with the algorithm TF-IDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Experimental Results</h3>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Image captioning results. We compare our method which generates captions with GPT-3 with the <span id="S5.T1.4.1" class="ltx_text ltx_font_italic">General</span> and the <span id="S5.T1.5.2" class="ltx_text ltx_font_italic">Question-based</span> approaches. In the <span id="S5.T1.6.3" class="ltx_text ltx_font_italic">Question-based</span> approach we concatenate all the outputs of GPT-3 after conditioning it with different questions related to the image. We compare the results against visual captions, contextual captions or both.</figcaption>
<table id="S5.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.7.1.1" class="ltx_tr">
<th id="S5.T1.7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Description type</th>
<th id="S5.T1.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">¬†¬†¬†¬†Metric</th>
<th id="S5.T1.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">¬†¬†OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</th>
<th id="S5.T1.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">¬†¬† Ours General</th>
<th id="S5.T1.7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ours Question-based</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.7.2.1" class="ltx_tr">
<th id="S5.T1.7.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T1.7.2.1.1.1" class="ltx_text">Visual</span></th>
<th id="S5.T1.7.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BLEU1</th>
<td id="S5.T1.7.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.048</td>
<td id="S5.T1.7.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.7.2.1.4.1" class="ltx_text ltx_font_bold">0.181</span></td>
<td id="S5.T1.7.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.137</td>
</tr>
<tr id="S5.T1.7.3.2" class="ltx_tr">
<th id="S5.T1.7.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ROUGE</th>
<td id="S5.T1.7.3.2.2" class="ltx_td ltx_align_center">0.138</td>
<td id="S5.T1.7.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T1.7.3.2.3.1" class="ltx_text ltx_font_bold">0.188</span></td>
<td id="S5.T1.7.3.2.4" class="ltx_td ltx_align_center">0.16</td>
</tr>
<tr id="S5.T1.7.4.3" class="ltx_tr">
<th id="S5.T1.7.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CIDEr</th>
<td id="S5.T1.7.4.3.2" class="ltx_td ltx_align_center">0.091</td>
<td id="S5.T1.7.4.3.3" class="ltx_td ltx_align_center">0.079</td>
<td id="S5.T1.7.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.4.3.4.1" class="ltx_text ltx_font_bold">0.172</span></td>
</tr>
<tr id="S5.T1.7.5.4" class="ltx_tr">
<th id="S5.T1.7.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COSINE</th>
<td id="S5.T1.7.5.4.2" class="ltx_td ltx_align_center">0.113</td>
<td id="S5.T1.7.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.7.5.4.3.1" class="ltx_text ltx_font_bold">0.157</span></td>
<td id="S5.T1.7.5.4.4" class="ltx_td ltx_align_center">0.110</td>
</tr>
<tr id="S5.T1.7.6.5" class="ltx_tr">
<th id="S5.T1.7.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T1.7.6.5.1.1" class="ltx_text">Contextual</span></th>
<th id="S5.T1.7.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BLEU1</th>
<td id="S5.T1.7.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.002</td>
<td id="S5.T1.7.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.7.6.5.4.1" class="ltx_text ltx_font_bold">0.168</span></td>
<td id="S5.T1.7.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.160</td>
</tr>
<tr id="S5.T1.7.7.6" class="ltx_tr">
<th id="S5.T1.7.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ROUGE</th>
<td id="S5.T1.7.7.6.2" class="ltx_td ltx_align_center">0.062</td>
<td id="S5.T1.7.7.6.3" class="ltx_td ltx_align_center">0.178</td>
<td id="S5.T1.7.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.7.6.4.1" class="ltx_text ltx_font_bold">0.179</span></td>
</tr>
<tr id="S5.T1.7.8.7" class="ltx_tr">
<th id="S5.T1.7.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CIDEr</th>
<td id="S5.T1.7.8.7.2" class="ltx_td ltx_align_center">0.000</td>
<td id="S5.T1.7.8.7.3" class="ltx_td ltx_align_center"><span id="S5.T1.7.8.7.3.1" class="ltx_text ltx_font_bold">0.248</span></td>
<td id="S5.T1.7.8.7.4" class="ltx_td ltx_align_center">0.129</td>
</tr>
<tr id="S5.T1.7.9.8" class="ltx_tr">
<th id="S5.T1.7.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COSINE</th>
<td id="S5.T1.7.9.8.2" class="ltx_td ltx_align_center">0.082</td>
<td id="S5.T1.7.9.8.3" class="ltx_td ltx_align_center">0.218</td>
<td id="S5.T1.7.9.8.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.9.8.4.1" class="ltx_text ltx_font_bold">0.324</span></td>
</tr>
<tr id="S5.T1.7.10.9" class="ltx_tr">
<th id="S5.T1.7.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T1.7.10.9.1.1" class="ltx_text">All</span></th>
<th id="S5.T1.7.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BLEU1</th>
<td id="S5.T1.7.10.9.3" class="ltx_td ltx_align_center ltx_border_t">0.000</td>
<td id="S5.T1.7.10.9.4" class="ltx_td ltx_align_center ltx_border_t">0.113</td>
<td id="S5.T1.7.10.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.7.10.9.5.1" class="ltx_text ltx_font_bold">0.185</span></td>
</tr>
<tr id="S5.T1.7.11.10" class="ltx_tr">
<th id="S5.T1.7.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ROUGE</th>
<td id="S5.T1.7.11.10.2" class="ltx_td ltx_align_center">0.053</td>
<td id="S5.T1.7.11.10.3" class="ltx_td ltx_align_center">0.158</td>
<td id="S5.T1.7.11.10.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.11.10.4.1" class="ltx_text ltx_font_bold">0.184</span></td>
</tr>
<tr id="S5.T1.7.12.11" class="ltx_tr">
<th id="S5.T1.7.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CIDEr</th>
<td id="S5.T1.7.12.11.2" class="ltx_td ltx_align_center">0.000</td>
<td id="S5.T1.7.12.11.3" class="ltx_td ltx_align_center">0.016</td>
<td id="S5.T1.7.12.11.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.12.11.4.1" class="ltx_text ltx_font_bold">0.098</span></td>
</tr>
<tr id="S5.T1.7.13.12" class="ltx_tr">
<th id="S5.T1.7.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COSINE</th>
<td id="S5.T1.7.13.12.2" class="ltx_td ltx_align_center">0.122</td>
<td id="S5.T1.7.13.12.3" class="ltx_td ltx_align_center">0.253</td>
<td id="S5.T1.7.13.12.4" class="ltx_td ltx_align_center"><span id="S5.T1.7.13.12.4.1" class="ltx_text ltx_font_bold">0.341</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Captioning results</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">We start by assessing the quality of the captions generated by GPT-3.
First of all, we ask GPT-3 to generate captions with our <span id="S5.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">General</span> approach. In Tab. <a href="#S5.T1" title="Table 1 ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we compare the captions using as reference visual captions, contextual captions and both. All reference captions are ground truth captions taken from the Artpedia dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">Interestingly, the model appears to better results for visual captions using BLEU1 and ROUGE metrics, while using CIDEr and cosine similarity, the model obtaines higher results for contextual captions. This may seem counter-intuitive but can be explained looking at the nature of the metrics. BLEU1 and ROUGE in fact respectively check for word-wise precision and recall, while CIDEr and cosine distance perform a sentence level scoring, which is closer to human consensus.
We observe that the model is able to obtain good results, especially with the cosine metric, even when using all the captions as reference.</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p">We then evaluate the method by taking a concatenation of the outputs generated by GPT-3 after being conditioned by different questions related to the image.
This obviously introduces a strong bias, given also the fact that questions have been generated from information contained in the captions, but at the same time proves the usefulness of such captions for more advanced applications such as visual question answering. As can be seen in Tab. <a href="#S5.T1" title="Table 1 ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, conditioning GPT-3 with the captions leads to better captions according to most metrics.</p>
</div>
<div id="S5.SS3.SSS1.p4" class="ltx_para">
<p id="S5.SS3.SSS1.p4.1" class="ltx_p">In Tab. <a href="#S5.T1" title="Table 1 ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we also provide a baseline as reference, i.e. the output of the state of the art OFA captioning model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. We observe that captions generated by OFA do not align well with the ground truth sentences. We attribute this to a domain shift between the datasets commonly used to train captioning models and descriptions of artworks. In fact, the former are sentences written by non-experts while for applications in cultural heritage a domain knowledge is required. This further motivates the usage of GPT-3, which seems to have integrated sufficient knowledge to articulate complex sentences with a domain specific jargon.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results for Visual Question Answering. We compare our approach against VQA-CH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to understand whether GPT-3 can replace information sheets for artworks either for visual or contextual questions. We compare two versions of our model, the <span id="S5.T2.3.1" class="ltx_text ltx_font_italic">General</span> version, which produces generic descriptions of artworks and the <span id="S5.T2.4.2" class="ltx_text ltx_font_italic">Question-based</span> version, where prompts are conditioned with the input question to generate more specific descriptions.</figcaption>
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.5.1.1" class="ltx_tr">
<th id="S5.T2.5.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Visual</th>
<th id="S5.T2.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Contextual</th>
<th id="S5.T2.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Accuracy</th>
<th id="S5.T2.5.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">F1 score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.5.2.1" class="ltx_tr">
<th id="S5.T2.5.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VQA-CH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S5.T2.5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S5.T2.5.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.684</td>
<td id="S5.T2.5.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.832</td>
</tr>
<tr id="S5.T2.5.3.2" class="ltx_tr">
<th id="S5.T2.5.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VQA-CH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.5.3.2.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.3.2.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S5.T2.5.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.176</td>
<td id="S5.T2.5.3.2.5" class="ltx_td ltx_align_center">0.150</td>
</tr>
<tr id="S5.T2.5.4.3" class="ltx_tr">
<th id="S5.T2.5.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VQA-CH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.5.4.3.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.4.3.3" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.504</td>
<td id="S5.T2.5.4.3.5" class="ltx_td ltx_align_center">0.417</td>
</tr>
<tr id="S5.T2.5.5.4" class="ltx_tr">
<th id="S5.T2.5.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours - General</th>
<td id="S5.T2.5.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S5.T2.5.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S5.T2.5.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.557</td>
<td id="S5.T2.5.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.719</td>
</tr>
<tr id="S5.T2.5.6.5" class="ltx_tr">
<th id="S5.T2.5.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours - General</th>
<td id="S5.T2.5.6.5.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.6.5.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S5.T2.5.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.070</td>
<td id="S5.T2.5.6.5.5" class="ltx_td ltx_align_center">0.055</td>
</tr>
<tr id="S5.T2.5.7.6" class="ltx_tr">
<th id="S5.T2.5.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours - General</th>
<td id="S5.T2.5.7.6.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.7.6.3" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.7.6.4" class="ltx_td ltx_align_center ltx_border_r">0.239</td>
<td id="S5.T2.5.7.6.5" class="ltx_td ltx_align_center">0.360</td>
</tr>
<tr id="S5.T2.5.8.7" class="ltx_tr">
<th id="S5.T2.5.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours - Question-based</th>
<td id="S5.T2.5.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S5.T2.5.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S5.T2.5.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.473</td>
<td id="S5.T2.5.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.602</td>
</tr>
<tr id="S5.T2.5.9.8" class="ltx_tr">
<th id="S5.T2.5.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours - Question-based</th>
<td id="S5.T2.5.9.8.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.9.8.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S5.T2.5.9.8.4" class="ltx_td ltx_align_center ltx_border_r">0.134</td>
<td id="S5.T2.5.9.8.5" class="ltx_td ltx_align_center">0.202</td>
</tr>
<tr id="S5.T2.5.10.9" class="ltx_tr">
<th id="S5.T2.5.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours - Question-based</th>
<td id="S5.T2.5.10.9.2" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.10.9.3" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S5.T2.5.10.9.4" class="ltx_td ltx_align_center ltx_border_r">0.256</td>
<td id="S5.T2.5.10.9.5" class="ltx_td ltx_align_center">0.330</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>VQA results</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">To evaluate the Visual Question Answering capabilities of our proposed method, we follow the setting of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, we do not rely on any vision-based model but rather on a fully textual question answering model based on DistilBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, as explained in Sec. <a href="#S4" title="4 Method ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
In Tab. <a href="#S5.T2" title="Table 2 ‚Ä£ 5.3.1 Captioning results ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare our approach to the one of VQA-CH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. It has to be noted that, contrary to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we do not rely on real textual descriptions, which are known to contain the answer, but we only extract information from GPT-3. This is a strong disadvantage for our method. However, we are not interested in obtaining better results than VQA-CH, but rather our goal is to demonstrate if GPT-3 can act as a substitute of textual descriptions handcrafted by domain experts.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">We test our method evaluating the accuracy for visual questions, contextual questions and both together.
Quantitative results indicate that captions generated by GPT-3 can yield to high results for contextual questions, yet very low accuracy for visual questions.
As for the captioning setting, we impute this behavior to the fact that GPT-3 generates generic descriptions, without including a fine-grained description of the visual content.
Thus, on the one hand the question answering model is capable of extracting meaningful information from the generated captions. This means that GPT-3 is indeed capable of integrating domain knowledge during training and is capable of generating a complete information sheet of the artwork. On the other hand, captions appear to be too generic to obtain information about specific details in the image.</p>
</div>
<div id="S5.SS3.SSS2.p3" class="ltx_para">
<p id="S5.SS3.SSS2.p3.1" class="ltx_p">To overcome this limitation, we test the model using captions generated with out <span id="S5.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_italic">Question-based</span> approach. By feeding the answer to GPT-3 along with the title of the artwork, the model is able to generate more specific captions. Such captions, as explained in Sec. <a href="#S6" title="6 Qualitative Analysis ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> are usually shorter but are focused on the prompt. This is particularly interesting since it means that a purely text-based model is capable of addressing a vision-based task. In Tab. <a href="#S5.T2" title="Table 2 ‚Ä£ 5.3.1 Captioning results ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> it can be seen that for visual questions alone, our method with question-based captions performs on par or better than the vision-based VQA-CH model.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2207.12101/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative results of our method. <span id="S5.F3.4.1" class="ltx_text ltx_font_italic">Green</span>: ground truth description from the Artpedia dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and input question. <span id="S5.F3.5.2" class="ltx_text ltx_font_italic">Yellow</span>: general descriptions provided by GPT-3 and answer obtained based on such text. <span id="S5.F3.6.3" class="ltx_text ltx_font_italic">Blue</span>: Question-based description and correspondent answer.
General descriptions are longer and more detailed than question-based generated descriptions. However, question-based generated descriptions are customized for the specific question.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Qualitative Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section we provide a qualitative analysis of the captions generated by GPT-3 in order to characterize which kind of information they contain in both the <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">General</span> and <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">Question-based</span> formulation.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Since the prompts that we feed to GPT-3 are different, with one being more general and the other being question-based, we expect that the corresponding generated text by GPT-3 will be different.
In Fig. <a href="#S5.F3" title="Figure 3 ‚Ä£ 5.3.2 VQA results ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we can observe these differences.
Generated general descriptions are very long and have the aspect of artwork information sheets in which we can find some visual and contextual information. Question-based generated descriptions are instead shorter and contain the knowledge needed to answer to the specific questions.
From Fig <a href="#S5.F3" title="Figure 3 ‚Ä£ 5.3.2 VQA results ‚Ä£ 5.3 Experimental Results ‚Ä£ 5 Experiments ‚Ä£ Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we can observe that the general description is very useful to answer to contextual questions but fails on some visual questions. This is likely due to different reasons:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">The generated text does not take into account any specific question and this can lead to the generation of a description without specific information useful to answer to the question.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Visual questions are very specific since they refer to object relationships, colors, counting, etc. and the GPT-3 model tends to be more shallow in generating its descriptions.</p>
</div>
</li>
</ul>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">On the other hand, question-based generated descriptions are helpful to answer visual questions but the small generated description useful to answer those specific questions could contain incorrect information leading to wrong answer predictions.
In conclusion these two ways of generating text to answer visual and contextual questions have some pros and cons:</p>
<ul id="S6.I2" class="ltx_itemize">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.1" class="ltx_p">General descriptions are longer and contain several pieces of information about the artwork. However this is fixed and could not contain the information needed to answer some questions.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.1" class="ltx_p">Question-based descriptions are generated for specific questions and contain only the information needed to answer the question on which GPT-3 has been conditioned. If the model has not memorized any specific information regarding such questions it may contain mistakes and descriptions will have to be re-computed for each question.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Considerations on complexity and accessibility of GPT-3</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In the previous sections we have demonstrated that GPT-3 could indeed replace the usage of an information sheet handcrafted by a domain expert. However, we need to understand the actual applicability of GPT-3 in a real case application.
GPT-3 has 175B parameters, which approximately amounts to 700GB. This means that inference on a single GPU is unfeasible due to current technological limits. The model however has been made available from OpenAI and is accessible through API that have a pricing fee per generated token. These considerations somewhat limit a large-scale usage of the model, especially if a description has to be generated for each question to be answered. On the other hand, generating fixed descriptions offline, one for each artwork, appears a viable solution at least for addressing contextual questions.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper we presented a method for Visual Question Answering in the Cultural Heritage domain. In particular we have addressed the problem of data annotation for artworks, generating descriptions with GPT-3. The performances for the VQA task show that the generated descriptions are useful to answer the questions correctly. This technique allows to answer visual and contextual questions focusing only on the generated description and can be used for any artwork. In fact, there is no need to retrain the model to incorporate new knowledge. This is possibile thanks to the memorization capabilities of GPT-3, which at training time has observed millions of tokens regarding domain-specific knowledge. Finally the generated description can be integrated as textual input (textual description) in a more complex architecture as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> in order to address tasks like Visual Question Answering. This is of particular interest for Cultural Heritage due to the domain shift between common VQA and captioning datasets compared to the technical jargon that is needed to properly address questions about art.</p>
</div>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Acknowledgements</h5>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p">This work was partially supported by the project ARS01 00421:
‚ÄúPON IDEHA - Innovazioni per l‚Äôelaborazione dei dati nel settore del Patrimonio Culturale.‚Äù
This work was partially supported by the
European Commission under European Horizon 2020 Programme, grant number 101004545 - ReInHerit.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 6077‚Äì6086 (2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: Vqa: Visual question answering. In: Proceedings of the IEEE international
conference on computer vision. pp. 2425‚Äì2433 (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Asprino, L., Bulla, L., Marinucci, L., Mongiov√¨, M., Presutti, V.: A large
visual question answering dataset for cultural heritage. In: International
Conference on Machine Learning, Optimization, and Data Science. pp. 193‚Äì197.
Springer (2021)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Barra, S., Bisogni, C., De¬†Marsico, M., Ricciardi, S.: Visual question
answering: Which investigated applications? Pattern Recognition Letters
<span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">151</span>, 325‚Äì331 (2021)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Becattini, F., Ferracani, A., Landucci, L., Pezzatini, D., Uricchio, T.,
Del¬†Bimbo, A.: Imaging novecento. a mobile app for automatic recognition of
artworks and transfer of artistic styles. In: Euro-Mediterranean Conference.
pp. 781‚Äì791. Springer (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bongini, P., Becattini, F., Bagdanov, A.D., Del¬†Bimbo, A.: Visual question
answering for cultural heritage. In: IOP Conference Series: Materials Science
and Engineering. vol.¬†949, p. 012074. IOP Publishing (2020)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
I., Amodei, D.: Language models are few-shot learners (2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Cetinic, E., She, J.: Understanding and creating art with ai: Review and
outlook. ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM) <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">18</span>(2), 1‚Äì22 (2022)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Cornia, M., Stefanini, M., Baraldi, L., Cucchiara, R.: Meshed-memory
transformer for image captioning. In: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition. pp. 10578‚Äì10587 (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dale, R.: Gpt-3: What‚Äôs it good for? Natural Language Engineering
<span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">27</span>(1), 113‚Äì118 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Elkins, K., Chun, J.: Can gpt-3 pass a writer‚Äôs turing test? Journal of
Cultural Analytics <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">5</span>(2), 17212 (2020)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Fiorucci, M., Khoroshiltseva, M., Pontil, M., Traviglia, A., Del¬†Bue, A.,
James, S.: Machine learning for cultural heritage: A survey. Pattern
Recognition Letters <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">133</span>, 102‚Äì108 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Garcia, N., Ye, C., Liu, Z., Hu, Q., Otani, M., Chu, C., Nakashima, Y.,
Mitamura, T.: A dataset and baselines for visual question answering on art.
In: European Conference on Computer Vision. pp. 92‚Äì108. Springer (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Han, J., Zhang, D., Cheng, G., Liu, N., Xu, D.: Advanced deep-learning
techniques for salient and category-specific object detection: a survey. IEEE
Signal Processing Magazine <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">35</span>(1), 84‚Äì100 (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kheradpisheh, S.R., Ganjtabesh, M., Thorpe, S.J., Masquelier, T.: Stdp-based
spiking deep convolutional neural networks for object recognition. Neural
Networks <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">99</span>, 56‚Äì67 (2018)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong,
L., Wei, F., et¬†al.: Oscar: Object-semantics aligned pre-training for
vision-language tasks. In: European Conference on Computer Vision. pp.
121‚Äì137. Springer (2020)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text
summarization branches out. pp. 74‚Äì81 (2004)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Liu, S., Zhu, Z., Ye, N., Guadarrama, S., Murphy, K.: Improved image captioning
via policy gradient optimization of spider. In: Proceedings of the IEEE
international conference on computer vision. pp. 873‚Äì881 (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image
co-attention for visual question answering. In: Advances In Neural
Information Processing Systems. pp. 289‚Äì297 (2016)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th annual meeting
of the Association for Computational Linguistics. pp. 311‚Äì318 (2002)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et¬†al.: Improving
language understanding by generative pre-training (2018)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et¬†al.:
Language models are unsupervised multitask learners. OpenAI blog
<span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">1</span>(8), ¬†9 (2019)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: Squad: 100,000+ questions for
machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. In: Advances in neural information
processing systems. pp. 91‚Äì99 (2015)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Salton, G., Buckley, C.: Term-weighting approaches in automatic text retrieval.
Information processing &amp; management <span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">24</span>(5), 513‚Äì523 (1988)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108
(2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Seidenari, L., Galteri, L., Bongini, P., Bertini, M., Del¬†Bimbo, A.: Language
based image quality assessment. In: ACM Multimedia Asia, pp.¬†1‚Äì7 (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Sheng, S., Laenen, K., Moens, M.F.: Can image captioning help passage retrieval
in multimodal question answering? In: European Conference on Information
Retrieval. pp. 94‚Äì101. Springer (2019)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Shih, K.J., Singh, S., Hoiem, D.: Where to look: Focus regions for visual
question answering. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 4613‚Äì4621 (2016)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Stefanini, M., Cornia, M., Baraldi, L., Corsini, M., Cucchiara, R.: Artpedia: A
new visual-semantic dataset with visual and contextual sentences in the
artistic domain. In: International Conference on Image Analysis and
Processing. pp. 729‚Äì740. Springer (2019)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert:
Pre-training of generic visual-linguistic representations. arXiv preprint
arXiv:1908.08530 (2019)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Sun, T.X., Liu, X.Y., Qiu, X.P., Huang, X.J.: Paradigm shift in natural
language processing. Machine Intelligence Research <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">19</span>(3), 169‚Äì183
(2022)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations
from transformers. arXiv preprint arXiv:1908.07490 (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Vannoni, F., Bongini, P., Becattini, F., Bagdanov, A.D., Bimbo, A.: Data
collection for contextual and visual question answering in the cultural
heritage domain (2020)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, ≈Å., Polosukhin, I.: Attention is all you need. In: Advances in
neural information processing systems. pp. 5998‚Äì6008 (2017)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Vedantam, R., Lawrence¬†Zitnick, C., Parikh, D.: Cider: Consensus-based image
description evaluation. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 4566‚Äì4575 (2015)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,
J., Yang, H.: Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052
(2022)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., Wang, L.: An empirical
study of gpt-3 for few-shot knowledge-based vqa. In: Proceedings of the AAAI
Conference on Artificial Intelligence. vol.¬†36, pp. 3081‚Äì3089 (2022)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.12100" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.12101" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.12101">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.12101" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.12102" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 03:00:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
