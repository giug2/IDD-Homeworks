<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  An Approach to Solving the ARC Challenge
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    John Chong Min Tan
    <br class="ltx_break"/>
    Department of Electrical and Computer Engineering
    <br class="ltx_break"/>
    National University of Singapore
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id1.1.id1">
     johntancm@u.nus.edu
    </span>
    <br class="ltx_break"/>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id2.id1">
   We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task. Here, we give the model some human priors via text, along with some typical procedures for solving the ARC tasks, and ask it to generate the i) broad description of the input-output relation, ii) detailed steps of the input-output mapping, iii) use the detailed steps to perform manipulation on the test input and derive the test output. The current GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those with small grids of 8x8 and below). With tweaks to the prompt to make it more specific for the use case, it can solve more. We posit that when scaled to a multi-agent system with usage of past memory and equipped with an image interpretation tool via Visual Question Answering, we may actually be able to solve the majority of the ARC challenge.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Background
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The ARC Challenge is a very interesting challenge, as it is doing something counter to mainstream deep learning – learning from very few samples. Deep learning typically uses tens of thousands of samples to do well, for instance learning to classify digits (MNIST)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Deng,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2012
     </a>
     )
    </cite>
    requires around 50,000 training samples. Humans, in comparison, can learn how to identify different animals by just one or two different observations. For instance, my 3 year-old kid can learn how to identify a giraffe in real life for the first time, even though the only other time he was exposed to a giraffe was through a cartoon flash card. Such capabilities are not well endowed in modern AI systems, and that means that such AI systems will need to be trained extensively before deploying in the real world. After deploying them in the real world, they will also be limited in their ability to adapt and learn as the environment changes.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    In contrast, traditional rule-based systems (e.g. GOFAI) can “learn” quite fast, as any new situation can be interpreted without any learning phase, provided that the situation is already in the system rules given to it. Such a rule-based system could be symbolic systems or expert systems which already have the domain knowledge fed to it by human experts. However, the history of GOFAI has shown that it is difficult to engineer these rules out, and at many times, even humans face difficulty to come up with the rules as they may not be able to express it in words.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    As you can see, there are shortcomings with the above two approaches, and a new kind of approach will need to be used in order to learn fast and generalise to new situations, in order to even have a chance at solving the ARC Challenge.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Next token prediction for self-supervised learning
  </h2>
  <figure class="ltx_figure" id="S2.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="272" id="S2.F1.g1" src="/html/2306.03553/assets/Pictures/transformer-decoder-intro.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Transformer next-token prediction framework illustration. Here, for simplicity, we just denote the tokens as words, when in actual fact, it should be at the sub-word level built up using a variant of Byte Pair Encoding (BPE)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shibata et al.,
     <a class="ltx_ref" href="#bib.bib17" title="">
      1999
     </a>
     )
    </cite>
    . Image taken taken from:
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" title="">
     https://jalammar.github.io/illustrated-gpt2/
    </a>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Alammar,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2023a
     </a>
     )
    </cite>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    There is a lot of structure in the world. These structures can be hard to represent via verbal rules, yet children can learn how physics work and how to interact with the world just by observation and action. Personally, I believe that simply observing is not enough – one has to perform actions in order to learn how one’s actions can affect the world. However, for tasks like learning language, the next action to take is simply to predict the next token and can be done without interaction with the world. Large Language Models (LLMs) such as GPT2
    <cite class="ltx_cite ltx_citemacro_citep">
     (Radford et al.,
     <a class="ltx_ref" href="#bib.bib13" title="">
      2019
     </a>
     )
    </cite>
    , GPT 3.5
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ouyang et al.,
     <a class="ltx_ref" href="#bib.bib11" title="">
      2022
     </a>
     )
    </cite>
    and GPT4
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bubeck et al.,
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     )
    </cite>
    have utilised an extensive amount of self-supervised learning via next-token prediction in order to learn the structure of text (See Fig.
    <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Next token prediction for self-supervised learning ‣ An Approach to Solving the ARC Challenge">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ). This is a huge breakthrough, as the predominant approach to deep learning - supervised learning - requires extensive human labelling and is expensive and impractical to obtain for large amounts of data. This self-supervised learning approach can generate labels simply by predicting the next token and is easily obtainable from the world’s worth of text on the World Wide Web. For instance, the sentence "The cat sat on the mat" can easily be used in at least 5 different prediction tasks (assuming tokens are defined at the word level), as shown below:
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <ol class="ltx_enumerate" id="S2.I1">
    <li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S2.I1.i1.p1">
      <p class="ltx_p" id="S2.I1.i1.p1.1">
       <math alttext="The\ \rightarrow cat" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1">
        <semantics id="S2.I1.i1.p1.1.m1.1a">
         <mrow id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">
          <mrow id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">
           <mi id="S2.I1.i1.p1.1.m1.1.1.2.2" xref="S2.I1.i1.p1.1.m1.1.1.2.2.cmml">
            T
           </mi>
           <mo id="S2.I1.i1.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S2.I1.i1.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i1.p1.1.m1.1.1.2.3" xref="S2.I1.i1.p1.1.m1.1.1.2.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i1.p1.1.m1.1.1.2.1a" lspace="0em" rspace="0em" xref="S2.I1.i1.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i1.p1.1.m1.1.1.2.4" xref="S2.I1.i1.p1.1.m1.1.1.2.4.cmml">
            e
           </mi>
          </mrow>
          <mo id="S2.I1.i1.p1.1.m1.1.1.1" lspace="0.778em" stretchy="false" xref="S2.I1.i1.p1.1.m1.1.1.1.cmml">
           →
          </mo>
          <mrow id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">
           <mi id="S2.I1.i1.p1.1.m1.1.1.3.2" xref="S2.I1.i1.p1.1.m1.1.1.3.2.cmml">
            c
           </mi>
           <mo id="S2.I1.i1.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.I1.i1.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i1.p1.1.m1.1.1.3.3" xref="S2.I1.i1.p1.1.m1.1.1.3.3.cmml">
            a
           </mi>
           <mo id="S2.I1.i1.p1.1.m1.1.1.3.1a" lspace="0em" rspace="0em" xref="S2.I1.i1.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i1.p1.1.m1.1.1.3.4" xref="S2.I1.i1.p1.1.m1.1.1.3.4.cmml">
            t
           </mi>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b">
          <apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">
           <ci id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.1">
            →
           </ci>
           <apply id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">
            <times id="S2.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.1">
            </times>
            <ci id="S2.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.2">
             𝑇
            </ci>
            <ci id="S2.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.3">
             ℎ
            </ci>
            <ci id="S2.I1.i1.p1.1.m1.1.1.2.4.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.4">
             𝑒
            </ci>
           </apply>
           <apply id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">
            <times id="S2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.1">
            </times>
            <ci id="S2.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.2">
             𝑐
            </ci>
            <ci id="S2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3">
             𝑎
            </ci>
            <ci id="S2.I1.i1.p1.1.m1.1.1.3.4.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.4">
             𝑡
            </ci>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">
          The\ \rightarrow cat
         </annotation>
        </semantics>
       </math>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S2.I1.i2.p1">
      <p class="ltx_p" id="S2.I1.i2.p1.1">
       <math alttext="The\ cat\ \rightarrow sat" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1">
        <semantics id="S2.I1.i2.p1.1.m1.1a">
         <mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">
          <mrow id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.2" xref="S2.I1.i2.p1.1.m1.1.1.2.2.cmml">
            T
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.3" xref="S2.I1.i2.p1.1.m1.1.1.2.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.2.1a" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.4" xref="S2.I1.i2.p1.1.m1.1.1.2.4.cmml">
            e
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.2.1b" lspace="0.500em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.5" xref="S2.I1.i2.p1.1.m1.1.1.2.5.cmml">
            c
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.2.1c" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.6" xref="S2.I1.i2.p1.1.m1.1.1.2.6.cmml">
            a
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.2.1d" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.2.7" xref="S2.I1.i2.p1.1.m1.1.1.2.7.cmml">
            t
           </mi>
          </mrow>
          <mo id="S2.I1.i2.p1.1.m1.1.1.1" lspace="0.778em" stretchy="false" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">
           →
          </mo>
          <mrow id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">
           <mi id="S2.I1.i2.p1.1.m1.1.1.3.2" xref="S2.I1.i2.p1.1.m1.1.1.3.2.cmml">
            s
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.3.3" xref="S2.I1.i2.p1.1.m1.1.1.3.3.cmml">
            a
           </mi>
           <mo id="S2.I1.i2.p1.1.m1.1.1.3.1a" lspace="0em" rspace="0em" xref="S2.I1.i2.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i2.p1.1.m1.1.1.3.4" xref="S2.I1.i2.p1.1.m1.1.1.3.4.cmml">
            t
           </mi>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b">
          <apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">
           <ci id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1">
            →
           </ci>
           <apply id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">
            <times id="S2.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.1">
            </times>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.2">
             𝑇
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.3">
             ℎ
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.4.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.4">
             𝑒
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.5.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.5">
             𝑐
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.6.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.6">
             𝑎
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.2.7.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.7">
             𝑡
            </ci>
           </apply>
           <apply id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">
            <times id="S2.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.1">
            </times>
            <ci id="S2.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.2">
             𝑠
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3">
             𝑎
            </ci>
            <ci id="S2.I1.i2.p1.1.m1.1.1.3.4.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.4">
             𝑡
            </ci>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">
          The\ cat\ \rightarrow sat
         </annotation>
        </semantics>
       </math>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S2.I1.i3.p1">
      <p class="ltx_p" id="S2.I1.i3.p1.1">
       <math alttext="The\ cat\ sat\ \rightarrow on" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1">
        <semantics id="S2.I1.i3.p1.1.m1.1a">
         <mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml">
          <mrow id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">
            T
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1a" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.4" xref="S2.I1.i3.p1.1.m1.1.1.2.4.cmml">
            e
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1b" lspace="0.500em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.5" xref="S2.I1.i3.p1.1.m1.1.1.2.5.cmml">
            c
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1c" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.6" xref="S2.I1.i3.p1.1.m1.1.1.2.6.cmml">
            a
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1d" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.7" xref="S2.I1.i3.p1.1.m1.1.1.2.7.cmml">
            t
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1e" lspace="0.500em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.8" xref="S2.I1.i3.p1.1.m1.1.1.2.8.cmml">
            s
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1f" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.9" xref="S2.I1.i3.p1.1.m1.1.1.2.9.cmml">
            a
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.2.1g" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.2.10" xref="S2.I1.i3.p1.1.m1.1.1.2.10.cmml">
            t
           </mi>
          </mrow>
          <mo id="S2.I1.i3.p1.1.m1.1.1.1" lspace="0.778em" stretchy="false" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">
           →
          </mo>
          <mrow id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">
           <mi id="S2.I1.i3.p1.1.m1.1.1.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.2.cmml">
            o
           </mi>
           <mo id="S2.I1.i3.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i3.p1.1.m1.1.1.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.cmml">
            n
           </mi>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b">
          <apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">
           <ci id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1">
            →
           </ci>
           <apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">
            <times id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.1">
            </times>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.2">
             𝑇
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.3">
             ℎ
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.4.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.4">
             𝑒
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.5.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.5">
             𝑐
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.6.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.6">
             𝑎
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.7.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.7">
             𝑡
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.8.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.8">
             𝑠
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.9.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.9">
             𝑎
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.2.10.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.10">
             𝑡
            </ci>
           </apply>
           <apply id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">
            <times id="S2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.1">
            </times>
            <ci id="S2.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.2">
             𝑜
            </ci>
            <ci id="S2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3">
             𝑛
            </ci>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">
          The\ cat\ sat\ \rightarrow on
         </annotation>
        </semantics>
       </math>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S2.I1.i4.p1">
      <p class="ltx_p" id="S2.I1.i4.p1.1">
       <math alttext="The\ cat\ sat\ on\ \rightarrow the" class="ltx_Math" display="inline" id="S2.I1.i4.p1.1.m1.1">
        <semantics id="S2.I1.i4.p1.1.m1.1a">
         <mrow id="S2.I1.i4.p1.1.m1.1.1" xref="S2.I1.i4.p1.1.m1.1.1.cmml">
          <mrow id="S2.I1.i4.p1.1.m1.1.1.2" xref="S2.I1.i4.p1.1.m1.1.1.2.cmml">
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.2" xref="S2.I1.i4.p1.1.m1.1.1.2.2.cmml">
            T
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.3" xref="S2.I1.i4.p1.1.m1.1.1.2.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1a" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.4" xref="S2.I1.i4.p1.1.m1.1.1.2.4.cmml">
            e
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1b" lspace="0.500em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.5" xref="S2.I1.i4.p1.1.m1.1.1.2.5.cmml">
            c
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1c" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.6" xref="S2.I1.i4.p1.1.m1.1.1.2.6.cmml">
            a
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1d" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.7" xref="S2.I1.i4.p1.1.m1.1.1.2.7.cmml">
            t
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1e" lspace="0.500em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.8" xref="S2.I1.i4.p1.1.m1.1.1.2.8.cmml">
            s
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1f" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.9" xref="S2.I1.i4.p1.1.m1.1.1.2.9.cmml">
            a
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1g" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.10" xref="S2.I1.i4.p1.1.m1.1.1.2.10.cmml">
            t
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1h" lspace="0.500em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.11" xref="S2.I1.i4.p1.1.m1.1.1.2.11.cmml">
            o
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.2.1i" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.2.12" xref="S2.I1.i4.p1.1.m1.1.1.2.12.cmml">
            n
           </mi>
          </mrow>
          <mo id="S2.I1.i4.p1.1.m1.1.1.1" lspace="0.778em" stretchy="false" xref="S2.I1.i4.p1.1.m1.1.1.1.cmml">
           →
          </mo>
          <mrow id="S2.I1.i4.p1.1.m1.1.1.3" xref="S2.I1.i4.p1.1.m1.1.1.3.cmml">
           <mi id="S2.I1.i4.p1.1.m1.1.1.3.2" xref="S2.I1.i4.p1.1.m1.1.1.3.2.cmml">
            t
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.3.3" xref="S2.I1.i4.p1.1.m1.1.1.3.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i4.p1.1.m1.1.1.3.1a" lspace="0em" rspace="0em" xref="S2.I1.i4.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i4.p1.1.m1.1.1.3.4" xref="S2.I1.i4.p1.1.m1.1.1.3.4.cmml">
            e
           </mi>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.I1.i4.p1.1.m1.1b">
          <apply id="S2.I1.i4.p1.1.m1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1">
           <ci id="S2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1.1">
            →
           </ci>
           <apply id="S2.I1.i4.p1.1.m1.1.1.2.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2">
            <times id="S2.I1.i4.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.1">
            </times>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.2">
             𝑇
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.3">
             ℎ
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.4.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.4">
             𝑒
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.5.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.5">
             𝑐
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.6.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.6">
             𝑎
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.7.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.7">
             𝑡
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.8.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.8">
             𝑠
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.9.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.9">
             𝑎
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.10.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.10">
             𝑡
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.11.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.11">
             𝑜
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.2.12.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2.12">
             𝑛
            </ci>
           </apply>
           <apply id="S2.I1.i4.p1.1.m1.1.1.3.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3">
            <times id="S2.I1.i4.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3.1">
            </times>
            <ci id="S2.I1.i4.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3.2">
             𝑡
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3.3">
             ℎ
            </ci>
            <ci id="S2.I1.i4.p1.1.m1.1.1.3.4.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3.4">
             𝑒
            </ci>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.I1.i4.p1.1.m1.1c">
          The\ cat\ sat\ on\ \rightarrow the
         </annotation>
        </semantics>
       </math>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      5.
     </span>
     <div class="ltx_para" id="S2.I1.i5.p1">
      <p class="ltx_p" id="S2.I1.i5.p1.1">
       <math alttext="The\ cat\ sat\ on\ the\ \rightarrow mat" class="ltx_Math" display="inline" id="S2.I1.i5.p1.1.m1.1">
        <semantics id="S2.I1.i5.p1.1.m1.1a">
         <mrow id="S2.I1.i5.p1.1.m1.1.1" xref="S2.I1.i5.p1.1.m1.1.1.cmml">
          <mrow id="S2.I1.i5.p1.1.m1.1.1.2" xref="S2.I1.i5.p1.1.m1.1.1.2.cmml">
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.2" xref="S2.I1.i5.p1.1.m1.1.1.2.2.cmml">
            T
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.3" xref="S2.I1.i5.p1.1.m1.1.1.2.3.cmml">
            h
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1a" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.4" xref="S2.I1.i5.p1.1.m1.1.1.2.4.cmml">
            e
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1b" lspace="0.500em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.5" xref="S2.I1.i5.p1.1.m1.1.1.2.5.cmml">
            c
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1c" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.6" xref="S2.I1.i5.p1.1.m1.1.1.2.6.cmml">
            a
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1d" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.7" xref="S2.I1.i5.p1.1.m1.1.1.2.7.cmml">
            t
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1e" lspace="0.500em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.8" xref="S2.I1.i5.p1.1.m1.1.1.2.8.cmml">
            s
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1f" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.9" xref="S2.I1.i5.p1.1.m1.1.1.2.9.cmml">
            a
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1g" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.10" xref="S2.I1.i5.p1.1.m1.1.1.2.10.cmml">
            t
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1h" lspace="0.500em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.11" xref="S2.I1.i5.p1.1.m1.1.1.2.11.cmml">
            o
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1i" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.12" xref="S2.I1.i5.p1.1.m1.1.1.2.12.cmml">
            n
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1j" lspace="0.500em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.13" xref="S2.I1.i5.p1.1.m1.1.1.2.13.cmml">
            t
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1k" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.14" xref="S2.I1.i5.p1.1.m1.1.1.2.14.cmml">
            h
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.2.1l" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.2.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.2.15" xref="S2.I1.i5.p1.1.m1.1.1.2.15.cmml">
            e
           </mi>
          </mrow>
          <mo id="S2.I1.i5.p1.1.m1.1.1.1" lspace="0.778em" stretchy="false" xref="S2.I1.i5.p1.1.m1.1.1.1.cmml">
           →
          </mo>
          <mrow id="S2.I1.i5.p1.1.m1.1.1.3" xref="S2.I1.i5.p1.1.m1.1.1.3.cmml">
           <mi id="S2.I1.i5.p1.1.m1.1.1.3.2" xref="S2.I1.i5.p1.1.m1.1.1.3.2.cmml">
            m
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.3.3" xref="S2.I1.i5.p1.1.m1.1.1.3.3.cmml">
            a
           </mi>
           <mo id="S2.I1.i5.p1.1.m1.1.1.3.1a" lspace="0em" rspace="0em" xref="S2.I1.i5.p1.1.m1.1.1.3.1.cmml">
            ​
           </mo>
           <mi id="S2.I1.i5.p1.1.m1.1.1.3.4" xref="S2.I1.i5.p1.1.m1.1.1.3.4.cmml">
            t
           </mi>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.I1.i5.p1.1.m1.1b">
          <apply id="S2.I1.i5.p1.1.m1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1">
           <ci id="S2.I1.i5.p1.1.m1.1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1.1">
            →
           </ci>
           <apply id="S2.I1.i5.p1.1.m1.1.1.2.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2">
            <times id="S2.I1.i5.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.1">
            </times>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.2">
             𝑇
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.3">
             ℎ
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.4.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.4">
             𝑒
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.5.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.5">
             𝑐
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.6.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.6">
             𝑎
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.7.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.7">
             𝑡
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.8.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.8">
             𝑠
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.9.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.9">
             𝑎
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.10.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.10">
             𝑡
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.11.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.11">
             𝑜
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.12.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.12">
             𝑛
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.13.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.13">
             𝑡
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.14.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.14">
             ℎ
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.2.15.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2.15">
             𝑒
            </ci>
           </apply>
           <apply id="S2.I1.i5.p1.1.m1.1.1.3.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3">
            <times id="S2.I1.i5.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.1">
            </times>
            <ci id="S2.I1.i5.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.2">
             𝑚
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.3">
             𝑎
            </ci>
            <ci id="S2.I1.i5.p1.1.m1.1.1.3.4.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.4">
             𝑡
            </ci>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.I1.i5.p1.1.m1.1c">
          The\ cat\ sat\ on\ the\ \rightarrow mat
         </annotation>
        </semantics>
       </math>
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">
     High sample efficiency.
    </span>
    This means that the observations from the world can be reused in multiple input-output pairs and there is very high sample efficiency due to such a self-supervised learning method able to reuse the same sections of text multiple times.
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    <span class="ltx_text ltx_font_bold" id="S2.p4.1.1">
     Iterative processing of semantic meaning.
    </span>
    Moreover, the Transformer architecture actually allows the embeddings of each token to be infleunced by the most similar and closest neighbours via self-attention (via a combination of token embeddings plus position embeddings), which allows for the input representation to be refined in an iterative fashion, solving the case of ambiguous inputs or polysemy (multiple meanings of the same word). Such a hierarchical structure is illustrated in Fig.
    <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2 Next token prediction for self-supervised learning ‣ An Approach to Solving the ARC Challenge">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S2.p5">
   <p class="ltx_p" id="S2.p5.1">
    <span class="ltx_text ltx_font_bold" id="S2.p5.1.1">
     Feedback connections.
    </span>
    I always believe that current deep learning methods suffer from lack of feedback connections to ground the lower levels of processing - it is widely known that in the brain neurons do not just exist in feedforward connections but also have a lot of feedback connections as well. However, recently, observing that increasing the size of Transformers was already sufficient to achieve better and better performance, such as in GPT3.5 and GPT4, I start to wonder if there is indeed a way for Transformers to ground the earlier layers’ processing in the later layers’ processing. I hypothesise that it is actually able to do some form of feedback grounding, because of the skip connections present between decoder blocks, as illustrated in Fig.
    <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Next token prediction for self-supervised learning ‣ An Approach to Solving the ARC Challenge">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    . The embeddings at the lower levels can actually be passed all the way to the later layers (largely unchanged except for LayerNormalisation, which affects all embeddings similarly), and can be processed in the same layer with potential grounding by the embeddings of the later layers. This is extremely powerful, and can actually ground the input processing with knowledge gained at the later part. For instance, in the text "The following did not happen: John went to the market and bought a bunch of eggs, vegetables and meat.", we are able to interpret the entire text in the opposite semantic meaning just because of the words "The following did not happen" at the beginning of the sentence. In fact, as will be discussed in the next section, this presence of skip connections may be the way prompting and grounding in earlier context is so effective in LLMs.
   </p>
  </div>
  <figure class="ltx_figure" id="S2.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S2.F2.g1" src="/html/2306.03553/assets/Pictures/abstraction.png" width="479"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    As we proceed up the decoder architectures of a Transformer, the embeddings are increasingly at a higher level from the input embeddings. I posit that this means that they will be processed at more and more abstract levels. For instance, if the initial input embeddings are just purely word level, the middle level embeddings would have taken into account neighbouring words and be more at the sentence level, while those at the top level would be more abstract and related to the concepts expressed by the text rather than the text itself. Image taken taken from:
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jalammar.github.io/illustrated-transformer/" target="_blank" title="">
     https://jalammar.github.io/illustrated-transformer/
    </a>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Alammar,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023b
     </a>
     )
    </cite>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S2.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="272" id="S2.F3.g1" src="/html/2306.03553/assets/Pictures/skip_connection.png" width="479"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    There are skip connections in the encoder and decoder blocks of the Transformer. I posit that this helps the earlier input embeddings to be influenced by later layers in the Transformer, as such there can be some grounding of the later layers embeddings in the earlier layers’ processing. Image taken taken from:
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jalammar.github.io/illustrated-transformer/" target="_blank" title="">
     https://jalammar.github.io/illustrated-transformer/
    </a>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Alammar,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023b
     </a>
     )
    </cite>
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Prompting and Zero-shot/Few-shot learning
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Given that LLMs are seemingly able to perform inference at multiple scales of abstraction (see earlier section), this opens an avenue of approaches whereby we can just tell the LLM what we want to do in natural language, and use it to ground the generation. Such an instruction-based method of conditioning generations has proven useful in multiple natural language tasks, as shown in the usage of LLMs flexibly by just an instruction to prompt the task in GLUE
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wang et al.,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2018
     </a>
     )
    </cite>
    and SuperGLUE
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wang et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2019
     </a>
     )
    </cite>
    benchmarks.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Zero-shot learning
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     LLMs are also able to do zero-shot learning very well. For instance, it is able to do zero-shot classification of new contexts simply by using semantic meaning of the tokens it has encountered during training:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.SS1.p1.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S3.SS1.p1.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1.1.1">
        "You are a classification model meant to classify the context of an input.
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS1.p1.1.1.2">
       <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1.2.1">
        Context A: In the garden
        <br class="ltx_break"/>
        Context B: In the hospital
        <br class="ltx_break"/>
        Context C: In the mountains
        <br class="ltx_break"/>
        Context D: In the sky
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS1.p1.1.1.3">
       <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1.3.1">
        Give the contexts for the following inputs:
        <br class="ltx_break"/>
        1. Wow, the clouds are so fluffy today
        <br class="ltx_break"/>
        2. The IV drip is running out, get a nurse
        <br class="ltx_break"/>
        3. The sheep on the pasture are so pretty
        <br class="ltx_break"/>
        4. Have you watered the flowers today?
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS1.p1.1.1.4">
       <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1.4.1">
        Return in the following form:
        <br class="ltx_break"/>
        Number: Context Letter"
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     ChatGPT (GPT3.5, May 3 2023 version) returns the following output, which are in general correct:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.SS1.p2.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S3.SS1.p2.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1.1.1">
        1. D: In the sky
        <br class="ltx_break"/>
        2. B: In the hospital
        <br class="ltx_break"/>
        3. C: In the mnountains
        <br class="ltx_break"/>
        4. A: In the garden
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Few-shot learning
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     LLMs are also able to do few-shot learning pretty reliably. For instance, it is able to do few-shot classification of odd and even numbers from just a few sample input and output pairs. In order for to generate consistently, it needs to be given the framework of what the task is about and the possible outputs to ground the generation. Here is the example prompt given:
     <br class="ltx_break"/>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.SS2.p2.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S3.SS2.p2.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.1.1">
        You are a classification machine meant to classify between output A and B.
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS2.p2.1.1.2">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.2.1">
        Input: 5
        <br class="ltx_break"/>
        Output: A
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS2.p2.1.1.3">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.3.1">
        Input: 7
        <br class="ltx_break"/>
        Output: A
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS2.p2.1.1.4">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.4.1">
        Input: 8
        <br class="ltx_break"/>
        Output: B
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS2.p2.1.1.5">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.5.1">
        Input: 10
        <br class="ltx_break"/>
        Output: B
        <br class="ltx_break"/>
       </span>
      </span>
      <span class="ltx_p" id="S3.SS2.p2.1.1.6">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1.6.1">
        Input: 13
        <br class="ltx_break"/>
        Output:
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     ChatGPT (GPT3.5, May 3 2023 version) returns the following output, which is correct:
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.SS2.p3.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S3.SS2.p3.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1.1.1">
        A
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     Hence, a trained LLM has shown that it can be equipped with the knowledge of a new task either through zero-shot description-based prompting, or few-shot example-based prompting, and can be the basis of a fast learning system that is adaptive to real-world inputs. Given the quick learning ability of LLMs via prompting, it is no wonder why prompt engineering quickly became very popular following the rise of larger LLMs.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Getting the LLM to reverse engineer the instruction
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    LLMs are actually capable of observing multiple input-output pairs and coming up with an instruction to derive the relation between them
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhou et al.,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2022
     </a>
     )
    </cite>
    . Furthermore, the Language annotated Abstraction and Reasoning Corpus (LARC) showed that 88% of the original ARC tasks can be represented in a text instruction where another human can solve it without needing the input-output examples
    <cite class="ltx_cite ltx_citemacro_citep">
     (Acquaviva et al.,
     <a class="ltx_ref" href="#bib.bib1" title="">
      2021
     </a>
     )
    </cite>
    . Another paper has also highlighted the efficiency of prompt-based instructions, as one prompt can be worth 100s of training examples on various classification tasks
    <cite class="ltx_cite ltx_citemacro_citep">
     (Scao &amp; Rush,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2021
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    The difficulty of the ARC challenge is that the machine (or human) needs to infer instructions based on limited examples. These instructions are usually difficult to deduce, as one needs to find the pattern with very few sample input-output pairs. However, once the instruction is deduced, it is very easily communicable to other humans using text. Hence, we reframe the ARC Challenge with the following steps:
   </p>
  </div>
  <div class="ltx_para" id="S4.p3">
   <ol class="ltx_enumerate" id="S4.I1">
    <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S4.I1.i1.p1">
      <p class="ltx_p" id="S4.I1.i1.p1.1">
       Deduce the input-output mapping rule using the LLM from the input-output examples
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S4.I1.i2.p1">
      <p class="ltx_p" id="S4.I1.i2.p1.1">
       Apply this rule to the test input to get the test output
      </p>
     </div>
    </li>
   </ol>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Chain of Thought
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    It is often difficult to do planning on complicated tasks which involve multiple steps. The ARC Challenge sometimes also involves multiple manipulations of the input image in order to derive the output. For this kind of problems, we can utilize approaches such as Chain of Thought (CoT) prompting
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei et al.,
     <a class="ltx_ref" href="#bib.bib23" title="">
      2022
     </a>
     )
    </cite>
    , which uses demonstrations of details like the steps for mathematical computation to train the language model. Moreover, we do not even need to provide the human-labelled detailed demonstration as shown in the CoT paper, but can get the LLM to generate its own thoughts. The "ReAct: Synergizing reasoning and acting in language models" paper shows how one way of prompting the LLM for it to generate detailed thoughts and act upon it
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yao et al.,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2022
     </a>
     )
    </cite>
    - using the Thought, Action, Observation framework.
   </p>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">
     Hierarchical Planning.
    </span>
    CoT is still a largely linear way to do planning, as it involves having the previous action or plan before generating the next one. More recently, LLMs have been utilised in a hierarchical fashion, whereby the first step involves coming up with the broad plan, and the second step is to come up with the details. This is utilised in HuggingGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shen et al.,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023
     </a>
     )
    </cite>
    and AutoGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Richards,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2023
     </a>
     )
    </cite>
    to generate an overall plan before breaking down into the detailed steps. This way of hierarchical planning was also used in the Generative Agents paper
    <cite class="ltx_cite ltx_citemacro_citep">
     (Park et al.,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     )
    </cite>
    to generate a detailed action plan for an agent’s day.
   </p>
  </div>
  <div class="ltx_para" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    This approach of hierarchical planning is actually quite similar to how humans think. We do not have a detailed plan of our day right at the beginning, but think in a broad way like doing work in the morning, lunch, meet friends in afternoon, home in the evening and so on. Then, when prompted
    <span class="ltx_text ltx_font_bold" id="S5.p3.1.1">
     why
    </span>
    do you want to do this, we go up a layer of abstraction to think about the goals of our lives. When prompted
    <span class="ltx_text ltx_font_bold" id="S5.p3.1.2">
     how
    </span>
    do you want to do this, we go down a layer of abstraction to think about the specifics of the various plans of our lives. Hence, explicitly prompting the LLM to come up with the broad plan, and then using the broad plan to ground the generation for the detailed plan is a promising approach. It also helps circumvents the problem of the LLM having limited planning abilities, as we can plan the broad steps first, which are usually much shorter than the entire sequence of detailed steps.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Grounding in Human Biases
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    The ARC Challenge is difficult for computers because there is a huge number of possibilities to interpret high-dimensional real-world data, but easy for humans because humans can curate the possibilities based on some innate biases, like that of the Gestalt principles
    <cite class="ltx_cite ltx_citemacro_citep">
     (Todorovic,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2008
     </a>
     )
    </cite>
    . In fact, without such innate biases, it can be difficult for anyone to learn quickly in the real world.
    <cite class="ltx_cite ltx_citemacro_cite">
     Vallortigara (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2021
     </a>
     )
    </cite>
    wrote a book, "Born Knowing", which highlights that chicks come born with plenty of innate biases like preference for animate objects, which could help them learn faster. Similarly, human newborns come with a preference for face-like objects to help with recognition of the mother. Some human behaviours like suckling are also innate, rather than learnt, to facilitate survival.
   </p>
  </div>
  <div class="ltx_para" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    Alas, we may not be born
    <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">
     tabular rasa
    </span>
    like what is done in AlphaZero
    <cite class="ltx_cite ltx_citemacro_citep">
     (Silver et al.,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2018
     </a>
     )
    </cite>
    . In experiments with AlphaZero, it takes weeks with a single GPU just to learn how to play well enough to win a human
    <cite class="ltx_cite ltx_citemacro_citep">
     (John &amp; Motani,
     <a class="ltx_ref" href="#bib.bib6" title="">
      2022
     </a>
     )
    </cite>
    in a 4-in-a-row Tic-Tac-Toe game in a 7x7 grid with an unplayable position. Simply changing the unplayable position was enough to cause AlphaZero to become weaker than humans, and extensive training of various random unplayable positions was required for it to learn. Hence, for generalisability, pursuing optimality in Reinforcement Learning from a clean slate like that in static games like Chess or Go may not be the way to go. Rather, we need to ground the possibilities of what we need to do or interpret perception with some innate bias or some past experience in order to learn fast and be generalisable.
   </p>
  </div>
  <div class="ltx_para" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    Since LLMs like GPT4 are not able to be trained to a new set of input-output due to constrains of API, we utilise prompting to instill the human biases required for the machine to reduce the possibilities of interpreting the input-output pairs of the ARC Challenge.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Naïve Method (Single Prompt)
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    Given that LLMs have proven effective at learning an arbitrary task just by prompting, we try to do a naïve method of getting it to solve ARC tasks just from a single prompt alone. This prompt should be as generalisable as possible and should not be fine-tuned to any single one task.
   </p>
  </div>
  <div class="ltx_para" id="S7.p2">
   <p class="ltx_p" id="S7.p2.1">
    Using the above ideas of grounding in human biases, CoT prompting and getting LLMs to come up with broad descriptions, then detailed steps, and then using the detailed steps to map from test input to test output, we come up with an example prompt for ARC as given below:
   </p>
  </div>
  <div class="ltx_para" id="S7.p3">
   <p class="ltx_p" id="S7.p3.6">
    <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S7.p3.6.6.6" style="width:426.8pt;">
     <span class="ltx_p" id="S7.p3.6.6.6.7">
      <span class="ltx_text ltx_font_italic" id="S7.p3.6.6.6.7.1">
       “You are given a series of inputs and output pairs.
       <br class="ltx_break"/>
       These are all in the form of a 2D array, representing a 2D grid, with values from 0-9.
       <br class="ltx_break"/>
       The values are not representative of any ordinal ranking.
       <br class="ltx_break"/>
       Input/output pairs may not reflect all possibilities, you are to infer the simplest possible relation making use of symmetry and invariance as much as possible.
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.6.6.6.8">
      <span class="ltx_text ltx_font_italic" id="S7.p3.6.6.6.8.1">
       The input can be something like:
       <br class="ltx_break"/>
       &gt; entire grid being the sandbox to manipulate
       <br class="ltx_break"/>
       &gt; using a part of the grid (individual squares or portions of the grid) to depict instructions of how to do the task. symmetry is important.
       <br class="ltx_break"/>
       &gt; using regions of similar value to depict area for answer of the task
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.6.6.6.9">
      <span class="ltx_text ltx_font_italic" id="S7.p3.6.6.6.9.1">
       The output can be something like:
       <br class="ltx_break"/>
       &gt; same output size as input after performing action
       <br class="ltx_break"/>
       &gt; output one of the fixed predetermined patterns used to classify the input image
       <br class="ltx_break"/>
       &gt; using output to show the ordering of objects, such as by size, height, width, position, value
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.1.1.1.1">
      <span class="ltx_text ltx_font_italic" id="S7.p3.1.1.1.1.1">
       Each of the input-output relation can be done with one or more actions chained together, which could be something like (not exhaustive):
       <br class="ltx_break"/>
       <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.1.1.1.1.1.m1.1">
        <semantics id="S7.p3.1.1.1.1.1.m1.1a">
         <mo id="S7.p3.1.1.1.1.1.m1.1.1" xref="S7.p3.1.1.1.1.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S7.p3.1.1.1.1.1.m1.1b">
          <minus id="S7.p3.1.1.1.1.1.m1.1.1.cmml" xref="S7.p3.1.1.1.1.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S7.p3.1.1.1.1.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       object view (defined as continuous squares connected horizontally, vertically and/or diagonally, separated by 0 values)
       <br class="ltx_break"/>
       &gt; objects can be of the same value, or different values combined together
       <br class="ltx_break"/>
       &gt; objects may be hidden beneath other objects
       <br class="ltx_break"/>
       &gt; rotating or shifting objects
       <br class="ltx_break"/>
       &gt; changing value of object
       <br class="ltx_break"/>
       &gt; objects can be manipulated and mapped to a different number of output squares
       <br class="ltx_break"/>
       &gt; different objects may be manipulated differently based on context
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.2.2.2.2">
      <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.2.2.2.2.m1.1">
       <semantics id="S7.p3.2.2.2.2.m1.1a">
        <mo id="S7.p3.2.2.2.2.m1.1.1" xref="S7.p3.2.2.2.2.m1.1.1.cmml">
         −
        </mo>
        <annotation-xml encoding="MathML-Content" id="S7.p3.2.2.2.2.m1.1b">
         <minus id="S7.p3.2.2.2.2.m1.1.1.cmml" xref="S7.p3.2.2.2.2.m1.1.1">
         </minus>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S7.p3.2.2.2.2.m1.1c">
         -
        </annotation>
       </semantics>
      </math>
      <span class="ltx_text ltx_font_italic" id="S7.p3.2.2.2.2.1">
       overall view
       <br class="ltx_break"/>
       &gt; rotation / reflection symmetry
       <br class="ltx_break"/>
       &gt; continuation of a pattern
       <br class="ltx_break"/>
       &gt; changing values
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.3.3.3.3">
      <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.3.3.3.3.m1.1">
       <semantics id="S7.p3.3.3.3.3.m1.1a">
        <mo id="S7.p3.3.3.3.3.m1.1.1" xref="S7.p3.3.3.3.3.m1.1.1.cmml">
         −
        </mo>
        <annotation-xml encoding="MathML-Content" id="S7.p3.3.3.3.3.m1.1b">
         <minus id="S7.p3.3.3.3.3.m1.1.1.cmml" xref="S7.p3.3.3.3.3.m1.1.1">
         </minus>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S7.p3.3.3.3.3.m1.1c">
         -
        </annotation>
       </semantics>
      </math>
      <span class="ltx_text ltx_font_italic" id="S7.p3.3.3.3.3.1">
       segment view
       <br class="ltx_break"/>
       &gt; combine two segments of the input into one single one based on a simple rule
       <br class="ltx_break"/>
       &gt; rule can be certain values are prioritized over others, or combination of values into new ones
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.6.6.6.6">
      <span class="ltx_text ltx_font_italic" id="S7.p3.6.6.6.6.3">
       Do the following:
       <br class="ltx_break"/>
       <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.4.4.4.4.1.m1.1">
        <semantics id="S7.p3.4.4.4.4.1.m1.1a">
         <mo id="S7.p3.4.4.4.4.1.m1.1.1" xref="S7.p3.4.4.4.4.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S7.p3.4.4.4.4.1.m1.1b">
          <minus id="S7.p3.4.4.4.4.1.m1.1.1.cmml" xref="S7.p3.4.4.4.4.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S7.p3.4.4.4.4.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       What is the broad description of the input/output relation that holds for all input/output pairs?
       <br class="ltx_break"/>
       <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.5.5.5.5.2.m2.1">
        <semantics id="S7.p3.5.5.5.5.2.m2.1a">
         <mo id="S7.p3.5.5.5.5.2.m2.1.1" xref="S7.p3.5.5.5.5.2.m2.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S7.p3.5.5.5.5.2.m2.1b">
          <minus id="S7.p3.5.5.5.5.2.m2.1.1.cmml" xref="S7.p3.5.5.5.5.2.m2.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S7.p3.5.5.5.5.2.m2.1c">
          -
         </annotation>
        </semantics>
       </math>
       What is the step by step description of the input/output relation that holds for all input/output pairs?
       <br class="ltx_break"/>
       <math alttext="-" class="ltx_Math" display="inline" id="S7.p3.6.6.6.6.3.m3.1">
        <semantics id="S7.p3.6.6.6.6.3.m3.1a">
         <mo id="S7.p3.6.6.6.6.3.m3.1.1" xref="S7.p3.6.6.6.6.3.m3.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S7.p3.6.6.6.6.3.m3.1b">
          <minus id="S7.p3.6.6.6.6.3.m3.1.1.cmml" xref="S7.p3.6.6.6.6.3.m3.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S7.p3.6.6.6.6.3.m3.1c">
          -
         </annotation>
        </semantics>
       </math>
       Apply this description to the test input and find out the answer ’to_be_filled’.”
       <br class="ltx_break"/>
       <br class="ltx_break"/>
      </span>
     </span>
     <span class="ltx_p" id="S7.p3.6.6.6.10">
      <span class="ltx_text ltx_font_italic" id="S7.p3.6.6.6.10.1">
       [Insert .json for task here with all the input-output pairs in json format, with the test output replaced by ’to_be_filled’]
      </span>
     </span>
    </span>
   </p>
  </div>
  <div class="ltx_para" id="S7.p4">
   <p class="ltx_p" id="S7.p4.1">
    The method to derive the json format is simply replacing the output section of the original json format from the ARC Challenge 2 dataset with ’to_be_filled’. The code to do so can be found here:
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tanchongmin/ARC-Challenge/blob/main/arc_challenge.ipynb" target="_blank" title="">
     https://github.com/tanchongmin/ARC-Challenge/blob/main/arc_challenge.ipynb
    </a>
   </p>
  </div>
  <section class="ltx_subsection" id="S7.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.1
    </span>
    Example: Public Evaluation Task 157 (66e6c45b.json)
   </h3>
   <div class="ltx_para" id="S7.SS1.p1">
    <p class="ltx_p" id="S7.SS1.p1.1">
     Here is the modified .json for this task without test output:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S7.SS1.p1.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S7.SS1.p1.1.1.1">
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.1">
        ’train’: [
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.2">
        ’input’: [[0, 0, 0, 0], [0, 3, 4, 0], [0, 7, 6, 0], [0, 0, 0, 0]], ’output’: [[3, 0, 0, 4], [0, 0, 0, 0], [0, 0, 0, 0], [7, 0, 0, 6]]
       </span>
       }
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.3">
        ,
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.4">
        ’input’: [[0, 0, 0, 0], [0, 5, 6, 0], [0, 8, 3, 0], [0, 0, 0, 0]], ’output’: [[5, 0, 0, 6], [0, 0, 0, 0], [0, 0, 0, 0], [8, 0, 0, 3]]
       </span>
       }
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.5">
        ], ’test’:
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.6">
        ’input’: [[0, 0, 0, 0], [0, 2, 3, 0], [0, 4, 9, 0], [0, 0, 0, 0]], ’output’: ’to_be_filled’
       </span>
       }}
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1.1.7">
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S7.SS1.p2">
    <p class="ltx_p" id="S7.SS1.p2.1">
     Here is the output for Public Evaluation Task 157 (66e6c45b.json) using GPT3.5:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S7.SS1.p2.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S7.SS1.p2.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1.1.1">
        Broad Description:
The input/output relation involves manipulating the input grid by applying a series of transformations based on certain rules and symmetries to produce the output grid.
       </span>
      </span>
      <span class="ltx_p" id="S7.SS1.p2.1.1.2">
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1.2.1">
        Step-by-Step Description:
       </span>
      </span>
      <span class="ltx_p" id="S7.SS1.p2.1.1.3">
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1.3.1">
        Identify any objects in the input grid based on contiguous squares with the same value.
Apply transformations to the identified objects based on their position, symmetry, and value. These transformations may include rotating, reflecting, shifting, or changing the value of the object.
Combine or split objects as needed based on certain rules or conditions.
Repeat steps 1-3 until the output grid is produced.
For this specific test input, we can apply the following transformations to obtain the output grid:
       </span>
      </span>
      <span class="ltx_p" id="S7.SS1.p2.1.1.4">
       <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1.4.1">
        Identify two objects with values 2 and 3 respectively.
Rotate the object with value 2 by 90 degrees counterclockwise and shift it to the right to fill the empty space.
Rotate the object with value 3 by 90 degrees clockwise and shift it to the left to fill the empty space.
Combine the two objects to form the output grid:
[[2, 0, 0, 3],
[0, 0, 0, 0],
[0, 0, 0, 0],
[4, 0, 0, 9]]
Therefore, the answer is [[2, 0, 0, 3], [0, 0, 0, 0], [0, 0, 0, 0], [4, 0, 0, 9]].
       </span>
      </span>
     </span>
    </p>
   </div>
   <figure class="ltx_figure" id="S7.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="432" id="S7.F4.g1" src="/html/2306.03553/assets/Pictures/157.png" width="299"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     The Test Output and GPT Output for Public Evaluation Task 157 (66e6c45b.json)
    </figcaption>
   </figure>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.2
    </span>
    Example: Public Evaluation Task 162 (68b67ca3.json)
   </h3>
   <div class="ltx_para" id="S7.SS2.p1">
    <p class="ltx_p" id="S7.SS2.p1.1">
     Here is the modified .json for this task without test output:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S7.SS2.p1.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S7.SS2.p1.1.1.1">
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.1">
        ’train’: [
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.2">
        ’input’: [[2, 0, 2, 0, 1, 0], [0, 0, 0, 0, 0, 0], [2, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], ’output’: [[2, 2, 1], [2, 1, 0], [3, 0, 0]]
       </span>
       }
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.3">
        ,
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.4">
        ’input’: [[3, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 8, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0]], ’output’: [[3, 0, 2], [8, 0, 8], [0, 1, 0]]
       </span>
       }
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.5">
        ,
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.6">
        ’input’: [[1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 6, 0], [0, 0, 0, 0, 0, 0]], ’output’: [[1, 0, 0], [0, 2, 0], [6, 0, 6]]
       </span>
       }
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.7">
        ], ’test’:
       </span>
       {
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.8">
        ’input’: [[3, 0, 3, 0, 4, 0], [0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0]], ’output’: ’to_be_filled’
       </span>
       }}
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1.1.9">
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S7.SS2.p2">
    <p class="ltx_p" id="S7.SS2.p2.1">
     Here is the output for Public Evaluation Task 162 (68b67ca3.json) using GPT4:
     <br class="ltx_break"/>
     <span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S7.SS2.p2.1.1" style="width:426.8pt;">
      <span class="ltx_p" id="S7.SS2.p2.1.1.1">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.1.1">
        Broad description of the input/output relation:
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.2">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.2.1">
        The input/output relation involves extracting non-zero elements from specific positions in the input grid and arranging them into a smaller 3x3 output grid, maintaining their relative positions.
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.3">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.3.1">
        Step by step description of the input/output relation:
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.4">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.4.1">
        Identify non-zero elements in the following positions of the input grid: (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), and (5,5).
Place these non-zero elements into the corresponding positions in a 3x3 output grid.
Applying this description to the test input:
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.5">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.5.1">
        Test input:
[[3, 0, 3, 0, 4, 0],
[0, 0, 0, 0, 0, 0],
[7, 0, 0, 0, 1, 0],
[0, 0, 0, 0, 0, 0],
[7, 0, 0, 0, 1, 0],
[0, 0, 0, 0, 0, 0]]
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.6">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.6.1">
        Following the step-by-step description, we extract the non-zero elements from the specified positions:
(1,1): 3, (1,3): 3, (1,5): 4, (3,1): 7, (3,3): 0, (3,5): 1, (5,1): 7, (5,3): 0, (5,5): 1
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.7">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.7.1">
        Place these elements into the 3x3 output grid:
[[3, 3, 4],
[7, 0, 1],
[7, 0, 1]]
       </span>
      </span>
      <span class="ltx_p" id="S7.SS2.p2.1.1.8">
       <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.8.1">
        So, the output for the test input is:
[[3, 3, 4],
[7, 0, 1],
[7, 0, 1]]
       </span>
      </span>
     </span>
    </p>
   </div>
   <figure class="ltx_figure" id="S7.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="367" id="S7.F5.g1" src="/html/2306.03553/assets/Pictures/162.png" width="180"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     The Test Output and GPT Output for Public Evaluation Task 162 (68b67ca3.json)
    </figcaption>
   </figure>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.3
    </span>
    Evaluation
   </h3>
   <div class="ltx_para" id="S7.SS3.p1">
    <p class="ltx_p" id="S7.SS3.p1.1">
     This naïve approach has some success with the smaller ARC tasks. So far, with limited testing, this naïve method on GPT3.5 or GPT4 has solved the following tasks out of 4 tested tasks on the Evaluation set: 157 (66e6c45b.json), 162 (68b67ca3.json). These two tasks have failed, although only slightly and are likely to be solved with more specific prompt engineering: 158 (66f2d22f.json), 170 (6ea4a07e.json). See the testing of GPT4 on the ARC Challenge via this url:
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=vt2yG1da8Fg" target="_blank" title="">
      https://www.youtube.com/watch?v=vt2yG1da8Fg
     </a>
     . With some more fine-tuning of the actions that can be performed, I believe we can get it to work for more tasks. The key takeaway is that prompting can help to ground the model to think of feasible solutions it would otherwise not have.
    </p>
   </div>
   <div class="ltx_para" id="S7.SS3.p2">
    <p class="ltx_p" id="S7.SS3.p2.1">
     That said, the json input for the 2D array is not a great one to extract object-level relations, and the prompt needs to continuously ask for GPT4 to think of the input as an object. The prompt is intended to be very generic and gives the broad input-output relation, along with some tips as to how prior ARC puzzles can be solved. As GPT4 is not that great at doing detailed planning, we follow the hierarchical approach done by HuggingGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shen et al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      )
     </cite>
     or AutoGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Richards,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023
      </a>
      )
     </cite>
     , and ask the model to list out the broad description first. Thereafter, after being grounded by the broad description, the model then generates the detailed step by step description. This description is then used to get the answer by applying these steps to the test input.
    </p>
   </div>
   <div class="ltx_para" id="S7.SS3.p3">
    <p class="ltx_p" id="S7.SS3.p3.1">
     Initially, I tried to get GPT4 to output a Python program to handle the manipulation from input to output. While this could work for simple problems, in general, I find that the program output generated may be different from the intention in the step by step description, and in general, the step by step description in words was more accurate. As such, the example prompt above did not ask for a program output from GPT4.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S8">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    8
   </span>
   Improvements to the Naïve Method
  </h2>
  <div class="ltx_para" id="S8.p1">
   <p class="ltx_p" id="S8.p1.1">
    Following my experiments with the naïve method, I have identified the following issues:
   </p>
  </div>
  <div class="ltx_para" id="S8.p2">
   <ol class="ltx_enumerate" id="S8.I1">
    <li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S8.I1.i1.p1">
      <p class="ltx_p" id="S8.I1.i1.p1.1">
       Limited understanding of what an object is from the json file
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S8.I1.i2.p1">
      <p class="ltx_p" id="S8.I1.i2.p1.1">
       Limited context length to store json representations of large grids / multiple input-output samples
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S8.I1.i3.p1">
      <p class="ltx_p" id="S8.I1.i3.p1.1">
       Limited context length to store instructions
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S8.I1.i4.p1">
      <p class="ltx_p" id="S8.I1.i4.p1.1">
       Limited fact-checking abilities to determine if the input-output relation derived is correct
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S8.p3">
   <p class="ltx_p" id="S8.p3.1">
    These are the potential solutions to the above issues:
   </p>
  </div>
  <div class="ltx_para" id="S8.p4">
   <ol class="ltx_enumerate" id="S8.I2">
    <li class="ltx_item" id="S8.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S8.I2.i1.p1">
      <p class="ltx_p" id="S8.I2.i1.p1.1">
       In order to do the ARC challenge well, it would be good to imbue in the model a sense of what an object is, and also how images look like in the real world. This is because there are some ARC challenges which use concepts like object permanence, gravity, which would be present in real world situations but not for a computer which is only trained on pixels in the ARC challenge. As such, we could take a leaf from the Visual Question Answering (QA) domain
       <cite class="ltx_cite ltx_citemacro_citep">
        (Wu et al.,
        <a class="ltx_ref" href="#bib.bib24" title="">
         2017
        </a>
        )
       </cite>
       , and give the LLM the ability to ask questions about the input and output images and iteratively refine its input-output relation based on it. This Visual QA could be done with the base model as images in the wild, but should be fine-tuned with past ARC challenge data, as the distribution of pixel information between the real world and ARC challenge dataset may be different, though the concepts may be the same. My hypothesis is that pixel-based representation may be too high-dimensional to model the world, hence, being able to compress it down to low-dimensional text via Visual QA would be a huge plus for interpretability.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I2.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S8.I2.i2.p1">
      <p class="ltx_p" id="S8.I2.i2.p1.1">
       Instead of putting all the input-output examples in the same json, we can separately ask the model to give a description for each of the input-output pairs. Then, we can prompt another model to find similarities between each of the descriptions of these input-output pairs and collate to a general input-output representation.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I2.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S8.I2.i3.p1">
      <p class="ltx_p" id="S8.I2.i3.p1.1">
       Instead of having only one GPT model to give the prompt for the instructions, we could split the prompt into multiple parts. For instance, the object view can be one model, the overall view can be another model, the segment view can be another model, and so on. This would mean that we can ground the instructions in more fine-grained action space that would increase the likelihood of solving the ARC challenges. We can then select the best performing instruction, by asking the various models to come up with different sets of input-output instructions, and collating them into one pool of potential instructions. Then, we can evaluate all of them and use the best one.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S8.I2.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S8.I2.i4.p1">
      <p class="ltx_p" id="S8.I2.i4.p1.1">
       We could have a separate GPT model to evaluate the input-output mapping. This model takes in the pool of potential instructions generated by the above steps, and evaluates them one by one. The moment any of the instructions fails to generate the input-output map of the training cases, it is discarded. This approach of generating more potential mappings and discarding them based on grounding by the training set is used in AlphaCode, where they generate multiple programs by simply changing the hyperparameters or by using more random generations of the LLM, and then eliminate those non-performant ones that do not give the right output in the training cases
       <cite class="ltx_cite ltx_citemacro_citep">
        (Li et al.,
        <a class="ltx_ref" href="#bib.bib9" title="">
         2022
        </a>
        )
       </cite>
       (See Fig.
       <a class="ltx_ref" href="#S8.F6" title="Figure 6 ‣ 8 Improvements to the Naïve Method ‣ An Approach to Solving the ARC Challenge">
        <span class="ltx_text ltx_ref_tag">
         6
        </span>
       </a>
       for an illustration). Currently, I envision this model to take in just the instruction and the input json, and output the json after the instruction and check that it matches with the actual output json. An alternative is to ask GPT4 to come up with the Python code to do the input-output mapping, and then run the code to check for correct output - I suspect this may be inferior due to problems mapping to the right Python program from the instruction.
      </p>
     </div>
    </li>
   </ol>
  </div>
  <figure class="ltx_figure" id="S8.F6">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="S8.F6.g1" src="/html/2306.03553/assets/Pictures/alphacode.png" width="479"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 6:
    </span>
    Large-scale sampling and filtering in AlphaCode. Extracted from Fig. 4 in
    <cite class="ltx_cite ltx_citemacro_cite">
     Li et al. (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2022
     </a>
     )
    </cite>
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S9">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    9
   </span>
   GPT as a System
  </h2>
  <div class="ltx_para" id="S9.p1">
   <p class="ltx_p" id="S9.p1.1">
    With the recent trends of utilising multiple LLMs together as a system, such as in AutoGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Richards,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2023
     </a>
     )
    </cite>
    , it could potentially allow the model to scale better by off-loading various tasks to different LLM models, and letting all these models work together in a large ecosystem. Such a model is outlined in the Improvements section above, and more can be tuned in order to make the system as performant as possible.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S10">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    10
   </span>
   Memory as the way ahead
  </h2>
  <div class="ltx_para" id="S10.p1">
   <p class="ltx_p" id="S10.p1.1">
    Given that we are not able to train the weights of GPT to fit to the training set of the ARC challenge, using memory is the best way to go about imbuing the model with learnt knowledge. Humans learn very fast because we have memory to ground our current experiences and we can choose the best action based on what we have seen in the past. For example, if I see a snake on Path A, I will avoid Path A next time and choose Path B instead. This instantaneous way of learning is something that is not natural in deep learning, as it typically takes hundreds or thousands or more iterations in order to update the weights sufficiently so that it can learn well for Deep Learning, such as in Deep Reinforcement Learning. A more detailed explanation can be found in "Learning, Fast and Slow"
    <cite class="ltx_cite ltx_citemacro_citep">
     (John &amp; Motani,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S10.p2">
   <p class="ltx_p" id="S10.p2.1">
    Currently the naïve method does not use memory of what has been stored earlier. If we were to use memory, I posit the best way to use it will be via text descriptions of the broad and detailed input-output relations stored from the earlier training examples. This would make the memory more generic rather than storing memory of the images. We then have two memories of instructions, one I call
    <span class="ltx_text ltx_font_italic" id="S10.p2.1.1">
     BroadInstruct
    </span>
    , and the other
    <span class="ltx_text ltx_font_italic" id="S10.p2.1.2">
     DetailedInstruct
    </span>
    , which details the broad description and detailed steps of earlier instructions from earlier ARC tasks. I can envision a system using it to be as such:
   </p>
  </div>
  <div class="ltx_para" id="S10.p3">
   <ol class="ltx_enumerate" id="S10.I1">
    <li class="ltx_item" id="S10.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S10.I1.i1.p1">
      <p class="ltx_p" id="S10.I1.i1.p1.1">
       Use the naïve method to determine the broad description of the task
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S10.I1.i2.p1">
      <p class="ltx_p" id="S10.I1.i2.p1.2">
       From the broad description, retrieve from a database (e.g. Pinecone) using OpenAI Vector Embeddings
       <cite class="ltx_cite ltx_citemacro_citep">
        (Neelakantan et al.,
        <a class="ltx_ref" href="#bib.bib10" title="">
         2022
        </a>
        )
       </cite>
       or similar embeddings to retrieve the top
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I1.i2.p1.1.m1.1">
        <semantics id="S10.I1.i2.p1.1.m1.1a">
         <mi id="S10.I1.i2.p1.1.m1.1.1" xref="S10.I1.i2.p1.1.m1.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I1.i2.p1.1.m1.1b">
          <ci id="S10.I1.i2.p1.1.m1.1.1.cmml" xref="S10.I1.i2.p1.1.m1.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I1.i2.p1.1.m1.1c">
          k
         </annotation>
        </semantics>
       </math>
       neighbours from
       <span class="ltx_text ltx_font_italic" id="S10.I1.i2.p1.2.1">
        BroadInstruct
       </span>
       .
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I1.i2.p1.2.m2.1">
        <semantics id="S10.I1.i2.p1.2.m2.1a">
         <mi id="S10.I1.i2.p1.2.m2.1.1" xref="S10.I1.i2.p1.2.m2.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I1.i2.p1.2.m2.1b">
          <ci id="S10.I1.i2.p1.2.m2.1.1.cmml" xref="S10.I1.i2.p1.2.m2.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I1.i2.p1.2.m2.1c">
          k
         </annotation>
        </semantics>
       </math>
       is a hyperparameter that can be tuned, and can be set to 5 by default.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S10.I1.i3.p1">
      <p class="ltx_p" id="S10.I1.i3.p1.1">
       Conditioned on the top
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I1.i3.p1.1.m1.1">
        <semantics id="S10.I1.i3.p1.1.m1.1a">
         <mi id="S10.I1.i3.p1.1.m1.1.1" xref="S10.I1.i3.p1.1.m1.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I1.i3.p1.1.m1.1b">
          <ci id="S10.I1.i3.p1.1.m1.1.1.cmml" xref="S10.I1.i3.p1.1.m1.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I1.i3.p1.1.m1.1c">
          k
         </annotation>
        </semantics>
       </math>
       neighbours as context, perform retrieval-augmented generation
       <cite class="ltx_cite ltx_citemacro_citep">
        (Lewis et al.,
        <a class="ltx_ref" href="#bib.bib8" title="">
         2020
        </a>
        )
       </cite>
       to generate the refined broad description of the task
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S10.I1.i4.p1">
      <p class="ltx_p" id="S10.I1.i4.p1.1">
       Repeat the earlier steps until convergence
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S10.p4">
   <p class="ltx_p" id="S10.p4.1">
    Now, having generated the broad description of the task, we move on to generate the detailed steps.
   </p>
   <ol class="ltx_enumerate" id="S10.I2">
    <li class="ltx_item" id="S10.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S10.I2.i1.p1">
      <p class="ltx_p" id="S10.I2.i1.p1.1">
       Use the naïve method with the broad descrption as context to determine the detailed steps of the task
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I2.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S10.I2.i2.p1">
      <p class="ltx_p" id="S10.I2.i2.p1.2">
       From the generated detailed steps, retrieve from a database (e.g. Pinecone) using OpenAI Vector Embeddings
       <cite class="ltx_cite ltx_citemacro_citep">
        (Neelakantan et al.,
        <a class="ltx_ref" href="#bib.bib10" title="">
         2022
        </a>
        )
       </cite>
       or similar embeddings to retrieve the top
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I2.i2.p1.1.m1.1">
        <semantics id="S10.I2.i2.p1.1.m1.1a">
         <mi id="S10.I2.i2.p1.1.m1.1.1" xref="S10.I2.i2.p1.1.m1.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I2.i2.p1.1.m1.1b">
          <ci id="S10.I2.i2.p1.1.m1.1.1.cmml" xref="S10.I2.i2.p1.1.m1.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I2.i2.p1.1.m1.1c">
          k
         </annotation>
        </semantics>
       </math>
       neighbours from
       <span class="ltx_text ltx_font_italic" id="S10.I2.i2.p1.2.1">
        DetailedInstruct
       </span>
       .
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I2.i2.p1.2.m2.1">
        <semantics id="S10.I2.i2.p1.2.m2.1a">
         <mi id="S10.I2.i2.p1.2.m2.1.1" xref="S10.I2.i2.p1.2.m2.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I2.i2.p1.2.m2.1b">
          <ci id="S10.I2.i2.p1.2.m2.1.1.cmml" xref="S10.I2.i2.p1.2.m2.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I2.i2.p1.2.m2.1c">
          k
         </annotation>
        </semantics>
       </math>
       is a hyperparameter that can be tuned, and can be set to 5 by default.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I2.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S10.I2.i3.p1">
      <p class="ltx_p" id="S10.I2.i3.p1.1">
       Conditioned on the top
       <math alttext="k" class="ltx_Math" display="inline" id="S10.I2.i3.p1.1.m1.1">
        <semantics id="S10.I2.i3.p1.1.m1.1a">
         <mi id="S10.I2.i3.p1.1.m1.1.1" xref="S10.I2.i3.p1.1.m1.1.1.cmml">
          k
         </mi>
         <annotation-xml encoding="MathML-Content" id="S10.I2.i3.p1.1.m1.1b">
          <ci id="S10.I2.i3.p1.1.m1.1.1.cmml" xref="S10.I2.i3.p1.1.m1.1.1">
           𝑘
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S10.I2.i3.p1.1.m1.1c">
          k
         </annotation>
        </semantics>
       </math>
       neighbours as context, perform retrieval-augmented generation
       <cite class="ltx_cite ltx_citemacro_citep">
        (Lewis et al.,
        <a class="ltx_ref" href="#bib.bib8" title="">
         2020
        </a>
        )
       </cite>
       to generate the refined detailed steps of the task
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S10.I2.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S10.I2.i4.p1">
      <p class="ltx_p" id="S10.I2.i4.p1.1">
       Repeat the earlier steps until convergence
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S10.p5">
   <p class="ltx_p" id="S10.p5.1">
    Hence, we can utilise past knowledge of earlier ARC tasks for more accurate conditioning of the broad description and detailed steps needed for future ARC tasks. If the task is solved, we can then add in this broad and detailed description into
    <span class="ltx_text ltx_font_italic" id="S10.p5.1.1">
     BroadInstruct
    </span>
    and
    <span class="ltx_text ltx_font_italic" id="S10.p5.1.2">
     DetailedInstruct
    </span>
    respectively.
   </p>
  </div>
  <div class="ltx_para" id="S10.p6">
   <p class="ltx_p" id="S10.p6.1">
    Apart from imbuing learning ability, retrieval-augmented generation has an added benefit of increasing the consistency of the LLM-generated output, as it is more in line with what is required, which may be helpful with getting the right solution in fewer generations. For more complicated problems (more complex than ARC challenge), in order to constrain memory storage given a limited storage space, we can also selectively store memory based on how surprising it is and how "emotional" the experience is. These can be explored in future challenges where there is too much perceptual information and memory storage is a constraint, but for ARC, I believe we can just keep all the memory as the number of ARC tasks are not large.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S11">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    11
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S11.p1">
   <p class="ltx_p" id="S11.p1.1">
    Overall, the ARC challenge is a very unique one, and can serve to pave the way for systems that are fast learning and can generalise well to arbitrary tasks. With the right innate biases via prompting, the right hierarchical structure to condition generation of detailed steps from broad description, a multi-agent architecture to split long prompts up into performant smaller sub-systems, a better way to interpret images using Visual QA, as well as better learning and grounding in past memory, I posit that GPT4 can eventually be made to solve the majority of the ARC tasks.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Acquaviva et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Sam Acquaviva, Yewen Pu, Maxwell Nye, Catherine Wong, Michael Henry Tessler,
and Josh Tenenbaum.
    </span>
    <span class="ltx_bibblock">
     Larc: Language annotated abstraction and reasoning corpus.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Proceedings of the Annual Meeting of the Cognitive Science
Society
     </em>
     , volume 43, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alammar (2023a)
    </span>
    <span class="ltx_bibblock">
     Jay Alammar.
    </span>
    <span class="ltx_bibblock">
     The illustrated gpt-2 (visualizing transformer language models),
2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" title="">
      https://jalammar.github.io/illustrated-gpt2/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alammar (2023b)
    </span>
    <span class="ltx_bibblock">
     Jay Alammar.
    </span>
    <span class="ltx_bibblock">
     The illustrated transformer, 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jalammar.github.io/illustrated-transformer/" target="_blank" title="">
      https://jalammar.github.io/illustrated-transformer/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
et al.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with
gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2303.12712
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deng (2012)
    </span>
    <span class="ltx_bibblock">
     Li Deng.
    </span>
    <span class="ltx_bibblock">
     The mnist database of handwritten digit images for machine learning
research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      IEEE Signal Processing Magazine
     </em>
     , 29(6):141–142, 2012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     John &amp; Motani (2022)
    </span>
    <span class="ltx_bibblock">
     Tan Chong Min John and Mehul Motani.
    </span>
    <span class="ltx_bibblock">
     Brick tic-tac-toe: Exploring the generalizability of alphazero to
novel test environments.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2207.05991
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     John &amp; Motani (2023)
    </span>
    <span class="ltx_bibblock">
     Tan Chong Min John and Mehul Motani.
    </span>
    <span class="ltx_bibblock">
     Learning, fast and slow: A goal-directed memory-based approach for
dynamic environments.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2301.13758
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewis et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al.
    </span>
    <span class="ltx_bibblock">
     Retrieval-augmented generation for knowledge-intensive nlp tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Advances in Neural Information Processing Systems
     </em>
     ,
33:9459–9474, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,
et al.
    </span>
    <span class="ltx_bibblock">
     Competition-level code generation with alphacode.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Science
     </em>
     , 378(6624):1092–1097, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Neelakantan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry
Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.
    </span>
    <span class="ltx_bibblock">
     Text and code embeddings by contrastive pre-training.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2201.10005
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ouyang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Advances in Neural Information Processing Systems
     </em>
     ,
35:27730–27744, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy
Liang, and Michael S Bernstein.
    </span>
    <span class="ltx_bibblock">
     Generative agents: Interactive simulacra of human behavior.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2304.03442
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are unsupervised multitask learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      OpenAI blog
     </em>
     , 1(8):9, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Richards (2023)
    </span>
    <span class="ltx_bibblock">
     Toran Bruce Richards.
    </span>
    <span class="ltx_bibblock">
     Auto-gpt, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank" title="">
      https://github.com/Significant-Gravitas/Auto-GPT
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Scao &amp; Rush (2021)
    </span>
    <span class="ltx_bibblock">
     Teven Le Scao and Alexander M Rush.
    </span>
    <span class="ltx_bibblock">
     How many data points is a prompt worth?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2103.08493
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      arXiv preprint arXiv:2303.17580
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shibata et al. (1999)
    </span>
    <span class="ltx_bibblock">
     Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi
Shinohara, Takeshi Shinohara, and Setsuo Arikawa.
    </span>
    <span class="ltx_bibblock">
     Byte pair encoding: A text compression scheme that accelerates
pattern matching.
    </span>
    <span class="ltx_bibblock">
     1999.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Silver et al. (2018)
    </span>
    <span class="ltx_bibblock">
     David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
Graepel, et al.
    </span>
    <span class="ltx_bibblock">
     A general reinforcement learning algorithm that masters chess, shogi,
and go through self-play.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Science
     </em>
     , 362(6419):1140–1144, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Todorovic (2008)
    </span>
    <span class="ltx_bibblock">
     Dejan Todorovic.
    </span>
    <span class="ltx_bibblock">
     Gestalt principles.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Scholarpedia
     </em>
     , 3(12):5345, 2008.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vallortigara (2021)
    </span>
    <span class="ltx_bibblock">
     Giorgio Vallortigara.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Born knowing: Imprinting and the origins of knowledge
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     MIT press, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R
Bowman.
    </span>
    <span class="ltx_bibblock">
     Glue: A multi-task benchmark and analysis platform for natural
language understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:1804.07461
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.
    </span>
    <span class="ltx_bibblock">
     Superglue: A stickier benchmark for general-purpose language
understanding systems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Advances in neural information processing systems
     </em>
     , 32, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and
Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Chain of thought prompting elicits reasoning in large language
models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2201.11903
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van
Den Hengel.
    </span>
    <span class="ltx_bibblock">
     Visual question answering: A survey of methods and datasets.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Computer Vision and Image Understanding
     </em>
     , 163:21–40,
2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2210.03629
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
Harris Chan, and Jimmy Ba.
    </span>
    <span class="ltx_bibblock">
     Large language models are human-level prompt engineers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      arXiv preprint arXiv:2211.01910
     </em>
     , 2022.
    </span>
   </li>
  </ul>
 </section>
</article>
