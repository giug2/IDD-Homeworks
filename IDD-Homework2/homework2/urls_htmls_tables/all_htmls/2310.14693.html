<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.14693] Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS.</title><meta property="og:description" content="Federated Learning (FL) is a promising distributed method for edge-level machine learning, particularly for privacy-sensitive applications such as those in military and medical domains, where client data cannot be sharâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.14693">

<!--Generated on Tue Feb 27 21:18:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Compression,  Federated Learning,  Embedded Systems
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated learning compression designed for lightweight communications
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>This work is supported by the <span id="id1.id1.1" class="ltx_text ltx_font_italic">Futur et Ruptures</span> program funded by IMT and Institut Carnot TSN, and by the GdR ISIS.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Lucas Grativol1,2,
Mathieu LÃ©onardon1,
Guillaume Muller3,
Virginie Fresse2 and
Matthieu Arzel1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1IMT Atlantique, Lab-STICC, UMR CNRS 6285, F-29238 Brest, France
</span>
<span class="ltx_contact ltx_role_affiliation">2Hubert Curien Laboratory, Saint-Etienne, France
</span>
<span class="ltx_contact ltx_role_affiliation">3Mines Saint-Etienne, Institut Henri Fayol, Saint-Etienne, France
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated Learning (FL) is a promising distributed method for edge-level machine learning, particularly for privacy-sensitive applications such as those in military and medical domains, where client data cannot be shared or transferred to a cloud computing server. In many use-cases, communication cost is a major challenge in FL due to its natural intensive network usage. Client devices, such as smartphones or Internet of Things (IoT) nodes, have limited resources in terms of energy, computation, and memory. To address these hardware constraints, lightweight models and compression techniques such as pruning and quantization are commonly adopted in centralised paradigms. In this paper, we investigate the impact of compression techniques on FL for a typical image classification task. Going further, we demonstrate that a straightforward method can compresses messages up to 50% while having less than 1% of accuracy loss, competing with state-of-the-art techniques.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Compression, Federated Learning, Embedded Systems

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The development of approaches for training machine learning models while preserving data privacy has long been a goal. In traditional machine learning, embedded systems send their raw data over a network to a powerful server, which then trains the model and sends it back. However, this process raises confidentiality issues, such as data interception during communication and unauthorised access to user data by the server owner or a third party. In standard Federated Learning (FL), the server sends a model to a group of clients, who train it on their local data and then send their updated parameters back to the server for aggregation. By reversing the training process in this way, FL attempts to better guarantee the confidentiality of user data, since data never leaves a client device. An overview of the process can be seen in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">These embedded devices such as IoT devices, smartphones and drones are well suited to FL applications due to their proximity to real-world data and applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, many of these devices have limited computational resources and co-design techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are continually being explored to match algorithms to hardware constraints. Among the emerging research topics for FL at the edge/device level, the field of neural network compression is a promising way to tackle the constraints of devices exploiting FLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In addition to message compression, the FL domain encompasses important ongoing research efforts. These include addressing challenges related to client heterogeneity in terms of both data and hardwareÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, ensuring secure aggregation against attacksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and increasing clientâ€™s privacyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. While our work primarily focuses on reducing message sizes for energy and bandwidth reductions, we emphasize the importance of seamless integration with other ongoing research in FL. When compared to previous approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, we propose a simpler and more effective solution that not only reduces message sizes but also ensures the possibility to be combined with other techniques without compromising accuracy. Our code is publicly accessibleÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/lgrativol/fl_exps" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lgrativol/fl_exps</a></span></span></span>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.14693/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="187" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The pipeline of our study. We propose a simple way to insert the pruning technique as extra step before communicating training results.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Overview of Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Federated Learning (FL)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is a distributed framework that enables collaborative training of machine learning (ML) models on multiple devices, called clients, via a central coordinator, usually a server with large compute resources. Clients are commonly embedded devices, such as smartphones. Differing from traditional ML, each client trains its own local model and shares only the local training results, like model parameters or gradients, with the server. Through this mechanism, multiple clients can jointly contribute to train a global model without sharing their data. In each federated training cycle, commonly referred to as a â€™round,â€™ the server distributes the current model to a subset of clients, who perform local training on the model and subsequently send back the updated results. The final step involves aggregating, on the server-side, clientâ€™s contribution to create a global model, which ideally can represent the knowledge from each client. Each round involves downloading the model and several training iterations at client level.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Model Compression</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Model compression is a widely adopted solutionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to reduce the computational and memory requirements of a model. Among existing compression techniques, quantization and pruning have been implemented to reduce the complexity of inference and training of neural networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Pruning aims to reduce the complexity of a model by removing redundant or unnecessary parts of an architecture. Very wide and deep models tend to yield good results, but the contribution of each of its elements to the performance of the whole network is not homogeneous. So, by observing each architectural element of the network, it is possible to eliminate those that have little impact. There are two possible approaches to pruning in the context of neural networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The first is to replace the value of certain weights with zeros, which is commonly referred to as unstructured pruning. The second approach consists of pruning entire structures within the network, such as kernels, filters or layers, which do not contribute significantly to the networkâ€™s performance. This approach is known as structured pruning. On the other hand, unstructured pruning can also offer compression benefits for FL through the use of entropy coding techniques, such as HuffmanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> coding, by exploiting sparse parameters. So far works in the literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have used pruning to reduce communication cost by an order of 4.5 times. This is a significant consideration since typical FL clients often encompass low-power devices and operate in challenging transmission environments, such as long-distance or underwater communications.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Another widely applied technique is quantization, neural network models are generally constructed using 32-bit floating point numbers (FP32), which are more expensive in terms of computation, memory and energy than integersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In centralized machine learning is well-known that full-precision, FP32, itâ€™s not a necessary condition to obtain close to state-of-the-art results for inference and trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">ZeroFLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is a recent work that seeks to reduce simultaneously communication and training costs with a double optimization scheme to FL. First, a sparse training method named SWATÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and second, a layer-wise pruning based on weight importance. However as shown inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> communications cost can be much higher than the training cost. What should be done in the case where the focus is solely on communication costs ?</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Magnitude Pruning for Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We address the invoked problem inÂ <a href="#S2" title="II Background â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> by proposing a distributed non-structured pruning method. Unlike previous works, our objective is to demonstrate that the conventional FL framework can be modified to support sparse messages. This method results in a compression of approximately 50% of the original size while preserving accuracy with less than a 1% loss. Our implementation is streamlined and easily extendable, making it compatible with more advanced FL algorithms.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Starting from the standard FL pipeline, we introduced pruning as a way to sparsify messages, server to client and vice-versa. Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, both server and clients perform a non-structured magnitude pruning just before transmitting a message. This pruning method is based on pruning the absolute value of the global weights following a predetermined pruning rate. Accordingly, the <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\theta\%" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">Î¸</mi><mo id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1">percent</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\theta\%</annotation></semantics></math> smaller weights are substituted with zeros, thereby pruned. Consequently, both the server and clients attain an equivalent level of sparsity in the message throughout each round.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">At first, we conducted experiments to study the behaviour of our method while taking into account the impact on message compression. We applied different levels of pruning to detect trade-offs between compression and FL training mechanisms according to the experimental setup illustrated in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Building upon the results of our experiment, we extended our study to include a comparison with a recently published paper to showcase the viability of our approach.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Exploring Magnitude Pruning</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">To explore the impacts of our technique in FL, we simulated an image classification task, on the CIFAR-10 dataset, using a ResNet-12 with 780K parameters and 2.97 MB. We used the FlowerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> framework to simulate 10 FL clients. At each round, 40% of the clients are selected for the training process, and 100 rounds were performed to study the evolution of the server model accuracy. Each client used SGD with momentum as the optimizer. For simplicity, we use the same hyperparameters as Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, also replacing the batch-norm layer by a group-norm layer. The server uses FedAvg as the aggregation strategy.
Training examples are distributed across clients with a Latent Dirichlet Allocation (LDA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> on the original training set. The LDA partition is controlled by a distribution parameter, <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\alpha</annotation></semantics></math>. A smaller value of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\alpha</annotation></semantics></math> results in a more non-IID task, making it more challenging. We examined the behavior of our technique in a relatively IID (Independent and Identically Distributed) scenario with <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\alpha=100" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><eq id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></eq><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\alpha=100</annotation></semantics></math>. In this setting, clients possess examples of all the classes.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.14693/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="191" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Pruning effect on the accuracy in function of the pruning rate, where the rate represents the % of total parameters pruned, for 1 and 10 clients epochs.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As noted in previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> the number of local iterations performed by clients during training can have an important impact on model aggregation. For so, we decided to investigate this behaviour in the presence of model compression. The results on Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV-A Exploring Magnitude Pruning â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show that spending more time on each client contributes to a more robust model, allowing sparser data communications while retaining approximately the same accuracy, even though this approach also results in a higher total number of local iterations.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To further investigate and evaluate the feasibility of our method in a non-IID scenario, we adopted the same test case as ZeroFL. The model is a ResNet-18 with 11M trainable parameters, occupying 44.7 MB. The FL scenario simulates 100 clients with 10% participation rate, for only 1 local epoch and with <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha=0.1" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ğ›¼</ci><cn type="float" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\alpha=0.1</annotation></semantics></math>, where clients donâ€™t have access to all classes and the number of examples is randomly distributed. TableÂ <a href="#S4.T1" title="TABLE I â€£ IV-A Exploring Magnitude Pruning â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> presents the model evaluation results. The reported results are the means of three separate runs, with different seeds applied to generate distinct distributions of clientsâ€™ data. Unless otherwise stated, the size of the models is reported after being compressed using a ZIP algorithm.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.17.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.18.2" class="ltx_text" style="font-size:90%;">Comparation to ZeroFL </span></figcaption>
<table id="S4.T1.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.15.16.1" class="ltx_tr">
<td id="S4.T1.15.16.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S4.T1.15.16.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Compression</td>
<td id="S4.T1.15.16.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T1.15.16.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T1.15.16.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.15.16.1.4.1.1" class="ltx_tr">
<td id="S4.T1.15.16.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Message</td>
</tr>
<tr id="S4.T1.15.16.1.4.1.2" class="ltx_tr">
<td id="S4.T1.15.16.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Size (MB)</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.1.1.2.1" class="ltx_text ltx_font_bold">ZeroFL</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.3.1" class="ltx_text ltx_font_bold">Full model</span></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="80.62\pm 0.72" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mn id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">80.62</mn><mo id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">0.72</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">80.62</cn><cn type="float" id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">0.72</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">80.62\pm 0.72</annotation></semantics></math></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.4.1" class="ltx_text ltx_font_bold">44.7</span></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;">
<table id="S4.T1.2.2.2.1" class="ltx_tabular ltx_align_middle" style="background-color:#FDC086;">
<tr id="S4.T1.2.2.2.1.1" class="ltx_tr">
<td id="S4.T1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">90 % SP +</td>
</tr>
<tr id="S4.T1.2.2.2.1.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">0.2 Mask Ratio</td>
</tr>
</table>
</td>
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" style="background-color:#FDC086;" alttext="81.04\pm 0.28" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mrow id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mn mathbackground="#FDC086" id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml">81.04</mn><mo mathbackground="#FDC086" id="S4.T1.2.2.1.m1.1.1.1" xref="S4.T1.2.2.1.m1.1.1.1.cmml">Â±</mo><mn mathbackground="#FDC086" id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml">0.28</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2">81.04</cn><cn type="float" id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3">0.28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">81.04\pm 0.28</annotation></semantics></math></td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;"><span id="S4.T1.2.2.3.1" class="ltx_text" style="background-color:#FDC086;">27.3</span></td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;">
<table id="S4.T1.3.3.2.1" class="ltx_tabular ltx_align_middle" style="background-color:#ABDDA4;">
<tr id="S4.T1.3.3.2.1.1" class="ltx_tr">
<td id="S4.T1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">90 % SP +</td>
</tr>
<tr id="S4.T1.3.3.2.1.2" class="ltx_tr">
<td id="S4.T1.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">0.0 Mask Ratio</td>
</tr>
</table>
</td>
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;"><math id="S4.T1.3.3.1.m1.1" class="ltx_Math" style="background-color:#ABDDA4;" alttext="73.87\pm 0.50" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mrow id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mn mathbackground="#ABDDA4" id="S4.T1.3.3.1.m1.1.1.2" xref="S4.T1.3.3.1.m1.1.1.2.cmml">73.87</mn><mo mathbackground="#ABDDA4" id="S4.T1.3.3.1.m1.1.1.1" xref="S4.T1.3.3.1.m1.1.1.1.cmml">Â±</mo><mn mathbackground="#ABDDA4" id="S4.T1.3.3.1.m1.1.1.3" xref="S4.T1.3.3.1.m1.1.1.3.cmml">0.50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2">73.87</cn><cn type="float" id="S4.T1.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3">0.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">73.87\pm 0.50</annotation></semantics></math></td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;"><span id="S4.T1.3.3.3.1" class="ltx_text" style="background-color:#ABDDA4;">10.1</span></td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T1.4.4.2.1" class="ltx_text">
<span id="S4.T1.4.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.4.4.2.1.1.1" class="ltx_tr">
<span id="S4.T1.4.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T1.4.4.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Global</span></span></span>
<span id="S4.T1.4.4.2.1.1.2" class="ltx_tr">
<span id="S4.T1.4.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T1.4.4.2.1.1.2.1.1" class="ltx_text ltx_font_bold">magnitude</span></span></span>
<span id="S4.T1.4.4.2.1.1.3" class="ltx_tr">
<span id="S4.T1.4.4.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T1.4.4.2.1.1.3.1.1" class="ltx_text ltx_font_bold">(Ours)</span></span></span>
</span></span></td>
<td id="S4.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.4.3.1" class="ltx_text ltx_font_bold">Full model</span></td>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="84.43\pm 0.36" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mrow id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml"><mn id="S4.T1.4.4.1.m1.1.1.2" xref="S4.T1.4.4.1.m1.1.1.2.cmml">84.43</mn><mo id="S4.T1.4.4.1.m1.1.1.1" xref="S4.T1.4.4.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.4.4.1.m1.1.1.3" xref="S4.T1.4.4.1.m1.1.1.3.cmml">0.36</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><apply id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.1.m1.1.1.2">84.43</cn><cn type="float" id="S4.T1.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.1.m1.1.1.3">0.36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">84.43\pm 0.36</annotation></semantics></math></td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.4.4.1" class="ltx_text ltx_font_bold">44.7</span></td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">10 % pruning rate</td>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="85.96\pm 0.37" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mrow id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml"><mn id="S4.T1.5.5.1.m1.1.1.2" xref="S4.T1.5.5.1.m1.1.1.2.cmml">85.96</mn><mo id="S4.T1.5.5.1.m1.1.1.1" xref="S4.T1.5.5.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.5.5.1.m1.1.1.3" xref="S4.T1.5.5.1.m1.1.1.3.cmml">0.37</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><apply id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2">85.96</cn><cn type="float" id="S4.T1.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.1.m1.1.1.3">0.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">85.96\pm 0.37</annotation></semantics></math></td>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">38.1</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">20 % pruning rate</td>
<td id="S4.T1.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.6.6.1.m1.1" class="ltx_Math" alttext="85.57\pm 0.19" display="inline"><semantics id="S4.T1.6.6.1.m1.1a"><mrow id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml"><mn id="S4.T1.6.6.1.m1.1.1.2" xref="S4.T1.6.6.1.m1.1.1.2.cmml">85.57</mn><mo id="S4.T1.6.6.1.m1.1.1.1" xref="S4.T1.6.6.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.6.6.1.m1.1.1.3" xref="S4.T1.6.6.1.m1.1.1.3.cmml">0.19</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><apply id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.6.6.1.m1.1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.6.6.1.m1.1.1.2.cmml" xref="S4.T1.6.6.1.m1.1.1.2">85.57</cn><cn type="float" id="S4.T1.6.6.1.m1.1.1.3.cmml" xref="S4.T1.6.6.1.m1.1.1.3">0.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">85.57\pm 0.19</annotation></semantics></math></td>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">34.8</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<td id="S4.T1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">30 % pruning rate</td>
<td id="S4.T1.7.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="85.03\pm 0.32" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mrow id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml"><mn id="S4.T1.7.7.1.m1.1.1.2" xref="S4.T1.7.7.1.m1.1.1.2.cmml">85.03</mn><mo id="S4.T1.7.7.1.m1.1.1.1" xref="S4.T1.7.7.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.7.7.1.m1.1.1.3" xref="S4.T1.7.7.1.m1.1.1.3.cmml">0.32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><apply id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.7.7.1.m1.1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.7.7.1.m1.1.1.2.cmml" xref="S4.T1.7.7.1.m1.1.1.2">85.03</cn><cn type="float" id="S4.T1.7.7.1.m1.1.1.3.cmml" xref="S4.T1.7.7.1.m1.1.1.3">0.32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">85.03\pm 0.32</annotation></semantics></math></td>
<td id="S4.T1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">31.1</td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;"><span id="S4.T1.8.8.2.1" class="ltx_text" style="background-color:#FDC086;">40 % pruning rate</span></td>
<td id="S4.T1.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;"><math id="S4.T1.8.8.1.m1.1" class="ltx_Math" style="background-color:#FDC086;" alttext="85.20\pm 0.20" display="inline"><semantics id="S4.T1.8.8.1.m1.1a"><mrow id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml"><mn mathbackground="#FDC086" id="S4.T1.8.8.1.m1.1.1.2" xref="S4.T1.8.8.1.m1.1.1.2.cmml">85.20</mn><mo mathbackground="#FDC086" id="S4.T1.8.8.1.m1.1.1.1" xref="S4.T1.8.8.1.m1.1.1.1.cmml">Â±</mo><mn mathbackground="#FDC086" id="S4.T1.8.8.1.m1.1.1.3" xref="S4.T1.8.8.1.m1.1.1.3.cmml">0.20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><apply id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.8.8.1.m1.1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.8.8.1.m1.1.1.2.cmml" xref="S4.T1.8.8.1.m1.1.1.2">85.20</cn><cn type="float" id="S4.T1.8.8.1.m1.1.1.3.cmml" xref="S4.T1.8.8.1.m1.1.1.3">0.20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">85.20\pm 0.20</annotation></semantics></math></td>
<td id="S4.T1.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FDC086;"><span id="S4.T1.8.8.3.1" class="ltx_text" style="background-color:#FDC086;">27.1</span></td>
</tr>
<tr id="S4.T1.9.9" class="ltx_tr">
<td id="S4.T1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">50 % pruning rate</td>
<td id="S4.T1.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.9.9.1.m1.1" class="ltx_Math" alttext="83.85\pm 0.65" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><mrow id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml"><mn id="S4.T1.9.9.1.m1.1.1.2" xref="S4.T1.9.9.1.m1.1.1.2.cmml">83.85</mn><mo id="S4.T1.9.9.1.m1.1.1.1" xref="S4.T1.9.9.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.9.9.1.m1.1.1.3" xref="S4.T1.9.9.1.m1.1.1.3.cmml">0.65</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><apply id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.9.9.1.m1.1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.9.9.1.m1.1.1.2.cmml" xref="S4.T1.9.9.1.m1.1.1.2">83.85</cn><cn type="float" id="S4.T1.9.9.1.m1.1.1.3.cmml" xref="S4.T1.9.9.1.m1.1.1.3">0.65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">83.85\pm 0.65</annotation></semantics></math></td>
<td id="S4.T1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">23.0</td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<td id="S4.T1.10.10.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">60 % pruning rate</td>
<td id="S4.T1.10.10.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.10.10.1.m1.1" class="ltx_Math" alttext="83.19\pm 0.44" display="inline"><semantics id="S4.T1.10.10.1.m1.1a"><mrow id="S4.T1.10.10.1.m1.1.1" xref="S4.T1.10.10.1.m1.1.1.cmml"><mn id="S4.T1.10.10.1.m1.1.1.2" xref="S4.T1.10.10.1.m1.1.1.2.cmml">83.19</mn><mo id="S4.T1.10.10.1.m1.1.1.1" xref="S4.T1.10.10.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.10.10.1.m1.1.1.3" xref="S4.T1.10.10.1.m1.1.1.3.cmml">0.44</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><apply id="S4.T1.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.10.10.1.m1.1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.10.10.1.m1.1.1.2.cmml" xref="S4.T1.10.10.1.m1.1.1.2">83.19</cn><cn type="float" id="S4.T1.10.10.1.m1.1.1.3.cmml" xref="S4.T1.10.10.1.m1.1.1.3">0.44</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">83.19\pm 0.44</annotation></semantics></math></td>
<td id="S4.T1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">18.9</td>
</tr>
<tr id="S4.T1.11.11" class="ltx_tr">
<td id="S4.T1.11.11.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">70 % pruning rate</td>
<td id="S4.T1.11.11.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.11.11.1.m1.1" class="ltx_Math" alttext="82.25\pm 0.63" display="inline"><semantics id="S4.T1.11.11.1.m1.1a"><mrow id="S4.T1.11.11.1.m1.1.1" xref="S4.T1.11.11.1.m1.1.1.cmml"><mn id="S4.T1.11.11.1.m1.1.1.2" xref="S4.T1.11.11.1.m1.1.1.2.cmml">82.25</mn><mo id="S4.T1.11.11.1.m1.1.1.1" xref="S4.T1.11.11.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.11.11.1.m1.1.1.3" xref="S4.T1.11.11.1.m1.1.1.3.cmml">0.63</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b"><apply id="S4.T1.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.11.11.1.m1.1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.11.11.1.m1.1.1.2.cmml" xref="S4.T1.11.11.1.m1.1.1.2">82.25</cn><cn type="float" id="S4.T1.11.11.1.m1.1.1.3.cmml" xref="S4.T1.11.11.1.m1.1.1.3">0.63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">82.25\pm 0.63</annotation></semantics></math></td>
<td id="S4.T1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">14.5</td>
</tr>
<tr id="S4.T1.12.12" class="ltx_tr">
<td id="S4.T1.12.12.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.12.12.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;"><span id="S4.T1.12.12.3.1" class="ltx_text" style="background-color:#ABDDA4;">80 % pruning rate</span></td>
<td id="S4.T1.12.12.1" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;"><math id="S4.T1.12.12.1.m1.1" class="ltx_Math" style="background-color:#ABDDA4;" alttext="80.70\pm 0.24" display="inline"><semantics id="S4.T1.12.12.1.m1.1a"><mrow id="S4.T1.12.12.1.m1.1.1" xref="S4.T1.12.12.1.m1.1.1.cmml"><mn mathbackground="#ABDDA4" id="S4.T1.12.12.1.m1.1.1.2" xref="S4.T1.12.12.1.m1.1.1.2.cmml">80.70</mn><mo mathbackground="#ABDDA4" id="S4.T1.12.12.1.m1.1.1.1" xref="S4.T1.12.12.1.m1.1.1.1.cmml">Â±</mo><mn mathbackground="#ABDDA4" id="S4.T1.12.12.1.m1.1.1.3" xref="S4.T1.12.12.1.m1.1.1.3.cmml">0.24</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.1.m1.1b"><apply id="S4.T1.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.12.12.1.m1.1.1.1.cmml" xref="S4.T1.12.12.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.12.12.1.m1.1.1.2.cmml" xref="S4.T1.12.12.1.m1.1.1.2">80.70</cn><cn type="float" id="S4.T1.12.12.1.m1.1.1.3.cmml" xref="S4.T1.12.12.1.m1.1.1.3">0.24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.1.m1.1c">80.70\pm 0.24</annotation></semantics></math></td>
<td id="S4.T1.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#ABDDA4;"><span id="S4.T1.12.12.4.1" class="ltx_text" style="background-color:#ABDDA4;">9.8</span></td>
</tr>
<tr id="S4.T1.13.13" class="ltx_tr">
<td id="S4.T1.13.13.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">90 % pruning rate</td>
<td id="S4.T1.13.13.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.13.13.1.m1.1" class="ltx_Math" alttext="76.77\pm 0.47" display="inline"><semantics id="S4.T1.13.13.1.m1.1a"><mrow id="S4.T1.13.13.1.m1.1.1" xref="S4.T1.13.13.1.m1.1.1.cmml"><mn id="S4.T1.13.13.1.m1.1.1.2" xref="S4.T1.13.13.1.m1.1.1.2.cmml">76.77</mn><mo id="S4.T1.13.13.1.m1.1.1.1" xref="S4.T1.13.13.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.13.13.1.m1.1.1.3" xref="S4.T1.13.13.1.m1.1.1.3.cmml">0.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><apply id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.13.13.1.m1.1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.13.13.1.m1.1.1.2.cmml" xref="S4.T1.13.13.1.m1.1.1.2">76.77</cn><cn type="float" id="S4.T1.13.13.1.m1.1.1.3.cmml" xref="S4.T1.13.13.1.m1.1.1.3">0.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">76.77\pm 0.47</annotation></semantics></math></td>
<td id="S4.T1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">4.9</td>
</tr>
<tr id="S4.T1.14.14" class="ltx_tr">
<td id="S4.T1.14.14.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.14.14.3" class="ltx_td ltx_align_center ltx_border_r">95 % pruning rate</td>
<td id="S4.T1.14.14.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.14.14.1.m1.1" class="ltx_Math" alttext="69.14\pm 0.85" display="inline"><semantics id="S4.T1.14.14.1.m1.1a"><mrow id="S4.T1.14.14.1.m1.1.1" xref="S4.T1.14.14.1.m1.1.1.cmml"><mn id="S4.T1.14.14.1.m1.1.1.2" xref="S4.T1.14.14.1.m1.1.1.2.cmml">69.14</mn><mo id="S4.T1.14.14.1.m1.1.1.1" xref="S4.T1.14.14.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.14.14.1.m1.1.1.3" xref="S4.T1.14.14.1.m1.1.1.3.cmml">0.85</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.1.m1.1b"><apply id="S4.T1.14.14.1.m1.1.1.cmml" xref="S4.T1.14.14.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.14.14.1.m1.1.1.1.cmml" xref="S4.T1.14.14.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.14.14.1.m1.1.1.2.cmml" xref="S4.T1.14.14.1.m1.1.1.2">69.14</cn><cn type="float" id="S4.T1.14.14.1.m1.1.1.3.cmml" xref="S4.T1.14.14.1.m1.1.1.3">0.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.1.m1.1c">69.14\pm 0.85</annotation></semantics></math></td>
<td id="S4.T1.14.14.4" class="ltx_td ltx_align_center ltx_border_r">2.5</td>
</tr>
<tr id="S4.T1.15.15" class="ltx_tr">
<td id="S4.T1.15.15.2" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="S4.T1.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">99 % pruning rate</td>
<td id="S4.T1.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S4.T1.15.15.1.m1.1" class="ltx_Math" alttext="0.10\pm 0.0" display="inline"><semantics id="S4.T1.15.15.1.m1.1a"><mrow id="S4.T1.15.15.1.m1.1.1" xref="S4.T1.15.15.1.m1.1.1.cmml"><mn id="S4.T1.15.15.1.m1.1.1.2" xref="S4.T1.15.15.1.m1.1.1.2.cmml">0.10</mn><mo id="S4.T1.15.15.1.m1.1.1.1" xref="S4.T1.15.15.1.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.15.15.1.m1.1.1.3" xref="S4.T1.15.15.1.m1.1.1.3.cmml">0.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.1.m1.1b"><apply id="S4.T1.15.15.1.m1.1.1.cmml" xref="S4.T1.15.15.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.15.15.1.m1.1.1.1.cmml" xref="S4.T1.15.15.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.15.15.1.m1.1.1.2.cmml" xref="S4.T1.15.15.1.m1.1.1.2">0.10</cn><cn type="float" id="S4.T1.15.15.1.m1.1.1.3.cmml" xref="S4.T1.15.15.1.m1.1.1.3">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.1.m1.1c">0.10\pm 0.0</annotation></semantics></math></td>
<td id="S4.T1.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.5</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">In TableÂ <a href="#S4.T1" title="TABLE I â€£ IV-A Exploring Magnitude Pruning â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we present a comparison with ZeroFLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Initially, without pruning, our baseline has a higher accuracy than ZeroFL and as far as we have understood, there are two main distinctions. Firstly, we do not employ SWAT for local training. Secondly, we use a batch size of 8, whereas ZeroFL does not indicate the specific batch size used. As previously observed in FLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the batch size is a crucial hyperparameter that influences the aggregation accuracy. Even though SWAT plays a significant role in reducing the communication cost, it also has an impact on the model accuracy, resulting in an overall hindrance. This effect can be noticed as our baseline, which uses pure FedAvg without any compression, already achieves higher accuracy, 4%, when compared to ZeroFL. We observe that for the same level of pruning, our approach exhibits proportionally less degradation. For instance, while ZeroFL experiences an 8% accuracy degradation to prune the model to 10 MB, we only experience a 4.63% degradation. As the results show, clientâ€™s flexibility to perform pruning on its own better compensates for the sparsity introduced. This compensation enables messages to be more sparse while resulting in a more robust global model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Compressing more with Quantization</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">From the message savings observed with the pruning experiments, one could ask if it is possible to have even smaller messages. As exposed in sectionÂ <a href="#S2.SS2" title="II-B Model Compression â€£ II Background â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> another well-known technique for compression is quantization. In FigÂ <a href="#S4.F3" title="Figure 3 â€£ IV-B Compressing more with Quantization â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show the impact of Quantization-Aware Training (QAT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in the IID scenario described before. During QAT, weights are still represented as floating-point numbers but are limited to power-of-two values. At each gradient update, the values are re-evaluated and scaled. The motivation behind using QAT is to incorporate quantization noise into the training procedure, allowing the network to learn from it. We chose to work with 1-bit, 4-bit and 8-bit quantization levels, using Binary ConnectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for binary networks and the BrevitasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> framework for 4- and 8-bit with the default quantization scheme. The weights are quantized to 4-bit and 8-bit integers and the QAT scaling is calculated per layer.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2310.14693/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="198" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Accuracy evolution comparison between baseline (32-bit FP), 1-bit, 4-bit and 8-bit, for 1 and 10 clients epochs.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Looking at Fig.Â <a href="#S4.F3" title="Figure 3 â€£ IV-B Compressing more with Quantization â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that the convergence time, i.e. the number of rounds needed to reach maximum accuracy, is not the same from one experiment to another, as it also depends on the level of quantization. In addition to the fact that the 4- and 8-bit format enables us to achieve an accuracy comparable to the reference, it also reveals a compromise between communication and computation. In order to achieve a similar accuracy of around 75%, Fig.Â <a href="#S4.F3" title="Figure 3 â€£ IV-B Compressing more with Quantization â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it is necessary to perform 40 rounds of communication and 40 total epochs when using 1 local epoch, while in the case of 10 local epochs, 100 total epochs are needed within 10 rounds. Still, in the case of one bit, increasing the number of epochs per round on the client from 1 to 10 considerably increases accuracy, from 48.8Â % to 70.9Â %, with the total number of epochs increasing from 100 to 1000, with the same communication cost. As seen in the IID pruning experiment in Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV-A Exploring Magnitude Pruning â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, spending more time on each client contributes to a more robust model to the perturbations introduced by the quantization.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Summary of message size and accuracy for the CIFAR-10 dataset for the IID case</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.1.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Compression</span></td>
</tr>
<tr id="S4.T2.4.1.1.1.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Technique</span></td>
</tr>
</table>
</td>
<td id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S4.T2.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.2.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S4.T2.4.1.1.2.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</td>
<td id="S4.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S4.T2.4.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.3.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Message Size</span></td>
</tr>
<tr id="S4.T2.4.1.1.3.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">(MB)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.4.2.2" class="ltx_tr">
<td id="S4.T2.4.2.2.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S4.T2.4.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.2.2.2.1.1" class="ltx_tr">
<td id="S4.T2.4.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1 Local</td>
</tr>
<tr id="S4.T2.4.2.2.2.1.2" class="ltx_tr">
<td id="S4.T2.4.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Epoch</td>
</tr>
</table>
</td>
<td id="S4.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S4.T2.4.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.2.2.3.1.1" class="ltx_tr">
<td id="S4.T2.4.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">10 Local</td>
</tr>
<tr id="S4.T2.4.2.2.3.1.2" class="ltx_tr">
<td id="S4.T2.4.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Epochs</td>
</tr>
</table>
</td>
<td id="S4.T2.4.2.2.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.4.3.3" class="ltx_tr">
<td id="S4.T2.4.3.3.1" class="ltx_td ltx_align_center ltx_border_t">Baseline</td>
<td id="S4.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">78.94</td>
<td id="S4.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">78.18</td>
<td id="S4.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_border_t">2.97</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t">Pruning</td>
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.4.5.5" class="ltx_tr">
<td id="S4.T2.4.5.5.1" class="ltx_td ltx_align_center ltx_border_t">10 %</td>
<td id="S4.T2.4.5.5.2" class="ltx_td ltx_align_center ltx_border_t">74.79</td>
<td id="S4.T2.4.5.5.3" class="ltx_td ltx_align_center ltx_border_t">78.18</td>
<td id="S4.T2.4.5.5.4" class="ltx_td ltx_align_center ltx_border_t">2.57</td>
</tr>
<tr id="S4.T2.4.6.6" class="ltx_tr">
<td id="S4.T2.4.6.6.1" class="ltx_td ltx_align_center">20 %</td>
<td id="S4.T2.4.6.6.2" class="ltx_td ltx_align_center">76.01</td>
<td id="S4.T2.4.6.6.3" class="ltx_td ltx_align_center">78.12</td>
<td id="S4.T2.4.6.6.4" class="ltx_td ltx_align_center">2.34</td>
</tr>
<tr id="S4.T2.4.7.7" class="ltx_tr">
<td id="S4.T2.4.7.7.1" class="ltx_td ltx_align_center">30 %</td>
<td id="S4.T2.4.7.7.2" class="ltx_td ltx_align_center">78.20</td>
<td id="S4.T2.4.7.7.3" class="ltx_td ltx_align_center">77.83</td>
<td id="S4.T2.4.7.7.4" class="ltx_td ltx_align_center">2.10</td>
</tr>
<tr id="S4.T2.4.8.8" class="ltx_tr">
<td id="S4.T2.4.8.8.1" class="ltx_td ltx_align_center">40 %</td>
<td id="S4.T2.4.8.8.2" class="ltx_td ltx_align_center">77.50</td>
<td id="S4.T2.4.8.8.3" class="ltx_td ltx_align_center">77.81</td>
<td id="S4.T2.4.8.8.4" class="ltx_td ltx_align_center">1.85</td>
</tr>
<tr id="S4.T2.4.9.9" class="ltx_tr">
<td id="S4.T2.4.9.9.1" class="ltx_td ltx_align_center">50 %</td>
<td id="S4.T2.4.9.9.2" class="ltx_td ltx_align_center">72.74</td>
<td id="S4.T2.4.9.9.3" class="ltx_td ltx_align_center">77.65</td>
<td id="S4.T2.4.9.9.4" class="ltx_td ltx_align_center">1.57</td>
</tr>
<tr id="S4.T2.4.10.10" class="ltx_tr">
<td id="S4.T2.4.10.10.1" class="ltx_td ltx_align_center">60 %</td>
<td id="S4.T2.4.10.10.2" class="ltx_td ltx_align_center">76.00</td>
<td id="S4.T2.4.10.10.3" class="ltx_td ltx_align_center">77.65</td>
<td id="S4.T2.4.10.10.4" class="ltx_td ltx_align_center">1.29</td>
</tr>
<tr id="S4.T2.4.11.11" class="ltx_tr">
<td id="S4.T2.4.11.11.1" class="ltx_td ltx_align_center">70 %</td>
<td id="S4.T2.4.11.11.2" class="ltx_td ltx_align_center">73.43</td>
<td id="S4.T2.4.11.11.3" class="ltx_td ltx_align_center">78.11</td>
<td id="S4.T2.4.11.11.4" class="ltx_td ltx_align_center">1.01</td>
</tr>
<tr id="S4.T2.4.12.12" class="ltx_tr">
<td id="S4.T2.4.12.12.1" class="ltx_td ltx_align_center">80 %</td>
<td id="S4.T2.4.12.12.2" class="ltx_td ltx_align_center">75.18</td>
<td id="S4.T2.4.12.12.3" class="ltx_td ltx_align_center">77.89</td>
<td id="S4.T2.4.12.12.4" class="ltx_td ltx_align_center">0.70</td>
</tr>
<tr id="S4.T2.4.13.13" class="ltx_tr">
<td id="S4.T2.4.13.13.1" class="ltx_td ltx_align_center">90 %</td>
<td id="S4.T2.4.13.13.2" class="ltx_td ltx_align_center">73.37</td>
<td id="S4.T2.4.13.13.3" class="ltx_td ltx_align_center">76.63</td>
<td id="S4.T2.4.13.13.4" class="ltx_td ltx_align_center">0.37</td>
</tr>
<tr id="S4.T2.4.14.14" class="ltx_tr">
<td id="S4.T2.4.14.14.1" class="ltx_td ltx_align_center">95 %</td>
<td id="S4.T2.4.14.14.2" class="ltx_td ltx_align_center">71.05</td>
<td id="S4.T2.4.14.14.3" class="ltx_td ltx_align_center">74.81</td>
<td id="S4.T2.4.14.14.4" class="ltx_td ltx_align_center">0.19</td>
</tr>
<tr id="S4.T2.4.15.15" class="ltx_tr">
<td id="S4.T2.4.15.15.1" class="ltx_td ltx_align_center">99 %</td>
<td id="S4.T2.4.15.15.2" class="ltx_td ltx_align_center">52.77</td>
<td id="S4.T2.4.15.15.3" class="ltx_td ltx_align_center">66.82</td>
<td id="S4.T2.4.15.15.4" class="ltx_td ltx_align_center">0.04</td>
</tr>
<tr id="S4.T2.4.16.16" class="ltx_tr">
<td id="S4.T2.4.16.16.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4">Quantization</td>
</tr>
<tr id="S4.T2.4.17.17" class="ltx_tr">
<td id="S4.T2.4.17.17.1" class="ltx_td ltx_align_center ltx_border_t">8 bits</td>
<td id="S4.T2.4.17.17.2" class="ltx_td ltx_align_center ltx_border_t">78.80</td>
<td id="S4.T2.4.17.17.3" class="ltx_td ltx_align_center ltx_border_t">78.58</td>
<td id="S4.T2.4.17.17.4" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
</tr>
<tr id="S4.T2.4.18.18" class="ltx_tr">
<td id="S4.T2.4.18.18.1" class="ltx_td ltx_align_center">4 bits</td>
<td id="S4.T2.4.18.18.2" class="ltx_td ltx_align_center">79.74</td>
<td id="S4.T2.4.18.18.3" class="ltx_td ltx_align_center">77.04</td>
<td id="S4.T2.4.18.18.4" class="ltx_td ltx_align_center">0.38</td>
</tr>
<tr id="S4.T2.4.19.19" class="ltx_tr">
<td id="S4.T2.4.19.19.1" class="ltx_td ltx_align_center ltx_border_b">1 bit</td>
<td id="S4.T2.4.19.19.2" class="ltx_td ltx_align_center ltx_border_b">48.93</td>
<td id="S4.T2.4.19.19.3" class="ltx_td ltx_align_center ltx_border_b">70.89</td>
<td id="S4.T2.4.19.19.4" class="ltx_td ltx_align_center ltx_border_b">0.10</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">TableÂ <a href="#S4.T2" title="TABLE II â€£ IV-B Compressing more with Quantization â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> summarises the size of a message exchanged between client and server for IID scenario. For quantization, the message size depends only on the quantized weights, since the server knows the clientâ€™s quantization. TableÂ <a href="#S4.T2" title="TABLE II â€£ IV-B Compressing more with Quantization â€£ IV Experiments â€£ Federated learning compression designed for lightweight communications This work is supported by the Futur et Ruptures program funded by IMT and Institut Carnot TSN, and by the GdR ISIS." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> also shows that even simple approaches can be used to compress a network, representing savings of 2 to 4 times in bandwidth without significantly affecting accuracy.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Federated learning represents a new approach to training models in a distributed manner, bringing forth fresh optimization challenges due to the presence of embedded systems serving as FL clients. These clients operate with limited hardware, energy, and communication resources. In this article, we demonstrated the promising application of traditional neural network compression methods in the context of FL. Our easy to implement yet effective technique achieved up to a 50% reduction in message size without any significant impact on accuracy, thereby resulting in direct savings in energy and bandwidth costs. Moreover, our method allows each client to customize their pruning process, enabling greater flexibility to adapt to their unique datasets. By integrating quantization into the training process, we introduced an additional compression technique to the framework. It is conceivable that combining quantization and pruning could further enhance message compression, although our results already demonstrate the significance of both techniques individually. Based on these findings, we posit that incorporating a compression-aware training method, while ensuring seamless integration, is a crucial step in advancing the field of FL.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P.Â P. Ray, â€œA review on tinyml: State-of-the-art and prospects,â€
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of King Saud University-Computer and Information Sciences</em>,
2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.Â Cheng and etÂ al, â€œA survey of model compression and acceleration for deep
neural networks,â€ <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv:1710.09282</em>, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P.Â e.Â a. Kairouz, â€œAdvances and open problems in federated learning,â€
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Foundations and TrendsÂ® in Machine Learning</em>, vol.Â 14,
no. 1â€“2, pp. 1â€“210, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.Â e.Â a. Reddi, â€œAdaptive federated optimization,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv:2003.00295</em>,
2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.Â U. Manzoor, M.Â S. Khan, A.Â R. Khan, F.Â Ayaz, D.Â Flynn, M.Â A. Imran, and
A.Â Zoha, â€œFedclamp: An algorithm for identification of anomalous client in
federated learning,â€ in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2022 29th IEEE International Conference on
Electronics, Circuits and Systems (ICECS)</em>.Â Â Â IEEE, 2022, pp. 1â€“4.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X.Â e.Â a. Qiu, â€œZerofl: Efficient on-device training for federated learning
with local sparsity,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.02507</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.Â Li, G.Â Cheng, X.Â Huang, J.Â Kang, R.Â Yu, Y.Â Wu, and M.Â Pan, â€œAnycostfl:
Efficient on-demand federated learning over heterogeneous edge devices,â€
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.03062</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B.Â e.Â a. McMahan, â€œCommunication-efficient learning of deep networks from
decentralized data,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.Â Â Â PMLR, 2017, pp. 1273â€“1282.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T.Â e.Â a. Hoefler, â€œSparsity in deep learning: Pruning and growth for efficient
inference and training in neural networks.â€ <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>,
vol.Â 22, no. 241, pp. 1â€“124, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.Â e.Â a. Lin, â€œOn-device training under 256kb memory,â€
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv:2206.15472</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H.Â e.Â a. Tessier, â€œRethinking weight decay for efficient neural network
pruning,â€ <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Journal of Imaging</em>, vol.Â 8, no.Â 3, p.Â 64, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S.Â e.Â a. Han, â€œDeep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding,â€ <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1510.00149</em>, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M.Â Horowitz, â€œ1.1 computingâ€™s energy problem (and what we can do about it),â€
in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE International Solid-State Circuits Conference Digest of
Technical Papers (ISSCC)</em>, 2014, pp. 10â€“14.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.Â A. Raihan and T.Â Aamodt, â€œSparse weight activation training,â€
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 33, pp.
15â€‰625â€“15â€‰638, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D.Â J. e.Â a. Beutel, â€œFlower: A friendly federated learning research
framework,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv:2007.14390</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T.-M. H. e.Â a. Hsu, â€œMeasuring the effects of non-identical data distribution
for federated visual classification,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv:1909.06335</em>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Pappalardo, â€œXilinx/brevitas,â€ 2021. [Online]. Available:
<a target="_blank" href="https://doi.org/10.5281/zenodo.3333552" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3333552</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M.Â e.Â a. Courbariaux, â€œBinaryconnect: Training deep neural networks with
binary weights during propagations,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol.Â 28, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.14692" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.14693" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.14693">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.14693" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.14694" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 21:18:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
