<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SOS: Segment Object System for Open-World Instance Segmentation With Object Priors</title>
<!--Generated on Sun Sep 22 23:31:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Open-world Instance Segmentation Object Localization Prompting" lang="en" name="keywords"/>
<base href="/html/2409.14627v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S2" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S2.SS1" title="In 2 Related Work ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Open-world Instance Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S2.SS2" title="In 2 Related Work ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Class-agnostic Object Localization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S2.SS3" title="In 2 Related Work ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Applications of SAM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Segment Object System for OWIS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS1" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Revisit Segment Anything Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Object Localization Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS3" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pseudo Annotations Creator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS4" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Instance Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Object Priors for SOS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1" title="In 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Object Priors</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px1" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Baselines:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px2" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Superpixels:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px3" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Contour Density:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px4" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Saliency:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px5" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Class Activation Maps:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px6" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Self-attention:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS1.SSS0.Px7" title="In 4.1 Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Learned Object Locations:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS2" title="In 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Study Setup for Object Priors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS3" title="In 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results of Object Priors in SOS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS1" title="In 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implementation Details and Data Usage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS2" title="In 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Cross-category COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline"><semantics><mrow><mtext>(VOC)</mtext><mo stretchy="false">→</mo><mtext>COCO</mtext></mrow><annotation-xml encoding="MathML-Content"><apply><ci>→</ci><ci><mtext>(VOC)</mtext></ci><ci><mtext>COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun">(VOC) → COCO</annotation></semantics></math> (non-VOC) Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS3" title="In 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Cross-dataset Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS3.SSS0.Px1" title="In 5.3 Cross-dataset Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline"><semantics><mrow><mtext class="ltx_mathvariant_bold">COCO</mtext><mo stretchy="false">→</mo><mtext class="ltx_mathvariant_bold">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content"><apply><ci>→</ci><ci><mtext class="ltx_mathvariant_bold">COCO</mtext></ci><ci><mtext class="ltx_mathvariant_bold">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun">COCO → LVIS</annotation></semantics></math><span class="ltx_text ltx_font_bold">:</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS3.SSS0.Px2" title="In 5.3 Cross-dataset Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><math alttext="\text{COCO}\rightarrow\text{ADE20k}" class="ltx_Math" display="inline"><semantics><mrow><mtext class="ltx_mathvariant_bold">COCO</mtext><mo stretchy="false">→</mo><mtext class="ltx_mathvariant_bold">ADE20k</mtext></mrow><annotation-xml encoding="MathML-Content"><apply><ci>→</ci><ci><mtext class="ltx_mathvariant_bold">COCO</mtext></ci><ci><mtext class="ltx_mathvariant_bold">ADE20k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">\text{COCO}\rightarrow\text{ADE20k}</annotation><annotation encoding="application/x-llamapun">COCO → ADE20k</annotation></semantics></math><span class="ltx_text ltx_font_bold">:</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS3.SSS0.Px3" title="In 5.3 Cross-dataset Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><math alttext="\text{COCO}\rightarrow\text{UVO}" class="ltx_Math" display="inline"><semantics><mrow><mtext class="ltx_mathvariant_bold">COCO</mtext><mo stretchy="false">→</mo><mtext class="ltx_mathvariant_bold">UVO</mtext></mrow><annotation-xml encoding="MathML-Content"><apply><ci>→</ci><ci><mtext class="ltx_mathvariant_bold">COCO</mtext></ci><ci><mtext class="ltx_mathvariant_bold">UVO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">\text{COCO}\rightarrow\text{UVO}</annotation><annotation encoding="application/x-llamapun">COCO → UVO</annotation></semantics></math><span class="ltx_text ltx_font_bold">:</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS4" title="In 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Quality of Pseudo Annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS5" title="In 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS5.SSS0.Px1" title="In 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Influence of SOS’s Components:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.SS5.SSS0.Px2" title="In 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title">Number of Pseudo Annotations:</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S6" title="In SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Group, University of Hamburg, Germany
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Human-Computer Interaction Group, University of Hamburg, Germany
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{firstname.lastname}@uni-hamburg.de</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">SOS: Segment Object System for Open-World Instance Segmentation With Object Priors</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Wilms<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0009-0003-2490-7029
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tim Rolff<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0001-9038-3196
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maris Hillemann
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robert Johanson
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simone Frintrop<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-9475-3593
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">We propose an approach for Open-World Instance Segmentation (OWIS), a task that aims to segment arbitrary unknown objects in images by generalizing from a limited set of annotated object classes during training. Our Segment Object System (SOS) explicitly addresses the generalization ability and the low precision of state-of-the-art systems, which often generate background detections. To this end, we generate high-quality pseudo annotations based on the foundation model SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>. We thoroughly study various object priors to generate prompts for SAM, explicitly focusing the foundation model on objects. The strongest object priors were obtained by self-attention maps from self-supervised Vision Transformers, which we utilize for prompting SAM. Finally, the post-processed segments from SAM are used as pseudo annotations to train a standard instance segmentation system. Our approach shows strong generalization capabilities on COCO, LVIS, and ADE20k datasets and improves on the precision by up to 81.6% compared to the state-of-the-art. Source code is available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/chwilms/SOS" title="">https://github.com/chwilms/SOS</a></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Open-world Instance Segmentation Object Localization Prompting
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Open-World Instance Segmentation (OWIS) is the task of segmenting all object instances in an image by learning from a limited set of known object classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib50" title="">50</a>]</cite>. In contrast to instance segmentation, OWIS is not limited by the closed-world assumption, which assumes that all object classes are known in advance.
Since OWIS methods aim to detect not only learned but also unknown object classes, they return class-agnostic detections. This is of particular interest in real-world scenarios with previously unknown object classes (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib55" title="">55</a>]</cite>) and challenges the systems’ generalization capabilities. An example of this is shown in the first row of <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1.F1" title="In 1 Introduction ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, where annotations for classes such as surfboard or tennis racket do not exist during training, but the objects should be detected during testing.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Comparison of (pseudo) annotations (left) used by Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>, GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>, and our SOS, when only original annotations of VOC object classes are given. While Mask R-CNN only uses original annotations without object classes such as tennis racket or surfboard (red arrows), SOS generates pseudo annotations covering those classes (green arrows). GGN generates noisy pseudo annotations including background areas. As a result, only SOS constantly detects these objects not annotated in training (green vs. red arrows on the right). Filled masks denote annotations (left) or detected objects (right), while red frames indicate missed objects.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Previous OWIS methods mostly generate pseudo annotations for unannotated objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib56" title="">56</a>]</cite> or replace the foreground-background classification of possible objects in instance segmentation systems with a learning target focusing on the localization quality (e.g., intersection over union) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib48" title="">48</a>]</cite>. While OWIS methods generalize better to unseen object classes than standard instance segmentation systems, they still exhibit low precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>.
One reason for this is the use of noisy pseudo annotations covering background areas as visible in the left part of the second row in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1.F1" title="In 1 Introduction ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recently, foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib4" title="">4</a>]</cite> emerged as a promising technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib29" title="">29</a>]</cite>. They are trained on large datasets with a surrogate task and applied to other tasks in a zero- or few-shot manner using prompting. The first foundation model for image segmentation, the Segment Anything Model (SAM), was proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>. Trained on a large automatically annotated dataset, SAM generates segments for object or stuff regions based on point, box, mask, or text prompts without further training. Despite not only segmenting objects but also stuff regions, given well-designed prompts, SAM is able to generate high-quality object segments. Note that vanilla SAM does not address object segmentation tasks like OWIS itself due to the aforementioned lacking focus on objects.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we propose the <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">Segment Object System</em> (SOS), a novel OWIS method utilizing high-quality pseudo annotations to improve the generalization capability of a standard instance segmentation system (see lower row in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1.F1" title="In 1 Introduction ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). For generating pseudo annotations, we apply the foundation model SAM with prompts derived from an object prior to roughly localize objects of arbitrary classes and to limit segmentations of stuff regions by SAM. To ensure the high quality of our pseudo annotations based on SAM, improving the overall precision, we thoroughly study various hand-crafted and learned object priors for an object-focused application of SAM. This is relevant beyond the scope of OWIS. Using our study findings, SOS utilizes self-attention maps from self-supervised Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib11" title="">11</a>]</cite> as object prior for prompting SAM. Finally, SOS filters low-quality pseudo annotations, combines the pseudo annotations with the original annotations of the known classes, and trains a standard instance segmentation system with these mixed annotations. Our extensive evaluation shows the strong generalization abilities of SOS, considerably outperforming all previous state-of-the-art systems across COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib31" title="">31</a>]</cite>, LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib17" title="">17</a>]</cite>, and ADE20k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib61" title="">61</a>]</cite> datasets. Most notably, the results improve by up to 81.6% in terms of precision over the state-of-the-art due to the high-quality pseudo annotations. We also show that SOS is better suited for OWIS than directly applying SAM.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Overall, our contributions are threefold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose SOS, a novel OWIS method based on a learned object prior, prompt-based pseudo annotations, and an arbitrary instance segmentation system.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We thoroughly study various object priors for focusing SAM on objects, leading to high-quality object segments for pseudo annotations and beyond.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our extensive evaluation shows that the high-quality pseudo annotations in SOS lead to high precision and strong overall results, clearly outperforming other methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Open-world Instance Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To address OWIS, literature mainly offers two streams. First, systems replace the hard classification in instance segmentation systems with a localization score. This avoids classifying unseen objects as background and improves generalization ability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>. In OLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>, centerness and Intersection over Union (IoU) are learned as localization scores in a Mask R-CNN system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>. SWORD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib54" title="">54</a>]</cite> and OpenInst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib48" title="">48</a>]</cite> follow the same idea for query-based systems.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The second line of works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib56" title="">56</a>]</cite> addresses OWIS by augmenting the annotations from known classes with pseudo annotations aiming to cover objects of unknown classes. To generate pseudo annotations, GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite> learns pixel affinities from known classes to generate segments, which are grouped. Similarly, UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite> groups object proposals of object parts to create pseudo annotations, while LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite> applies copy-paste-augmentation. Recently, a new stream emerged <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib58" title="">58</a>]</cite> explicitly utilizing class labels from known classes.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">We follow the second stream and create pseudo annotations within SOS, since it offers the most flexibility w.r.t. the base instance segmentation system. Different from existing approaches that generate noisy pseudo annotations (see GGN in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1.F1" title="In 1 Introduction ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>), we investigate and develop object priors to effectively use recent foundation models to create high-quality object-focused pseudo annotations, leading to high recall and precision.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Class-agnostic Object Localization</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Localizing objects in images is related to several computer vision tasks. For instance, in traditional object proposal generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib2" title="">2</a>]</cite>, cues like saliency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib37" title="">37</a>]</cite> or contour information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib36" title="">36</a>]</cite> were used to localize class-agnostic object candidates for object detection. Since the advent of deep learning, the models are learned from data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib53" title="">53</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Focusing on arbitrary salient objects, known as salient object detection, several approaches use the contrast of hand-crafted features such as color <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib28" title="">28</a>]</cite> or edge information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib28" title="">28</a>]</cite>, as well as CNN-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib59" title="">59</a>]</cite>. In a different task, several authors investigated the information stored in learned classifiers or self-supervised feature extractors based on CNNs or ViTs to locate objects. Notable avenues include Class Activation Maps (CAMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib60" title="">60</a>]</cite> that highlight discriminative object or image parts in CNN-based classifiers and the self-attention maps in the self-supervised DINO feature extractor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite> based on ViTs, which contain information on the scene layout indicating object locations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In our study on object priors to focus SAM on objects, we investigate several of these object localization cues. In contrast to these approaches, we follow a different goal, focusing prompt-based segmentations on objects for OWIS.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Applications of SAM</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> is used in several segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib38" title="">38</a>]</cite>. Similar to us, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib7" title="">7</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib57" title="">57</a>]</cite> use SAM to generate pseudo annotation masks for weakly-supervised semantic segmentation based on image-level supervision. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib18" title="">18</a>]</cite> generate pseudo annotation masks for concealed object segmentation based on scribble annotations. Finally, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib51" title="">51</a>]</cite> generate pseudo annotation masks for weakly-supervised instance segmentation in a multiple instance learning framework.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Different from the weakly-supervised approaches utilizing SAM, we do not rely on supervision. Hence, a key novelty of our work lies in investigating object priors to focus SAM on segmenting arbitrary objects in the absence of supervision for all object classes. Moreover, we are the first to apply SAM-based pseudo annotations in OWIS.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Segment Object System for OWIS</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To address OWIS, we propose our <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">Segment Object System</em> (SOS) that generates high-quality pseudo annotations to train a standard instance segmentation system. As visible from <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, SOS consists of three main blocks. The first block, the Object Localization Module (OLM), described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS2" title="3.2 Object Localization Module ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> (yellow area in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>), aims to roughly localize objects by sampling locations from an object prior that reflects the object probability per image coordinate. The output of OLM is a set of likely object locations, that is used in the Pseudo Annotation Creator (PAC), described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS3" title="3.3 Pseudo Annotations Creator ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> (green area in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>), to prompt the pre-trained SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>, which we briefly revisit in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS1" title="3.1 Revisit Segment Anything Model ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>. From the segments generated from likely object locations, the PAC generates pseudo annotations by filtering low-quality segments and removing near-duplicates. Finally, the pseudo annotations are merged with the original annotations, and a standard instance segmentation system is trained with the merged annotations (see <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS4" title="3.4 Instance Segmentation ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, blue area in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>). During testing, only this instance segmentation system is used.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="537" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Overview of our Segment Object System (SOS) for OWIS consisting of three blocks. First, the input image is processed in our Object Localization Module (OLM, yellow area) to create object-focused point prompts roughly localizing objects. Second, our Pseudo Annotations Creator (PAC, green area) generates segments based on the previously generated prompts using SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>, and further processes them, leading to a final set of merged original and pseudo annotations. Finally, the merged annotations are used to train an instance segmentation system (blue area). </span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Revisit Segment Anything Model</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Since SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> is integral to SOS, we will briefly review the model. The idea of SAM is to utilize prompts such as point coordinates to generate segments of image parts like objects, object parts, or stuff regions localized by these prompts. Hence, SAM does not exclusively segment objects unless explicitly instructed by object-focused prompts, as proposed in this paper. SAM is based on a ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib11" title="">11</a>]</cite> image encoder creating an image embedding, while the embeddings for point prompts are generated based on positional encodings. Both embeddings are processed in a mask decoder utilizing a transformer decoder block and a mask prediction head to create the final segment including a confidence score. Note that SAM is class-agnostic and does not classify the segments.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For training SAM, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> propose the new large-scale SA-1B dataset that is annotated with a data engine using SAM itself. In the pre-stage, SAM is trained with public segmentation datasets to aid the first stage of the data engine, where annotators use SAM as an interactive segmentation tool to create initial segmentations. Subsequently, SAM is retrained with the newly annotated data, fully avoiding data leakage w.r.t. public segmentation datasets. In subsequent stages, manual annotations were provided for segments missed by SAM. At last, annotations were created fully automatically by SAM. See <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> for more details.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Object Localization Module</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The first block of SOS (yellow area in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>) is the Object Localization Module (OLM). OLM roughly localizes all objects in the input image, and creates object-focused point prompts. As the first step, OLM generates an object prior from the input image. The object prior is a probability mass function constrained to the image plane, although other formulations are possible (see <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4" title="4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>), and indicates the probability of an image coordinate being part of any object. Hence, the object prior is class-agnostic and not limited to known object classes from the training set in OWIS. In our study in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4" title="4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we analyze various object priors. For final SOS, we utilize the six self-attention maps from the self-attention heads of a ViT’s final layer, pre-trained in the self-supervised DINO framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite> on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib45" title="">45</a>]</cite>. See <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4" title="4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> for justifications. OLM aggregates the six self-attention maps by elementwise max yielding one map highlighting all relevant scene elements. Subsequently, OLM converts the map to an object prior, by rescaling such that the minimum equals 0 and the sum over the object prior is 1.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.6">Next, OLM randomly samples <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_S</annotation></semantics></math> (here: <math alttext="S=50" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">S</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑆</ci><cn id="S3.SS2.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p2.2.m2.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">S=50</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_S = 50</annotation></semantics></math>) image coordinates from the object prior, given the per-coordinate values of the object prior as probabilities. To diversify the samples across multiple objects, we prune parts of the object prior around the sampled coordinate. Specifically, given a coordinate <math alttext="(x_{s},y_{s})" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.2"><semantics id="S3.SS2.p2.3.m3.2a"><mrow id="S3.SS2.p2.3.m3.2.2.2" xref="S3.SS2.p2.3.m3.2.2.3.cmml"><mo id="S3.SS2.p2.3.m3.2.2.2.3" stretchy="false" xref="S3.SS2.p2.3.m3.2.2.3.cmml">(</mo><msub id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.1.1.2" xref="S3.SS2.p2.3.m3.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.3.m3.1.1.1.1.3" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.SS2.p2.3.m3.2.2.2.4" xref="S3.SS2.p2.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.3.m3.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.2.cmml"><mi id="S3.SS2.p2.3.m3.2.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p2.3.m3.2.2.2.2.3" xref="S3.SS2.p2.3.m3.2.2.2.2.3.cmml">s</mi></msub><mo id="S3.SS2.p2.3.m3.2.2.2.5" stretchy="false" xref="S3.SS2.p2.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.2b"><interval closure="open" id="S3.SS2.p2.3.m3.2.2.3.cmml" xref="S3.SS2.p2.3.m3.2.2.2"><apply id="S3.SS2.p2.3.m3.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2">𝑥</ci><ci id="S3.SS2.p2.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.3">𝑠</ci></apply><apply id="S3.SS2.p2.3.m3.2.2.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.2">𝑦</ci><ci id="S3.SS2.p2.3.m3.2.2.2.2.3.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.3">𝑠</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.2c">(x_{s},y_{s})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.2d">( italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math>, we set the object prior to zero for all coordinates
<math alttext="\{(x,y)\ |\ |x-x_{s}|\leq N\land|y-y_{s}|\leq N\}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.4"><semantics id="S3.SS2.p2.4.m4.4a"><mrow id="S3.SS2.p2.4.m4.4.4.2" xref="S3.SS2.p2.4.m4.4.4.3.cmml"><mo id="S3.SS2.p2.4.m4.4.4.2.3" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.3.1.cmml">{</mo><mrow id="S3.SS2.p2.4.m4.3.3.1.1.2" xref="S3.SS2.p2.4.m4.3.3.1.1.1.cmml"><mo id="S3.SS2.p2.4.m4.3.3.1.1.2.1" stretchy="false" xref="S3.SS2.p2.4.m4.3.3.1.1.1.cmml">(</mo><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">x</mi><mo id="S3.SS2.p2.4.m4.3.3.1.1.2.2" xref="S3.SS2.p2.4.m4.3.3.1.1.1.cmml">,</mo><mi id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml">y</mi><mo id="S3.SS2.p2.4.m4.3.3.1.1.2.3" rspace="0.222em" stretchy="false" xref="S3.SS2.p2.4.m4.3.3.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.4" rspace="0.500em" xref="S3.SS2.p2.4.m4.4.4.3.1.cmml">|</mo><mrow id="S3.SS2.p2.4.m4.4.4.2.2" xref="S3.SS2.p2.4.m4.4.4.2.2.cmml"><mrow id="S3.SS2.p2.4.m4.4.4.2.2.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.1.2.cmml"><mo id="S3.SS2.p2.4.m4.4.4.2.2.1.1.2" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.2.2.1.2.1.cmml">|</mo><mrow id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.2" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.2.cmml">x</mi><mo id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.1.cmml">−</mo><msub id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.2" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.2.cmml">x</mi><mi id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.3" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.2.1.1.3" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.2.2.1.2.1.cmml">|</mo></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.2.4" xref="S3.SS2.p2.4.m4.4.4.2.2.4.cmml">≤</mo><mrow id="S3.SS2.p2.4.m4.4.4.2.2.2" xref="S3.SS2.p2.4.m4.4.4.2.2.2.cmml"><mi id="S3.SS2.p2.4.m4.4.4.2.2.2.3" xref="S3.SS2.p2.4.m4.4.4.2.2.2.3.cmml">N</mi><mo id="S3.SS2.p2.4.m4.4.4.2.2.2.2" xref="S3.SS2.p2.4.m4.4.4.2.2.2.2.cmml">∧</mo><mrow id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.2.cmml"><mo id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.2" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.2.1.cmml">|</mo><mrow id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.2" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.2.cmml">y</mi><mo id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.1" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.1.cmml">−</mo><msub id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.2" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.2.cmml">y</mi><mi id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.3" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.3" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.2.1.cmml">|</mo></mrow></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.2.5" xref="S3.SS2.p2.4.m4.4.4.2.2.5.cmml">≤</mo><mi id="S3.SS2.p2.4.m4.4.4.2.2.6" xref="S3.SS2.p2.4.m4.4.4.2.2.6.cmml">N</mi></mrow><mo id="S3.SS2.p2.4.m4.4.4.2.5" stretchy="false" xref="S3.SS2.p2.4.m4.4.4.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.4b"><apply id="S3.SS2.p2.4.m4.4.4.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2"><csymbol cd="latexml" id="S3.SS2.p2.4.m4.4.4.3.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.3">conditional-set</csymbol><interval closure="open" id="S3.SS2.p2.4.m4.3.3.1.1.1.cmml" xref="S3.SS2.p2.4.m4.3.3.1.1.2"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑥</ci><ci id="S3.SS2.p2.4.m4.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2">𝑦</ci></interval><apply id="S3.SS2.p2.4.m4.4.4.2.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2"><and id="S3.SS2.p2.4.m4.4.4.2.2a.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2"></and><apply id="S3.SS2.p2.4.m4.4.4.2.2b.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2"><leq id="S3.SS2.p2.4.m4.4.4.2.2.4.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.4"></leq><apply id="S3.SS2.p2.4.m4.4.4.2.2.1.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1"><abs id="S3.SS2.p2.4.m4.4.4.2.2.1.2.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.2"></abs><apply id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1"><minus id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.1"></minus><ci id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.2">𝑥</ci><apply id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.2">𝑥</ci><ci id="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.1.1.1.3.3">𝑠</ci></apply></apply></apply><apply id="S3.SS2.p2.4.m4.4.4.2.2.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2"><and id="S3.SS2.p2.4.m4.4.4.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.2"></and><ci id="S3.SS2.p2.4.m4.4.4.2.2.2.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.3">𝑁</ci><apply id="S3.SS2.p2.4.m4.4.4.2.2.2.1.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1"><abs id="S3.SS2.p2.4.m4.4.4.2.2.2.1.2.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.2"></abs><apply id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1"><minus id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.1"></minus><ci id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.2">𝑦</ci><apply id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.2">𝑦</ci><ci id="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.2.1.1.1.3.3">𝑠</ci></apply></apply></apply></apply></apply><apply id="S3.SS2.p2.4.m4.4.4.2.2c.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2"><leq id="S3.SS2.p2.4.m4.4.4.2.2.5.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.5"></leq><share href="https://arxiv.org/html/2409.14627v1#S3.SS2.p2.4.m4.4.4.2.2.2.cmml" id="S3.SS2.p2.4.m4.4.4.2.2d.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2"></share><ci id="S3.SS2.p2.4.m4.4.4.2.2.6.cmml" xref="S3.SS2.p2.4.m4.4.4.2.2.6">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.4c">\{(x,y)\ |\ |x-x_{s}|\leq N\land|y-y_{s}|\leq N\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.4d">{ ( italic_x , italic_y ) | | italic_x - italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | ≤ italic_N ∧ | italic_y - italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | ≤ italic_N }</annotation></semantics></math>,
with <math alttext="N=20" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">N</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><eq id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></eq><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑁</ci><cn id="S3.SS2.p2.5.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p2.5.m5.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">N=20</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_N = 20</annotation></semantics></math>. To transform the object prior back to a probability mass function, we again apply the rescaling as described above. Subsequently, OLM iteratively applies this sampling until a set of <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_S</annotation></semantics></math> coordinates are extracted, representing object-focused point prompts. This set is the output of the OLM and will be used for prompting in the Pseudo Annotation Creator.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pseudo Annotations Creator</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.5">The Pseudo Annotations Creator (PAC, green area in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.F2" title="In 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>) is the second block of SOS. It generates pseudo annotations utilizing SAM, given the object-focused point prompts provided by OLM. These object-focused point prompts lifts SAM from a system that segments anything to a system that segments objects. As the first step, PAC prompts a pre-trained SAM with the <math alttext="S" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_S</annotation></semantics></math> point prompts from OLM. To handle ambiguous point prompts that may indicate multiple objects or object parts, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> and allow SAM to generate three segments per prompt. Each generated segment is a potential pseudo annotation. The resulting set of class-agnostic segments is noisy, with segments covering objects and background, or covering the same image area. Therefore, PAC first utilizes SAM’s confidence score per segment and removes segments with a confidence below <math alttext="\tau_{\text{conf}}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">τ</mi><mtext id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3a.cmml">conf</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝜏</ci><ci id="S3.SS3.p1.2.m2.1.1.3a.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><mtext id="S3.SS3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p1.2.m2.1.1.3">conf</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\tau_{\text{conf}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_τ start_POSTSUBSCRIPT conf end_POSTSUBSCRIPT</annotation></semantics></math>. Second, PAC removes near-duplicates by non-maximum suppression with an IoU threshold <math alttext="\tau_{\text{NMS}}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">τ</mi><mtext id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3a.cmml">NMS</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝜏</ci><ci id="S3.SS3.p1.3.m3.1.1.3a.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><mtext id="S3.SS3.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p1.3.m3.1.1.3">NMS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\tau_{\text{NMS}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_τ start_POSTSUBSCRIPT NMS end_POSTSUBSCRIPT</annotation></semantics></math>. We set <math alttext="\tau_{\text{conf}}=0.9" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><msub id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2.2" xref="S3.SS3.p1.4.m4.1.1.2.2.cmml">τ</mi><mtext id="S3.SS3.p1.4.m4.1.1.2.3" xref="S3.SS3.p1.4.m4.1.1.2.3a.cmml">conf</mtext></msub><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><eq id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></eq><apply id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2.2">𝜏</ci><ci id="S3.SS3.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS3.p1.4.m4.1.1.2.3"><mtext id="S3.SS3.p1.4.m4.1.1.2.3.cmml" mathsize="70%" xref="S3.SS3.p1.4.m4.1.1.2.3">conf</mtext></ci></apply><cn id="S3.SS3.p1.4.m4.1.1.3.cmml" type="float" xref="S3.SS3.p1.4.m4.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">\tau_{\text{conf}}=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_τ start_POSTSUBSCRIPT conf end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="\tau_{\text{NMS}}=0.95" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><msub id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.2" xref="S3.SS3.p1.5.m5.1.1.2.2.cmml">τ</mi><mtext id="S3.SS3.p1.5.m5.1.1.2.3" xref="S3.SS3.p1.5.m5.1.1.2.3a.cmml">NMS</mtext></msub><mo id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><eq id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1"></eq><apply id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.2.1.cmml" xref="S3.SS3.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.2">𝜏</ci><ci id="S3.SS3.p1.5.m5.1.1.2.3a.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3"><mtext id="S3.SS3.p1.5.m5.1.1.2.3.cmml" mathsize="70%" xref="S3.SS3.p1.5.m5.1.1.2.3">NMS</mtext></ci></apply><cn id="S3.SS3.p1.5.m5.1.1.3.cmml" type="float" xref="S3.SS3.p1.5.m5.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\tau_{\text{NMS}}=0.95</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_τ start_POSTSUBSCRIPT NMS end_POSTSUBSCRIPT = 0.95</annotation></semantics></math> for SOS.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.3">Given this set of pseudo annotations, PAC merges the set with the original annotations representing known classes of the training dataset to combine the knowledge of the human annotators and the knowledge of our object-focused SAM. Since we only want to keep pseudo annotations for objects not covered by the original annotations, we suppress pseudo annotations that have a high IoU (<math alttext="\tau_{\text{NMS}}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">τ</mi><mtext id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3a.cmml">NMS</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝜏</ci><ci id="S3.SS3.p2.1.m1.1.1.3a.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><mtext id="S3.SS3.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p2.1.m1.1.1.3">NMS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\tau_{\text{NMS}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_τ start_POSTSUBSCRIPT NMS end_POSTSUBSCRIPT</annotation></semantics></math>) with at least one original annotation. Moreover, we limit the number of pseudo annotations to <math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_P</annotation></semantics></math> in order to balance original and pseudo annotations. In our final SOS, we set <math alttext="P=10" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">P</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><eq id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></eq><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝑃</ci><cn id="S3.SS3.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">P=10</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_P = 10</annotation></semantics></math>. However, across the COCO training dataset, only 7.8 pseudo annotations are added on average. The other pseudo annotations were suppressed in previous steps. Overall, the output of PAC is a mixed set of annotations, containing all original annotations augmented with high-quality pseudo annotations covering objects of unknown classes.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Instance Segmentation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In the last block of SOS, we take the merged annotations generated by PAC and train a standard instance segmentation system. While this system can be arbitrary and is generally replaceable, we choose Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite> with a ResNet-50+FPN backbone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib30" title="">30</a>]</cite> trained in a class-agnostic setting, following recent OWIS methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>. Overall, this leads to an OWIS method based on high-quality pseudo annotations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Object Priors for SOS</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS2" title="3.2 Object Localization Module ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, SOS uses an object prior for rough object localization and subsequent point prompt generation for PAC. This section thoroughly investigates the performance of various object priors derived from previous class-agnostic object localization works in SOS. For details on the object priors beyond the subsequent descriptions, we refer to our supplementary.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Object Priors</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Baselines:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We introduce three baseline object priors that ignore the image content. First, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.1">Grid</span> uses SAM with a regular <math alttext="64\times 64" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">64</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">64</cn><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">64\times 64</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">64 × 64</annotation></semantics></math> grid of points, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>. This directly leads to prompts without the sampling described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS2" title="3.2 Object Localization Module ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. Note that the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.2">Grid</span> baseline will lead to segments for both object and stuff regions. Second, we use the spatial distribution of object centroids from the known classes across the training dataset as object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.3">Dist</span>). We assume that this generalizes well since it ignores class-specific object features. Finally, we utilize the centroids of the training set’s objects from unknown classes as optimal prompts (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.4">GT</span>), representing an upper bound.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Superpixels:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib15" title="">15</a>]</cite>, we utilize the centroids of superpixels generated with an adaptive superpixel segmentation method as object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">Spx</span>). The intuition is that large uniform areas not containing objects should be covered by few superpixels and vice versa. Hence, the density of superpixels is a surrogate for the density of objects. We use the well-known FH superpixels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib13" title="">13</a>]</cite> since they adapt their density to the image content following our assumption. Note that every superpixel centroid is a point prompt.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Contour Density:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">Since objects are defined by their outer contours <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib63" title="">63</a>]</cite>, we use contour density as an object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.1">Contour</span>). Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib52" title="">52</a>]</cite>, we assume that in areas of high contour density, several objects are located and vice versa. As a surrogate for the contour density, we use edge density based on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Saliency:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">Saliency is used in object proposal generation to localize objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib37" title="">37</a>]</cite> as they stick out from their surrounding. Here, we evaluate two saliency methods as object priors. First, we apply the traditional approach VOCUS2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib14" title="">14</a>]</cite> based on color contrast and used by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib37" title="">37</a>]</cite> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.1">VOCUS2</span>). Second, we use pre-trained DeepGaze IIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib32" title="">32</a>]</cite> saliency maps learned from eye fixation data (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.2">DeepGaze</span>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Class Activation Maps:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px5.p1.1">CAMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib60" title="">60</a>]</cite> indicate discriminative image regions for CNN-based classifiers. Since they only need image-level supervision, they are frequently applied in weakly-supervised tasks to locate objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib25" title="">25</a>]</cite>. We use the CAM of the predicted class in a ResNet-50 classifier <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib20" title="">20</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib45" title="">45</a>]</cite> as object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px5.p1.1.1">CAM</span>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px6">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Self-attention:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px6.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px6.p1.1">Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite> have shown that self-attention maps learned inside self-supervised ViTs encode the scene layout, including object locations. Therefore, we explore the self-attention maps from the final layer of a ViT-S backbone trained in the self-supervised DINO framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite> as an object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px6.p1.1.1">DINO</span>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px7">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Learned Object Locations:</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px7.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px7.p1.1">Finally, we learn object locations based on the known classes of a dataset using a U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib44" title="">44</a>]</cite> in a binary segmentation task (object vs. no object) as object prior (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px7.p1.1.1">U-Net</span>). This assumes that, on the level of individual pixels, the model generalizes from known to unknown classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib56" title="">56</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Study Setup for Object Priors</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">To assess each object prior, we train SOS with the respective prior on the COCO training set using only annotations from the 20 PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib12" title="">12</a>]</cite> classes. We evaluate on COCO’s validation set with annotations from the remaining 60 classes in COCO, following standard COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mtext id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S4.SS2.p1.1.m1.1.1.1" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><ci id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">→</ci><ci id="S4.SS2.p1.1.m1.1.1.2a.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><mtext id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S4.SS2.p1.1.m1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><mtext id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">(VOC) → COCO</annotation></semantics></math> (non-VOC) cross-category evaluation in OWIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>. Note that we use the default class-agnostic Mask R-CNN in SOS but reduce the training schedule to only a quarter of steps for faster training. We evaluate the results of SOS given each object priors and report Average Recall (AR) for 100 detections and Average Precision (AP) as previous OWIS research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>. We also report F<sub class="ltx_sub" id="S4.SS2.p1.2.1">1</sub> score, the harmonic mean between AR and AP, yielding a single number for comparison.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results of Object Priors in SOS</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.T1" title="Table 1 ‣ 4.3 Results of Object Priors in SOS ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of our object prior study. First, all priors outperform the baselines <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Grid</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">Dist</span> in terms of F<sub class="ltx_sub" id="S4.SS3.p1.1.3">1</sub>. Both baselines exhibit a lower AP compared to all other object priors, reflecting the missing focus on objects. Hence, simply applying SAM (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.4">Grid</span>) is not suitable to segment only objects, as several stuff regions are segmented as well. Second, most priors exhibit a similar performance in terms of all measures. For instance, learning-based <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.5">CAM</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.6">DeepGaze</span> do not outperform simple priors (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.7">Spx</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.8">Contour</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.9">VOCUS2</span>). Only the priors <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.10">U-Net</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.11">DINO</span> perform substantially better, with <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.12">DINO</span> producing the best result. Comparing <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.13">DINO</span> to <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.14">GT</span> reveals that <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.15">DINO</span> is able to recall almost the same amount of objects, but exhibits a lower precision, as expected.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F3.7">
<tr class="ltx_tr" id="S4.F3.4.4">
<td class="ltx_td ltx_align_center" id="S4.F3.1.1.1" rowspan="2"><span class="ltx_text" id="S4.F3.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="118" id="S4.F3.1.1.1.1.g1" src="extracted/5871922/figs/1737.jpg" width="177"/></span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.2.2.2.g1" src="extracted/5871922/figs/1737_vocus.png" width="89"/></td>
<td class="ltx_td ltx_align_center" id="S4.F3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.3.3.3.g1" src="extracted/5871922/figs/1737_UNet.png" width="89"/></td>
<td class="ltx_td ltx_align_center" id="S4.F3.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.4.4.4.g1" src="extracted/5871922/figs/1737_dino.png" width="89"/></td>
</tr>
<tr class="ltx_tr" id="S4.F3.7.7">
<td class="ltx_td ltx_align_center" id="S4.F3.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.5.5.1.g1" src="extracted/5871922/figs/1737_gtVOCUS.png" width="89"/></td>
<td class="ltx_td ltx_align_center" id="S4.F3.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.6.6.2.g1" src="extracted/5871922/figs/1737_gtUNet.png" width="89"/></td>
<td class="ltx_td ltx_align_center" id="S4.F3.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="59" id="S4.F3.7.7.3.g1" src="extracted/5871922/figs/1737_gtDINO.png" width="89"/></td>
</tr>
<tr class="ltx_tr" id="S4.F3.7.8">
<td class="ltx_td ltx_align_center" id="S4.F3.7.8.1">Input image</td>
<td class="ltx_td ltx_align_center" id="S4.F3.7.8.2"><span class="ltx_text ltx_font_italic" id="S4.F3.7.8.2.1">VOCUS2</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.7.8.3"><span class="ltx_text ltx_font_italic" id="S4.F3.7.8.3.1">U-Net</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.7.8.4"><span class="ltx_text ltx_font_italic" id="S4.F3.7.8.4.1">DINO</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.12.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.13.2" style="font-size:90%;">Object priors <span class="ltx_text ltx_font_italic" id="S4.F3.13.2.1">VOCUS2</span>, <span class="ltx_text ltx_font_italic" id="S4.F3.13.2.2">U-Net</span>, and <span class="ltx_text ltx_font_italic" id="S4.F3.13.2.3">DINO</span> with resulting pseudo annotations.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To illustrate the results, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.F3" title="In 4.3 Results of Object Priors in SOS ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> depicts object priors for top-performing <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">VOCUS2</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">U-Net</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3">DINO</span>, as heatmaps (upper row) with the resulting pseudo annotations generated in SOS (lower row) for an example image. The heatmaps show that <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.4">DINO</span> is very focused on the discriminative object parts, here the polar bears’ faces, while <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.5">U-Net</span> highlights, as learned, the entire objects. Moreover, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.6">U-Net</span> also highlights several background areas. Similarly, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.7">VOCUS2</span> highlights background areas and the polar bears since it is genrally focused on high-contrast areas. Overall, only <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.8">DINO</span> exclusively generates pseudo annotations for the polar bears and some sub-parts, while <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.9">U-Net</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.10">VOCUS2</span> produce noisy annotations including background patches, similar to previous approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The results show that a well-designed object prior like <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">DINO</span> substantially outperforms the baseline <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.2">Grid</span> (+7.5 in F<sub class="ltx_sub" id="S4.SS3.p3.1.3">1</sub>) in SOS. Therefore, we use <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.4">DINO</span> as the object prior in SOS. More generally, the study also reveals important insights on how to prompt SAM for class-agnostic, object-focused results for arbitrary tasks. For more study results, we refer to our supplementary.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.8.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.2.1" style="font-size:90%;">Results of our SOS with various object priors in the cross-category COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S4.T1.2.1.m1.1"><semantics id="S4.T1.2.1.m1.1b"><mrow id="S4.T1.2.1.m1.1.1" xref="S4.T1.2.1.m1.1.1.cmml"><mtext id="S4.T1.2.1.m1.1.1.2" xref="S4.T1.2.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S4.T1.2.1.m1.1.1.1" stretchy="false" xref="S4.T1.2.1.m1.1.1.1.cmml">→</mo><mtext id="S4.T1.2.1.m1.1.1.3" xref="S4.T1.2.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.1.m1.1c"><apply id="S4.T1.2.1.m1.1.1.cmml" xref="S4.T1.2.1.m1.1.1"><ci id="S4.T1.2.1.m1.1.1.1.cmml" xref="S4.T1.2.1.m1.1.1.1">→</ci><ci id="S4.T1.2.1.m1.1.1.2a.cmml" xref="S4.T1.2.1.m1.1.1.2"><mtext id="S4.T1.2.1.m1.1.1.2.cmml" xref="S4.T1.2.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S4.T1.2.1.m1.1.1.3a.cmml" xref="S4.T1.2.1.m1.1.1.3"><mtext id="S4.T1.2.1.m1.1.1.3.cmml" xref="S4.T1.2.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) setting. *: Uses ground truth of unknown classes.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.6">
<tr class="ltx_tr" id="S4.T1.6.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.6.4.5">Object Prior</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.4.6">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.1.1">AR<sub class="ltx_sub" id="S4.T1.3.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.3.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.2.2">F<sub class="ltx_sub" id="S4.T1.4.2.2.1">1</sub>
</td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T1.6.4.7"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.6.4.8"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.6.4.9">Object Prior</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.4.10">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.3.3">AR<sub class="ltx_sub" id="S4.T1.5.3.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.5.3.3.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T1.6.4.4">F<sub class="ltx_sub" id="S4.T1.6.4.4.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.6.5.1">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.5.1.1">Grid</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.5.2">3.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.5.3">36.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.5.4">6.9</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.6.5.5"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.6.5.6"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.6.5.7">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.5.7.1">DeepGaze</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.5.8">5.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.5.9">35.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.5.10">9.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<td class="ltx_td ltx_align_left" id="S4.T1.6.6.1">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.6.1.1">Dist</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.2">3.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.3">27.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.4">6.0</td>
<td class="ltx_td ltx_border_r" id="S4.T1.6.6.5"></td>
<td class="ltx_td" id="S4.T1.6.6.6"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.6.6.7">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.6.7.1">CAM</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.8">5.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.9">36.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.6.6.10">9.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.7">
<td class="ltx_td ltx_align_left" id="S4.T1.6.7.1">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.7.1.1">Spx</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.7.2">5.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.7.3">34.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.7.4">9.6</td>
<td class="ltx_td ltx_border_r" id="S4.T1.6.7.5"></td>
<td class="ltx_td" id="S4.T1.6.7.6"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.6.7.7">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.7.7.1">DINO</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.7.8"><span class="ltx_text ltx_font_bold" id="S4.T1.6.7.8.1">8.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.7.9"><span class="ltx_text ltx_font_bold" id="S4.T1.6.7.9.1">38.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.6.7.10"><span class="ltx_text ltx_font_bold" id="S4.T1.6.7.10.1">14.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.8">
<td class="ltx_td ltx_align_left" id="S4.T1.6.8.1">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.8.1.1">Contour</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.8.2">5.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.8.3">36.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.8.4">9.7</td>
<td class="ltx_td ltx_border_r" id="S4.T1.6.8.5"></td>
<td class="ltx_td" id="S4.T1.6.8.6"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.6.8.7">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.8.7.1">U-Net</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.8.8">7.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.8.9">37.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.6.8.10">12.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.6.9.1">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.9.1.1">VOCUS2</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.6.9.2">6.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.6.9.3">37.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.6.9.4">10.5</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T1.6.9.5"></td>
<td class="ltx_td ltx_border_bb" id="S4.T1.6.9.6"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.6.9.7">SOS + <span class="ltx_text ltx_font_italic" id="S4.T1.6.9.7.1">GT</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.6.9.8">18.1*</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.6.9.9">42.5*</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.9.10">25.4*</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.4">In our evaluation, we compare SOS to recent OWIS methods OLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>, LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>, GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>, SWORD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib54" title="">54</a>]</cite>, SOIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib56" title="">56</a>]</cite>, OpenInst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib48" title="">48</a>]</cite>, UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>, and against Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite> and SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> baselines, depending on the availability of public source code or reported results. To assess the quality of the approaches, we use four cross-category and cross-dataset settings, commonly used in OWIS. Specifically, we choose COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mtext id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.p1.1.m1.1.1.1" stretchy="false" xref="S5.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><ci id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1">→</ci><ci id="S5.p1.1.m1.1.1.2a.cmml" xref="S5.p1.1.m1.1.1.2"><mtext id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.p1.1.m1.1.1.3a.cmml" xref="S5.p1.1.m1.1.1.3"><mtext id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">(VOC) → COCO</annotation></semantics></math> (non-VOC) on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib31" title="">31</a>]</cite>, already discussed in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS2" title="4.2 Study Setup for Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. For cross-dataset evaluation, we use <math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mtext id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2a.cmml">COCO</mtext><mo id="S5.p1.2.m2.1.1.1" stretchy="false" xref="S5.p1.2.m2.1.1.1.cmml">→</mo><mtext id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3a.cmml">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><ci id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1">→</ci><ci id="S5.p1.2.m2.1.1.2a.cmml" xref="S5.p1.2.m2.1.1.2"><mtext id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">COCO</mtext></ci><ci id="S5.p1.2.m2.1.1.3a.cmml" xref="S5.p1.2.m2.1.1.3"><mtext id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">COCO → LVIS</annotation></semantics></math>, <math alttext="\text{COCO}\rightarrow\text{ADE20k}" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mtext id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2a.cmml">COCO</mtext><mo id="S5.p1.3.m3.1.1.1" stretchy="false" xref="S5.p1.3.m3.1.1.1.cmml">→</mo><mtext id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3a.cmml">ADE20k</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><ci id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1">→</ci><ci id="S5.p1.3.m3.1.1.2a.cmml" xref="S5.p1.3.m3.1.1.2"><mtext id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">COCO</mtext></ci><ci id="S5.p1.3.m3.1.1.3a.cmml" xref="S5.p1.3.m3.1.1.3"><mtext id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">ADE20k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">\text{COCO}\rightarrow\text{ADE20k}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">COCO → ADE20k</annotation></semantics></math>, and <math alttext="\text{COCO}\rightarrow\text{UVO}" class="ltx_Math" display="inline" id="S5.p1.4.m4.1"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><mtext id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2a.cmml">COCO</mtext><mo id="S5.p1.4.m4.1.1.1" stretchy="false" xref="S5.p1.4.m4.1.1.1.cmml">→</mo><mtext id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3a.cmml">UVO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><ci id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1">→</ci><ci id="S5.p1.4.m4.1.1.2a.cmml" xref="S5.p1.4.m4.1.1.2"><mtext id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2">COCO</mtext></ci><ci id="S5.p1.4.m4.1.1.3a.cmml" xref="S5.p1.4.m4.1.1.3"><mtext id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3">UVO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">\text{COCO}\rightarrow\text{UVO}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.4.m4.1d">COCO → UVO</annotation></semantics></math> that train on the entire COCO training set and evaluate on the validation sets of LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib17" title="">17</a>]</cite>, ADE20k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib61" title="">61</a>]</cite>, and UVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib50" title="">50</a>]</cite> with exhaustive annotations. The LVIS dataset covers the same images as COCO but extends the annotated object classes from 80 to 1203. ADE20k includes 3169 classes of objects, object parts, and stuff regions. Finally, UVO features exhaustive annotations of all objects and, therefore, no system is penalized for detecting objects outside a datasets’ taxonomy.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">As in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS2" title="4.2 Study Setup for Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> and following common practice in OWIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>, we report AR for up to 100 detections and AP. We again also report the F<sub class="ltx_sub" id="S5.p2.1.1">1</sub> score between AR and AP as a single, combined quantity, described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS2" title="4.2 Study Setup for Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. Note that we only evaluate based on masks, not boxes.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details and Data Usage</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS3" title="4.3 Results of Object Priors in SOS ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>, SOS uses the object prior <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">DINO</span> for best results. DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite> with a ViT-S backbone is used as an ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib45" title="">45</a>]</cite> pre-trained model without fine-tuning. Similarly, SOS uses SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> with the ViT-H backbone as a pre-trained model based on SA-1B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> without fine-tuning. As an instance segmentation model, we apply Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite> with an ImageNet pre-trained ResNet-50+FPN backbone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib30" title="">30</a>]</cite>, and the default configurations except for the class-agnostic training.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Overall, SOS is based on models pre-trained on standard ImageNet and SA-1B. Pre-training on SA-1B or similar large-scale datasets becomes common with the emergence of foundation models and is widely adopted by systems using SAM in similar contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib51" title="">51</a>]</cite>. It is comparable to using models pre-trained on ImageNet that even have class information available during training and also allow a rough localization of objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib6" title="">6</a>]</cite>. Moreover, the final, noisy annotations of SA-1B representing objects, object parts, and stuff region were automatically generated by SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>, and no data leakage w.r.t. other datasets exists.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Cross-category COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.SS2.1.m1.1"><semantics id="S5.SS2.1.m1.1b"><mrow id="S5.SS2.1.m1.1.1" xref="S5.SS2.1.m1.1.1.cmml"><mtext id="S5.SS2.1.m1.1.1.2" xref="S5.SS2.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.SS2.1.m1.1.1.1" stretchy="false" xref="S5.SS2.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS2.1.m1.1.1.3" xref="S5.SS2.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.1.m1.1c"><apply id="S5.SS2.1.m1.1.1.cmml" xref="S5.SS2.1.m1.1.1"><ci id="S5.SS2.1.m1.1.1.1.cmml" xref="S5.SS2.1.m1.1.1.1">→</ci><ci id="S5.SS2.1.m1.1.1.2a.cmml" xref="S5.SS2.1.m1.1.1.2"><mtext id="S5.SS2.1.m1.1.1.2.cmml" xref="S5.SS2.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.SS2.1.m1.1.1.3a.cmml" xref="S5.SS2.1.m1.1.1.3"><mtext id="S5.SS2.1.m1.1.1.3.cmml" xref="S5.SS2.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) Results</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.6">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T3" title='Table 3 ‣ 5.2 Cross-category COCO "(VOC)"→"COCO" (non-VOC) Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors'><span class="ltx_text ltx_ref_tag">3</span></a> presents the results of various OWIS methods on the cross-category setup COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mtext id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.SS2.p1.1.m1.1.1.1" stretchy="false" xref="S5.SS2.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><ci id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1">→</ci><ci id="S5.SS2.p1.1.m1.1.1.2a.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><mtext id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.SS2.p1.1.m1.1.1.3a.cmml" xref="S5.SS2.p1.1.m1.1.1.3"><mtext id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">(VOC) → COCO</annotation></semantics></math> (non-VOC). Our SOS clearly outperforms all OWIS methods in AR<sub class="ltx_sub" id="S5.SS2.p1.6.1"><span class="ltx_text ltx_font_italic" id="S5.SS2.p1.6.1.1">100</span></sub>, AP, and F<sub class="ltx_sub" id="S5.SS2.p1.6.2">1</sub>. Specifically, SOS outperforms the second-best system in terms of F<sub class="ltx_sub" id="S5.SS2.p1.6.3">1</sub>, GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>, by 6.1. While some of the improvement comes from a better recall (+38.9% compared to GGN), the results are driven by a much-improved precision (+81.6% compared to GGN). This reflects the high quality of the pseudo annotations in SOS on this cross-category setting. Moreover, there is a clear improvement of SOS over Mask R-CNN (+12.7 in F<sub class="ltx_sub" id="S5.SS2.p1.6.4">1</sub>). Compared to original SAM, the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.6.5">DINO</span>-based SOS substantially improves the precision by focusing on objects, leading to an improved F<sub class="ltx_sub" id="S5.SS2.p1.6.6">1</sub> score (+7.8).</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The qualitative results of various OWIS methods in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.F4" title='In 5.2 Cross-category COCO "(VOC)"→"COCO" (non-VOC) Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors'><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> support the quantitative results. In the first example, only GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite> and SOS detect both giraffes, however, SOS generates more accurate segmentations. In the second example, only SOS detects all five small surfboards, while other systems detect at most three. Note that class-agnostic Mask R-CNN misses all non-VOC objects in both examples. For more qualitative results, see our supplementary.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T3.8" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T3.8.10.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T3.4.4.2" style="font-size:90%;">Results of two baselines and various OWIS methods in the COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.T3.3.3.1.m1.1"><semantics id="S5.T3.3.3.1.m1.1b"><mrow id="S5.T3.3.3.1.m1.1.1" xref="S5.T3.3.3.1.m1.1.1.cmml"><mtext id="S5.T3.3.3.1.m1.1.1.2" xref="S5.T3.3.3.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.T3.3.3.1.m1.1.1.1" stretchy="false" xref="S5.T3.3.3.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T3.3.3.1.m1.1.1.3" xref="S5.T3.3.3.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.m1.1c"><apply id="S5.T3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.1.m1.1.1"><ci id="S5.T3.3.3.1.m1.1.1.1.cmml" xref="S5.T3.3.3.1.m1.1.1.1">→</ci><ci id="S5.T3.3.3.1.m1.1.1.2a.cmml" xref="S5.T3.3.3.1.m1.1.1.2"><mtext id="S5.T3.3.3.1.m1.1.1.2.cmml" xref="S5.T3.3.3.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.T3.3.3.1.m1.1.1.3a.cmml" xref="S5.T3.3.3.1.m1.1.1.3"><mtext id="S5.T3.3.3.1.m1.1.1.3.cmml" xref="S5.T3.3.3.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) setting. <sup class="ltx_sup" id="S5.T3.4.4.2.1">†</sup>: uses automatically annotated SA-1B dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.8.8">
<tr class="ltx_tr" id="S5.T3.6.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.6.6.2.3">System</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.6.6.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.5.5.1.1">AR<sub class="ltx_sub" id="S5.T3.5.5.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.5.5.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T3.6.6.2.2">F<sub class="ltx_sub" id="S5.T3.6.6.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.8.8.5.1">Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.8.8.5.2">1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.8.8.5.3">8.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.8.8.5.4">1.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.7.3">
<td class="ltx_td ltx_align_left" id="S5.T3.7.7.3.1">SAM<sup class="ltx_sup" id="S5.T3.7.7.3.1.1">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.7.3.2">3.6</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.7.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.7.7.3.3.1">48.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.7.7.3.4">6.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.8.8.6.1">OLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.8.8.6.2">4.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.8.8.6.3">28.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.8.8.6.4">7.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.7">
<td class="ltx_td ltx_align_left" id="S5.T3.8.8.7.1">LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.7.2">4.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.7.3">24.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.8.8.7.4">7.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.8.8.8.1">GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.8.2">4.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.8.3">28.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.8.8.8.4">8.4</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.9">
<td class="ltx_td ltx_align_left" id="S5.T3.8.8.9.1">SWORD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib54" title="">54</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.9.2">4.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.9.3">30.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.8.8.9.4">8.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.10">
<td class="ltx_td ltx_align_left" id="S5.T3.8.8.10.1">UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.10.2">2.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.10.3">34.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.8.8.10.4">5.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.8.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T3.8.8.4.1">SOS<sup class="ltx_sup" id="S5.T3.8.8.4.1.1">†</sup> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.8.8.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.8.8.4.2.1">8.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.8.8.4.3">39.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.8.8.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.8.8.4.4.1">14.5</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T3.16" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T3.16.10.3.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.12.4.2" style="font-size:90%;">Results of two baselines and various OWIS methods in the <math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline" id="S5.T3.11.3.1.m1.1"><semantics id="S5.T3.11.3.1.m1.1b"><mrow id="S5.T3.11.3.1.m1.1.1" xref="S5.T3.11.3.1.m1.1.1.cmml"><mtext id="S5.T3.11.3.1.m1.1.1.2" xref="S5.T3.11.3.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.T3.11.3.1.m1.1.1.1" stretchy="false" xref="S5.T3.11.3.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T3.11.3.1.m1.1.1.3" xref="S5.T3.11.3.1.m1.1.1.3a.cmml">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.11.3.1.m1.1c"><apply id="S5.T3.11.3.1.m1.1.1.cmml" xref="S5.T3.11.3.1.m1.1.1"><ci id="S5.T3.11.3.1.m1.1.1.1.cmml" xref="S5.T3.11.3.1.m1.1.1.1">→</ci><ci id="S5.T3.11.3.1.m1.1.1.2a.cmml" xref="S5.T3.11.3.1.m1.1.1.2"><mtext id="S5.T3.11.3.1.m1.1.1.2.cmml" xref="S5.T3.11.3.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.T3.11.3.1.m1.1.1.3a.cmml" xref="S5.T3.11.3.1.m1.1.1.3"><mtext id="S5.T3.11.3.1.m1.1.1.3.cmml" xref="S5.T3.11.3.1.m1.1.1.3">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.3.1.m1.1d">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.3.1.m1.1e">COCO → LVIS</annotation></semantics></math> setting. <sup class="ltx_sup" id="S5.T3.12.4.2.1">†</sup>: uses automatically annotated SA-1B dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.16.8">
<tr class="ltx_tr" id="S5.T3.14.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.14.6.2.3">System</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.14.6.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.13.5.1.1">AR<sub class="ltx_sub" id="S5.T3.13.5.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.13.5.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T3.14.6.2.2">F<sub class="ltx_sub" id="S5.T3.14.6.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.16.8.5.1">Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.16.8.5.2">7.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.16.8.5.3">23.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.16.8.5.4">11.4</td>
</tr>
<tr class="ltx_tr" id="S5.T3.15.7.3">
<td class="ltx_td ltx_align_left" id="S5.T3.15.7.3.1">SAM<sup class="ltx_sup" id="S5.T3.15.7.3.1.1">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.15.7.3.2">6.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.15.7.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.15.7.3.3.1">45.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.15.7.3.4">11.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.16.8.6.1">LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.16.8.6.2">6.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.16.8.6.3">24.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.16.8.6.4">10.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.7">
<td class="ltx_td ltx_align_left" id="S5.T3.16.8.7.1">GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.7.2">6.5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.7.3">27.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.16.8.7.4">10.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.16.8.8.1">SOIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib56" title="">56</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.8.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.8.3">25.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.16.8.8.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.9">
<td class="ltx_td ltx_align_left" id="S5.T3.16.8.9.1">OpenInst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib48" title="">48</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.9.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.9.3">29.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.16.8.9.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.10">
<td class="ltx_td ltx_align_left" id="S5.T3.16.8.10.1">UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.10.2">3.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.16.8.10.3">24.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.16.8.10.4">6.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.16.8.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T3.16.8.4.1">SOS<sup class="ltx_sup" id="S5.T3.16.8.4.1.1">†</sup> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.16.8.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.16.8.4.2.1">8.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.16.8.4.3">33.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.16.8.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.16.8.4.4.1">13.3</span></td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S5.F4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.F4.12">
<tr class="ltx_tr" id="S5.F4.6.6">
<td class="ltx_td ltx_align_center" id="S5.F4.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.1.1.1.g1" src="extracted/5871922/figs/33707_gt.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.2.2.2.g1" src="extracted/5871922/figs/33707_baseline.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.3.3.3.g1" src="extracted/5871922/figs/33707_ldet.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.4.4.4.g1" src="extracted/5871922/figs/33707_ggn.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.5.5.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.5.5.5.g1" src="extracted/5871922/figs/33707_UDOS.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.6.6.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="96" id="S5.F4.6.6.6.g1" src="extracted/5871922/figs/33707_SOS.png" width="96"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.12.12">
<td class="ltx_td ltx_align_center" id="S5.F4.7.7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.7.7.1.g1" src="extracted/5871922/figs/326174_gt.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.8.8.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.8.8.2.g1" src="extracted/5871922/figs/326174_baseline.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.9.9.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.9.9.3.g1" src="extracted/5871922/figs/326174_ldet.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.10.10.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.10.10.4.g1" src="extracted/5871922/figs/326174_ggn.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.11.11.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.11.11.5.g1" src="extracted/5871922/figs/326174_UDOS.png" width="96"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.12.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S5.F4.12.12.6.g1" src="extracted/5871922/figs/326174_SOS.png" width="96"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.12.13">
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.1"><span class="ltx_text" id="S5.F4.12.13.1.1" style="font-size:70%;">Ground Truth</span></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.2"><span class="ltx_text" id="S5.F4.12.13.2.1" style="font-size:70%;">Mask R-CNN</span></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.3"><span class="ltx_text" id="S5.F4.12.13.3.1" style="font-size:70%;">LDET</span></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.4"><span class="ltx_text" id="S5.F4.12.13.4.1" style="font-size:70%;">GGN</span></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.5"><span class="ltx_text" id="S5.F4.12.13.5.1" style="font-size:70%;">UDOS</span></td>
<td class="ltx_td ltx_align_center" id="S5.F4.12.13.6"><span class="ltx_text" id="S5.F4.12.13.6.1" style="font-size:70%;">SOS (ours)</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.16.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.14.1" style="font-size:90%;">Qualitative results of OWIS methods and baseline Mask R-CNN in the cross-category COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.F4.14.1.m1.1"><semantics id="S5.F4.14.1.m1.1b"><mrow id="S5.F4.14.1.m1.1.1" xref="S5.F4.14.1.m1.1.1.cmml"><mtext id="S5.F4.14.1.m1.1.1.2" xref="S5.F4.14.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.F4.14.1.m1.1.1.1" stretchy="false" xref="S5.F4.14.1.m1.1.1.1.cmml">→</mo><mtext id="S5.F4.14.1.m1.1.1.3" xref="S5.F4.14.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.14.1.m1.1c"><apply id="S5.F4.14.1.m1.1.1.cmml" xref="S5.F4.14.1.m1.1.1"><ci id="S5.F4.14.1.m1.1.1.1.cmml" xref="S5.F4.14.1.m1.1.1.1">→</ci><ci id="S5.F4.14.1.m1.1.1.2a.cmml" xref="S5.F4.14.1.m1.1.1.2"><mtext id="S5.F4.14.1.m1.1.1.2.cmml" xref="S5.F4.14.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.F4.14.1.m1.1.1.3a.cmml" xref="S5.F4.14.1.m1.1.1.3"><mtext id="S5.F4.14.1.m1.1.1.3.cmml" xref="S5.F4.14.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.14.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.F4.14.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) setting. Filled masks denote detected objects, while red frames indicate missed objects.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Cross-dataset Results</h3>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">
<math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.1.m1.1"><semantics id="S5.SS3.SSS0.Px1.1.m1.1b"><mrow id="S5.SS3.SSS0.Px1.1.m1.1.1" xref="S5.SS3.SSS0.Px1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px1.1.m1.1.1.2" xref="S5.SS3.SSS0.Px1.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px1.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px1.1.m1.1.1.1.cmml">→</mo><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px1.1.m1.1.1.3" xref="S5.SS3.SSS0.Px1.1.m1.1.1.3a.cmml">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px1.1.m1.1c"><apply id="S5.SS3.SSS0.Px1.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1"><ci id="S5.SS3.SSS0.Px1.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px1.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px1.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px1.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px1.1.m1.1.1.3">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px1.1.m1.1d">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px1.1.m1.1e">COCO → LVIS</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px1.2.1">:</span>
</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.3">We evaluate the cross-dataset generalization capabilities of OWIS methods, starting with <math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mtext id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3a.cmml">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1"><ci id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2"><mtext id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3"><mtext id="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px1.p1.1.m1.1.1.3">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px1.p1.1.m1.1c">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px1.p1.1.m1.1d">COCO → LVIS</annotation></semantics></math>. The results in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T3" title='In 5.2 Cross-category COCO "(VOC)"→"COCO" (non-VOC) Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors'><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> show a trend similar to the cross-category results. SOS outperforms all OWIS methods across all measures with an improvement of 2.8 in F<sub class="ltx_sub" id="S5.SS3.SSS0.Px1.p1.3.1">1</sub> compared to second-best methods GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite> and LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>. Similar to the previous results, the improvement is based on gains in both recall and precision, however, the relative improvements in AP and AR<sub class="ltx_sub" id="S5.SS3.SSS0.Px1.p1.3.2"><span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.3.2.1">100</span></sub> are more similar to each other. Overall, our high-quality pseudo annotations in SOS lead to new state-of-the-art results, which indicate strong generalization to unknown object classes outside COCO.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T5.7" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.7.9.3.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T5.4.4.2" style="font-size:90%;">Results of Mask R-CNN baseline and various OWIS methods in the <math alttext="\text{COCO}\rightarrow\text{ADE20k}" class="ltx_Math" display="inline" id="S5.T5.3.3.1.m1.1"><semantics id="S5.T5.3.3.1.m1.1b"><mrow id="S5.T5.3.3.1.m1.1.1" xref="S5.T5.3.3.1.m1.1.1.cmml"><mtext id="S5.T5.3.3.1.m1.1.1.2" xref="S5.T5.3.3.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.T5.3.3.1.m1.1.1.1" stretchy="false" xref="S5.T5.3.3.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T5.3.3.1.m1.1.1.3" xref="S5.T5.3.3.1.m1.1.1.3a.cmml">ADE20k</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.m1.1c"><apply id="S5.T5.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1"><ci id="S5.T5.3.3.1.m1.1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1.1">→</ci><ci id="S5.T5.3.3.1.m1.1.1.2a.cmml" xref="S5.T5.3.3.1.m1.1.1.2"><mtext id="S5.T5.3.3.1.m1.1.1.2.cmml" xref="S5.T5.3.3.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.T5.3.3.1.m1.1.1.3a.cmml" xref="S5.T5.3.3.1.m1.1.1.3"><mtext id="S5.T5.3.3.1.m1.1.1.3.cmml" xref="S5.T5.3.3.1.m1.1.1.3">ADE20k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.m1.1d">\text{COCO}\rightarrow\text{ADE20k}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.1.m1.1e">COCO → ADE20k</annotation></semantics></math> setting. <sup class="ltx_sup" id="S5.T5.4.4.2.1">†</sup>: uses automatically annotated SA-1B dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.7.7">
<tr class="ltx_tr" id="S5.T5.6.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T5.6.6.2.3">System</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.6.6.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.5.5.1.1">AR<sub class="ltx_sub" id="S5.T5.5.5.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.5.5.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T5.6.6.2.2">F<sub class="ltx_sub" id="S5.T5.6.6.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.7.7.4.1">Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.7.4.2">6.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.7.4.3">11.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.7.7.4.4">8.7</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.7.7.5.1">OLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.7.5.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.7.5.3">20.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.7.7.5.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.6">
<td class="ltx_td ltx_align_left" id="S5.T5.7.7.6.1">LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.6.2">9.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.6.3">18.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.7.7.6.4">12.6</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.7">
<td class="ltx_td ltx_align_left" id="S5.T5.7.7.7.1">GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.2">9.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.3">21.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.7.7.7.4">13.3</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.8">
<td class="ltx_td ltx_align_left" id="S5.T5.7.7.8.1">UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.8.2">7.6</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.8.3">22.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.7.7.8.4">11.4</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T5.7.7.3.1">SOS<sup class="ltx_sup" id="S5.T5.7.7.3.1.1">†</sup> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.7.7.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.7.7.3.2.1">12.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.7.7.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.7.7.3.3.1">26.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.7.7.3.4"><span class="ltx_text ltx_font_bold" id="S5.T5.7.7.3.4.1">17.0</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T5.15" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.15.10.3.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.11.4.2" style="font-size:90%;">Results of two baselines and various OWIS methods in the <math alttext="\text{COCO}\rightarrow\text{UVO}" class="ltx_Math" display="inline" id="S5.T5.10.3.1.m1.1"><semantics id="S5.T5.10.3.1.m1.1b"><mrow id="S5.T5.10.3.1.m1.1.1" xref="S5.T5.10.3.1.m1.1.1.cmml"><mtext id="S5.T5.10.3.1.m1.1.1.2" xref="S5.T5.10.3.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.T5.10.3.1.m1.1.1.1" stretchy="false" xref="S5.T5.10.3.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T5.10.3.1.m1.1.1.3" xref="S5.T5.10.3.1.m1.1.1.3a.cmml">UVO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.10.3.1.m1.1c"><apply id="S5.T5.10.3.1.m1.1.1.cmml" xref="S5.T5.10.3.1.m1.1.1"><ci id="S5.T5.10.3.1.m1.1.1.1.cmml" xref="S5.T5.10.3.1.m1.1.1.1">→</ci><ci id="S5.T5.10.3.1.m1.1.1.2a.cmml" xref="S5.T5.10.3.1.m1.1.1.2"><mtext id="S5.T5.10.3.1.m1.1.1.2.cmml" xref="S5.T5.10.3.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.T5.10.3.1.m1.1.1.3a.cmml" xref="S5.T5.10.3.1.m1.1.1.3"><mtext id="S5.T5.10.3.1.m1.1.1.3.cmml" xref="S5.T5.10.3.1.m1.1.1.3">UVO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.3.1.m1.1d">\text{COCO}\rightarrow\text{UVO}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.3.1.m1.1e">COCO → UVO</annotation></semantics></math> setting. <sup class="ltx_sup" id="S5.T5.11.4.2.1">†</sup>: uses automatically annotated SA-1B dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.15.8">
<tr class="ltx_tr" id="S5.T5.13.6.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T5.13.6.2.3">System</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.13.6.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.12.5.1.1">AR<sub class="ltx_sub" id="S5.T5.12.5.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.12.5.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T5.13.6.2.2">F<sub class="ltx_sub" id="S5.T5.13.6.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.15.8.5.1">Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.15.8.5.2">20.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.15.8.5.3">36.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.15.8.5.4">26.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.14.7.3">
<td class="ltx_td ltx_align_left" id="S5.T5.14.7.3.1">SAM<sup class="ltx_sup" id="S5.T5.14.7.3.1.1">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.7.3.2">11.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.7.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.14.7.3.3.1">50.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.14.7.3.4">18.4</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.15.8.6.1">OLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.15.8.6.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.15.8.6.3">41.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.15.8.6.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.7">
<td class="ltx_td ltx_align_left" id="S5.T5.15.8.7.1">LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.7.2"><span class="ltx_text ltx_font_bold" id="S5.T5.15.8.7.2.1">22.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.7.3">40.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.15.8.7.4"><span class="ltx_text ltx_font_bold" id="S5.T5.15.8.7.4.1">28.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.8">
<td class="ltx_td ltx_align_left" id="S5.T5.15.8.8.1">GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.8.2">20.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.8.3">43.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.15.8.8.4">27.7</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.9">
<td class="ltx_td ltx_align_left" id="S5.T5.15.8.9.1">UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.9.2">10.6</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.8.9.3">43.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.15.8.9.4">17.0</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.8.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T5.15.8.4.1">SOS<sup class="ltx_sup" id="S5.T5.15.8.4.1.1">†</sup> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.15.8.4.2">20.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.15.8.4.3">42.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.15.8.4.4">28.0</td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">
<math alttext="\text{COCO}\rightarrow\text{ADE20k}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.1.m1.1"><semantics id="S5.SS3.SSS0.Px2.1.m1.1b"><mrow id="S5.SS3.SSS0.Px2.1.m1.1.1" xref="S5.SS3.SSS0.Px2.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px2.1.m1.1.1.2" xref="S5.SS3.SSS0.Px2.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px2.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px2.1.m1.1.1.1.cmml">→</mo><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px2.1.m1.1.1.3" xref="S5.SS3.SSS0.Px2.1.m1.1.1.3a.cmml">ADE20k</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.1.m1.1c"><apply id="S5.SS3.SSS0.Px2.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1"><ci id="S5.SS3.SSS0.Px2.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px2.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px2.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px2.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px2.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.1.m1.1.1.3">ADE20k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.1.m1.1d">\text{COCO}\rightarrow\text{ADE20k}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.1.m1.1e">COCO → ADE20k</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px2.2.1">:</span>
</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.4">The results for <math alttext="\text{COCO}\rightarrow\text{ADE20k}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S5.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mtext id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3a.cmml">ADE20k</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1"><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2"><mtext id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.3">ADE20k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.p1.1.m1.1c">\text{COCO}\rightarrow\text{ADE20k}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.p1.1.m1.1d">COCO → ADE20k</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T5" title='In "COCO"→"LVIS": ‣ 5.3 Cross-dataset Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors'><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, which include labeled object parts of ADE20k, show that SOS again outperforms all other OWIS methods across all measures. The gain over the second-best method in F<sub class="ltx_sub" id="S5.SS3.SSS0.Px2.p1.4.1">1</sub>, GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>, is 3.7. Similar to <math alttext="\text{COCO}\rightarrow\text{LVIS}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS3.SSS0.Px2.p1.3.m3.1a"><mrow id="S5.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.cmml"><mtext id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml">→</mo><mtext id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3a.cmml">LVIS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1"><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2a.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2"><mtext id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3a.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3"><mtext id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.3">LVIS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.p1.3.m3.1c">\text{COCO}\rightarrow\text{LVIS}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.p1.3.m3.1d">COCO → LVIS</annotation></semantics></math>, the relative gains in AR<sub class="ltx_sub" id="S5.SS3.SSS0.Px2.p1.4.2"><span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px2.p1.4.2.1">100</span></sub> and AP are equally distributed. These results indicate that object parts do not degrade the results of SOS. This is in line with SOS’s pseudo annotations in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.F3" title="In 4.3 Results of Object Priors in SOS ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, covering entire objects and selected object parts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">
<math alttext="\text{COCO}\rightarrow\text{UVO}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.1.m1.1"><semantics id="S5.SS3.SSS0.Px3.1.m1.1b"><mrow id="S5.SS3.SSS0.Px3.1.m1.1.1" xref="S5.SS3.SSS0.Px3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px3.1.m1.1.1.2" xref="S5.SS3.SSS0.Px3.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px3.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px3.1.m1.1.1.1.cmml">→</mo><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px3.1.m1.1.1.3" xref="S5.SS3.SSS0.Px3.1.m1.1.1.3a.cmml">UVO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px3.1.m1.1c"><apply id="S5.SS3.SSS0.Px3.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1"><ci id="S5.SS3.SSS0.Px3.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px3.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px3.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px3.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S5.SS3.SSS0.Px3.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px3.1.m1.1.1.3">UVO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px3.1.m1.1d">\text{COCO}\rightarrow\text{UVO}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px3.1.m1.1e">COCO → UVO</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px3.2.1">:</span>
</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.3">Finally, <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T5" title='In "COCO"→"LVIS": ‣ 5.3 Cross-dataset Results ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors'><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> presents results for <math alttext="\text{COCO}\rightarrow\text{UVO}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS3.SSS0.Px3.p1.1.m1.1a"><mrow id="S5.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.cmml"><mtext id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2a.cmml">COCO</mtext><mo id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.1" stretchy="false" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3a.cmml">UVO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px3.p1.1.m1.1b"><apply id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1"><ci id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.1">→</ci><ci id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2a.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2"><mtext id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.2">COCO</mtext></ci><ci id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3a.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3"><mtext id="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.SSS0.Px3.p1.1.m1.1.1.3">UVO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px3.p1.1.m1.1c">\text{COCO}\rightarrow\text{UVO}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px3.p1.1.m1.1d">COCO → UVO</annotation></semantics></math>. SOS still outperforms Mask R-CNN, SAM and recent UDOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib24" title="">24</a>]</cite> in F<sub class="ltx_sub" id="S5.SS3.SSS0.Px3.p1.3.1">1</sub>. Outperforming SAM implies that SOS does not exploit the limited taxonomy of annotations in COCO (only 60 object classes in test), but also improves the segmentation of objects outside the COCO object classes on the exhaustively labeled UVO dataset. However, the results of SOS are below LDET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib46" title="">46</a>]</cite> in AP and F<sub class="ltx_sub" id="S5.SS3.SSS0.Px3.p1.3.2">1</sub>. We attribute this to several classes in UVO not available in ImageNet, which was used to learn <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px3.p1.3.3">DINO</span> object prior. Hence, <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px3.p1.3.4">DINO</span> will miss such objects in the COCO dataset during training, and SOS might miss them during testing. This could be mitigated using more diverse unlabeled images for training <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px3.p1.3.5">DINO</span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Quality of Pseudo Annotations</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.7">We also investigate the quality of the pseudo annotations generated in SOS. To this end, we evaluate the pseudo annotations on the COCO training set against the annotations of non-VOC object classes, reflecting the COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mtext id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.SS4.p1.1.m1.1.1.1" stretchy="false" xref="S5.SS4.p1.1.m1.1.1.1.cmml">→</mo><mtext id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><ci id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1">→</ci><ci id="S5.SS4.p1.1.m1.1.1.2a.cmml" xref="S5.SS4.p1.1.m1.1.1.2"><mtext id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.SS4.p1.1.m1.1.1.3a.cmml" xref="S5.SS4.p1.1.m1.1.1.3"><mtext id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">(VOC) → COCO</annotation></semantics></math> non-VOC) setting. The results in terms of precision and recall for <math alttext="\text{IoU}=0.5" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><mrow id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mtext id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2a.cmml">IoU</mtext><mo id="S5.SS4.p1.2.m2.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><eq id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1"></eq><ci id="S5.SS4.p1.2.m2.1.1.2a.cmml" xref="S5.SS4.p1.2.m2.1.1.2"><mtext id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2">IoU</mtext></ci><cn id="S5.SS4.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS4.p1.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">\text{IoU}=0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">IoU = 0.5</annotation></semantics></math> as well as the F<sub class="ltx_sub" id="S5.SS4.p1.7.1">1</sub> score are presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T6" title="In 5.4 Quality of Pseudo Annotations ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>. We use up to three and 10 pseudo annotations generated per image in SOS (SOS<sub class="ltx_sub" id="S5.SS4.p1.7.2">3</sub> and SOS<sub class="ltx_sub" id="S5.SS4.p1.7.3"><span class="ltx_text ltx_font_italic" id="S5.SS4.p1.7.3.1">10</span></sub>), and GGN<sub class="ltx_sub" id="S5.SS4.p1.7.4">3</sub> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite> that generates three pseudo annotations. The results show that with only three SOS pseudo annotations, more than 25% of the non-VOC objects are covered. This is substantially more than GGN<sub class="ltx_sub" id="S5.SS4.p1.7.5">3</sub> (12.1%). With up to 10 annotations, SOS’s pseudo annotations cover more than 40% of the non-VOC objects. In our supplementary, we further provide class-specific results.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.3">To complement these results, we visualize the annotations of GGN<sub class="ltx_sub" id="S5.SS4.p2.3.1">3</sub> and SOS<sub class="ltx_sub" id="S5.SS4.p2.3.2">3</sub> on two sample images in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.F5" title="In Table 6 ‣ 5.4 Quality of Pseudo Annotations ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. It is visible that GGN<sub class="ltx_sub" id="S5.SS4.p2.3.3">3</sub>’s annotations cover background regions, leading to low precision in the overall results. Moreover, the pseudo annotations in the upper example do not adhere well to the airplane’s boundaries. More qualitative examples are given in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S1.F1" title="In 1 Introduction ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and our supplementary.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T6.6" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T6.6.8.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.2.2.1" style="font-size:90%;">Quality of pseudo annotation generated in GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite> and our SOS evaluated on the non-VOC annotations of the COCO training dataset. <sup class="ltx_sup" id="S5.T6.2.2.1.1">†</sup>: uses automatically annotated SA-1B dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T6.6.6">
<tr class="ltx_tr" id="S5.T6.3.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T6.3.3.1.2">System</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.3.3.1.3">Prec.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.3.3.1.4">Rec.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T6.3.3.1.1">F<sub class="ltx_sub" id="S5.T6.3.3.1.1.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.4.4.2.1">GGN<sub class="ltx_sub" id="S5.T6.4.4.2.1.1">3</sub> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.2.2">7.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.2.3">12.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.4.4.2.4">9.1</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.5.5.3.1">SOS<math alttext="{}_{3}^{\dagger}" class="ltx_Math" display="inline" id="S5.T6.5.5.3.1.m1.1"><semantics id="S5.T6.5.5.3.1.m1.1a"><mmultiscripts id="S5.T6.5.5.3.1.m1.1.1" xref="S5.T6.5.5.3.1.m1.1.1.cmml"><mi id="S5.T6.5.5.3.1.m1.1.1.2.2" xref="S5.T6.5.5.3.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S5.T6.5.5.3.1.m1.1.1a" xref="S5.T6.5.5.3.1.m1.1.1.cmml"></mprescripts><mrow id="S5.T6.5.5.3.1.m1.1.1b" xref="S5.T6.5.5.3.1.m1.1.1.cmml"></mrow><mo id="S5.T6.5.5.3.1.m1.1.1.3" xref="S5.T6.5.5.3.1.m1.1.1.3.cmml">†</mo><mn id="S5.T6.5.5.3.1.m1.1.1.2.3" xref="S5.T6.5.5.3.1.m1.1.1.2.3.cmml">3</mn><mrow id="S5.T6.5.5.3.1.m1.1.1c" xref="S5.T6.5.5.3.1.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S5.T6.5.5.3.1.m1.1b"><apply id="S5.T6.5.5.3.1.m1.1.1.cmml" xref="S5.T6.5.5.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.5.5.3.1.m1.1.1.1.cmml" xref="S5.T6.5.5.3.1.m1.1.1">superscript</csymbol><apply id="S5.T6.5.5.3.1.m1.1.1.2.cmml" xref="S5.T6.5.5.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.5.5.3.1.m1.1.1.2.1.cmml" xref="S5.T6.5.5.3.1.m1.1.1">subscript</csymbol><csymbol cd="latexml" id="S5.T6.5.5.3.1.m1.1.1.2.2.cmml" xref="S5.T6.5.5.3.1.m1.1.1.2.2">absent</csymbol><cn id="S5.T6.5.5.3.1.m1.1.1.2.3.cmml" type="integer" xref="S5.T6.5.5.3.1.m1.1.1.2.3">3</cn></apply><ci id="S5.T6.5.5.3.1.m1.1.1.3.cmml" xref="S5.T6.5.5.3.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.5.5.3.1.m1.1c">{}_{3}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.5.5.3.1.m1.1d">start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.3.2"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.3.2.1">19.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.3.3">26.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.5.5.3.4">22.1</td>
</tr>
<tr class="ltx_tr" id="S5.T6.6.6.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T6.6.6.4.1">SOS<math alttext="{}_{10}^{\dagger}" class="ltx_Math" display="inline" id="S5.T6.6.6.4.1.m1.1"><semantics id="S5.T6.6.6.4.1.m1.1a"><mmultiscripts id="S5.T6.6.6.4.1.m1.1.1" xref="S5.T6.6.6.4.1.m1.1.1.cmml"><mi id="S5.T6.6.6.4.1.m1.1.1.2.2" xref="S5.T6.6.6.4.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S5.T6.6.6.4.1.m1.1.1a" xref="S5.T6.6.6.4.1.m1.1.1.cmml"></mprescripts><mrow id="S5.T6.6.6.4.1.m1.1.1b" xref="S5.T6.6.6.4.1.m1.1.1.cmml"></mrow><mo id="S5.T6.6.6.4.1.m1.1.1.3" xref="S5.T6.6.6.4.1.m1.1.1.3.cmml">†</mo><mn id="S5.T6.6.6.4.1.m1.1.1.2.3" xref="S5.T6.6.6.4.1.m1.1.1.2.3.cmml">10</mn><mrow id="S5.T6.6.6.4.1.m1.1.1c" xref="S5.T6.6.6.4.1.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S5.T6.6.6.4.1.m1.1b"><apply id="S5.T6.6.6.4.1.m1.1.1.cmml" xref="S5.T6.6.6.4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.6.6.4.1.m1.1.1.1.cmml" xref="S5.T6.6.6.4.1.m1.1.1">superscript</csymbol><apply id="S5.T6.6.6.4.1.m1.1.1.2.cmml" xref="S5.T6.6.6.4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.6.6.4.1.m1.1.1.2.1.cmml" xref="S5.T6.6.6.4.1.m1.1.1">subscript</csymbol><csymbol cd="latexml" id="S5.T6.6.6.4.1.m1.1.1.2.2.cmml" xref="S5.T6.6.6.4.1.m1.1.1.2.2">absent</csymbol><cn id="S5.T6.6.6.4.1.m1.1.1.2.3.cmml" type="integer" xref="S5.T6.6.6.4.1.m1.1.1.2.3">10</cn></apply><ci id="S5.T6.6.6.4.1.m1.1.1.3.cmml" xref="S5.T6.6.6.4.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.6.6.4.1.m1.1c">{}_{10}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.6.6.4.1.m1.1d">start_FLOATSUBSCRIPT 10 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.6.6.4.2">15.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.6.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T6.6.6.4.3.1">41.7</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T6.6.6.4.4"><span class="ltx_text ltx_font_bold" id="S5.T6.6.6.4.4.1">22.6</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S5.F5" style="width:208.1pt;">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S5.T6.10.4">
<tr class="ltx_tr" id="S5.T6.8.2.2">
<td class="ltx_td ltx_align_center" id="S5.T6.7.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="159" id="S5.T6.7.1.1.1.g1" src="extracted/5871922/figs/540_GGN.png" width="240"/></td>
<td class="ltx_td ltx_align_center" id="S5.T6.8.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="159" id="S5.T6.8.2.2.2.g1" src="extracted/5871922/figs/540_DINO.png" width="240"/></td>
</tr>
<tr class="ltx_tr" id="S5.T6.10.4.4">
<td class="ltx_td ltx_align_center" id="S5.T6.9.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="160" id="S5.T6.9.3.3.1.g1" src="extracted/5871922/figs/650_GGN.png" width="240"/></td>
<td class="ltx_td ltx_align_center" id="S5.T6.10.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="160" id="S5.T6.10.4.4.2.g1" src="extracted/5871922/figs/650_DINO.png" width="240"/></td>
</tr>
<tr class="ltx_tr" id="S5.T6.10.4.5">
<td class="ltx_td ltx_align_center" id="S5.T6.10.4.5.1"><span class="ltx_text" id="S5.T6.10.4.5.1.1" style="font-size:70%;">GGN</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.10.4.5.2"><span class="ltx_text" id="S5.T6.10.4.5.2.1" style="font-size:70%;">SOS</span></td>
</tr>
</table>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.6.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.7.2" style="font-size:90%;">Pseudo annotations generated by GGN and our SOS.</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Ablation Studies</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">All ablation studies follow the evaluation setup described in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S4.SS2" title="4.2 Study Setup for Object Priors ‣ 4 Object Priors for SOS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. More studies evaluating design choices in SOS are presented in our supplementary.</p>
</div>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Influence of SOS’s Components:</h4>
<div class="ltx_para" id="S5.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS5.SSS0.Px1.p1.2">We investigate the importance of each component in SOS and present the results in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T8" title="In Influence of SOS’s Components: ‣ 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>. The baseline for this study is a class-agnostic Mask R-CNN trained without pseudo annotations (first row in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T8" title="In Influence of SOS’s Components: ‣ 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>). Subsequently, we add pseudo annotations based on SAM with the baseline <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.2.1">Grid</span> object prior as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib27" title="">27</a>]</cite> and without our post-processing in PAC (confidence-based thresholding and NMS, see <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S3.SS3" title="3.3 Pseudo Annotations Creator ‣ 3 Segment Object System for OWIS ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). Next, we replace <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.2.2">Grid</span> with <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.2.3">DINO</span> (third row in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T8" title="In Influence of SOS’s Components: ‣ 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>), and finally add the post-processing leading to the complete SOS. The results in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T8" title="In Influence of SOS’s Components: ‣ 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a> show each step improves the F<sub class="ltx_sub" id="S5.SS5.SSS0.Px1.p1.2.4">1</sub> results. Mainly, introducing SAM for generating pseudo annotations (first row vs. second row) substantially improves AR<sub class="ltx_sub" id="S5.SS5.SSS0.Px1.p1.2.5"><span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.2.5.1">100</span></sub>, while adding the <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.2.6">DINO</span> object prior removes background annotations leading to considerably improved precision. Hence, the object prior selection in SOS is crucial for strong results.</p>
</div>
<figure class="ltx_table" id="S5.T8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T8.4" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T8.4.5.2.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S5.T8.2.2.1" style="font-size:90%;">Influence of added components in our SOS evaluated in the COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.T8.2.2.1.m1.1"><semantics id="S5.T8.2.2.1.m1.1b"><mrow id="S5.T8.2.2.1.m1.1.1" xref="S5.T8.2.2.1.m1.1.1.cmml"><mtext id="S5.T8.2.2.1.m1.1.1.2" xref="S5.T8.2.2.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.T8.2.2.1.m1.1.1.1" stretchy="false" xref="S5.T8.2.2.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T8.2.2.1.m1.1.1.3" xref="S5.T8.2.2.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.2.2.1.m1.1c"><apply id="S5.T8.2.2.1.m1.1.1.cmml" xref="S5.T8.2.2.1.m1.1.1"><ci id="S5.T8.2.2.1.m1.1.1.1.cmml" xref="S5.T8.2.2.1.m1.1.1.1">→</ci><ci id="S5.T8.2.2.1.m1.1.1.2a.cmml" xref="S5.T8.2.2.1.m1.1.1.2"><mtext id="S5.T8.2.2.1.m1.1.1.2.cmml" xref="S5.T8.2.2.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.T8.2.2.1.m1.1.1.3a.cmml" xref="S5.T8.2.2.1.m1.1.1.3"><mtext id="S5.T8.2.2.1.m1.1.1.3.cmml" xref="S5.T8.2.2.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.2.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.T8.2.2.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) setting.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T8.4.4">
<tr class="ltx_tr" id="S5.T8.4.4.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T8.4.4.2.3">Components</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T8.4.4.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T8.3.3.1.1">AR<sub class="ltx_sub" id="S5.T8.3.3.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T8.4.4.2.2">F<sub class="ltx_sub" id="S5.T8.4.4.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.4.4.3.1">Mask R-CNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.4.4.3.2">1.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.4.4.3.3">10.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T8.4.4.3.4">2.2</td>
</tr>
<tr class="ltx_tr" id="S5.T8.4.4.4">
<td class="ltx_td ltx_align_left" id="S5.T8.4.4.4.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.4.4.4.1.1">Grid</span>, w/o pp.</td>
<td class="ltx_td ltx_align_center" id="S5.T8.4.4.4.2">3.4</td>
<td class="ltx_td ltx_align_center" id="S5.T8.4.4.4.3">35.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.4.4.4.4">6.2</td>
</tr>
<tr class="ltx_tr" id="S5.T8.4.4.5">
<td class="ltx_td ltx_align_left" id="S5.T8.4.4.5.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.4.4.5.1.1">DINO</span>, w/o pp.</td>
<td class="ltx_td ltx_align_center" id="S5.T8.4.4.5.2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.4.5.2.1">8.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.4.4.5.3">37.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.4.4.5.4">14.3</td>
</tr>
<tr class="ltx_tr" id="S5.T8.4.4.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T8.4.4.6.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.4.4.6.1.1">DINO</span>, w/ pp.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.6.2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.4.6.2.1">8.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.6.3"><span class="ltx_text ltx_font_bold" id="S5.T8.4.4.6.3.1">38.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T8.4.4.6.4"><span class="ltx_text ltx_font_bold" id="S5.T8.4.4.6.4.1">14.4</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S5.T8.8" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T8.8.5.2.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S5.T8.6.2.1" style="font-size:90%;">SOS results with various numbers of pseudo annotations in the COCO <math alttext="\text{(VOC)}\rightarrow\text{COCO}" class="ltx_Math" display="inline" id="S5.T8.6.2.1.m1.1"><semantics id="S5.T8.6.2.1.m1.1b"><mrow id="S5.T8.6.2.1.m1.1.1" xref="S5.T8.6.2.1.m1.1.1.cmml"><mtext id="S5.T8.6.2.1.m1.1.1.2" xref="S5.T8.6.2.1.m1.1.1.2a.cmml">(VOC)</mtext><mo id="S5.T8.6.2.1.m1.1.1.1" stretchy="false" xref="S5.T8.6.2.1.m1.1.1.1.cmml">→</mo><mtext id="S5.T8.6.2.1.m1.1.1.3" xref="S5.T8.6.2.1.m1.1.1.3a.cmml">COCO</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.T8.6.2.1.m1.1c"><apply id="S5.T8.6.2.1.m1.1.1.cmml" xref="S5.T8.6.2.1.m1.1.1"><ci id="S5.T8.6.2.1.m1.1.1.1.cmml" xref="S5.T8.6.2.1.m1.1.1.1">→</ci><ci id="S5.T8.6.2.1.m1.1.1.2a.cmml" xref="S5.T8.6.2.1.m1.1.1.2"><mtext id="S5.T8.6.2.1.m1.1.1.2.cmml" xref="S5.T8.6.2.1.m1.1.1.2">(VOC)</mtext></ci><ci id="S5.T8.6.2.1.m1.1.1.3a.cmml" xref="S5.T8.6.2.1.m1.1.1.3"><mtext id="S5.T8.6.2.1.m1.1.1.3.cmml" xref="S5.T8.6.2.1.m1.1.1.3">COCO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.6.2.1.m1.1d">\text{(VOC)}\rightarrow\text{COCO}</annotation><annotation encoding="application/x-llamapun" id="S5.T8.6.2.1.m1.1e">(VOC) → COCO</annotation></semantics></math> (non-VOC) setting.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T8.8.4">
<tr class="ltx_tr" id="S5.T8.8.4.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T8.8.4.2.3">#Pseudo Anns.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T8.8.4.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T8.7.3.1.1">AR<sub class="ltx_sub" id="S5.T8.7.3.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T8.7.3.1.1.1.1">100</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T8.8.4.2.2">F<sub class="ltx_sub" id="S5.T8.8.4.2.2.1">1</sub>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.8.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.8.4.3.1">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.8.4.3.2">8.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.8.4.3.3">34.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T8.8.4.3.4">13.4</td>
</tr>
<tr class="ltx_tr" id="S5.T8.8.4.4">
<td class="ltx_td ltx_align_left" id="S5.T8.8.4.4.1">5</td>
<td class="ltx_td ltx_align_center" id="S5.T8.8.4.4.2">8.8</td>
<td class="ltx_td ltx_align_center" id="S5.T8.8.4.4.3">36.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.8.4.4.4">14.2</td>
</tr>
<tr class="ltx_tr" id="S5.T8.8.4.5">
<td class="ltx_td ltx_align_left" id="S5.T8.8.4.5.1">10</td>
<td class="ltx_td ltx_align_center" id="S5.T8.8.4.5.2"><span class="ltx_text ltx_font_bold" id="S5.T8.8.4.5.2.1">8.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.8.4.5.3"><span class="ltx_text ltx_font_bold" id="S5.T8.8.4.5.3.1">38.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.8.4.5.4"><span class="ltx_text ltx_font_bold" id="S5.T8.8.4.5.4.1">14.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.8.4.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T8.8.4.6.1">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.8.4.6.2">8.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.8.4.6.3"><span class="ltx_text ltx_font_bold" id="S5.T8.8.4.6.3.1">38.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T8.8.4.6.4">14.3</td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Number of Pseudo Annotations:</h4>
<div class="ltx_para" id="S5.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS5.SSS0.Px2.p1.1">In the second ablation study, we investigate the influence of the number of pseudo annotations per image on the results of SOS. To this end, we evaluate SOS with up to 3, 5, 10, and 20 pseudo annotations per image. The results in <a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#S5.T8" title="In Influence of SOS’s Components: ‣ 5.5 Ablation Studies ‣ 5 Evaluation ‣ SOS: Segment Object System for Open-World Instance Segmentation With Object Priors"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a> show that 10 pseudo annotations are preferable. Hence, the ideal number of pseudo annotations in SOS is larger than in GGN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14627v1#bib.bib49" title="">49</a>]</cite>, which only uses three. This indicates again the high quality of our pseudo annotations, since more than three annotations per image can cover relevant objects without adding too many noisy annotations.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we addressed the challenging OWIS task and aimed to improve the low precision of previous methods in this open-world task. To improve the precision and the overall detection results, we generate high-quality pseudo annotations based on a prompt-guided foundation model, SAM. To focus the pseudo annotations on objects, we thoroughly investigated various object priors for prompt creation, which revealed important insights on how to prompt SAM for class-agnostic object-focused results in arbitrary tasks. As a result, our novel OWIS method SOS, which uses a self-attention-based object prior for generating pseudo annotations, outperforms recent state-of-the-art OWIS methods across challenging open-world setups in COCO, LVIS, and ADE20k datasets. SOS especially improves the precision based on the high-quality pseudo annotations. Moreover, SOS with its focus on objects also outperforms vanilla SAM that segments both object and stuff regions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ahn, J., Cho, S., Kwak, S.: Weakly supervised learning of instance segmentation
with inter-pixel relations. In: Conference on Computer Vision and Pattern
Recognition (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Alexe, B., Deselaers, T., Ferrari, V.: What is an object? In: Conference on
Computer Vision and Pattern Recognition (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Baugh, M., Batten, J., Müller, J.P., Kainz, B.: Zero-shot anomaly detection
with pre-trained segmentation models. arXiv preprint arXiv:2306.09269 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258
(2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models
are few-shot learners. Advances in Neural Information Processing Systems
<span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">33</span> (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P.,
Joulin, A.: Emerging properties in self-supervised vision transformers. In:
International Conference on Computer Vision (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen, T., Mai, Z., Li, R., Chao, W.l.: Segment anything model (SAM) enhanced
pseudo labels for weakly supervised semantic segmentation. arXiv preprint
arXiv:2305.05803 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Cheng, M.M., Mitra, N.J., Huang, X., Torr, P.H., Hu, S.M.: Global contrast
based salient region detection. IEEE Transactions on Pattern Analysis and
Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">37</span>(3) (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Human Language
Technology Conference of the North American Chapter of the Association for
Computational Linguistics (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dollár, P., Zitnick, C.L.: Structured forests for fast edge detection. In:
International Conference on Computer Vision (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.:
An image is worth 16x16 words: Transformers for image recognition at scale.
In: International Conference on Learning Representations (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J.,
Zisserman, A.: The PASCAL visual object classes challenge: A retrospective.
International Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">111</span> (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Felzenszwalb, P.F., Huttenlocher, D.P.: Efficient graph-based image
segmentation. International Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">59</span> (2004)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Frintrop, S., Werner, T., Martin Garcia, G.: Traditional saliency reloaded: A
good old model in new shape. In: Conference on Computer Vision and Pattern
Recognition (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gao, G., Lauri, M., Zhang, J., Frintrop, S.: Saliency-guided adaptive seeding
for supervoxel segmentation. In: International Conference on Intelligent
Robots and Systems (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Girshick, R.: Fast R-CNN. In: International Conference on Computer Vision
(2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gupta, A., Dollár, P., Girshick, R.: LVIS: A dataset for large vocabulary
instance segmentation. In: Conference on Computer Vision and Pattern
Recognition (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
He, C., Li, K., Zhang, Y., Xu, G., Tang, L., Zhang, Y., Guo, Z., Li, X.:
Weakly-supervised concealed object segmentation with SAM-based pseudo
labeling and multi-scale feature grouping. arXiv preprint arXiv:2305.11003
(2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:
International Conference on Computer Vision (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Conference on Computer Vision and Pattern Recognition (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P.H.: Deeply supervised
salient object detection with short connections. In: Conference on Computer
Vision and Pattern Recognition (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hu, H., Lan, S., Jiang, Y., Cao, Z., Sha, F.: FastMask: Segment multi-scale
object candidates in one shot. In: Conference on Computer Vision and Pattern
Recognition (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung,
Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language
representation learning with noisy text supervision. In: International
Conference on Machine Learning (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:
Open-world instance segmentation: Top-down learning with bottom-up
supervision. arXiv preprint arXiv:2303.05503 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kim, B., Yoo, Y., Rhee, C.E., Kim, J.: Beyond semantic to instance
segmentation: Weakly-supervised instance segmentation via semantic knowledge
transfer and self-refinement. In: Conference on Computer Vision and Pattern
Recognition (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world
object proposals without learning to classify. IEEE Robotics and Automation
Letters <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">7</span>(2) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:
International Conference on Computer Vision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Klein, D.A., Frintrop, S.: Center-surround divergence of feature statistics for
salient object detection. In: International Conference on Computer Vision
(2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.:
Feature pyramid networks for object detection. In: Conference on Computer
Vision and Pattern Recognition (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.: Microsoft COCO: Common objects in context.
In: European Conference on Computer Vision (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Linardos, A., Kümmerer, M., Press, O., Bethge, M.: DeepGaze IIE:
Calibrated prediction in and out-of-domain for state-of-the-art saliency
modeling. In: International Conference on Computer Vision (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Liu, N., Han, J., Yang, M.H.: PiCANet: Learning pixel-wise contextual
attention for saliency detection. In: Conference on Computer Vision and
Pattern Recognition (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Liu, T., Yuan, Z., Sun, J., Wang, J., Zheng, N., Tang, X., Shum, H.Y.: Learning
to detect a salient object. IEEE Transactions on Pattern analysis and machine
intelligence <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">33</span>(2) (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Liu, Y., Wu, Y.H., Wen, P., Shi, Y., Qiu, Y., Cheng, M.M.: Leveraging
instance-, image-and dataset-level information for weakly supervised instance
segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence
<span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">44</span>(3) (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ma, J., Ming, A., Huang, Z., Wang, X., Zhou, Y.: Object-level proposals. In:
International Conference on Computer Vision (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Martin García, G., Potapova, E., Werner, T., Zillich, M., Vincze, M.,
Frintrop, S.: Saliency-based object discovery on RGB-D data with a
late-fusion approach. In: International Conference on Robotics and Automation
(2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Mazurowski, M.A., Dong, H., Gu, H., Yang, J., Konz, N., Zhang, Y.: Segment
anything model for medical image analysis: an experimental study. Medical
Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib38.1.1">89</span> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Perazzi, F., Krähenbühl, P., Pritch, Y., Hornung, A.: Saliency filters:
Contrast based filtering for salient region detection. In: Conference on
Computer Vision and Pattern Recognition (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Pinheiro, P.O., Collobert, R., Dollár, P.: Learning to segment object
candidates. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib40.1.1">28</span>
(2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Pinheiro, P.O., Lin, T.Y., Collobert, R., Dollár, P.: Learning to refine
object segments. In: European Conference on Computer Vision (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual
models from natural language supervision. In: International Conference on
Machine Learning (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Rahtu, E., Kannala, J., Blaschko, M.: Learning a category independent object
detection cascade. In: International Conference on Computer Vision (2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for
biomedical image segmentation. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual
recognition challenge. International Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib45.1.1">115</span>
(2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Saito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in
an open world. In: European Conference on Computer Vision (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Shi, Z., Sun, Y., Zhang, M.: Training-free object counting with prompts. In:
Winter Conference on Applications of Computer Vision (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Wang, C., Wang, G., Zhang, Q., Guo, P., Liu, W., Wang, X.: OpenInst: A simple
query-based method for open-world instance segmentation. arXiv preprint
arXiv:2303.15859 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance
segmentation: Exploiting pseudo ground truth from learned pairwise affinity.
In: Conference on Computer Vision and Pattern Recognition (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wang, W., Feiszli, M., Wang, H., Tran, D.: Unidentified video objects: A
benchmark for dense, open-world segmentation. In: International Conference on
Computer Vision (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wei, Z., Chen, P., Yu, X., Li, G., Jiao, J., Han, Z.: Semantic-aware SAM for
point-prompted instance segmentation. arXiv preprint arXiv:2312.15895 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Wilms, C., Frintrop, S.: Edge adaptive seeding for superpixel segmentation. In:
German Conference on Pattern Recognition (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Wilms, C., Frintrop, S.: AttentionMask: Attentive, efficient object proposal
generation focusing on small objects. In: Asian Conference on Computer Vision
(2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Wu, J., Jiang, Y., Yan, B., Lu, H., Yuan, Z., Luo, P.: Exploring transformers
for open-world instance segmentation. In: International Conference on
Computer Vision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Xie, C., Xiang, Y., Mousavian, A., Fox, D.: Unseen object instance segmentation
for robotic environments. IEEE Transactions on Robotics <span class="ltx_text ltx_font_bold" id="bib.bib55.1.1">37</span>(5)
(2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Xue, X., Yu, D., Liu, L., Liu, Y., Li, Y., Yuan, Z., Song, P., Shou, M.Z.:
Single-stage open-world instance segmentation with cross-task consistency
regularization. arXiv preprint arXiv:2208.09023 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Yang, X., Gong, X.: Foundation model assisted weakly supervised semantic
segmentation. In: Winter Conference on Applications of Computer Vision (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Yang, Y., Zhou, Z., Wu, J., Wang, Y., Xiong, R.: Class semantics modulation for
open-set instance segmentation. IEEE Robotics and Automation Letters (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: EGNet: Edge
guidance network for salient object detection. In: International Conference
on Computer Vision (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep
features for discriminative localization. In: Conference on Computer Vision
and Pattern Recognition (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:
Semantic understanding of scenes through the ADE20K dataset. International
Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib61.1.1">127</span> (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Zhu, M., Li, H., Chen, H., Fan, C., Mao, W., Jing, C., Liu, Y., Shen, C.:
SegPrompt: Boosting open-world segmentation via category-level prompt
learning. In: International Conference on Computer Vision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Zitnick, C.L., Dollár, P.: Edge boxes: Locating object proposals from
edges. In: European Conference on Computer Vision (2014)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 22 23:31:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
