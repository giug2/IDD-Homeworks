<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.02015] Parameterizing Federated Continual Learning for Reproducible Research</title><meta property="og:description" content="Federated Learning (FL) systems evolve in heterogeneous and ever-evolving environments that challenge their performance.
Under real deployments, the learning tasks of clients can also evolve with time, which calls for …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Parameterizing Federated Continual Learning for Reproducible Research">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Parameterizing Federated Continual Learning for Reproducible Research">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.02015">

<!--Generated on Fri Jul  5 20:17:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Continual Learning Resource and Data heterogeneity">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Delft University of Technology, Delft, The Netherlands
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{b.a.cox,j.m.galjaard,a.shankar,j.decouchant}@tudelft.nl</span></span></span>
<br class="ltx_break"><span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>lydiaychen@ieee.org</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Parameterizing Federated Continual Learning for Reproducible Research</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bart Cox
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5209-6161" title="ORCID identifier" class="ltx_ref">0000-0001-5209-6161</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jeroen Galjaard
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-3681-7226" title="ORCID identifier" class="ltx_ref">0000-0003-3681-7226</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aditya Shankar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0009-3046-8724" title="ORCID identifier" class="ltx_ref">0009-0009-3046-8724</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jérémie Decouchant
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9143-3984" title="ORCID identifier" class="ltx_ref">0000-0001-9143-3984</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lydia Y. Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4228-6735" title="ORCID identifier" class="ltx_ref">0000-0002-4228-6735</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) systems evolve in heterogeneous and ever-evolving environments that challenge their performance.
Under real deployments, the learning tasks of clients can also evolve with time, which calls for the integration of methodologies such as Continual Learning.
To enable research reproducibility, we propose a set of experimental best practices that precisely capture and emulate complex learning scenarios. Our framework, <span id="id1.id1.1" class="ltx_text ltx_font_typewriter">Freddie</span>, is the first entirely configurable framework for Federated Continual Learning (FCL), and it can be seamlessly deployed on a large number of machines thanks to the use of Kubernetes and containerization.
We demonstrate the effectiveness of <span id="id1.id1.2" class="ltx_text ltx_font_typewriter">Freddie</span> on two use cases, (i) large-scale FL on CIFAR100 and (ii) heterogeneous task sequence on FCL, which highlight unaddressed performance challenges in FCL scenarios.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
Federated Continual Learning Resource and Data heterogeneity
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> performs distributed optimization thanks to a central federator server that maintains a global model using model updates computed by clients.
It is common for data to be distributed among the clients of an FL system in a non-independent and identically distributed (non-IID) way.
Moreover, in practice, client learning tasks also evolve over time.
Continual Learning (CL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
is a technique that addresses the scenario where a model is continuously trained on evolving client tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One of the key challenges in CL is catastrophic forgetting: parameters or semantic representations learned for past tasks drift under the influence of new tasks.
Three categories of techniques address this challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Replay mechanisms, like GEM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and DGR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, retain or generate data from earlier tasks for new task adaptation, which allow the network to revise previously learned tasks.
Regularization techniques, such as EWC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, penalize the divergence of model parameters, preventing the adaptation process on new tasks from deviating too far from the model learned on prior tasks. Parameter isolation methods use specific weights of the network for the task at hand, i.e., use a mask to freeze the weights of other tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Continual Learning allows a client to learn from its previous tasks if features are repeated over time.
Federated Continual Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (FCL) combines CL and FL, enabling clients to indirectly learn from each other.
Existing CL frameworks
do not take this indirect learning into account and therefore
provide limited support for Federated Continual Learning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition, reproducing FCL results that were obtained in deployment is difficult.
For example, experimental environments are often tightly controlled and steady, while real-world environments are often dynamic and heterogeneous. In addition, clients might be punctually busy processing co-located tasks.
Several FL simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and emulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> frameworks have been proposed, but they cannot be easily extended to support heterogeneous data, learning tasks and hardware platforms.
In addition, frameworks that focus on enabling large-scale FL experiments impose a significant overhead to manage the execution or require the use of a strict pipeline.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we address the lack of a scalable yet flexible framework for reproducible FCL experiments.
Overall, we make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We identify key requirements for FL and FCL emulation: ease of use, reproducibility, support for complex workloads, and resource heterogeneity.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">Freddie</span>—a framework for <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">F</span>ederated and dist<span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_framed ltx_framed_underline">r</span>ibut<span id="S1.I1.i2.p1.1.4" class="ltx_text ltx_framed ltx_framed_underline">ed</span> mach<span id="S1.I1.i2.p1.1.5" class="ltx_text ltx_framed ltx_framed_underline">i</span>ne-l<span id="S1.I1.i2.p1.1.6" class="ltx_text ltx_framed ltx_framed_underline">e</span>arning—the first open source<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://gitlab.ewi.tudelft.nl/dmls/publications/freddie" title="" class="ltx_ref ltx_href">https://gitlab.ewi.tudelft.nl/dmls/publications/freddie</a></span></span></span>
framework that addresses these requirements.
<span id="S1.I1.i2.p1.1.7" class="ltx_text ltx_font_typewriter">Freddie</span> supports small scale deployments, i.e., single machine simulations, and large scale emulation over self-managed and cloud systems using Kubernetes. <span id="S1.I1.i2.p1.1.8" class="ltx_text ltx_font_typewriter">Freddie</span> enables the emulation of both data and resource heterogeneity.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide benchmarking generating methods for FCL that explore both data and task heterogeneity across clients —realistic workloads tailored for Federated Continual Learning systems.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning (FL)</span>. Existing FL frameworks support a fixed set of learning tasks across clients. Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> provides a client-server framework that needs to be manually started on different devices. Differently, Fate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> focuses on providing a secure and production-ready federated learning setup.
Fate supports Kubernetes deployments but requires the use of its pipelines to run experiments.
Although this might provide desirable security additions for production, it also tends less to prototyping and active research needs.
Besides research endeavours, popular deep learning platforms such as TorchX and Tensorflow Federated
can respectively run distributed and federated experiments at scale, but
they lack the flexibility to use other ML libraries. 
<br class="ltx_break"></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Continual Learning (CL)</span>. FACIL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, PyCIL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and Pycontinual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> provide CL frameworks and CL algorithms such as LwF, iCaRL, EWC, and GEM.
Continual World <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> adds a simulation world for robotics tasks for Continual Reinforcement Learning.
Avalanche <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is focused on reproducible End-to-End Continual Learning. The aforementioned frameworks support CL only on a single machine. FedWEIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> combines parameter isolation and regularization and extends CL to a federated setting.
However, it does not consider the impact of task sequences on the global model’s quality.
Last but not least, current FL frameworks cannot be easily extended to support CL scenarios where the output types evolve.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Continual Learning methods leverage task IDs during training and evaluation, enabling the exploitation of a specific set of model weights or restricting the output classes based on a task’s ID. This is also known as Task-Interactive Learning (<em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">task-IL</em>) and Domain-Interactive Learning (<em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">domain-IL</em>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which are both supported by <span id="S2.p3.1.3" class="ltx_text ltx_font_typewriter">Freddie</span>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Specifying Data Heterogeneity and Learning Workloads</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section briefly surveys system requirements addressed by <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">Freddie</span>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Classical FL parameters.</span>
The number of clients, the federators’ aggregation and client selection strategies must be configurable.
In addition, FL-related and common hyperparameters, such as the training epochs, learning rates, etc., must be configurable.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Statistical heterogeneity.</span> As in FL, local data distributions remain of high importance, and their non-iidness should be configurable. In the context of FCL, local distributions also limit the tasks that clients might be able to train for.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Resource heterogeneity.</span> It should be possible to specify the computing power of the clients and of the federator, and the characteristics (latency, throughput) of the network links that interconnect them.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Task description.</span>
The task sequence of each client can be specified. Non-IID task distribution can be assimilated to the situation where clients learn tasks with high intra-task variance, e.g. due to different domains. In such settings, it is often unclear how the quality of current CL methods is impacted by aggregation.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.02015/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.4.2" class="ltx_text" style="font-size:90%;">Overview of <span id="S3.F1.4.2.1" class="ltx_text ltx_font_typewriter">Freddie</span>. An Orchestrator and an Extractor are respectively used for deploying experiments and collecting data.
Experiments are run as TrainJobs managed by Kubeflow Training Operators.
Within such a job, the experiment is controlled by the federator and learned by the clients.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_typewriter">Freddie</span>: A Framework for Reproducible FCL Research</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, for space reasons, we focus on <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">Freddie</span>’s implementation based on containerization and orchestration methods, and on its support for FCL. 
<br class="ltx_break"></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Kubernetes and containers.</span>
Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Specifying Data Heterogeneity and Learning Workloads ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a high-level overview of <span id="S4.p2.1.2" class="ltx_text ltx_font_typewriter">Freddie</span>.
The Orchestrator provides functionality to kick-off experiments, in turn, deployed on the cluster by Kubeflow’s training operators.
The extractor provides volumes for experiments to write to provided by an NFS provisioner and server.
Experiments themselves are performed by federator and client nodes. The overall flow of the federated learning system with <span id="S4.p2.1.3" class="ltx_text ltx_font_typewriter">Freddie</span> is as follows.
First, the user submits an experiment description of the system and hyper-parameters.
Following this, the Orchestrator deploys and monitors the experiment.
This design allows the user to scale his experiment up from small-scale prototyping with minimal effort.
The Extractor allows users to store and retrieve experiment statistics and artefacts created by the federator or clients.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The communication between any two parties in the system is asynchronous, allowing the development of FL systems with non-blocking federator-client interactions.
With this flexibility, clients can run in Kubernetes clusters and on individual machines. To allow users to describe their experiment as distributed code, we rely on Kubeflow’s training operators, which provide a means to set up distributed learning on Kubernetes with popular ML libraries. 
<br class="ltx_break"></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Novel support for FCL.</span>
<span id="S4.p4.1.2" class="ltx_text ltx_font_typewriter">Freddie</span> supports the SOTA algorithms for FCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and common CL methods such as EWC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and GEM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
For CL, <span id="S4.p4.1.3" class="ltx_text ltx_font_typewriter">Freddie</span> implements Task-IL and Domain-IL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> through the use of sliding, expanding, and full window mechanisms.
A sliding window restricts the output classes only to those of the task evaluated at a time <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">t</annotation></semantics></math>.
Expanding windows do not utilize the task IDs to make any such restriction, so the output classes include all classes learned until that time. A full window does not restrict outputs based on the task and can be used in the standard federated learning scenario.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The added complexity of FCL allows for workloads over the same set of tasks to produce different results.
We devise three different schemes that partition tasks differently, and that can be used to evaluate a FCL scheme over a representative set of scenarios.
We discuss these three schemes, which we coin Column, Balanced, and Shuffled, respectively. <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">Column</span>, as shown in Fig. <a href="#S4.F2.sf1" title="In Figure 2 ‣ 4 Freddie: A Framework for Reproducible FCL Research ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, all clients handle tasks in the same order.
Thereby naively splitting the CL workload across clients, resulting in a significant expected catastrophic forgetting effect. <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">Balanced</span>, as depicted in Fig. <a href="#S4.F2.sf2" title="In Figure 2 ‣ 4 Freddie: A Framework for Reproducible FCL Research ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, aims to prevent catastrophic forgetting by organizing the tasks so that different clients train on them across multiple steps.
We propose a partitioning scheme to lessen the effect of catastrophic forgetting, resulting in a task being trained on by at most one client.
While this scheme addresses short term-forgetting, long-term forgetting may still occur.
<span id="S4.p5.1.3" class="ltx_text ltx_font_italic">Shuffled</span>, see Fig. <a href="#S4.F2.sf3" title="In Figure 2 ‣ 4 Freddie: A Framework for Reproducible FCL Research ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>, randomly orders each client’s task, thereby relying on pseudo-randomness in conjunction with a pre-specified seed.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02015/assets/x2.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="142" height="66" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Column-wise.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02015/assets/x3.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="77" height="47" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Balanced.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02015/assets/x4.png" id="S4.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="77" height="47" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Shuffled.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Column, balanced and shuffled task partition schemes for CL.</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Performance evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We demonstrate some features of <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">Freddie</span> through experiments. For this purpose, we use the overlapping CIFAR100 dataset. The labels that already exist in CIFAR100 are used to partition the data into different tasks for FCL, following the same steps as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
FL can be viewed as a corner case of FCL where there is a single task consisting of the whole dataset. We first consider a FL scenario with the default version of CIFAR100, and then consider the overlapping CIFAR100 split into 10 separate tasks in a FCL scenario. We use the average accuracy metric following the CL literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
<br class="ltx_break"></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Scalability.</span> To investigate <span id="S5.p2.1.2" class="ltx_text ltx_font_typewriter">Freddie</span>’s emulation capability, we perform a <em id="S5.p2.1.3" class="ltx_emph ltx_font_italic">small</em> and <em id="S5.p2.1.4" class="ltx_emph ltx_font_italic">large</em> scale experiment on a Google Kubernetes Engine (GKE) cluster to cover possible use cases.
During deployment, the pods of the federation and clients were run on a separate node pool scaled to meet each experiment’s requirements.
We study the performance of an FL experiment emulated on a CPU-enabled Kubernetes cluster, where multiple clients may run on a single Kubernetes node.
Parameters of the experiments are provided in Tab. <a href="#S5.T1" title="Table 1 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">System and hyperparameters used in ‘small’ and ‘scale’ experiments. All experiments were run on ‘e2-standard-8’ nodes.</span></figcaption>
<table id="S5.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.4.1.1" class="ltx_tr">
<td id="S5.T1.4.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S5.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">System</th>
<th id="S5.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Federator (F)</th>
<th id="S5.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Clients (C)</th>
</tr>
<tr id="S5.T1.4.2.2" class="ltx_tr">
<td id="S5.T1.4.2.2.1" class="ltx_td"></td>
<th id="S5.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Nodes</th>
<th id="S5.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#C</th>
<th id="S5.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S5.T1.4.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.4.2.2.4.1.1" class="ltx_tr">
<td id="S5.T1.4.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CPU</td>
</tr>
<tr id="S5.T1.4.2.2.4.1.2" class="ltx_tr">
<td id="S5.T1.4.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(F/C)</td>
</tr>
</table>
</th>
<th id="S5.T1.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S5.T1.4.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.4.2.2.5.1.1" class="ltx_tr">
<td id="S5.T1.4.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Memory</td>
</tr>
<tr id="S5.T1.4.2.2.5.1.2" class="ltx_tr">
<td id="S5.T1.4.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(F/C)</td>
</tr>
</table>
</th>
<th id="S5.T1.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Strategy</th>
<th id="S5.T1.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S5.T1.4.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.4.2.2.7.1.1" class="ltx_tr">
<td id="S5.T1.4.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">#Rounds</td>
</tr>
<tr id="S5.T1.4.2.2.7.1.2" class="ltx_tr">
<td id="S5.T1.4.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(R)</td>
</tr>
</table>
</th>
<th id="S5.T1.4.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#C/R</th>
<th id="S5.T1.4.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Model</th>
<th id="S5.T1.4.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Data</th>
<th id="S5.T1.4.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">BS</th>
</tr>
<tr id="S5.T1.4.3.3" class="ltx_tr">
<td id="S5.T1.4.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Small</td>
<td id="S5.T1.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">2,2,3</td>
<td id="S5.T1.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">5,10,20</td>
<td id="S5.T1.4.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T1.4.3.3.4.1" class="ltx_text">2/1</span></td>
<td id="S5.T1.4.3.3.5" class="ltx_td ltx_align_center ltx_border_t">2/2G</td>
<td id="S5.T1.4.3.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T1.4.3.3.6.1" class="ltx_text">FedAvg</span></td>
<td id="S5.T1.4.3.3.7" class="ltx_td ltx_align_center ltx_border_t">100</td>
<td id="S5.T1.4.3.3.8" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S5.T1.4.3.3.9" class="ltx_td ltx_align_center ltx_border_t">LeNet</td>
<td id="S5.T1.4.3.3.10" class="ltx_td ltx_align_center ltx_border_t">CIFAR10</td>
<td id="S5.T1.4.3.3.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T1.4.3.3.11.1" class="ltx_text">64</span></td>
</tr>
<tr id="S5.T1.4.4.4" class="ltx_tr">
<td id="S5.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb">Scale</td>
<td id="S5.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">4,12</td>
<td id="S5.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">25,75</td>
<td id="S5.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">2/2,6G</td>
<td id="S5.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">85</td>
<td id="S5.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb">all</td>
<td id="S5.T1.4.4.4.7" class="ltx_td ltx_align_center ltx_border_bb">ResNet</td>
<td id="S5.T1.4.4.4.8" class="ltx_td ltx_align_center ltx_border_bb">CIFAR100</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The <em id="S5.p3.1.1" class="ltx_emph ltx_font_italic">small</em> experiment in Fig. <a href="#S5.F3.sf1" title="In Figure 3 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> depicts the spread round times of clients (scaled) and the federator, with 5 selected clients per round.
The client round duration is scaled by the number of clients (World Size WS) (<math id="S5.p3.1.m1.1" class="ltx_Math" alttext="|\mathcal{D}_{Cifar}|/\text{WS}" display="inline"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mrow id="S5.p3.1.m1.1.1.1.1" xref="S5.p3.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.p3.1.m1.1.1.1.1.2" xref="S5.p3.1.m1.1.1.1.2.1.cmml">|</mo><msub id="S5.p3.1.m1.1.1.1.1.1" xref="S5.p3.1.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p3.1.m1.1.1.1.1.1.2" xref="S5.p3.1.m1.1.1.1.1.1.2.cmml">𝒟</mi><mrow id="S5.p3.1.m1.1.1.1.1.1.3" xref="S5.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S5.p3.1.m1.1.1.1.1.1.3.2" xref="S5.p3.1.m1.1.1.1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1.1.1.3.1" xref="S5.p3.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.1.1.1.3.3" xref="S5.p3.1.m1.1.1.1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1.1.1.3.1a" xref="S5.p3.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.1.1.1.3.4" xref="S5.p3.1.m1.1.1.1.1.1.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1.1.1.3.1b" xref="S5.p3.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.1.1.1.3.5" xref="S5.p3.1.m1.1.1.1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1.1.1.3.1c" xref="S5.p3.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.1.1.1.3.6" xref="S5.p3.1.m1.1.1.1.1.1.3.6.cmml">r</mi></mrow></msub><mo stretchy="false" id="S5.p3.1.m1.1.1.1.1.3" xref="S5.p3.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">/</mo><mtext id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3a.cmml">WS</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><divide id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2"></divide><apply id="S5.p3.1.m1.1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.1.1"><abs id="S5.p3.1.m1.1.1.1.2.1.cmml" xref="S5.p3.1.m1.1.1.1.1.2"></abs><apply id="S5.p3.1.m1.1.1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S5.p3.1.m1.1.1.1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.1.1.1.2">𝒟</ci><apply id="S5.p3.1.m1.1.1.1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3"><times id="S5.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.1"></times><ci id="S5.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.2">𝐶</ci><ci id="S5.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.3">𝑖</ci><ci id="S5.p3.1.m1.1.1.1.1.1.3.4.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.4">𝑓</ci><ci id="S5.p3.1.m1.1.1.1.1.1.3.5.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.5">𝑎</ci><ci id="S5.p3.1.m1.1.1.1.1.1.3.6.cmml" xref="S5.p3.1.m1.1.1.1.1.1.3.6">𝑟</ci></apply></apply></apply><ci id="S5.p3.1.m1.1.1.3a.cmml" xref="S5.p3.1.m1.1.1.3"><mtext id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">WS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">|\mathcal{D}_{Cifar}|/\text{WS}</annotation></semantics></math>) to account for differences in clients’ datasets as the WS increases.
The outliers in the plots originate from the first epoch run on clients, which are inherently slower due to loading data into memory.
Nevertheless, it is expected that the scaled client duration stays relatively constant, while the result shows an increase as the number of clients increases (from 115 to 123 s, and from 136 to 138 s).
Similarly, the federator sees a positive correlation between round duration and WS.
The number of co-scheduled clients on the same node can explain this trend, as the networking overhead stays the same.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">For the <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">scale</span> experiments, we provide the round time density estimate in Fig. <a href="#S5.F3.sf2" title="In Figure 3 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>. The client round times exhibit the same range of processing times that were observed in the ‘small’ setting.
In both settings, participating clients in each round may run on the same node, varying from 3 to 7 clients per node.
We use similar settings in the ‘small’ configuration that involves 20 clients, where 4 nodes are used.
As such, confirming that resource contingencies due to co-scheduling will likely cause the increased client round time with 20 clients.
The different modes within the client’s round duration can be explained by imperfect data splits and the imbalanced assignment of the number of clients to be co-scheduled with the federator.
The federator’s density estimate shows a similar pattern with two distinct modes.
With the cluster configurations employed, i.e., 4 and 12 nodes, it is possible for the federator to be co-scheduled on a machine with different numbers of clients.
As a result, the federator experiences a variable level of resource contingency.
However, an increase in the two modes is visible as the number of clients increases, which is expected due to the increased communication volumes. 
<br class="ltx_break"></p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02015/assets/x5.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Experiments with 5 participating clients per round (n=1).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02015/assets/x6.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Scaled experiments where all clients participate in each round (n=3).</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.4.2" class="ltx_text" style="font-size:90%;">Client and federator round durations with <span id="S5.F3.4.2.1" class="ltx_text ltx_font_typewriter">Freddie</span> for small (5-20 clients, LeNet5 &amp; CIFAR10) and large scale experiment (ResNet-18 &amp; CIFAR100). Client durations are scaled by the total number of clients (WS).
</span></figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.2" class="ltx_p"><span id="S5.p5.2.1" class="ltx_text ltx_font_bold">Task-IL vs Domain-IL.</span>
Assuming that the model knows the ID of the task it is currently training on, or evaluating, increases its accuracy.
For FCL, Task-Interactive Learning and Domain-Interactive Learning are implemented using the sliding and expanding-window, respectively. Let us recall that sliding-windows use task IDs, contrary to expanding-windows.
For the overlapping CIFAR100 dataset, if one assumes that the task ID is known, then the number of output classes is restricted to only the five sub-classes within that task.
Thus leading to higher average task probabilities for Task-IL scenarios.
This difference is prevalent in Fig. <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>.
Under the expanding window scheme, classification outputs one of <math id="S5.p5.1.m1.1" class="ltx_Math" alttext="5T" display="inline"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mn id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><times id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"></times><cn type="integer" id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">5</cn><ci id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">5T</annotation></semantics></math> classes, where <math id="S5.p5.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S5.p5.2.m2.1a"><mi id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><ci id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">T</annotation></semantics></math> is the number of tasks learned until evaluation time.
Therefore, the probability of classifying correctly is even lower than in the sliding window scenario.
Fig. <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> shows the positive impact of using a task ID on accuracy.
Using sliding-window results in higher accuracy than expanding-window, which sometimes has to be used because of the application use case.
Because of this difference, <span id="S5.p5.2.2" class="ltx_text ltx_font_typewriter">Freddie</span> supports both Task-IL and Domain-IL.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2406.02015/assets/x7.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="214" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Reduced catastrophic forgetting effect window (Task-IL) compared to expanding window (Domain-IL).
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2406.02015/assets/x8.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="167" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Impact of partition scheme on evaluation accuracy (10 tasks of overlapping CIFAR100, LeNet-5, 10 rounds of 2 epochs per task).</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Impact of task heterogeneity on FCL.</span></figcaption>
</figure>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">FCL Task Heterogeneity.</span>
As discussed in Section <a href="#S3" title="3 Specifying Data Heterogeneity and Learning Workloads ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, tasks can be processed in different orders at each client.
To demonstrate the different effects that different sequences of tasks produce, we implement the Overlapped-CIFAR100 dataset with 20 tasks that can be used for FCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
The accuracy in Fig. <a href="#S5.F4.sf2" title="In Figure 4 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> is calculated as the average accuracy of all tasks seen until that point, resulting in expected ‘drops’ in accuracy as new tasks are introduced.
Indeed, the learning curves in Fig. <a href="#S5.F4.sf2" title="In Figure 4 ‣ 5 Performance evaluation ‣ Parameterizing Federated Continual Learning for Reproducible Research" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> show noticeable drops over time.
However, different trends are visible between workloads. The column scheme suffers more from more pronounced <em id="S5.p7.1.2" class="ltx_emph ltx_font_italic">catastrophic forgetting</em> than the shuffled and balanced scheme, resulting in lower accuracy.
We observe that the column scheme, on average, results in a 4% test accuracy drop compared to the column and shuffled schemes.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We presented <span id="S6.p1.1.1" class="ltx_text ltx_font_typewriter">Freddie</span>, the first framework for reproducible Federated Continual Learning research, which is motivated by the increasing importance of Federated and Continual Learning.
<span id="S6.p1.1.2" class="ltx_text ltx_font_typewriter">Freddie</span>’s deployment abilities on different platforms, scalability with the number of clients, and support for data and task heterogeneity provide FL practitioners with a powerful tool.
Our experimental results showcase previously unaddressed performance issues that Federated Continual Learning systems might face: severe catastrophic forgetting in different task heterogeneity settings.
<span id="S6.p1.1.3" class="ltx_text ltx_font_typewriter">Freddie</span> is open-source, and will soon be extended to support new CL datasets, algorithms, and generative models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., et al.: Flower: A friendly federated
learning research framework. CoRR <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">abs/2007.14390</span> (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Chaudhry, A., Ranzato, M., Rohrbach, M., et al.: Efficient lifelong learning
with A-GEM. In: ICLR (2019)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
FedAI: Fate (2019), <a target="_blank" href="https://fate.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fate.fedai.org/</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ke, Z., Liu, B., Ma, N., et al.: Achieving forgetting prevention and knowledge
transfer in continual learning. In: NeurIPS (2021)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Kirkpatrick, J., Pascanu, R., Rabinowitz, N.C., et al.: Overcoming catastrophic
forgetting in neural networks. CoRR <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">abs/1612.00796</span> (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Lange, M.D., Aljundi, R., Masana, M., et al.: Continual learning: A
comparative study on how to defy forgetting in classification tasks. CoRR
<span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">abs/1909.08383</span> (2019)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lomonaco, V., Pellegrini, L., Cossu, A., et al.: Avalanche: an end-to-end
library for continual learning. In: CVPR (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Lopez-Paz, D., Ranzato, M.A.: Gradient episodic memory for continual learning.
In: NeurIPS (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ma, Y., Yu, D., Wu, T., et al.: PaddlePaddle: An open-source deep learning
platform from industrial practice. Frontiers of Data and Computing
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">1</span>(1), 105–115 (2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mallya, A., Lazebnik, S.: Packnet: Adding multiple tasks to a single network by
iterative pruning. In: CVPR (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Masana, M., Liu, X., Twardowski, B., Menta, M., et al.: Class-incremental
learning: Survey and performance evaluation on image classification. IEEE
Trans. Pattern Anal. Mach. Intell. <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">45</span>(5), 5513–5533 (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., et al.: Communication-efficient learning of
deep networks from decentralized data. In: Artificial intelligence and
statistics. pp. 1273–1282. PMLR (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Mirzadeh, S.I., Farajtabar, M., Pascanu, R., et al.: Understanding the role of
training regimes in continual learning. NeurIPS (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Reina, G.A., Gruzdev, A., Foley, P., et al.: Openfl: An open-source framework
for federated learning. CoRR <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">abs/2105.06413</span> (2021)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Shin, H., Lee, J.K., Kim, J., Kim, J.: Continual learning with deep generative
replay. In: NeurIPS. pp. 2990–2999 (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
van de Ven, G.M., Tolias, A.S.: Three scenarios for continual learning.
CoRR <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">abs/1904.07734</span> (2019)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Wołczyk, M., Zajkac, M., Pascanu, R., et al.: Continual world: A robotic
benchmark for continual reinforcement learning. In: NeurIPS (2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yoon, J., Jeong, W., Lee, G., et al.: Federated continual learning with
weighted inter-client transfer. In: ICML (2021)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Zhou, D.W., Wang, F.Y., Ye, H.J., et al.: Pycil: A python toolbox for
class-incremental learning. CoRR <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">abs/2112.12533</span> (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.02014" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.02015" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.02015">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.02015" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.02016" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:17:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
