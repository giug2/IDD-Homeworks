<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.11325] PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation</title><meta property="og:description" content="In order to cope with the increasing demand for labeling data and privacy issues with human detection, synthetic data has been used as a substitute and showing promising results in human detection and tracking tasks. W…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.11325">

<!--Generated on Wed Mar 13 15:33:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\DeclareAcronym</span>
<p id="p1.2" class="ltx_p">FPN
short=FPN,
long=feature pyramid network

<span id="p1.2.1" class="ltx_ERROR undefined">\DeclareAcronym</span>PAN
short=PAN,
long=path aggregation network

<span id="p1.2.2" class="ltx_ERROR undefined">\DeclareAcronym</span>NMS
short=NMS,
long=non-maximum suppression

<span id="p1.2.3" class="ltx_ERROR undefined">\DeclareAcronym</span>GSI
short=GSI,
long=Gaussian-smoothed interpolation

<span id="p1.2.4" class="ltx_ERROR undefined">\DeclareAcronym</span>FPs
short=FPs,
long=false positives

<span id="p1.2.5" class="ltx_ERROR undefined">\DeclareAcronym</span>FNs
short=FNs,
long=false negatives

<span id="p1.2.6" class="ltx_ERROR undefined">\DeclareAcronym</span>IDs
short=IDs,
long=ID switches

 
</p>
</div>
<h1 class="ltx_title ltx_title_document">PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yirui Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shenghua He
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Youbao Tang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingyu Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Honghao Zhou
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sanliang Hong
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junjie Liang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanxin Huang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ning Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruei-Sung Lin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mei Han 
<br class="ltx_break">PAII Inc. &amp; Ping An Bank Co
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ltd.
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In order to cope with the increasing demand for labeling data and privacy issues with human detection, synthetic data has been used as a substitute and showing promising results in human detection and tracking tasks. We participate in the 7th Workshop on Benchmarking Multi-Target Tracking (BMTT), themed on ”How Far Can Synthetic Data Take us”? Our solution, <span id="id1.id1.1" class="ltx_text ltx_font_bold">PieTrack</span>, is developed based on synthetic data without using any pre-trained weights. We propose a self-supervised domain adaptation method that enables mitigating the domain shift issue between the synthetic (<span id="id1.id1.2" class="ltx_ERROR undefined">\eg</span>, <em id="id1.id1.3" class="ltx_emph ltx_font_italic">MOTSynth</em>) and real data (<span id="id1.id1.4" class="ltx_ERROR undefined">\eg</span>, <em id="id1.id1.5" class="ltx_emph ltx_font_italic">MOT17</em>) without involving extra human labels. By leveraging the proposed multi-scale ensemble inference, we achieved a final HOTA score of <span id="id1.id1.6" class="ltx_text ltx_font_bold">58.7</span> on the MOT17 testing set, ranked third place in the challenge.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multiple object tracking (MOT) is an active research area with profound applications. With more advanced and bigger deep learning-based models, the need of labeled real data has also been increased greatly. This is especially the case in human-centered applications such as detection, tracking, action recognition and spatial-temporal localization, etc. Although labeling people in clear background with large-scale bounding-box is not difficult, precising labelling of people in small-scale with occlusion is not easy and could be error-prone. Meanwhile, there has been an increasing awareness of the privacy protection and avoid exposing real human faces for deep learning applications. This is such an important and ethical precaution but difficult in practice due to the face obfuscation (blurring, mosaicing, etc.) still far from practical in most human detection-related applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the other hand, the synthetic data has served as a good alternative in certain applications such as autonomous-driven vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, with virtual driving datasets such as virtual KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, GTA5<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, VIPER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Part of the success with the virtual driving datasets is due to the rigid body of cars which does not have flexible limbs like human being.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Last year (2021), thanks to the advanced game engine with physics simulation as well as human body simulation, MOTSynth, a large-scale synthetic data has been released for the multiple object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This dataset benefits the community in multiple areas of research and applications including pedestrian detection, tracking, re-identification, and instance segmentation.
Such a synthetic data drew our attention because of its advantages in both labeling effort and privacy preservation. In particular, we are delighted to be part of the 7th BMTT Challenges “MOTSynth-MOT-CVPR22”, and our participating approach has the following highlights:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">A two-stage detection and association strategy is adopted, where our detector is trained using synthetic data from scratch without pre-trained weights.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">Self-supervised domain adaptation approach is used to fill the gap between the synthetic trained detector and the real MOT data, by using <span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">only</span> unlabeled MOT17 training split.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">Multi-scale ensemble inference by taking different resolutions as input is used. This strategy helps to further boost the performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Tracking-by-Detection.</span> Powerful detection capability thanks to the deep-learning models, enables better tracking performance in the MOT domain. One-stage approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, tackle the detection and the tracking tasks together in an end-to-end fashion and achieves high-speed process. However, this group of approaches suffers a lower performance than the two-stage because of its compromise between detection and tracking branches of the deep learning model. Recent developments of YOLO-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> provides high-speed, high-accuracy detectors. These advances facilitate the two-stage approaches by striking a balance between the detection speed and the data association <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Synthetic Training Data.</span> Autonomous driving applications have adopted synthetic data in its model training since early time. Virtual KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> cloned the real data to virtual proxy for detection validation. Omni-MOT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> provided a dataset, focuses on car-based detection and tracking. Parallel Domain is a commercial content provider for MOT of cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In the people-centered MOT domain, JTA (Joint Track Auto) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed a dataset on CG human poses for estimation and tracking. MOTSynth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is the most comprehensive dataset for pedestrian-based MOT and have drawn a lot attention since its release.
We are also drawn to this dataset and participated the Challenge using the MOTSynth. Next section presents our methodology for the challenge with detailed explanation and illustration.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we adopt the tracking by detection paradigm and propose an iterative approach to alleviate the domain gap between the synthetic and the real data. To follow the competition rule that <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">no human labels for the MOT17 dataset can be used for fine-tuning</span>, we propose a self-supervised pseudo-label mining and refining procedure on the MOT17 training set to shrink the domain gap. And we also adopt an multi-scale ensemble strategy to improve the robustness during the inference. The overview of the training framework is shown in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.2 Self-supervised Domain Adaptation ‣ 3 Method ‣ PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-Object Detection and Tracking</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Object Detection Network.</span> The proposed PieTrack employs YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> detection network to detect object of interests (<span id="S3.SS1.p1.1.2" class="ltx_ERROR undefined">\eg</span>, pedestrian) in each frame. The network is built upon YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> architecture with modified CSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> backbone and PAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> head to enhance gradient propagation and feature hierarchy. YOLOX decouples the classification and regression tasks into separate branches as they are prone to conflict with each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The detector adopts an anchor-free design to avoid domain-specific anchor configuration and reduce computational cost during inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. To alleviate the imbalance between positive and negative samples, YOLOX consider each <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">3\times 3</annotation></semantics></math> area around the true object centers as positive samples. Moreover, a simplified OTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> strategy, named SimOTA, is proposed to improve the training efficiency without sacrificing the performance. The network can be customized to different sizes and trained in an end-to-end manner. In our work, we adopt YOLOX-X as our detection network and we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to train the detector from scratch <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">without using any pretrained weights</span>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Data Association and Tracking.</span> We adopt a two-stage data association method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> with modifications to partially mitigate the domain shift issue. In the first stage, the association is performed between the high confidence detection boxes and all tracklets. In the second stage, the association is further performed between the unmatches less confident detection boxes and the remain tracklets. Different from the original strategy in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> that excludes lost tracklets from the second association, we propose to consider both tracked and lost tracklets for sensitivity consideration. Because of domain shift issue, we observed significantly increased false negatives. By considering the lost tracklets in both stage, we empirically find it effectively increases the tracking sensitivity by at least <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="1.1\%" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">1.1</mn><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">1.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">1.1\%</annotation></semantics></math>. In addition, we employ NSA Kalman algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as the dynamic motion model and use <span id="S3.SS1.p2.1.2" class="ltx_ERROR undefined">\ac</span>GSI to remediate missing tracklets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Self-supervised Domain Adaptation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Motivations.</span> Domain shift issue is the most challenging part in this task. It is noticeable that patterns, such as pedestrian’s appearance, lighting conditions, distractors, <span id="S3.SS2.p1.1.2" class="ltx_ERROR undefined">\etc</span>, are significantly different between the source domain (<span id="S3.SS2.p1.1.3" class="ltx_ERROR undefined">\ie</span> MOTSynth) and the target domain (<span id="S3.SS2.p1.1.4" class="ltx_ERROR undefined">\ie</span> MOT17), casting a shadow on achieving high detection/tracking performance on the testing scenario. Therefore, in our work, we focus on leveraging self-supervised domain adaptation method to boost the detection performance by using <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_bold">only</span> unlabeled MOT17 training split.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2207.11325/assets/system.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S3.F1.2.1" class="ltx_text ltx_font_bold">Overview of the iterative domain adaptation pipeline</span>. Firstly, the detector is trained on pure MOTSynth dataset. Then we perform the iterative domain adaptation by repeatedly generating and training with the pseudo bounding-box labels of the MOT17 training set.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p"><span id="S3.SS2.p2.6.1" class="ltx_text ltx_font_bold">Domain Adaptation via Iterative Refinement</span>
Domain adaptation is crucial for acquiring good generalization ability on unseen data, especially when there is a domain gap between the training and testing scenario. Recently, training from synthetic data draws many attentions, as it is an efficient way to collect large-scale dataset. In our work, we propose to leverage a simple yet effective self-supervised iterative refinement approach to address the domain shift issue. We firstly train the detector for <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">80</annotation></semantics></math> epochs on the whole MOTSynth dataset (with a subsampling ratio of 10), and we validate the performance on the MOT17 validation half to determine the best model. Then, we start the iterative domain adaptation by firstly using a pre-defined detection confidence threshold (<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="T=0.5" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">T</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑇</ci><cn type="float" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">T=0.5</annotation></semantics></math> for the initial iteration, and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="T=0.1" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">T</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑇</ci><cn type="float" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">T=0.1</annotation></semantics></math> for the rest) to generate pseudo bounding-box labels for the MOT17 training set, and we fine-tune the detector with the pseudo-labels for another <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">N</annotation></semantics></math> (<math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="N=40" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">N</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><eq id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></eq><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">N=40</annotation></semantics></math> for the initial iteration, and <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="N=20" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">N</mi><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><eq id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></eq><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">N=20</annotation></semantics></math> for the rest) epochs. We repeat this process until there is no improvement when validating the model with MOT17 training set true labels. We find this process significantly reduce false negatives and noticeably improve the overall tracking performance, as demonstrated in the Sec. <a href="#S4" title="4 Experiments ‣ PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-scale Ensemble Inference</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">To improve the robustness of detection results during inference, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we employ an ensemble strategy to further boost the model performance. We take three different resolutions as inputs (<span id="S3.SS3.p1.5.1" class="ltx_ERROR undefined">\eg</span>, <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="640\times 1152" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">1152</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">640</cn><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">1152</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">640\times 1152</annotation></semantics></math>, <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="800\times 1440" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">1440</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">800</cn><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">1440</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">800\times 1440</annotation></semantics></math>, and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1280\times 2304" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">1280</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">2304</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">1280</cn><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">2304</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1280\times 2304</annotation></semantics></math>), and augment each input by horizontal flipping. This creates three pairs of inputs for the detection network. We design a strategy to make the input resolution compatible with the predicted box size. To be specific, we only allow the lowest resolution input to predict boxes that larger than <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mn id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><times id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></times><cn type="integer" id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">32</cn><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">32\times 32</annotation></semantics></math>, and restrict the largest input to produce boxes that smaller than <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="96\times 96" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mn id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><times id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">96</cn><cn type="integer" id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">96\times 96</annotation></semantics></math>. All box predictions from the medium resolution inputs are used. And we collect all the legal predictions together and perform <span id="S3.SS3.p1.5.2" class="ltx_ERROR undefined">\ac</span>NMS at once. The effectiveness of this multi-scale ensemble strategy is demonstrated in Sec. <a href="#S4" title="4 Experiments ‣ PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset and Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">MOTSynth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is a large-scale synthetic dataset for object detection and tracking. It consists of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">768</annotation></semantics></math> sequences and results in <math id="S4.SS1.p1.2.m2.2" class="ltx_Math" alttext="1,382" display="inline"><semantics id="S4.SS1.p1.2.m2.2a"><mrow id="S4.SS1.p1.2.m2.2.3.2" xref="S4.SS1.p1.2.m2.2.3.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">1</mn><mo id="S4.SS1.p1.2.m2.2.3.2.1" xref="S4.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">382</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.2b"><list id="S4.SS1.p1.2.m2.2.3.1.cmml" xref="S4.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">1</cn><cn type="integer" id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">382</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.2c">1,382</annotation></semantics></math>k frames. We employ MOTSynth as the only source of labeled data for the development. For parameter tuning and methodology validation, we use MOT17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> training split as our validation set. We primarily use HOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and MOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to assess the overall tracking performance, and also take <span id="S4.SS1.p1.2.1" class="ltx_ERROR undefined">\ac</span>FPs, <span id="S4.SS1.p1.2.2" class="ltx_ERROR undefined">\ac</span>FNs, and <span id="S4.SS1.p1.2.3" class="ltx_ERROR undefined">\ac</span>IDs as references.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We conduct ablation study to reveal the effectiveness of the proposed iterative domain adaptation and multi-scale ensemble inference. As demonstrated in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the baseline method that trained on the MOTSynth labeled dataset produces large amount of false negatives when testing on the MOT17 training set. We hypotheses that this poor performance stems from the domain shift issue as described in Sec. <a href="#S3.SS2" title="3.2 Self-supervised Domain Adaptation ‣ 3 Method ‣ PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Therefore, we further conduct iterative domain adaptation to improve the sensitivity of the model. As can be seen, we repeat this process twice and obtain consistent and significant improvements over the baseline method. By introducing the multi-scale ensemble inference, we further boost the HOTA score to <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="57.68" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">57.68</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="float" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">57.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">57.68</annotation></semantics></math>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.2.1" class="ltx_text ltx_font_bold">Ablation study on the iterative domain adaptation and the multi-scale ensemble learning on MOT17 training set</span>.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.3.1" class="ltx_tr">
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="S4.T1.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.2.1.1" class="ltx_p" style="width:28.5pt;">HOTA</span>
</span>
</td>
<td id="S4.T1.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.3.1.1" class="ltx_p" style="width:28.5pt;">MOTA</span>
</span>
</td>
<td id="S4.T1.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.4.1.1" class="ltx_p" style="width:28.5pt;">FP</span>
</span>
</td>
<td id="S4.T1.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.5.1.1" class="ltx_p" style="width:28.5pt;">FN</span>
</span>
</td>
<td id="S4.T1.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.6.1.1" class="ltx_p" style="width:28.5pt;">IDs</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.2" class="ltx_tr">
<td id="S4.T1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">Baseline</td>
<td id="S4.T1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.2.1.1" class="ltx_p" style="width:28.5pt;">52.87</span>
</span>
</td>
<td id="S4.T1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.3.1.1" class="ltx_p" style="width:28.5pt;">60.50</span>
</span>
</td>
<td id="S4.T1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.4.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.3.2.4.1.1.1" class="ltx_text ltx_font_bold">10553</span></span>
</span>
</td>
<td id="S4.T1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.5.1.1" class="ltx_p" style="width:28.5pt;">33258</span>
</span>
</td>
<td id="S4.T1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.6.1.1" class="ltx_p" style="width:28.5pt;">514</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Iter.1</td>
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.2.1.1" class="ltx_p" style="width:28.5pt;">55.66</span>
</span>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.3.1.1" class="ltx_p" style="width:28.5pt;">63.9</span>
</span>
</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.4.1.1" class="ltx_p" style="width:28.5pt;">11426</span>
</span>
</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.5.1.1" class="ltx_p" style="width:28.5pt;">28643</span>
</span>
</td>
<td id="S4.T1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.6.1.1" class="ltx_p" style="width:28.5pt;">517</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.4" class="ltx_tr">
<td id="S4.T1.3.4.1" class="ltx_td ltx_align_left">Iter.2</td>
<td id="S4.T1.3.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.2.1.1" class="ltx_p" style="width:28.5pt;">56.82</span>
</span>
</td>
<td id="S4.T1.3.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.3.1.1" class="ltx_p" style="width:28.5pt;">63.8</span>
</span>
</td>
<td id="S4.T1.3.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.4.1.1" class="ltx_p" style="width:28.5pt;">11574</span>
</span>
</td>
<td id="S4.T1.3.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.5.1.1" class="ltx_p" style="width:28.5pt;">28569</span>
</span>
</td>
<td id="S4.T1.3.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.3.4.6.1.1.1" class="ltx_text ltx_font_bold">456</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.3.5" class="ltx_tr">
<td id="S4.T1.3.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Ensemble</td>
<td id="S4.T1.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.3.5.2.1.1.1" class="ltx_text ltx_font_bold">57.68</span></span>
</span>
</td>
<td id="S4.T1.3.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.3.5.3.1.1.1" class="ltx_text ltx_font_bold">64.30</span></span>
</span>
</td>
<td id="S4.T1.3.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.4.1.1" class="ltx_p" style="width:28.5pt;">15528</span>
</span>
</td>
<td id="S4.T1.3.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.3.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.3.5.5.1.1.1" class="ltx_text ltx_font_bold">23985</span></span>
</span>
</td>
<td id="S4.T1.3.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.3.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.6.1.1" class="ltx_p" style="width:28.5pt;">534</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>MOTSynth Challenge Performance</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We use the best model obtained by the iterative domain adaptation to perform inference the MOT17 testing split. We submit the results to the MOTChallenge platform, which yields a HOTA score of <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">58.7</span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present our approach for 7th BMTT’s Challenge on MOTSynth-MOT-CVPR22. We adopt tracking-by-detection paradigm with two-stage approach, where detector is trained from scratch using MOTSynth data. Domain adaptation uses self-supervised fashion where only the unlabeled MOT17 training data is used. We also further boosts our performance with multiple-scale ensemble inference. We achieved a HOTA score of <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">58.7</span> as the final ranking result showing on the challenge website.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixé.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Tracking without bells and whistles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">,
October 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Keni Bernardin and Rainer Stiefelhagen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Evaluating multiple object tracking performance: the clear mot
metrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">EURASIP Journal on Image and Video Processing</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2008:1–10,
2008.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Yolov4: Optimal speed and accuracy of object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.10934</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Ángela Casado-García and Jónathan Heras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Ensemble methods for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECAI 2020</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 2688–2695. IOS Press, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Chi-Tung Cheng, Yirui Wang, Huan-Wu Chen, Po-Meng Hsiao, Chun-Nan Yeh, Chi-Hsun
Hsieh, Shun Miao, Jing Xiao, Chien-Hung Liao, and Le Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">A scalable physician-level deep learning algorithm detects universal
trauma on pelvic radiographs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature communications</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 12(1):1–10, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yunhao Du, Yang Song, Bo Yang, and Yanyun Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Strongsort: Make deepsort great again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.13514</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yunhao Du, Junfeng Wan, Yanyun Zhao, Binyu Zhang, Zhihang Tong, and Junhao
Dong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Giaotracker: A comprehensive framework for mcmot with global
information and optimizing strategies in visdrone 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 2809–2819, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Matteo Fabbri, Guillem Brasó, Gianluca Maugeri, Orcun Cetintas, Riccardo
Gasparini, Aljoša Ošep, Simone Calderara, Laura Leal-Taixé,
and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Motsynth: How can synthetic data help pedestrian detection and
tracking?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 10849–10859, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto Vezzani,
and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning to detect and track visible and occluded body joints in a
virtual world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 430–446, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Virtual worlds as proxy for multi-object tracking analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4340–4349, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Ota: Optimal transport assignment for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 303–312, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Yolox: Exceeding yolo series in 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.08430</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Chen-I Hsieh, Kang Zheng, Chihung Lin, Ling Mei, Le Lu, Weijian Li, Fang-Ping
Chen, Yirui Wang, Xiaoyun Zhou, Fakai Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Automated bone mineral density prediction and fracture risk
assessment using plain radiographs via deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature communications</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 12(1):1–9, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp Krähenbühl,
Trevor Darrell, and Fisher Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Joint monocular 3d vehicle detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Path aggregation network for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 8759–8768, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger,
Laura Leal-Taixé, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Hota: A higher order metric for evaluating multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 129(2):548–578,
2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Mot16: A benchmark for multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1603.00831</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai,
Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Chained-tracker: Chaining paired attentive regression results for
end-to-end joint multiple-object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.02767</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Playing for benchmarks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 2213–2222, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 102–118.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Guanglu Song, Yu Liu, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Revisiting the sibling head in object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 11563–11572, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
ShiJie Sun, Naveed Akhtar, XiangYu Song, HuanSheng Song, Ajmal Mian, and
Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Simultaneous detection and tracking with motion modelling for
multiple object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 626–643.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Learning to track with object permanence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 10860–10869, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei
Hsieh, and I-Hau Yeh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Cspnet: A new backbone that can enhance learning capability of cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition workshops</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 390–391, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, and Yun
Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Rethinking classification and localization for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 10186–10195, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">A study of face obfuscation in imagenet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.06191</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu,
and Xinggang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Bytetrack: Multi-object tracking by associating every detection box.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.06864</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Tracking objects as points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.11324" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.11325" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.11325">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.11325" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.11326" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 15:33:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
