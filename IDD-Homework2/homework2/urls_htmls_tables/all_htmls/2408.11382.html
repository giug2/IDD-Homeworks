<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models</title>
<!--Generated on Wed Aug 21 07:21:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.11382v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S1" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S2" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S2.SS0.SSS0.Px1" title="In 2 Related Works ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Multilingual Neural Machine Translation (MNMT)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S2.SS0.SSS0.Px2" title="In 2 Related Works ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Positional Embedding Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S2.SS0.SSS0.Px3" title="In 2 Related Works ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Parameter-Efficient Fine-tuning (PEFT)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS1" title="In 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS2" title="In 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS2.SSS1" title="In 3.2 Data ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Train set</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS2.SSS1.Px1" title="In 3.2.1 Train set ‣ 3.2 Data ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS2.SSS1.Px2" title="In 3.2.1 Train set ‣ 3.2 Data ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS2.SSS2" title="In 3.2 Data ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Dev/Test sets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS3" title="In 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Metrics and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS4" title="In 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Finetuning Strategies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS4.SSS0.Px1" title="In 3.4 Finetuning Strategies ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">FFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS4.SSS0.Px2" title="In 3.4 Finetuning Strategies ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">LoRA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS4.SSS0.Px3" title="In 3.4 Finetuning Strategies ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">minLoRA</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S4" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S4.SS1" title="In 4 Experiments ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1" title="In 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Swapping out Positional Embeddings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1.SSS0.Px1" title="In 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Performance Degradation Post-Swap</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1.SSS0.Px2" title="In 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Fine-tuning is key to performance recovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1.SSS0.Px3" title="In 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Efficacy of the Positional Embedding switch</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1.SSS0.Px4" title="In 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">NoPE is not a solution in Encoder-Decoder models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS1.SSS0.Px5" title="In 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title">Minimalistic Fine-tuning is sufficient</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.SS2" title="In 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison with Training from scratch</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S6" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S7" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S8" title="In On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">On the Interchangeability of Positional Embeddings in 
<br class="ltx_break"/>Multilingual Neural Machine Translation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="id1.1.id1" style="width:390.3pt;">
<span class="ltx_p" id="id1.1.id1.1">Varun Gumma  Pranjal A. Chitale  Kalika Bali</span>
<span class="ltx_p ltx_align_center" id="id1.1.id1.2">Microsoft Corporation</span>
<span class="ltx_p ltx_align_center" id="id1.1.id1.3"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1.3.1">varun230999@gmail.com</span></span>
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Standard Neural Machine Translation (NMT) models have traditionally been trained with Sinusoidal Positional Embeddings (PEs), which are inadequate for capturing long-range dependencies and are inefficient for long-context or document-level translation. In contrast, state-of-the-art large language models (LLMs) employ relative PEs, demonstrating superior length generalization. This work explores the potential for efficiently switching the Positional Embeddings of pre-trained NMT models from absolute sinusoidal PEs to relative approaches such as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be effectively replaced with RoPE and ALiBi with negligible or no performance loss, achieved by fine-tuning on a small fraction of high-quality data. Additionally, models trained without Positional Embeddings (NoPE) are not a viable solution for Encoder-Decoder architectures, as they consistently under-perform compared to models utilizing any form of Positional Embedding. Furthermore, even a model trained from scratch with these relative PEs slightly under-performs a fine-tuned model, underscoring the efficiency and validity of our hypothesis.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">On the Interchangeability of Positional Embeddings in 
<br class="ltx_break"/>Multilingual Neural Machine Translation Models</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="p1.1.2.1.1.1.1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="p1.1.2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1.1.1">Varun Gumma  Pranjal A. Chitale  Kalika Bali</span></span>
<span class="ltx_p ltx_align_center" id="p1.1.2.1.1.1.1.1.1.2"><span class="ltx_text" id="p1.1.2.1.1.1.1.1.1.2.1">Microsoft Corporation 
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1.2.2">
</span><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.1.1.1.1.2.3">varun230999@gmail.com</span><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1.2.4"></span></span>
</span></span></span>
</span>
</span></span></p>
<br class="ltx_break"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neural Machine Translation (NMT) models have become integral to a broad spectrum of applications across various Natural Language Processing (NLP) tasks, particularly in generating synthetic data for training multilingual Large Language Models (LLMs). Despite their widespread use, state-of-the-art NMT systems often encounter sequence length constraints due to their reliance on Sinusoidal Positional Embeddings. Recent advancements have introduced alternative methods for embedding positional information, such as Rotary Positional Embeddings (RoPE) <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib30" title="">2024</a>)</cite> and Attention with Linear Biases (ALiBi) <cite class="ltx_cite ltx_citemacro_cite">Press et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib26" title="">2022</a>)</cite>, which demonstrate improved extrapolation capabilities for longer contexts. However, retraining existing models with these new embeddings is often computationally infeasible. This raises a critical question: <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Can we alter the Positional Embeddings in pre-trained models while preserving their performance?</span> To the best of our knowledge, no prior study has examined the feasibility of swapping out Positional Embeddings in pre-trained transformer models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This study addresses this question by investigating the feasibility of substituting Positional Embeddings in pre-trained NMT models. Additionally, inspired by prior works <cite class="ltx_cite ltx_citemacro_citep">(Haviv et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib15" title="">2022</a>; Kazemnejad et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib18" title="">2023</a>)</cite>, we explore whether Positional Embeddings are necessary in Encoder-Decoder models and whether these models can recover performance through fine-tuning after the removal of explicit positional information.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="378" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An basic overview of switching out Positional Embeddings</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We conduct a thorough investigation into both parameter-efficient fine-tuning (PEFT) methods and full fine-tuning (FFT) strategies to restore performance and adapt pre-trained models to new Positional Embeddings. Our results demonstrate that it is indeed possible to substitute Positional Embeddings post-hoc without compromising model performance. Additionally, we find that adding extra parameters specifically to the self-attention block is sufficient for effective adaptation. This approach not only preserves sentence-level performance but also improves extrapolation capabilities for longer contexts when alternative Positional Embeddings, such as RoPE or ALiBi, are introduced.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the remainder of this paper, we make the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a comprehensive analysis of the feasibility of modifying the Positional Embeddings of pre-trained transformer models post-hoc while maintaining comparable performance levels.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate that these models can be adapted in a parameter-efficient manner by simply incorporating additional parameters into the self-attention block.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our findings show that RoPE and ALiBi exhibit equivalent performance, while NoPE significantly degrades performance, highlighting it as a sub-optimal design choice for encoder-decoder transformer models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We establish that post-hoc modification of Positional Embeddings achieves performance comparable to that of a model trained from scratch with the respective Positional Embeddings.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Lastly, we open-source<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/VarunGumma/fairseq" title="">https://github.com/VarunGumma/fairseq</a>
</span></span></span> our framework to facilitate the efficient training and fine-tuning of long-context machine translation models.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Neural Machine Translation (MNMT)</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib34" title="">2019</a>); Firat et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib11" title="">2016</a>); Aharoni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib1" title="">2019</a>); Arivazhagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib2" title="">2019</a>)</cite> has emerged as a leading methodology for creating translation systems capable of managing multiple languages. These MNMT systems leverage a unified encoder-decoder transformer framework <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib32" title="">2017</a>)</cite> along with language-specific embeddings to integrate linguistic nuances. Previous works <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib9" title="">2021</a>); Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib4" title="">2022</a>)</cite> have developed many-to-many (X-Y) models at a global scale, while other works <cite class="ltx_cite ltx_citemacro_cite">Ramesh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib27" title="">2022</a>); Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite> have primarily focused on many-to-one (X-En) and one-to-many (En-X) multilingual models for Indic languages. An in-depth examination of MNMT methodologies is presented in the survey by <cite class="ltx_cite ltx_citemacro_citet">Dabre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib5" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Positional Embedding Methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Transformer-based models require an explicit Positional Embedding information as all the timesteps are processed parallely, and hence ther is a need for explicit positional. To this end, <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib32" title="">2017</a>)</cite> propose an empirical absolute Positional Embedding approach following a fourier series with alternating sine and cosine terms <cite class="ltx_cite ltx_citemacro_cite">Kernes (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib19" title="">2024</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib6" title="">2019</a>)</cite> introduce trainable Positional Embeddings which are learnt along with the other parameters of the model, however they have not shown a significant performance difference from prior static versions. <cite class="ltx_cite ltx_citemacro_citet">Press et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib26" title="">2022</a>)</cite> discuss ALiBi, a simple relative Positional Embedding strategy by adding the relative distance between tokens to the attention matrix and show that it can help the model generalize to longer sequence during inference even though trained on shorter sequences. RoPE <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib30" title="">2024</a>)</cite> introduces the concept of rotating vectors according to their positions to explicitly encode relative position dependency in the self-attention formulation. Recent works <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib31" title="">2023</a>); Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib24" title="">2023</a>); Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib7" title="">2024</a>); Ostmeier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib22" title="">2024</a>)</cite> have also provided fixes to adapt RoPE to much longer sequences. <cite class="ltx_cite ltx_citemacro_citet">Kazemnejad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib18" title="">2023</a>)</cite> find that no Positional Embedding can represents both relative and absolute strategies, and that it elicits high length generalizability. We refer the readers to <cite class="ltx_cite ltx_citemacro_citet">Wang and Chen (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib33" title="">2020</a>)</cite> for a visual understanding of Positional Embeddings methods.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Parameter-Efficient Fine-tuning (PEFT)</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib16" title="">2019</a>); Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib3" title="">2019</a>); Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib17" title="">2022</a>)</cite> deals with the concept of updating only a certain subset of parameters of the network, to <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px3.p1.1.1">efficiently</span> generalize the model to new tasks or domains, instead of tuning all parameters, which might lead to catastrophic forgetting or over-fitting in large models. Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib17" title="">2022</a>)</cite> is currently the most popular parameter efficient finetuning strategy, which hypothesizes that the update to weights are low-rank, and injects small trainable parameters directly into the pre-trained weights to mimic the low-rank decomposed updates. An extension, Weight-Decomposed Low-Rank Adaptation (DoRA) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib20" title="">2024</a>)</cite> decomposes the pre-trained weight into a magnitude and direction, and employs LoRA only for directional updates to efficiently minimize the number of trainable parameters.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we discuss the dataset curation, model training and evaluation procedures.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our experimentation focuses on standard Encoder-Decoder transformer models. While several massively multilingual models, such as NLLB <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib4" title="">2022</a>)</cite> and its distilled variants, have been released, our experimentation requires the availability of long-context or document-level training and evaluation datasets. Consequently, we focus on language-family specific models that allow for more controlled experimentation.
For our experiments, we chose the distilled variants of IndicTrans2 <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite>. A key difference is that the distilled models released in <cite class="ltx_cite ltx_citemacro_citet">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite> we used were not trained with BackTranslation <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib29" title="">2016</a>)</cite> data (BPCC-BT), which we incorporate in our experimental setup.
We experimented with models based on RoPE, ALiBi, and NoPE by replacing the sinusoidal Positional Embedding in the pre-trained model with the corresponding relative Positional Embedding. This replacement led to a significant performance drop, prompting us to fine-tune the models using high-quality data <cite class="ltx_cite ltx_citemacro_cite">Mohiuddin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib21" title="">2022</a>); Gumma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib14" title="">2023</a>)</cite> to study their recovery in each setting.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Train set</h4>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">For maximum performance gains for our models, we choose a high-quality mixture of seen and unseen distributions for our training data. To this end, for the seen distribution, we select the BPCC-Seed, which is a high-quality human-verified subset BPCC. This subset consists of X pairs, and has shown to be effective in improving model performance <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite>. As for the unseen distribution, we sample the top 150K pairs for each language from the BackTranslation (BPCC-BT) subset of BPCC based on the provided LASER <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib4" title="">2022</a>)</cite> or LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib10" title="">2022</a>)</cite> cosine-similarity scores. In case similarity scores are unavailable for a translation pair, a random sample was chosen.
Additionally, for the unseen data, we also include the Asian Language Treebank<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT" title="">https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT</a></span></span></span> (ALT) Corpus <cite class="ltx_cite ltx_citemacro_cite">Riza et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib28" title="">2016</a>)</cite> which contains long-context translation pairs for Hindi and Bengali, and the CoPara Aligned corpus <cite class="ltx_cite ltx_citemacro_cite">E et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib8" title="">2023</a>)</cite>, which contains document-level translation pairs for 4 Dravidian languages, Telugu, Kannada, Malayalam and Tamil. Overall, our train set consists of 4.45M and 3.46M translation pairs across 22 languages in the X-En and En-X directions respectively.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Training</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">We combine the fine-tuning data prepared above with the training data of the IT2 distilled models as described in <cite class="ltx_cite ltx_citemacro_citet">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite>, to create 114.09M and 113.10M translation pairs for the X-En and En-X directions, respectively.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Dev/Test sets</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Most existing benchmarks for Indian languages are composed of sentence-level test sets, which may not adequately reflect long-context performance or extrapolation capabilities. To address this, we construct document-level development and test sets using Flores<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/facebook/Flores" title="">https://huggingface.co/datasets/facebook/Flores</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib4" title="">2022</a>)</cite> and IN22-Conv<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/ai4bharat/IN22-Conv" title="">https://huggingface.co/datasets/ai4bharat/IN22-Conv</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Gala et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite>. In this setup, sentences are grouped within the same context window, making them suitable for document-level evaluation.
For Flores, we group sentences by metadata, including URL, domain, and topic headers, to map sentence-level pairs into document-level pairs, where each document-level unit consists of three sentences. This method is viable since this benchmark was created by professional translators who translate three consecutive sentences from every selected context window, as described by <cite class="ltx_cite ltx_citemacro_citet">Goyal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib13" title="">2022</a>)</cite>.
In case of IN22-Conv, the dataset was originally created by translating entire conversations from English into the target languages by professional translators, and used for sentence-level evaluation by <cite class="ltx_cite ltx_citemacro_citet">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite>. We use metadata provided to trace back the conversations and treat the entire conversation as a sequence for long-context evaluation. As a result, we obtain 289 merged sentence pairs per language, with a total of 6,380 pairs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Metrics and Evaluation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For consistency, we use the ChrF++ score <cite class="ltx_cite ltx_citemacro_citep">(Popović, <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib25" title="">2017</a>)</cite> as the primary metric, and follow the same evaluation practices as outlined in <cite class="ltx_cite ltx_citemacro_citet">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib12" title="">2023</a>)</cite> by normalizing the indic texts using <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">indic_nlp_library<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url" href="https://github.com/anoopkunchukuttan/indic_nlp_library" title="">https://github.com/anoopkunchukuttan/indic_nlp_library</a></span></span></span></span>. For brevity, we report the aggregate score across all the languages in the respective test set, which are denoted in <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.T1" title="In 3.3 Metrics and Evaluation ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.1.1.1.1">Test Set</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Languages Covered</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.1.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.2.1.1.1">IN22</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S3.T1.1.2.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.2.1.2.1">
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.1.1">Assamese, Bengali, <span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.2.1.1.1.1">Bodo</span>, <span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.2.1.1.1.2">Dogri</span>,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.2.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.2.1.2.1.1">Konkani</span>, Gujarati, Hindi, Kannada,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.3.1">Kashmiri, Maithili, Malayalam, Marathi,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.4.1">Manipuri, Nepali, Odia, Punjabi,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.5.1">Sanskrit, Santali, Sindhi, Tamil,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.1.2.1.6">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.2.1.2.1.6.1">Telugu, Urdu</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.3.2.1.1">Flores</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S3.T1.1.3.2.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.3.2.2.1">
<tr class="ltx_tr" id="S3.T1.1.3.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.2.2.1.1.1">Assamese, Bengali, Gujarati, Hindi,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.2.2.1.2.1">Kannada, Kashmiri, Maithili, Malayalam,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.2.2.1.3.1">Marathi, Manipuri, Nepali, Odia,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2.2.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.2.2.1.4.1">Punjabi, Sanskrit, Santali, Sindhi,</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2.2.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.2.2.1.5.1">Tamil, Telugu, Urdu</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.4.3.1.1">ALT</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="S3.T1.1.4.3.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.4.3.2.1">
<tr class="ltx_tr" id="S3.T1.1.4.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.4.3.2.1.1.1">Bengali, Hindi</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>List of languages in each test set.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Finetuning Strategies</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In our fine-tuning experiments, we swap the sinusoidal Positional Embedding module with a RoPE, ALiBi or NoPE module, and fine-tune the model <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">as efficiently as possible</span> from that point to analyse the performance recovery.</p>
</div>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">FFT</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">First, we choose a full-fine-tuning (FFT) approach which tweaks all the 211.78M parameters of the model. This would allow the embeddings, layer-normalizations, and the rest of the linear layers<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>self-attention, cross-attention, feed-forward modules</span></span></span> to be tuned as per the new relative Positional Embedding.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">LoRA</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">In vanilla PEFT method, we add trainable parameters only to the linear layers resulting in 6.48M (3.06%) trainable parameters. Note that, unlike FFT, in this case, the embeddings and layer-normalizations do not have any trainable parameters. The main hypothesis in this experiment is we intend to understand if the model can adapt to the change in the Positional Embeddings by addition of few trainable parameters as FFT may be computationally expensive as well as result in catastrophic forgetting.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">minLoRA</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">The relative Positional Embedding modules are added to the self-attention modules at every layers. Therefore, we intend to leverage this property and understand whether, we can further cut down on parameters and apply these additional parameters only to the affected modules. Hence, we test the hypothesis of tuning only the self-attention modules with LoRA resulting in 2.35M (1.11%) trainable parameters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">We use the <span class="ltx_text ltx_font_typewriter" id="S4.p1.2.1">fairseq<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote7.1.1.1">7</span></span><span class="ltx_text ltx_font_serif" id="footnote7.9">Our variant implements, LoRA, RoPE, ALiBi, and a slightly efficient SDPA as per the requirements of this work</span></span></span></span></span> framework <cite class="ltx_cite ltx_citemacro_cite">Ott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib23" title="">2019</a>)</cite> for all of our experiments. All the finetuning experiments were conducted on 4 <math alttext="\times" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">×</annotation></semantics></math> A100 80Gb GPUs, the train-from-scratch experiments on 8 <math alttext="\times" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><times id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">×</annotation></semantics></math> H100 80Gb GPUs, and every model was trained till convergence with early-stopping. The exact hyperparameter sets are mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S4.T2" title="In 4 Experiments ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S4.T3" title="Table 3 ‣ 4 Experiments ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T2.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.3.2.1.1">Global Max Tokens</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.3.2.2">32K</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.4.3.1.1">Optimizer</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.4.3.2">Adam</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.5.4.1.1">LR scheduler</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.5.4.2">inverse_sqrt</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.5.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.6.5.1.1">Gradient clip norm</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.6.5.2">1.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.7.6.1.1">Adam betas</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.7.6.2">(0.9, 0.98) / (0.9, 0.999)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.2.1">Checkpoint</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.1">1000 steps, BLEU @ <math alttext="k=5" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">k</mi><mo id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><eq id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1.1"></eq><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">𝑘</ci><cn id="S4.T2.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.1.1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_k = 5</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.7.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.8.7.1.1">Patience</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.8.7.2">10</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.9.8.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.9.8.1.1">Max Positions</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.9.8.2">(4096, 4096)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.9.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.10.9.1.1">Warmup steps</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.10.9.2">2000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.11.10.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.11.10.1.1">Learning rate</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.11.10.2">6e-5 / 1e-4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.12.11.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.12.11.1.1">Dropout</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.12.11.2">0.2 / 0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.13.12.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.13.12.1.1">Label smoothing</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.13.12.2">0.1 / 0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.14.13.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.14.13.1.1">LoRA rank</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.14.13.2">16</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.15.14.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.15.14.1.1">LoRA alpha</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.15.14.2">32</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.16.15.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.16.15.1.1">LoRA dropout</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.16.15.2">0.05</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.17.16.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.17.16.1.1">LoRA Rank Scaled</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.17.16.2">True</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.18.17.1"><span class="ltx_text ltx_font_italic" id="S4.T2.1.18.17.1.1">FP16</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.1.18.17.2">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Default hyperparameter settings for finetuning. The cells with two numbers present the values for FFT/LoRA. LoRA finetuning acts on very specific modules, and adds minimal trainable parameters, while keeping the rest frozen. This acts like an implicit regularization, and hence we do not add any explicit strategies like label-smoothing and dropout to the model in this setup.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T3.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.3.2.1.1">Global Max Tokens</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.3.2.2">256K</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.4.3.1.1">Optimizer</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.4.3.2">Adam</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.5.4.1.1">LR scheduler</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.5.4.2">inverse_sqrt</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.6.5.1.1">Gradient clip norm</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.6.5.2">1.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.7.6.1.1">Adam betas</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.7.6.2">(0.9, 0.98)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.2.1">Checkpoint</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.1">1000 steps, BLEU @ <math alttext="k=5" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">k</mi><mo id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><eq id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1"></eq><ci id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">𝑘</ci><cn id="S4.T3.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.1.1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">italic_k = 5</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.8.7.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.8.7.1.1">Patience</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.8.7.2">10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.9.8.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.9.8.1.1">Max Positions</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.9.8.2">(4096, 4096)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.10.9.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.10.9.1.1">Warmup steps</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.10.9.2">4000</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.11.10.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.11.10.1.1">Learning rate</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.11.10.2">7e-4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.12.11.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.12.11.1.1">Dropout</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.12.11.2">0.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.13.12.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.13.12.1.1">Label smoothing</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.13.12.2">0.1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.14.13.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.14.13.1.1">FP16</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.1.14.13.2">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Default hyperparameter settings for training from scratch</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We compare our hypothesis against several baselines as outlined below. First, we examine a direct swapping approach, where models with sinusoidal Positional Embeddings are altered to not use Positional Embeddings (NoPE) or subject to alternate Positional Embeddings like RoPE, ALiBi, and their performance is evaluated. Additionally, we consider fine-tuning these models under three conditions outlined in <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S3.SS4" title="3.4 Finetuning Strategies ‣ 3 Methodology ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>. Finally, to estimate a bound for each Positional Embedding method without swapping, we train transformer models from scratch using Sinusoidal, RoPE, ALiBi, and NoPE.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we discuss the results that aim to understand whether Positional Embeddings can be efficiently swapped posthoc in a trained model.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Swapping out Positional Embeddings</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T4" title="In Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T5" title="Table 5 ‣ Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrate the performance of several experimental setups that involve swapping of Positional Embeddings and followed by fine-tuning (if applicable).</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Performance Degradation Post-Swap</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">As expected, a significant drop in performance is observed when Positional Embeddings are removed, compared to the based, with further deterioration occurring if the new type of Positional Embeddings are directly swapped in without subsequent fine-tuning. However, a key observation in this scenario is that performance does not completely collapse, suggesting that positional information may be partially encoded within the model itself. This indicates that attributing the entirety of positional information to the Positional Embeddings alone may not be entirely accurate.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning is key to performance recovery</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">When Positional Embeddings such as RoPE or ALiBi are introduced, across various fine-tuning setups, we consistently observe that the performance can be regained through fine-tuning. This demonstrates that it is feasible to modify Positional Embeddings at later stages and still retain the performance of the base model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Efficacy of the Positional Embedding switch</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">A notable finding in this setting is that fine-tuning a model originally using sinusoidal embeddings yielded performance comparable to models fine-tuned after the switch to using RoPE or ALiBi embeddings. This further reinforces the conclusion that Positional Embeddings in pre-trained transformer models can be safely tweaked even in later stages. Although sinusoidal, rotary, and ALiBi embeddings show similar performance in long-context document-level tests used in the context of this study. The advantage of switching to RoPE or ALiBi is that further length extrapolation may now be feasible in these transformer models, as demonstrated by the findings of <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib30" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Press et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib26" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">NoPE is not a solution in Encoder-Decoder models</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">Another key observation in our setting is that the findings of <cite class="ltx_cite ltx_citemacro_citet">Kazemnejad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#bib.bib18" title="">2023</a>)</cite> about explicit Positional Embeddings not being necessary does not generalize to Encoder-Decoder models. As we observe that no fine-tuning strategy is capable of bridging the gap and regaining performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Minimalistic Fine-tuning is sufficient</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px5.p1.1">Across all the fine-tuning strategies evaluated, minimalistic LoRA fine-tuning, as well as FFT and LoRA, produced comparable performance. This suggests that extensive parameter adjustments are not necessary; adding LoRA parameters solely to the self-attention modules, where the relative Positional Embedding modules are integrated, is sufficient for adaptation. This approach is highly parameter-efficient, requiring only 2.35M trainable parameters (1.11%).</p>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:433.6pt;height:107pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.8pt,18.8pt) scale(0.738319661632284,0.738319661632284) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.3.3.4.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T4.3.3.4.1.2">Base</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.3.3.4.1.3">SINE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.3.3.4.1.4">NoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.3.3.4.1.5">RoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T4.3.3.4.1.6">ALiBi</th>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.5">IT2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1">N<sup class="ltx_sup" id="S5.T4.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.1.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.2.2.2.2">R<sup class="ltx_sup" id="S5.T4.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.2.2.2.2.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.3.3.3.3">A<sup class="ltx_sup" id="S5.T4.3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.3.3.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.6">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.7">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.3.3.3.8">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.9">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.10">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.3.3.3.11">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.12">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.13">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.3.3.3.14">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.15">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.16">L</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3.17">ML</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.3.5.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.5.1.1.1">Flores Indic-En</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.2">59.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.3">34.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.4">31.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.5.1.5">23.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.6"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.5.1.6.1">59.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.7">59.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.5.1.8">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.9">42.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.10">41.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.5.1.11">41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.12">59.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.13">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.5.1.14">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.15">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.16">58.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.3.3.5.1.17">58.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.3.3.6.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.6.2.1.1">IN22-Conv Indic-En</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.2.2.1">55.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.3">38.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.4">38.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.6.2.5">35.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.6"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.2.6.1">55.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.7">55.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.6.2.8">55.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.9">44.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.10">44.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.6.2.11">43.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.12">55.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.13">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.6.2.14">55.4</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.15">55.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.6.2.16">55.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.3.6.2.17">55.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.3.3.7.3.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.7.3.1.1">IN22-Gen Indic-En</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.7.3.2.1">62.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.3">34.1</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.4">28.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.7.3.5">19.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.6">61.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.7">61.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.7.3.8">61.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.9">42.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.10">41.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.7.3.11">40.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.12">61.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.13">61.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.7.3.14">61.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.15">60.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.3.16">60.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.3.7.3.17">60.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.3.8.4.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.8.4.1.1">Flores En-Indic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.2">47.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.3">31.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.4">27.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.8.4.5">15.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.6">48.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.7">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.8.4.8">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.9">39.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.10">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.8.4.11">36.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.12">48.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.13">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.3.8.4.14">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.15"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.8.4.15.1">49.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.16">47.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.3.3.8.4.17">47.5</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.3.3.9.5.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.9.5.1.1">IN22-Conv En-Indic</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.9.5.2.1">44.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.3">35.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.4">31.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.9.5.5">26.1</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.6">42.4</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.7">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.9.5.8">42.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.9">38.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.10">37.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.9.5.11">37.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.12">42.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.13">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.9.5.14">42.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.15">42.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.9.5.16">42.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.3.3.9.5.17">42.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T4.3.3.10.6.1"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.10.6.1.1">IN22-Gen En-Indic</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.10.6.2.1">48.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.3">30.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.4">24.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.10.6.5">13.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.6">46.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.7">46.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.10.6.8">46.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.9">36.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.10">35.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.10.6.11">33.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.12">46.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.13">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.10.6.14">46.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.15">46.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.16">46.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T4.3.3.10.6.17">45.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>ChrF++ scores obtained when swapping out the Positional Embeddings and fine-tuning, evaluated across <span class="ltx_text ltx_font_bold" id="S5.T4.14.1">Sentence-level benchmarks</span>. N<sup class="ltx_sup" id="S5.T4.15.2"><span class="ltx_text ltx_font_italic" id="S5.T4.15.2.1">∗</span></sup> denotes a direct transition (without any tuning) to No-Positional Embeddings, R<sup class="ltx_sup" id="S5.T4.16.3"><span class="ltx_text ltx_font_italic" id="S5.T4.16.3.1">∗</span></sup> denotes a direct transition to Rotary Positional Embeddings, and A<sup class="ltx_sup" id="S5.T4.17.4"><span class="ltx_text ltx_font_italic" id="S5.T4.17.4.1">∗</span></sup> denotes a direct transition to ALiBi Positional Embeddings. “FFT” represents Full Fine-Tuning, “L” signifies LoRA fine-tuning, and “ML” refers to minimalistic LoRA fine-tuning (applied exclusively to the Self-Attention block).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.3" style="width:433.6pt;height:107pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.8pt,18.8pt) scale(0.738319661632284,0.738319661632284) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T5.3.3.4.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T5.3.3.4.1.2">Base</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T5.3.3.4.1.3">SINE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T5.3.3.4.1.4">NoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T5.3.3.4.1.5">RoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T5.3.3.4.1.6">ALiBi</th>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.5">IT2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1">N<sup class="ltx_sup" id="S5.T5.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.1.1.1.1.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.2.2.2">R<sup class="ltx_sup" id="S5.T5.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T5.2.2.2.2.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.3.3">A<sup class="ltx_sup" id="S5.T5.3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.3.3.1.1">∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.6">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.7">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.3.8">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.9">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.10">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.3.11">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.12">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.13">L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.3.14">ML</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.15">FFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.16">L</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.17">ML</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.3.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.3.3.5.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.5.1.1.1">Flores Indic-En</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.2">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.3">28.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.4">12.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.5.1.5">5.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.6"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.5.1.6.1">61.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.7">61.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.5.1.8">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.9">37.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.10">34.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.5.1.11">33.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.12">61.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.13">60.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.5.1.14">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.15">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.16">59.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.3.3.5.1.17">59</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.3.3.6.2.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.6.2.1.1">IN22-Conv Indic-En</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.2">27.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.3">10.2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.4">2.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.6.2.5">1.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.6">32.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.7">32.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.6.2.8">31.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.9">15.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.10">13.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.6.2.11">13.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.12">33.0</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.13">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.6.2.14"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.6.2.14.1">33.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.15">30.2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.6.2.16">29.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.3.6.2.17">28.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.7.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.3.3.7.3.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.7.3.1.1">ALT Indic-En</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.7.3.2.1">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.3">39.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.4">35.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.3.5">26.8</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.6"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.7.3.6.1">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.7"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.7.3.7.1">65.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.3.8">64.8</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.9">49.2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.10">48.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.3.11">47.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.12">65.0</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.13">65.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.3.14">64.9</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.15">64.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.3.16">64.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.3.7.3.17">64.8</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.8.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.3.3.8.4.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.8.4.1.1">Flores En-Indic</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.2">49.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.3">23.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.4">14.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.8.4.5">4.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.6">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.7">49.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.8.4.8">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.9">33.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.10">29.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.8.4.11">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.12"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.4.12.1">49.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.13">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.8.4.14">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.15">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.16">48.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.3.3.8.4.17">48.3</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.9.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.3.3.9.5.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.9.5.1.1">IN22-Conv En-Indic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.9.5.2.1">25.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.3">7.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.4">2.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.9.5.5">1.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.6">24.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.7">23.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.9.5.8">22.9</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.9">11.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.10">9.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.9.5.11">8.0</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.12">24.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.13">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.9.5.14">22.6</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.15">23.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.9.5.16">20.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.3.9.5.17">21.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.10.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T5.3.3.10.6.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.3.10.6.1.1">ALT En-Indic</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.2">54.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.3">36.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.4">31.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.3.3.10.6.5">19.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.6">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.7">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.3.3.10.6.8">54.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.9">45.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.10">43.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.3.3.10.6.11">43.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.12">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.13">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.3.3.10.6.14">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.15"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.10.6.15.1">55.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.16">55.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T5.3.3.10.6.17">55.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>ChrF++ scores obtained when swapping out the Positional Embeddings and or fine-tuning, evaluated across <span class="ltx_text ltx_font_bold" id="S5.T5.14.1">Document-level benchmarks</span> averaged across languages. N<sup class="ltx_sup" id="S5.T5.15.2"><span class="ltx_text ltx_font_italic" id="S5.T5.15.2.1">∗</span></sup> denotes a direct transition (without any tuning) to No-Positional Embeddings, R<sup class="ltx_sup" id="S5.T5.16.3"><span class="ltx_text ltx_font_italic" id="S5.T5.16.3.1">∗</span></sup> denotes a direct transition to Rotary Positional Embeddings, and A<sup class="ltx_sup" id="S5.T5.17.4"><span class="ltx_text ltx_font_italic" id="S5.T5.17.4.1">∗</span></sup> denotes a direct transition to ALiBi Positional Embeddings. “FFT” represents Full Fine-Tuning, “L” signifies LoRA fine-tuning, and “ML” refers to minimalistic LoRA fine-tuning (applied exclusively to the Self-Attention block).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T6.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.1.2">NoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.1.3">RoPE</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.1.4">ALiBi</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.2.1.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.2.1.1.1">Flores Indic-En</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1.2">38.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.2.1.3.1">59.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.1.2.1.4">58.4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.3.2.1.1">IN22-Conv Indic-En</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2.2">14.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.3.2.3.1">33.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.4">31.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.4.3.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.4.3.1.1">ALT Indic-En</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.3.2">52.5</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.4.3.3.1">64.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.4">63.9</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.5.4.1.1">Flores En-Indic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.5.4.2">37.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.5.4.3.1">49.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.1.5.4.4">49.1</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.6.5.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.6.5.1.1">IN22-Conv En-Indic</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.5.2">17.3</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.5.3.1">26.0</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.6.5.4">25.8</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S5.T6.1.7.6.1.1">ALT En-Indic</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.7.6.2">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.7.6.3.1">55.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T6.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.7.6.4.1">55.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>ChrF++ scores of models trained from scratch with various Positional Embeddings evaluated across <span class="ltx_text ltx_font_bold" id="S5.T6.3.1">Document-Level benchmarks</span> averaged across languages.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T7.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.1.1.1.2">NoPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.1.1.1.3">RoPE</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.1.1.1.4">ALiBi</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.2.1.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.2.1.1.1">Flores Indic-En</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.1.2">44.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.2.1.3.1">57.9</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T7.1.2.1.4">57.5</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.3.2.1.1">IN22-Conv Indic-En</span></th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.3.2.2">45.5</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.3.2.3.1">54.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T7.1.3.2.4">54.0</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.4.3.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.4.3.1.1">IN22-Gen En-Indic</span></th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.3.2">40.9</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.4.3.3.1">48.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T7.1.4.3.4">47.9</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.5.4.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.5.4.1.1">Flores En-Indic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.5.4.2">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.5.4.3.1">47.6</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T7.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S5.T7.1.5.4.4.1">47.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.6.5.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.6.5.1.1">IN22-Conv En-Indic</span></th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.5.2">41.0</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.6.5.3.1">44.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T7.1.6.5.4">44.2</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T7.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S5.T7.1.7.6.1.1">IN22-Gen Indic-En</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.7.6.2">44.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.7.6.3.1">60.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T7.1.7.6.4">59.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>ChrF++ scores of models trained from scratch with various Positional Embeddings evaluated across <span class="ltx_text ltx_font_bold" id="S5.T7.3.1">Sentence-Level benchmarks</span> averaged across languages.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with Training from scratch</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T6" title="In Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T7" title="Table 7 ‣ Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">7</span></a> demonstrate the performance trends of the models with the respective Positional Embedding trained from scratch. We compare these with the results in <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T4" title="In Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.11382v1#S5.T5" title="Table 5 ‣ Minimalistic Fine-tuning is sufficient ‣ 5.1 Swapping out Positional Embeddings ‣ 5 Results and Discussions ‣ On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">5</span></a>. The comparison shows that models with swapped Positional Embeddings either outperform or are on par with their counterparts trained from scratch. However, training from scratch is resource-intensive, whereas the proposed approach of post-hoc Positional Embedding swapping proves to be a more efficient alternative in terms of both computation and data. Additionally, models trained from scratch using NoPE slightly outperform their swap counterparts, but remain suboptimal compared to any other Positional Embeddings (SINE / ALiBi / RoPE), further reinforcing the prior finding that NoPE is not a viable option for Encoder-Decoder models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we have demonstrated that sinusoidal Positional Embeddings can be <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">efficiently</span> replaced with RoPE or ALiBi in a standard transformer model. We show that this substitution can be achieved in a parameter-efficient manner by simply introducing additional parameters within the self-attention module. Our findings also indicate that NoPE is not a viable solution for Encoder-Decoder models, underscoring the necessity of explicit positional information in such architectures. Although our experiments are focused on the machine translation (MT) task, we believe that this approach can be extended to other Encoder-Decoder models or Encoder-only models to enhance their long-context capabilities.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our work aligns with modular deep learning principles, exploring the post-hoc interchangeability of key components like Positional Embeddings. Although our preliminary findings suggest this is possible, several limitations should be noted:</p>
</div>
<div class="ltx_para" id="S7.p2">
<ul class="ltx_itemize" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1">All our experiments have been limited to a single model primarily due to the complete availability of relevant information regarding the IndicTrans2 model and dataset. Further the lack of availability of long-context data is also a limiting factor in performing this experimentation across languages.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1">Our investigation does not extend to examining the interchangeability between two relative PEs or from relative to sinusoidal PEs. This is due to the fact that older models predominantly employed sinusoidal PEs, whereas contemporary SOTA LLMs tend to favor RoPE, which has demonstrated robustness in handling long contexts. Therefore, a practically viable setup would involve a switch from sinusoidal to relative and not the other way around.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1">Our study concludes with demonstrating that interchangeability is feasible. Further, we also demonstrate that fine-tuning with sinusoidal embeddings also results in nearly similar performance. However, it is important to note that prior works demonstrate that RoPE provides superior length generalization capabilities, however as we do not have any long-context benchmark at our disposal as of now, we leave this aspect to future work.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1">Lastly, we also acknowledge that no specific hyperparameter tuning was conducted for this task, and we utilize the standard set available in the literature.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We thank Jay Gala (Research Assistant, MBZUAI), and Raj Dabre (Senior Researcher, NICT) for their invaluable insights and comments on paper. We also thank Amit Sharma (Principal Researcher, Microsoft Research India) for the discussions that helped us in refining the idea.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2019)</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1388" title="">Massively multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 3874–3884, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al. (2019)</span>
<span class="ltx_bibblock">
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation in the wild: Findings and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv: 1907.05019</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna and Firat (2019)</span>
<span class="ltx_bibblock">
Ankur Bapna and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1165" title="">Simple, scalable adaptation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1538–1548, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabre et al. (2020)</span>
<span class="ltx_bibblock">
Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-tutorials.3" title="">Multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts</em>, pages 16–21, Barcelona, Spain (Online). International Committee for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2024)</span>
<span class="ltx_bibblock">
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024.

</span>
<span class="ltx_bibblock">Longrope: Extending llm context window beyond 2 million tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv: 2402.13753</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">E et al. (2023)</span>
<span class="ltx_bibblock">
Nikhil E, Mukund Choudhary, and Radhika Mamidi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.dravidianlangtech-1.12" title="">CoPara: The first Dravidian paragraph-level n-way aligned corpus</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Third Workshop on Speech and Language Technologies for Dravidian Languages</em>, pages 88–96, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v22/20-1307.html" title="">Beyond english-centric multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Journal of Machine Learning Research</em>, 22(107):1–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.62" title="">Language-agnostic BERT sentence embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878–891, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firat et al. (2016)</span>
<span class="ltx_bibblock">
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N16-1101" title="">Multi-way, multilingual neural machine translation with a shared attention mechanism</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 866–875, San Diego, California. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gala et al. (2023)</span>
<span class="ltx_bibblock">
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar M, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=vfT4YuzAYA" title="">Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Transactions on Machine Learning Research</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00474" title="">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Transactions of the Association for Computational Linguistics</em>, 10:522–538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gumma et al. (2023)</span>
<span class="ltx_bibblock">
Varun Gumma, Raj Dabre, and Pratyush Kumar. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.11" title="">An empirical study of leveraging knowledge distillation for compressing multilingual neural machine translation models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 103–114, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haviv et al. (2022)</span>
<span class="ltx_bibblock">
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.99" title="">Transformer language models without positional encodings still learn positional information</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 1382–1390, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="">Parameter-efficient transfer learning for NLP</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 36th International Conference on Machine Learning</em>, volume 97 of <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">Proceedings of Machine Learning Research</em>, pages 2790–2799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemnejad et al. (2023)</span>
<span class="ltx_bibblock">
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan, Payel Das, and Siva Reddy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Drrl2gcjzl" title="">The impact of positional encoding on length generalization in transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kernes (2024)</span>
<span class="ltx_bibblock">
Jonathan Kernes. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3" title="">Master positional encoding part i</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">TowardsDataScience</em>.

</span>
<span class="ltx_bibblock">Accessed: 2024-08-20.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v235/liu24bn.html" title="">DoRA: Weight-decomposed low-rank adaptation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, volume 235 of <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">Proceedings of Machine Learning Research</em>, pages 32100–32121. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohiuddin et al. (2022)</span>
<span class="ltx_bibblock">
Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, and Shafiq Joty. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.113" title="">Data selection curriculum for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 1569–1582, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostmeier et al. (2024)</span>
<span class="ltx_bibblock">
Sophie Ostmeier, Brian Axelrod, Michael E. Moseley, Akshay Chaudhari, and Curtis Langlotz. 2024.

</span>
<span class="ltx_bibblock">Liere: Generalizing rotary position encodings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv: 2406.10322</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast, extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>, pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv: 2309.00071</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2017)</span>
<span class="ltx_bibblock">
Maja Popović. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4770" title="">chrF++: words helping character n-grams</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 612–618, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Noah Smith, and Mike Lewis. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=R8sQPpGCv0" title="">Train short, test long: Attention with linear biases enables input length extrapolation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al. (2022)</span>
<span class="ltx_bibblock">
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00452" title="">Samanantar: The largest publicly available parallel corpora collection for 11 Indic languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Transactions of the Association for Computational Linguistics</em>, 10:145–162.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riza et al. (2016)</span>
<span class="ltx_bibblock">
Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, and Chenchen Ding. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICSDA.2016.7918974" title="">Introduction of the asian language treebank</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)</em>, pages 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1009" title="">Improving neural machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1016/j.neucom.2023.127063" title="">Roformer: Enhanced transformer with rotary position embedding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Neurocomputing</em>, 568:127063.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.816" title="">A length-extrapolatable transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14590–14604, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Chen (2020)</span>
<span class="ltx_bibblock">
Yu-An Wang and Yun-Nung Chen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.555" title="">What do position embeddings learn? an empirical study of pre-trained language model positional encoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6840–6849, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Wen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1426" title="">Bridging the gap between training and inference for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4334–4343, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Aug 21 07:21:18 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
