<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles</title>
<!--Generated on Tue Oct  1 14:50:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.00769v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S1" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S2" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">RELATED WORK</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S2.SS1" title="In II RELATED WORK ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Data Acquisition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S2.SS2" title="In II RELATED WORK ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Processing Aerial Imagery</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S2.SS3" title="In II RELATED WORK ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Lanelet2 Format</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S3" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">SEMANTIC SEGMENTATION OF AERIAL IMAGERY</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S4" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">SYMBOL CLASSIFICATION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">CREATION OF HD MAPS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5.SS1" title="In V CREATION OF HD MAPS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Road Border</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5.SS2" title="In V CREATION OF HD MAPS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Lane Markings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5.SS3" title="In V CREATION OF HD MAPS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Lanes</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5.SS4" title="In V CREATION OF HD MAPS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Exporting as Lanelet2 Map</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.SS1" title="In VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Semantic Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.SS2" title="In VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Symbol Classification</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.SS3" title="In VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Map Creation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S7" title="In DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robert Krajewski<sup class="ltx_sup" id="id5.5.id1"><span class="ltx_text ltx_font_italic" id="id5.5.id1.1">1</span></sup> and Huijo Kim<sup class="ltx_sup" id="id6.6.id2"><span class="ltx_text ltx_font_italic" id="id6.6.id2.1">2</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id7.7.id1"><span class="ltx_text ltx_font_italic" id="id7.7.id1.1">1</span></sup>Robert Krajewski is with RWTH Aachen University,
<span class="ltx_text ltx_font_typewriter" id="id8.8.id2" style="font-size:90%;">robert.krajewski@rwth-aachen.de</span><sup class="ltx_sup" id="id9.9.id1"><span class="ltx_text ltx_font_italic" id="id9.9.id1.1">2</span></sup>Huijo Kim is with hexafarms,
<span class="ltx_text ltx_font_typewriter" id="id10.10.id2" style="font-size:90%;">ccomkhj@gmail.com</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">High-definition maps (HD maps) play a crucial role in the development, safety validation, and operation of highly automated vehicles. Efficiently collecting up-to-date sensor data from road segments and obtaining accurate maps from these are key challenges in HD map creation. Commonly used methods, such as dedicated measurement vehicles and crowd-sourced data from series vehicles, often face limitations in commercial viability. Although high-resolution aerial imagery offers a cost-effective or even free alternative, it requires significant manual effort and time to transform it into maps. In this paper, we introduce a semi-automatic method for creating HD maps from high-resolution aerial imagery. Our method involves training neural networks to semantically segment aerial images into classes relevant to HD maps. The resulting segmentation is then hierarchically post-processed to generate a prototypical HD map of visible road elements. Exporting the map to the Lanelet2 format allows easy extension for different use cases using standard tools. To train and evaluate our method, we created a dataset using public aerial imagery of urban road segments in Germany. In our evaluation, we achieved an automatic mapping of lane markings and road borders with a recall and precision exceeding 96%. The source code for our method is publicly available at https://github.com/RobertKrajewski/DeepAerialMapper.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.2.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Current trends in electromobility and automated driving will shape the future of transportation. Although the share of electrified vehicles is already substantial, automated vehicles at higher levels of automation are still either prototypes or restricted to specific geographical areas. The development, safety validation and operation of highly automated vehicles pose a complex problem, as the human driver‚Äôs task must (temporarily) be completely taken over depending on the automation level <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib1" title="">1</a>]</cite>. The driving task itself can be divided into different components, including perceiving surrounding traffic, interpreting the current driving scenario, and planning the vehicle‚Äôs actions.</p>
</div>
<figure class="ltx_figure ltx_figure_panel" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="222" id="S1.1.g1" src="extracted/5749206/images/1_white.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Exemplary semantic segmentation of an intersection shown as overlay. The annotated classes are represented by colours (Red: symbols, cyan: lane markings, gray: road, green: vegetation, blue: walkway, purple: traffic island)</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For some of these tasks, automated vehicles benefit from supplementing their perception and positioning systems by map information¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib2" title="">2</a>]</cite>. While standard-definition (SD) maps describing the coarse road topology are sufficient for, e.g., route planning, higher levels of automation require high-definition maps. Detailed lane-level information, including lane markings and symbols, can be used by various subsystems of automated vehicles: One example is localisation, which can better estimate the vehicle‚Äôs position based on known prominent points in the immediate environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib4" title="">4</a>]</cite>. Another example is behaviour planning, as vehicle-mounted sensors perceive on limited ranges and suffer from vehicle-to-vehicle occlusions. HD maps extend the visual horizon beyond the range of installed sensors, enabling a more predictive driving style, e.g., getting into the correct lane at an early opportunity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib5" title="">5</a>]</cite>. Safety is also enhanced, for example, by facilitating the perception to more easily differentiate between traffic signals and a car‚Äôs rear lights at intersections¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">When creating and using HD maps, it is essential to ensure that the resulting maps are complete, correct, and up-to-date.
The methods used to collect the raw data and process it into an HD map determine how this can be achieved and at what cost.
There are two fundamental methods for creating data for HD maps. The typical method is recording road segments from a vehicle‚Äôs perspective. Dedicated measurement vehicles with extensive sensor setups are used to perform systematic measurement campaigns¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib2" title="">2</a>]</cite>. In addition, series vehicles with cheaper sensor setups and data-upload capabilities are used to continuously crowd-source data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib7" title="">7</a>]</cite>. However, as access to dedicated measurement vehicles or a fleet of series vehicles is required, these methods are mainly used by car manufacturers or companies specialising in mapping.
The alternative to the vehicle perspective is the aerial perspective (e.g., aircraft or satellites), which has received less attention to date.
Due to the typically low resolution in terms of Ground Sampling Distance (GSD), most research has focused on producing SD maps that only cover road topology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib8" title="">8</a>]</cite>. However, resolution is steadily increasing, and data is becoming more affordable and available, as, for example, local authorities provide digital orthophotos free of charge.
Although the orthophotos have already been pre-processed and geo-referenced, one issue is that the manual creation of the HD map is labour-intensive. For instance, the boundaries of each lane must be manually annotated with polylines and attribute tags in mapping software such as JOSM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Inspired by existing systems for the automated creation of prototypes for significantly simpler SD maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib10" title="">10</a>]</cite>, we have developed a method to support the creation of HD maps.
Although methods for semantic segmentation of aerial imagery already exist, they lack the necessary complex post-processing and export to an HD map.
As a foundation for our method, we create a dataset of freely available, high-resolution, geo-referenced aerial images of urban road segments in Aachen, Germany.
We manually semantically annotate these using eight categories in order to train a convolutional neural network for semantic segmentation.
To subsequently create an HD map from a semantic segmentation, we present a method that hierarchically detects, groups, classifies, and combines individual elements such as symbols and lane markings.
For this purpose, mainly classical methods for image processing are used, as well as a separate neural network for symbol classification.
The result of these automatic processing steps for the visible road elements is a prototypical HD map, which already covers the otherwise very labour-intensive elements.
As our method is purely vision-based, we export the resulting map in the Lanelet2 format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib11" title="">11</a>]</cite> to allow manual completion of the map using standard tools.
This is necessary since, typically, some map elements are barely visible or occluded by a vehicle or tree.
In addition, other elements, such as speed limits or signs, cannot be extracted from aerial imagery and therefore must be added manually from complementary sources.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">RELATED WORK</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Data Acquisition</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Both the vehicle and aerial perspectives are viable options for collecting imagery for HD maps. Depending on the method and perspective chosen, there are advantages and disadvantages in terms of costs, accuracy, processing effort, and result verification.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Dedicated measurement vehicles are used to capture road data from a vehicle‚Äôs perspective. These vehicles are typically equipped with a high-precision inertial navigation system for localisation, along with multiple camera and lidar sensors to record their surroundings. This setup allows for high-quality data capture but it necessitates a complex pre-processing pipeline to fuse the data from individual sensors and temporally aggregate the perceived information. This is crucial for handling temporary occlusions caused by other objects or vehicles that block the line of sight. However, expensive sensor setups impose limitations on the scale of the vehicle fleet used for data recording in measurement campaigns¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">An alternative method involves crowd-sourcing data by employing a fleet of series vehicles that upload data during regular operations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib13" title="">13</a>]</cite>. With a large fleet, it becomes easier to collect substantial amounts of data and keep maps of frequently used road segments up-to-date. Consequently, no dedicated measurement campaigns are required to cover changes in symbols or road construction. However, the deployment of large fleets of vehicles is typically feasible only for car manufacturers, and series vehicles often utilise simpler GPS modules and less sophisticated sensor setups compared to measurement vehicles. Having to partially process the collected data online within the vehicle, primarily for data privacy and to manage the uploaded data volume, further compromises its quality. Hence, only processed and anonymised data, such as detected sign positions or lane markings, are typically uploaded¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib14" title="">14</a>]</cite>. Since raw sensor data is not available for map creation, utilising more sophisticated object detection methods or verifying the sent data becomes impractical. Consequently, the uploaded data must undergo extensive post-processing to extract an HD map from the noisy data collected by individual vehicles.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.2">An alternative to vehicle-based data collection is to acquire data from an aerial perspective. Aerial images can be efficiently captured using aircraft, satellites, or drones. The resulting resolution, known as the ground sampling distance, depends on factors such as flight altitude and camera optics. Currently, aircraft and drones can achieve resolutions of around <math alttext="5\text{\,}\mathrm{c}\mathrm{m}\mathrm{/}{px}" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.3"><semantics id="S2.SS1.p4.1.m1.3a"><mrow id="S2.SS1.p4.1.m1.3.3" xref="S2.SS1.p4.1.m1.3.3.cmml"><mn id="S2.SS1.p4.1.m1.1.1.1.1.1.1" xref="S2.SS1.p4.1.m1.1.1.1.1.1.1.cmml">5</mn><mtext id="S2.SS1.p4.1.m1.2.2.2.2.2.2" xref="S2.SS1.p4.1.m1.2.2.2.2.2.2.cmml">¬†</mtext><mrow id="S2.SS1.p4.1.m1.3.3.3.3.3.3" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.cmml"><mrow id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.cmml"><mi id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.2" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.2.cmml">cm</mi><mo id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.1" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.1.cmml">/</mo><mi id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.3" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.3.cmml">p</mi></mrow><mo id="S2.SS1.p4.1.m1.3.3.3.3.3.3.1" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.1.cmml">‚Å¢</mo><mi id="S2.SS1.p4.1.m1.3.3.3.3.3.3.3" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.3b"><apply id="S2.SS1.p4.1.m1.3.3.cmml" xref="S2.SS1.p4.1.m1.3.3"><csymbol cd="latexml" id="S2.SS1.p4.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p4.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S2.SS1.p4.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.SS1.p4.1.m1.1.1.1.1.1.1">5</cn><apply id="S2.SS1.p4.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3"><times id="S2.SS1.p4.1.m1.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.1"></times><apply id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2"><divide id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.1.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.1"></divide><ci id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.2.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.2">cm</ci><ci id="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.3.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.2.3">ùëù</ci></apply><ci id="S2.SS1.p4.1.m1.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p4.1.m1.3.3.3.3.3.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.3c">5\text{\,}\mathrm{c}\mathrm{m}\mathrm{/}{px}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.3d">start_ARG 5 end_ARG start_ARG times end_ARG start_ARG roman_cm / italic_p italic_x end_ARG</annotation></semantics></math> or higher, while civil satellites are limited to approximately <math alttext="25\text{\,}\mathrm{c}\mathrm{m}\mathrm{/}{px}" class="ltx_Math" display="inline" id="S2.SS1.p4.2.m2.3"><semantics id="S2.SS1.p4.2.m2.3a"><mrow id="S2.SS1.p4.2.m2.3.3" xref="S2.SS1.p4.2.m2.3.3.cmml"><mn id="S2.SS1.p4.2.m2.1.1.1.1.1.1" xref="S2.SS1.p4.2.m2.1.1.1.1.1.1.cmml">25</mn><mtext id="S2.SS1.p4.2.m2.2.2.2.2.2.2" xref="S2.SS1.p4.2.m2.2.2.2.2.2.2.cmml">¬†</mtext><mrow id="S2.SS1.p4.2.m2.3.3.3.3.3.3" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.cmml"><mrow id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.cmml"><mi id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.2" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.2.cmml">cm</mi><mo id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.1" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.1.cmml">/</mo><mi id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.3" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.3.cmml">p</mi></mrow><mo id="S2.SS1.p4.2.m2.3.3.3.3.3.3.1" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.1.cmml">‚Å¢</mo><mi id="S2.SS1.p4.2.m2.3.3.3.3.3.3.3" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.3b"><apply id="S2.SS1.p4.2.m2.3.3.cmml" xref="S2.SS1.p4.2.m2.3.3"><csymbol cd="latexml" id="S2.SS1.p4.2.m2.2.2.2.2.2.2.cmml" xref="S2.SS1.p4.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S2.SS1.p4.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S2.SS1.p4.2.m2.1.1.1.1.1.1">25</cn><apply id="S2.SS1.p4.2.m2.3.3.3.3.3.3.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3"><times id="S2.SS1.p4.2.m2.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.1"></times><apply id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2"><divide id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.1.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.1"></divide><ci id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.2.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.2">cm</ci><ci id="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.3.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.2.3">ùëù</ci></apply><ci id="S2.SS1.p4.2.m2.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p4.2.m2.3.3.3.3.3.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.3c">25\text{\,}\mathrm{c}\mathrm{m}\mathrm{/}{px}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.3d">start_ARG 25 end_ARG start_ARG times end_ARG start_ARG roman_cm / italic_p italic_x end_ARG</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib15" title="">15</a>]</cite>. While aerial perspective enables coverage of large areas, certain objects, such as signs, may not be visible or can be occluded by large trees. One significant advantage of aerial imagery is its accessibility, as it is often created by local governments for various purposes like surveying, preprocessed, and occasionally made freely available as digital orthophotos on a regular, annual basis.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Processing Aerial Imagery</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The processing of static aerial imagery has numerous potential applications across various domains, including agriculture and municipal use. In this paper, we focus specifically on methods for generating road maps. These methods can be categorised based on whether they create Standard Definition (SD) or High Definition (HD) maps. For road-level maps (SD), aerial imagery with lower resolution is generally sufficient. For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib16" title="">16</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib17" title="">17</a>]</cite>, data-driven methods have been proposed for detecting roads and determining their connectivity.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Other methods utilise higher-resolution imagery to extract the elements required for the creation of HD maps, with a particular emphasis on lane markings.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib18" title="">18</a>]</cite> propose an efficient method for detecting lane markings and symbols in complex urban regions using a set of simple image processing algorithms, including template matching. Azimi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib19" title="">19</a>]</cite> propose a method based on deep learning, where the typical encoder-decoder architecture is enhanced by wavelet transforms to achieve accurate semantic segmentation of lane markings. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib20" title="">20</a>]</cite>, a network architecture dedicated to semantic segmentation of lane markings is proposed, utilising capsule networks and self-attention.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Although existing methods have focused on accurately extracting high-level features, such as road connectivity, or low-level features, such as lane markings, from aerial images, none of them provide a complete semantic segmentation of a road segment in an aerial image to subsequently create a detailed HD map, including lane markings and symbols, in a typical format.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">In summary, the existing research has primarily concentrated on extracting specific road features, such as road connectivity or lane markings, from aerial images. However, there is a lack of methods that generate a comprehensive semantic segmentation of an aerial image, encompassing all relevant road features, including symbols, lane markings, roads, vegetation, walkways, and traffic islands, to create an HD map.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Lanelet2 Format</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">There are several formats available for HD maps, each offering different features and tools. One prominent format is Lanelet2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib11" title="">11</a>]</cite>, an open-source format introduced in 2018 that is primarily used in research. Lanelet2 uses a hierarchical structure to define the various components of an HD map. At the lowest level, the map consists of points (nodes) that represent positions in global coordinates. These points are organised into lines (linestrings), which are used to describe lane markings and other similar features. These lines can then be combined to create lanelets, which represent short segments of a lane. Lanelets can also be annotated with symbols and regulatory elements, such as speed limits. The storage format for Lanelet2 is an extension of the XML-based OSM file format. Given its intuitive nature, popularity within the research community and compatibility with free editing tools, we have selected the Lanelet2 format as the basis for our method. As the Lanelet2 format encompasses a vast range of elements, this paper focuses specifically on the essential elements that can be extracted from an aerial perspective.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">SEMANTIC SEGMENTATION OF AERIAL IMAGERY</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">High-resolution aerial imagery, captured by satellites or aircraft, is typically available as tiles of RGB images. In order to analyse the composition of the visible road segment at each pixel position, semantic segmentation is required. While each pixel in the RGB image represents a colour, semantic segmentation assigns a category from a predefined catalogue of semantic categories to each pixel, based on the object visible at that location. This representation is necessary for the subsequent process of creating the HD map.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Deep neural networks are the state-of-the-art method for generating semantic segmentations. These networks require training data to learn the mapping between RGB images and semantic segmentation. Specialised neural network architectures have been proposed for aerial imagery; however, since our focus is primarily on map generation, we utilise less complex architectures and simpler semantic categories, which have proven to be sufficient.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">At the time of this paper‚Äôs inception, there was no publicly available dataset that provided both sufficiently high resolution and all the necessary categories. Hence, we created our own dataset. We used aerial images published by the Aachen city administration, which are freely available for research purposes. We selected a total of 63 digital orthophotos taken in and around Aachen, Germany. These images depict typical urban elements such as straight roads, intersections, and roundabouts. All images have been rectified and geo-referenced, enabling the determination of each pixel‚Äôs position in UTM coordinates. Additionally, the images have a ground-sampling distance of 5 centimetres per pixel, providing a sufficiently high resolution to clearly discern individual elements such as lane markings. Typically, the images cover an area of approximately <span class="ltx_ERROR undefined" id="S3.p3.1.1">\qtyproduct</span>180x70.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">In order to reduce the manual annotation effort and consider the subsequent steps in the process, we removed all areas irrelevant to an HD map from the images in the initial annotation step by painting them over in black. This includes areas that are not in close proximity to a road or walkway. We used eight additional classes, divided into three groups, to annotate the remaining image areas. The first group describes the ground surface and includes five classes: road, walkway, parking space, vegetation, and traffic island. The second group comprises ground markings, such as lane markings and symbols. Symbols primarily encompass road arrows but can also include text written on the road. The last group involves annotating vehicles to detect occlusions of symbols, although this group is not used for semantic segmentation. Annotation was performed group by group, starting with the first group and not considering objects from subsequent groups. For example, a street‚Äôs annotation disregards occlusions caused by vehicles.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Limiting the annotations to lane markings and symbols, as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib19" title="">19</a>]</cite>, would have considerably reduced the annotation effort. However, this approach would have been impractical due to the absence of crucial information necessary for creating an HD map. For instance, the absence of lane markings on the outer edge of some roads implies that the transition from the road to vegetation only implicitly describes the end of a lane.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S1.F1" title="Figure 1 ‚Ä£ I INTRODUCTION ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">1</span></a> shows an annotation overlay on the RGB image. The class distribution (see Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S3.T1" title="TABLE I ‚Ä£ III SEMANTIC SEGMENTATION OF AERIAL IMAGERY ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">I</span></a>) reveals a notable imbalance among the classes. Particularly relevant classes, such as lane markings and symbols, are noticeably underrepresented. Although more than 50% of the total annotation points are used for annotating these polygons, they only cover less than 1.5% of the total area. Consequently, the weight of these classes was significantly increased during training. Another challenge arises from the fact that objects like lane markings or symbols are very small despite the high ground-sampling distance, yet they need to be accurately segmented, especially in the case of road arrows. Therefore, we chose network architectures capable of providing non-upscaled semantic segmentation at full or half input resolution.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">Dataset Class Distribution</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.1.1">Irrelevant</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.2.1">Road</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.3.1">Walkway</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.4.1">Vegetation</span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.2.2.1">58.7%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.2.2.2">14.8%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.2.2.3">5.22%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.4.2.2.4">19.9%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.3.1.1.1">Parking</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.3.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.3.1.2.1">Traffic Island</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.3.1.3.1">Symbol</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.3.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.3.1.4.1">Lane Marking</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.4.4.2.1">0.210%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.4.4.2.2">0.214%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.4.4.2.3">0.109%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.4.4.2.4">0.826%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">SYMBOL CLASSIFICATION</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In addition to lane markings, symbols play a crucial role in HD maps for vehicle trajectory planning. Symbols provide directional information for vehicles.
Given the presence of various symbol categories and their relatively small size in aerial images, it is beneficial to separate their classification from semantic segmentation by employing a dedicated network.
Among the symbols in our dataset, road arrows are the most frequently observed. Therefore, this paper focuses on road arrows and classifies them based on their pointing directions.
As the classification relies solely on shape rather than colour or texture, we can classify the symbols using their mask in the semantic segmentation.
If less than half of a road arrow is misclassified as a lane marking, potentially due to the same colour, we automatically detect this and assign the symbol class to all connected pixels.
Following that, we extract the road arrow‚Äôs mask from the resulting semantic segmentation for further classification.
Given the standardised shape of the symbols, their classification is relatively straightforward compared to the semantic segmentation of the entire image.
However, our experiments demonstrated that traditional classification methods, such as utilising HuMoments¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib21" title="">21</a>]</cite>, did not yield satisfactory results. Consequently, we developed a dedicated neural network for this purpose.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">CREATION OF HD MAPS</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The creation of HD maps is based on the semantic segmentation of aerial images and symbol classification. The segmentation identifies relevant parts in the image, which may be grouped and abstracted as needed to be mapped in the Lanelet2 format. Classical image processing techniques are primarily employed for this purpose. The outcome is an automatically generated prototypical HD map that encompasses visible elements and can subsequently be subjected to manual refinement using complementary information.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The process comprises a series of sequential steps. Initially, low-level elements such as contours are extracted through segmentation. After that, these elements are progressively processed and combined to derive road borders, lane markings, and lanes. The following overview provides a brief description of each step:</p>
</div>
<div class="ltx_para" id="S5.p3">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Segmentation: Aerial images are subjected to semantic segmentation to identify and isolate relevant parts.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Extraction of low-level elements: Initially, contours are extracted as low-level elements from the segmented image.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Processing and combination: The extracted contours are further processed and gradually combined to obtain road borders, lane markings, and eventually lanes.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Lanelet2 format mapping: The abstracted elements are mapped in the Lanelet2 format, facilitating their integration into the HD map.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The above steps provide a high-level overview of the HD map creation process, with further implementation details available in the associated source code publication.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Road Border</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The initial processing step involves extracting the drivable area and its boundary from the semantic segmentation. To achieve this, a mask is generated that includes all pixels belonging to the classes of road, lane markings, and symbols. Subsequently, the boundary of this area is determined as a set of contours. Since these contours describe the boundary with pixel-level precision and may exhibit slight noise, they are approximated as polylines through downsampling, enabling simplified further processing.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To ensure compliance with the Lanelet2 format, the polylines are then split whenever consecutive segments exhibit significant differences in orientation. This allows the generation of separate contours for each driving direction, as exemplified in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S5.F2" title="Figure 2 ‚Ä£ V-A Road Border ‚Ä£ V CREATION OF HD MAPS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">In the final stage, the contours are classified according to the Lanelet2 format¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib11" title="">11</a>]</cite> as either road borders or curbstones. This classification is based on the presence or absence of a sidewalk adjacent to the contour in the semantic segmentation.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="260" id="S5.F2.sf1.g1" src="extracted/5749206/images/Before_Split_2.jpg" width="150"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="259" id="S5.F2.sf2.g1" src="extracted/5749206/images/After_Split_2.png" width="150"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S5.F2.3.2" style="font-size:90%;">Exemplary result of road border splitting. Different colours represent individual road borders.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Lane Markings</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Once the drivable area is identified, all lane markings within it are located and classified. The process for this is similar to that for the road border, as a mask and contours are also extracted from the semantic segmentation. However, since the contours of the lane markings are not of interest, only their centre lines are extracted from the skeletonised mask as contours (see Fig.¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:overview</span>). Another important characteristic of lane markings is that they can split, for example, when lanes divide, and they can also merge subsequently. To detect these scenarios, the number of neighbouring points for each point on the contour is calculated. If a point has three neighbours, the angled part is identified as a separate contour. Detecting <math class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1.2"><mn id="S5.SS2.p1.1.m1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.1.cmml">90</mn><mo id="S5.SS2.p1.1.m1.1.1.2.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="S5.SS2.p1.1.m1.1.1.2.3" mathvariant="normal" xref="S5.SS2.p1.1.m1.1.1.1.cmml">¬∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn id="S5.SS2.p1.1.m1.1.1.1.cmml" type="float" xref="S5.SS2.p1.1.m1.1.1.2.2">90¬∞</cn></annotation-xml><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1c">90 ‚Å¢ ¬∞</annotation></semantics></math> angles, like those at stop lines, involves a similar process.
The final step involves grouping the dashed lane markings. Initially, the lane markings are detected in the mask by a size filter. Subsequently, they are iteratively grouped by searching for similar lane markings in terms of length and orientation. In the end, the lane markings are classified as either <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.1">dashed</em> or <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.2">solid</em>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Lanes</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The final processing step heuristically pairs the extracted information to form lanes. Road borders and lane markings are available as classified polylines resulting from the previous steps. To perform the pairing, possible corresponding counterparts are searched for each segment of the road borders or lane markings. We accomplish this by taking into account the distances and orientations of the elements. If a sufficient number of matches are found for the segments and there is a continuous road between the elements (without any obstructions such as a traffic island), they are assigned as part of a lane.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Exporting as Lanelet2 Map</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The final map needs to be exported in Lanelet2 format for further processing and utilisation. To accomplish this, all elements are converted from the image coordinate system to UTM coordinates. This conversion requires knowledge of the position of the upper left corner and the ground sampling distance for each image. Upon exporting, the internal map elements are mapped to the elements in Lanelet2 format, which utilises and extends the OSM format. Consequently, points are represented as OSM nodes, linestrings (polylines) are transformed into ways, and lanelets are converted into relations. Two points along the major axis and the symbol category describe detected symbols.</p>
</div>
<figure class="ltx_figure" id="S5.F3">
<p class="ltx_p ltx_align_center" id="S5.F3.1"><span class="ltx_text" id="S5.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="801" id="S5.F3.1.1.g1" src="x1.png" width="1196"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.4.2" style="font-size:90%;">Exemplary result of symbol classification.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">EXPERIMENTS</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">For a comprehensive evaluation of the resulting system, we evaluate semantic segmentation, symbol classification, and map creation as separate components.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Semantic Segmentation</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">As our initial architecture, we select the UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib22" title="">22</a>]</cite> architecture, which is well-suited for segmenting small objects since it operates at full resolution. However, due to high memory requirements, we modify the architecture by removing the last upsampling and the last block in the decoder to enable prediction at half resolution. As an alternative architecture, we employ a simplified UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib23" title="">23</a>]</cite> architecture, which combines FPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib24" title="">24</a>]</cite> with PPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib25" title="">25</a>]</cite> to achieve a larger receptive field. To obtain a semantic segmentation at half the original resolution, we modify the variant by eliminating the maxpooling layer at the beginning of the ResNet with a ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib26" title="">26</a>]</cite> backbone for prediction.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">To train the semantic segmentation networks, we randomly divide the entire dataset into training, validation, and testing sets in a 70:15:15 ratio. We train all networks using SGD with a learning rate of 0.01. Since the aerial images are large, we train on random crops and apply additional augmentation techniques such as rotations, scaling, and mirroring. Given the under-representation of lane markings and symbols in the cross-entropy loss, we empirically set the class weights for these two classes to 40 and 20, respectively. Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.T2" title="TABLE II ‚Ä£ VI-A Semantic Segmentation ‚Ä£ VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">II</span></a> presents a comparison of networks that use mIoU and IoU for the symbol class, which is crucial for the symbol classifier. The results indicate that the segmentation resolution used does not significantly affect the mIoU. However, upon qualitative examination of the segmentation masks, it is evident that only at half or full resolution is the segmentation of symbols sufficiently detailed for subsequent classification.
A more substantial quantitative difference is observed when using different architectures, with the UPerNet architecture yielding significantly better results. Furthermore, the results exhibit less noise, likely due to the larger receptive field. One limitation of all networks is the poor segmentation of parking areas and traffic islands. However, even for human annotators, distinguishing these areas during the annotation process was challenging, and the data only sparsely represents them (see Section III). Fortunately, these classes are less critical for HD map creation than the classes used to derive contours (lane markings, symbols, and roads).</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S6.T2.3.2" style="font-size:90%;">Semantic Segmentation Results</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.4.1.1.1.1">Architecture</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.4.1.1.2.1">Resolution</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.4.1.1.3.1">mIoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.4.1.1.4.1">IoU[Symbol]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T2.4.2.1.1">UNet</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.4.2.1.2">full</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T2.4.2.1.3">0.64</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.4.2.1.4">0.74</td>
</tr>
<tr class="ltx_tr" id="S6.T2.4.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T2.4.3.2.1">UNet</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.4.3.2.2">half</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T2.4.3.2.3">0.65</th>
<td class="ltx_td ltx_align_center" id="S6.T2.4.3.2.4">0.76</td>
</tr>
<tr class="ltx_tr" id="S6.T2.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T2.4.4.3.1">UPerNet</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.4.4.3.2">half</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T2.4.4.3.3"><span class="ltx_text ltx_font_bold" id="S6.T2.4.4.3.3.1">0.70</span></th>
<td class="ltx_td ltx_align_center" id="S6.T2.4.4.3.4"><span class="ltx_text ltx_font_bold" id="S6.T2.4.4.3.4.1">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.4.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T2.4.5.4.1">UPerNet</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T2.4.5.4.2">quarter</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T2.4.5.4.3">0.69</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.4.5.4.4">0.76</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Symbol Classification</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">For the purpose of training, validation, and testing, we utilised symbol masks extracted from the corresponding semantic segmentations produced by one of our trained networks. In order to expand the training and validation sets, which initially consisted of 470 symbols, we applied augmentation techniques such as slight rotation and scaling to the images. Since road arrows in aerial imagery are frequently subject to occlusion by vehicles and other objects, or may be cropped at the image boundaries, we also introduced random cropping and mask deletion to simulate these scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#bib.bib27" title="">27</a>]</cite> (refer to Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.F4" title="Figure 4 ‚Ä£ VI-B Symbol Classification ‚Ä£ VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">4</span></a>). Our symbol classification involves seven defined classes, namely left, right, straight, straight-or-left, straight-or-right, left-or-right, and other. The ‚Äùother‚Äù class encompasses symbols such as symbols at bus stops. To evaluate the robustness of our trained network against rotations, we augmented the test set with randomly rotated instances, ranging from 48 to 192 symbols.</p>
</div>
<figure class="ltx_figure" id="S6.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="98" id="S6.F4.sf1.g1" src="extracted/5749206/images/augment_example_ori.png" width="189"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="98" id="S6.F4.sf2.g1" src="extracted/5749206/images/augment_example_sym.png" width="102"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S6.F4.3.2" style="font-size:90%;">Mimicking occlusion of symbols by cropping and cutting out random parts.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">For our network architecture, we adopted a simple sequential model consisting of five convolutional layers with ReLU activations. The network was trained using Stochastic Gradient Descent (SGD) with a learning rate of 0.01 (gamma=0.92) and a batch size of 64, until convergence was achieved on the validation dataset. When evaluating the trained network on the test set, we observed a high accuracy of 94%. While the segmentation network failed to accurately segment two symbols, partial occlusions were responsible for the remaining misclassifications.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Map Creation</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">For the evaluation of the automatically created maps, manually created maps are necessary as a reference. Since the manual annotation of road borders and lane markings by polylines typically takes a large part of the time, we focused our evaluation on this. Accordingly, we manually annotated all lane markings and road borders by polylines for all images in the test dataset.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.2">In order to evaluate the automatically created polylines, their completeness and their local precision are to be analysed based on their control points. Since the control points of the automatically and manually created polylines are inhomogeneously distributed, they are resampled uniformly with a distance of <math alttext="10\text{\,}\mathrm{cm}" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.3"><semantics id="S6.SS3.p2.1.m1.3a"><mrow id="S6.SS3.p2.1.m1.3.3" xref="S6.SS3.p2.1.m1.3.3.cmml"><mn id="S6.SS3.p2.1.m1.1.1.1.1.1.1" xref="S6.SS3.p2.1.m1.1.1.1.1.1.1.cmml">10</mn><mtext id="S6.SS3.p2.1.m1.2.2.2.2.2.2" xref="S6.SS3.p2.1.m1.2.2.2.2.2.2.cmml">¬†</mtext><mi class="ltx_unit" id="S6.SS3.p2.1.m1.3.3.3.3.3.3" xref="S6.SS3.p2.1.m1.3.3.3.3.3.3.cmml">cm</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.3b"><apply id="S6.SS3.p2.1.m1.3.3.cmml" xref="S6.SS3.p2.1.m1.3.3"><csymbol cd="latexml" id="S6.SS3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S6.SS3.p2.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S6.SS3.p2.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S6.SS3.p2.1.m1.1.1.1.1.1.1">10</cn><csymbol cd="latexml" id="S6.SS3.p2.1.m1.3.3.3.3.3.3.cmml" xref="S6.SS3.p2.1.m1.3.3.3.3.3.3">centimeter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.3c">10\text{\,}\mathrm{cm}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.3d">start_ARG 10 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG</annotation></semantics></math>. Subsequently, individual points can be optimally assigned in a 1:1 manner, regardless of class, using the Hungarian algorithm with a maximum assignment distance of <math alttext="20\text{\,}\mathrm{cm}" class="ltx_Math" display="inline" id="S6.SS3.p2.2.m2.3"><semantics id="S6.SS3.p2.2.m2.3a"><mrow id="S6.SS3.p2.2.m2.3.3" xref="S6.SS3.p2.2.m2.3.3.cmml"><mn id="S6.SS3.p2.2.m2.1.1.1.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.1.1.1.cmml">20</mn><mtext id="S6.SS3.p2.2.m2.2.2.2.2.2.2" xref="S6.SS3.p2.2.m2.2.2.2.2.2.2.cmml">¬†</mtext><mi class="ltx_unit" id="S6.SS3.p2.2.m2.3.3.3.3.3.3" xref="S6.SS3.p2.2.m2.3.3.3.3.3.3.cmml">cm</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.3b"><apply id="S6.SS3.p2.2.m2.3.3.cmml" xref="S6.SS3.p2.2.m2.3.3"><csymbol cd="latexml" id="S6.SS3.p2.2.m2.2.2.2.2.2.2.cmml" xref="S6.SS3.p2.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S6.SS3.p2.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S6.SS3.p2.2.m2.1.1.1.1.1.1">20</cn><csymbol cd="latexml" id="S6.SS3.p2.2.m2.3.3.3.3.3.3.cmml" xref="S6.SS3.p2.2.m2.3.3.3.3.3.3">centimeter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.3c">20\text{\,}\mathrm{cm}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.2.m2.3d">start_ARG 20 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG</annotation></semantics></math>. We calculate the precision and recall for the test dataset based on the resulting assignments.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">To determine the impact of semantic segmentation errors on the generated map, we created maps for both the manually annotated semantic segmentation and the semantic segmentation created by the trained network. The results for the test set (see Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.00769v1#S6.T3" title="TABLE III ‚Ä£ VI-C Map Creation ‚Ä£ VI EXPERIMENTS ‚Ä£ DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles"><span class="ltx_text ltx_ref_tag">III</span></a>) show that the polylines are positioned with a high recall of 97% on the ground truth data. A high recall is essential, as it largely eliminates the need for time-consuming manual annotation of the contours. At the same time, there are only a few false positives, resulting in a precision of 97%. Visually analyzing the results reveals that errors typically occur near the image border and in more complex situations. The HD map created based on the semantic segmentation predicted by the neural network is only slightly worse, with a recall of 96% and a precision of 96%.
This indicates that the quality of semantic segmentation is high enough for the intended purpose.
In summary, the mapping results suggest that more than 95% of the manual contour annotations can be done automatically. This drastically reduces the effort required to create maps from aerial imagery.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T3.2.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S6.T3.3.2" style="font-size:90%;">Map Creation Error</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.4.1.1.1.1">Input</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.4.1.1.2.1">Precision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.4.1.1.3.1">Recall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.4.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.4.2.1.1">reference (human annotations)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.4.2.1.2">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.4.2.1.3">0.97</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.3.2">
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.4.3.2.1">prediction (trained model outputs)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.4.3.2.2">0.96</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.4.3.2.3">0.96</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">CONCLUSIONS</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper is motivated by the increasing automation of road traffic, which has created a high demand for HD maps for the research, development, and operation of automated vehicles. Aerial imagery provides a readily available source of data compared to expensive sensor-equipped measurement vehicles. However, manually creating HD maps from aerial photographs is labour-intensive.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">To address this challenge, we have proposed a method that combines neural networks for semantic segmentation with classical image processing algorithms to automatically generate prototype HD maps in the Lanelet2 format from aerial photographs. We created a dataset of 63 aerial photographs of urban street segments in Germany and semantically annotated them into eight relevant classes to train the network.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">To achieve a large receptive field and highly effective segmentation resolution simultaneously, we made modifications to the UPerNet architecture. This allowed us to achieve a mean Intersection over Union (mIoU) of 70% for segmenting the aerial images. Furthermore, we achieved 95% accuracy in classifying road arrows and a contour recall of 91% compared to the human reference for creating the HD map.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">By exporting the map to the Lanelet2 format, the automatically created prototype can be easily extended with application-specific information. This demonstrates the potential of semi-automated processing of aerial imagery for HD map creation, as a significant proportion of the labour-intensive steps can be automated.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
The Society of Automotive Engineers International, ‚ÄúTaxonomy and definitions
for terms related to driving automation systems for on-road motor vehicles,‚Äù
2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R.¬†Liu, J.¬†Wang, and B.¬†Zhang, ‚ÄúHigh definition map for automated driving:
Overview and analysis,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Journal of Navigation</span>, vol.¬†73, no.¬†2, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H.¬†Cai, Z.¬†Hu, G.¬†Huang, D.¬†Zhu, and X.¬†Su, ‚ÄúIntegration of gps, monocular
vision, and high definition (hd) map for accurate vehicle localization,‚Äù
<span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Sensors</span>, vol.¬†18, no.¬†10, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
F.¬†Poggenhans, N.¬†O. Salscheider, and C.¬†Stiller, ‚ÄúPrecise localization in
high-definition road maps for urban regions,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">2018 IEEE/RSJ
international conference on intelligent robots and systems (IROS)</span>,
pp.¬†2167‚Äì2174, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y.¬†Yoon, H.¬†Chae, and K.¬†Yi, ‚ÄúHigh-definition map based motion planning, and
control for urban autonomous driving,‚Äù tech. rep., Society of Automotive
Engineers, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y.Keisuke, K.¬†Akisuke, S.¬†Naoki, A.¬†Toru, A.¬†Mohammad, and Y.¬†Ryo, ‚ÄúRobust
traffic light and arrow detection using digital map with spatial prior
information for automated driving,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Sensors</span>, vol.¬†20, no.¬†4, p.¬†1181,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L.Martin, J.¬†Dominik, S.¬†Julian, P.¬†David, and H.¬†Andreas, ‚ÄúCrowdsourced hd
map patches based on road model inference and graph-based slam,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">IEEE Intell. Veh. Symp. (IV)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z.¬†Xu, Y.¬†Sun, and M.¬†Liu, ‚ÄúTopo-boundary: A benchmark dataset on topological
road-boundary detection using aerial images for autonomous driving,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Robotics and Automation Letters</span>, vol.¬†6, no.¬†4, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M.¬†Haklay and P.¬†Weber, ‚ÄúOpenstreetmap: User-generated street maps,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Pervasive computing</span>, vol.¬†7, no.¬†4, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Vargas-Munoz, S.¬†Srivastava, D.¬†Tuia, and A.¬†Falcao, ‚ÄúOpenstreetmap:
Challenges and opportunities in machine learning and remote sensing,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Geoscience and Remote Sensing Magazine</span>, vol.¬†9, no.¬†1, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P.¬†Fabian, P.¬†Jan-Hendrik, J.¬†Johannes, O.¬†Stefan, N.¬†Maximilian, K.¬†Florian,
and M.¬†Matthias, ‚ÄúLanelet2: A high-definition map framework for the future
of automated driving,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">IEEE Intell. Trans. Sys. Conf. (ITSC)</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S.¬†Hausler and M.¬†Milford, ‚ÄúMap creation, monitoring and maintenance for
automated driving ‚Äì literature review,‚Äù tech. rep., 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S.¬†Liu, L.¬†Li, J.¬†Tang, S.¬†Wu, and J.-L. Gaudiot, ‚ÄúCreating autonomous vehicle
systems,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Synthesis Lectures on Computer Science</span>, vol.¬†8, no.¬†2, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.¬†Liebner, D.¬†Jain, J.¬†Schauseil, D.¬†Pannen, and A.¬†Hackel√∂er, ‚ÄúCrowdsourced
hd map patches based on road model inference and graph-based slam,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE Intell. Veh. Symp. (IV)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.¬†Metwally, T.¬†M. Bazan, and F.¬†Eltohamy, ‚ÄúDesign of very high-resolution
satellite telescopes part i: Optical system design,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">IEEE Transactions
on Aerospace and Electronic Systems</span>, vol.¬†56, no.¬†2, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H.¬†Jiuxiang, R.¬†Anshuman, F.¬†J. C., C.¬†Ming, and W.¬†Peter, ‚ÄúRoad network
extraction and intersection detection from aerial images by tracking road
footprints,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Transactions on Geoscience and Remote Sensing</span>,
vol.¬†45, no.¬†12, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G.¬†M√°ttyus, W.¬†Luo, and R.¬†Urtasun, ‚ÄúDeeproadmapper: Extracting road topology
from aerial images,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">2017 IEEE International Conference on Computer
Vision (ICCV)</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J.¬†Kim, D.¬†Han, K.¬†Yu, Y.¬†Kim, and S.¬†Rhee, ‚ÄúEfficient extraction of road
information for car navigation applications using road pavement markings
obtained from aerial images,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Canadian Journal of Civil Engineering</span>,
vol.¬†33, no.¬†10, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S.¬†Azimi, P.¬†Fischer, M.¬†Korner, and P.¬†Reinartz, ‚ÄúAerial lanenet:
Lane-marking semantic segmentation in aerial imagery using wavelet-enhanced
cost-sensitive symmetric fully convolutional neural networks,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE
Transactions on Geoscience and Remote Sensing</span>, vol.¬†57, no.¬†5, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y.¬†Yu, Y.¬†Li, C.¬†Liu, J.¬†Wang, C.¬†Yu, X.¬†Jiang, L.¬†Wang, Z.¬†Liu, and Y.¬†Zhang,
‚ÄúMarkcapsnet: Road marking extraction from aerial images using
self-attention-guided capsule network,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">IEEE Geoscience and Remote
Sensing Letters</span>, vol.¬†19, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.¬†Zhang, F.¬†Xiang, J.¬†Pu, and Z.¬†Zhang, ‚ÄúApplication of improved hu moments
in object recognition,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">2012 IEEE International Conference on
Automation and Logistics</span>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O.¬†Ronneberger, P.¬†Fischer, and T.¬†Brox, ‚ÄúU-net: Convolutional networks for
biomedical image segmentation,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">International Conference on Medical
image computing and computer-assisted intervention</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T.¬†Xiao, Y.¬†Liu, B.¬†Zhou, Y.¬†Jiang, and J.¬†Sun, ‚ÄúUnified perceptual parsing
for scene understanding,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">European Conf. on Comput. Vision (ECCV)</span>,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T.¬†Lin, P.¬†Doll, R.¬†Girshick, K.¬†He, B.¬†Hariharan, and S.¬†Belongie, ‚ÄúFeature
pyramid networks for object detection,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">IEEE Conf. on Comput. vision
and pattern recognition</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H.¬†Zhao, J.¬†Shi, X.¬†Qi, X.¬†Wang, and J.¬†Jia, ‚ÄúPyramid scene parsing network,‚Äù
in <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">IEEE Conf. on Comput. vision and pattern recognition</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K.¬†He, X.¬†Zhang, S.¬†Ren, and J.¬†Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE Conf. on Comput. vision and pattern recognition</span>,
2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T.¬†DeVries and G.¬†W. Taylor, ‚ÄúImproved regularization of convolutional neural
networks with cutout,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv:1708.04552</span>, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 14:50:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
