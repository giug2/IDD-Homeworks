<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.20581] Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings</title><meta property="og:description" content="We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the Knesset Corpus, which comprises Israeli parliamentary proceedings. The model is based on the DictaBERT architecture and demonstrates signifi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.20581">

<!--Generated on Mon Aug  5 18:32:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Knesset-DictaBERT:
<br class="ltx_break">A Hebrew Language Model for Parliamentary Proceedings</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gili Goldin 
<br class="ltx_break">University of Haifa 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">gili.sommer@gmail.com</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Shuly Wintner 
<br class="ltx_break">University of Haifa 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">shuly@cs.haifa.ac.il</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the Knesset Corpus, which comprises Israeli parliamentary proceedings. The model is based on the DictaBERT architecture and demonstrates significant improvements in understanding parliamentary language according to the MLM task. We provide a detailed evaluation of the model’s performance, showing improvements in perplexity and accuracy over the baseline DictaBERT model.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Knesset-DictaBERT:
<br class="ltx_break">A Hebrew Language Model for Parliamentary Proceedings</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Gili Goldin</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center">University of Haifa</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.3.3.1.1" class="ltx_text ltx_font_typewriter">gili.sommer@gmail.com</span></span></span>
</span>
</span></span>                      <span id="p1.1.2.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.2.1.1.1" class="ltx_tr">
<span id="p1.1.2.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Shuly Wintner</span></span></span>
<span id="p1.1.2.2.1.2.2" class="ltx_tr">
<span id="p1.1.2.2.1.2.2.1" class="ltx_td ltx_align_center">University of Haifa</span></span>
<span id="p1.1.2.2.1.3.3" class="ltx_tr">
<span id="p1.1.2.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">shuly@cs.haifa.ac.il</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The field of natural language processing (NLP) has seen remarkable advancements in recent years, driven by the development of large language models. These models have significantly enhanced the ability to understand and generate human language. However, much of the effort of creating and training NLP models has been focused on English, while fewer NLP models are available for lower-resource languages such as Hebrew. We present one such model here.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Parliamentary proceedings are a valuable source of information for understanding political discourse, legislative processes, and more. Analyzing these texts requires models that can accurately capture the nuances of the language used in such settings. Despite the importance of this task, there has been a lack of specialized models trained on parliamentary corpora in Hebrew.
To address this gap, we fine-tuned the pre-trained Dicta-BERT model <cite class="ltx_cite ltx_citemacro_citep">(Shmidman et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> on the Knesset Corpus <cite class="ltx_cite ltx_citemacro_citep">(Goldin et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite>, a dataset of Israeli parliamentary proceedings. DictaBERT is a state-of-the-art Hebrew language model based on the BERT architecture <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>, which was trained on a diverse set of Hebrew texts.
Knesset-DictaBERT is the resulting fine-tuned model: it is specifically tailored for Hebrew parliamentary text. In this paper we describe the training process of this model, provide a detailed evaluation of its performance, and demonstrate its superiority over the baseline DictaBERT model on the Knesset data.
We believe that Knesset-DictaBERT will be a valuable resource for researchers and other users working on Hebrew language processing and political text analysis.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We fine-tuned the pre-trained DictaBERT model, a state of the art language model for Hebrew, specifically for the masked language modeling (MLM) task, on the full Knesset Corpus dataset <cite class="ltx_cite ltx_citemacro_citep">(Goldin et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite>. The model was initialized using the pre-trained weights from the <a target="_blank" href="https://huggingface.co/dicta-il/dictabert" title="" class="ltx_ref ltx_href">dicta-il/dictabert</a> checkpoint available on Hugging Face.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset and Data preprocessing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We used the Knesset Corpus, which is is a large Hebrew dataset of Israeli parliamentary proceedings, as the dataset for fine-tuning DictaBERT. The corpus contains over 32M sentences, over 384M tokens and comprises texts from both plenary and committee protocols.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The corpus was preprocessed to create text shards for efficient loading and processing. We split the dataset into training, validation, and test sets with ratios of 80%, 10%, and 10%, respectively.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">First, we tokenized the input text using the <a target="_blank" href="https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoTokenizer" title="" class="ltx_ref ltx_href">AutoTokenizer</a> from Hugging Face’s transformers library. Each text sample was tokenized into sequences of tokens, with each token represented by its corresponding ID from the tokenizer’s vocabulary.
To prepare the data for the MLM training, we grouped the tokenized texts into fixed length chunks of 256 tokens. The last chunk was padded to ensure consistent input sizes for the model. We used the <a target="_blank" href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling" title="" class="ltx_ref ltx_href">DataCollatorForLanguageModeling</a> from the transformers library to dynamically mask tokens during training, with a masking probability of 15%.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training procedure and hyperparameters</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We trained the model on a <a target="_blank" href="https://slurm.schedmd.com/documentation.html" title="" class="ltx_ref ltx_href">SLURM</a> (Simple Linux Utility for Resource Management) environment.
We utilized a distributed training setup with the <a target="_blank" href="https://developer.nvidia.com/nccl" title="" class="ltx_ref ltx_href">NCCL</a> backend to leverage multiple GPUs, ensuring efficient training and gradient synchronization.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We used the following training configuration: We used a per-device batch size of 32. In order to effectively double the batch size without increasing memory usage, we set the gradient accumulation steps to 4. A learning rate of <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mrow id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mn id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.cmml">​</mo><msup id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml"><mi id="S2.SS2.p2.1.m1.1.1.3.2" xref="S2.SS2.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S2.SS2.p2.1.m1.1.1.3.3" xref="S2.SS2.p2.1.m1.1.1.3.3.cmml"><mo id="S2.SS2.p2.1.m1.1.1.3.3a" xref="S2.SS2.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS2.p2.1.m1.1.1.3.3.2" xref="S2.SS2.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><times id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">1</cn><apply id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.3.1.cmml" xref="S2.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.3.2.cmml" xref="S2.SS2.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S2.SS2.p2.1.m1.1.1.3.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3.3"><minus id="S2.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">1e^{-4}</annotation></semantics></math> was chosen for the AdamW optimizer and a weight decay of 0.01 was applied to regularize the model. The model was trained for 2 full epochs.
We enabled ‘fp16’ (mixed precision) to speed up training and reduce memory usage. Mixed precision training uses 16-bit floating-point numbers instead of the standard 32-bit, which can significantly improve computational efficiency and decrease memory usage <cite class="ltx_cite ltx_citemacro_cite">Micikevicius et al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>.
Periodic evaluations were conducted on the validation set, and the best-performing model checkpoint was identified based on the validation loss.
Final evaluations were performed on the test set to assess the model’s performance. The model is available on Hugging Face hub at <a target="_blank" href="https://huggingface.co/GiliGold/Knesset-DictaBERT" title="" class="ltx_ref ltx_href">Knesset-DictaBERT</a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The fine-tuned Knesset-DictaBERT model was evaluated on the test set, which contained about 3.2 million sentences (about 38 million tokens), using perplexity as the primary metric. The model achieved a perplexity of 6.60, significantly outperforming the original DictaBERT model, which showed a perplexity of 22.87.
Evidently, the Knesset-DictaBERT model reflects the language of Knesset proceedings much better than the baseline DictaBERT model.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We also evaluated the fine-tuned Knesset-DictaBERT model for its accuracy in predicting masked tokens on a subset of the test set containing approximately 300K sentences (about 3.5 million tokens). Since MLM is considered a challenging task, where multiple options may be equally plausible, the evaluation focused on three accuracy metrics: top-1 accuracy, top-2 accuracy, and top-5 accuracy. These metrics measure the model’s ability to correctly predict the masked token as the first, within the first two, or within the first five predictions, respectively.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The Knesset-DictaBERT model correctly identified the masked token in the top-1 prediction 52.55% of the time, compared to the original Dicta model, which achieved a top-1 accuracy of 48.02%. Additionally, Knesset-DictaBERT succeeded in cases where the original DictaBERT model did not a total of 52,464 times. In contrast, the original DictaBERT model succeeded where Knesset-DictaBERT did not in only 27,995 times.
Furthermore, when considering the top-2 predictions, Knesset-DictaBERT correctly identified the masked token 63.07% of the time, whereas the original DictaBERT model had a top-2 accuracy of 58.60%. Moreover, Knesset-DictaBERT succeeded in 19,400 instances where the original model failed to provide a correct prediction within the top-2, while the original DictaBERT model, succeeded in only 13,862 instances where Knesset-DictaBERT did not.
On top of that, when extending the scope to the top-5 predictions, Knesset-DictaBERT demonstrated a significant improvement with a 73.59% accuracy, while the original DictaBERT model achieved a 68.98% accuracy.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">In all tested metrics the Knesset-DictaBERT model outperformed the original DictaBERT model, indicating a more robust performance in predicting masked tokens within parliamentary text. These results highlight the effectiveness of fine-tuning on the specific parliamentary corpus.
The results are presented in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiments and Results ‣ Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Metric</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Knesset-DictaBERT</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Original DictaBERT</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Perplexity</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.2.1.2.1" class="ltx_text ltx_font_bold">6.60</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.87</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Top-1 Accuracy</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.3.2.2.1" class="ltx_text ltx_font_bold">52.55%</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.02%</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Top-2 Accuracy</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.4.3.2.1" class="ltx_text ltx_font_bold">63.07%</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.60%</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Top-5 Accuracy</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.5.4.2.1" class="ltx_text ltx_font_bold">73.59%</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">68.98%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Knesset-DictaBERT and Original DictaBERT on perplexity and accuracy metrics</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and Future Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we successfully fine-tuned the DictaBERT model on the Knesset Corpus to create Knesset-DictaBERT, a model proficient at understanding and generating parliamentary language in Hebrew. The results indicate a robust model performance, with substantial improvements over the baseline model. Future work may involve evaluation on additional Hebrew datasets to enhance the model’s generalization capabilities and fine-tuning other language models on the Knesset corpus.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The model was fine-tuned specifically on the Knesset Corpus, which comprises parliamentary proceedings. As a result, its performance on general Hebrew text or other domains may not be as robust. However, the original DictaBERT model was trained on a variety of resources in Hebrew, which probably allows the Knesset-DictaBERT to still benefit from the diverse linguistic patterns and vocabulary present in the broader training data of the original model.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The Knesset Corpus may contain inherent biases, reflecting the political and social biases present in parliamentary discussions. Consequently, Knesset-DictaBERT may inherit these biases.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldin et al. (2024)</span>
<span class="ltx_bibblock">
Gili Goldin, Nick Howell, Noam Ordan, Ella Rabinovich, and Shuly Wintner. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:270068168" title="" class="ltx_ref ltx_href">The Knesset Corpus: An annotated corpus of Hebrew parliamentary proceedings</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2405.18115.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius et al. (2017)</span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1710.03740" title="" class="ltx_ref ltx_href">Mixed precision training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1710.03740.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shmidman et al. (2023)</span>
<span class="ltx_bibblock">
Shaltiel Shmidman, Avi Shmidman, and Moshe Koppel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.16687" title="" class="ltx_ref ltx_href">DictaBERT: A state-of-the-art BERT suite for Modern Hebrew</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.20580" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.20581" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.20581">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.20581" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.20582" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:32:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
