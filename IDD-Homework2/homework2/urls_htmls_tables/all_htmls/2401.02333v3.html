<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.02333] Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models</title><meta property="og:description" content="The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective
for retrieving information from diverse documents. However, challenges arise in handling complex
table queries, especia…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.02333">

<!--Generated on Tue Feb 27 07:25:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Retrieval-Augmented Generation (RAG) , Complex table queries, Context enrichment,  Information retrieval
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Uday Allu, Biddwan Ahmed, Vishesh Tripathi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">NLP Research Team</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Yellow.ai
<br class="ltx_break"></span>{uday, biddwan, vishesh.tripathi}@yellow.ai
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective
for retrieving information from diverse documents. However, challenges arise in handling complex
table queries, especially within PDF documents containing intricate tabular structures. This research
introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based
systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular
content separately. The extracted tables undergo a process of context enrichment, concatenating
headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we
employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG
architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5
API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside
other PDFs. Our approach aims to significantly improve the precision of complex table queries,
offering a promising solution to a longstanding challenge in information retrieval.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Retrieval-Augmented Generation (RAG) ,Complex table queries,Context enrichment, Information retrieval

</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2401.02333/assets/Example_1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The figure illustrates the Tabular Data Enhancement Assistant’s task, enhancing clarity by adding headings to row values and generating coherent sentences for tabular data extracted from PDFs. It provides a guide for the assistant, highlighting the desired outcome of improving tabular content understandability.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the era of information retrieval, the Retrieval-Augmented Generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> architecture stands as a robust framework for retrieving pertinent information from diverse documents. However, its efficacy faces a substantial hurdle when confronted with complex table queries, particularly within PDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> documents housing intricate tabular structures. This persistent challenge has prompted the development of a novel approach aimed at elevating the accuracy of complex table queries within RAG systems. Some previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> have been done, but no significant improvements have been made, especially when it comes to modifying the extracted data. This highlights the need for innovative solutions to address the limitations observed in existing methodologies</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Our approach begins by addressing the inherent limitations of RAG when dealing with tabular content. Instead of relying solely on textual retrieval, we advocate for a two-fold strategy. Firstly, PDF documents are stored in the retrieval Vector database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, ensuring a comprehensive repository of the original data. Secondly, a meticulous extraction process is implemented to separate and enrich tabular content. The enrichment process involves combining the headers and their corresponding values within the tables of PDF documents. This concatenation ensures that the context within complex rows is preserved, creating a more cohesive representation of the tabular content. By linking headers with their associated data, the augmented information becomes more structured and interpretable, allowing for improved understanding and accuracy in responding to complex table queries within the Retrieval-Augmented Generation architecture.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To imbue a deeper understanding of this enriched data, we integrate a fine-tuned version of the Llama-2-chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, a large language model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, specifically tailored for summarisation, within the RAG architecture. This adaptation allows for a nuanced interpretation of the tabular content through effective summarisation. To further enhance contextual comprehension, the enriched tabular data undergoes an additional layer of augmentation using the ChatGPT 3.5 API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with a one-shot prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The fine-tuned Llama-2 model for summarisation ensures a specialised capability in distilling key information. The augmented data, now possessing a refined contextual sense, is seamlessly integrated into the retrieval database alongside the original PDFs. This multifaceted approach aims to significantly enhance the accuracy of complex table queries, addressing a long-standing gap in information retrieval methodologies. As we delve into the intricacies of our methodology, this paper unfolds the layers of innovation driving a paradigm shift in the domain of document-based information retrieval</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Model Used in RAG Architecture</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our methodological framework is centered on the innovative Retrieval-Augmented Generation (RAG) architecture, which has garnered much acclaim for its unparalleled proficiency in information retrieval applications. At the nucleus of our framework is the seamless integration with Llama-2, a cutting-edge large language model explicitly fine-tuned for the nuanced task of summarization using massive datasets spanning various domains.The aptitude of Llama-2 in effectively condensing voluminous information into succinct summaries with remarkable coherence makes it an ideal companion for RAG architectures. During text generation, Llama-2 enhances the contextual awareness of the framework through selective retrieval of the most relevant knowledge from the provided documents and past context. The retrieved knowledge provides pertinent factual details and language cues to the generative model, guiding it to produce summaries that are abstractive yet grounded in the source content.Our methodology undertakes multi-step training of Llama-2 on large datasets to enable it to develop a comprehensive understanding of summarization across diverse topics and styles. The models are fine-tuned using supervised learning techniques that leverage gold-standard human written summaries as targets. This helps Llama-2 intrinsically build advanced capabilities for identifying and connecting key information from retrieved knowledge while generating cohesive, succinct summaries reflecting the essence of source texts.By bringing together the learned knowledge extraction strengths of Llama-2 and the informed text generation capacity of RAG, our framework is uniquely positioned to deliver superior summarization performance on complex real-world tasks, fulfilling core objectives such as generating accurate meeting notes, project reports and literature reviews.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The bedrock of our investigation is a meticulously curated dataset culled from a comprehensive repository of policy documents issued by authoritative entities. This judiciously assembled corpus, designed to closely mirror the complexities and multidimensionality of real-world information retrieval, comprises a diverse array of policy domains reflecting the intricate tapestry of contemporary governance. To rigorously evaluate the Retrieval-Augmented Generation (RAG) architecture in this context, we meticulously engineered a suite of 200 targeted queries. Each query functions as a finely calibrated instrument, crafted to probe the architecture’s prowess in navigating both the nuanced realm of textual information and the structured domain of tabular data. Notably, we placed a deliberate emphasis on achieving equilibrium in the representation of text-focused and table-related inquiries, ensuring a robust and well-rounded evaluation of the architecture’s capabilities.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiment Setup</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2401.02333/assets/Chart.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="354" height="196" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Visualizes query distribution in the experimental dataset: 170 complex table queries and 30 simple text queries out of a total of 200. Offers a balanced evaluation of Retrieval-Augmented Generation (RAG) architecture for both textual and tabular dimensions</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2401.02333/assets/Architecture_diagram.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="589" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Figure Represents presents the architectural diagram illustrating the experimental setup. The architecture is designed to demonstrate the workflow of our approach, showcasing the key components involved in enhancing the accuracy of complex table queries within the Retrieval-Augmented Generation (RAG) framework</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Query Processing</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our experimental design meticulously orchestrated a suite of 200 diverse queries, mirroring the multifaceted nature of real-world information retrieval. This deliberate composition comprised 170 queries delving into the intricacies of tabular data. Within this domain, we further established a nuanced dichotomy: 110 complex queries probed the architecture’s ability to parse intricate relationships and patterns within tables, while 60 assessed its proficiency in handling simpler tabular structures. Additionally, 30 control benchmarks, focusing on non-tabular text queries, provided a robust baseline for gauging the architecture’s comprehension of unstructured text information. This comprehensive query set ensured a rigorous and well-rounded evaluation of the architecture’s capabilities across a broad spectrum of information retrieval tasks.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Data Preparation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A pivotal facet of our methodological framework lies in the scrupulous preparation of data. Initially, PDF documents were judiciously archived within the retrieval database, creating an expansive reservoir of raw data. Subsequently, our approach veered towards a bifurcation strategy, wherein tabular content underwent a meticulous extraction process using the Camelot library, resulting in a repository of tables imbued with intricate structures. To catalyse context
enrichment, we embarked on a journey of amalgamation, deftly concatenating column headings with corresponding row values, thereby bestowing upon the extracted tables a deeper and more nuanced contextual essence</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Integration with Chat-GPT</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the symphony of our approach, the integration of advanced language models emerges as a crescendo of innovation. The Llama-2 model, an inherent constituent of the RAG architecture, permeated the experimental landscape. Its role was not merely perfunctory; rather, it served as the discerning arbiter of summarisation tasks, illuminating the tabular data with a refined understanding. Further enhancing the contextual tapestry, the tabular data traversed an additional stratum of augmentation through the ChatGPT 3.5 API. A one-shot prompt acted as the crucible, refining the data and imbuing it with a heightened sensibility, rendering it more palatable to the discerning faculties of our summarisation model. This meticulously choreographed integration culminated in the seamless storage of the enriched data within our retrieval repository, nestled alongside the original PDFs. This archival strategy was meticulously devised to furnish our summarisation model with an augmented understanding, transcending the complexities of information retrieval and fostering an environment conducive to heightened accuracy</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Methodology</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Simple Text Queries</span></td>
</tr>
<tr id="S3.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Accuracy (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Table Queries</span></td>
</tr>
<tr id="S3.T1.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Accuracy (%)</span></td>
</tr>
</table>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Overall Accuracy</span></td>
</tr>
<tr id="S3.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.2.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Normal Existing</td>
</tr>
<tr id="S3.T1.1.2.1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Pipeline</td>
</tr>
</table>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">86.6</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">48.2</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">54</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.3.2.1.1.1" class="ltx_tr">
<td id="S3.T1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Table Extracting</td>
</tr>
<tr id="S3.T1.1.3.2.1.1.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">separately &amp; Context</td>
</tr>
<tr id="S3.T1.1.3.2.1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Enrichment</td>
</tr>
</table>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">86.6</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">54.1</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">59.4</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S3.T1.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.4.3.1.1.1" class="ltx_tr">
<td id="S3.T1.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Parsing Enriched</td>
</tr>
<tr id="S3.T1.1.4.3.1.1.2" class="ltx_tr">
<td id="S3.T1.1.4.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Extracted Text to ChatGPT 3.5 Turbo</td>
</tr>
</table>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">93.3</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">61.1</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">66</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summarizes experiment outcomes, evaluating three methodologies for information retrieval accuracy. Improved metrics observed, especially in handling complex table queries. Parsing enriched text with Chat-GPT 3.5 Turbo achieves a substantial accuracy leap to 66%, addressing challenges in complex table structures</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section presents the outcomes of our meticulously designed experiments, comparing the performance of three distinct policy document summarization methods on our curated dataset of 200 queries. To comprehensively evaluate the efficacy of different approaches in policy document summarization, we meticulously designed a series of experiments involving three distinct methods and a curated dataset of 200 queries. As Table-I outlines, each method was assessed on its accuracy in retrieving relevant information from both textual and tabular sections of the documents.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Normal Existing Pipeline</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The conventional pipeline, serving as the baseline, solely extracts text from the documents and feeds it into a retrieval database. While achieving a respectable accuracy of 86.6% for text-based queries, its performance plummets to 48.2% for table-related queries. This stark drop highlights the inherent limitations of text-only approaches in comprehending the complexities of structured data within tables. Overall, the baseline approach yields an average accuracy of 54%, demonstrating the significant challenges posed by integrating tabular data into the summarization process.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">It’s worth noting that we employed the Camelot library for our table extraction method. Despite the advanced capabilities of Camelot, the challenges associated with intricate table structures still contributed to the lower performance in table-related queries.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Table Extracting Separately and Context Enrichment</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our second approach introduces a dedicated stage for extracting and enriching the context of tabular data. This refined method maintains the baseline’s 86.6% accuracy for text queries, while exhibiting a dramatic improvement in table query performance, achieving an accuracy of 54.1%. This substantial leap, resulting in an overall accuracy of 59.4%, confirms the effectiveness of our strategy in addressing the challenges of table-based information retrieval.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Parsing Extracted Text to Chat-GPT 3.5 API</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The pinnacle of our methodology lies in the integration of the Chat-GPT 3.5 API for parsing extracted text, unveiling a striking leap in performance. Remarkably, the accuracy for text queries surged to 93.3%, underscoring the adeptness in handling the intricacies of unformatted text. This notable improvement is particularly noteworthy as it attests to the robustness of our approach in retrieving relevant information from less structured textual data. Simultaneously, the accuracy for table queries experienced a substantial boost, reaching 61.1%. This transformative augmentation represents a significant stride in the adept handling of complex table queries. The amalgamation of robust text query accuracy and enhanced table query performance culminated in an impressive overall accuracy of 66%. This achievement signifies a noteworthy advancement in our methodology’s capacity to navigate and extract valuable insights from both unstructured
text and intricate table structures</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study embarks on a rigorous exploration of the Retrieval-Augmented Generation (RAG) architecture’s efficacy in tackling complex table queries within diverse policy documents. Through a meticulously crafted combination of model selection, dataset curation, and experimental design, we illuminate the intricacies of retrieving and comprehending information nestled within intricate tabular structures.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our findings, meticulously outlined in the prior section, shed light on the transformative potential of our proposed methodological interventions. While traditional pipelines prove adept at handling text-based queries, they stumble when navigating the labyrinthine complexities of complex tables. Our strategy, introducing separate extraction and context enrichment for tabular data, fosters a significant upward trajectory in accuracy metrics. This nuanced approach not only bolsters the system’s ability to handle table-related inquiries but also unveils the inherent challenges posed by these intricate structures.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The pinnacle of our work lies in the elegant integration of advanced language models within the RAG framework. Employing the Llama-2-chat model within the RAG architecture and subsequently parsing enriched data through the Chat-GPT 3.5 API results in a synergistic fusion that propels accuracy metrics to previously unattainable levels, particularly for complex table queries. These observed improvements highlight the profound understanding achieved through context enrichment and advanced language model integration, signaling a paradigm shift in the domain of information retrieval.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">However, our journey transcends mere improvements in accuracy. It serves as a powerful testament to the boundless potential for innovation within natural language processing and information retrieval. As we meticulously navigate the landscape of our findings, it becomes readily apparent that this proposed approach not only augments the current state of the art but also lays the groundwork for future advancements in information retrieval systems. By shedding light on the efficacy of context-aware language models and RAG architectures in tackling complex table queries, we pave the way for a future where information retrieval systems effectively bridge the gap between human cognition and the intricate structures of data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020, pp. 9459–9474.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Hardy, L. M. Masinter, D. Markovic, D. Johnson, and M. Bailey, “The application/pdf Media Type,” RFC 8118, Mar. 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Pan, M. Canim, M. Glass, A. Gliozzo, and J. Hendler, “End-to-end table question answering via retrieval-augmented generation,” 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. Stata, K. Bharat, and F. Maghoul, “The term vector database: fast access to indexing terms for web pages,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Networks</em>, vol. 33, no. 1, pp. 247–255, 2000.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,” 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30.   Curran Associates, Inc., 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020, pp. 1877–1901.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How Can We Know What Language Models Know?” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 8, pp. 423–438, 07 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.02332" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.02333" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.02333">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.02333" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.02334" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 07:25:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
