<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.08543] ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model</title><meta property="og:description" content="Recommender Systems (RS) play a pivotal role in boosting user satisfaction by providing personalized product suggestions in domains such as e-commerce and entertainment. This study examines the integration of multimoda…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.08543">

<!--Generated on Sat Oct  5 19:54:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Recommender system,  large language model,  audio,  Multimodal Recommender System
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zezheng Qin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Faculty of Computer Science and Information Technology</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Universiti Putra Malaysia
<br class="ltx_break"></span>Selangor, Malaysia 
<br class="ltx_break">Zezheng.chin@gmail.com
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Recommender Systems (RS) play a pivotal role in boosting user satisfaction by providing personalized product suggestions in domains such as e-commerce and entertainment. This study examines the integration of multimodal data—text and audio—into large language models (LLMs) with the aim of enhancing recommendation performance. Traditional text and audio recommenders encounter limitations such as the cold-start problem, and recent advancements in LLMs, while promising, are computationally expensive. To address these issues, Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without compromising performance. The ATFLRec framework is proposed to integrate audio and text modalities into a multimodal recommendation system, utilizing various LoRA configurations and modality fusion techniques. Results indicate that ATFLRec outperforms baseline models, including traditional and graph neural network-based approaches, achieving higher AUC scores. Furthermore, separate fine-tuning of audio and text data with distinct LoRA modules yields optimal performance, with different pooling methods and Mel filter bank numbers significantly impacting performance. This research offers valuable insights into optimizing multimodal recommender systems and advancing the integration of diverse data modalities in LLMs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Recommender system, large language model, audio, Multimodal Recommender System

</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">During activities such as listening to music, reading novels, and online shopping, Provider can leverage historical data and user preferences to recommend products that better align with user tastes, thereby increasing user satisfaction<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This is made possible with the assistance of recommender systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recommender Systems (RS) are information filtering systems designed to predict and suggest items or content that users may find interesting, such as products, movies, music, or articles<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. These predictions are based on past user behaviors, preferences, or the behaviors of similar users. The primary goal of RS is to enhance user experience, increase engagement, and facilitate decision-making processes. This applies to various domains, including e-commerce, entertainment, and social media. Based on the primary data modalities of the recommended content, recommender systems are categorized into audio recommender systems, text recommender systems, multimodal recommender systems, and others<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Audio recommender systems are designed to recommend personalized audio content based on users’ listening habits, preferences, and behaviors. Audio recommender systems find extensive application in music recommendations, audiobook recommendations and short video recommendations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For short video recommendations, audio recommender systems utilize fewer training and deployment resources compared to directly using video data for recommendations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Text recommender systems process and analyze text data generated from user interactions with the system, employing methods such as content-based filtering, collaborative filtering, and machine learning approaches, including GRU4Rec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and Graph Neural Network-based recommendation systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, these methods struggle with the cold-start problem<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. With the rapid development of large language models (LLMs), LLMs have demonstrated outstanding capabilities in recommender systems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Nonetheless, the substantial number of parameters in these LLM-based systems makes adapting the entire system to performing recommended work computationally impractical and costly. Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>addresses these issues by modifying specific system parameters using low-rank matrices, showing great promise. This approach is memory-efficient during training and does not impact the runtime of recommender systems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As a result, recommender systems leveraging the fine-tuning of large language models have gained popularity. These approaches utilize a few-shot training setup, selecting a limited number of samples from the training set for model training. While they effectively address the cold-start problem, they do not account for other data modalities present in recommender system data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In many cases, purely text-based interactions with LLMs may be limited, as they often fail to capture information that is difficult to convey through text alone. For instance, audio can encode a range of emotions in speech, while images can represent geometric shapes and object positions, which may be challenging to describe in text<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To address these issues, multimodal recommender models have emerged. Multimodal recommender systems combine multiple data modalities (such as text, images, audio, video, etc.) for content recommendation. They are more complex than traditional single-modal recommender systems because they need to simultaneously handle and integrate information from different modalities, providing more accurate and personalized recommendations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Although multimodal data offers more comprehensive user and content information, making recommendations more personalized and precise, effectively integrating modalities and enhancing the recommendation performance of large language models remains a significant challenge<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Additionally, as noted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, fine-tuning large language models with a combination of audio and text content can significantly improve their inference capabilities. However, this study only integrates text and audio information into a single LoRA module for fine-tuning, without exploring the potential benefits of separately fine-tuning audio and text modalities on the performance of large language models.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To address these challenges, this paper proposes the following research questions:
RQ1: How can audio data and text information be effectively integrated to achieve collaborative recommendation?
RQ2: How does the proposed method perform compared to other models?
RQ3: What are the differences in training recommender systems using various low-rank adaptation modules?
RQ4: What are the effects of using different modality aggregation methods on the performance of recommender systems?</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Based on these questions and to better leverage the rich knowledge capabilities of large language models, this paper integrates audio modality content into large language models for joint recommendation. Additionally, the model attempts fine-tuning using various numbers and combinations of LoRA modules at a low parameter level. To the best of the author’s knowledge, this is the first study to integrate audio and text data modalities while employing instruction-tuning of large language models for fine-tuning and joint recommendations.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The main contributions of this paper are as follows:
1. Proposing a multimodal recommendation system that integrates audio modality content into large language models.
2. Exploring the impact of different LoRA modules at a low parameter level on large language models, providing empirical insights for fine-tuning multimodal models.
3. Investigating the effects of audio stacking pooling methods, multimodal data fusion pooling methods, and the number of filters on recommendation system performance when processing audio data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This work focuses on integrating text and audio modalities within large language model recommender systems, with particular emphasis on exploring methods to enhance the performance of multimodal recommender systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Model Framework</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This section introduces the ATFLRec framework, aimed at facilitating the integration of audio and text data and the effective and efficient alignment of LLMs with recommendation tasks, especially in low GPU memory settings. The ATFLRec framework includes audio extraction and embedding, alignment of audio and text features, and the setup of the LoRA fine-tuning model.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The overall framework of ATFLRec is illustrated in Figure 1. Initially, user historical data is matched with item data to generate textual instructions, as shown in Table I. The text is then tokenized using the LLM’s tokenizer and fed into the embedding module to obtain text features. Simultaneously, FBank feature extraction is applied to the audio of user-liked and target items, followed by deep feature processing. The audio features of item and target items are then fused and aligned dimensionally with the text features. Finally, both text and audio features are input into the multimodal large model recommender system, where fine-tuning is performed using LoRA modules based on the labels.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.08543/assets/Fframework.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model Framework</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Internal Structure of the Large Model recommender System</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Figure 2 illustrates the internal structure of the large model recommender system. Initially, the input data comprises audio and text vectors. Attention masks and position embedding features are then applied to the text vectors. Both audio and text vectors are subsequently fed into the large model for fine-tuning. The resulting audio and text features are fused through feature fusion. Finally, the fused features are input into a classifier for classification.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.08543/assets/FMODELIN.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="332" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Internal Structure of the Large Model</figcaption>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Audio Embedding Extraction</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">To effectively extract time-frequency features from audio, this study uses a Mel filter bank(FBank) for audio feature representation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as figure 3. Specifically, the Mel filter bank is computed with a frame length of 25 milliseconds and a frame shift of 10 milliseconds. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, after feature processing, the audio embedding model processes these FBank features through a series of fully connected layers with nonlinear activation functions (SiLU).</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">The specific structure of the audio embedding network is as follows: initially, the low-dimensional FBank input is mapped to a 256-dimensional hidden space through a linear transformation. This is followed by the application of the SiLU activation function to introduce nonlinearity, and further mapping to a higher-dimensional space to align with the text features dimensions, finally undergoing batch normalization to ensure stable gradient propagation and faster convergence.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.08543/assets/FAUDIOEX.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="336" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Audio Embedding Extraction</figcaption>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Recommender system fine-tuning modules</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The features generated by the audio and text are used as fine-tuning data for various LoRA modules. This study utilizes the RoBERTa model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The causal self-attention parameters of the model are adjusted using parameter-efficient low-rank adaptation (LoRA), while all other parameters remain frozen.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">To explore the impact of different LoRA module configurations on recommender system (RS) performance and optimize the integration of text and audio data, this paper adjusts the original LoRA fine-tuning module. As shown in Figure 4, Figure 4(a) illustrates a structure with two LoRA low-rank matrix modules, which independently fine-tune audio and text data before feature fusion through the Fusion module. Figure 4(b) features a single LoRA low-rank matrix module that first integrates audio and text data before performing overall fine-tuning. Figure 4(c) follows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, using a single LoRA low-rank matrix module to fine-tune text data. Through these different model design configurations, this study evaluates their impact on model performance.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">In the ablation study, I examine the impact of different LoRA modules on the performance of the recommender system.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2409.08543/assets/FLORA.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Three Fine-Tuning LORA Modules</figcaption>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Instruction Tuning</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Instruction tuning is a core technique for training large-scale language models based on human-annotated instructions and responses<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This paper aims to perform recommendation tuning (rec-tuning) on large language models (LLMs) using recommendation data to develop a large-scale recommender system capable of predicting user preferences for new items. To achieve this, I convert recommendation data into the format required for instruction tuning, as illustrated in Table I.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>A tuning sample for rec-tuning.</figcaption><img src="/html/2409.08543/assets/T6.png" id="S2.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="281" alt="[Uncaptioned image]">
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<section id="S3.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS1.4.1.1" class="ltx_text">III-</span>1 </span>Dataset Description</h4>

<div id="S3.SS0.SSS1.p1" class="ltx_para">
<p id="S3.SS0.SSS1.p1.1" class="ltx_p">MicroLens is a large-scale dataset focused on micro-video recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The MicroLens dataset contains 1 billion interaction records between users and videos, involving 34 million users and 1 million micro-videos. Each video provides comprehensive multimodal information, including titles, video content, user likes, and views. Audio data is extracted from the videos using an audio extraction program. Since critical information in short videos is typically found at the beginning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, to better handle the audio data features, the audio is truncated to the first 30 seconds.</p>
</div>
<div id="S3.SS0.SSS1.p2" class="ltx_para">
<p id="S3.SS0.SSS1.p2.1" class="ltx_p">To test whether the model can effectively address the cold-start problem of recommender systems with very limited training data, I use a few-shot training setting, where a limited number of samples are randomly selected from the training set for model training, as followed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This is referred to as the ”K-shot” training setting, where K represents the number of training samples used. By setting a very small K value, it is possible to evaluate whether the method can quickly gain recommendation capability from the LLM with very limited training data.</p>
</div>
</section>
<section id="S3.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS2.4.1.1" class="ltx_text">III-</span>2 </span>Comparison Models</h4>

<div id="S3.SS0.SSS2.p1" class="ltx_para">
<p id="S3.SS0.SSS2.p1.1" class="ltx_p">Since this approach utilizes historical interactions to predict subsequent interactions, similar to sequential recommendation methods, the following sequential models are compared: GRU4Rec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a recommendation algorithm based on Gated Recurrent Units. It captures the dynamic interests of users and makes recommendations based on historical behavior sequences. SASRec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a sequence recommendation model based on the self-attention mechanism, which uses self-attention layers from the Transformer to model dependencies in user behavior sequences. GCN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is a graph neural network model designed to learn node feature representations by performing convolution operations on graph-structured data. GraphSAGE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is a graph neural network model focused on handling large-scale graph data, which is particularly useful for managing large-scale user-item interaction graphs.</p>
</div>
</section>
<section id="S3.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS3.4.1.1" class="ltx_text">III-</span>3 </span>Evaluation Metrics</h4>

<div id="S3.SS0.SSS3.p1" class="ltx_para">
<p id="S3.SS0.SSS3.p1.1" class="ltx_p">Since this model aims to predict user preferences for a given target item as a binary classification problem, this paper uses the widely adopted evaluation metric in recommendation systems: the Area Under the Receiver Operating Characteristic Curve (AUC)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS0.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS4.4.1.1" class="ltx_text">III-</span>4 </span>Implementation Details</h4>

<div id="S3.SS0.SSS4.p1" class="ltx_para">
<p id="S3.SS0.SSS4.p1.1" class="ltx_p">To ensure consistent sequence length, this paper standardizes the text length to 512, which matches the model’s maximum supported length.</p>
</div>
<div id="S3.SS0.SSS4.p2" class="ltx_para">
<p id="S3.SS0.SSS4.p2.1" class="ltx_p">For training all LoRA modules, binary cross-entropy loss is used, with default LoRA hyperparameters set to R = 4. The optimizer is Adam. To enhance model convergence speed and avoid instability during training, a two-phase learning rate scheduling scheme is employed. Initially, a warm-up scheduler is used to gradually increase the learning rate; subsequently, a step decay learning rate is applied during the main training phase. In the warm-up phase, the learning rate increases from a minimal value (<math id="S3.SS0.SSS4.p2.1.m1.1" class="ltx_Math" alttext="10^{-9}" display="inline"><semantics id="S3.SS0.SSS4.p2.1.m1.1a"><msup id="S3.SS0.SSS4.p2.1.m1.1.1" xref="S3.SS0.SSS4.p2.1.m1.1.1.cmml"><mn id="S3.SS0.SSS4.p2.1.m1.1.1.2" xref="S3.SS0.SSS4.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS0.SSS4.p2.1.m1.1.1.3" xref="S3.SS0.SSS4.p2.1.m1.1.1.3.cmml"><mo id="S3.SS0.SSS4.p2.1.m1.1.1.3a" xref="S3.SS0.SSS4.p2.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS0.SSS4.p2.1.m1.1.1.3.2" xref="S3.SS0.SSS4.p2.1.m1.1.1.3.2.cmml">9</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS4.p2.1.m1.1b"><apply id="S3.SS0.SSS4.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS4.p2.1.m1.1.1.1.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS0.SSS4.p2.1.m1.1.1.2.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1.2">10</cn><apply id="S3.SS0.SSS4.p2.1.m1.1.1.3.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1.3"><minus id="S3.SS0.SSS4.p2.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS0.SSS4.p2.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS4.p2.1.m1.1.1.3.2">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS4.p2.1.m1.1c">10^{-9}</annotation></semantics></math>) to the set initial learning rate over 50 iterations. After the warm-up phase, the learning rate begins to gradually decay, linearly decreasing from the initial value to 0. The batch size is set to 8, with gradient accumulation performed every 4 iterations. For each training epoch, incomplete batches of data are discarded. The training process is completed on a NVIDIA RTX 3080 16GB GPU.</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Performance Comparison</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As shown in Table II, this study evaluates various recommendation models, including GRU4REC, SASRec, GCN, GraphSAGE, and ATFLRec, under few-shot learning settings with sample sizes of 100 and 500. The results reveal that ATFLRec performs exceptionally well in both few-shot settings, achieving AUC values of 0.6042 and 0.6708, respectively.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The superior performance of ATFLRec can be attributed to its advanced integration of text and audio modalities, which enhances the model’s ability to capture user preferences. The higher AUC values indicate that ATFLRec is more effective at distinguishing between positive and negative recommendations, thus offering more personalized and accurate suggestions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In contrast, traditional deep learning recommendation methods consistently perform poorly in the few-shot training settings. This suggests that conventional deep learning approaches require a large number of training parameters and struggle to rapidly acquire recommendation capabilities with limited training samples.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Furthermore, for graph neural network-based recommendation systems, GraphSAGE demonstrates its strength through its neighbor sampling and aggregation mechanisms, which enable the model to maintain high computational efficiency when processing large-scale graph data. However, this sampling mechanism has limitations, as it may overlook significant graph structural information, potentially affecting the final recommendation performance<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance Comparison</figcaption><img src="/html/2409.08543/assets/T1.png" id="S3.T2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="340" alt="[Uncaptioned image]">
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Ablation Study: Assessing the Impact of Fine-Tuning Modules on Recommender System Performance</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The purpose of this section is to investigate the impact of different LoRA fine-tuning modules on the performance of recommender systems. As shown in Table III, the analysis reveals that using separate LoRA modules to train text and audio data, followed by modal fusion for recommendation, yields the best performance. Conversely, models employing a single LoRA module and considering only textual information exhibit the worst performance. This suggests that integrating audio data with text data can enhance the performance of recommender systems.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Moreover, the results indicate that training audio and text data separately with different low-rank matrices for subsequent fusion produces better performance than using a single low-rank matrix to handle both modalities simultaneously. This finding provides valuable insights into the fine-tuning of multimodal data.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ablation Study</figcaption><img src="/html/2409.08543/assets/T2.png" id="S3.T3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="219" alt="[Uncaptioned image]">
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Exploring the Impact of Feature Fusion from Same and Different Modalities on Recommender System Performance</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">This section explores the impact of data fusion methods on system performance. Modal fusion can be categorized into fusion within the same modality and fusion across different modalities. During the fusion process, some important features of the data are strengthened and amplified, while less important features may be omitted. To identify the optimal fusion method, this paper conducted experiments on both audio-only fusion and audio-text fusion, as shown in Table IV. The results indicate that the recommender system performs optimally with the maximum pooling operation. In the final step of the model, audio and text features extracted from LoRA training are stacked and fused using pooling methods. Among these, the Sum pooling operation yields the best performance for the recommender system<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Impact of Feature Fusion from Same and Different Modalities</figcaption><img src="/html/2409.08543/assets/T3.png" id="S3.T4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="163" alt="[Uncaptioned image]">
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Exploring the Impact of the Number of FBanks</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">As shown in Table V, FBank captures the energy distribution of audio signals across various frequency bands, helping the model better understand frequency characteristics<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Therefore, the choice of the number of FBanks used can significantly impact the final performance of the recommender system. The system achieves the best performance with 80 FBanks.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Impact of the Number of FBanks</figcaption><img src="/html/2409.08543/assets/T5.png" id="S3.T5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="127" alt="[Uncaptioned image]">
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This paper introduces ATFLRec, an innovative multimodal recommender system that combines audio and text data to enhance performance. Integrating these modalities into a large-scale language model allows for more precise capture of user preferences and more personalized recommendations. The system’s core advantage is its efficient use of low-rank adaptation (LoRA) modules, which improve recommendation capability while saving computational resources. Experimental results show that ATFLRec outperforms traditional deep learning methods in few-shot settings, with the best performance achieved when audio and text data are separately trained and then fused using different LoRA modules. Additionally, maximum pooling of audio and text features and a configuration of 80 FBANKs optimize performance, capturing frequency characteristics effectively. Future research will focus on exploring interactions between modalities and optimizing techniques for multimodal data fusion.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Tongxiao (Catherine) Zhang, Ritu Agarwal, and Henry C Lucas Jr.

</span>
<span class="ltx_bibblock">The value of it-enabled retailer learning: personalized product recommendations and customer store loyalty in electronic markets.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">MIS quarterly</span>, pages 859–881, 2011.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Pradeep Kumar Singh, Pijush Kanti Dutta Pramanik, Avick Kumar Dey, and Prasenjit Choudhury.

</span>
<span class="ltx_bibblock">Recommender systems: an overview, research trends, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Journal of Business and Systems Research</span>, 15(1):14–52, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yashar Deldjoo, Markus Schedl, Balázs Hidasi, Yinwei Wei, and Xiangnan He.

</span>
<span class="ltx_bibblock">Multimedia recommender systems: Algorithms and challenges.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Recommender systems handbook</span>, pages 973–1014. Springer, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Darius Afchar, Alessandro Melchiorre, Markus Schedl, Romain Hennequin, Elena Epure, and Manuel Moussallam.

</span>
<span class="ltx_bibblock">Explainability in music recommender systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">AI Magazine</span>, 43(2):190–208, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Dietmar Jannach and Malte Ludewig.

</span>
<span class="ltx_bibblock">When recurrent neural networks meet the neighborhood for session-based recommendation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the eleventh ACM conference on recommender systems</span>, pages 306–310, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui.

</span>
<span class="ltx_bibblock">Graph neural networks in recommender systems: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, 55(5):1–37, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Nícollas Silva, Diego Carvalho, Adriano CM Pereira, Fernando Mourão, and Leonardo Rocha.

</span>
<span class="ltx_bibblock">The pure cold-start problem: A deep study about how to conquer first-time users in recommendations domains.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Information Systems</span>, 80:1–12, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al.

</span>
<span class="ltx_bibblock">Recommender systems in the era of large language models (llms).

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al.

</span>
<span class="ltx_bibblock">A survey on efficient inference for large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.14294</span>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.

</span>
<span class="ltx_bibblock">Tallrec: An effective and efficient tuning framework to align large language model with recommendation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the 17th ACM Conference on Recommender Systems</span>, pages 1007–1014, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.

</span>
<span class="ltx_bibblock">M6-rec: Generative pretrained language models are open-ended recommender systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.08084</span>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al.

</span>
<span class="ltx_bibblock">Prompting large language models with speech recognition abilities.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 13351–13355. IEEE, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Quoc-Tuan Truong, Aghiles Salah, and Hady Lauw.

</span>
<span class="ltx_bibblock">Multi-modal recommender systems: Hands-on exploration.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the 15th ACM Conference on Recommender Systems</span>, pages 834–837, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zhanli Li and Pengfei Song.

</span>
<span class="ltx_bibblock">Audio similarity detection algorithm based on siamese lstm network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP)</span>, pages 182–186. IEEE, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.

</span>
<span class="ltx_bibblock">Swin-unet: Unet-like pure transformer for medical image segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 205–218. Springer, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Pieter Delobelle, Thomas Winters, and Bettina Berendt.

</span>
<span class="ltx_bibblock">Robbert: a dutch roberta-based language model.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.06286</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.10792</span>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan He, Yongfeng Zhang, and Fajie Yuan.

</span>
<span class="ltx_bibblock">A content-driven micro-video recommendation dataset at scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.15379</span>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Xing Lu and Zhicong Lu.

</span>
<span class="ltx_bibblock">Fifteen seconds of fame: A qualitative study of douyin, a short video sharing mobile application in china.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Social Computing and Social Media. Design, Human Behavior and Analytics: 11th International Conference, SCSM 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26-31, 2019, Proceedings, Part I 21</span>, pages 233–244. Springer, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B Hidasi.

</span>
<span class="ltx_bibblock">Session-based recommendations with recurrent neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06939</span>, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wang-Cheng Kang and Julian McAuley.

</span>
<span class="ltx_bibblock">Self-attentive sequential recommendation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2018 IEEE international conference on data mining (ICDM)</span>, pages 197–206. IEEE, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang.

</span>
<span class="ltx_bibblock">Lightgcn: Simplifying and powering graph convolution network for recommendation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</span>, pages 639–648, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Will Hamilton, Zhitao Ying, and Jure Leskovec.

</span>
<span class="ltx_bibblock">Inductive representation learning on large graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Davide Chicco and Giuseppe Jurman.

</span>
<span class="ltx_bibblock">The matthews correlation coefficient (mcc) should replace the roc auc as the standard metric for assessing binary classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">BioData Mining</span>, 16(1):4, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang.

</span>
<span class="ltx_bibblock">Understanding negative sampling in graph representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</span>, pages 1666–1676, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Adolfo Almeida, Johan Pieter de Villiers, Allan De Freitas, and Mergandran Velayudan.

</span>
<span class="ltx_bibblock">The complementarity of a diverse range of deep learning features extracted from video content for video recommendation.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, 192:116335, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zrar Kh Abdul and Abdulbasit K Al-Talabani.

</span>
<span class="ltx_bibblock">Mel frequency cepstral coefficient and its applications: A review.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 10:122136–122158, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.08542" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.08543" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.08543">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.08543" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.08544" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:54:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
