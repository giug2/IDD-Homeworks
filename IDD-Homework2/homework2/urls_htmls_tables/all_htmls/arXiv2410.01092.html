<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer</title>
<!--Generated on Tue Oct  1 21:39:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.01092v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S1" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S2" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S3" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S4" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S5" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S7" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S8" title="In Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgement</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Vlatko Spasev 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1">vlatko.spasev@finki.ukim.mk</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id5.2.id2">\And</span>Ivica Dimitrovski 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.3.id3">ivica.dimitrovski@finki.ukim.mk</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id7.4.id4">\And</span>Ivan Chorbev 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.5.id5">ivan.chorbev@finki.ukim.mk</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id9.6.id6">\And</span>Ivan Kitanovski 
<br class="ltx_break"/>Faculty of Computer Science and Engineering
<br class="ltx_break"/>University Ss Cyril and Methodius
<br class="ltx_break"/>Skopje 1000, North Macedonia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.7.id7">ivan.kitanovski@finki.ukim.mk</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">The escalating use of Unmanned Aerial Vehicles (UAVs) as remote sensing platforms has garnered considerable attention, proving invaluable for ground object recognition. While satellite remote sensing images face limitations in resolution and weather susceptibility, UAV remote sensing, employing low-speed unmanned aircraft, offers enhanced object resolution and agility. The advent of advanced machine learning techniques has propelled significant strides in image analysis, particularly in semantic segmentation for UAV remote sensing images. This paper evaluates the effectiveness and efficiency of SegFormer, a semantic segmentation framework, for the semantic segmentation of UAV images. SegFormer variants, ranging from real-time (B0) to high-performance (B5) models, are assessed using the UAVid dataset tailored for semantic segmentation tasks. The research details the architecture and training procedures specific to SegFormer in the context of UAV semantic segmentation. Experimental results showcase the model’s performance on benchmark dataset, highlighting its ability to accurately delineate objects and land cover features in diverse UAV scenarios, leading to both high efficiency and performance.</p>
<p class="ltx_p" id="id3.3"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="id3.3.1">K</em><span class="ltx_text ltx_font_bold" id="id3.3.2">eywords</span> Semantic segmentation  <math alttext="\cdot" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⋅</annotation></semantics></math>
Deep learning  <math alttext="\cdot" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">⋅</annotation></semantics></math>
SegFormer  <math alttext="\cdot" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">⋅</annotation></semantics></math>
UAV images.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the increasing utilization of Unmanned Aerial Vehicles (UAVs) as remote sensing platforms has generated substantial interest and served as valuable resources for ground object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib1" title="">1</a>]</cite>. Satellite remote sensing images, while widely used, often exhibit limitations such as low resolution for low-altitude targets and susceptibility to weather conditions, leading to obscured ground objects and challenges in object recognition. In contrast, UAV remote sensing employs low-speed unmanned aircraft as aerial platforms equipped with infrared and camera technology to capture image data. UAVs, flying at lower altitudes compared to satellites, can closely approach the ground, enhancing object resolution significantly. The close-range image resolution can reach the centimeter level, enabling the efficient collection of low-altitude, high-resolution aerial images in a timely and cost-effective manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib2" title="">2</a>]</cite>. Camera tilt is pivotal in shaping UAV imagery quality and coverage. Vertical aerial photography, with a perpendicular camera axis, offers limited coverage. In contrast, low oblique images result from deliberate tilting (15° to 30°), excluding the horizon and providing a broader perspective. High oblique imagery, with a greater tilt (approximately 60°), captures a larger land area. The visible horizon distinguishes high oblique photos, making them suitable for comprehensive landscape analysis and documentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent advancements in machine learning, along with the wealth of remote sensing data available, have significantly improved image analysis and interpretation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib3" title="">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib4" title="">4</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib5" title="">5</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib6" title="">6</a>]</cite>. A particularly exciting area of research is semantic segmentation for UAV remote sensing images, which allows for precise analysis of ground objects and their relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib7" title="">7</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib8" title="">8</a>]</cite>. Semantic segmentation tasks focus on labeling each pixel of an image with a corresponding class of what the pixel represents. This results in a detailed segmentation map where every pixel has a specific class designation. This technique allows for fine-grained object identification within an image, unlike object detection which focuses on broader localization of objects. Semantic segmentation of remote sensing images is a fundamental task in the field of remote sensing and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib9" title="">9</a>]</cite>. The goal is to partition the image into meaningful regions, enabling detailed analysis and understanding of the Earth’s surface. This finer granularity of analysis provides a more profound understanding of the intricate spatial distribution of features within remote sensing images. In the past few years, global research interest in UAV remote sensing has surged, driven by its mobility, speed, and economic advantages. Evolving from research and development, this technology has transitioned to practical applications, positioning itself as one of the forefront aerial remote sensing technologies for the future.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Semantic segmentation of UAV remote sensing images finds diverse applications, including environmental monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib10" title="">10</a>]</cite>. Its ability to swiftly update, correct, and enhance geo-environmental information and outdated GIS databases provides crucial technical support for the administration of government and related departments, as well as for land and geo-environmental management. Furthermore, UAV remote sensing proves valuable in electric power inspection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib11" title="">11</a>]</cite>, agricultural monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib12" title="">12</a>]</cite>, high-speed patrol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib13" title="">13</a>]</cite>, disaster monitoring and prevention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib14" title="">14</a>]</cite>, meteorological detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib15" title="">15</a>]</cite>, aerial survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib16" title="">16</a>]</cite>, and various other applications. The effectiveness of semantic segmentation methods is paramount for the practical implementation of various applications. As these applications continue to evolve, there is a growing demand for real-time execution of semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This paper assesses the effectiveness and efficiency of SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib18" title="">18</a>]</cite> in semantic segmentation tasks involving UAV images. SegFormer comes in several variants, denoted by code names B0 to B5. Among these, B0 is the smallest model tailored for real-time applications, while B5 is the largest model designed for high performance. We chose the UAVid dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib19" title="">19</a>]</cite>, specifically curated for UAV video data in semantic segmentation tasks, with a focus on urban scenes. The evaluation of effectiveness involves reporting the mean intersection over union (<math alttext="mIoU" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mi id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">m</mi><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">I</mi><mo id="S1.p4.1.m1.1.1.1a" xref="S1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p4.1.m1.1.1.4" xref="S1.p4.1.m1.1.1.4.cmml">o</mi><mo id="S1.p4.1.m1.1.1.1b" xref="S1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p4.1.m1.1.1.5" xref="S1.p4.1.m1.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><times id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1"></times><ci id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">𝑚</ci><ci id="S1.p4.1.m1.1.1.3.cmml" xref="S1.p4.1.m1.1.1.3">𝐼</ci><ci id="S1.p4.1.m1.1.1.4.cmml" xref="S1.p4.1.m1.1.1.4">𝑜</ci><ci id="S1.p4.1.m1.1.1.5.cmml" xref="S1.p4.1.m1.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math>), while efficiency is gauged through metrics such as the number of parameters, frames per second (FPS), and latency for the different SegFormer variants. Latency denotes the duration it takes for the model to analyze an image and provide information about the identified segments/objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The paper is organized as follows: Section 2 provides a review of existing research on semantic segmentation techniques applied to UAV remote sensing images. Section 3 elucidates the key features of the SegFormer semantic segmentation framework. Section 4 provides an overview of the dataset utilized in the research. Section 5 comprehensively outlines the experimental setup, including data preprocessing, training protocols, model parameters, and evaluation measures. Section 6 presents the experimental results alongside relevant discussions. Finally, Section 7 concludes the paper, summarizing the findings and contributions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Semantic segmentation extends the concept of image classification by assigning a class label to each pixel in an image, rather than just the entire image. Semantic segmentation in remote sensing images presents unique challenges due to factors such as high resolution, complex spatial structures, diverse object scales, and large data volumes. Early attempts at semantic segmentation relied on traditional machine learning methods. These methods fell into two main categories: pixel-based and region-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib20" title="">20</a>]</cite>. However, they had limitations. They depended heavily on manually designed features and setting thresholds for those features, which could be time-consuming and ineffective for complex images. These images often have challenges like different lighting conditions, textures, and object sizes. Traditional methods often struggled with these complexities, leading to inconsistent performance and limited usefulness. This paved the way for the adoption of modern deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The advent of deep learning, particularly with the introduction of Convolutional Neural Networks (CNNs) and Fully Convolutional Networks (FCNs), has transformed the field of semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib21" title="">21</a>]</cite>. FCNs, often combined with encoder-decoder architectures, are now the leading approach. Early FCNs used repeated convolutions and pooling to make predictions for each pixel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib22" title="">22</a>]</cite>. Newer models, like U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib23" title="">23</a>]</cite> and SegNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib24" title="">24</a>]</cite>, combine high-level features (capturing large-scale information) with low-level details (preserving sharp boundaries) during decoding. This improves both capturing the overall scene and precisely identifying objects. To see more of an image at once (increasing the receptive field), techniques, like dilated convolutions were introduced in DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib25" title="">25</a>]</cite>. Later advancements like PSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib26" title="">26</a>]</cite> and UperNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib27" title="">27</a>]</cite> incorporated spatial pyramid pooling to capture information at different scales within the image. DeepLabV3+ combined these ideas into a powerful yet efficient architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib28" title="">28</a>]</cite>. Subsequent advancements, as demonstrated by PSANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib29" title="">29</a>]</cite> and DRANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib30" title="">30</a>]</cite>, have replaced traditional pooling methods with attention mechanisms applied to encoder feature maps, enhancing the ability to capture long-range dependencies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Most recently, researchers have explored using transformers, a type of neural network architecture that excels at capturing long-range relationships between image parts. Models like Segmenter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib31" title="">31</a>]</cite>, SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib18" title="">18</a>]</cite>, and MaskFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib32" title="">32</a>]</cite> all leverage transformers for improved performance. Segmenter uses a specialized transformer backbone and a mask decoder, while SegFormer offers a simpler yet effective solution with transformers as encoders and lightweight decoders. Inspired by DEtection TRansformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib33" title="">33</a>]</cite>, MaskFormer uses transformers to directly generate object masks, making it versatile for various segmentation tasks. To address limitations in MaskFormer, Mask2Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib34" title="">34</a>]</cite> introduced a multi-scale decoder and a masked attention mechanism.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Semantic segmentation of UAV images presents a formidable challenge due to a confluence of factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib8" title="">8</a>]</cite>. The diverse nature of UAV imagery, characterized by a wide spectrum of resolutions and object orientations, necessitates robust models capable of generalizing across disparate datasets. The inherent scale variability within single images, ranging from expansive structures to diminutive objects, demands models that can adeptly handle such disparities, accurately segmenting both large-scale and fine-grained elements. Densely populated urban environments and the minute details often found in natural landscapes pose significant obstacles to precise object delineation. Moreover, the imbalanced distribution of classes within UAV imagery, where certain categories are represented far less frequently than others, hinders model training and can lead to biased segmentation results. Finally, the intricate and cluttered backgrounds common in aerial imagery introduce additional complexity, making the separation of objects from their surroundings a demanding task.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The complex nature of remote sensing imagery, characterized by varying resolutions, diverse object scales, and intricate background patterns, has necessitated the development of sophisticated semantic segmentation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib35" title="">35</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib36" title="">36</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib37" title="">37</a>]</cite>. Recent advancements in deep learning have spurred significant progress in this domain. AerialFormer, for example, presents a hierarchical framework that effectively captures multi-scale features through a Transformer encoder while refining segmentation details using a Multi-Dilated Convolutional Neural Network (MD-CNN) decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib35" title="">35</a>]</cite>. UNetFormer, on the other hand, introduces a global-local Transformer block (GLTB) to enhance contextual understanding, coupled with a feature refinement head for precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib36" title="">36</a>]</cite>. To optimize computational efficiency, the model integrates a lightweight CNN-based encoder with a Transformer decoder. A novel approach is embodied by the Uncertainty-Aware Network (UANet), which incorporates uncertainty modeling to mitigate the challenges posed by complex background elements and improve the accuracy of building footprint segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib37" title="">37</a>]</cite>. These innovative architectures collectively demonstrate the ongoing exploration of effective strategies for tackling the unique challenges inherent in remote sensing image analysis. By leveraging the power of deep learning and addressing specific limitations, researchers are steadily advancing the field of semantic segmentation in this domain.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">SegFormer is a semantic segmentation model that, unlike traditional methods heavily reliant on convolutional neural networks (CNNs), combines Transformers with lightweight multilayer perceptron (MLP) decoders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib18" title="">18</a>]</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S3.F1" title="Figure 1 ‣ 3 Model Architecture ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the architecture of the SegFormer semantic segmentation framework.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F1.g1" src="extracted/5894136/figures/segformer_architecture.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the SegFormer semantic segmentation framework architecture. The image is taken from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib18" title="">18</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The SegFormer model incorporates a novel hierarchically structured Transformer encoder that produces multiscale features. This component excels at capturing intricate relationships between distant image regions, crucial for tasks like segmenting objects with complex shapes or fine details. Additionally, the encoder’s design allows it to extract features at various resolutions. This proves beneficial for segmentation as it captures both the overall image context and minute details necessary for accurate pixel-by-pixel classification. The encoder that is used in SegFormer is named Mix Transformer (MiT). A series of MiT encoders labeled MiT-B0 through MiT-B5, has been designed with identical architectures but differing sizes. MiT-B0 is utilized as the lightweight model for rapid inference, while MiT-B5 is employed as the largest model to achieve optimal performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Furthermore, SegFormer does not rely on positional encoding, which is a standard approach in transformer-based models. This technique, originally used in natural language processing, accounts for the order of words in a sentence. In image data, the relative position of pixels is inherent, making positional encoding redundant in the case of SegFormer. Not requiring positional encoding alleviates the need for interpolating positional codes, which otherwise degrades performance when the testing resolution diverges from the training resolution.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Finally, what is also specific about SegFormer is its’ lightweight decoder setup. It employs an MLP decoder that aggregates information from different layers. This design choice maintains good performance while ensuring efficiency. The decoder is key in combining information from the different encoder outputs, effectively merging local and global attention resulting in robust representations for precise segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The UAVid dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib19" title="">19</a>]</cite> is composed of 420 images, each with dimensions of 4096×2160 or 3840×2160 pixels. For training and validation purposes, 200 and 70 images were allocated, respectively, while the remaining images were dedicated to testing. Captured within complex urban settings from an oblique perspective at a 50-meter altitude, the UAVid images present a variety of stationary and moving objects. The dataset exhibits a side view perspective, offering a unique vantage point for analysis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Comprising eight distinct classes, the UAVid dataset categorizes objects into building, road, static car, tree, low vegetation, human, moving car, and background clutter. The UAVid dataset encompasses a comprehensive set of urban scene elements, including residential and under-construction buildings collectively categorized as buildings, stationary and moving vehicles, clearly defined road surfaces (excluding parking lots and sidewalks), and a background clutter class encompassing miscellaneous urban features. Buildings, trees, and roads constitute the predominant visual components within UAVid images, while cars and pedestrians represent a smaller but significant portion of the dataset, accounting for approximately 3% of the total class distribution. A detailed breakdown of pixel distribution across the training, validation, and testing subsets is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S4.F2" title="Figure 2 ‣ 4 Dataset ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">2</span></a>. To provide visual context, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.F4" title="Figure 4 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">4</span></a> offers representative examples of UAVid images alongside their corresponding ground truth masks, showcasing the dataset’s rich and varied content.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="519" id="S4.F2.g1" src="x1.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The pixel distribution across labels in the train, validation, and test splits of the UAVid dataset.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Original images of UAVid dataset are very large, thus pre-processing of them is adopted. Utilizing a fixed clip size of 512 pixels and a stride size of 256 pixels for generating clipped images, the intersection of width and height is calculated to ensure comprehensive coverage of the entire image. Via this procedure, a cumulative sum of 8000 images was assigned for training, with an additional 2800 images specifically designated for validation. The images maintain a resolution of 512x512 pixels. The images within the test split remained unaltered and underwent no modifications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We are evaluating the performance of three SegFormer variants utilizing distinct MiT encoders, namely MiX-B0, MiX-B3, and MiX-B5 as encoders with input size of 512. Additionally, the fine-tuned versions of the MiT encoders on the ImageNet-1k dataset are incorporated into our analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib18" title="">18</a>]</cite>. We evaluate the efficiency of the different encoders by reporting the number of parameters, FPS, and latency. Additionally, we evaluate the performance of an ensemble method, which involves combining the SegFormer-B3 and SegFormer-B5 models. In this ensemble approach, we derive the final predictions by calculating the geometric mean of the base models’ predictions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Models were trained on the training split, with hyperparameter tuning conducted using the validation set. To mitigate overfitting, early stopping was implemented, terminating training if validation loss failed to improve over 20 epochs. The optimal model, determined by the highest evaluation metric on the validation set, was retained and subsequently evaluated on the unseen test split to assess final performance. The maximum training duration was capped at 100 epochs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">To bolster model robustness and generalization on the UAVid dataset, we implemented a multifaceted data augmentation pipeline during training. This process encompassed a combination of standard and more complex image transformations. Basic augmentations included random horizontal flipping and adjustments to image brightness and contrast. To introduce additional variability and challenge the model, we randomly applied more intricate transformations such as contrast limited adaptive histogram equalization, grid distortion, and optical distortion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib38" title="">38</a>]</cite>. To ensure consistency with the ImageNet-1k dataset, we standardized image pixel values using their corresponding mean and standard deviation. It is essential to note that these augmentations were exclusively applied during the training phase. When evaluating model performance on the validation and test sets, images underwent only normalization to provide an unbiased assessment. This rigorous data augmentation approach was instrumental in improving model resilience to variations within the UAVid dataset, ultimately enhancing overall segmentation accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.3">Our experimental protocol adhered to a fixed batch size of 12 samples per training iteration. To optimize model parameters, we leveraged the AdamW optimizer, a variant of Adam that incorporates weight decay, with an initial learning rate set to <math alttext="1e-4" class="ltx_Math" display="inline" id="S5.p5.1.m1.1"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mrow id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml"><mn id="S5.p5.1.m1.1.1.2.2" xref="S5.p5.1.m1.1.1.2.2.cmml">1</mn><mo id="S5.p5.1.m1.1.1.2.1" xref="S5.p5.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.p5.1.m1.1.1.2.3" xref="S5.p5.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml">−</mo><mn id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><minus id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"></minus><apply id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2"><times id="S5.p5.1.m1.1.1.2.1.cmml" xref="S5.p5.1.m1.1.1.2.1"></times><cn id="S5.p5.1.m1.1.1.2.2.cmml" type="integer" xref="S5.p5.1.m1.1.1.2.2">1</cn><ci id="S5.p5.1.m1.1.1.2.3.cmml" xref="S5.p5.1.m1.1.1.2.3">𝑒</ci></apply><cn id="S5.p5.1.m1.1.1.3.cmml" type="integer" xref="S5.p5.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S5.p5.1.m1.1d">1 italic_e - 4</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib39" title="">39</a>]</cite>. Recognizing the potential benefits of a gradually decreasing learning rate in stabilizing training and enhancing convergence, we employed a polynomial decay schedule. This learning rate adjustment strategy involves a polynomial function that systematically reduces the learning rate from an initial value of <math alttext="1e-4" class="ltx_Math" display="inline" id="S5.p5.2.m2.1"><semantics id="S5.p5.2.m2.1a"><mrow id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml"><mrow id="S5.p5.2.m2.1.1.2" xref="S5.p5.2.m2.1.1.2.cmml"><mn id="S5.p5.2.m2.1.1.2.2" xref="S5.p5.2.m2.1.1.2.2.cmml">1</mn><mo id="S5.p5.2.m2.1.1.2.1" xref="S5.p5.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S5.p5.2.m2.1.1.2.3" xref="S5.p5.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S5.p5.2.m2.1.1.1" xref="S5.p5.2.m2.1.1.1.cmml">−</mo><mn id="S5.p5.2.m2.1.1.3" xref="S5.p5.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><apply id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1"><minus id="S5.p5.2.m2.1.1.1.cmml" xref="S5.p5.2.m2.1.1.1"></minus><apply id="S5.p5.2.m2.1.1.2.cmml" xref="S5.p5.2.m2.1.1.2"><times id="S5.p5.2.m2.1.1.2.1.cmml" xref="S5.p5.2.m2.1.1.2.1"></times><cn id="S5.p5.2.m2.1.1.2.2.cmml" type="integer" xref="S5.p5.2.m2.1.1.2.2">1</cn><ci id="S5.p5.2.m2.1.1.2.3.cmml" xref="S5.p5.2.m2.1.1.2.3">𝑒</ci></apply><cn id="S5.p5.2.m2.1.1.3.cmml" type="integer" xref="S5.p5.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S5.p5.2.m2.1d">1 italic_e - 4</annotation></semantics></math> to a final value of <math alttext="1e-7" class="ltx_Math" display="inline" id="S5.p5.3.m3.1"><semantics id="S5.p5.3.m3.1a"><mrow id="S5.p5.3.m3.1.1" xref="S5.p5.3.m3.1.1.cmml"><mrow id="S5.p5.3.m3.1.1.2" xref="S5.p5.3.m3.1.1.2.cmml"><mn id="S5.p5.3.m3.1.1.2.2" xref="S5.p5.3.m3.1.1.2.2.cmml">1</mn><mo id="S5.p5.3.m3.1.1.2.1" xref="S5.p5.3.m3.1.1.2.1.cmml">⁢</mo><mi id="S5.p5.3.m3.1.1.2.3" xref="S5.p5.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="S5.p5.3.m3.1.1.1" xref="S5.p5.3.m3.1.1.1.cmml">−</mo><mn id="S5.p5.3.m3.1.1.3" xref="S5.p5.3.m3.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.3.m3.1b"><apply id="S5.p5.3.m3.1.1.cmml" xref="S5.p5.3.m3.1.1"><minus id="S5.p5.3.m3.1.1.1.cmml" xref="S5.p5.3.m3.1.1.1"></minus><apply id="S5.p5.3.m3.1.1.2.cmml" xref="S5.p5.3.m3.1.1.2"><times id="S5.p5.3.m3.1.1.2.1.cmml" xref="S5.p5.3.m3.1.1.2.1"></times><cn id="S5.p5.3.m3.1.1.2.2.cmml" type="integer" xref="S5.p5.3.m3.1.1.2.2">1</cn><ci id="S5.p5.3.m3.1.1.2.3.cmml" xref="S5.p5.3.m3.1.1.2.3">𝑒</ci></apply><cn id="S5.p5.3.m3.1.1.3.cmml" type="integer" xref="S5.p5.3.m3.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.3.m3.1c">1e-7</annotation><annotation encoding="application/x-llamapun" id="S5.p5.3.m3.1d">1 italic_e - 7</annotation></semantics></math> over a specified number of training steps. To achieve a robust and balanced optimization of the semantic segmentation model, we adopted a hybrid loss function that combines the strengths of Cross Entropy loss and Dice loss. Cross Entropy loss, a standard measure of classification performance, quantifies the pixel-wise discrepancy between predicted and ground truth segmentation masks. However, it may not adequately capture object boundaries, a critical aspect of semantic segmentation. To address this limitation, we incorporated Dice loss, a metric that specifically focuses on the overlap between predicted and ground truth regions. By combining these complementary loss functions, our model was able to effectively learn discriminative features, accurately localize object boundaries, and achieve a comprehensive understanding of the image content.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.2">To process large input images, we adopt a sliding window approach with a 1024-pixel window size and a 128-pixel overlap between adjacent patches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib40" title="">40</a>]</cite>. To further enhance prediction accuracy, we incorporate test-time augmentation (TTA) by horizontally flipping input images. The final segmentation map is obtained by averaging the predictions from both the original and flipped images. To assess model performance, we employed the Intersection over Union (<math alttext="IoU" class="ltx_Math" display="inline" id="S5.p6.1.m1.1"><semantics id="S5.p6.1.m1.1a"><mrow id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml"><mi id="S5.p6.1.m1.1.1.2" xref="S5.p6.1.m1.1.1.2.cmml">I</mi><mo id="S5.p6.1.m1.1.1.1" xref="S5.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.p6.1.m1.1.1.3" xref="S5.p6.1.m1.1.1.3.cmml">o</mi><mo id="S5.p6.1.m1.1.1.1a" xref="S5.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.p6.1.m1.1.1.4" xref="S5.p6.1.m1.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><apply id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1"><times id="S5.p6.1.m1.1.1.1.cmml" xref="S5.p6.1.m1.1.1.1"></times><ci id="S5.p6.1.m1.1.1.2.cmml" xref="S5.p6.1.m1.1.1.2">𝐼</ci><ci id="S5.p6.1.m1.1.1.3.cmml" xref="S5.p6.1.m1.1.1.3">𝑜</ci><ci id="S5.p6.1.m1.1.1.4.cmml" xref="S5.p6.1.m1.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">IoU</annotation><annotation encoding="application/x-llamapun" id="S5.p6.1.m1.1d">italic_I italic_o italic_U</annotation></semantics></math>) metric, calculated as the ratio of the overlapping area between predicted and ground truth segmentation masks to their combined area. Additionally, we reported the mean IoU (<math alttext="mIoU" class="ltx_Math" display="inline" id="S5.p6.2.m2.1"><semantics id="S5.p6.2.m2.1a"><mrow id="S5.p6.2.m2.1.1" xref="S5.p6.2.m2.1.1.cmml"><mi id="S5.p6.2.m2.1.1.2" xref="S5.p6.2.m2.1.1.2.cmml">m</mi><mo id="S5.p6.2.m2.1.1.1" xref="S5.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.p6.2.m2.1.1.3" xref="S5.p6.2.m2.1.1.3.cmml">I</mi><mo id="S5.p6.2.m2.1.1.1a" xref="S5.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.p6.2.m2.1.1.4" xref="S5.p6.2.m2.1.1.4.cmml">o</mi><mo id="S5.p6.2.m2.1.1.1b" xref="S5.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.p6.2.m2.1.1.5" xref="S5.p6.2.m2.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p6.2.m2.1b"><apply id="S5.p6.2.m2.1.1.cmml" xref="S5.p6.2.m2.1.1"><times id="S5.p6.2.m2.1.1.1.cmml" xref="S5.p6.2.m2.1.1.1"></times><ci id="S5.p6.2.m2.1.1.2.cmml" xref="S5.p6.2.m2.1.1.2">𝑚</ci><ci id="S5.p6.2.m2.1.1.3.cmml" xref="S5.p6.2.m2.1.1.3">𝐼</ci><ci id="S5.p6.2.m2.1.1.4.cmml" xref="S5.p6.2.m2.1.1.4">𝑜</ci><ci id="S5.p6.2.m2.1.1.5.cmml" xref="S5.p6.2.m2.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.2.m2.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S5.p6.2.m2.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math>) as an aggregate performance indicator across all classes. All experiments were conducted on NVIDIA A100-PCIe GPUs with 40GB memory using CUDA 11.5. The PyTorch Lightning framework facilitated model development and training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib41" title="">41</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.3">The results of the experiments are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>, showcasing the label-wise <math alttext="IoU" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">I</mi><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">o</mi><mo id="S6.p1.1.m1.1.1.1a" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.4" xref="S6.p1.1.m1.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">𝐼</ci><ci id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">𝑜</ci><ci id="S6.p1.1.m1.1.1.4.cmml" xref="S6.p1.1.m1.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">IoU</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">italic_I italic_o italic_U</annotation></semantics></math> in percentage, along with the mean <math alttext="IoU" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mi id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">I</mi><mo id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml">o</mi><mo id="S6.p1.2.m2.1.1.1a" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.4" xref="S6.p1.2.m2.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><times id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1"></times><ci id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">𝐼</ci><ci id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3">𝑜</ci><ci id="S6.p1.2.m2.1.1.4.cmml" xref="S6.p1.2.m2.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">IoU</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">italic_I italic_o italic_U</annotation></semantics></math> for each of the models. Based on the results, it can be inferred that the SegFormer model with the MiX-B5 encoder yields slightly better results compared to the model trained with the MiX-B3 encoder. The SegFormer model with the MiX-B0 encoder exhibits the lowest prediction performance. Furthermore, the incorporation of test-time augmentation contributes to improving the results of the models. Ensemble methods play a vital role in further enhancing the predictions of base SegFormer models. In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>, the model labeled as <em class="ltx_emph ltx_font_italic" id="S6.p1.3.1">Ensemble</em> represents a fusion of the predictions from the base SegFormer-B3 and SegFormer-B5 models, while <em class="ltx_emph ltx_font_italic" id="S6.p1.3.2">Ensemble (tta)</em> denotes a fusion of the predictions obtained through test-time augmentation of the base SegFormer-B3 and SegFormer-B5 models. The model Ensemble (tta) achieves the highest <math alttext="mIoU" class="ltx_Math" display="inline" id="S6.p1.3.m3.1"><semantics id="S6.p1.3.m3.1a"><mrow id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mi id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">m</mi><mo id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.3" xref="S6.p1.3.m3.1.1.3.cmml">I</mi><mo id="S6.p1.3.m3.1.1.1a" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.4" xref="S6.p1.3.m3.1.1.4.cmml">o</mi><mo id="S6.p1.3.m3.1.1.1b" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.5" xref="S6.p1.3.m3.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><times id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1"></times><ci id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">𝑚</ci><ci id="S6.p1.3.m3.1.1.3.cmml" xref="S6.p1.3.m3.1.1.3">𝐼</ci><ci id="S6.p1.3.m3.1.1.4.cmml" xref="S6.p1.3.m3.1.1.4">𝑜</ci><ci id="S6.p1.3.m3.1.1.5.cmml" xref="S6.p1.3.m3.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S6.p1.3.m3.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math>, surpassing the performance of the base SegFormer models. In the comparison, we have included existing methods such as U-Net with ResNet50 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib23" title="">23</a>]</cite>, DeepLabV3+ with ResNet50 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib25" title="">25</a>]</cite>, Category attention guided network (CAGNet) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib42" title="">42</a>]</cite>, UNetFormer with ResNet18 encoder, Densely Connected Swin Transformer (DC-Swin) with Swin-S encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib43" title="">43</a>]</cite>. When looking at the results, SegFormer with the MiX-B5 encoder performs competitively against other methods, it outperforms the models included in the comparison.</p>
</div>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparative performance analysis of SegFormer and established semantic segmentation approaches on the UAVid dataset. Performance is evaluated using Mean Intersection over Union (<math alttext="mIoU\%" class="ltx_Math" display="inline" id="S6.T1.3.m1.1"><semantics id="S6.T1.3.m1.1b"><mrow id="S6.T1.3.m1.1.1" xref="S6.T1.3.m1.1.1.cmml"><mi id="S6.T1.3.m1.1.1.2" xref="S6.T1.3.m1.1.1.2.cmml">m</mi><mo id="S6.T1.3.m1.1.1.1" xref="S6.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.T1.3.m1.1.1.3" xref="S6.T1.3.m1.1.1.3.cmml">I</mi><mo id="S6.T1.3.m1.1.1.1b" xref="S6.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.T1.3.m1.1.1.4" xref="S6.T1.3.m1.1.1.4.cmml">o</mi><mo id="S6.T1.3.m1.1.1.1c" xref="S6.T1.3.m1.1.1.1.cmml">⁢</mo><mrow id="S6.T1.3.m1.1.1.5" xref="S6.T1.3.m1.1.1.5.cmml"><mi id="S6.T1.3.m1.1.1.5.2" xref="S6.T1.3.m1.1.1.5.2.cmml">U</mi><mo id="S6.T1.3.m1.1.1.5.1" xref="S6.T1.3.m1.1.1.5.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.3.m1.1c"><apply id="S6.T1.3.m1.1.1.cmml" xref="S6.T1.3.m1.1.1"><times id="S6.T1.3.m1.1.1.1.cmml" xref="S6.T1.3.m1.1.1.1"></times><ci id="S6.T1.3.m1.1.1.2.cmml" xref="S6.T1.3.m1.1.1.2">𝑚</ci><ci id="S6.T1.3.m1.1.1.3.cmml" xref="S6.T1.3.m1.1.1.3">𝐼</ci><ci id="S6.T1.3.m1.1.1.4.cmml" xref="S6.T1.3.m1.1.1.4">𝑜</ci><apply id="S6.T1.3.m1.1.1.5.cmml" xref="S6.T1.3.m1.1.1.5"><csymbol cd="latexml" id="S6.T1.3.m1.1.1.5.1.cmml" xref="S6.T1.3.m1.1.1.5.1">percent</csymbol><ci id="S6.T1.3.m1.1.1.5.2.cmml" xref="S6.T1.3.m1.1.1.5.2">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.m1.1d">mIoU\%</annotation><annotation encoding="application/x-llamapun" id="S6.T1.3.m1.1e">italic_m italic_I italic_o italic_U %</annotation></semantics></math>) and labels-wise Intersection over Union (<math alttext="IoU\%" class="ltx_Math" display="inline" id="S6.T1.4.m2.1"><semantics id="S6.T1.4.m2.1b"><mrow id="S6.T1.4.m2.1.1" xref="S6.T1.4.m2.1.1.cmml"><mi id="S6.T1.4.m2.1.1.2" xref="S6.T1.4.m2.1.1.2.cmml">I</mi><mo id="S6.T1.4.m2.1.1.1" xref="S6.T1.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.T1.4.m2.1.1.3" xref="S6.T1.4.m2.1.1.3.cmml">o</mi><mo id="S6.T1.4.m2.1.1.1b" xref="S6.T1.4.m2.1.1.1.cmml">⁢</mo><mrow id="S6.T1.4.m2.1.1.4" xref="S6.T1.4.m2.1.1.4.cmml"><mi id="S6.T1.4.m2.1.1.4.2" xref="S6.T1.4.m2.1.1.4.2.cmml">U</mi><mo id="S6.T1.4.m2.1.1.4.1" xref="S6.T1.4.m2.1.1.4.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.4.m2.1c"><apply id="S6.T1.4.m2.1.1.cmml" xref="S6.T1.4.m2.1.1"><times id="S6.T1.4.m2.1.1.1.cmml" xref="S6.T1.4.m2.1.1.1"></times><ci id="S6.T1.4.m2.1.1.2.cmml" xref="S6.T1.4.m2.1.1.2">𝐼</ci><ci id="S6.T1.4.m2.1.1.3.cmml" xref="S6.T1.4.m2.1.1.3">𝑜</ci><apply id="S6.T1.4.m2.1.1.4.cmml" xref="S6.T1.4.m2.1.1.4"><csymbol cd="latexml" id="S6.T1.4.m2.1.1.4.1.cmml" xref="S6.T1.4.m2.1.1.4.1">percent</csymbol><ci id="S6.T1.4.m2.1.1.4.2.cmml" xref="S6.T1.4.m2.1.1.4.2">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.m2.1d">IoU\%</annotation><annotation encoding="application/x-llamapun" id="S6.T1.4.m2.1e">italic_I italic_o italic_U %</annotation></semantics></math>) metrics. Top-performing models for each label are indicated in bold, with the second-best performance denoted by underlining.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5" style="width:557.0pt;height:306.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T1.5.1"><span class="ltx_text" id="S6.T1.5.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T1.5.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T1.5.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T1.5.1.1.1.1.1.1">Model \Label</span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.2.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.2.1.1.1" style="width:6.9pt;height:31.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.7pt;transform:translate(-12.38pt,-12.38pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.2.1.1.1.1">Clutter</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.3.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.3.1.1.1" style="width:8.9pt;height:41pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:41.0pt;transform:translate(-16.07pt,-15.1pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.3.1.1.1.1">Buildings</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.4.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.4.1.1.1" style="width:6.9pt;height:22.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:22.9pt;transform:translate(-7.99pt,-7.99pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.4.1.1.1.1">Road</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.5.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.5.1.1.1" style="width:6.8pt;height:19.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:19.2pt;transform:translate(-6.18pt,-6.18pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.5.1.1.1.1">Tree</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.6.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.6.1.1.1" style="width:8.8pt;height:66.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:66.5pt;transform:translate(-28.88pt,-27.9pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.6.1.1.1.1">Low vegetation</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.7.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.7.1.1.1" style="width:8.8pt;height:49.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.2pt;transform:translate(-20.21pt,-19.24pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.7.1.1.1.1">Moving car</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T1.5.1.1.1.1.1.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.8.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.8.1.1.1" style="width:6.8pt;height:42.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.3pt;transform:translate(-17.71pt,-17.71pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.8.1.1.1.1">Static car</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S6.T1.5.1.1.1.1.1.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.1.1.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.9.1.1" style="width:30.4pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T1.5.1.1.1.1.1.9.1.1.1" style="width:6.8pt;height:31.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.9pt;transform:translate(-12.56pt,-12.56pt) rotate(-90deg) ;">
<span class="ltx_p" id="S6.T1.5.1.1.1.1.1.9.1.1.1.1">Human</span>
</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T1.5.1.1.1.1.1.10">mIoU</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T1.5.1.1.1.2.2.1">U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib23" title="">23</a>]</cite></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.2.1.1" style="width:30.4pt;">67.69</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.3.1.1" style="width:30.4pt;">87.28</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.4.1.1" style="width:30.4pt;">80.2</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.5.1.1" style="width:30.4pt;">79.69</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.6.1.1" style="width:30.4pt;">63.46</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.7.1.1" style="width:30.4pt;">70.72</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.2.2.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.8.1.1" style="width:30.4pt;">58.11</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T1.5.1.1.1.2.2.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.2.2.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.2.2.9.1.1" style="width:30.4pt;">30.62</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.5.1.1.1.2.2.10">67.22</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.3.3.1">DeepLabV3+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib25" title="">25</a>]</cite></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.2.1.1" style="width:30.4pt;">67.86</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.3.1.1" style="width:30.4pt;">87.87</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.4.1.1" style="width:30.4pt;">80.23</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.5.1.1" style="width:30.4pt;">79.74</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.6.1.1" style="width:30.4pt;">62.03</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.7.1.1" style="width:30.4pt;">71.51</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.3.3.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.8.1.1" style="width:30.4pt;">62.99</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.3.3.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.3.3.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.3.3.9.1.1" style="width:30.4pt;">29.5</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.3.3.10">67.72</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.4.4.1">CAGNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib42" title="">42</a>]</cite></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.2.1.1" style="width:30.4pt;">69.8</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.3.1.1" style="width:30.4pt;">88.4</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.4.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.4.4.4.1.1.1">82.7</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.5.1.1" style="width:30.4pt;">80.6</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.6.1.1" style="width:30.4pt;">64.6</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.7.1.1" style="width:30.4pt;">76.0</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.4.4.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.8.1.1" style="width:30.4pt;">57.8</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.4.4.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.4.4.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.4.4.9.1.1" style="width:30.4pt;">32.1</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.4.4.10">69.0</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.5.5.1">UNetFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib36" title="">36</a>]</cite></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.2.1.1" style="width:30.4pt;">68.4</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.3.1.1" style="width:30.4pt;">87.4</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.4.1.1" style="width:30.4pt;">81.5</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.5.1.1" style="width:30.4pt;">80.2</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.6.1.1" style="width:30.4pt;">63.5</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.7.1.1" style="width:30.4pt;">73.6</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.5.5.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.8.1.1" style="width:30.4pt;">56.4</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.5.5.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.5.5.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.5.5.9.1.1" style="width:30.4pt;">31.0</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.5.5.10">67.8</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.6.6">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.6.6.1">DC-Swin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib43" title="">43</a>]</cite></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.2.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.6.6.2.1.1.1">70.72</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.3.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.6.6.3.1.1.1">89.66</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.4.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.6.6.4.1.1.1">83.42</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.5.1.1" style="width:30.4pt;">80.75</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.6.1.1" style="width:30.4pt;">65.23</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.7.1.1" style="width:30.4pt;">74.97</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.6.6.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.8.1.1" style="width:30.4pt;">59.77</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.6.6.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.6.6.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.6.6.9.1.1" style="width:30.4pt;">32.02</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.6.6.10">69.57</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.7.7">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T1.5.1.1.1.7.7.1">SegFormer-B0</span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.2.1.1" style="width:30.4pt;">65.55</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.3.1.1" style="width:30.4pt;">85.97</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.4.1.1" style="width:30.4pt;">78.31</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.5.1.1" style="width:30.4pt;">79.3</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.6.1.1" style="width:30.4pt;">62.94</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.7.1.1" style="width:30.4pt;">70.05</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T1.5.1.1.1.7.7.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.8.1.1" style="width:30.4pt;">58.4</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T1.5.1.1.1.7.7.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.7.7.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.7.7.9.1.1" style="width:30.4pt;">28.99</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.5.1.1.1.7.7.10">66.19</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.8.8">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.8.8.1">SegFormer-B0 (tta)</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.2.1.1" style="width:30.4pt;">66.37</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.3.1.1" style="width:30.4pt;">86.57</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.4.1.1" style="width:30.4pt;">79.16</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.5.1.1" style="width:30.4pt;">79.8</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.6.1.1" style="width:30.4pt;">63.5</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.7.1.1" style="width:30.4pt;">71.25</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.8.8.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.8.1.1" style="width:30.4pt;">58.66</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.8.8.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.8.8.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.8.8.9.1.1" style="width:30.4pt;">29.94</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.8.8.10">66.91</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.9.9">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.9.9.1">SegFormer-B3</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.2.1.1" style="width:30.4pt;">68.8</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.3.1.1" style="width:30.4pt;">88.46</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.4.1.1" style="width:30.4pt;">80.19</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.5.1.1" style="width:30.4pt;">80.54</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.6.1.1" style="width:30.4pt;">65.24</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.7.1.1" style="width:30.4pt;">73.44</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.9.9.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.8.1.1" style="width:30.4pt;">64.92</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.9.9.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.9.9.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.9.9.9.1.1" style="width:30.4pt;">32.21</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.9.9.10">69.22</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.10.10">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.10.10.1">SegFormer-B3 (tta)</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.2.1.1" style="width:30.4pt;">69.46</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.3.1.1" style="width:30.4pt;">88.81</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.4.1.1" style="width:30.4pt;">80.77</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.5.1.1" style="width:30.4pt;">81.03</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.6.1.1" style="width:30.4pt;">65.88</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.7.1.1" style="width:30.4pt;">73.75</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.10.10.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.8.1.1" style="width:30.4pt;">65.88</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.10.10.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.10.10.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.10.10.9.1.1" style="width:30.4pt;">32.81</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.10.10.10">69.8</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.11.11">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.11.11.1">SegFormer-B5</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.2.1.1" style="width:30.4pt;">69.69</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.3.1.1" style="width:30.4pt;">88.08</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.4.1.1" style="width:30.4pt;">82.15</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.5.1.1" style="width:30.4pt;">80.42</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.6.1.1" style="width:30.4pt;">63.96</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.7.1.1" style="width:30.4pt;">75.12</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.11.11.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.8.1.1" style="width:30.4pt;">65.16</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.11.11.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.11.11.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.11.11.9.1.1" style="width:30.4pt;">31.83</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.11.11.10">69.55</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.12.12">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.12.12.1">SegFormer-B5 (tta)</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.2.1.1" style="width:30.4pt;">70.21</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.3.1.1" style="width:30.4pt;">88.41</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.4.1.1" style="width:30.4pt;">82.54</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.5.1.1" style="width:30.4pt;">80.81</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.6.1.1" style="width:30.4pt;">64.54</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.7.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.12.12.7.1.1.1">76.38</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.12.12.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.8.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.12.12.8.1.1.1">66.31</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.12.12.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.12.12.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.12.12.9.1.1" style="width:30.4pt;">32.61</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.12.12.10">70.23</span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.13.13">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T1.5.1.1.1.13.13.1">Ensemble</span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.2.1.1" style="width:30.4pt;">70.33</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.3.1.1" style="width:30.4pt;">88.9</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.4.1.1" style="width:30.4pt;">81.98</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.5.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.13.13.5.1.1.1">81.24</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.6.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.13.13.6.1.1.1">65.94</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.7.1.1" style="width:30.4pt;">75.55</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T1.5.1.1.1.13.13.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.8.1.1" style="width:30.4pt;">66.18</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T1.5.1.1.1.13.13.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.13.13.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.13.13.9.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.13.13.9.1.1.1">32.83</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T1.5.1.1.1.13.13.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.13.13.10.1">70.37</span></span></span>
<span class="ltx_tr" id="S6.T1.5.1.1.1.14.14">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T1.5.1.1.1.14.14.1">Ensemble (tta)</span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.2.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.2.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.2.1.1.1">70.85</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.3.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.3.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.14.14.3.1.1.1">89.19</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.4.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.4.1.1" style="width:30.4pt;">82.46</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.5.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.5.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.5.1.1.1">81.57</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.6">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.6.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.6.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.6.1.1.1">66.26</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.7">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.7.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.7.1.1" style="width:30.4pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.5.1.1.1.14.14.7.1.1.1">76.17</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T1.5.1.1.1.14.14.8">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.8.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.8.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.8.1.1.1">67.39</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="S6.T1.5.1.1.1.14.14.9">
<span class="ltx_inline-block ltx_align_top" id="S6.T1.5.1.1.1.14.14.9.1">
<span class="ltx_p" id="S6.T1.5.1.1.1.14.14.9.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.9.1.1.1">33.16</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.5.1.1.1.14.14.10"><span class="ltx_text ltx_font_bold" id="S6.T1.5.1.1.1.14.14.10.1">70.88</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.F4" title="Figure 4 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">4</span></a> offers a visual representation of the UAVid dataset, showcasing sample images, their corresponding ground truth masks, and the segmentation outputs generated by our proposed model. The quantitative evaluation metrics presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>, specifically the Intersection over Union (IoU) scores for individual labels, highlight the model’s strengths and weaknesses. Notably, the model exhibits superior performance in segmenting buildings, roads, trees, and moving vehicles. Conversely, the ’Human’ label poses a significant challenge, resulting in considerably lower IoU values. A deeper examination of the confusion matrix presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.F3" title="Figure 3 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">3</span></a> provides valuable insights into the model’s error patterns. The recurrent misclassifications between semantically similar classes such as ’Moving car’ and ’Static car’, as well as ’Tree’ and ’Low vegetation’, are unsurprising. The confusion between ’Human’ and ’Low vegetation’ is particularly noteworthy, likely attributed to the spatial proximity and overlapping nature of these classes in the image data. The ’Background clutter’ class, by its very definition as a residual category, inevitably exhibits a high error rate due to its heterogeneous composition. A closer inspection of a specific image region (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.F5" title="Figure 5 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">5</span></a>) offers further evidence of the model’s limitations. In this particular instance, the challenge of distinguishing between ’Moving cars’ and ’Static cars’ is exacerbated by the static nature of the vehicles, which reduces the discriminative cues available to the model. The accurate segmentation of the ’Human’ class is further hindered by the dense and overlapping nature of human figures within the scene. This visual comparison underscores the complexities inherent in accurately segmenting objects within crowded urban environments, particularly when dealing with overlapping instances, occlusions, and subtle visual distinctions between similar object categories.</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="591" id="S6.F3.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Confusion matrix obtained from the Ensemble (tta) model as in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S6.F4.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example images and corresponding ground truth and predicted masks from the UAVid dataset. The first row presents UAV-captured images, while the second row displays their respective ground truth segmentation masks. The third row showcases the segmentation results produced by the Ensemble (tta) model as detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="241" id="S6.F5.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Zoomed-in view of a complex urban scene highlighting pedestrians and moving vehicles from the UAVid dataset, ground truth mask, and the predicted mask obtained using the Ensemble (tta) model, as outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T1" title="Table 1 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">1</span></a>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#S6.T2" title="Table 2 ‣ 6 Results ‣ Semantic Segmentation of Unmanned Aerial Vehicle Remote Sensing Images using SegFormer"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the effectiveness of the selected models in terms of the number of parameters, latency, and frames per second (FPS) for an image size of 1024x1024 pixels. SegFormer-B0 demonstrates satisfactory performance, achieving a <math alttext="mIoU" class="ltx_Math" display="inline" id="S6.p3.1.m1.1"><semantics id="S6.p3.1.m1.1a"><mrow id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml"><mi id="S6.p3.1.m1.1.1.2" xref="S6.p3.1.m1.1.1.2.cmml">m</mi><mo id="S6.p3.1.m1.1.1.1" xref="S6.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p3.1.m1.1.1.3" xref="S6.p3.1.m1.1.1.3.cmml">I</mi><mo id="S6.p3.1.m1.1.1.1a" xref="S6.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p3.1.m1.1.1.4" xref="S6.p3.1.m1.1.1.4.cmml">o</mi><mo id="S6.p3.1.m1.1.1.1b" xref="S6.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p3.1.m1.1.1.5" xref="S6.p3.1.m1.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><apply id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1"><times id="S6.p3.1.m1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1"></times><ci id="S6.p3.1.m1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.2">𝑚</ci><ci id="S6.p3.1.m1.1.1.3.cmml" xref="S6.p3.1.m1.1.1.3">𝐼</ci><ci id="S6.p3.1.m1.1.1.4.cmml" xref="S6.p3.1.m1.1.1.4">𝑜</ci><ci id="S6.p3.1.m1.1.1.5.cmml" xref="S6.p3.1.m1.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S6.p3.1.m1.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math> of 66.19% with a latency of 7.67 milliseconds while utilizing only 3.7 million parameters. SegFormer-B0 model is well-suited for applications demanding real-time semantic segmentation of UAV images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01092v1#bib.bib17" title="">17</a>]</cite>. For example, the model can continuously analyze frames from a camera mounted on a UAV, alerting for any potential hazards or dangerous situations it detects.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Models summary for a batch size of 1 and profiling on a NVIDIA A100-PCIe GPUs with 40GB memory and CUDA 11.5.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.1" style="width:305.8pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T2.1.1"><span class="ltx_text" id="S6.T2.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T2.1.1.1.1.1.1.1">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.1.1.2">Parameters</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.1.1.3">Image size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.1.1.4">Latency (ms)</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.1.1.5">FPS</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T2.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.2.1.1">SegFormer-B0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.1.1.1.2.1.2">3.7M</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.1.1.1.2.1.3">1024x1024</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.1.1.1.2.1.4">7.67</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.1.1.1.2.1.5">132.16</span></span>
<span class="ltx_tr" id="S6.T2.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T2.1.1.1.1.3.2.1">SegFormer-B3</span>
<span class="ltx_td ltx_align_right" id="S6.T2.1.1.1.1.3.2.2">47.2M</span>
<span class="ltx_td ltx_align_right" id="S6.T2.1.1.1.1.3.2.3">1024x1024</span>
<span class="ltx_td ltx_align_right" id="S6.T2.1.1.1.1.3.2.4">21.24</span>
<span class="ltx_td ltx_align_right" id="S6.T2.1.1.1.1.3.2.5">47.07</span></span>
<span class="ltx_tr" id="S6.T2.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T2.1.1.1.1.4.3.1">SegFormer-B5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.1.1.1.4.3.2">84.6M</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.1.1.1.4.3.3">1024x1024</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.1.1.1.4.3.4">40.34</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.1.1.1.4.3.5">24.79</span></span>
</span>
</span></span></p>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we investigated the efficacy of SegFormer models in semantic segmentation tasks utilizing UAV images. Leveraging the SegFormer framework, tailored to accommodate various encoder sizes from B0 to B5, we conducted experiments on the UAVid dataset, specializing in urban scene semantic segmentation. Our investigation aimed to evaluate both the effectiveness and efficiency of SegFormer variants across different performance metrics. The results obtained shed light on the performance of SegFormer models across various encoder sizes. Notably, SegFormer models with larger encoders, such as MiX-B5, exhibited slightly superior performance compared to their counterparts with smaller encoders. However, it’s important to highlight that the incorporation of test-time augmentation notably enhanced the overall performance of the models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Furthermore, Ensemble methods emerged as a crucial strategy to further enhance the predictions of base SegFormer models. Combining predictions from multiple models, both with and without test-time augmentation, resulted in improved <math alttext="mIoU" class="ltx_Math" display="inline" id="S7.p2.1.m1.1"><semantics id="S7.p2.1.m1.1a"><mrow id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml"><mi id="S7.p2.1.m1.1.1.2" xref="S7.p2.1.m1.1.1.2.cmml">m</mi><mo id="S7.p2.1.m1.1.1.1" xref="S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p2.1.m1.1.1.3" xref="S7.p2.1.m1.1.1.3.cmml">I</mi><mo id="S7.p2.1.m1.1.1.1a" xref="S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p2.1.m1.1.1.4" xref="S7.p2.1.m1.1.1.4.cmml">o</mi><mo id="S7.p2.1.m1.1.1.1b" xref="S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p2.1.m1.1.1.5" xref="S7.p2.1.m1.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><apply id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1"><times id="S7.p2.1.m1.1.1.1.cmml" xref="S7.p2.1.m1.1.1.1"></times><ci id="S7.p2.1.m1.1.1.2.cmml" xref="S7.p2.1.m1.1.1.2">𝑚</ci><ci id="S7.p2.1.m1.1.1.3.cmml" xref="S7.p2.1.m1.1.1.3">𝐼</ci><ci id="S7.p2.1.m1.1.1.4.cmml" xref="S7.p2.1.m1.1.1.4">𝑜</ci><ci id="S7.p2.1.m1.1.1.5.cmml" xref="S7.p2.1.m1.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">mIoU</annotation><annotation encoding="application/x-llamapun" id="S7.p2.1.m1.1d">italic_m italic_I italic_o italic_U</annotation></semantics></math> values compared to the base models. This highlights the effectiveness of ensemble techniques in semantic segmentation tasks. While the SegFormer models demonstrated promising results across various labels, certain challenges were evident. The model tended to misclassify certain labels, such as "Moving car" and "Static car", which could be attributed to semantic similarities between these labels. Additionally, fine-grained segmentation for labels like "Human" proved challenging due to overlapping objects and dense segmentation in ground truth masks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Despite these challenges, SegFormer-B0, the smallest model tailored for real-time applications, showcased satisfactory performance. With a mean <math alttext="IoU" class="ltx_Math" display="inline" id="S7.p3.1.m1.1"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mi id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">I</mi><mo id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.3" xref="S7.p3.1.m1.1.1.3.cmml">o</mi><mo id="S7.p3.1.m1.1.1.1a" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.4" xref="S7.p3.1.m1.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><times id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1"></times><ci id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">𝐼</ci><ci id="S7.p3.1.m1.1.1.3.cmml" xref="S7.p3.1.m1.1.1.3">𝑜</ci><ci id="S7.p3.1.m1.1.1.4.cmml" xref="S7.p3.1.m1.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">IoU</annotation><annotation encoding="application/x-llamapun" id="S7.p3.1.m1.1d">italic_I italic_o italic_U</annotation></semantics></math> of 66.187% and a low latency of 7.67 milliseconds, while utilizing only 3.7 million parameters, SegFormer-B0 proves to be well-suited for real-time semantic segmentation of UAV images, offering promising prospects for practical applications in various domains, including environmental monitoring, disaster management, and aerial surveying. As further work, we aim to deploy this model on an edge device mounted on a UAV. This deployment would leverage the model’s real-time capabilities, enhancing the UAV’s ability to analyze and respond to its environment autonomously. This step is crucial for applications requiring immediate data processing and decision-making, ensuring timely alerts and responses to potential hazards or dangerous situations detected during UAV missions. In conclusion, this study underscores the effectiveness of SegFormer models in semantic segmentation tasks involving UAV images, providing valuable insights into their performance and potential applications in real-world scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgement</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The authors gratefully acknowledge the financial support provided by the Faculty of Computer Science and Engineering at the Ss. Cyril and Methodius University in Skopje through the SatTime project focused on satellite image time-series analysis.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Lucas Prado Osco, José Marcato Junior, Ana Paula Marques Ramos, Lúcio André de Castro Jorge, Sarah Narges Fatholahi, Jonathan de Andrade Silva, Edson Takashi Matsubara, Hemerson Pistori, Wesley Nunes Gonçalves, and Jonathan Li.

</span>
<span class="ltx_bibblock">A review on deep learning in uav remote sensing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">International Journal of Applied Earth Observation and Geoinformation</span>, 102:102456, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jian Cheng, Changjian Deng, Yanzhou Su, Zeyu An, and Qi Wang.

</span>
<span class="ltx_bibblock">Methods and datasets on semantic segmentation for unmanned aerial vehicle remote sensing images: A review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 211:1–34, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Elena Merdjanovska, Ivan Kitanovski, Žiga Kokalj, Ivica Dimitrovski, and Dragi Kocev.

</span>
<span class="ltx_bibblock">Crop type prediction across countries and years: Slovenia, denmark and the netherlands.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium</span>, pages 5945–5948. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ronald Kemker, Carl Salvaggio, and Christopher Kanan.

</span>
<span class="ltx_bibblock">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, and Nikola Simidjievski.

</span>
<span class="ltx_bibblock">Current trends in deep learning for earth observation: An open-source benchmark arena for image classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 197:18–35, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ivica Dimitrovski, Ivan Kitanovski, Nikola Simidjievski, and Dragi Kocev.

</span>
<span class="ltx_bibblock">In-domain self-supervised learning improves remote sensing image scene classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Geoscience and Remote Sensing Letters</span>, 21:1–5, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Xiaohui Yuan, Jianfang Shi, and Lichuan Gu.

</span>
<span class="ltx_bibblock">A review of deep learning methods for semantic segmentation of remote sensing imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Expert Systems with Applications</span>, 169:114417, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Vlatko Spasev, Ivica Dimitrovski, Ivan Kitanovski, and Ivan Chorbev.

</span>
<span class="ltx_bibblock">Semantic segmentation of remote sensing images: Definition, methods, datasets and applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">ICT Innovations 2023. Learning: Humans, Theory, Machines, and Data</span>, pages 127–140, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ivica Dimitrovski, Ivan Kitanovski, Panče Panov, Ana Kostovska, Nikola Simidjievski, and Dragi Kocev.

</span>
<span class="ltx_bibblock">Aitlas: Artificial intelligence toolbox for earth observation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Remote Sensing</span>, 15(9), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
David R Green, Jason J Hagon, Cristina Gómez, and Billy J Gregory.

</span>
<span class="ltx_bibblock">Using low-cost uavs for environmental monitoring, mapping, and modelling: Examples from the coastal zone.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Coastal management</span>, pages 465–501. Academic Press, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yong Zhang, Xiuxiao Yuan, Wenzhuo Li, and Shiyu Chen.

</span>
<span class="ltx_bibblock">Automatic power line inspection using uav images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Remote Sensing</span>, 9(8):824, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Haidong Zhang, Lingqing Wang, Ting Tian, and Jianghai Yin.

</span>
<span class="ltx_bibblock">A review of unmanned aerial vehicle low-altitude remote sensing (uav-lars) use in agricultural monitoring in china.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Remote Sensing</span>, 13(6):1221, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jianhua Yang, Zhaowei Ding, and Lei Wang.

</span>
<span class="ltx_bibblock">The programming model of air-ground cooperative patrol between multi-uav and police car.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Access</span>, 9:134503–134517, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Andreas Kamilaris and Francesc X Prenafeta-Boldú.

</span>
<span class="ltx_bibblock">Disaster monitoring using unmanned aerial vehicles and deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1807.11805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Franziska Funk and Peter Stütz.

</span>
<span class="ltx_bibblock">A passive cloud detection system for uav: Weather situation mapping with imaging sensors.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">2017 IEEE Aerospace Conference</span>, pages 1–12, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Massimiliano De Benedetti, Fabio D’Urso, Giancarlo Fortino, Fabrizio Messina, Giuseppe Pappalardo, and Corrado Santoro.

</span>
<span class="ltx_bibblock">A fault-tolerant self-organizing flocking approach for uav aerial survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Journal of Network and Computer Applications</span>, 96:14–30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Farshad Safavi and Maryam Rahnemoonfar.

</span>
<span class="ltx_bibblock">Comparative study of real-time semantic segmentation networks in aerial images during flooding events.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</span>, 16:4–20, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.

</span>
<span class="ltx_bibblock">Segformer: Simple and efficient design for semantic segmentation with transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Advances in Neural Information Processing Systems</span>, 34:12077–12090, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ye Lyu, George Vosselman, Gui-Song Xia, Alper Yilmaz, and Michael Ying Yang.

</span>
<span class="ltx_bibblock">Uavid: A semantic segmentation dataset for uav imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 165:108–119, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Daniel Weinland, Remi Ronfard, and Edmond Boyer.

</span>
<span class="ltx_bibblock">A survey of vision-based methods for action representation, segmentation and recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Computer vision and image understanding</span>, 115(2):224–241, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew.

</span>
<span class="ltx_bibblock">A review of semantic segmentation using deep neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">International journal of multimedia information retrieval</span>, 7:87–93, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 3431–3440, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</span>, pages 234–241, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Segnet: A deep convolutional encoder-decoder architecture for image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 39(12):2481–2495, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.

</span>
<span class="ltx_bibblock">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 40(4):834–848, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Pyramid scene parsing network.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 2881–2890, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.

</span>
<span class="ltx_bibblock">Unified perceptual parsing for scene understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 418–434, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Encoder-decoder with atrous separable convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 801–818, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Psanet: Point-wise spatial attention network for scene parsing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the European conference on computer vision (ECCV)</span>, pages 267–283, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Hanqing Lu.

</span>
<span class="ltx_bibblock">Scene segmentation with dual relation-aware attention network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">IEEE Transactions on Neural Networks and Learning Systems</span>, 32(6):2547–2560, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Segmenter: Transformer for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 7262–7272, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Bowen Cheng, Alex Schwing, and Alexander Kirillov.

</span>
<span class="ltx_bibblock">Per-pixel classification is not all you need for semantic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Advances in neural information processing systems</span>, 34:17864–17875, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">European conference on computer vision</span>, pages 213–229, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G Schwing.

</span>
<span class="ltx_bibblock">Mask2former for video instance segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2112.10764</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kashu Yamazaki, Taisei Hanyu, Minh Tran, Adrian Garcia, Anh Tran, Roy McCann, Haitao Liao, Chase Rainwater, Meredith Adkins, Andrew Molthan, et al.

</span>
<span class="ltx_bibblock">Aerialformer: Multi-resolution transformer for aerial image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2306.06842</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Libo Wang, Rui Li, Ce Zhang, Shenghui Fang, Chenxi Duan, Xiaoliang Meng, and Peter M Atkinson.

</span>
<span class="ltx_bibblock">Unetformer: A unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">ISPRS Journal of Photogrammetry and Remote Sensing</span>, 190:196–214, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Wei He, Jiepan Li, Weinan Cao, Liangpei Zhang, and Hongyan Zhang.

</span>
<span class="ltx_bibblock">Building extraction from remote sensing images via an uncertainty-aware network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2307.12309</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin.

</span>
<span class="ltx_bibblock">Albumentations: Fast and flexible image augmentations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Information</span>, 11(2), 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1711.05101</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun.

</span>
<span class="ltx_bibblock">Overfeat: Integrated recognition, localization and detection using convolutional networks, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
William Falcon and The PyTorch Lightning team.

</span>
<span class="ltx_bibblock">PyTorch Lightning, March 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Shunli Wang, Qingwu Hu, Shaohua Wang, Pengcheng Zhao, Jiayuan Li, and Mingyao Ai.

</span>
<span class="ltx_bibblock">Category attention guided network for semantic segmentation of fine-resolution remote sensing images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">International Journal of Applied Earth Observation and Geoinformation</span>, 127:103661, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Libo Wang, Rui Li, Chenxi Duan, Ce Zhang, Xiaoliang Meng, and Shenghui Fang.

</span>
<span class="ltx_bibblock">A novel transformer based semantic segmentation scheme for fine-resolution remote sensing images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">IEEE Geoscience and Remote Sensing Letters</span>, 19:1–5, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 21:39:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
