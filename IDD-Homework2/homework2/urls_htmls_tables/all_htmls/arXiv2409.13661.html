<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models</title>
<!--Generated on Fri Sep 20 17:07:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13661v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S1" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S2" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S2.SS1" title="In II Background ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">System Level Testing of ADS</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S2.SS2" title="In II Background ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Vision Generative AI</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S3" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Solution</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S3.SS1" title="In III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Domain Augmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S3.SS2" title="In III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Semantic Validation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S3.SS3" title="In III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Knowledge Distillation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS1" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS2" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS3" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">RQ<sub class="ltx_sub">1</sub>: Semantic Validity and Realism</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS3.SSS1" title="In IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span>Semantic Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS3.SSS2" title="In IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span>GenAI Realism</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS4" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">RQ<sub class="ltx_sub">2</sub>: Effectiveness</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS5" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">RQ<sub class="ltx_sub">3</sub>: Efficiency</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS6" title="In IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_italic">Threats to Validity</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS6.SSS1" title="In IV-F Threats to Validity ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>1 </span>Internal validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S4.SS6.SSS2" title="In IV-F Threats to Validity ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>2 </span>External validity</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S5" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S5.SS1" title="In V Related Work ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Test Generation for Autonomous Driving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S5.SS2" title="In V Related Work ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Offline Testing with Generative AI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S5.SS3" title="In V Related Work ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Data-driven Simulation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S6" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusions and Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#S7" title="In Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Data Availability</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luciano Baresi


</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Politecnico di Milano</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Milano, Italy</span>
<br class="ltx_break"/>                    luciano.baresi@polimi.it                    
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Davide Yi Xian Hu


 <span class="ltx_text" id="id3.1.id1"></span><span class="ltx_text" id="id4.2.id2"></span> <span class="ltx_ERROR undefined" id="id5.3.id3">{@IEEEauthorhalign}</span>
Andrea Stocco
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.4.id1">Politecnico di Milano</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.5.id2">Milano, Italy</span>
<br class="ltx_break"/>                    davideyi.hu@polimi.it                    
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.6.id1">Technical University of Munich, Fortiss GmbH</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id9.7.id2">                    Munich, Germany                    </span>
<br class="ltx_break"/>andrea.stocco@tum.de, stocco@fortiss.org
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paolo Tonella
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id10.1.id1">Università della Svizzera italiana</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id11.2.id2">                    Lugano, Switzerland                    </span>
<br class="ltx_break"/>paolo.tonella@usi.ch
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Simulation-based testing is widely used to assess the reliability of Autonomous Driving Systems (ADS), but its effectiveness is limited by the operational design domain (ODD) conditions available in such simulators. To address this limitation, in this work, we explore the integration of generative artificial intelligence techniques with physics-based simulators to enhance ADS system-level testing.
Our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models, namely instruction-editing, inpainting, and inpainting with refinement.
Specifically, we assess these techniques’ capabilities to produce augmented simulator-generated images of driving scenarios representing new ODDs. We employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images.
We then perform system-level testing to evaluate the ADS’s generalization ability to newly synthesized ODDs.
Our findings show that diffusion models help increase the ODD coverage for system-level testing of ADS. Our automated semantic validator achieved a percentage of false positives as low as 3%, retaining the correctness and quality of the generated images for testing. Our approach successfully identified new ADS system failures before real-world testing.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Before deploying Autonomous Driving Systems (ADS) on public roads, extensive simulation-based testing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib1" title="">1</a>]</cite> is performed to ensure these systems can effectively handle the scenarios of the Operational Design Domain (ODD), which defines the specific conditions under which an ADS is designed to operate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib3" title="">3</a>]</cite>.
Current driving simulators are based on game engines (e.g., Unity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib4" title="">4</a>]</cite> or Unreal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib5" title="">5</a>]</cite>) and enable testing using a predefined set of ODD conditions, the default being typically sunny weather.
Simulators are often limited in the range of ODD they represent, as they primarily focus on photorealistic rendering and accurate physics representation. Thus, they fail to cover many ODD scenarios that are instead critical for testing ADS.
This limitation hinders the effectiveness of system-level testing of ADS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib7" title="">7</a>]</cite>, particularly for edge cases <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">beyond</span> the predefined ODD conditions available in these simulators.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Enhancing the ODD coverage in a simulator typically requires developing new conditions within the simulator engine, a task that demands significant domain knowledge and development effort. Moreover, even if it were feasible to improve the simulation platform, recent research highlighted the problem of the fidelity gap between the virtual environments represented in the simulators and the real world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Generative Artificial Intelligence (GenAI) solutions have been employed to improve the extend the range of ODD conditions for ADS testing, by improving both the variety and realism of simulated weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib13" title="">13</a>]</cite>. However, approaches such as DeepRoad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib12" title="">12</a>]</cite> and TACTICS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib11" title="">11</a>]</cite> use GenAI techniques that require mining a training corpus of ODD data and focus on offline model-level testing of individual Deep Neural Networks (DNNs) against different neural-generated images. Furthermore, these approaches have not been evaluated for system-level testing, which is crucial for assessing the safety requirements of autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib14" title="">14</a>]</cite>.
On the other hand, closed-loop data-driven approaches, such as DriveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib15" title="">15</a>]</cite>, use GenAI to produce a continuous stream of driving images. However, their primary limitations lie in their dependence on learned physics models, which may be inaccurate or unrealistic. Consequently, these approaches are mainly useful for training data augmentation, rather than for testing. The lack of robust physics engines leads to inconsistent physical behaviors and interactions, making it impossible to simulate system-level failures such as collisions or vehicles driving off-road as such scenarios are not available in the training data.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we evaluate three different augmentation strategies based on state-of-the-art pre-trained diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib18" title="">18</a>]</cite>: Instruction-editing, Inpainting, and Inpainting with Refinement, used to expand the set of ODD conditions in a driving simulator via real-time image-to-image translation. Unlike existing GenAI techniques that require explicit training, diffusion models only necessitate of an input image and conditioning inputs that represent the transformation to be applied (e.g., a textual description).
As the output of diffusion models can be affected by artifacts, distortions, or inconsistencies that can undermine the effectiveness of ADS testing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib19" title="">19</a>]</cite>, our methodology includes an automated validation technique based on image semantics to assess the correctness and reliability of the generated images.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While diffusion models can generate diverse images for testing, their direct use in a simulator faces two main challenges. First, diffusion models exhibit high inference times, in the order of seconds per image, which makes it impractical for real-time applications. Second, simulation platforms typically generate multiple image frames per second, but diffusion models lack the rendering consistency required for a coherent simulation, resulting in consecutive frames that may be drastically different.
To address these limitations, our approach leverages knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib20" title="">20</a>]</cite> by integrating diffusion models with a cycle-consistent network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib21" title="">21</a>]</cite> to ensure domain generation consistency and high throughput.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.6">In our experiments, our approach generated images exhibiting a validity rate between <math alttext="52\%" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mn id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">52</mn><mo id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><csymbol cd="latexml" id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1">percent</csymbol><cn id="S1.p6.1.m1.1.1.2.cmml" type="integer" xref="S1.p6.1.m1.1.1.2">52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">52\%</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">52 %</annotation></semantics></math> and <math alttext="99\%" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mrow id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml"><mn id="S1.p6.2.m2.1.1.2" xref="S1.p6.2.m2.1.1.2.cmml">99</mn><mo id="S1.p6.2.m2.1.1.1" xref="S1.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><apply id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1"><csymbol cd="latexml" id="S1.p6.2.m2.1.1.1.cmml" xref="S1.p6.2.m2.1.1.1">percent</csymbol><cn id="S1.p6.2.m2.1.1.2.cmml" type="integer" xref="S1.p6.2.m2.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">99 %</annotation></semantics></math>, with the best approach being Inpainting. When used as a rendering engine within the simulator to produce <math alttext="108" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mn id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">108</mn><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><cn id="S1.p6.3.m3.1.1.cmml" type="integer" xref="S1.p6.3.m3.1.1">108</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">108</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">108</annotation></semantics></math> simulations using various ODD conditions, our approach was able to reveal a total of <math alttext="600" class="ltx_Math" display="inline" id="S1.p6.4.m4.1"><semantics id="S1.p6.4.m4.1a"><mn id="S1.p6.4.m4.1.1" xref="S1.p6.4.m4.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S1.p6.4.m4.1b"><cn id="S1.p6.4.m4.1.1.cmml" type="integer" xref="S1.p6.4.m4.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.4.m4.1c">600</annotation><annotation encoding="application/x-llamapun" id="S1.p6.4.m4.1d">600</annotation></semantics></math> failures across four lane-keeping ADS, <math alttext="20" class="ltx_Math" display="inline" id="S1.p6.5.m5.1"><semantics id="S1.p6.5.m5.1a"><mn id="S1.p6.5.m5.1.1" xref="S1.p6.5.m5.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S1.p6.5.m5.1b"><cn id="S1.p6.5.m5.1.1.cmml" type="integer" xref="S1.p6.5.m5.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.5.m5.1c">20</annotation><annotation encoding="application/x-llamapun" id="S1.p6.5.m5.1d">20</annotation></semantics></math> times more than using the ODD conditions available in the simulator, at the cost of a simulation time increase of <math alttext="2\%" class="ltx_Math" display="inline" id="S1.p6.6.m6.1"><semantics id="S1.p6.6.m6.1a"><mrow id="S1.p6.6.m6.1.1" xref="S1.p6.6.m6.1.1.cmml"><mn id="S1.p6.6.m6.1.1.2" xref="S1.p6.6.m6.1.1.2.cmml">2</mn><mo id="S1.p6.6.m6.1.1.1" xref="S1.p6.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.6.m6.1b"><apply id="S1.p6.6.m6.1.1.cmml" xref="S1.p6.6.m6.1.1"><csymbol cd="latexml" id="S1.p6.6.m6.1.1.1.cmml" xref="S1.p6.6.m6.1.1.1">percent</csymbol><cn id="S1.p6.6.m6.1.1.2.cmml" type="integer" xref="S1.p6.6.m6.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.6.m6.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="S1.p6.6.m6.1d">2 %</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our paper makes the following contributions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Approach.</span> A system-level testing technique for ADS that combines different GenAI (e.g., diffusion models and cycle-consistent generative networks) as a rendering engine and physics-based simulators for effective failure detection.
This novel combination of techniques achieves high ODD diversity, realism, semantic preservation, temporal consistency, and high throughput, while the simulator’s underlying physics ensures accurate representation.
Our approach is integrated into the Udacity simulator for self-driving cars and will be publicly available. To the best of our knowledge, this is the first solution that uses GenAI techniques within a driving simulator to improve the ODD coverage of DNN-based ADS.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text ltx_font_bold" id="S1.p9.1.1">Evaluation.</span> An empirical study concerning the validity and realism of neural-generated driving images, and their usage for system-level testing of ADS.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<p class="ltx_p" id="S1.p10.2"><span class="ltx_text ltx_font_bold" id="S1.p10.2.1">Dataset.</span> A dataset of more than <math alttext="1" class="ltx_Math" display="inline" id="S1.p10.1.m1.1"><semantics id="S1.p10.1.m1.1a"><mn id="S1.p10.1.m1.1.1" xref="S1.p10.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p10.1.m1.1b"><cn id="S1.p10.1.m1.1.1.cmml" type="integer" xref="S1.p10.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p10.1.m1.1d">1</annotation></semantics></math> million pair of images and <math alttext="52" class="ltx_Math" display="inline" id="S1.p10.2.m2.1"><semantics id="S1.p10.2.m2.1a"><mn id="S1.p10.2.m2.1.1" xref="S1.p10.2.m2.1.1.cmml">52</mn><annotation-xml encoding="MathML-Content" id="S1.p10.2.m2.1b"><cn id="S1.p10.2.m2.1.1.cmml" type="integer" xref="S1.p10.2.m2.1.1">52</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p10.2.m2.1c">52</annotation><annotation encoding="application/x-llamapun" id="S1.p10.2.m2.1d">52</annotation></semantics></math> OOD conditions, based on the Udacity simulator for self-driving cars. This dataset can be used to evaluate the generalizability of ADS to novel environmental conditions as well as the performance of failure prediction systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">System Level Testing of ADS</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">ADS must adhere to specific regulations that establish safety requirements essential for public acceptance and large-scale deployment.
Particularly, standards such as the ISO/PAS 21448 Safety of the Intended Function (SOTIF) standard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib22" title="">22</a>]</cite> or the UN Regulation No 157 (2021/389) concerning the approval of vehicles with regards to Automated Lane Keeping Systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib23" title="">23</a>]</cite>, demand extensive coverage of the Operational Design Domain (ODD) conditions.
The ODD should describe the conditions under which the automated vehicle is intended to drive autonomously, such as roadway types; geographic area; speed range; environmental conditions (weather as well as day/night time); and other domain constraints.
In this work, we focus on <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">ODD</span> conditions that <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">visually</span> impact the environment and the DNNs of the ADS. Specifically, we used the conditions described in the standard ISO 34505 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib2" title="">2</a>]</cite> “Scenery Elements (Section 9)” and “Environmental Conditions (Section 10)”, some examples being different geographic areas (e.g., European cities or coastal areas) and weather conditions (e.g., cloudy or rainy weather as well as day/night).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The safe deployment of ADS necessitates a thorough exploration of the ODDs through simulated and in-field testing. Due to the significant time, space, and cost constraints associated with in-field testing (i.e., real-world testing with physical vehicles), simulation-based testing has become the standard option for system-level testing of ADS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib8" title="">8</a>]</cite>.
Driving simulators can generate data and conditions that closely mimic those encountered in real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib24" title="">24</a>]</cite>.
To test the limits of the ADS, a simulator produces a vast amount of highly consistent data through synthetic input generation using a 3D image rendering engine.
However, a comprehensive test dataset must not only be statistically significant in volume but also adequately represent the diverse ODD conditions.
This is a major limitation of current driving simulators, which often have restricted ODD coverage, which is essential for comprehensive testing and fault exposure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Vision Generative AI</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Generative AI models have significantly advanced various vision tasks by enabling the creation of realistic and diverse data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In this paper, we consider techniques that allow the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">control of the content of the augmentation</span>.
Particularly, we experiment with Diffusion Models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib26" title="">26</a>]</cite>, a class of Vision Generative AI techniques that achieved state-of-the-art performance in image generation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib27" title="">27</a>]</cite>.
These models operate by reversing a gradual noising process, starting from a simple distribution and iteratively refining it to generate high-quality images.
For example, given an initial noisy image, the model denoises it step-by-step to produce a coherent image. Different randomly sampled noise seeds lead to variations in the generated images, allowing these models to create diverse and unique images.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Conditional Diffusion Models allow further control over image generation using different types of input conditioning.
For instance, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib18" title="">18</a>]</cite> and DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib28" title="">28</a>]</cite> use
<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">single conditioning</span> through a textual description to guide the denoising process, as a form of requirement specification (e.g., “<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">generate a sunny driving image scenario.</span>”).
This guidance concept aims to make the generated images closely align with the provided conditions or descriptions.
Other techniques use <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.3">multiple conditioning</span>. For example, InstructPix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib16" title="">16</a>]</cite> takes as input an image and a textual editing instruction that describes the modification to be applied to the image, whereas ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib29" title="">29</a>]</cite> facilitates the addition of arbitrary conditioning inputs to a pre-trained Stable Diffusion model.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Solution</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our methodology seamlessly integrates into the standard system-level testing loop without requiring major modifications to the simulator or the ADS.
The key idea lies in manipulating the environment perceived by the ADS through diffusion-based augmentation, while the actual driving commands are executed on the original simulator.
Our methodology consists of three main phases, namely Domain Augmentation, Semantic Validation, and Knowledge Distillation.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The first phase (Domain Augmentation) involves intercepting the images captured by the car cameras.
These images are then processed using diffusion models to generate a new image depicting the same road structure but a different ODD condition (such as background and weather conditions).
The main reason is that a well-trained lane-keeping ADS should focus on foreground features that characterize the road scenario, instead of features in the background <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The second phase (Semantic Validation) involves a validation step of the generated image to assess the validity and semantic and label preservation between the original and the augmented image.
Particularly, our approach aims to generate road images devoid of visual artifacts, distortions, inconsistencies, or hallucinations and that are <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">semantically equivalent</span> to the original one in terms of geometrical features (e.g., direction, lanes, length, width). The semantic validation process is fundamental to maintaining the validity of the augmentation process, as significant changes to the road structure introduced during augmentation could cause the ADS to make decisions based on misleading information. Our approach addresses visual and semantic consistency, whereas physics-related factors such as changes in friction and traction (e.g., due to snowy conditions) are not considered.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The third and last phase (Knowledge Distillation) enables the rendering of new ODDs online, during the execution of a simulation.
The diffusion models used in the first phase for domain augmentation are not suitable for online usage, because they are too slow at inference time. Hence, we train a faster cycle-consistent generative neural network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib21" title="">21</a>]</cite>, using the output images produced by the first phase as the training set. At each simulation step, this network transforms the input image to reproduce the domain augmentation of the diffusion models.
If the augmented image passes the semantic validation, it is forwarded to the ADS for processing.
The ADS processes the image and predicts the appropriate driving commands based on the augmented ODD. The predicted driving commands are sent to the simulator, which actuates them, completing the feedback loop. The simulator then modifies the virtual environment and provides the vehicle with updated sensor data, thus preparing for the next iteration of the testing loop.
In the next sections, we describe each step of each phase in more detail.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Domain Augmentation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We analyze three alternative controllable augmentation strategies based on categories of diffusion models to introduce environmental ODD changes in driving images: <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">Instruction-editing</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">Inpainting</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Inpainting with Refinement</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Instruction-editing.</span>
This category takes two inputs: the image to be modified and an editing instruction (e.g., “add trees” or “change season to autumn”), and produces an output image with the editing instruction applied.
Instances of Instruction-editing models are InstructPix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib16" title="">16</a>]</cite> and SDEdit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib17" title="">17</a>]</cite>.
Instruction-editing models can be configured with two parameters, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">image</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">text guidance scale</span>, that represent how much the two inputs influence the output generation.
The first parameter <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">image</span> dictates how much of the structure and spatial details of the input image should be preserved. The <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.5">text guidance scale</span> determines the strength to use when applying the editing instruction.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F1" title="Figure 1 ‣ III-A Domain Augmentation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> reports a schema of the strategy (top) and how the two guidance scales can influence the augmentation process (bottom).
Specifically, we can observe that an excessively high text guidance scale can compromise the road semantics, while overly increasing the image guidance scale too much may result in the insufficient application of the desired edit.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Instruction-edited Domain Augmentation Strategy.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Inpainting.</span>
This category employs a text-to-image diffusion model that performs inpainting. Instances of Inpainting models are Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib18" title="">18</a>]</cite>, DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib28" title="">28</a>]</cite>, and Pixart-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_α</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">We customized the inpainting pipeline to preserve the parts of the images related to the driving actions (i.e., the road), while the rest of the image can be “repainted” by the diffusion model.
We identify the road automatically by using a semantic mask that describes which pixels of the image belong to the semantic class <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">road</span>.
In this study, the semantic mask is provided directly by the simulator, ensuring perfect semantic segmentation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">The inpainting text-to-image model takes three inputs: an input image, a mask, and a textual prompt that describes the desired image.
The model generates an image by preserving only the content selected by the mask while guiding the entire image to align as closely as possible with the textual prompt. In our setting, this process ensures that only the parts outside the road are replaced with new content, thus maintaining the shape of the road since it is not modified by the inpainting strategy.
Note that the road in the inpainted image is the same as the one in the input image. For instance, in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F2" title="Figure 2 ‣ III-A Domain Augmentation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> (top left), the surrounding environment has been transformed, while the road structure, markings, and position are unchanged.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="S3.F2.g1" src="extracted/5865909/images/refining.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Inpainting and Inpainting with Refinement Domain Augmentation Strategies.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Inpainting with Refinement.</span>
This model category adds a step to the <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">Inpainting</span> strategy by performing a refinement of the entire image.
The goal is to improve the visual coherence between the preserved and the generated parts of the inpainted image.
While the traditional denoising process of diffusion models starts from fully noised images, the refinement step starts from a partially noised (inpainted) image.
This makes the refined image more similar to the initial, inpainted one. The noise level removed during denoising determines the difference between the inpainted and final image; higher noise removal leads to greater differences.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">The refinement step employs a different type of diffusion model compared to the inpainting step. This is because image generation with a text-to-image model (used during inpainting) can be guided using only text. This is sufficient for the inpainting step as the road semantics are ensured by the semantic mask that preserves the road. However, during the refinement step, the whole image is modified, and thus the road shape might change.
For this reason, in our study, we evaluated a category of diffusion model with conditional controls that allow to better guide the augmentation process, such as ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib29" title="">29</a>]</cite> with Canny edge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib32" title="">32</a>]</cite> conditioning, or T2I Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib33" title="">33</a>]</cite>.
This model takes three inputs: the edge map derived from the original input image (captured in the simulator), a partially noisy inpainted image, and a textual prompt that describes the desired image. The model takes the noisy image and refines it using both the edge map and the textual prompt. The edge map ensures that the refined image retains edge structures similar to the original, while the textual prompt directs the overall content.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F2" title="Figure 2 ‣ III-A Domain Augmentation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> provides an overview of the Inpainting with Refinement process (top) and it reports some augmentations obtained with different levels of denoising and guidance scales for the same input image (bottom). A stronger refinement process (more denoising) results in significant differences from the initial image, but can also lead to inferior preservation of road semantics.
Similarly, higher edge guidance scale values preserve road semantics more, while lower values encourage greater freedom, diversity, and realism in the generated image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Semantic Validation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Diffusion model categories aim to produce visually appealing outputs, but they may still generate invalid outputs, failing to preserve road semantics during augmentation, for instance, by widening the road or introducing new intersections. To mitigate this, our methodology includes a semantic validation step to minimize incorrect augmentations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The main objective of this phase is to check that the generated augmentation is characterized by a road that is semantically equivalent to the road in the image captured in the simulator.
To this aim, we filter out augmentations that do not preserve the original road semantics using the metric OC-TSS (One Class - Targeted Semantic Segmentation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">OC-TSS is a similarity metric that measures semantic details and structural differences between two images by focusing on a single, task-relevant class within the semantic masks predicted by a fine-tuned segmentation model.
This metric ranges from 0 to 1, where 1 indicates perfect semantic equivalence between the original and augmented images, and 0 suggests complete dissimilarity.
OC-TSS has been applied in previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib19" title="">19</a>]</cite> to assess the accuracy of Generative AI models in translating images across domains for a lane-keeping ADS. In line with this study, our analysis focuses on the semantic class “road”, as road lanes represent the relevant image characteristics for a lane-keeping ADS. Differently from the original work, we employ a U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib34" title="">34</a>]</cite> for semantic segmentation, rather than the SegFormer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib35" title="">35</a>]</cite>. Our choice of U-Net is motivated by its computational efficiency with respect to SegFormer.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Semantic Validation using OC-TSS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib19" title="">19</a>]</cite>.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F3" title="Figure 3 ‣ III-B Semantic Validation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> illustrates how our semantic validation process distinguishes between semantically valid and invalid augmentations.
The top row of <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F3" title="Figure 3 ‣ III-B Semantic Validation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> (a) shows a semantically valid augmentation of the input image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F3" title="Figure 3 ‣ III-B Semantic Validation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> (b), while the bottom row reports an incorrect augmentation (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S3.F3" title="Figure 3 ‣ III-B Semantic Validation ‣ III Solution ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> (c) of the same input image. Specifically, the augmentation in the bottom figure is considered semantically invalid because the road’s orientation changes to the left instead of continuing straight as in the original image.
The middle column presents the semantic segmentation masks computed by the U-Net model for each image. In these masks, the road is represented in white, and the background is represented in black.
The final step of our process, represented in the right column, involves measuring the distance between the semantic masks of the augmented and original images, with the differences highlighted in pink. Augmentations with a similarity score below a threshold are discarded.
Higher threshold values ensure validity, but they may discard valid images and increase the time required to generate a semantically valid augmentation. On the other hand, lower thresholds could compromise testing by allowing too many invalid images, potentially undermining the test results. In this study, the threshold is determined based on empirical observations to balance filtering out invalid augmentations and retaining enough variability for thorough ADS testing (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.SS1" title="IV-A Experimental Setup ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">IV-A</span></span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Knowledge Distillation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The final phase involves creating a fast and consistent neural rendering engine in the simulator, using outputs from the previous phases. The diffusion-based models, while effective for diversity, are computationally expensive and can produce potentially inconsistent augmentations, which may be problematic for the temporal coherence of a simulation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Therefore, we adopt a technique known as knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib20" title="">20</a>]</cite>, where a smaller model (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.1">student</span>) is trained to replicate the behavior of a larger, more complex model (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">teacher</span>, i.e., the diffusion model).
Particularly, for the student model, we use a cycle-consistent generative network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib21" title="">21</a>]</cite> that can map images from the original domain (virtual images from the simulator) to another domain (augmented images by the diffusion models). Instances of these architectures are CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib21" title="">21</a>]</cite> or UNIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib36" title="">36</a>]</cite>. This technique is widely used for image-to-image translation tasks, including the autonomous driving domain, for its low computational overhead, making it suitable for runtime usage in simulators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib12" title="">12</a>]</cite>. Moreover, training a separate student model for each domain allows effective learning of the key aspects of the teacher model’s output, enhancing rendering consistency.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">This strategy involves first training the cycle-consistent network to learn the mapping between the original and augmented image domains produced by the diffusion models. This process, while computationally intensive, is performed only once, for each domain. Then, during the online system-level testing of the ADS, the trained cycle-consistent network model generator translates images at runtime, i.e., during the execution of the simulation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">An important advantage of this approach is that it does not require collecting new data to train the model, as it leverages existing pre-trained diffusion models. This means the process can be easily automated and does not require human intervention (e.g., collecting and labeling data), making it an efficient solution for rapidly generating consistent and high-quality domain augmentations online, during ADS simulation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We carried out the evaluation of the proposed approach to answer the following research questions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">RQ<sub class="ltx_sub" id="S4.p2.1.1.1">1</sub> (semantic validity and realism):</span>
<span class="ltx_text ltx_font_italic" id="S4.p2.1.2">Do diffusion models generate augmented images that are semantically valid and realistic ODDs? How effective is the semantic validator at detecting invalid augmentations?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">RQ<sub class="ltx_sub" id="S4.p3.1.1.1">2</sub> (effectiveness):</span>
<span class="ltx_text ltx_font_italic" id="S4.p3.1.2">How effective are augmented images in exposing faulty system-level misbehaviors of ADS?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">RQ<sub class="ltx_sub" id="S4.p4.1.1.1">3</sub> (efficiency):</span>
<span class="ltx_text ltx_font_italic" id="S4.p4.1.2">What is the overhead introduced by diffusion model techniques in simulation-based testing? Does the knowledge-distilled model speed up computation?</span></p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">The first research question aims to assess the semantic validity of the augmentations generated by our methodology. Specifically, the focus is on whether diffusion models can transform images while preserving road semantics, and if the proposed semantic validator can effectively identify roads with different semantics.
The second research question aims to check the utility of the proposed approach in identifying potential faults in ADS that may not be detected using the standard simulator alone.
The third research question evaluates the computational cost of our approach, which is crucial for understanding scalability in real-world ADS testing scenarios.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Experimental Setup</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Simulation Platform.</span>
We used the Udacity simulator with behavioral cloning ADS models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib37" title="">37</a>]</cite>, a widely adopted platform in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib39" title="">39</a>]</cite>.
The simulator supports various closed-loop tracks (divided into <math alttext="40" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">40</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">40</annotation></semantics></math> sectors) for testing behavioral cloning ADS, including a predefined set of ODDs, such as different times of day/night and three weather conditions (rainy, snowy, and foggy) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib40" title="">40</a>]</cite>.
We extended the simulator with the generation of semantic segmentation masks for vehicle camera images to accurately identify the regions of the images for inpainting.
Also, we developed a synchronous simulation mechanism that pauses the simulation during image augmentation and resumes it once the new image is generated. This ensures that the augmentation process is transparent to the system-level testing process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Lane-keeping ADS.</span>
We evaluated four different lane-keeping DNN-based ADS as systems under test to assess the performance of the proposed methodology.
In particular, we selected Nvidia <span class="ltx_text" id="S4.SS1.p2.1.2">DAVE-2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib41" title="">41</a>]</cite>, Chauffeur <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib42" title="">42</a>]</cite> and Epoch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib43" title="">43</a>]</cite> since they have been often used in multiple testing works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib46" title="">46</a>]</cite>. Finally, we also included a recent architecture based on Vision Transformer (ViT-based) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib47" title="">47</a>]</cite> that achieved state-of-the-art performance in lane-keeping tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Operational Design Domains Selection.</span>
We selected ODDs encompassing diverse conditions from existing standards (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S2.SS1" title="II-A System Level Testing of ADS ‣ II Background ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">II-A</span></span></a>).
We filtered out domains that do not preserve the driving action when applied for domain augmentation. For example, when converting a sunny road image to a snowy condition, it might require the prediction of a different steering angle that accounts for the different friction, despite the road being the same.
Overall, we identified 6 domain categories and 52 distinct label-preserving domains (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.T1" title="TABLE I ‣ IV-A Experimental Setup ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Table I</span></a>).</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">ODD Domains.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.4.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.1.1">
<span class="ltx_p" id="S4.T1.4.1.1.1.1.1" style="width:35.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1.1.1">Category</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.2.1">
<span class="ltx_p" id="S4.T1.4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.2.1.1.1">Domains</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.4.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.1.1.1">
<span class="ltx_p" id="S4.T1.4.2.1.1.1.1" style="width:35.6pt;">Weathers</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S4.T1.4.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.1.2.1">
<span class="ltx_p" id="S4.T1.4.2.1.2.1.1">cloudy, dust storm, foggy, lightnings, overcast, smoke, sunny</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.4.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.2.1.1">
<span class="ltx_p" id="S4.T1.4.3.2.1.1.1" style="width:35.6pt;">Seasons</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S4.T1.4.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.2.2.1">
<span class="ltx_p" id="S4.T1.4.3.2.2.1.1">autumn, spring, summer, winter</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.4.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.3.1.1">
<span class="ltx_p" id="S4.T1.4.4.3.1.1.1" style="width:35.6pt;">Times of day</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S4.T1.4.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.3.2.1">
<span class="ltx_p" id="S4.T1.4.4.3.2.1.1">afternoon, dawn, dusk, evening, morning, night, sunset</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.4.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.4.1.1">
<span class="ltx_p" id="S4.T1.4.5.4.1.1.1" style="width:35.6pt;">Locations</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S4.T1.4.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.4.2.1">
<span class="ltx_p" id="S4.T1.4.5.4.2.1.1">coast, desert, forest, lake, mountain, plains, rivers, rural, seaside</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.4.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.5.1.1">
<span class="ltx_p" id="S4.T1.4.6.5.1.1.1" style="width:35.6pt;">Cities</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S4.T1.4.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.5.2.1">
<span class="ltx_p" id="S4.T1.4.6.5.2.1.1">beijing, berlin, chicago, el cairo, london, new york, paris, rome, san francisco, sidney, tokyo, toronto</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T1.4.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.6.1.1">
<span class="ltx_p" id="S4.T1.4.7.6.1.1.1" style="width:35.6pt;">Countries</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S4.T1.4.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.6.2.1">
<span class="ltx_p" id="S4.T1.4.7.6.2.1.1">australia, brazil, canada, china, england, france, germany, italy, japan, mexico, morocco, usa</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">To provide further insight into the difficulty of these domains, we measured the challenge they pose by computing the distance between augmented domains and the training data. This was done using the reconstruction error of a Variational Autoencoder (VAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib48" title="">48</a>]</cite> to categorize the domains into three clusters: in-distribution domains (closer to the training distribution, e.g., familiar domain or road conditions), in-between domains (moderately different from the training distribution), and out-of-distribution domains (significantly different from the training distribution). In-distribution domains are useful for testing the robustness of the ADS by simulating scenarios similar to those the model has previously encountered. On the other hand, out-of-distribution domains challenge the ADS’s ability to generalize to new, unfamiliar conditions that are not present or rarely available in training data.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">To determine the classification of these domains, we first trained the VAE to reconstruct the training data. The lower the reconstruction error, the closer the domain is to the training distribution. We generated <math alttext="2,000" class="ltx_Math" display="inline" id="S4.SS1.p5.1.m1.2"><semantics id="S4.SS1.p5.1.m1.2a"><mrow id="S4.SS1.p5.1.m1.2.3.2" xref="S4.SS1.p5.1.m1.2.3.1.cmml"><mn id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">2</mn><mo id="S4.SS1.p5.1.m1.2.3.2.1" xref="S4.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p5.1.m1.2.2" xref="S4.SS1.p5.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.2b"><list id="S4.SS1.p5.1.m1.2.3.1.cmml" xref="S4.SS1.p5.1.m1.2.3.2"><cn id="S4.SS1.p5.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p5.1.m1.1.1">2</cn><cn id="S4.SS1.p5.1.m1.2.2.cmml" type="integer" xref="S4.SS1.p5.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.2c">2,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.m1.2d">2 , 000</annotation></semantics></math> augmented images for each domain using the three domain augmentation strategies and measured the reconstruction error for each. The domains were then sorted by reconstruction error and categorized as in-distribution, in-between, or out-of-distribution.
Finally, we selected three domains that were categorized in the same group for all three augmentation techniques. The final selections were as follows: for in-distribution domains, we included sunny, summer, and afternoon conditions; for in-between domains, we chose autumn, desert, and winter; and for out-of-distribution domains, we selected dust storm, forest, and night scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Diffusion Models Calibration.</span>
We used three state-of-the-art pre-trained diffusion models for our categories: InstructPix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib16" title="">16</a>]</cite> for Instruction-editing, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib18" title="">18</a>]</cite> for Inpainting, and ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib29" title="">29</a>]</cite> with Canny edge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib32" title="">32</a>]</cite> conditioning for Inpainting with Refinement.
We fine-tuned the hyperparameters of each considered model before answering the research questions. We prioritized image fidelity, adherence to instructions, and preservation of essential road features, which will be evaluated in our first research question.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Particularly, we configured all diffusion models to use the UNIPC multistep scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib49" title="">49</a>]</cite> with 30 inference denoising steps as <span class="ltx_text ltx_font_italic" id="S4.SS1.p7.1.1">noise sampling strategy</span>.
We chose UNIPC because it focuses on generating good images with a few denoising steps. In our exploratory experiments, a higher number of steps did not lead to significantly better images but only introduced additional computational overhead.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.5">For InstructPix2Pix, we set the image guidance scale to <math alttext="2" class="ltx_Math" display="inline" id="S4.SS1.p8.1.m1.1"><semantics id="S4.SS1.p8.1.m1.1a"><mn id="S4.SS1.p8.1.m1.1.1" xref="S4.SS1.p8.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.1.m1.1b"><cn id="S4.SS1.p8.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p8.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.1.m1.1d">2</annotation></semantics></math> and the text guidance scale to <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p8.2.m2.1"><semantics id="S4.SS1.p8.2.m2.1a"><mn id="S4.SS1.p8.2.m2.1.1" xref="S4.SS1.p8.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.2.m2.1b"><cn id="S4.SS1.p8.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p8.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.2.m2.1d">10</annotation></semantics></math>. These settings were found to be a good balance between image and instruction inputs, preserving key features from both sources.
In the Stable Diffusion inpainting pipeline, we used a text guidance scale of <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p8.3.m3.1"><semantics id="S4.SS1.p8.3.m3.1a"><mn id="S4.SS1.p8.3.m3.1.1" xref="S4.SS1.p8.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.3.m3.1b"><cn id="S4.SS1.p8.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p8.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.3.m3.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.3.m3.1d">10</annotation></semantics></math>, which maintained a high level of control over the generated content without compromising the quality of the inpainting process.
ControlNet refining was configured with a text guidance scale of <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p8.4.m4.1"><semantics id="S4.SS1.p8.4.m4.1a"><mn id="S4.SS1.p8.4.m4.1.1" xref="S4.SS1.p8.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.4.m4.1b"><cn id="S4.SS1.p8.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p8.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.4.m4.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.4.m4.1d">10</annotation></semantics></math> and a noise level of <math alttext="50\%" class="ltx_Math" display="inline" id="S4.SS1.p8.5.m5.1"><semantics id="S4.SS1.p8.5.m5.1a"><mrow id="S4.SS1.p8.5.m5.1.1" xref="S4.SS1.p8.5.m5.1.1.cmml"><mn id="S4.SS1.p8.5.m5.1.1.2" xref="S4.SS1.p8.5.m5.1.1.2.cmml">50</mn><mo id="S4.SS1.p8.5.m5.1.1.1" xref="S4.SS1.p8.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.5.m5.1b"><apply id="S4.SS1.p8.5.m5.1.1.cmml" xref="S4.SS1.p8.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p8.5.m5.1.1.1.cmml" xref="S4.SS1.p8.5.m5.1.1.1">percent</csymbol><cn id="S4.SS1.p8.5.m5.1.1.2.cmml" type="integer" xref="S4.SS1.p8.5.m5.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.5.m5.1c">50\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.5.m5.1d">50 %</annotation></semantics></math>. This configuration preserved the structural integrity of the road while still allowing for meaningful and diverse augmentations. Higher noise levels were found to risk excessive alteration of images and potential loss of essential road semantics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p9.3.1">Semantic Validator Configuration.</span>
To determine an appropriate threshold for the OC-TSS metric, we collected <math alttext="150" class="ltx_Math" display="inline" id="S4.SS1.p9.1.m1.1"><semantics id="S4.SS1.p9.1.m1.1a"><mn id="S4.SS1.p9.1.m1.1.1" xref="S4.SS1.p9.1.m1.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.1.m1.1b"><cn id="S4.SS1.p9.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p9.1.m1.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.1.m1.1c">150</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p9.1.m1.1d">150</annotation></semantics></math> images from the simulator of different road semantics, which were manually assigned to three categories: images of straight roads, right turns, and left turns. We computed the OC-TSS for all images and evaluated the OC-TSS similarity between images within the same category and across different categories.
We aimed for a threshold that deems images within the same category as semantically similar and those in different categories as distinct, giving higher priority to filtering out invalid images that belong to another category, rather than including as many valid images from the same category as possible. Correspondingly, after manual inspection of a sample of included/excluded images, we chose a conservative threshold of <math alttext="0.9" class="ltx_Math" display="inline" id="S4.SS1.p9.2.m2.1"><semantics id="S4.SS1.p9.2.m2.1a"><mn id="S4.SS1.p9.2.m2.1.1" xref="S4.SS1.p9.2.m2.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.2.m2.1b"><cn id="S4.SS1.p9.2.m2.1.1.cmml" type="float" xref="S4.SS1.p9.2.m2.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.2.m2.1c">0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p9.2.m2.1d">0.9</annotation></semantics></math>, which minimizes the inclusion of semantically incorrect augmentations, while at the same time avoiding the exclusion of too many valid images. Thus, augmentations with an OC-TSS <math alttext="\geq 0.9" class="ltx_Math" display="inline" id="S4.SS1.p9.3.m3.1"><semantics id="S4.SS1.p9.3.m3.1a"><mrow id="S4.SS1.p9.3.m3.1.1" xref="S4.SS1.p9.3.m3.1.1.cmml"><mi id="S4.SS1.p9.3.m3.1.1.2" xref="S4.SS1.p9.3.m3.1.1.2.cmml"></mi><mo id="S4.SS1.p9.3.m3.1.1.1" xref="S4.SS1.p9.3.m3.1.1.1.cmml">≥</mo><mn id="S4.SS1.p9.3.m3.1.1.3" xref="S4.SS1.p9.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.3.m3.1b"><apply id="S4.SS1.p9.3.m3.1.1.cmml" xref="S4.SS1.p9.3.m3.1.1"><geq id="S4.SS1.p9.3.m3.1.1.1.cmml" xref="S4.SS1.p9.3.m3.1.1.1"></geq><csymbol cd="latexml" id="S4.SS1.p9.3.m3.1.1.2.cmml" xref="S4.SS1.p9.3.m3.1.1.2">absent</csymbol><cn id="S4.SS1.p9.3.m3.1.1.3.cmml" type="float" xref="S4.SS1.p9.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.3.m3.1c">\geq 0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p9.3.m3.1d">≥ 0.9</annotation></semantics></math> are considered semantically consistent with the original road layout by our automated semantic validator, while those below 0.9 are considered invalid and discarded.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p10.1.1">Knowledge Distillation Configuration.</span>
We used CycleGAN as the student model to distill knowledge from the pre-trained diffusion models. The CycleGAN architecture was adjusted based on recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib50" title="">50</a>]</cite> to reduce droplet artifacts.
We trained one CycleGAN for each of the nine selected ODDs and the three augmentation strategies, resulting in 27 models. Each model was trained for 10 epochs using <math alttext="2,000" class="ltx_Math" display="inline" id="S4.SS1.p10.1.m1.2"><semantics id="S4.SS1.p10.1.m1.2a"><mrow id="S4.SS1.p10.1.m1.2.3.2" xref="S4.SS1.p10.1.m1.2.3.1.cmml"><mn id="S4.SS1.p10.1.m1.1.1" xref="S4.SS1.p10.1.m1.1.1.cmml">2</mn><mo id="S4.SS1.p10.1.m1.2.3.2.1" xref="S4.SS1.p10.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p10.1.m1.2.2" xref="S4.SS1.p10.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p10.1.m1.2b"><list id="S4.SS1.p10.1.m1.2.3.1.cmml" xref="S4.SS1.p10.1.m1.2.3.2"><cn id="S4.SS1.p10.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p10.1.m1.1.1">2</cn><cn id="S4.SS1.p10.1.m1.2.2.cmml" type="integer" xref="S4.SS1.p10.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p10.1.m1.2c">2,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p10.1.m1.2d">2 , 000</annotation></semantics></math> pair of images (from the simulator and augmented), with checkpoints saved at the end of every epoch. The best checkpoint was selected according to the Fréchet Inception Distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib51" title="">51</a>]</cite>, a metric that measures the distance between two sets of images (the ones generated by CycleGAN and the ones generated by the diffusion models) by comparing their feature distributions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p11.1.1">Hardware and Software.</span>
All experiments were executed on a server equipped with an AMD 5950X CPU, 64 GB RAM, and two Nvidia 4090 GPUs (24 GB VRAM). The software environment includes Python 3.10, CUDA 12.1 for GPU acceleration, Pytorch 2.3.0 for the ADS implementations, and Huggingface <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p11.1.2">diffusers</span> 0.27.2 for the diffusion models.</p>
</div>
<div class="ltx_para" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.6">Overall, our evaluation required over <math alttext="600" class="ltx_Math" display="inline" id="S4.SS1.p12.1.m1.1"><semantics id="S4.SS1.p12.1.m1.1a"><mn id="S4.SS1.p12.1.m1.1.1" xref="S4.SS1.p12.1.m1.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.1.m1.1b"><cn id="S4.SS1.p12.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p12.1.m1.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.1.m1.1c">600</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.1.m1.1d">600</annotation></semantics></math> GPU hours and involved over 1.5 million image pairs generated across <math alttext="52" class="ltx_Math" display="inline" id="S4.SS1.p12.2.m2.1"><semantics id="S4.SS1.p12.2.m2.1a"><mn id="S4.SS1.p12.2.m2.1.1" xref="S4.SS1.p12.2.m2.1.1.cmml">52</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.2.m2.1b"><cn id="S4.SS1.p12.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p12.2.m2.1.1">52</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.2.m2.1c">52</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.2.m2.1d">52</annotation></semantics></math> ODD domains using <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.p12.3.m3.1"><semantics id="S4.SS1.p12.3.m3.1a"><mn id="S4.SS1.p12.3.m3.1.1" xref="S4.SS1.p12.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.3.m3.1b"><cn id="S4.SS1.p12.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p12.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.3.m3.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.3.m3.1d">3</annotation></semantics></math> augmentation techniques. This process included filtering the domains to keep the experiment manageable within a reasonable timeframe as training <math alttext="27" class="ltx_Math" display="inline" id="S4.SS1.p12.4.m4.1"><semantics id="S4.SS1.p12.4.m4.1a"><mn id="S4.SS1.p12.4.m4.1.1" xref="S4.SS1.p12.4.m4.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.4.m4.1b"><cn id="S4.SS1.p12.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p12.4.m4.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.4.m4.1c">27</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.4.m4.1d">27</annotation></semantics></math> CycleGAN models for <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p12.5.m5.1"><semantics id="S4.SS1.p12.5.m5.1a"><mn id="S4.SS1.p12.5.m5.1.1" xref="S4.SS1.p12.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.5.m5.1b"><cn id="S4.SS1.p12.5.m5.1.1.cmml" type="integer" xref="S4.SS1.p12.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.5.m5.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.5.m5.1d">10</annotation></semantics></math> epochs each, required more than <math alttext="100" class="ltx_Math" display="inline" id="S4.SS1.p12.6.m6.1"><semantics id="S4.SS1.p12.6.m6.1a"><mn id="S4.SS1.p12.6.m6.1.1" xref="S4.SS1.p12.6.m6.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.6.m6.1b"><cn id="S4.SS1.p12.6.m6.1.1.cmml" type="integer" xref="S4.SS1.p12.6.m6.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.6.m6.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.6.m6.1d">100</annotation></semantics></math> GPU hours.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Metrics</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Semantic Validator Effectiveness.</span>
We consider valid augmentations as the positive class and invalid augmentations as the negative class. Correspondingly, a True (resp. False) Positive TP (resp. FP) is an image regarded as a valid augmentation by our semantic validator, which is valid (resp. invalid) according to the ground truth. Similarly, a True (resp. False) Negative TN (resp. FN) is an image regarded as an invalid augmentation by our semantic validator, which is invalid (resp. valid) according to the ground truth.
To assess the effectiveness of our semantic validation methodology, we utilize the confusion matrix
[[TP, FP], [FN, TN]], either with absolute or percentage values.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Testing Effectiveness.</span>
We utilize two categories of metrics to evaluate ADS performance at the system level: one for measuring misbehavior and another for assessing driving quality. The first category quantifies errors directly, including incidents where the vehicle deviates from lane boundaries (Out-of-Bounds, OOB) or collides with obstacles (C). To capture the spatial distribution of errors, we use Failure Track Coverage (FTC), which identifies the percentage of track sectors where misbehaviors occur. This metric indicates whether errors are concentrated in specific challenging areas or distributed across the entire track. In this way, we can determine if errors primarily arise from the complexity of specific track sections or are induced more broadly by the new domain.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">For driving quality, we assess two key metrics relative to the ADS’s nominal behavior. The first is Relative Cross-Track Error (RCTE), which measures the ratio of the average distance from the lane center in the test domain compared to the nominal domain. An RCTE value greater than 1 indicates degraded performance (the vehicle is closer to the edge of the road), while a value less than 1 indicates improved position accuracy (the vehicle is closer to the center of the road). The second metric is Relative Steering Jerk (RSJ), which calculates the difference in the rate of change of steering angle between the test and nominal domains. Higher RSJ values suggest more abrupt steering adjustments, while lower values indicate smoother driving. Although these metrics do not directly indicate errors, they provide valuable insights into potential performance degradation caused by specific domains.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Computational Overhead.</span>
We evaluate the computational overhead of our domain augmentation strategies by measuring the average time to generate the augmented image and the average time required to complete our experiments, including both the augmentation process and the subsequent testing of the ADS. We compare these timings against a baseline where no augmentation is applied, allowing us to quantify the additional computational load introduced by each strategy.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">RQ<sub class="ltx_sub" id="S4.SS3.6.2.1">1</sub>: Semantic Validity and Realism</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.7">We conducted two surveys with human assessors to evaluate the semantic validity of the images generated by the diffusion models, as well as their degree of realism. We recruited participants from Amazon Mechanical Turk (MTurk) and personal contacts using convenience sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib52" title="">52</a>]</cite>.
Each MTurk participant answered <math alttext="200" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn id="S4.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">200</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">200</annotation></semantics></math> questions, while the others answered <math alttext="100" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn id="S4.SS3.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">100</annotation></semantics></math> questions. Across the two studies, we collected <math alttext="5,300" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.2"><semantics id="S4.SS3.p1.3.m3.2a"><mrow id="S4.SS3.p1.3.m3.2.3.2" xref="S4.SS3.p1.3.m3.2.3.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">5</mn><mo id="S4.SS3.p1.3.m3.2.3.2.1" xref="S4.SS3.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS3.p1.3.m3.2.2" xref="S4.SS3.p1.3.m3.2.2.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.2b"><list id="S4.SS3.p1.3.m3.2.3.1.cmml" xref="S4.SS3.p1.3.m3.2.3.2"><cn id="S4.SS3.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1">5</cn><cn id="S4.SS3.p1.3.m3.2.2.cmml" type="integer" xref="S4.SS3.p1.3.m3.2.2">300</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.2c">5,300</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.2d">5 , 300</annotation></semantics></math> responses, of which only <math alttext="4,500" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.2"><semantics id="S4.SS3.p1.4.m4.2a"><mrow id="S4.SS3.p1.4.m4.2.3.2" xref="S4.SS3.p1.4.m4.2.3.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">4</mn><mo id="S4.SS3.p1.4.m4.2.3.2.1" xref="S4.SS3.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS3.p1.4.m4.2.2" xref="S4.SS3.p1.4.m4.2.2.cmml">500</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.2b"><list id="S4.SS3.p1.4.m4.2.3.1.cmml" xref="S4.SS3.p1.4.m4.2.3.2"><cn id="S4.SS3.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1">4</cn><cn id="S4.SS3.p1.4.m4.2.2.cmml" type="integer" xref="S4.SS3.p1.4.m4.2.2">500</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.2c">4,500</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.2d">4 , 500</annotation></semantics></math> were retained due to failures in answering the control questions of our surveys. Ultimately, we retained responses from <math alttext="35" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mn id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><cn id="S4.SS3.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p1.5.m5.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">35</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">35</annotation></semantics></math> participants, of which <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.p1.6.m6.1"><semantics id="S4.SS3.p1.6.m6.1a"><mn id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><cn id="S4.SS3.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS3.p1.6.m6.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.6.m6.1d">10</annotation></semantics></math> from MTurk and <math alttext="25" class="ltx_Math" display="inline" id="S4.SS3.p1.7.m7.1"><semantics id="S4.SS3.p1.7.m7.1a"><mn id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><cn id="S4.SS3.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS3.p1.7.m7.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">25</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.7.m7.1d">25</annotation></semantics></math> from personal contacts.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.5.1.1">IV-C</span>1 </span>Semantic Validity</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.5">Participants were shown two randomly ordered images and asked to determine whether the images represented the same road semantics, focusing on aspects like road shape and turning direction.
For this study, we randomly selected <math alttext="36" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mn id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><cn id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">36</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">36</annotation></semantics></math> pairs of images for each of the three domain augmentation strategies (Instruction-editing, Inpainting, and Inpainting with Refinement). This resulted in a total of <math alttext="108" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.m2.1"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mn id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">108</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><cn id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.2.m2.1.1">108</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">108</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.m2.1d">108</annotation></semantics></math> pairs, with each strategy contributing <math alttext="18" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.3.m3.1"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><mn id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><cn id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.3.m3.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.3.m3.1d">18</annotation></semantics></math> semantically valid transformations and <math alttext="18" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.4.m4.1"><semantics id="S4.SS3.SSS1.p1.4.m4.1a"><mn id="S4.SS3.SSS1.p1.4.m4.1.1" xref="S4.SS3.SSS1.p1.4.m4.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.4.m4.1b"><cn id="S4.SS3.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.4.m4.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.4.m4.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.4.m4.1d">18</annotation></semantics></math> invalid transformations, according to our semantic validator. Additionally, we included two control questions to filter out low-quality responses: one where the road was the same and one where the road was entirely different. In total, the first questionnaire contained <math alttext="110" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.5.m5.1"><semantics id="S4.SS3.SSS1.p1.5.m5.1a"><mn id="S4.SS3.SSS1.p1.5.m5.1.1" xref="S4.SS3.SSS1.p1.5.m5.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.5.m5.1b"><cn id="S4.SS3.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.5.m5.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.5.m5.1c">110</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.5.m5.1d">110</annotation></semantics></math> questions, of which two were used for quality checks.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">RQ1: Semantic Validity Confusion Matrix.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.4.1.1.1" style="padding:-0.5pt 10.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.1.1.2" style="padding:-0.5pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.2.1">Valid Augmentation</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.1.1.3" style="padding:-0.5pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.3.1">Invalid Augmentation</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.2.2.1" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.2.2.1.1">
<tr class="ltx_tr" id="S4.T2.4.2.2.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.1.1.1.1" style="padding:-0.5pt 10.0pt;">Predicted</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2.2.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.1.1.2.1" style="padding:-0.5pt 10.0pt;">Valid Augmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.2.2.2" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.2.2.2.1">
<tr class="ltx_tr" id="S4.T2.4.2.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.2.1.1.1" style="padding:-0.5pt 10.0pt;">48</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.2.1.2.1" style="padding:-0.5pt 10.0pt;">(55%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.4.2.2.3" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.2.2.3.1">
<tr class="ltx_tr" id="S4.T2.4.2.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.3.1.1.1" style="padding:-0.5pt 10.0pt;">3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.2.2.3.1.2.1" style="padding:-0.5pt 10.0pt;">(3%)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.4.3.3.1" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.3.3.1.1">
<tr class="ltx_tr" id="S4.T2.4.3.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.1.1.1.1" style="padding:-0.5pt 10.0pt;">Predicted</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3.3.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.1.1.2.1" style="padding:-0.5pt 10.0pt;">Invalid Augmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.4.3.3.2" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.3.3.2.1">
<tr class="ltx_tr" id="S4.T2.4.3.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.2.1.1.1" style="padding:-0.5pt 10.0pt;">16</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.2.1.2.1" style="padding:-0.5pt 10.0pt;">(18%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.4.3.3.3" style="padding:-0.5pt 10.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.3.3.3.1">
<tr class="ltx_tr" id="S4.T2.4.3.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.3.1.1.1" style="padding:-0.5pt 10.0pt;">20</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.3.3.3.1.2.1" style="padding:-0.5pt 10.0pt;">(23%)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.5">We considered a road to be semantically equal (or different) when at least <math alttext="\frac{2}{3}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.1.m1.1"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mfrac id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p2.1.m1.1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml">2</mn><mn id="S4.SS3.SSS1.p2.1.m1.1.1.3" xref="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml">3</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"><divide id="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"></divide><cn id="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p2.1.m1.1.1.2">2</cn><cn id="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.SSS1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">\frac{2}{3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.1.m1.1d">divide start_ARG 2 end_ARG start_ARG 3 end_ARG</annotation></semantics></math> of the participants agreed on the outcome. Overall, participants reached a consensus on <math alttext="80.6\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.2.m2.1"><semantics id="S4.SS3.SSS1.p2.2.m2.1a"><mrow id="S4.SS3.SSS1.p2.2.m2.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p2.2.m2.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml">80.6</mn><mo id="S4.SS3.SSS1.p2.2.m2.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.2.m2.1b"><apply id="S4.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.SSS1.p2.2.m2.1.1.2">80.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.2.m2.1c">80.6\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.2.m2.1d">80.6 %</annotation></semantics></math> of the pairs (87 out of 108).
Our study revealed a positive correlation between OC-TSS similarity scores and user opinions, with a Pearson correlation coefficient of <math alttext="0.63" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.3.m3.1"><semantics id="S4.SS3.SSS1.p2.3.m3.1a"><mn id="S4.SS3.SSS1.p2.3.m3.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.cmml">0.63</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.3.m3.1b"><cn id="S4.SS3.SSS1.p2.3.m3.1.1.cmml" type="float" xref="S4.SS3.SSS1.p2.3.m3.1.1">0.63</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.3.m3.1c">0.63</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.3.m3.1d">0.63</annotation></semantics></math> (<math alttext="p" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.4.m4.1"><semantics id="S4.SS3.SSS1.p2.4.m4.1a"><mi id="S4.SS3.SSS1.p2.4.m4.1.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.4.m4.1b"><ci id="S4.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.4.m4.1d">italic_p</annotation></semantics></math>-value=<math alttext="2.63\cdot 10^{-13}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.5.m5.1"><semantics id="S4.SS3.SSS1.p2.5.m5.1a"><mrow id="S4.SS3.SSS1.p2.5.m5.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.cmml"><mn id="S4.SS3.SSS1.p2.5.m5.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml">2.63</mn><mo id="S4.SS3.SSS1.p2.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml">⋅</mo><msup id="S4.SS3.SSS1.p2.5.m5.1.1.3" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.cmml"><mn id="S4.SS3.SSS1.p2.5.m5.1.1.3.2" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.SSS1.p2.5.m5.1.1.3.3" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3.cmml"><mo id="S4.SS3.SSS1.p2.5.m5.1.1.3.3a" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p2.5.m5.1.1.3.3.2" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3.2.cmml">13</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.5.m5.1b"><apply id="S4.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1"><ci id="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.1">⋅</ci><cn id="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml" type="float" xref="S4.SS3.SSS1.p2.5.m5.1.1.2">2.63</cn><apply id="S4.SS3.SSS1.p2.5.m5.1.1.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.5.m5.1.1.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.3">superscript</csymbol><cn id="S4.SS3.SSS1.p2.5.m5.1.1.3.2.cmml" type="integer" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.2">10</cn><apply id="S4.SS3.SSS1.p2.5.m5.1.1.3.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3"><minus id="S4.SS3.SSS1.p2.5.m5.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3"></minus><cn id="S4.SS3.SSS1.p2.5.m5.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.SSS1.p2.5.m5.1.1.3.3.2">13</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.5.m5.1c">2.63\cdot 10^{-13}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.5.m5.1d">2.63 ⋅ 10 start_POSTSUPERSCRIPT - 13 end_POSTSUPERSCRIPT</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.2"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.T2" title="TABLE II ‣ IV-C1 Semantic Validity ‣ IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Table II</span></a> presents the results as a confusion matrix, where columns represent human participants judgments and rows show the semantic validator outcomes, with valid augmentations considered as the positive class.
Our semantic validator failed to filter out invalid generations in only <math alttext="3\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p3.1.m1.1"><semantics id="S4.SS3.SSS1.p3.1.m1.1a"><mrow id="S4.SS3.SSS1.p3.1.m1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p3.1.m1.1.1.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml">3</mn><mo id="S4.SS3.SSS1.p3.1.m1.1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.1.m1.1b"><apply id="S4.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p3.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.1.m1.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p3.1.m1.1d">3 %</annotation></semantics></math> of cases (3 FPs), while it rejected <math alttext="18\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p3.2.m2.1"><semantics id="S4.SS3.SSS1.p3.2.m2.1a"><mrow id="S4.SS3.SSS1.p3.2.m2.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p3.2.m2.1.1.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml">18</mn><mo id="S4.SS3.SSS1.p3.2.m2.1.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.2.m2.1b"><apply id="S4.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p3.2.m2.1.1.2">18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.2.m2.1c">18\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p3.2.m2.1d">18 %</annotation></semantics></math> of the valid transformations (16 FNs).
The former error can affect the effectiveness of the proposed methodology as it could lead to testing ADS on images with different semantics, potentially compromising the validity of ADS testing. The latter, while less severe, may increase the time required to find valid augmentations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.5.1.1">IV-C</span>2 </span>GenAI Realism</h4>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="346" id="S4.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.4.2" style="font-size:90%;">RQ<sub class="ltx_sub" id="S4.F4.4.2.1">1</sub>: Realism.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.6">In the second study, we evaluated the realism of the augmented images. Participants were presented with individual images and asked to rate their realism on a 5-point scale, ranging from <math alttext="1" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.1.m1.1"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mn id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><cn id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.1.m1.1d">1</annotation></semantics></math> (not realistic) to <math alttext="5" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.2.m2.1"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><mn id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><cn id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.2.m2.1d">5</annotation></semantics></math> (very realistic).
For this study, we selected <math alttext="18" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.3.m3.1"><semantics id="S4.SS3.SSS2.p1.3.m3.1a"><mn id="S4.SS3.SSS2.p1.3.m3.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.3.m3.1b"><cn id="S4.SS3.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.3.m3.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.3.m3.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.3.m3.1d">18</annotation></semantics></math> semantically valid transformations for each of the three domain augmentation strategies. We also included <math alttext="18" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.4.m4.1"><semantics id="S4.SS3.SSS2.p1.4.m4.1a"><mn id="S4.SS3.SSS2.p1.4.m4.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.4.m4.1b"><cn id="S4.SS3.SSS2.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.4.m4.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.4.m4.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.4.m4.1d">18</annotation></semantics></math> images from the simulator and <math alttext="18" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.5.m5.1"><semantics id="S4.SS3.SSS2.p1.5.m5.1a"><mn id="S4.SS3.SSS2.p1.5.m5.1.1" xref="S4.SS3.SSS2.p1.5.m5.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.5.m5.1b"><cn id="S4.SS3.SSS2.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.5.m5.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.5.m5.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.5.m5.1d">18</annotation></semantics></math> real-world driving images for a better comparison. In total, the second questionnaire contained <math alttext="90" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.6.m6.1"><semantics id="S4.SS3.SSS2.p1.6.m6.1a"><mn id="S4.SS3.SSS2.p1.6.m6.1.1" xref="S4.SS3.SSS2.p1.6.m6.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.6.m6.1b"><cn id="S4.SS3.SSS2.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS3.SSS2.p1.6.m6.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.6.m6.1c">90</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.6.m6.1d">90</annotation></semantics></math> questions. We computed the average realism score for each category of images (augmented, simulator, and real-world) based on the participants’ ratings.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.6"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.F4" title="Figure 4 ‣ IV-C2 GenAI Realism ‣ IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> shows the results. The images generated by <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.1">Inpainting</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.2">Inpainting with Refinement</span> strategies are perceived as more realistic compared to those generated by the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.3">Instruction-editing</span> strategy. The <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.4">Mann-Whitney U test</span> with an alpha of <math alttext="0.05" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.1.m1.1"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mn id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><cn id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" type="float" xref="S4.SS3.SSS2.p2.1.m1.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.1.m1.1d">0.05</annotation></semantics></math> indicates statistically significant differences in the realism scores between the strategies, with a medium effect size (<math alttext="0.5" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.2.m2.1"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><mn id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><cn id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" type="float" xref="S4.SS3.SSS2.p2.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.2.m2.1d">0.5</annotation></semantics></math> and <math alttext="0.6" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.3.m3.1"><semantics id="S4.SS3.SSS2.p2.3.m3.1a"><mn id="S4.SS3.SSS2.p2.3.m3.1.1" xref="S4.SS3.SSS2.p2.3.m3.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.3.m3.1b"><cn id="S4.SS3.SSS2.p2.3.m3.1.1.cmml" type="float" xref="S4.SS3.SSS2.p2.3.m3.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.3.m3.1c">0.6</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.3.m3.1d">0.6</annotation></semantics></math>, respectively). Additionally, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.5">Inpainting with Refinement</span> was found to produce more realistic images than <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.6.6">Inpainting</span> (<math alttext="p" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.4.m4.1"><semantics id="S4.SS3.SSS2.p2.4.m4.1a"><mi id="S4.SS3.SSS2.p2.4.m4.1.1" xref="S4.SS3.SSS2.p2.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.4.m4.1b"><ci id="S4.SS3.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p2.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.4.m4.1d">italic_p</annotation></semantics></math>-value=<math alttext="0.02" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.5.m5.1"><semantics id="S4.SS3.SSS2.p2.5.m5.1a"><mn id="S4.SS3.SSS2.p2.5.m5.1.1" xref="S4.SS3.SSS2.p2.5.m5.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.5.m5.1b"><cn id="S4.SS3.SSS2.p2.5.m5.1.1.cmml" type="float" xref="S4.SS3.SSS2.p2.5.m5.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.5.m5.1c">0.02</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.5.m5.1d">0.02</annotation></semantics></math>), with a small effect size (<math alttext="0.17" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.6.m6.1"><semantics id="S4.SS3.SSS2.p2.6.m6.1a"><mn id="S4.SS3.SSS2.p2.6.m6.1.1" xref="S4.SS3.SSS2.p2.6.m6.1.1.cmml">0.17</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.6.m6.1b"><cn id="S4.SS3.SSS2.p2.6.m6.1.1.cmml" type="float" xref="S4.SS3.SSS2.p2.6.m6.1.1">0.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.6.m6.1c">0.17</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.6.m6.1d">0.17</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p3">
<svg class="ltx_picture" height="91.06" id="S4.SS3.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,91.06) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 85.15 C 0 88.41 2.64 91.06 5.91 91.06 L 594.09 91.06 C 597.36 91.06 600 88.41 600 85.15 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 85.15 C 1.97 87.32 3.73 89.09 5.91 89.09 L 594.09 89.09 C 596.27 89.09 598.03 87.32 598.03 85.15 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="63.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:402.3pt;">
<span class="ltx_p ltx_minipage ltx_align_center ltx_align_top" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:429.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3">RQ<sub class="ltx_sub" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1">1</sub></span>: <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
The output of our automated semantic validator matched human judgment to a large extent, with only <math alttext="3\%" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">3</mn><mo id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">3 %</annotation></semantics></math> of the augmented images incorrectly regarded as valid by the validator (and <math alttext="18\%" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1"><semantics id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mrow id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml">18</mn><mo id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><apply id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2">18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">18\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1d">18 %</annotation></semantics></math> valid augmentations incorrectly discarded by the validator). Inpainting with Refinement is the augmentation approach that produces the most realistic images.
</span></span>
</span></foreignobject></g></g></svg>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">RQ<sub class="ltx_sub" id="S4.T3.4.2.1">2</sub>: Effectiveness results for system-level testing on augmented domains.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T3.5.1.1.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T3.5.1.1.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.2.1" style="font-size:80%;">In-distribution domains</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T3.5.1.1.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.3.1" style="font-size:80%;">In-between domains</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T3.5.1.1.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.4.1" style="font-size:80%;">Out-of-distribution domains</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.2.2">
<td class="ltx_td" id="S4.T3.5.2.2.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.2.1" style="font-size:80%;">C</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.3.1" style="font-size:80%;">OOB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.4.1" style="font-size:80%;">FTC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.5.1" style="font-size:80%;">RCTE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.6.1" style="font-size:80%;">RSJ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.7.1" style="font-size:80%;">C</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.8.1" style="font-size:80%;">OOB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.9.1" style="font-size:80%;">FTC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.10.1" style="font-size:80%;">RCTE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.11.1" style="font-size:80%;">RSJ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.12.1" style="font-size:80%;">C</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.13.1" style="font-size:80%;">OOB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.14.1" style="font-size:80%;">FTC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.2.2.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.15.1" style="font-size:80%;">RCTE</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.2.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.2.2.16.1" style="font-size:80%;">RSJ</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.5.3.3.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.3.3.1.1" style="font-size:80%;">DAVE-2</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.5.3.3.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S4.T3.5.3.3.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.4.4">
<td class="ltx_td ltx_align_left" id="S4.T3.5.4.4.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.1.1" style="font-size:80%;">Instruction-editing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.3.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.4.1" style="font-size:80%;">5.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.5.1" style="font-size:80%;">1.13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.6.1" style="font-size:80%;">0.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.7.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.8.1" style="font-size:80%;">19</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.9.1" style="font-size:80%;">45.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.10.1" style="font-size:80%;">1.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.11.1" style="font-size:80%;">0.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.12.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.13.1" style="font-size:80%;">66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.14.1" style="font-size:80%;">82.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.4.4.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.15.1" style="font-size:80%;">1.90</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.4.4.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.4.4.16.1" style="font-size:80%;">0.70</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5">
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.5.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.1.1" style="font-size:80%;">Inpainting</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.3.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.4.1" style="font-size:80%;">2.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.5.1" style="font-size:80%;">1.10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.6.1" style="font-size:80%;">1.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.7.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.8.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.9.1" style="font-size:80%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.10.1" style="font-size:80%;">1.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.11.1" style="font-size:80%;">0.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.12.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.13.1" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.14.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.15.1" style="font-size:80%;">1.21</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.5.5.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.5.5.16.1" style="font-size:80%;">1.06</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.6.6">
<td class="ltx_td ltx_align_left" id="S4.T3.5.6.6.1" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.1.1" style="font-size:80%;">Inpainting with Refinement</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.2" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.3" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.3.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.4" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.4.1" style="font-size:80%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.5" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.5.1" style="font-size:80%;">0.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.6" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.6.1" style="font-size:80%;">2.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.7" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.7.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.8" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.8.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.9" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.9.1" style="font-size:80%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.10" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.10.1" style="font-size:80%;">0.90</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.11" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.11.1" style="font-size:80%;">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.12" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.12.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.13" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.13.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.14" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.14.1" style="font-size:80%;">5.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.6.6.15" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.15.1" style="font-size:80%;">0.96</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.6.6.16" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.6.6.16.1" style="font-size:80%;">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.7.7">
<td class="ltx_td ltx_align_left" id="S4.T3.5.7.7.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.7.7.1.1" style="font-size:80%;">Chauffeur</span></td>
<td class="ltx_td" id="S4.T3.5.7.7.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.7.7.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T3.5.7.7.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.8.8">
<td class="ltx_td ltx_align_left" id="S4.T3.5.8.8.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.1.1" style="font-size:80%;">Instruction-editing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.3.1" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.5.1" style="font-size:80%;">1.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.6.1" style="font-size:80%;">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.7.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.8.1" style="font-size:80%;">19</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.9.1" style="font-size:80%;">35.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.10.1" style="font-size:80%;">1.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.11.1" style="font-size:80%;">0.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.12.1" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.13.1" style="font-size:80%;">67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.14.1" style="font-size:80%;">70.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.8.8.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.15.1" style="font-size:80%;">1.60</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.8.8.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.8.8.16.1" style="font-size:80%;">0.57</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.9.9">
<td class="ltx_td ltx_align_left" id="S4.T3.5.9.9.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.1.1" style="font-size:80%;">Inpainting</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.3.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.4.1" style="font-size:80%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.5.1" style="font-size:80%;">0.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.6.1" style="font-size:80%;">0.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.7.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.8.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.9.1" style="font-size:80%;">7.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.10.1" style="font-size:80%;">1.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.11.1" style="font-size:80%;">0.80</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.12.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.13.1" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.14.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.9.9.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.15.1" style="font-size:80%;">1.05</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.9.9.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.9.9.16.1" style="font-size:80%;">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.10.10">
<td class="ltx_td ltx_align_left" id="S4.T3.5.10.10.1" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.1.1" style="font-size:80%;">Inpainting with Refinement</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.2" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.2.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.3" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.3.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.4" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.4.1" style="font-size:80%;">7.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.5" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.5.1" style="font-size:80%;">1.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.6" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.6.1" style="font-size:80%;">0.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.7" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.7.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.8" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.8.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.9" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.9.1" style="font-size:80%;">0.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.10" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.10.1" style="font-size:80%;">0.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.11" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.11.1" style="font-size:80%;">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.12" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.12.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.13" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.13.1" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.14" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.14.1" style="font-size:80%;">20.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.10.10.15" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.15.1" style="font-size:80%;">1.39</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.10.10.16" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.10.10.16.1" style="font-size:80%;">0.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.11.11">
<td class="ltx_td ltx_align_left" id="S4.T3.5.11.11.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.11.11.1.1" style="font-size:80%;">Epoch</span></td>
<td class="ltx_td" id="S4.T3.5.11.11.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.11.11.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T3.5.11.11.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.12.12">
<td class="ltx_td ltx_align_left" id="S4.T3.5.12.12.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.1.1" style="font-size:80%;">Instruction-editing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.2.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.3.1" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.4.1" style="font-size:80%;">25.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.5.1" style="font-size:80%;">2.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.6.1" style="font-size:80%;">0.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.7.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.8.1" style="font-size:80%;">54</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.9.1" style="font-size:80%;">75.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.10.1" style="font-size:80%;">2.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.11.1" style="font-size:80%;">0.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.12.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.13.1" style="font-size:80%;">70</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.14.1" style="font-size:80%;">87.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.12.12.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.15.1" style="font-size:80%;">2.18</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.12.12.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.12.12.16.1" style="font-size:80%;">0.41</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.13.13">
<td class="ltx_td ltx_align_left" id="S4.T3.5.13.13.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.1.1" style="font-size:80%;">Inpainting</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.3.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.4.1" style="font-size:80%;">7.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.5.1" style="font-size:80%;">1.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.6.1" style="font-size:80%;">0.86</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.7.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.8.1" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.9.1" style="font-size:80%;">7.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.10.1" style="font-size:80%;">1.65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.11.1" style="font-size:80%;">0.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.12.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.13.1" style="font-size:80%;">30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.14.1" style="font-size:80%;">52.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.13.13.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.15.1" style="font-size:80%;">1.98</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.13.13.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.13.13.16.1" style="font-size:80%;">0.71</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.14.14">
<td class="ltx_td ltx_align_left" id="S4.T3.5.14.14.1" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.1.1" style="font-size:80%;">Inpainting with Refinement</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.2" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.3" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.3.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.4" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.5" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.5.1" style="font-size:80%;">1.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.6" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.6.1" style="font-size:80%;">0.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.7" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.7.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.8" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.8.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.9" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.9.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.10" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.10.1" style="font-size:80%;">1.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.11" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.11.1" style="font-size:80%;">0.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.12" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.12.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.13" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.13.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.14" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.14.1" style="font-size:80%;">15.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.14.14.15" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.15.1" style="font-size:80%;">1.80</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.14.14.16" style="padding-bottom:4.0pt;padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.14.14.16.1" style="font-size:80%;">0.62</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.15.15">
<td class="ltx_td ltx_align_left" id="S4.T3.5.15.15.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.15.15.1.1" style="font-size:80%;">ViT-based</span></td>
<td class="ltx_td" id="S4.T3.5.15.15.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td" id="S4.T3.5.15.15.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T3.5.15.15.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.16.16">
<td class="ltx_td ltx_align_left" id="S4.T3.5.16.16.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.1.1" style="font-size:80%;">Instruction-editing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.2.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.3.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.5.1" style="font-size:80%;">1.15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.6.1" style="font-size:80%;">1.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.7.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.8.1" style="font-size:80%;">13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.9.1" style="font-size:80%;">27.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.10.1" style="font-size:80%;">1.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.11.1" style="font-size:80%;">2.45</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.12.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.13.1" style="font-size:80%;">21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.14.1" style="font-size:80%;">45.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.16.16.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.15.1" style="font-size:80%;">1.77</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.16.16.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.16.16.16.1" style="font-size:80%;">3.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.17.17">
<td class="ltx_td ltx_align_left" id="S4.T3.5.17.17.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.1.1" style="font-size:80%;">Inpainting</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.2.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.3.1" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.5.1" style="font-size:80%;">1.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.6.1" style="font-size:80%;">1.74</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.7.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.8.1" style="font-size:80%;">15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.9.1" style="font-size:80%;">25.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.10.1" style="font-size:80%;">1.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.11.1" style="font-size:80%;">1.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.12.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.13.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.14.1" style="font-size:80%;">17.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.17.17.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.15.1" style="font-size:80%;">1.07</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.17.17.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.17.17.16.1" style="font-size:80%;">2.02</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.18.18">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.5.18.18.1" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.1.1" style="font-size:80%;">Inpainting with Refinement</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.2" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.2.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.3" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.3.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.4" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.5" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.5.1" style="font-size:80%;">1.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.6" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.6.1" style="font-size:80%;">1.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.7" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.7.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.8" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.8.1" style="font-size:80%;">12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.9" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.9.1" style="font-size:80%;">25.0%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.10" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.10.1" style="font-size:80%;">1.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.11" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.11.1" style="font-size:80%;">1.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.12" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.12.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.13" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.13.1" style="font-size:80%;">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.14" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.14.1" style="font-size:80%;">22.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.15" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.15.1" style="font-size:80%;">1.02</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.18.18.16" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span class="ltx_text" id="S4.T3.5.18.18.16.1" style="font-size:80%;">1.02</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">RQ<sub class="ltx_sub" id="S4.SS4.6.2.1">2</sub>: Effectiveness</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">This experiment evaluates the effectiveness of the domain augmentation approaches in discovering errors in lane-keeping ADS.
The experiment aims to determine how many errors each approach could find and the nature of these errors.
For each strategy, we ran <math alttext="2,000" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.2"><semantics id="S4.SS4.p1.1.m1.2a"><mrow id="S4.SS4.p1.1.m1.2.3.2" xref="S4.SS4.p1.1.m1.2.3.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">2</mn><mo id="S4.SS4.p1.1.m1.2.3.2.1" xref="S4.SS4.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.1.m1.2.2" xref="S4.SS4.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.2b"><list id="S4.SS4.p1.1.m1.2.3.1.cmml" xref="S4.SS4.p1.1.m1.2.3.2"><cn id="S4.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS4.p1.1.m1.1.1">2</cn><cn id="S4.SS4.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS4.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.2c">2,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.2d">2 , 000</annotation></semantics></math> ADS simulation steps for each augmented domain, collecting the failures and driving quality metrics (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.SS2" title="IV-B Metrics ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">IV-B</span></span></a>).
As a baseline, we used the predefined set of domains available within the simulator (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.SS1" title="IV-A Experimental Setup ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">IV-A</span></span></a>).</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">RQ<sub class="ltx_sub" id="S4.T4.4.2.1">2</sub>: Effectiveness results for system-level testing on domains available in the simulator.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.5.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T4.5.1.1.1" style="padding:-0.4pt 14.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S4.T4.5.1.1.2" style="padding:-0.4pt 14.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.5.1.1.2.1" style="font-size:80%;">Simulator Domains</span><span class="ltx_text" id="S4.T4.5.1.1.2.2" style="font-size:80%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T4.5.2.2">
<td class="ltx_td" id="S4.T4.5.2.2.1" style="padding:-0.4pt 14.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.2.2.2" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.2.2.2.1" style="font-size:80%;">C</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.2.2.3" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.2.2.3.1" style="font-size:80%;">OOB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.2.2.4" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.2.2.4.1" style="font-size:80%;">FTC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.2.2.5" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.2.2.5.1" style="font-size:80%;">RCTE</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.2.2.6" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.2.2.6.1" style="font-size:80%;">RSJ</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.5.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.5.3.3.1" style="padding:-0.4pt 14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.3.3.1.1" style="font-size:80%;">DAVE-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.3.3.2" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.3.3.2.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.3.3.3" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.3.3.3.1" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.3.3.4" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.3.3.4.1" style="font-size:80%;">12.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.3.3.5" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.3.3.5.1" style="font-size:80%;">1.19</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.3.3.6" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.3.3.6.1" style="font-size:80%;">0.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.4.4">
<td class="ltx_td ltx_align_left" id="S4.T4.5.4.4.1" style="padding:-0.4pt 14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.4.4.1.1" style="font-size:80%;">Chauffeur</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.4.4.2" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.4.4.2.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.4.4.3" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.4.4.3.1" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.4.4.4" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.4.4.4.1" style="font-size:80%;">17.5%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.4.4.5" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.4.4.5.1" style="font-size:80%;">1.20</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.5.4.4.6" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.4.4.6.1" style="font-size:80%;">0.99</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.5">
<td class="ltx_td ltx_align_left" id="S4.T4.5.5.5.1" style="padding:-0.4pt 14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.1.1" style="font-size:80%;">Epoch</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.5.5.2" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.5.5.3" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.3.1" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.5.5.4" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.4.1" style="font-size:80%;">10.0%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.5.5.5.5" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.5.1" style="font-size:80%;">1.68</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.5.5.5.6" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.6.1" style="font-size:80%;">1.01</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.5.6.6.1" style="padding:-0.4pt 14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.6.6.1.1" style="font-size:80%;">ViT-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.5.6.6.2" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.6.6.2.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.5.6.6.3" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.6.6.3.1" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.5.6.6.4" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.6.6.4.1" style="font-size:80%;">17.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.5.6.6.5" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.6.6.5.1" style="font-size:80%;">1.21</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.5.6.6.6" style="padding:-0.4pt 14.0pt;"><span class="ltx_text" id="S4.T4.5.6.6.6.1" style="font-size:80%;">1.22</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.6"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.T3" title="TABLE III ‣ IV-C2 GenAI Realism ‣ IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Table III</span></a> reports the results of the experiments with the augmented domains generated by our methodology and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.T4" title="TABLE IV ‣ IV-D RQ2: Effectiveness ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Table IV</span></a> shows the results with the domains available within the simulator.
The findings indicate that the proposed methodology can identify misbehaviors across all four ADS.
As expected, domains that are more similar to the training conditions (in-distribution) showed fewer errors and lower failure track coverage compared to in-between and out-of-distribution domains, reflecting the increased difficulty due to domain shifts.
However, even for <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.6.1">in-distribution</span> domains, our approach revealed failures, ranging from a total of <math alttext="6" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><cn id="S4.SS4.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS4.p2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">6</annotation></semantics></math> OOB incidents (<span class="ltx_text" id="S4.SS4.p2.6.2">DAVE-2</span>) to <math alttext="5" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><mn id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><cn id="S4.SS4.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS4.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.1d">5</annotation></semantics></math> collisions and <math alttext="22" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><mn id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><cn id="S4.SS4.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS4.p2.3.m3.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">22</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">22</annotation></semantics></math> OOB incidents (ViT-based). The maximum failure track coverage with in-distribution domains was <math alttext="25\%" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m4.1"><semantics id="S4.SS4.p2.4.m4.1a"><mrow id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mn id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">25</mn><mo id="S4.SS4.p2.4.m4.1.1.1" xref="S4.SS4.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1.1">percent</csymbol><cn id="S4.SS4.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS4.p2.4.m4.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">25\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.4.m4.1d">25 %</annotation></semantics></math>.
The failure track coverage increased as the domains deviated further from the training set. For example, when ADS Epoch was tested with in-between domains, new errors were identified in up to <math alttext="75\%" class="ltx_Math" display="inline" id="S4.SS4.p2.5.m5.1"><semantics id="S4.SS4.p2.5.m5.1a"><mrow id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mn id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">75</mn><mo id="S4.SS4.p2.5.m5.1.1.1" xref="S4.SS4.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1.1">percent</csymbol><cn id="S4.SS4.p2.5.m5.1.1.2.cmml" type="integer" xref="S4.SS4.p2.5.m5.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">75\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.5.m5.1d">75 %</annotation></semantics></math> of the sectors. With out-of-distribution domains, failure track coverage increased to <math alttext="87.5\%" class="ltx_Math" display="inline" id="S4.SS4.p2.6.m6.1"><semantics id="S4.SS4.p2.6.m6.1a"><mrow id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml"><mn id="S4.SS4.p2.6.m6.1.1.2" xref="S4.SS4.p2.6.m6.1.1.2.cmml">87.5</mn><mo id="S4.SS4.p2.6.m6.1.1.1" xref="S4.SS4.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><apply id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS4.p2.6.m6.1.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1.1">percent</csymbol><cn id="S4.SS4.p2.6.m6.1.1.2.cmml" type="float" xref="S4.SS4.p2.6.m6.1.1.2">87.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">87.5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.6.m6.1d">87.5 %</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.2">Simulated domains revealed errors in up to <math alttext="17.5\%" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">17.5</mn><mo id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1">percent</csymbol><cn id="S4.SS4.p3.1.m1.1.1.2.cmml" type="float" xref="S4.SS4.p3.1.m1.1.1.2">17.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">17.5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">17.5 %</annotation></semantics></math> of the track, while augmented domains generated errors in up to <math alttext="87.5\%" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mn id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">87.5</mn><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1">percent</csymbol><cn id="S4.SS4.p3.2.m2.1.1.2.cmml" type="float" xref="S4.SS4.p3.2.m2.1.1.2">87.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">87.5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">87.5 %</annotation></semantics></math> of different sectors of the track. However, in-distribution domains generated by the augmentation strategies did not reveal more misbehaviors than the simulator domains. In contrast, in-between and out-of-distribution domains highlighted more errors, particularly with the Instruction-editing strategy. It is important to note that while in-between and out-of-distribution domains represent testing scenarios that the ADS has seen more seldom or was less trained for, the generated roads are semantically validated and sufficiently representative to be considered valid test scenarios.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.6">Additionally, we found that the Instruction-editing strategy was the most effective across all three domain sets (in-distribution, in-between, and out-of-distribution).
This effectiveness can likely be attributed to two main factors.
First, the domains generated using instruction-editing were perceived as less realistic in our human study (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.SS3" title="IV-C RQ1: Semantic Validity and Realism ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Section <span class="ltx_text">IV-C</span></span></a>).
Despite being semantically validated by our automated validator, these less realistic images may still mislead the ADS into making incorrect decisions.
Second, the average distance from these domains to the training domains was higher compared to those generated by the other two domain augmentation strategies.
Specifically, the average reconstruction errors for the Instruction-editing strategy ranged from <math alttext="0.074" class="ltx_Math" display="inline" id="S4.SS4.p4.1.m1.1"><semantics id="S4.SS4.p4.1.m1.1a"><mn id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">0.074</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><cn id="S4.SS4.p4.1.m1.1.1.cmml" type="float" xref="S4.SS4.p4.1.m1.1.1">0.074</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">0.074</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.1.m1.1d">0.074</annotation></semantics></math> (in-distribution) to <math alttext="0.218" class="ltx_Math" display="inline" id="S4.SS4.p4.2.m2.1"><semantics id="S4.SS4.p4.2.m2.1a"><mn id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml">0.218</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><cn id="S4.SS4.p4.2.m2.1.1.cmml" type="float" xref="S4.SS4.p4.2.m2.1.1">0.218</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">0.218</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.2.m2.1d">0.218</annotation></semantics></math> (out-of-distribution), while the errors for the Inpainting strategy ranged from <math alttext="0.067" class="ltx_Math" display="inline" id="S4.SS4.p4.3.m3.1"><semantics id="S4.SS4.p4.3.m3.1a"><mn id="S4.SS4.p4.3.m3.1.1" xref="S4.SS4.p4.3.m3.1.1.cmml">0.067</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.3.m3.1b"><cn id="S4.SS4.p4.3.m3.1.1.cmml" type="float" xref="S4.SS4.p4.3.m3.1.1">0.067</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.3.m3.1c">0.067</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.3.m3.1d">0.067</annotation></semantics></math> to <math alttext="0.078" class="ltx_Math" display="inline" id="S4.SS4.p4.4.m4.1"><semantics id="S4.SS4.p4.4.m4.1a"><mn id="S4.SS4.p4.4.m4.1.1" xref="S4.SS4.p4.4.m4.1.1.cmml">0.078</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.4.m4.1b"><cn id="S4.SS4.p4.4.m4.1.1.cmml" type="float" xref="S4.SS4.p4.4.m4.1.1">0.078</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.4.m4.1c">0.078</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.4.m4.1d">0.078</annotation></semantics></math>, and for the Inpainting with Refinement strategy, they ranged from <math alttext="0.070" class="ltx_Math" display="inline" id="S4.SS4.p4.5.m5.1"><semantics id="S4.SS4.p4.5.m5.1a"><mn id="S4.SS4.p4.5.m5.1.1" xref="S4.SS4.p4.5.m5.1.1.cmml">0.070</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.5.m5.1b"><cn id="S4.SS4.p4.5.m5.1.1.cmml" type="float" xref="S4.SS4.p4.5.m5.1.1">0.070</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.5.m5.1c">0.070</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.5.m5.1d">0.070</annotation></semantics></math> to <math alttext="0.082" class="ltx_Math" display="inline" id="S4.SS4.p4.6.m6.1"><semantics id="S4.SS4.p4.6.m6.1a"><mn id="S4.SS4.p4.6.m6.1.1" xref="S4.SS4.p4.6.m6.1.1.cmml">0.082</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.6.m6.1b"><cn id="S4.SS4.p4.6.m6.1.1.cmml" type="float" xref="S4.SS4.p4.6.m6.1.1">0.082</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.6.m6.1c">0.082</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.6.m6.1d">0.082</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">We observed different behaviors between ADS models based on Convolutional DNNs (<span class="ltx_text" id="S4.SS4.p5.1.1">DAVE-2</span>, Chauffeur, and Epoch) and the one based on Vision Transformer. For the first ADSs, our augmented domains generally led to lower RSJ, indicating smoother steering responses. In contrast, the ViT-based ADS showed an increase in RSJ as the domain distance increased, suggesting more abrupt steering adjustments in response to unfamiliar scenarios. These differences can likely be attributed to how these two types of neural network architectures process data. Convolutional DNNs, with their hierarchical structure and localized receptive fields, tend to focus on local features, which may allow for more stable and incremental responses. In contrast, ViTs, which utilize global attention mechanisms, can capture broader contextual information but may also be more sensitive to domain shifts, leading to more pronounced reactions to unfamiliar data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p6">
<svg class="ltx_picture" height="88.36" id="S4.SS4.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,88.36) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 82.46 C 0 85.72 2.64 88.36 5.91 88.36 L 594.09 88.36 C 597.36 88.36 600 85.72 600 82.46 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 82.46 C 1.97 84.63 3.73 86.4 5.91 86.4 L 594.09 86.4 C 596.27 86.4 598.03 84.63 598.03 82.46 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="60.81" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS4.p6.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p ltx_minipage ltx_align_center ltx_align_top" id="S4.SS4.p6.pic1.1.1.1.1.1.1" style="width:429.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.pic1.1.1.1.1.1.1.1">RQ<sub class="ltx_sub" id="S4.SS4.p6.pic1.1.1.1.1.1.1.1.1">2</sub></span>: <span class="ltx_text ltx_font_italic" id="S4.SS4.p6.pic1.1.1.1.1.1.1.2">
The proposed augmentation technique has been able to expose failures of four existing ADS models, even in domains that are close to the training one. It represents a valuable complement to the execution of tests in domains supported by the simulator, as it was capable of discovering failures in sectors that the simulator deemed failure-free.
</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">RQ<sub class="ltx_sub" id="S4.SS5.6.2.1">3</sub>: Efficiency</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In this experiment, we assessed the overhead introduced by domain augmentation strategies, particularly focusing on the impact of large diffusion models and the potential efficiency gains from a knowledge-distilled model.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.6"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13661v1#S4.T5" title="TABLE V ‣ IV-E RQ3: Efficiency ‣ IV Evaluation ‣ Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models"><span class="ltx_text ltx_ref_tag">Table V</span></a> presents the results from testing <span class="ltx_text" id="S4.SS5.p2.6.1">DAVE-2</span>, with similar results observed for other lane-keeping ADS systems. A test run of <span class="ltx_text" id="S4.SS5.p2.6.2">DAVE-2</span>, consisting of <math alttext="2,000" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.2"><semantics id="S4.SS5.p2.1.m1.2a"><mrow id="S4.SS5.p2.1.m1.2.3.2" xref="S4.SS5.p2.1.m1.2.3.1.cmml"><mn id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">2</mn><mo id="S4.SS5.p2.1.m1.2.3.2.1" xref="S4.SS5.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS5.p2.1.m1.2.2" xref="S4.SS5.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.2b"><list id="S4.SS5.p2.1.m1.2.3.1.cmml" xref="S4.SS5.p2.1.m1.2.3.2"><cn id="S4.SS5.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS5.p2.1.m1.1.1">2</cn><cn id="S4.SS5.p2.1.m1.2.2.cmml" type="integer" xref="S4.SS5.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.2c">2,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.2d">2 , 000</annotation></semantics></math> simulation steps without augmentation, took approximately <math alttext="15.7" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mn id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml">15.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><cn id="S4.SS5.p2.2.m2.1.1.cmml" type="float" xref="S4.SS5.p2.2.m2.1.1">15.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">15.7</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">15.7</annotation></semantics></math> minutes (about <math alttext="471.0" class="ltx_Math" display="inline" id="S4.SS5.p2.3.m3.1"><semantics id="S4.SS5.p2.3.m3.1a"><mn id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml">471.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><cn id="S4.SS5.p2.3.m3.1.1.cmml" type="float" xref="S4.SS5.p2.3.m3.1.1">471.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">471.0</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.3.m3.1d">471.0</annotation></semantics></math>ms per simulation step). <span class="ltx_text" id="S4.SS5.p2.6.3">DAVE-2</span> required only <math alttext="1.2" class="ltx_Math" display="inline" id="S4.SS5.p2.4.m4.1"><semantics id="S4.SS5.p2.4.m4.1a"><mn id="S4.SS5.p2.4.m4.1.1" xref="S4.SS5.p2.4.m4.1.1.cmml">1.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.4.m4.1b"><cn id="S4.SS5.p2.4.m4.1.1.cmml" type="float" xref="S4.SS5.p2.4.m4.1.1">1.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.4.m4.1c">1.2</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.4.m4.1d">1.2</annotation></semantics></math>ms per prediction, while the remaining time was consumed by infrastructure tasks such as communication between the simulator and agent, image processing, persistent logging, and simulation management. Other ADS systems showed inference times ranging from <math alttext="1.1" class="ltx_Math" display="inline" id="S4.SS5.p2.5.m5.1"><semantics id="S4.SS5.p2.5.m5.1a"><mn id="S4.SS5.p2.5.m5.1.1" xref="S4.SS5.p2.5.m5.1.1.cmml">1.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.5.m5.1b"><cn id="S4.SS5.p2.5.m5.1.1.cmml" type="float" xref="S4.SS5.p2.5.m5.1.1">1.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.5.m5.1c">1.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.5.m5.1d">1.1</annotation></semantics></math>ms to <math alttext="2.0" class="ltx_Math" display="inline" id="S4.SS5.p2.6.m6.1"><semantics id="S4.SS5.p2.6.m6.1a"><mn id="S4.SS5.p2.6.m6.1.1" xref="S4.SS5.p2.6.m6.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.6.m6.1b"><cn id="S4.SS5.p2.6.m6.1.1.cmml" type="float" xref="S4.SS5.p2.6.m6.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.6.m6.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.6.m6.1d">2.0</annotation></semantics></math>ms.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.5">The use of diffusion models significantly increased the test duration. Specifically, the Inpainting strategy extended the testing time to <math alttext="53.2" class="ltx_Math" display="inline" id="S4.SS5.p3.1.m1.1"><semantics id="S4.SS5.p3.1.m1.1a"><mn id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml">53.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><cn id="S4.SS5.p3.1.m1.1.1.cmml" type="float" xref="S4.SS5.p3.1.m1.1.1">53.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">53.2</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.1.m1.1d">53.2</annotation></semantics></math> minutes (more than three times longer than the baseline), while the Instruction-editing and Inpainting with Refinement strategies increased it beyond <math alttext="70" class="ltx_Math" display="inline" id="S4.SS5.p3.2.m2.1"><semantics id="S4.SS5.p3.2.m2.1a"><mn id="S4.SS5.p3.2.m2.1.1" xref="S4.SS5.p3.2.m2.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.2.m2.1b"><cn id="S4.SS5.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS5.p3.2.m2.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.2.m2.1c">70</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.2.m2.1d">70</annotation></semantics></math> minutes (more than four times longer). While the Instruction-editing strategy had a faster per-augmentation time (<math alttext="894.7" class="ltx_Math" display="inline" id="S4.SS5.p3.3.m3.1"><semantics id="S4.SS5.p3.3.m3.1a"><mn id="S4.SS5.p3.3.m3.1.1" xref="S4.SS5.p3.3.m3.1.1.cmml">894.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.3.m3.1b"><cn id="S4.SS5.p3.3.m3.1.1.cmml" type="float" xref="S4.SS5.p3.3.m3.1.1">894.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.3.m3.1c">894.7</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.3.m3.1d">894.7</annotation></semantics></math>ms), its overall testing duration was longer because a higher percentage of semantically invalid images were detected by our semantic validator and needed to be regenerated. Specifically, the semantic validator filtered out about 48% of images augmented by Instruction-editing, less than <math alttext="1\%" class="ltx_Math" display="inline" id="S4.SS5.p3.4.m4.1"><semantics id="S4.SS5.p3.4.m4.1a"><mrow id="S4.SS5.p3.4.m4.1.1" xref="S4.SS5.p3.4.m4.1.1.cmml"><mn id="S4.SS5.p3.4.m4.1.1.2" xref="S4.SS5.p3.4.m4.1.1.2.cmml">1</mn><mo id="S4.SS5.p3.4.m4.1.1.1" xref="S4.SS5.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.4.m4.1b"><apply id="S4.SS5.p3.4.m4.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS5.p3.4.m4.1.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1.1">percent</csymbol><cn id="S4.SS5.p3.4.m4.1.1.2.cmml" type="integer" xref="S4.SS5.p3.4.m4.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.4.m4.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.4.m4.1d">1 %</annotation></semantics></math> generated by Inpainting, and <math alttext="12\%" class="ltx_Math" display="inline" id="S4.SS5.p3.5.m5.1"><semantics id="S4.SS5.p3.5.m5.1a"><mrow id="S4.SS5.p3.5.m5.1.1" xref="S4.SS5.p3.5.m5.1.1.cmml"><mn id="S4.SS5.p3.5.m5.1.1.2" xref="S4.SS5.p3.5.m5.1.1.2.cmml">12</mn><mo id="S4.SS5.p3.5.m5.1.1.1" xref="S4.SS5.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.5.m5.1b"><apply id="S4.SS5.p3.5.m5.1.1.cmml" xref="S4.SS5.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS5.p3.5.m5.1.1.1.cmml" xref="S4.SS5.p3.5.m5.1.1.1">percent</csymbol><cn id="S4.SS5.p3.5.m5.1.1.2.cmml" type="integer" xref="S4.SS5.p3.5.m5.1.1.2">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.5.m5.1c">12\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.5.m5.1d">12 %</annotation></semantics></math> generated with Inpainting with Refinement.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.3">Conversely, the knowledge-distilled model based on a CycleGAN architecture significantly reduced the augmentation overhead. It generated images in just <math alttext="12.3" class="ltx_Math" display="inline" id="S4.SS5.p4.1.m1.1"><semantics id="S4.SS5.p4.1.m1.1a"><mn id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml">12.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><cn id="S4.SS5.p4.1.m1.1.1.cmml" type="float" xref="S4.SS5.p4.1.m1.1.1">12.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">12.3</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.1.m1.1d">12.3</annotation></semantics></math>ms on average, resulting in a total testing time of approximately <math alttext="16" class="ltx_Math" display="inline" id="S4.SS5.p4.2.m2.1"><semantics id="S4.SS5.p4.2.m2.1a"><mn id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><cn id="S4.SS5.p4.2.m2.1.1.cmml" type="integer" xref="S4.SS5.p4.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">16</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.2.m2.1d">16</annotation></semantics></math> minutes, less than a <math alttext="2\%" class="ltx_Math" display="inline" id="S4.SS5.p4.3.m3.1"><semantics id="S4.SS5.p4.3.m3.1a"><mrow id="S4.SS5.p4.3.m3.1.1" xref="S4.SS5.p4.3.m3.1.1.cmml"><mn id="S4.SS5.p4.3.m3.1.1.2" xref="S4.SS5.p4.3.m3.1.1.2.cmml">2</mn><mo id="S4.SS5.p4.3.m3.1.1.1" xref="S4.SS5.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.3.m3.1b"><apply id="S4.SS5.p4.3.m3.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.SS5.p4.3.m3.1.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1.1">percent</csymbol><cn id="S4.SS5.p4.3.m3.1.1.2.cmml" type="integer" xref="S4.SS5.p4.3.m3.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.3.m3.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.3.m3.1d">2 %</annotation></semantics></math> increase over the baseline without augmentation.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.11.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S4.T5.12.2" style="font-size:90%;">RQ3: Performance Overhead.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.9.10.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T5.9.10.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.9.10.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.9.10.1.2.1">
<tr class="ltx_tr" id="S4.T5.9.10.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.9.10.1.2.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.9.10.1.2.1.1.1.1">Augmentation Time</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.10.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.9.10.1.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.9.10.1.2.1.2.1.1">(ms)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.9.10.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.9.10.1.3.1">
<tr class="ltx_tr" id="S4.T5.9.10.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.9.10.1.3.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.9.10.1.3.1.1.1.1">Testing Run Time</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.10.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.9.10.1.3.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.9.10.1.3.1.2.1.1">(min)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Baseline (no augmentation)</th>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S4.T5.1.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="15.7" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mn id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">15.7</mn><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><cn id="S4.T5.1.1.1.m1.1.1.cmml" type="float" xref="S4.T5.1.1.1.m1.1.1">15.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">15.7</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">15.7</annotation></semantics></math> p  m 0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.3.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Instruction-editing</th>
<td class="ltx_td ltx_align_left" id="S4.T5.2.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="894.7" class="ltx_Math" display="inline" id="S4.T5.2.2.1.m1.1"><semantics id="S4.T5.2.2.1.m1.1a"><mn id="S4.T5.2.2.1.m1.1.1" xref="S4.T5.2.2.1.m1.1.1.cmml">894.7</mn><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.1.m1.1b"><cn id="S4.T5.2.2.1.m1.1.1.cmml" type="float" xref="S4.T5.2.2.1.m1.1.1">894.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.1.m1.1c">894.7</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.1.m1.1d">894.7</annotation></semantics></math> p  m 0.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.3.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="76.9" class="ltx_Math" display="inline" id="S4.T5.3.3.2.m1.1"><semantics id="S4.T5.3.3.2.m1.1a"><mn id="S4.T5.3.3.2.m1.1.1" xref="S4.T5.3.3.2.m1.1.1.cmml">76.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.2.m1.1b"><cn id="S4.T5.3.3.2.m1.1.1.cmml" type="float" xref="S4.T5.3.3.2.m1.1.1">76.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.2.m1.1c">76.9</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.2.m1.1d">76.9</annotation></semantics></math> p  m 5.1</td>
</tr>
<tr class="ltx_tr" id="S4.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.5.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Inpainting</th>
<td class="ltx_td ltx_align_left" id="S4.T5.4.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="1245.2" class="ltx_Math" display="inline" id="S4.T5.4.4.1.m1.1"><semantics id="S4.T5.4.4.1.m1.1a"><mn id="S4.T5.4.4.1.m1.1.1" xref="S4.T5.4.4.1.m1.1.1.cmml">1245.2</mn><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.m1.1b"><cn id="S4.T5.4.4.1.m1.1.1.cmml" type="float" xref="S4.T5.4.4.1.m1.1.1">1245.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.m1.1c">1245.2</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.1.m1.1d">1245.2</annotation></semantics></math> p  m 25.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.5.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="53.2" class="ltx_Math" display="inline" id="S4.T5.5.5.2.m1.1"><semantics id="S4.T5.5.5.2.m1.1a"><mn id="S4.T5.5.5.2.m1.1.1" xref="S4.T5.5.5.2.m1.1.1.cmml">53.2</mn><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.2.m1.1b"><cn id="S4.T5.5.5.2.m1.1.1.cmml" type="float" xref="S4.T5.5.5.2.m1.1.1">53.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.2.m1.1c">53.2</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.5.2.m1.1d">53.2</annotation></semantics></math> p  m 1.2</td>
</tr>
<tr class="ltx_tr" id="S4.T5.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.7.7.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Inpainting with Refinement</th>
<td class="ltx_td ltx_align_left" id="S4.T5.6.6.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="2172.6" class="ltx_Math" display="inline" id="S4.T5.6.6.1.m1.1"><semantics id="S4.T5.6.6.1.m1.1a"><mn id="S4.T5.6.6.1.m1.1.1" xref="S4.T5.6.6.1.m1.1.1.cmml">2172.6</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.1.m1.1b"><cn id="S4.T5.6.6.1.m1.1.1.cmml" type="float" xref="S4.T5.6.6.1.m1.1.1">2172.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.1.m1.1c">2172.6</annotation><annotation encoding="application/x-llamapun" id="S4.T5.6.6.1.m1.1d">2172.6</annotation></semantics></math> p  m 29.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T5.7.7.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="74.1" class="ltx_Math" display="inline" id="S4.T5.7.7.2.m1.1"><semantics id="S4.T5.7.7.2.m1.1a"><mn id="S4.T5.7.7.2.m1.1.1" xref="S4.T5.7.7.2.m1.1.1.cmml">74.1</mn><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.2.m1.1b"><cn id="S4.T5.7.7.2.m1.1.1.cmml" type="float" xref="S4.T5.7.7.2.m1.1.1">74.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.2.m1.1c">74.1</annotation><annotation encoding="application/x-llamapun" id="S4.T5.7.7.2.m1.1d">74.1</annotation></semantics></math> p  m 1.7</td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.9.9.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Knowledge Distillation</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.8.8.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="12.3" class="ltx_Math" display="inline" id="S4.T5.8.8.1.m1.1"><semantics id="S4.T5.8.8.1.m1.1a"><mn id="S4.T5.8.8.1.m1.1.1" xref="S4.T5.8.8.1.m1.1.1.cmml">12.3</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.1.m1.1b"><cn id="S4.T5.8.8.1.m1.1.1.cmml" type="float" xref="S4.T5.8.8.1.m1.1.1">12.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.1.m1.1c">12.3</annotation><annotation encoding="application/x-llamapun" id="S4.T5.8.8.1.m1.1d">12.3</annotation></semantics></math> p  m 0.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T5.9.9.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<math alttext="16.0" class="ltx_Math" display="inline" id="S4.T5.9.9.2.m1.1"><semantics id="S4.T5.9.9.2.m1.1a"><mn id="S4.T5.9.9.2.m1.1.1" xref="S4.T5.9.9.2.m1.1.1.cmml">16.0</mn><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.2.m1.1b"><cn id="S4.T5.9.9.2.m1.1.1.cmml" type="float" xref="S4.T5.9.9.2.m1.1.1">16.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.2.m1.1c">16.0</annotation><annotation encoding="application/x-llamapun" id="S4.T5.9.9.2.m1.1d">16.0</annotation></semantics></math> p  m 0.1</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS5.p5">
<svg class="ltx_picture" height="91.82" id="S4.SS5.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,91.82) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 85.92 C 0 89.18 2.64 91.82 5.91 91.82 L 594.09 91.82 C 597.36 91.82 600 89.18 600 85.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 85.92 C 1.97 88.09 3.73 89.86 5.91 89.86 L 594.09 89.86 C 596.27 89.86 598.03 88.09 598.03 85.92 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="64.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS5.p5.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p ltx_minipage ltx_align_center ltx_align_top" id="S4.SS5.p5.pic1.1.1.1.1.1.1" style="width:429.3pt;"><span class="ltx_text ltx_font_bold" id="S4.SS5.p5.pic1.1.1.1.1.1.1.1">RQ<sub class="ltx_sub" id="S4.SS5.p5.pic1.1.1.1.1.1.1.1.1">3</sub></span>: <span class="ltx_text ltx_font_italic" id="S4.SS5.p5.pic1.1.1.1.1.1.1.2">
Knowledge distillation is an essential component of our approach for achieving high simulation efficiency. The augmentation overhead without knowledge distillation is 470% for Inpainting with Refinement, the technique that produces more valid and more realistic images, which becomes just 2% with knowledge distillation.
</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.5.1.1">IV-F</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS6.6.2">Threats to Validity</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS1.5.1.1">IV-F</span>1 </span>Internal validity</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p" id="S4.SS6.SSS1.p1.1">A potential internal validity threat is our implementation of the scripts used to obtain the results, which we tested extensively. Additionally, we utilized widely used model architectures and simulators from the literature.
The selection of the semantic validity threshold poses another potential threat. In this study, we adopted a conservative threshold to minimize the inclusion of semantically invalid images.
We also assumed that domain augmentations preserve driving action labels. Although similar work has made this assumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib11" title="">11</a>]</cite>, we explicitly excluded domains that are unlikely to maintain label integrity, such as snowy or rainy weather, as those conditions can alter the vehicle dynamics and driving style due to changes in friction or traction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS2.5.1.1">IV-F</span>2 </span>External validity</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p" id="S4.SS6.SSS2.p1.1">The limited number of ADS models and driving tasks in our evaluation pose a threat to the generalizability of our results. Our initial exploration into the usage of diffusion models for enhancing simulation-based testing generation has revealed promising for lane keeping, a relevant task in autonomous driving.
Future work will be directed to study also other vision-based autonomous driving tasks such as object detection, or end-to-end urban driving.
For the diffusion models, we also considered a limited number of instances. To address this threat, we selected state-of-the-art diffusion models of different types, which consistently improved the simulator across ODDs.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Test Generation for Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Existing work leverages the ability of driving simulators to create diverse driving scenes for <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">scenario-based testing</span> of ADS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib55" title="">55</a>]</cite>. Generated scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib56" title="">56</a>]</cite> include a wide range of driving conditions, such as sudden lane changes, adverse weather, or interactions with other vehicles and pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib56" title="">56</a>]</cite>.
Majumdar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib57" title="">57</a>]</cite> propose Paracosm, a tool that simplifies creating complex driving scenarios for systematic testing of different ADS on simulators. Paracosm allows users to programmatically define complex scenarios, including road layouts, weather, and how other agents (e.g., vehicles, pedestrians) behave.
Woodlief et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib58" title="">58</a>]</cite> propose a framework that abstracts sensor inputs to coverage domains that account for the spatial semantics of a scene.
A new technique called Instance Space Analysis was recently proposed to identify the significant features of test scenarios that affect the ability to reveal the unsafe behavior of ADS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib59" title="">59</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">All these test generators operate within a confined range of predefined ODD scenarios, including specific weather conditions, background locations, and times of day, to maximize the number of failures <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">within</span> these predefined scenarios.
Our approach seeks to considerably broaden the range of ODD conditions <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">beyond</span> those currently available. Our methodology is complementary and can be integrated with existing test generators to enhance their effectiveness with no modifications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Offline Testing with Generative AI</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Approaches based on GenAI primarily focus on augmenting existing image datasets by introducing variations like adverse weather or other visual elements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib61" title="">61</a>]</cite>.
For example, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib9" title="">9</a>]</cite> propose DeepRoad, a solution that utilizes UNIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib36" title="">36</a>]</cite> to generate test images by altering the weather from sunny to foggy or snowy. Similarly, Pan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib10" title="">10</a>]</cite> present a method that leverages CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib21" title="">21</a>]</cite> combined with computer vision techniques to synthesize different fog levels with controllable intensity and direction in driving images. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib11" title="">11</a>]</cite> propose TACTICS, an ADS testing framework that uses search-based strategies to identify critical environmental conditions and employs MUNIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib62" title="">62</a>]</cite> to reproduce these conditions in existing driving images.
Attaoui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib63" title="">63</a>]</cite> combine GenAI and search-based testing to test the semantic segmentation module of an ADS.
Other approaches augment existing test images with diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib64" title="">64</a>]</cite>. For instance, Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib65" title="">65</a>]</cite> exploit semantic segmentation maps and a conditional generative model, ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib29" title="">29</a>]</cite>, to generate high-quality synthetic images. Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib66" title="">66</a>]</cite> employed a fine-tuned Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib18" title="">18</a>]</cite> model to create traffic signs in controllable environments.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">While these approaches allow one to assess the behavior of the ADS, they target <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">model-level testing</span> and only measure the discrepancy between predicted actions and the ground truth.
In contrast, our focus is on <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">system-level testing</span>, and our application of GenAI as a rendering engine within a physics-based simulator constitutes a novel contribution to the state of the art in ADS testing.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Data-driven Simulation</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Neural simulators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib67" title="">67</a>]</cite>
consist of data-driven approaches in which GenAI is used to produce a continuous stream of driving images.
Unlike traditional simulators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib68" title="">68</a>]</cite>, which rely on game-based 3D rendering and physics models, neural simulators employ a learnable world model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib69" title="">69</a>]</cite> to represent the environment of the ADS and target novel view synthesis (e.g., bird-eye’s view) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib70" title="">70</a>]</cite>.
For instance, DriveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib15" title="">15</a>]</cite> utilizes GANs to create driving scenarios with controllable weather conditions, traffic objects, and backgrounds.
DriveDreamer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib71" title="">71</a>]</cite> and GAIA-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib72" title="">72</a>]</cite> employ diffusion models to generate real-world driving scenarios.
UniSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13661v1#bib.bib73" title="">73</a>]</cite> is a neural closed-loop sensor simulator that transforms a single recorded log from an ADS into a realistic multi-sensor simulation.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">While neural simulators offer a significant advancement in generating novel and realistic training data for ADS, the lack of a physical representation of the world
limits their applicability for testing. This deficiency can lead to inaccurate simulations of failures, such as collisions, resulting in false positives. Consequently, neural simulators are not the best choice for testing ADS systems at the system level.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">To address this limitation, our approach integrates neural rendering and GenAI techniques with a physics simulator. This combination enables effective testing with precise failure determination while expanding the ODD conditions for testing.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusions and Future Work</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We have generated new ODD scenarios for ADS testing using diffusion models, prompted with instruction editing operations. We have addressed the validity of the augmented images by creating an automated semantic validator, which was found to be extremely accurate in a human study, with as few as 3% invalid images regarded incorrectly as valid.
We have considered the realism of the augmented images by conducting a human study that indicated the Inpainting with Refinement strategy as the technique generating the most realistic images.
We have reduced the simulation overhead to just 2% by introducing a CycleGAN model that takes advantage of knowledge distillation. Most importantly, we have shown that our approach can expose ADS failures even in domains close to the training one and in track sectors that were deemed error-free when considering only simulator-generated test scenarios.
Future work will be devoted to exploring additional domains and investigating alternative diffusion and knowledge distillation models.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Data Availability</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To support reproducibility, all our data, including the code of the diffusion models and our enhanced simulator will be available online.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey of deep learning techniques for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Journal of Field Robotics</em>, vol. 37, no. 3, pp. 362–386, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
“Road vehicles — test scenarios for automated driving systems — scenario categorization,” Standard, February 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Hu, S. Li, T. Huang, B. Tang, R. Huai, and L. Chen, “How simulation helps autonomous driving: A survey of sim2real, digital twins, and parallel intelligence,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Trans. Intell. Veh.</em>, vol. 9, no. 1, pp. 593–612, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
“Unity3d.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unity.com" title="">https://unity.com</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
“Unreal engine.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unrealengine.com/en-US" title="">https://www.unrealengine.com/en-US</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Gambi, M. Mueller, and G. Fraser, “Automatically testing self-driving cars with search-based procedural content generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis</em>, ser. ISSTA 2019.   New York, NY, USA: ACM, 2019, pp. 318–328. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.acm.org/10.1145/3293882.3330566" title="">http://doi.acm.org/10.1145/3293882.3330566</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Riccio and P. Tonella, “Model-Based Exploration of the Frontier of Behaviours for Deep Learning System Testing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>, ser. ESEC/FSE ’20, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Stocco, B. Pulfer, and P. Tonella, “Mind the Gap! A Study on the Transferability of Virtual vs Physical-world Testing of Autonomous Driving Systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Software Engineering</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the ACM/IEEE International Conference on Automated Software Engineering</em>.   ACM, 2018, pp. 132–142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Pan, H. Ao, and Y. Fan, “Metamorphic testing for autonomous driving systems in fog based on quantitative measurement,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE International Conference on Software Quality, Reliability and Security</em>, 2021, pp. 30–37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Li, M. Pan, T. Zhang, and X. Li, “Testing dnn-based autonomous driving systems under critical environmental conditions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the International Conference on Machine Learning</em>, vol. 139.   PMLR, 2021, pp. 6471–6482.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering</em>, ser. ASE 2018.   New York, NY, USA: ACM, 2018, pp. 132–142. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.acm.org/10.1145/3238147.3238187" title="">http://doi.acm.org/10.1145/3238147.3238187</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Biagiola, A. Stocco, V. Riccio, and P. Tonella, “Two is better than one: Digital siblings to improve autonomous driving testing,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Stocco, B. Pulfer, and P. Tonella, “Model vs system level testing of autonomous driving systems: a replication and extension study,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Empir. Softw. Eng.</em>, vol. 28, no. 3, p. 73, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. W. Kim, J. Philion, A. Torralba, and S. Fidler, “Drivegan: Towards a controllable high-quality neural simulation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>.   Computer Vision Foundation / IEEE, 2021, pp. 5820–5829.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T. Brooks, A. Holynski, and A. A. Efros, “Instructpix2pix: Learning to follow image editing instructions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   IEEE, 2023, pp. 18 392–18 402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Meng, Y. He, Y. Song, J. Song, J. Wu, J. Zhu, and S. Ermon, “Sdedit: Guided image synthesis and editing with stochastic differential equations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   IEEE, 2022, pp. 10 674–10 685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. C. Lambertenghi and A. Stocco, “Assessing quality metrics for neural reality gap input mitigation in autonomous driving testing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the International Conference on Software Testing</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE International Conference on Computer Vision</em>, 2017, pp. 2242–2251.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. R. I. . International Organization for Standardization, “Road vehicles - safety of the intended functionality,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
E. Union, “Un regulation no 157 – uniform provisions concerning the approval of vehicles with regards to automated lane keeping systems [2021/389],” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
F. U. Haq, D. Shin, S. Nejati, and L. C. Briand, “Comparing offline and online testing of deep neural networks: An autonomous car case study,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings IEEE International Conference on Software Testing, Validation and Verification</em>.   IEEE, 2020, pp. 85–95.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Li, C. Zhang, W. Zhu, and Y. Ren, “A comprehensive survey of image generation models based on deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Annals of Data Science</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion models in vision: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 9, pp. 10 850–10 869, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, “Zero-shot text-to-image generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the International Conference on Machine Learning</em>, ser. Proceedings of Machine Learning Research, vol. 139.   PMLR, 2021, pp. 8821–8831.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>.   IEEE, 2023, pp. 3813–3824.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, “End to end learning for self-driving cars.” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">CoRR</em>, vol. abs/1604.07316, 2016. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1604.07316" title="">http://arxiv.org/abs/1604.07316</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li, “Pixart-$\alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. F. Canny, “A computational approach to edge detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 8, no. 6, pp. 679–698, 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, and Y. Shan, “T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Conference on Artificial Intelligence, AAAI 2024</em>, 2024, pp. 4296–4304.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, ser. Lecture Notes in Computer Science, vol. 9351.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Álvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, vol. abs/2105.15203, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M. Liu, T. M. Breuel, and J. Kautz, “Unsupervised image-to-image translation networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 2017, pp. 700–708.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Udacity, “A self-driving car simulator built with Unity,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/udacity/self-driving-car-sim" title="">https://github.com/udacity/self-driving-car-sim</a>, 2017, online; accessed 25 October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Stocco, M. Weiss, M. Calzana, and P. Tonella, “Misbehaviour prediction for autonomous driving systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the International Conference on Software Engineering</em>.   ACM, 2020, pp. 359–371.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Udacity, “Udacity self-driving car’s challenge,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/udacity/self-driving-car/" title="">https://github.com/udacity/self-driving-car/</a>, 2017, online; accessed 18 August 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Stocco, M. Weiss, M. Calzana, and P. Tonella, “Misbehaviour prediction for autonomous driving systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of 42nd International Conference on Software Engineering</em>, ser. ICSE ’20.   ACM, 2020, p. 12 pages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. B. et al., “End to end learning for self-driving cars,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Arxiv</em>, vol. abs/1604.07316, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Team Chauffeur, “Steering angle model: Chauffeur,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/chauffeur" title="">https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/chauffeur</a>, 2016, online; accessed 18 August 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Team Epoch, “Steering angle model: Epoch,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/cg23" title="">https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/cg23</a>, 2016, online; accessed 18 August 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: automated testing of deep-neural-network-driven autonomous cars,” in <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the International Conference on Software Engineering</em>.   ACM, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M. Hussain, N. Ali, and J.-E. Hong, “Deepguard: A framework for safeguarding autonomous driving systems from inconsistent behaviour,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Automated Software Engg.</em>, vol. 29, no. 1, may 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox testing of deep learning systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 26th Symposium on Operating Systems Principles</em>, ser. SOSP ’17.   New York, NY, USA: ACM, 2017, pp. 1–18. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.acm.org/10.1145/3132747.3132785" title="">http://doi.acm.org/10.1145/3132747.3132785</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
I. Sonata, Y. Heryadi, A. Wibowo, and W. Budiharto, “End-to-end steering angle prediction for autonomous car using vision transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CommIT (Communication and Information Technology) Journal</em>, vol. 17, pp. 221–234, 09 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the International Conference on Learning Representationss</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
W. Zhao, L. Bai, Y. Rao, J. Zhou, and J. Lu, “Unipc: A unified predictor-corrector framework for fast sampling of diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Advances in Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, “Analyzing and improving the image quality of stylegan,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 8107–8116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">NIPS</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S. J. Stratton, “Population research: Convenience sampling strategies,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Prehospital and Disaster Medicine</em>, vol. 36, no. 4, p. 373–374, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S. T. et al., “A survey on automated driving system testing: Landscapes and trends,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ACM Transactions on Software Engineering and Methodologies</em>, vol. 32, no. 5, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
G. Lou, Y. Deng, X. Zheng, M. Zhang, and T. Zhang, “Testing of autonomous driving systems: where are we and where should we go?” in <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>.   ACM, 2022, pp. 31–43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Zhong, Y. Tang, Y. Zhou, V. de Oliveira Neves, Y. Liu, and B. Ray, “A survey on scenario-based testing for automated driving systems in high-fidelity simulation,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Arxiv</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
J. Jullien, C. Martel, L. Vignollet, and M. Wentland, “Openscenario: A flexible integrated environment to develop educational activities based on pedagogical scenarios,” in <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE International Conference on Advanced Learning Technologies</em>.   IEEE Computer Society, 2009, pp. 509–513.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
R. Majumdar, A. S. Mathur, M. Pirron, L. Stegner, and D. Zufferey, “Paracosm: A test framework for autonomous driving simulations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the International Conference</em>, vol. 12649.   Springer, 2021, pp. 172–195.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
T. Woodlief, F. Toledo, S. Elbaum, and M. B. Dwyer, “S3c: Spatial semantic scene coverage for autonomous vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ICSE</em>, ser. ICSE ’24.   New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3597503.3639178" title="">https://doi.org/10.1145/3597503.3639178</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
N. Neelofar and A. Aleti, “Identifying and explaining safety-critical scenarios for autonomous vehicles via key features,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">ACM Trans. Softw. Eng. Methodol.</em>, vol. 33, no. 4, apr 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3640335" title="">https://doi.org/10.1145/3640335</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
V. Ostankovich, R. Yagfarov, M. Rassabin, and S. Gafurov, “Application of cyclegan-based augmentation for autonomous driving at night,” in <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE International Conference on Nonlinearity, Information and Robotics</em>, 2020, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
K. Gao, J. Wang, B. Wang, R. Wang, and J. Jia, “UAV test data generation method based on cyclegan,” in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the International Conference on Dependable Systems and Their Applications</em>.   IEEE, 2021, pp. 338–343.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
X. Huang, M. Liu, S. J. Belongie, and J. Kautz, “Multimodal unsupervised image-to-image translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the European Conference on Computer Vision</em>, vol. 11207.   Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
M. O. Attaoui, F. Pastore, and L. Briand, “Search-based dnn testing and retraining with gan-enhanced simulations,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.13359" title="">https://arxiv.org/abs/2406.13359</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M. Yang, “Diffusion models: A comprehensive survey of methods and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ACM Comput. Surv.</em>, vol. 56, no. 4, pp. 105:1–105:39, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
H. Zhao, Y. Wang, T. Bashford-Rogers, V. Donzella, and K. Debattista, “Exploring generative AI for sim2real in driving data synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Arxiv</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M. Xu, D. Niyato, J. Chen, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, “Generative ai-empowered simulation for autonomous driving in vehicular mixed reality metaverses,” <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">IEEE J. Sel. Top. Signal Process.</em>, vol. 17, no. 5, pp. 1064–1079, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang, Y. You, Z. Zhang, D. Zhao, L. Xiao, J. Zhao, J. Lu, and G. Huang, “Is sora a world simulator? A comprehensive survey on general world models and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Arxiv</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. M. López, and V. Koltun, “CARLA: an open urban driving simulator,” in <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the Annual Conference on Robot Learning</em>, vol. 78.   PMLR, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y. Guan, H. Liao, Z. Li, G. Zhang, and C. Xu, “World models for autonomous driving: An initial survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Arxiv</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
W. Xia and J. Xue, “A survey on deep generative 3d-aware image synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ACM Comput. Surv.</em>, vol. 56, no. 4, pp. 90:1–90:34, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu, “Drivedreamer: Towards real-world-driven world models for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Arxiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado, “GAIA-1: A generative world model for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Arxiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Z. Yang, Y. Chen, J. Wang, S. Manivasagam, W. Ma, A. J. Yang, and R. Urtasun, “Unisim: A neural closed-loop sensor simulator,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   IEEE, 2023, pp. 1389–1399.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 17:07:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
