<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LiNR: Model Based Neural Retrieval on GPUs at LinkedIn</title>
<!--Generated on Wed Aug  7 16:57:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="information retrieval,  recommender systems,  candidate generation,  nearest neighbor search,  neural retrieval" lang="en" name="keywords"/>
<base href="/html/2407.13218v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S1" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S2" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Modeling Technology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS1" title="In 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Exhaustive Search with Attribute-Based Matching (ABM)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS1.SSS1" title="In 3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>KNN with Similarity Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS1.SSS2" title="In 3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>KNN with Explicit Pre-Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS2" title="In 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Quantized KNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS3" title="In 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Similarity Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS3.SSS1" title="In 3.3. Similarity Modeling ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Hadamard MLP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS3.SSS2" title="In 3.3. Similarity Modeling ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Mixture-of-Logits with Clustering</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>System Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS1" title="In 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Out-of-Network Recommendations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS2" title="In 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>ML Infra Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS2.SSS1" title="In 4.2. ML Infra Architecture ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS2.SSS2" title="In 4.2. ML Infra Architecture ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Ingestor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS2.SSS3" title="In 4.2. ML Infra Architecture ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Service</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS3" title="In 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Live Update</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS4" title="In 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Inference on Native Stack</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS1" title="In 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Model offline evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS2" title="In 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>A/B test of <span class="ltx_text ltx_font_italic">LiNR</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS3" title="In 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model Inference Benchmarking</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS3.SSS1" title="In 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>High-Pass-Rate ABM Dataset Benchmarking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS3.SSS2" title="In 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>Low-Pass-Rate ABM Dataset Benchmarking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS3.SSS3" title="In 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>Impact of Live Model Update on Inference</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S6" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Deployment lessons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S7" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S8" title="In LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">LiNR: Model Based Neural Retrieval on GPUs at LinkedIn</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fedor Borisyuk
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingquan Song
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingzhou Zhou
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ganesh Parameswaran
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Mountain View</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:fedorvb@gmail.com">fedorvb@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Madhu Arun
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siva Popuri
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tugrul Bingol
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Mountain View</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:maarun@linkedin.com">maarun@linkedin.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhuotao Pei
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kuang-Hsuan Lee
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lu Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Mountain View</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zpei@linkedin.com">zpei@linkedin.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qizhan Shao
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Naqvi
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sen Zhou
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aman Gupta
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Mountain View</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hshao@linkedin.com">hshao@linkedin.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id17.id1">This paper introduces <span class="ltx_text ltx_font_italic" id="id17.id1.1">LiNR</span>, LinkedIn’s large-scale, GPU-based retrieval system. <span class="ltx_text ltx_font_italic" id="id17.id1.2">LiNR</span> supports a billion-sized index on GPU models. We discuss our experiences and challenges in creating scalable, differentiable search indexes using TensorFlow and PyTorch at production scale. In <span class="ltx_text ltx_font_italic" id="id17.id1.3">LiNR</span>, both items and model weights are integrated into the model binary. Viewing index construction as a form of model training, we describe scaling our system for large indexes, incorporating full scans and efficient filtering. A key focus is on enabling attribute-based pre-filtering for exhaustive GPU searches, addressing the common challenge of post-filtering in KNN searches that often reduces system quality. We further provide multi-embedding retrieval algorithms and strategies for tackling cold start issues in retrieval. Our advancements in supporting larger indexes through quantization are also discussed. We believe <span class="ltx_text ltx_font_italic" id="id17.id1.4">LiNR</span> represents one of the industry’s first Live-updated model-based retrieval indexes. Applied to out-of-network post recommendations on LinkedIn Feed, <span class="ltx_text ltx_font_italic" id="id17.id1.5">LiNR</span> has contributed to a 3% relative increase in professional daily active users. We envisage <span class="ltx_text ltx_font_italic" id="id17.id1.6">LiNR</span> as a step towards integrating retrieval and ranking into a single GPU model, simplifying complex infrastructures and enabling end-to-end optimization of the entire differentiable infrastructure through gradient descent.</p>
</div>
<div class="ltx_keywords">information retrieval, recommender systems, candidate generation, nearest neighbor search, neural retrieval
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21–25, 2024; Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM ’24), October 21–25, 2024, Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3627673.3680091</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0436-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Similarity measures</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Search engine indexing</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Learning to rank</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">LinkedIn, the world’s largest professional network, serves over a billion members globally, offering services from job searches to content engagement. This paper explores <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">LiNR</span>, LinkedIn’s model-based GPU retrieval system, focusing on embedding-based retrieval (EBR). Traditional EBR uses unsupervised nearest neighbor search solutions <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib15" title="">2020</a>; Jégou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib10" title="">2011</a>)</cite>, indexing item vectors for fast retrieval. Our paper presents an innovative approach, combining exhaustive search with pre-filtering in a differentiable GPU model, using neural networks for distance learning and ranking. In <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">LiNR</span>, item vectors and model weights coexist within the same model binary, unlike traditional search indexing methods.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We believe the future of search and recommender systems lies in differentiable model-based serving, enabling joint optimization of retrieval and ranking. The K-nearest neighbor (KNN) search algorithm, an essential embedding-based retrieval method, uses learned query and item embeddings with a specific similarity metric to select the top-K closest items. Typically, KNN uses dot-product similarity, a form of matrix multiplication with normalized embeddings, which has been significantly sped up on modern GPUs (A100, H100, etc.) in frameworks like PyTorch and TensorFlow. Several challenges motivate us to propose model-based KNN algorithms implemented on GPUs:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Liquidity challenge: Real-time search systems rely on specific attributes to filter relevant items. In job recommendation systems, for example, filters like company names, locations, and skills are essential. Items meeting these conditions must be prioritized to avoid exclusion due to low KNN scores from embeddings alone.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Low latency requirement: Reducing retrieval latency and increasing throughput is a constant priority.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Huge memory cost: As number item embeddings and clauses increase, finding ways to lower memory usage and boost computational speed without compromising retrieval quality presents a significant challenge.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Freshness: Demonstrate that model-based approaches can enhance traditional nearest neighbor searches in quality and latency while supporting functionalities like live updates.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper we discuss deployment of large-scale, neural model-based retrieval system, highlighting key challenges and solutions. A major challenge was the absence of efficient pre-filtering in PyTorch and TensorFlow, addressed by our custom indexing and filtering methods detailed in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS1" title="3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">3.1</span></a>, which also tackle latency issues. We also cover memory cost management through quantization techniques for larger indexes in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS2" title="3.2. Quantized KNN ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">3.2</span></a>. <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">LiNR</span> enhanced search quality, utilizing multi-embedding retrieval algorithms discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.SS3" title="3.3. Similarity Modeling ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">3.3</span></a>. Our work positions us among the pioneers in the industry in introducing a retrieval model-based serving infrastructure (§<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS2" title="4.2. ML Infra Architecture ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4.2</span></a>), showcasing the capability of such model-based retrieval systems to be effectively live-updated at scale (§<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS3" title="4.3. Model Live Update ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4.3</span></a>). We perform our study of model-based index serving focuses on interest-based recommendations on LinkedIn’s Feed, also known as out-of-network (OON) recommendations. These recommendations leverage member profiles and previous interactions with the Feed, enabling LinkedIn members to access highly relevant content. We integrate OON content into various LinkedIn surfaces, like Feed and Notifications, based on predicted user engagement likelihood. The effectiveness of OON recommendations is gauged by member interactions with OON content. We use two-tower neural networks to create embeddings for members and Feed Posts, forming a candidate selection vertical for OON in the Feed through EBR with a differentiable model-based search index. <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">LiNR</span> significantly outperforms FAISS-based <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib9" title="">2021</a>)</cite> retrieval system in OON recommendations. We support full-scan model-based index serving on GPUs with latencies as low as 4 ms, handling indexes from 15 million to a billion entries. This capability, along with modeling enhancements, significantly boosts quality as detailed in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS2" title="5.2. A/B test of LiNR ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Industry focus has predominantly been on approximate neighbor search systems, with FAISS  <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib9" title="">2021</a>)</cite>, ScaNN  <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib5" title="">2020</a>)</cite>, SONG  <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib25" title="">2020</a>)</cite>, RAFT <cite class="ltx_cite ltx_citemacro_citep">(Nolet, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib16" title="">2023</a>)</cite> among notable examples. These support algorithms like HNSW  <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib15" title="">2020</a>)</cite>, IVFPQ  <cite class="ltx_cite ltx_citemacro_citep">(Jégou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib10" title="">2011</a>)</cite>, CAGRA <cite class="ltx_cite ltx_citemacro_citep">(Ootomo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib17" title="">2023</a>)</cite> on CPU and GPU platforms. Termed model-free, these methods use unsupervised algorithms for partitioning space using existing item embeddings, offering flexibility for any item set. In contrast, our approach employs deep neural networks for a model-based search index, fully operational on GPUs. We integrate item indexes with neural network weights within a PyTorch or TensorFlow model, training during index construction and using the model for retrieval.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Recently with more performance and memory available on GPUs several publications have appeared considering model based nearest neighbor search such as <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib23" title="">2023</a>; Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib22" title="">2022</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib19" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib24" title="">2023</a>)</cite>. Mixture of logits (MoL) <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib23" title="">2023</a>)</cite> in its production deployed form implements weighted combination of cosine similarities with neural network gates used to infer per distance component weights. The MoL paper does not provide information on examples of implementation of logits components, and which embedings have been used in production. We extend on top of MoL and introduce practical algorithms on how to learn components of MoL.
Conversely, research by <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib22" title="">2022</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib19" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib24" title="">2023</a>)</cite> has explored using transformers and generative techniques for search indexes. Unlike our system, which stores item embeddings directly, these studies create semantic structures through clustering and transformers to generate document IDs.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">A lot of research has been focused on representation learning with works representing posts and users in social networks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib13" title="">2021</a>; Rangadurai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib20" title="">2022</a>; Pal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib18" title="">2020</a>)</cite>. As one of the components in MoL we have used approaches similar to <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib13" title="">2021</a>; Rangadurai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib20" title="">2022</a>; Pal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib18" title="">2020</a>)</cite>, and additionally extended it with approaches for cold start infrequent users using clustering representations.
Several previous works have explored the concept of model live-updates, which we expand on in this paper. These works include Monolith <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib14" title="">2022</a>)</cite>, PERSIA <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib12" title="">2022</a>)</cite>, and XDL <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib8" title="">2019</a>)</cite>. In contrast, traditional search engines, as seen in Facebook Search EBR  <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib7" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib13" title="">2021</a>)</cite> via Unicorn <cite class="ltx_cite ltx_citemacro_citep">(Curtiss et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib4" title="">2013</a>)</cite>, and Lucene <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib3" title="">2023</a>)</cite>, have primarily focused on live-update functionality for unsupervised indexing techniques such as  <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib9" title="">2021</a>)</cite>. To the best of our knowledge, our paper represents one of the pioneering efforts in the realm of retrieval-based techniques for live-updating TensorFlow (TF) or PyTorch model-based retrieval indexes at a large-scale production level, with high QPS demands.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Modeling Technology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section we will describe how we modeled and developed exhaustive embedding-based search on GPU with attribute-based matching. We will provide details on how we scaled our model-based index to billion size on a single GPU with quantization. We extend Mixture of Logits (MoL) <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib23" title="">2023</a>)</cite> by automatically training cluster embedding components and experimenting with different gating functions and variety of embedding components.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Exhaustive Search with Attribute-Based Matching (ABM)</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Considering the post-filtering (filter after similarity-based retrieval) often suffers from the liquidity issue especially combining with ANN algorithms implemented on GPUs <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib26" title="">2022</a>)</cite>, we first focus on the KNN-based algorithm with attribute-based pre-filtering and introduce several basic approaches adopted to tackle the above challenges. Strategies to further improve the algorithm and tackle other online serving challenges including the live update problem will be introduced in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4" title="4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<p class="ltx_p ltx_align_center" id="S3.F1.1"><span class="ltx_text" id="S3.F1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.F1.1.1.1" style="width:431.2pt;height:226.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.F1.1.1.1.1"><span class="ltx_text" id="S3.F1.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="434" id="S3.F1.1.1.1.1.1.g1" src="x1.png" width="822"/></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
<span class="ltx_text" id="S3.F1.3.1" style="font-size:90%;">KNN with Similarity Masking. An example of five items with single query is used for illustration. Item similarities are computed and masked with two 0-1 vectors returned from the two clause checking. For each item, as long as one attribute is matched with the query attribute, the clause checking is passed (return one) in the masking matrix. The 2nd clause is a reverse matching clause. Top-1 selection is used in this example. D is dimension of item embedding.
</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>KNN with Similarity Masking</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Our first KNN algorithm with ABM is a two-step similarity masking approach. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F1" title="Figure 1 ‣ 3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">1</span></a>, given a query, we first compute the similarity between the query embedding with all item embeddings stored in a matrix to capture their semantic relationships in a similarity vector. Then, we filter out irrelevant items by multiplying it with the 0-1 mask vectors given by each clause to map the similarity scores of the filtered items to zero before the top-K selection. Each query clause could contain multiple attributes. Feasible items should satisfy all clauses, requiring at least one of the attribute in each clauses is matched. Reverse clauses are also supported (such as the company name attributes in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F1" title="Figure 1 ‣ 3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">1</span></a>). As each item could contain different number of attributes for each clause, to effectively utilize the GPU memory for saving and update the clauses, we store all clause attributes in a single matrix in practice and have an extra counting matrix to record the number of attributes for each item in each clause similar to the counting matrix in a CSR format but for each item separately without having the indexing vector. Each item clause is sorted before the concatenation for faster judgement (as we can stop checking early as long as one attribute is matched). We implement the algorithm in CUDA and registered the clause filtering kernel as TensorFlow and PyTorch operations to integrate and serve with other modules.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.1" style="width:433.6pt;height:175.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.2pt,16.6pt) scale(0.84034,0.84034) ;">
<p class="ltx_p" id="S3.F2.1.1"><span class="ltx_text" id="S3.F2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.F2.1.1.1.1" style="width:516.0pt;height:209.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.F2.1.1.1.1.1"><span class="ltx_text" id="S3.F2.1.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="400" id="S3.F2.1.1.1.1.1.1.g1" src="x2.png" width="991"/></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span><span class="ltx_text" id="S3.F2.3.1" style="font-size:90%;">KNN with Explicit Pre-Filtering. Clauses are checked one by one and a joint 0-1 mask vector is returned to retrieve the feasible items for matrix multiplication and top-K selection (K=1 here).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>KNN with Explicit Pre-Filtering</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The second iteration of our KNN with ABM uses a new approach, incorporating explicit filtering before embedding multiplication, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F2" title="Figure 2 ‣ 3.1.1. KNN with Similarity Masking ‣ 3.1. Exhaustive Search with Attribute-Based Matching (ABM) ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">2</span></a>. Initially, we slice the matrix to filter out irrelevant items, removing them early from subsequent computations. This method speeds up the process by reducing the computational burden during matrix multiplication and top-K selection, especially beneficial when the query filters result in a significantly smaller item set. We found that with custom CUDA implementation <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib21" title="">2024</a>)</cite> to merge the kernels, the speed could be generally faster than the first version introduced above. However, without customizing the masked matrix multiplication and kernel merging, simply adopting the matrix slicing in TensorFlow and PyTorch will introduce extra matrix copy and creation overhead, causing it to be slower than the first version when the pass rate is high.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Quantized KNN</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Addressing memory constraints, we adopt a quantized KNN strategy using the Sign One Permutation One Random Projection (Sign-OPORP) method to compress embeddings to 1-bit and approximate dot-products via bitwise matching. This technique balances prediction accuracy and search speed, akin to typical ANN methods, but as an exhaustive search, it seamlessly integrates with attribute-based pre-filtering, circumventing liquidity issues.
OPORP is a variant of count-sketch method. It leverages single random projection with fixed-length binning scheme to efficiently project embedding to a low-dimensional embedding. Sign-OPORP takes the sign of the projected embedding to generate 1-bit embedding that could accurately approximate the cosine similarity of the original floating-point embedding <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib11" title="">2023</a>)</cite> via bit-wise matching, i.e., counting the number of matched bits of two quantized 1-bit embedding.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As the bit-wise matching operation is often much faster than regular matrix multiplication, we can replace the original embedding with quantized embedding and adopt the bit-wise matching operations in the above-mentioned KNN algorithm with pre-filtering, which can help greatly reduce the memory consumption. Compressing 1 billion fp16 embedding of dimension 64 to 1-bit embedding of the same dimension can reduce the memory by 16 times and help serve 1 billion items in single V100 GPU. We could adjust the size of the quantized embedding to balance the trade-off between the memory/speed and accuracy. Besides, if memory is not the concern, we could leverage the approximated similarity as an extra pre-filtering step reduce the computation of the full-precision matrix multiplication (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F3" title="Figure 3 ‣ 3.2. Quantized KNN ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">3</span></a>), offering a unique perspective on exhaustive KNN with ABM. Note that, we call it exhaustive KNN to discriminate it from the regular ANN method with clustering such as HNSW and IVFPQ since our approach still computes all item similarity based on the quantized embeddings, which is easier to be combined with the pre-filtered ABM and live updates (see §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.SS3" title="4.3. Model Live Update ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F3.1" style="width:433.6pt;height:210.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.5pt,19.6pt) scale(0.8427,0.8427) ;">
<p class="ltx_p" id="S3.F3.1.1"><span class="ltx_text" id="S3.F3.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.F3.1.1.1.1" style="width:514.6pt;height:249.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.F3.1.1.1.1.1"><span class="ltx_text" id="S3.F3.1.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="478" id="S3.F3.1.1.1.1.1.1.g1" src="x3.png" width="989"/></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span><span class="ltx_text" id="S3.F3.3.1" style="font-size:90%;">KNN with Quantized Filtering helps to reduce the number of retrieved items before the full precision similarity computation. A bit-wise matching is used to measure the approximated similarity between 1-bit quantized embedding obtained via Sign-OPORP method. We use bit-wise XOR operation and perform an integer bit-wise NOT conversion for query or item embedding in advance to measure the number of matched bits in the packed integer vector. The quantized KNN module can be used without full precision matrix multiplication when K is large in top-K selection.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Similarity Modeling</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Hadamard MLP</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Dot Product or cosine similarity has been common in retrieval and it’s computationally efficient. On the other hand, the multilayer perceptron (MLP)-based learned similarity functions has been reported inferior compared to properly tuned dot product. To balance the computation cost/latency and retrieval metrics, we attempted to boost the MLP-based learned similarity function through hadamard product. The architecture is shown on the left of Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F4" title="Figure 4 ‣ 3.3.2. Mixture-of-Logits with Clustering ‣ 3.3. Similarity Modeling ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4</span></a>. A MLP block is applied to member and item embedding respectively, whose output performs hadamard product and then passes to another MLP block to output the final logit. With proper hyper-parameter tuning, it can reliably outperform dot product.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Mixture-of-Logits with Clustering</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.3">Mixture-of-logits  <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib23" title="">2023</a>)</cite> defines a model for computing high rank similarity based on adaptive gating of elementary logits across multiple embedding components <math alttext="\phi_{MoL}(x,u)=\sum\limits_{k}\pi_{k,\theta}(x,u)\delta_{k,\theta}(x,u)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.10"><semantics id="S3.SS3.SSS2.p1.1.m1.10a"><mrow id="S3.SS3.SSS2.p1.1.m1.10.11" xref="S3.SS3.SSS2.p1.1.m1.10.11.cmml"><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.cmml"><msub id="S3.SS3.SSS2.p1.1.m1.10.11.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.2.cmml">ϕ</mi><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.2.cmml">M</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1.cmml">⁢</mo><mi id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.3" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.3.cmml">o</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1a" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1.cmml">⁢</mo><mi id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.4" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.4.cmml">L</mi></mrow></msub><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.1" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.2.3.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.3.1.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS2.p1.1.m1.5.5" xref="S3.SS3.SSS2.p1.1.m1.5.5.cmml">x</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.3.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS2.p1.1.m1.6.6" xref="S3.SS3.SSS2.p1.1.m1.6.6.cmml">u</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.2.3.2.3" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS2.p1.1.m1.10.11.1" rspace="0.111em" xref="S3.SS3.SSS2.p1.1.m1.10.11.1.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.3" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.cmml"><munder id="S3.SS3.SSS2.p1.1.m1.10.11.3.1" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.2" movablelimits="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1.2.cmml">∑</mo><mi id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.3" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1.3.cmml">k</mi></munder><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.3.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.cmml"><msub id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.2.cmml">π</mi><mrow id="S3.SS3.SSS2.p1.1.m1.2.2.2.4" xref="S3.SS3.SSS2.p1.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.1.cmml">k</mi><mo id="S3.SS3.SSS2.p1.1.m1.2.2.2.4.1" xref="S3.SS3.SSS2.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.SSS2.p1.1.m1.2.2.2.2" xref="S3.SS3.SSS2.p1.1.m1.2.2.2.2.cmml">θ</mi></mrow></msub><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.1.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS2.p1.1.m1.7.7" xref="S3.SS3.SSS2.p1.1.m1.7.7.cmml">x</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS2.p1.1.m1.8.8" xref="S3.SS3.SSS2.p1.1.m1.8.8.cmml">u</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.2.3" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.1.cmml">)</mo></mrow><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1a" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1.cmml">⁢</mo><msub id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.2.cmml">δ</mi><mrow id="S3.SS3.SSS2.p1.1.m1.4.4.2.4" xref="S3.SS3.SSS2.p1.1.m1.4.4.2.3.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.3.3.1.1" xref="S3.SS3.SSS2.p1.1.m1.3.3.1.1.cmml">k</mi><mo id="S3.SS3.SSS2.p1.1.m1.4.4.2.4.1" xref="S3.SS3.SSS2.p1.1.m1.4.4.2.3.cmml">,</mo><mi id="S3.SS3.SSS2.p1.1.m1.4.4.2.2" xref="S3.SS3.SSS2.p1.1.m1.4.4.2.2.cmml">θ</mi></mrow></msub><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1b" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.1.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.1.cmml">(</mo><mi id="S3.SS3.SSS2.p1.1.m1.9.9" xref="S3.SS3.SSS2.p1.1.m1.9.9.cmml">x</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.2.2" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.1.cmml">,</mo><mi id="S3.SS3.SSS2.p1.1.m1.10.10" xref="S3.SS3.SSS2.p1.1.m1.10.10.cmml">u</mi><mo id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.2.3" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.10b"><apply id="S3.SS3.SSS2.p1.1.m1.10.11.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11"><eq id="S3.SS3.SSS2.p1.1.m1.10.11.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.1"></eq><apply id="S3.SS3.SSS2.p1.1.m1.10.11.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2"><times id="S3.SS3.SSS2.p1.1.m1.10.11.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.1"></times><apply id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.2">italic-ϕ</ci><apply id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3"><times id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.1"></times><ci id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.2">𝑀</ci><ci id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.3">𝑜</ci><ci id="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.4.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.2.3.4">𝐿</ci></apply></apply><interval closure="open" id="S3.SS3.SSS2.p1.1.m1.10.11.2.3.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.2.3.2"><ci id="S3.SS3.SSS2.p1.1.m1.5.5.cmml" xref="S3.SS3.SSS2.p1.1.m1.5.5">𝑥</ci><ci id="S3.SS3.SSS2.p1.1.m1.6.6.cmml" xref="S3.SS3.SSS2.p1.1.m1.6.6">𝑢</ci></interval></apply><apply id="S3.SS3.SSS2.p1.1.m1.10.11.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3"><apply id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1">subscript</csymbol><sum id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1.2"></sum><ci id="S3.SS3.SSS2.p1.1.m1.10.11.3.1.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.1.3">𝑘</ci></apply><apply id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2"><times id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.1"></times><apply id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.2.2">𝜋</ci><list id="S3.SS3.SSS2.p1.1.m1.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.2.2.2.4"><ci id="S3.SS3.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.1">𝑘</ci><ci id="S3.SS3.SSS2.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.2.2.2.2">𝜃</ci></list></apply><interval closure="open" id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.3.2"><ci id="S3.SS3.SSS2.p1.1.m1.7.7.cmml" xref="S3.SS3.SSS2.p1.1.m1.7.7">𝑥</ci><ci id="S3.SS3.SSS2.p1.1.m1.8.8.cmml" xref="S3.SS3.SSS2.p1.1.m1.8.8">𝑢</ci></interval><apply id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.4.2">𝛿</ci><list id="S3.SS3.SSS2.p1.1.m1.4.4.2.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.4.4.2.4"><ci id="S3.SS3.SSS2.p1.1.m1.3.3.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.3.3.1.1">𝑘</ci><ci id="S3.SS3.SSS2.p1.1.m1.4.4.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.4.4.2.2">𝜃</ci></list></apply><interval closure="open" id="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.11.3.2.5.2"><ci id="S3.SS3.SSS2.p1.1.m1.9.9.cmml" xref="S3.SS3.SSS2.p1.1.m1.9.9">𝑥</ci><ci id="S3.SS3.SSS2.p1.1.m1.10.10.cmml" xref="S3.SS3.SSS2.p1.1.m1.10.10">𝑢</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.10c">\phi_{MoL}(x,u)=\sum\limits_{k}\pi_{k,\theta}(x,u)\delta_{k,\theta}(x,u)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.10d">italic_ϕ start_POSTSUBSCRIPT italic_M italic_o italic_L end_POSTSUBSCRIPT ( italic_x , italic_u ) = ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_π start_POSTSUBSCRIPT italic_k , italic_θ end_POSTSUBSCRIPT ( italic_x , italic_u ) italic_δ start_POSTSUBSCRIPT italic_k , italic_θ end_POSTSUBSCRIPT ( italic_x , italic_u )</annotation></semantics></math>, where <math alttext="\pi_{k,\theta}(x,u)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.2.m2.4"><semantics id="S3.SS3.SSS2.p1.2.m2.4a"><mrow id="S3.SS3.SSS2.p1.2.m2.4.5" xref="S3.SS3.SSS2.p1.2.m2.4.5.cmml"><msub id="S3.SS3.SSS2.p1.2.m2.4.5.2" xref="S3.SS3.SSS2.p1.2.m2.4.5.2.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.4.5.2.2" xref="S3.SS3.SSS2.p1.2.m2.4.5.2.2.cmml">π</mi><mrow id="S3.SS3.SSS2.p1.2.m2.2.2.2.4" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.cmml">k</mi><mo id="S3.SS3.SSS2.p1.2.m2.2.2.2.4.1" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.cmml">θ</mi></mrow></msub><mo id="S3.SS3.SSS2.p1.2.m2.4.5.1" xref="S3.SS3.SSS2.p1.2.m2.4.5.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.2.m2.4.5.3.2" xref="S3.SS3.SSS2.p1.2.m2.4.5.3.1.cmml"><mo id="S3.SS3.SSS2.p1.2.m2.4.5.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.4.5.3.1.cmml">(</mo><mi id="S3.SS3.SSS2.p1.2.m2.3.3" xref="S3.SS3.SSS2.p1.2.m2.3.3.cmml">x</mi><mo id="S3.SS3.SSS2.p1.2.m2.4.5.3.2.2" xref="S3.SS3.SSS2.p1.2.m2.4.5.3.1.cmml">,</mo><mi id="S3.SS3.SSS2.p1.2.m2.4.4" xref="S3.SS3.SSS2.p1.2.m2.4.4.cmml">u</mi><mo id="S3.SS3.SSS2.p1.2.m2.4.5.3.2.3" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.4b"><apply id="S3.SS3.SSS2.p1.2.m2.4.5.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5"><times id="S3.SS3.SSS2.p1.2.m2.4.5.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5.1"></times><apply id="S3.SS3.SSS2.p1.2.m2.4.5.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.4.5.2.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.4.5.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5.2.2">𝜋</ci><list id="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.4"><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1">𝑘</ci><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2">𝜃</ci></list></apply><interval closure="open" id="S3.SS3.SSS2.p1.2.m2.4.5.3.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.5.3.2"><ci id="S3.SS3.SSS2.p1.2.m2.3.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3">𝑥</ci><ci id="S3.SS3.SSS2.p1.2.m2.4.4.cmml" xref="S3.SS3.SSS2.p1.2.m2.4.4">𝑢</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.4c">\pi_{k,\theta}(x,u)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.2.m2.4d">italic_π start_POSTSUBSCRIPT italic_k , italic_θ end_POSTSUBSCRIPT ( italic_x , italic_u )</annotation></semantics></math> represents a learnt gating function, which gives per component weight using soft-max gate given input of user and item features. The parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.3.m3.1"><semantics id="S3.SS3.SSS2.p1.3.m3.1a"><mi id="S3.SS3.SSS2.p1.3.m3.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.1b"><ci id="S3.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.3.m3.1d">italic_θ</annotation></semantics></math> are learnt through Adam optimization of gradients of a sampled soft-max loss.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">Mixture-of-logits requires the availability of multiple features to leverage the gates, because the gates will collapse to a value of 1 if there is only one feature for user and item pair. We augment the feature with learnt cluster id embedding that obviate the necessity for having multiple features.
In §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS1" title="5.1. Model offline evaluation ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">5.1</span></a> we show that learnt cluster id embedding leveraged through Mixture-of-Logits
can significantly improve on top of dot product in production settings.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">Across LinkedIn we observed variety of member behaviour with some members coming frequently and some coming from time to time. For the infrequent members we aimed to improve retrieval system performance. To achieve this we learn cluster id embeddings, which represent interests of cohorts of members and topics of posts. We describe the process on the right of Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S3.F4" title="Figure 4 ‣ 3.3.2. Mixture-of-Logits with Clustering ‣ 3.3. Similarity Modeling ‣ 3. Modeling Technology ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1">For training <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p4.1.1">LiNR</span>, we obtain two-tower embeddings for posts and members as part of the training data, along with available engagement labels. We initialize cluster ID embeddings using K-means on millions of post embeddings. During training for both members and posts, we find the closest cluster ID embedding based on cosine similarity to their two-tower embedding. These cluster IDs for members and posts are integrated into Mixture-of-Logits, along with the original two-tower embedding and other embeddings we developed for our use cases. We experimented with using K-means-initialized cluster ID embeddings as is and fine-tuning them through back propagation. We report the experiment results in §<a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.SS1" title="5.1. Model offline evaluation ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<p class="ltx_p ltx_align_center" id="S3.F4.1"><span class="ltx_text" id="S3.F4.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.F4.1.1.1" style="width:129.1pt;height:63.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.F4.1.1.1.1"><span class="ltx_text" id="S3.F4.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="88" id="S3.F4.1.1.1.1.1.g1" src="extracted/5779748/figures/similarity_modeling.png" width="174"/></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span class="ltx_text" id="S3.F4.3.1" style="font-size:90%;">Illustration of Hadamard MLP (left) and learning cluster id embedding for Mixture-of-Logits(right)</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>System Architecture</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Out-of-Network Recommendations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Out of Network Recommendations is one of the many sources (first pass rankers) of Linkedin Homepage Feed. When a member visits Linkedin feed, a request is triggered from the front end and sent to feed service. Feed service passes this request to many first pass rankers including feed-OON mid tier (a.k.a. interest discovery). This service is responsible for retrieving the top-K most eligible items for the member to send back to feed. Today, the underlying index used for retrieval is a lucene based index.
The runtime of the query of OON is depicted at Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S4.F5" title="Figure 5 ‣ 4.1. Out-of-Network Recommendations ‣ 4. System Architecture ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">5</span></a>. For every member query, a embedding based search is performed across all eligible item embeddings, followed by a layer 1 (L1) ranking model, which decides top-K items.
These are then sent to feed service and ranked by more sophisticated layer 2 (L2) ranking model for members consumption.
<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">LiNR</span> aims to provide an online service to run model based retrieval algorithms that can outperform our baselines: (1) dot-product based EBR, and (2) FAISS-IVFPQ, which is supported at Linkedin for lucene systems.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F5.1" style="width:433.6pt;height:280.8pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-359.9pt,232.7pt) scale(0.37594,0.37594) ;">
<p class="ltx_p" id="S4.F5.1.1"><span class="ltx_text" id="S4.F5.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.F5.1.1.1.1" style="width:1153.4pt;height:746.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.F5.1.1.1.1.1"><span class="ltx_text" id="S4.F5.1.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="1032" id="S4.F5.1.1.1.1.1.1.g1" src="extracted/5779748/figures/Feed_OON_small.png" width="1596"/></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span class="ltx_text" id="S4.F5.3.1" style="font-size:90%;">Feed OON Architechture.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">As shown in the figure, interest-discovery will call model-cloud-L0 to fetch candidate items for the member. Model-cloud-L0 hosts the RAR model that does (1) item attribute-based filtering (2) embedding based retrieval with ranking using model. The model consists of the item embeddings, features needed for filtering and the trained model weights. Item embeddings are generated on a nearline fashion as and when a document is created at Linkedin so as to keep the index up to date. The filters required for filtering are also ingested nearline.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>ML Infra Architecture</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We enhance Model Cloud, our hosted solution for serving model inferences, to support retrieval as ranking as shown in Figure 5.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Retriever</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">This component performs attribute-based filtering and embedding-based retrieval of the top-k documents for a query. At startup, retriever initializes with the retrieval model and bootstrapped data. Its framework-agnostic design allows easy extension to any framework, such as Torch or TensorFlow. AI engineers can experiment with new methods by developing and deploying corresponding models to this system.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Ingestor</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Model-based retrieval requires the entire document corpus to reside in GPU memory for low latency. To provide fresh results, this corpus must be updated near real-time (nearline). Several following components work together to achieve this functionality.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">Index Store:</span>
Attributes and embeddings come from offline sources and nearline data streams. We use Apache Beam to join and transform feature data for the entire document corpus. Offline, the full corpus is batch-pushed to a Venice Store. Nearline updates are also written to this Venice Store.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.1">Updator:</span>
Updator subscribes to the Index Store’s Change-Data-Capture (CDC) Stream. As the feature data gets batch pushed and live updated, the Updator gets notified to further process them and write to the model.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p4.1.1">Bootstrapper:</span>
At startup, the Ingestor bootstraps from the Venice CDC client by replaying all data from the beginning. The entire data corpus is transformed into the required format and copied to the GPU. To minimize bootstrapping time, we regularly compact the bootstrap data and store a snapshot on disk for a fast warm start.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Service</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">To meet our performance needs, we avoid the latency and unpredictability of managed, garbage-collected languages. We also minimize network hops, data copies, and transformations. Our Model Cloud L0 service is written in a native language with minimal data transformations. User queries from the L0 client land directly on our service, ensuring we meet latency requirements.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F6.1" style="width:433.6pt;height:211.9pt;vertical-align:-0.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-806.2pt,393.5pt) scale(0.21194,0.21194) ;">
<p class="ltx_p" id="S4.F6.1.1"><span class="ltx_text" id="S4.F6.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.F6.1.1.1.1" style="width:2046.0pt;height:999.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.F6.1.1.1.1.1"><span class="ltx_text" id="S4.F6.1.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="1382" id="S4.F6.1.1.1.1.1.1.g1" src="extracted/5779748/figures/Model_cloud.png" width="2831"/></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span class="ltx_text" id="S4.F6.3.1" style="font-size:90%;">Model Cloud <span class="ltx_text ltx_font_italic" id="S4.F6.3.1.1">LiNR</span> Architecture</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Model Live Update</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Live Update Ingestor subscribes to Venice CDC <cite class="ltx_cite ltx_citemacro_citep">(GV, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib6" title="">2008</a>)</cite> from the bootstrapped offset, classifying changes into upserts and deletes, then transforming and copying them to the GPU.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The system’s effectiveness depends on the quality of the document index, which must remain fresh. This can be done by either regularly rebuilding the index or updating it in near-real-time via a data change stream. We chose the latter for two reasons: it keeps the corpus current, reflecting changes within seconds, and it’s more efficient, avoiding the cost of rebuilding and replacing the entire index. To implement this, we modified the PyTorch model to expose Upsert and Delete APIs, ensuring safe and efficient concurrent index updates during inference. Techniques used include pre-allocating larger tensors, using a high-water mark to track the working set, and making thread-safe in-memory tensor manipulations with minimal data access serialization. These methods ensure that modifications have minimal to no impact on the inference path, as detailed in the Model Inference Benchmarking section.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Inference on Native Stack</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We built a native serving system as we made performance and efficiency our top priorities. To serve the PyTorch model in this system, we had to convert it to a compatible format. There are a few alternatives for this purpose such as TorchScipt and torch.export. PyTorch supports two execution modes: eager mode and graph mode. Optimal performance is achieved by executing everything in graph mode as the operators are first synthesized into a graph, which are compiled and executed as a whole. We picked TorchScript for our initial implementation to execute the model in graph mode. However, by doing so we traded off the performance with ease of development. TorchScript is a subset of Python and comes up with some constraints. It requires static typing and does not support things like exceptions and data-dependent control flows. We found executing this conversion quite challenging and concluded that it should be a part of the model development rather than an afterthought. We also decided to pursue other options which are deemed to be more recent technologies such as torch.export.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section we provide results on modeling ablation studies, online A/B experiments with OON application and infrastructure model-based retrieval inference benchmarking for production indexes.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Model offline evaluation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluated <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">LiNR</span> model on our internal dataset. The dataset consists of millions of examples where a member interacted with an item. The member and item are represented by embeddings learnt from a two-tower model.
The two-tower model contains variety of features including member interaction history modeled by <cite class="ltx_cite ltx_citemacro_citep">(Rangadurai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib20" title="">2022</a>)</cite> and member profile features, which usually contains member job title, job location, company, skills and professional summary of the member. Posts usually contain text, image, video or external link information. Therefore, such content features from member and posts can help us to identify topics of interests of posts or professional topics of members.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We used Hit Rate @ 400 over evaluation dataset to report the metrics. We use cosine similarity with exhaustive search as baseline to evaluate against <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">LiNR</span>. We report results of Hadamard MLP for single embedding feature and extended Mixture-of-Logits with clustering for both single and multi-embedding features in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.T1" title="Table 1 ‣ 5.1. Model offline evaluation ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Hadamard MLP is favored for production due to its simplicity for deployment and low latency. However, we found Hadamard MLP is very sensitive to weight initialization and general initialization methods such as GlorotNormal or HeNormal can’t stabilize the performance. Empirically we observed that the initial few steps determine the overall training trend, Thus we reinitialize the model if the first 100 steps go south.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">In addition to the two-tower model and cluster ID features, we enhanced Mixture-of-Logits by introducing multiple embedding features developed at LinkedIn. We incorporated Graph Neural Network (GNN) embeddings for members and posts mapped to the same space using a heterogeneous GNN <cite class="ltx_cite ltx_citemacro_citep">(Borisyuk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#bib.bib2" title="">2024</a>)</cite>. We found that adding more embeddings improved the Hit Rate @ 400 (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.T1" title="Table 1 ‣ 5.1. Model offline evaluation ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Our extended Mixture-of-Logits with clustering perform well for both single and multi-embedding features. One surprise finding is that fixed clusters (non-trainble) outperform trainable clusters in all cases we explored, one possible explanation is the convergence pace of the clustering and other trainable parameters are different, we’ll further investigate it in our future work. Another interesting observation is that it was important to carefully tune the number of clusters: having either too high or too low a value can cause performance to degrade.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1" style="font-size:90%;">Gain in Hit Rate @ 400</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.1.1"><span class="ltx_text" id="S5.T1.1.2.1.1.1" style="font-size:90%;">Cosine similarity</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2"><span class="ltx_text" id="S5.T1.1.2.1.2.1" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.1.1" style="font-size:90%;">Single Embedding Feature</span></th>
<td class="ltx_td ltx_border_t" id="S5.T1.1.3.2.2"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.3.1"><span class="ltx_text" id="S5.T1.1.4.3.1.1" style="font-size:90%;">Hadamard(Member &amp; Item MLP [50]+[10, 1])</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.2"><span class="ltx_text" id="S5.T1.1.4.3.2.1" style="font-size:90%;">10.21%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.5.4.1"><span class="ltx_text" id="S5.T1.1.5.4.1.1" style="font-size:90%;">MoL with 70 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.2"><span class="ltx_text" id="S5.T1.1.5.4.2.1" style="font-size:90%;">1.33% — 10.11%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.6.5.1"><span class="ltx_text" id="S5.T1.1.6.5.1.1" style="font-size:90%;">MoL with 100 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.2">
<span class="ltx_text" id="S5.T1.1.6.5.2.1" style="font-size:90%;">11.97% — </span><span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.2.2" style="font-size:90%;">15.16%</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.7.6.1"><span class="ltx_text" id="S5.T1.1.7.6.1.1" style="font-size:90%;">MoL with 150 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.7.6.2"><span class="ltx_text" id="S5.T1.1.7.6.2.1" style="font-size:90%;">4.26% — 11.17%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.8.7.1.1" style="font-size:90%;">Multiple Embedding Features</span></th>
<td class="ltx_td ltx_border_t" id="S5.T1.1.8.7.2"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.9.8.1"><span class="ltx_text" id="S5.T1.1.9.8.1.1" style="font-size:90%;">MoL without clustering</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.9.8.2"><span class="ltx_text" id="S5.T1.1.9.8.2.1" style="font-size:90%;">12.80%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.10.9.1"><span class="ltx_text" id="S5.T1.1.10.9.1.1" style="font-size:90%;">MoL with 140 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.10.9.2"><span class="ltx_text" id="S5.T1.1.10.9.2.1" style="font-size:90%;">20.75% — 22.61%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.11.10.1"><span class="ltx_text" id="S5.T1.1.11.10.1.1" style="font-size:90%;">MoL with 200 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.11.10.2"><span class="ltx_text" id="S5.T1.1.11.10.2.1" style="font-size:90%;">16.49% — 22.34%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T1.1.12.11.1"><span class="ltx_text" id="S5.T1.1.12.11.1.1" style="font-size:90%;">MoL with 300 trained — non-trained clusters</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.12.11.2">
<span class="ltx_text" id="S5.T1.1.12.11.2.1" style="font-size:90%;">19.04% — </span><span class="ltx_text ltx_font_bold" id="S5.T1.1.12.11.2.2" style="font-size:90%;">23.67%</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Hit Rate @ 400 for single embedding of two-tower model alone in Hadamard or combined with cluster id in MoL, and multiple embeddings combining two-tower, GNN, and cluster ID in MoL.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>A/B test of <span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">LiNR</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">For our baseline a dot-product EBR is done across all eligible items given member embedding query. We enabled cache on a cloud based storage for online lookup of computed results. This top K is retrieved by mid tier service when an online feed request is received.
We leveraged this retrieval framework to test RAR based algorithms to understand relevance impact. The baseline for these experiments are full scan dot-product.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1" style="font-size:90%;">Metric Name</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1" style="font-size:90%;">Metric Lift</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.1"><span class="ltx_text" id="S5.T2.1.2.1.1.1" style="font-size:90%;">Total professional interactions</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.2"><span class="ltx_text" id="S5.T2.1.2.1.2.1" style="font-size:90%;">+7%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.1"><span class="ltx_text" id="S5.T2.1.3.2.1.1" style="font-size:90%;">Daily Unique Gold Professional Interactors</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.2"><span class="ltx_text" id="S5.T2.1.3.2.2.1" style="font-size:90%;">+3%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.1"><span class="ltx_text" id="S5.T2.1.4.3.1.1" style="font-size:90%;">Feed Update Views With 30+ Secs Dwell</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.2"><span class="ltx_text" id="S5.T2.1.4.3.2.1" style="font-size:90%;">+2%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.4.1"><span class="ltx_text" id="S5.T2.1.5.4.1.1" style="font-size:90%;">Feed Update Viewers With 30+ Secs Dwell</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.4.2"><span class="ltx_text" id="S5.T2.1.5.4.2.1" style="font-size:90%;">+5%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.1.6.5.1"><span class="ltx_text" id="S5.T2.1.6.5.1.1" style="font-size:90%;">Skipped Update Rate</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.1.6.5.2"><span class="ltx_text" id="S5.T2.1.6.5.2.1" style="font-size:90%;">-20%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span><span class="ltx_text ltx_font_italic" id="S5.T2.5.1">LiNR</span> A/B test relative metric improvements.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.T2" title="Table 2 ‣ 5.2. A/B test of LiNR ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">2</span></a> shows the A/B test results from ramp of <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">LiNR</span>. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2">Total professional interactions</em> are the total amount of high quality interactions in the form of reshares, reposts, comments, message responses, reacts, votes, saves, and long dwells. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.3">Daily Unique Gold Professional Interactors</em> is the daily moving average of the number of members or companies generating high quality interactions. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.4">Feed Update Views With 30+ Secs Dwell</em> counts the total number of feed updates viewed with 30+ secs dwell time. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.5">Feed Update Viewers With 30 Plus Secs Dwell</em> counts the total number of unique members that viewed a feed update for 30+ secs. <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.6">Skipped Update Rate</em> is the ratio of updates that are skipped (viewed for less than 2 seconds) compared to all viewed updates.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Model Inference Benchmarking</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conducted offline experiments to benchmark the effectiveness of different framework implementations of two KNN variants with ABM on datasets with different pass-rate scenarios. V1 represents KNN with similarity masking, and V2 represents KNN with explicit pre-filtering. Both are implemented in TF and PyTorch with CUDA kernel for attribute matching registered as a custom operator.
We selected the Job recommendation index for benchmarking due to its variety of filters, providing multi-dimensional performance insights for <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">LiNR</span>. The high-pass-rate and low-pass-rate datasets are derived from job search tasks, containing around 15.5 million jobs with 25 thousand queries. The high-pass-rate dataset includes two clauses: geo-location matching and company name reverse matching (mismatched items are returned), with an average of 1.7 million items passing the clauses. The low-pass-rate dataset includes an additional job title exact matching clause, with a maximum pass rate of 1.2 million for single title matching and most queries having only thousands of passed items. Each item and query has one attribute per clause, converted to 64-bit integers before GPU comparison. The embedding dimension is 128, stored as fp16 values. Performance is measured by average latency, p95 latency in milliseconds, and recall label@2000.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>High-Pass-Rate ABM Dataset Benchmarking</h4>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1" style="font-size:90%;">Batch</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.1.1.3.1">
<tr class="ltx_tr" id="S5.T3.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1.1.1.1" style="font-size:90%;">Avg. Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.1.1.4.1">
<tr class="ltx_tr" id="S5.T3.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1.1.1.1" style="font-size:90%;">P95 Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.5.1" style="font-size:90%;">Recall@2k</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.2.1"><span class="ltx_text" id="S5.T3.1.2.2.1.1" style="font-size:90%;">TF-V1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.2.2"><span class="ltx_text" id="S5.T3.1.2.2.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.2.3"><span class="ltx_text" id="S5.T3.1.2.2.3.1" style="font-size:90%;">6.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.2.4"><span class="ltx_text" id="S5.T3.1.2.2.4.1" style="font-size:90%;">6.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.2.5"><span class="ltx_text" id="S5.T3.1.2.2.5.1" style="font-size:90%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.3.1"><span class="ltx_text" id="S5.T3.1.3.3.1.1" style="font-size:90%;">TF-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.3.3.2"><span class="ltx_text" id="S5.T3.1.3.3.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.3"><span class="ltx_text" id="S5.T3.1.3.3.3.1" style="font-size:90%;">6.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.4"><span class="ltx_text" id="S5.T3.1.3.3.4.1" style="font-size:90%;">14.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3.5"><span class="ltx_text" id="S5.T3.1.3.3.5.1" style="font-size:90%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.4.1"><span class="ltx_text" id="S5.T3.1.4.4.1.1" style="font-size:90%;">PyTorch-V1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.4.4.2"><span class="ltx_text" id="S5.T3.1.4.4.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.3"><span class="ltx_text" id="S5.T3.1.4.4.3.1" style="font-size:90%;">4.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.4"><span class="ltx_text" id="S5.T3.1.4.4.4.1" style="font-size:90%;">4.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.5"><span class="ltx_text" id="S5.T3.1.4.4.5.1" style="font-size:90%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.5.5.1"><span class="ltx_text" id="S5.T3.1.5.5.1.1" style="font-size:90%;">PyTorch-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.5.5.2"><span class="ltx_text" id="S5.T3.1.5.5.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.3"><span class="ltx_text" id="S5.T3.1.5.5.3.1" style="font-size:90%;">14.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.4"><span class="ltx_text" id="S5.T3.1.5.5.4.1" style="font-size:90%;">47.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.5.5"><span class="ltx_text" id="S5.T3.1.5.5.5.1" style="font-size:90%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.6.6.1"><span class="ltx_text" id="S5.T3.1.6.6.1.1" style="font-size:90%;">TF-V1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.6.6.2"><span class="ltx_text" id="S5.T3.1.6.6.2.1" style="font-size:90%;">16</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.6.6.3"><span class="ltx_text" id="S5.T3.1.6.6.3.1" style="font-size:90%;">34.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.6.6.4"><span class="ltx_text" id="S5.T3.1.6.6.4.1" style="font-size:90%;">36.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.6.6.5"><span class="ltx_text" id="S5.T3.1.6.6.5.1" style="font-size:90%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.7.7.1"><span class="ltx_text" id="S5.T3.1.7.7.1.1" style="font-size:90%;">PyTorch-V1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.7.7.2"><span class="ltx_text" id="S5.T3.1.7.7.2.1" style="font-size:90%;">16</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.7.3"><span class="ltx_text" id="S5.T3.1.7.7.3.1" style="font-size:90%;">22.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.7.4"><span class="ltx_text" id="S5.T3.1.7.7.4.1" style="font-size:90%;">23.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.7.5"><span class="ltx_text" id="S5.T3.1.7.7.5.1" style="font-size:90%;">0.688</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison of implementations on high-pass-rate dataset.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.T3" title="Table 3 ‣ 5.3.1. High-Pass-Rate ABM Dataset Benchmarking ‣ 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">3</span></a>, we see that on the high-pass-rate dataset, both TF and PyTorch implementations of V1 (exhaustive search) are faster than V2 (explicit pre-filtering). This is likely due to the native slicing and copying operations in TF and PyTorch, which are especially slow for large matrices, as in V2 with high-pass-rate filters. Benchmarking individual operations revealed that the top-K selection in the latest TF version is slower than in PyTorch, while large-matrix slicing is slower in PyTorch than in TF, leading to performance differences between frameworks. V1’s implementation in the high-pass-rate dataset benefits more from the PyTorch implementation with increased batch sizes. Testing the V3 quantized KNN version showed further latency improvements with a trade-off in recall. In this experiment, we used 512-bit quantized embeddings and explored filtering different percentages of items based on quantized embedding similarity before full-precision similarity calculation. The trade-off between latency and recall, correlated with the filter size hyperparameter, is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.F7" title="Figure 7 ‣ 5.3.1. High-Pass-Rate ABM Dataset Benchmarking ‣ 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">7</span></a>. By retaining 1% of items with an additional approximate ranking stage, we achieved around 10% further latency improvement with nearly parity performance.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1" style="font-size:90%;">Batch</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.1.3.1">
<tr class="ltx_tr" id="S5.T4.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1.1.1.1" style="font-size:90%;">Avg. Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.1.4.1">
<tr class="ltx_tr" id="S5.T4.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1.1.1.1" style="font-size:90%;">P95 Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.1"><span class="ltx_text" id="S5.T4.1.2.1.1.1" style="font-size:90%;">TF-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.2"><span class="ltx_text" id="S5.T4.1.2.1.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3"><span class="ltx_text" id="S5.T4.1.2.1.3.1" style="font-size:90%;">3.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.4"><span class="ltx_text" id="S5.T4.1.2.1.4.1" style="font-size:90%;">4.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.2.1"><span class="ltx_text" id="S5.T4.1.3.2.1.1" style="font-size:90%;">PyTorch-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.3.2.2"><span class="ltx_text" id="S5.T4.1.3.2.2.1" style="font-size:90%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.3"><span class="ltx_text" id="S5.T4.1.3.2.3.1" style="font-size:90%;">1.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.4"><span class="ltx_text" id="S5.T4.1.3.2.4.1" style="font-size:90%;">2.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.4.3.1"><span class="ltx_text" id="S5.T4.1.4.3.1.1" style="font-size:90%;">TF-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.4.3.2"><span class="ltx_text" id="S5.T4.1.4.3.2.1" style="font-size:90%;">16</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.3.3"><span class="ltx_text" id="S5.T4.1.4.3.3.1" style="font-size:90%;">14.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.3.4"><span class="ltx_text" id="S5.T4.1.4.3.4.1" style="font-size:90%;">14.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T4.1.5.4.1"><span class="ltx_text" id="S5.T4.1.5.4.1.1" style="font-size:90%;">PyTorch-V2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T4.1.5.4.2"><span class="ltx_text" id="S5.T4.1.5.4.2.1" style="font-size:90%;">16</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.5.4.3"><span class="ltx_text" id="S5.T4.1.5.4.3.1" style="font-size:90%;">21.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.5.4.4"><span class="ltx_text" id="S5.T4.1.5.4.4.1" style="font-size:90%;">21.9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparison of implementations on low-pass-rate dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F7">
<p class="ltx_p ltx_align_center" id="S5.F7.1"><span class="ltx_text" id="S5.F7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.F7.1.1.1" style="width:433.6pt;height:269.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.F7.1.1.1.1"><span class="ltx_text" id="S5.F7.1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S5.F7.1.1.1.1.1.g1" src="x4.png" width="833"/></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span><span class="ltx_text" id="S5.F7.3.1" style="font-size:90%;">Trade-off of latency and recall correlated with the filter size of V3 quantized KNN with ABM on high-pass-rate dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Low-Pass-Rate ABM Dataset Benchmarking</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">As V2 version has the superiority on the low-pass-rate dataset compared to the other two versions (V3 may introduce redundant quantize matrix computation and filtering in the low-pass-rate case), we compare the its performance implemented with TF and PyTorch in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.13218v3#S5.T5" title="Table 5 ‣ 5.3.2. Low-Pass-Rate ABM Dataset Benchmarking ‣ 5.3. Model Inference Benchmarking ‣ 5. Experiments ‣ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn"><span class="ltx_text ltx_ref_tag">5</span></a>. One single query, PyTorch still shows its advantage, but TF performs better on larger batch size. We attribute this to the fact the TF has better parallel schema for our case to conduct the retrieval in parallel. Though the queries are fetched in batch, since each query has different filters leading to different sets and number of retrieved items, we split the query batch and conduct the retrieval in parallel for each query independently. Considering that V2 is an exhaustive KNN search without liquidity issue, no recall drop and results are reported here.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1" style="font-size:90%;">Update Per Sec</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.2.1" style="font-size:90%;">Batch</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.3.1" style="font-size:90%;">QPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.1.1.4.1">
<tr class="ltx_tr" id="S5.T5.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.4.1.1.1.1" style="font-size:90%;">Avg. Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.4.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.1.1.5.1">
<tr class="ltx_tr" id="S5.T5.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.5.1.1.1.1" style="font-size:90%;">P95 Latency</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.5.1.2.1.1" style="font-size:90%;">(ms/batch)</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.1.2.2.1"><span class="ltx_text" id="S5.T5.1.2.2.1.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.2"><span class="ltx_text" id="S5.T5.1.2.2.2.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.3"><span class="ltx_text" id="S5.T5.1.2.2.3.1" style="font-size:90%;">218</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.4"><span class="ltx_text" id="S5.T5.1.2.2.4.1" style="font-size:90%;">4.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.5"><span class="ltx_text" id="S5.T5.1.2.2.5.1" style="font-size:90%;">4.79</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.3.1"><span class="ltx_text" id="S5.T5.1.3.3.1.1" style="font-size:90%;">300</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.3.2"><span class="ltx_text" id="S5.T5.1.3.3.2.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.3.3"><span class="ltx_text" id="S5.T5.1.3.3.3.1" style="font-size:90%;">215</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.3.4"><span class="ltx_text" id="S5.T5.1.3.3.4.1" style="font-size:90%;">4.64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.3.5"><span class="ltx_text" id="S5.T5.1.3.3.5.1" style="font-size:90%;">4.93</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T5.1.4.4.1"><span class="ltx_text" id="S5.T5.1.4.4.1.1" style="font-size:90%;">600</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.2"><span class="ltx_text" id="S5.T5.1.4.4.2.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.3"><span class="ltx_text" id="S5.T5.1.4.4.3.1" style="font-size:90%;">217</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.4"><span class="ltx_text" id="S5.T5.1.4.4.4.1" style="font-size:90%;">4.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.5"><span class="ltx_text" id="S5.T5.1.4.4.5.1" style="font-size:90%;">4.80</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.1.5.5.1"><span class="ltx_text" id="S5.T5.1.5.5.1.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.5.2"><span class="ltx_text" id="S5.T5.1.5.5.2.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.5.3"><span class="ltx_text" id="S5.T5.1.5.5.3.1" style="font-size:90%;">93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.5.4"><span class="ltx_text" id="S5.T5.1.5.5.4.1" style="font-size:90%;">10.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.5.5"><span class="ltx_text" id="S5.T5.1.5.5.5.1" style="font-size:90%;">11.10</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<td class="ltx_td ltx_align_left" id="S5.T5.1.6.6.1"><span class="ltx_text" id="S5.T5.1.6.6.1.1" style="font-size:90%;">300</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.2"><span class="ltx_text" id="S5.T5.1.6.6.2.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.3"><span class="ltx_text" id="S5.T5.1.6.6.3.1" style="font-size:90%;">93</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.4"><span class="ltx_text" id="S5.T5.1.6.6.4.1" style="font-size:90%;">10.70</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.5"><span class="ltx_text" id="S5.T5.1.6.6.5.1" style="font-size:90%;">11.15</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T5.1.7.7.1"><span class="ltx_text" id="S5.T5.1.7.7.1.1" style="font-size:90%;">600</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.7.7.2"><span class="ltx_text" id="S5.T5.1.7.7.2.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.7.7.3"><span class="ltx_text" id="S5.T5.1.7.7.3.1" style="font-size:90%;">93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.7.7.4"><span class="ltx_text" id="S5.T5.1.7.7.4.1" style="font-size:90%;">10.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.7.7.5"><span class="ltx_text" id="S5.T5.1.7.7.5.1" style="font-size:90%;">11.16</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5. </span>Inference latency with concurrent model update.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">We also conduct a stretch testing on single A100 GPU to measure the capacity of the exhaustive search method on handling large amount of items. For plain KNN with ABM (V1 &amp; V2) on the high-pass-rate dataset, we are able to handle upto 240 million embeddings with 128 dim and fp16 precision for top-2k selection with single query. For the quantized KNN on an internal notification use case, which we select top-50million members from 1 billion members (64 dimensional embedding saved as fp16) to send relevant notifications, the 1-bit quantized KNN method with 64 bits quantized embedding size can reduce the original 120GB embedding memory to 7.5GB. When processing single query on an A100 GPU, it achieves maximum 21GB high-bandwidth memory with 97.6ms p95 latency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Impact of Live Model Update on Inference</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">We run a benchmark to measure the impact of live model update on the inference latency on a single A100 GPU using our native serving system and bench marking tool. The bench marking tool uses a client for the native serving service and issues requests serially. We use plain KNN model with ABM (V1) and repeat the runs with various concurrent update rates and request batch sizes. We observe no measurable impact on the latency with increased update rate.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Deployment lessons</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Freshness:</span> We initially deployed <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">LiNR</span> with offline inference and found it missed some fresh candidates. A/B tests revealed that live updates are crucial for serving newly created LinkedIn posts. Enabling live updates resulted in a +6% gain in our production systems, highlighting their importance for improved performance.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_italic" id="S6.p2.1.1">Pre-filtering</span>: Our system employs EBR with pre-filtering, significantly enhancing retrieval quality. Many existing EBR infrastructures use KNN search with post-filtering, where results are first retrieved by KNN distance and then filtered. This post-filtering approach reduces system recall and quality by wasting candidate slots on items that don’t meet attribute constraints. By enabling pre-filtering on GPU retrieval, we greatly improved the quality of results compared to our production FAISS and lucene-based systems.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_italic" id="S6.p3.1.1">Custom filtering kernel</span>: One lesson we learned early is that native TF or PyTorch do not effectively support filtering operations because deep learning frameworks weren’t initially designed for model-based retrieval indexes. Native boolean masking and indexing cause a 100X latency increase, making them impractical for production. Therefore, we implemented a custom CUDA solution for pre-filtering on the GPU, which scans items in memory to find those that meet constraints. One approach is to create a CUDA filtering kernel and fuse it with the matrix multiplication kernel to perform masked-matrix multiplication for KNN with pre-filtering. However, this solution is hard to generalize to other similarity measures or operations, as each new architecture would require re-implementation and fine-tuning, slowing down model development and deployment.
In practice, we could make a trade-off of fully fused kernels and separately implemented kernels. For products needing regular KNN support, fusing the entire kernel with top-k selection and quantization improves serving speed. For general use cases, we create individual custom operations, like pre-filtering and quantization, to allow flexible development and deployment of advanced selection strategy with native neural network operations supported by TF and PyTorch. It is noteworthy that all the results reported in the paper are based on the second solution (even for the three plain KNN version) without extra custom kernel fusion for the purpose of self-consistency and generalizability.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduced <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">LiNR</span>, a state-of-the-art model-based embedding retrieval solution for LinkedIn’s production system. Deploying <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">LiNR</span> to our online systems resulted in significant improvements in Out-Of-Network post recommendations on the LinkedIn Feed. We believe we are among the first in the industry to support live-updated, differentiable model-based indexing for recommendation and search applications. Looking forward, <span class="ltx_text ltx_font_italic" id="S7.p1.1.3">LiNR</span> paves the way for unifying retrieval and ranking into a single GPU model, simplifying complex infrastructure and allowing end-to-end optimization of the entire differentiable system with gradient descent.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Acknowledgements</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The authors would like to thank Jerry Shen, Yuchin Juan, Xiaobing Xue, Souvik Ghosh, Amol Ghoting, Vivek Hariharan, Ping Li, Luke Simon and others who collaborated with us.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borisyuk et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, Kuang-Hsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, and Souvik Ghosh. 2024.

</span>
<span class="ltx_bibblock">LiGNN: Graph Neural Networks at LinkedIn. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haonan Chen, Carlos Lassance, and Jimmy Lin. 2023.

</span>
<span class="ltx_bibblock">End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.18503 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Curtiss et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Michael Curtiss, Iain Becker, Tudor Bosman, Sergey Doroshenko, Lucian Grijincu, Tom Jackson, Sandhya Kunnatur, Soren Lassen, Philip Pronin, Sriram Sankar, Guanghao Shen, Gintaras Woss, Chao Yang, and Ning Zhang. 2013.

</span>
<span class="ltx_bibblock">Unicorn: A System for Searching the Social Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">VLDB</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020.

</span>
<span class="ltx_bibblock">Accelerating Large-Scale Inference with Anisotropic Vector Quantization. ICML.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GV (2008)</span>
<span class="ltx_bibblock">
Félix GV. 2008.

</span>
<span class="ltx_bibblock">Open Sourcing Venice – LinkedIn’s Derived Data Platform.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020.

</span>
<span class="ltx_bibblock">Embedding-Based Retrieval in Facebook Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui Huang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng Sun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.

</span>
<span class="ltx_bibblock">XDL: An Industrial Deep Learning Framework for High-Dimensional Sparse Data. KDD.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.

</span>
<span class="ltx_bibblock">Billion-Scale Similarity Search with GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">IEEE Transactions on Big Data</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jégou et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Herve Jégou, Matthijs Douze, and Cordelia Schmid. 2011.

</span>
<span class="ltx_bibblock">Product Quantization for Nearest Neighbor Search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2011).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Li (2023)</span>
<span class="ltx_bibblock">
Ping Li and Xiaoyun Li. 2023.

</span>
<span class="ltx_bibblock">OPORP: One permutation+ one random projection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2302.03505</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xuezhong Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai Yu, Sen Yang, Ce Zhang, and Ji Liu. 2022.

</span>
<span class="ltx_bibblock">Persia: An Open, Hybrid System Scaling Deep Learning-Based Recommenders up to 100 Trillion Parameters. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui, Xiaoyi Liu, and Fedor Borisyuk. 2021.

</span>
<span class="ltx_bibblock">Que2Search: Fast and Accurate Query and Document Understanding for Search at Facebook. KDD.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022.

</span>
<span class="ltx_bibblock">Monolith: Real Time Recommendation System With Collisionless Embedding Table.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2209.07663 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2020)</span>
<span class="ltx_bibblock">
Yu A. Malkov and D. A. Yashunin. 2020.

</span>
<span class="ltx_bibblock">Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nolet (2023)</span>
<span class="ltx_bibblock">
Corey Nolet. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Reusable Computational Patterns for Machine Learning and Information Retrieval with RAPIDS RAFT</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/" title="">https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ootomo et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hiroyuki Ootomo, Akira Naruse, Corey Nolet, Ray Wang, Tamas Feher, and Yong Wang. 2023.

</span>
<span class="ltx_bibblock">CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.15136 [cs.DS]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020.

</span>
<span class="ltx_bibblock">PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest. KDD.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H. Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, and Maheswaran Sathiamoorthy. 2023.

</span>
<span class="ltx_bibblock">Recommender Systems with Generative Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.05065 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rangadurai et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaushik Rangadurai, Yiqun Liu, Siddarth Malreddy, Xiaoyi Liu, Piyush Maheshwari, Vishwanath Sangale, and Fedor Borisyuk. 2022.

</span>
<span class="ltx_bibblock">NxtPost: User To Post Recommendations In Facebook Groups. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianqiang Shen, Yuchin Juan, Shaobo Zhang, Ping Liu, Wen Pu, Sriram Vasudevan, Qingquan Song, Fedor Borisyuk, Kay Qianqi Shen, Haichao Wei, Yunxiang Ren, Yeou S. Chiou, Sicong Kuang, Yuan Yin, Ben Zheng, Muchen Wu, Shaghayegh Gharghabi, Xiaoqing Wang, Huichao Xue, Qi Guo, Daniel Hewlett, Luke Simon, Liangjie Hong, and Wenjing Zhang. 2024.

</span>
<span class="ltx_bibblock">Learning to Retrieve for Job Matching.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.13435 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.13435" title="">https://arxiv.org/abs/2402.13435</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock">Transformer Memory as a Differentiable Search Index. NEURIPS.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu. 2023.

</span>
<span class="ltx_bibblock">Revisiting Neural Retrieval on Accelerators. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib23.4.2">(KDD ’23)</em>. Association for Computing Machinery, New York, NY, USA, 5520–5531.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3580305.3599897" title="">https://doi.org/10.1145/3580305.3599897</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hailin Zhang, Yujing Wang, Qi Chen, Ruiheng Chang, Ting Zhang, Ziming Miao, Yingyan Hou, Yang Ding, Xupeng Miao, Haonan Wang, Bochen Pang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang, Xing Xie, Mao Yang, and Bin Cui. 2023.

</span>
<span class="ltx_bibblock">Model-enhanced Vector Index.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Weijie Zhao, Shulong Tan, and Ping Li. 2020.

</span>
<span class="ltx_bibblock">SONG: Approximate Nearest Neighbor Search on GPU. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">ICDE</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Weijie Zhao, Shulong Tan, and Ping Li. 2022.

</span>
<span class="ltx_bibblock">Constrained Approximate Similarity Search on Proximity Graph.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2210.14958</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Aug  7 16:57:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
