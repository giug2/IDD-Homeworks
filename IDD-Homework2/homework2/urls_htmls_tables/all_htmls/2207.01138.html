<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.01138] ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges</title><meta property="og:description" content="This paper describes the fourth Affective Behavior Analysis in-the-wild (ABAW) Competition, held in conjunction with European Conference on Computer Vision (ECCV), 2022. The 4th ABAW Competition is a continuation of th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.01138">

<!--Generated on Wed Mar 13 12:39:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="multi-task learning,  learning from synthetic data,  ABAW,  affective behavior analysis in-the-wild,  aff-wild2,  s-aff-wild2,  valence and arousal estimation,  expression recognition,  action unit detection">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dimitrios Kollias
</span><span class="ltx_author_notes">Queen Mary University of London, UK 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>d.kollias@qmul.ac.uk</span></span></span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This paper describes the fourth Affective Behavior Analysis in-the-wild (ABAW) Competition, held in conjunction with European Conference on Computer Vision (ECCV), 2022. The 4th ABAW Competition is a continuation of the Competitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017 Conferences, and aims at automatically analyzing affect. In the previous runs of this Competition, the Challenges targeted Valence-Arousal Estimation, Expression Classification and Action Unit Detection. This year the Competition encompasses two different Challenges: i) a Multi-Task-Learning one in which the goal is to learn at the same time (i.e., in a multi-task learning setting) all the three above mentioned tasks; and ii) a Learning from Synthetic Data one in which the goal is to learn to recognise the basic expressions from artificially generated data and generalise to real data.</p>
<p id="id3.id2" class="ltx_p">The Aff-Wild2 database is a large scale in-the-wild database and the first one that contains annotations for valence and arousal, expressions and action units. This database is the basis for the above Challenges. In more detail: i) s-Aff-Wild2 -a static version of Aff-Wild2 database- has been constructed and utilized for the purposes of the Multi-Task-Learning Challenge; and ii) some specific frames-images from the Aff-Wild2 database have been used in an expression manipulation manner for creating the synthetic dataset, which is the basis for the Learning from Synthetic Data Challenge.
In this paper, at first we present the two Challenges, along with the utilized corpora, then we outline the evaluation metrics and finally present the baseline systems per Challenge, as well as their derived results. More information regarding the Competition can be found in the competition’s website: <a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>multi-task learning, learning from synthetic data, ABAW, affective behavior analysis in-the-wild, aff-wild2, s-aff-wild2, valence and arousal estimation, expression recognition, action unit detection
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic facial behavior analysis has a long history of studies in the intersection of computer vision, physiology and psychology and has applications spread across a variety of fields, such as medicine, health, or driver fatigue, monitoring, e-learning, marketing, entertainment, lie detection and law. However it is only recently, with the collection of large-scale datasets and powerful machine learning methods such as deep neural networks, that automatic facial behavior analysis started to thrive.
When it comes to automatically recognising affect in-the-wild (i.e., in uncontrolled conditions and unconstrained environments), there exist three iconic tasks, which are: i) recognition of basic expressions
(anger, disgust, fear, happiness, sadness, surprise and the neutral state); ii) estimation of continuous affect (valence -how positive/negative a person is- and arousal -how active/passive a person is-); iii) detection of facial action units (coding of facial motion with respect to activation of facial muscles, e.g. upper/inner eyebrows, nose wrinkles).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Ekman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> defined the six basic emotions, i.e., Anger, Disgust, Fear, Happiness, Sadness, Surprise and the Neutral
State, based on a cross-culture study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which indicated that humans perceive certain basic emotions in the same way regardless of culture. Nevertheless, advanced research on neuroscience and psychology argued that the model of six basic emotions are culture-specific and not universal. Additionally, the affect model based on basic emotions is limited in the ability to represent the complexity and subtlety of our daily affective displays. Despite these findings, the categorical model that describes emotions in terms of discrete basic emotions is still the most popular perspective for Expression Recognition, due to its pioneering investigations along with the direct and intuitive definition of facial expressions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The dimensional model of affect, that is appropriate to represent not only extreme, but also subtle emotions
appearing in everyday human-computer interactions, has also attracted significant attention over the last years.
According to the dimensional approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, affective behavior is described by a number of latent continuous
dimensions. The most commonly used dimensions include valence (indicating how positive or negative an emotional
state is) and arousal (measuring the power of emotion activation).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Detection of Facial Action Units (AUs) has also attained large attention. The Facial Action Coding System
(FACS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> provides a standardised taxonomy of facial muscles’ movements and has been widely adopted as a common standard towards systematically categorising physical manifestation of complex facial expressions. Since any facial expression can be represented as a combination of action units, they constitute a natural physiological basis for face analysis. Consequently, in the last
years, there has been a shift of related research towards the detection of action units. The presence of action units
is typically brief and unconscious, and their detection requires analyzing subtle appearance changes in the human
face. Furthermore, action units do not appear in isolation, but as elemental units of facial expressions, and hence
some AUs co-occur frequently, while others are mutually exclusive.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The fourth Affective Behavior Analysis
in-the-wild (ABAW) Competition, held in conjunction with the European
Conference on Computer Vision (ECCV), 2022, is a continuation of the first <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, second <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and third <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/cvpr-2022-3rd-abaw/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ibug.doc.ic.ac.uk/resources/cvpr-2022-3rd-abaw/</a></span></span></span> ABAW Competitions held in conjunction with the IEEE Conference on Face and Gesture Recognition (IEEE FG) 2021, with the International Conference on Computer Vision (ICCV) 2022 and the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2022, respectively. The previous Competitions targeted dimensional (in terms of valence and arousal) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, categorical (in terms of the basic expressions) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and facial action unit analysis and recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. The third ABAW Challenge further targeted Multi-Task Learning for valence and arousal estimation, expression recognition and action unit detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The fourth ABAW Competition contains two Challenges (i) the Multi-Task-Learning (MTL) one in which the goal is to create a system that learns at the same time (i.e., in a multi-task learning
setting) to estimate valence and arousal, classify eight expressions (6 basic expressions plus the neutral state plus a category ’other’ which denotes expressions/affective states other than the 6 basic ones) and detect twelve action units; ii) the Learning from Synthetic Data (LSD) one in which the goal is to create a system that learns to recognise the six basic expressions (anger, disgust, fear, happiness, sadness, surprise) from artificially generated data (i.e., synthetic data) and generalise its knowledge to real-world (i.e., real) data.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Both Challenges’ corpora are based on the Aff-Wild2 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which is the first comprehensive in-the-wild benchmark for all the three above-mentioned affect recognition tasks; the Aff-Wild2 database is an extensions of the Aff-Wild database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, with more videos and annotations for all behavior tasks. The MTL Challenge utilises a a static version of the Aff-Wild2 database, named s-Aff-Wild2. The LSD Challenge utilizes a synthetic dataset which has been constructed after manipulating the displayed expressions in some frames of the Aff-Wild2 database.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of this paper is organised as follows. The Competition corpora is introduced in Section <a href="#S2" title="2 Competition Corpora ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the Competition evaluation metrics are mentioned and described in Section <a href="#S3" title="3 Evaluation Metrics for each Challenge ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the developed baselines in each Challenge are explained and their obtained results are presented in Section <a href="#S4" title="4 Baseline Networks and Performance ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, before concluding in Section <a href="#S5" title="5 Conclusion ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Competition Corpora</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The fourth Affective Behavior Analysis in-the-wild (ABAW) Competition relies on the Aff-Wild2 database, which is the first ever database annotated in terms of the tasks of: valence-arousal estimation, action unit detection and expression recognition. These three tasks constitute the basis of the two Challenges.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the following, we provide a short overview of each Challenge’s dataset along with a description of the pre-processing steps that we carried out for cropping and/or aligning the images of Aff-Wild2. These images have been utilized in our baseline experiments.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multi-Task Learning Challenge</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A static version of the Aff-Wild2 database has been generated by selecting some specific frames of the database; this Challenge’s corpora is named s-Aff-Wild2.
In total, 221,928 images are used that contain annotations in terms of: i) valence and arousal; ii) 6 basic expressions (anger, disgust, fear, happiness, sadness, surprise), plus the neutral state, plus the ’other’ category (which denotes expressions/affective states other than the 6 basic ones); 12 action units.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Multi-Task Learning Challenge ‣ 2 Competition Corpora ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the 2D Valence-Arousal histogram of annotations of s-Aff-Wild2.
Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Multi-Task Learning Challenge ‣ 2 Competition Corpora ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the distribution of the 8 expression annotations of s-Aff-Wild2.
Table <a href="#S2.T2" title="Table 2 ‣ 2.1 Multi-Task Learning Challenge ‣ 2 Competition Corpora ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the name of the 12 action units that have been annotated, the action that they correspond to and the distribution of their annotations in s-Aff-Wild2.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The s-Aff-Wild2 database is split into training, validation and test sets. At first the training and validation sets, along with their corresponding annotations, are being made public to the participants, so that they can develop their own methodologies and test them. At a later stage, the test set without annotations is given to the participants.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The participants are given two versions of s-Aff-Wild2: the cropped and cropped-aligned ones. At first, all images/frames of s-Aff-Wild2 are passed through the RetinaFace detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> so as to extract, for each image/frame, face bounding boxes and 5 facial landmarks. The images/frames are then cropped according the bounding box locations. All cropped-aligned images have the same dimensions <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="112\times 112\times 3" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mrow id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mn id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p4.1.m1.1.1.1" xref="S2.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p4.1.m1.1.1.1a" xref="S2.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p4.1.m1.1.1.4" xref="S2.SS1.p4.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><times id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2">112</cn><cn type="integer" id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3">112</cn><cn type="integer" id="S2.SS1.p4.1.m1.1.1.4.cmml" xref="S2.SS1.p4.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">112\times 112\times 3</annotation></semantics></math>. These cropped images/frames constitute the cropped version of s-Aff-Wild2 that is given to the participants.
The 5 facial landmarks (two eyes, nose and two mouth corners) have then been used to perform similarity transformation. The resulting cropped-aligned images/frames constitute the cropped-aligned version of s-Aff-Wild2 that is given to the participants. The cropped-aligned version has been utilized in our baseline experiments, described in Section <a href="#S4" title="4 Baseline Networks and Performance ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2207.01138/assets/hist_va_mtl.jpg" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="340" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Multi-Task-Learning Challenge: 2D Valence-Arousal Histogram of Annotations in s-Aff-Wild2</figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Multi-Task-Learning Challenge: Number of Annotated Images for each of the 8 Expressions</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Expressions</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">No of Images</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Neutral</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">37,073</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Anger</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8,094</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Disgust</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,922</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Fear</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,899</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Happiness</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32,397</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<th id="S2.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Sadness</th>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13,447</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<th id="S2.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Surprise</th>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9,873</td>
</tr>
<tr id="S2.T1.1.9.8" class="ltx_tr">
<th id="S2.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Other</th>
<td id="S2.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">39,701</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Multi-Task-Learning Challenge: : Distribution of AU Annotations in Aff-Wild2</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Action Unit #</th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Action</th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T2.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1.3.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Total Number</td>
</tr>
<tr id="S2.T2.1.1.1.3.1.2" class="ltx_tr">
<td id="S2.T2.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">of Activated AUs</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<td id="S2.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">AU 1</td>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">inner brow raiser</td>
<td id="S2.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">29,995</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<td id="S2.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 2</td>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">outer brow raiser</td>
<td id="S2.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14,183</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<td id="S2.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 4</td>
<td id="S2.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">brow lowerer</td>
<td id="S2.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31,926</td>
</tr>
<tr id="S2.T2.1.5.4" class="ltx_tr">
<td id="S2.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 6</td>
<td id="S2.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">cheek raiser</td>
<td id="S2.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49,413</td>
</tr>
<tr id="S2.T2.1.6.5" class="ltx_tr">
<td id="S2.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 7</td>
<td id="S2.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lid tightener</td>
<td id="S2.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72,806</td>
</tr>
<tr id="S2.T2.1.7.6" class="ltx_tr">
<td id="S2.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 10</td>
<td id="S2.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">upper lip raiser</td>
<td id="S2.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68,090</td>
</tr>
<tr id="S2.T2.1.8.7" class="ltx_tr">
<td id="S2.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 12</td>
<td id="S2.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lip corner puller</td>
<td id="S2.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47,820</td>
</tr>
<tr id="S2.T2.1.9.8" class="ltx_tr">
<td id="S2.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 15</td>
<td id="S2.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lip corner depressor</td>
<td id="S2.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,105</td>
</tr>
<tr id="S2.T2.1.10.9" class="ltx_tr">
<td id="S2.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 23</td>
<td id="S2.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lip tightener</td>
<td id="S2.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,538</td>
</tr>
<tr id="S2.T2.1.11.10" class="ltx_tr">
<td id="S2.T2.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 24</td>
<td id="S2.T2.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lip pressor</td>
<td id="S2.T2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8,052</td>
</tr>
<tr id="S2.T2.1.12.11" class="ltx_tr">
<td id="S2.T2.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AU 25</td>
<td id="S2.T2.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">lips part</td>
<td id="S2.T2.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">122,518</td>
</tr>
<tr id="S2.T2.1.13.12" class="ltx_tr">
<td id="S2.T2.1.13.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">AU 26</td>
<td id="S2.T2.1.13.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">jaw drop</td>
<td id="S2.T2.1.13.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">19,439</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Let us note that for the purposes of this Challenge, all participants are allowed to use the provided s-Aff-Wild2 database and/or any publicly available or private database; the participants are not allowed to use the audiovisual (A/V) Aff-Wild2 database (images and annotations).
Any methodological solution will be accepted for this Challenge.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Learning from Synthetic Data Challenge</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Some specific cropped images/frames of the Aff-Wild2 database have been selected; these images/frames, which show a face with an arbitrary expression/affective state, have been used -in a facial expression manipulation manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>- so as to synthesize basic facial expressions of the same person. Therefore a synthetic facial dataset has been generated and used for the purposes of this Challenge.
In total, 277,251 images that contain annotations in terms of the 6 basic expressions (anger, disgust, fear, happiness, sadness, surprise) have been generated. These images constitute the training set of this Challenge.
Table <a href="#S2.T3" title="Table 3 ‣ 2.2 Learning from Synthetic Data Challenge ‣ 2 Competition Corpora ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the distribution of the 6 basic expression annotations of these generated images.
The validation and test sets of this Challenge are real images of the Aff-Wild2 database.
Let us note that the synthetic data have been generated from subjects of the validation set, but not of the test set.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">At first the training (synthetic data) and validation (real data) sets, along with their corresponding annotations, are being made public to the participants, so that they can develop their own methodologies and test them. At a later stage, the test set (real data) without annotations is given to the participants.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Let us note that for the purposes of this Challenge, all participants are allowed to use any -publicly or not- available pre-trained model (as long as it has not been pre-trained on Aff-Wild2). The pre-trained model can be pre-trained on any task (eg VA estimation, Expression Classification, AU detection, Face Recognition). However when the teams are refining the model and developing the methodology they must only use the provided synthetic data. No real data should be used in model training/methodology development.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Learning from Synthetic Data Challenge: Number of Annotated Images for each of the 6 basic Expressions</figcaption>
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.1.1.1" class="ltx_tr">
<th id="S2.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Expressions</th>
<th id="S2.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">No of Images</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.1.2.1" class="ltx_tr">
<th id="S2.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Anger</th>
<td id="S2.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">18,286</td>
</tr>
<tr id="S2.T3.1.3.2" class="ltx_tr">
<th id="S2.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Disgust</th>
<td id="S2.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15,150</td>
</tr>
<tr id="S2.T3.1.4.3" class="ltx_tr">
<th id="S2.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Fear</th>
<td id="S2.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10,923</td>
</tr>
<tr id="S2.T3.1.5.4" class="ltx_tr">
<th id="S2.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Happiness</th>
<td id="S2.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73,285</td>
</tr>
<tr id="S2.T3.1.6.5" class="ltx_tr">
<th id="S2.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Sadness</th>
<td id="S2.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">144,631</td>
</tr>
<tr id="S2.T3.1.7.6" class="ltx_tr">
<th id="S2.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Surprise</th>
<td id="S2.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">14,976</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Metrics for each Challenge</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Next, we present the metrics that will be used for assessing the performance of the developed methodologies of the participating teams in each Challenge.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-Task Learning Challenge</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The performance measure is the sum of: the average between the Concordance Correlation Coefficient (CCC) of valence and arousal; the average F1 Score of the 8 expression categories (i.e., macro F1 Score); the average F1 Score of the 12 action units (i.e., macro F1 Score).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">CCC takes values in the range <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="[-1,1]" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.2.1" xref="S3.SS1.p2.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.2.2.1.2" xref="S3.SS1.p2.1.m1.2.2.2.cmml">[</mo><mrow id="S3.SS1.p2.1.m1.2.2.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml"><mo id="S3.SS1.p2.1.m1.2.2.1.1a" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml">−</mo><mn id="S3.SS1.p2.1.m1.2.2.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p2.1.m1.2.2.1.3" xref="S3.SS1.p2.1.m1.2.2.2.cmml">,</mo><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S3.SS1.p2.1.m1.2.2.1.4" xref="S3.SS1.p2.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><interval closure="closed" id="S3.SS1.p2.1.m1.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1"><apply id="S3.SS1.p2.1.m1.2.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1"><minus id="S3.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1"></minus><cn type="integer" id="S3.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.2">1</cn></apply><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">[-1,1]</annotation></semantics></math>; high values are desired. CCC is defined as follows:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\rho_{c}=\frac{2s_{xy}}{s_{x}^{2}+s_{y}^{2}+(\bar{x}-\bar{y})^{2}}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml">ρ</mi><mi id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">c</mi></msub><mo id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">s</mi><mrow id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml">y</mi></mrow></msub></mrow><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.3.2.2.cmml">s</mi><mi id="S3.E1.m1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.3.2.3.cmml">x</mi><mn id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">2</mn></msubsup><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">+</mo><msubsup id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.4.2.2.cmml">s</mi><mi id="S3.E1.m1.1.1.1.4.2.3" xref="S3.E1.m1.1.1.1.4.2.3.cmml">y</mi><mn id="S3.E1.m1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.4.3.cmml">2</mn></msubsup><mo id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">+</mo><msup id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml">¯</mo></mover><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mfrac></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"></eq><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2">𝜌</ci><ci id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3">𝑐</ci></apply><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1"></divide><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">2</cn><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝑠</ci><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">𝑥</ci><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">𝑦</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><plus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></plus><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.3.2.2">𝑠</ci><ci id="S3.E1.m1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.3.2.3">𝑥</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.4">superscript</csymbol><apply id="S3.E1.m1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.4.2.2">𝑠</ci><ci id="S3.E1.m1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.1.4.2.3">𝑦</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.4.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1">¯</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">𝑥</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1">¯</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">𝑦</ci></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\rho_{c}=\frac{2s_{xy}}{s_{x}^{2}+s_{y}^{2}+(\bar{x}-\bar{y})^{2}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.5" class="ltx_p">where <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="s_{x}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">s</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑠</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">s_{x}</annotation></semantics></math> and <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="s_{y}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">s</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑠</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">s_{y}</annotation></semantics></math> are the variances of all video valence/arousal annotations and predicted values, respectively, <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\bar{x}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mover accent="true" id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">x</mi><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><ci id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1">¯</ci><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\bar{x}</annotation></semantics></math> and <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\bar{y}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mover accent="true" id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">y</mi><mo id="S3.SS1.p4.4.m4.1.1.1" xref="S3.SS1.p4.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><ci id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1">¯</ci><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\bar{y}</annotation></semantics></math> are their corresponding mean values and <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="s_{xy}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">s</mi><mrow id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml"><mi id="S3.SS1.p4.5.m5.1.1.3.2" xref="S3.SS1.p4.5.m5.1.1.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.5.m5.1.1.3.1" xref="S3.SS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p4.5.m5.1.1.3.3" xref="S3.SS1.p4.5.m5.1.1.3.3.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝑠</ci><apply id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3"><times id="S3.SS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.p4.5.m5.1.1.3.1"></times><ci id="S3.SS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.p4.5.m5.1.1.3.2">𝑥</ci><ci id="S3.SS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">s_{xy}</annotation></semantics></math> is the corresponding covariance value.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.4" class="ltx_p">The <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><msub id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">F</mi><mn id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">F_{1}</annotation></semantics></math> score is a weighted average of the recall (i.e., the ability of the classifier to find all the positive samples) and precision (i.e., the ability of the classifier not to label as positive a sample that is negative). The <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><msub id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">F</mi><mn id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">F_{1}</annotation></semantics></math> score takes values in the range <math id="S3.SS1.p5.3.m3.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS1.p5.3.m3.2a"><mrow id="S3.SS1.p5.3.m3.2.3.2" xref="S3.SS1.p5.3.m3.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.3.m3.2.3.2.1" xref="S3.SS1.p5.3.m3.2.3.1.cmml">[</mo><mn id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">0</mn><mo id="S3.SS1.p5.3.m3.2.3.2.2" xref="S3.SS1.p5.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.3.m3.2.2" xref="S3.SS1.p5.3.m3.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS1.p5.3.m3.2.3.2.3" xref="S3.SS1.p5.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.2b"><interval closure="closed" id="S3.SS1.p5.3.m3.2.3.1.cmml" xref="S3.SS1.p5.3.m3.2.3.2"><cn type="integer" id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">0</cn><cn type="integer" id="S3.SS1.p5.3.m3.2.2.cmml" xref="S3.SS1.p5.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.2c">[0,1]</annotation></semantics></math>; high values are desired. The <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><msub id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">F</mi><mn id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">𝐹</ci><cn type="integer" id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">F_{1}</annotation></semantics></math> score is defined as:</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="F_{1}=\frac{2\times precision\times recall}{precision+recall}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">F</mi><mn id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mfrac id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mrow id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml"><mrow id="S3.E2.m1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.cmml"><mrow id="S3.E2.m1.1.1.3.2.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.2.cmml"><mn id="S3.E2.m1.1.1.3.2.2.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.3.2.2.2.2.1" xref="S3.E2.m1.1.1.3.2.2.2.2.1.cmml">×</mo><mi id="S3.E2.m1.1.1.3.2.2.2.2.3" xref="S3.E2.m1.1.1.3.2.2.2.2.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1a" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.4" xref="S3.E2.m1.1.1.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1b" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.5" xref="S3.E2.m1.1.1.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1c" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.6" xref="S3.E2.m1.1.1.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1d" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.7" xref="S3.E2.m1.1.1.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1e" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.8" xref="S3.E2.m1.1.1.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1f" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.9" xref="S3.E2.m1.1.1.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.2.1g" xref="S3.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.2.2.10" xref="S3.E2.m1.1.1.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.3.2.2.1" xref="S3.E2.m1.1.1.3.2.2.1.cmml">×</mo><mi id="S3.E2.m1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.3.2.2.3.cmml">r</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1a" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.4" xref="S3.E2.m1.1.1.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1b" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.5" xref="S3.E2.m1.1.1.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1c" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.6" xref="S3.E2.m1.1.1.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1d" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.2.7" xref="S3.E2.m1.1.1.3.2.7.cmml">l</mi></mrow><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mrow id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1a" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.4" xref="S3.E2.m1.1.1.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1b" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.5" xref="S3.E2.m1.1.1.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1c" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.6" xref="S3.E2.m1.1.1.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1d" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.7" xref="S3.E2.m1.1.1.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1e" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.8" xref="S3.E2.m1.1.1.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1f" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.9" xref="S3.E2.m1.1.1.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.2.1g" xref="S3.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.2.10" xref="S3.E2.m1.1.1.3.3.2.10.cmml">n</mi></mrow><mo id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.1" xref="S3.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.1a" xref="S3.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3.4" xref="S3.E2.m1.1.1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.1b" xref="S3.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3.5" xref="S3.E2.m1.1.1.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.1c" xref="S3.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3.6" xref="S3.E2.m1.1.1.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.1d" xref="S3.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3.7" xref="S3.E2.m1.1.1.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">𝐹</ci><cn type="integer" id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">1</cn></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><divide id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3"></divide><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><times id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.1"></times><apply id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2"><times id="S3.E2.m1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.1"></times><apply id="S3.E2.m1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2"><times id="S3.E2.m1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.2.1"></times><apply id="S3.E2.m1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2"><times id="S3.E2.m1.1.1.3.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2.1"></times><cn type="integer" id="S3.E2.m1.1.1.3.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2.2">2</cn><ci id="S3.E2.m1.1.1.3.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2.3">𝑝</ci></apply><ci id="S3.E2.m1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.2.3">𝑟</ci><ci id="S3.E2.m1.1.1.3.2.2.2.4.cmml" xref="S3.E2.m1.1.1.3.2.2.2.4">𝑒</ci><ci id="S3.E2.m1.1.1.3.2.2.2.5.cmml" xref="S3.E2.m1.1.1.3.2.2.2.5">𝑐</ci><ci id="S3.E2.m1.1.1.3.2.2.2.6.cmml" xref="S3.E2.m1.1.1.3.2.2.2.6">𝑖</ci><ci id="S3.E2.m1.1.1.3.2.2.2.7.cmml" xref="S3.E2.m1.1.1.3.2.2.2.7">𝑠</ci><ci id="S3.E2.m1.1.1.3.2.2.2.8.cmml" xref="S3.E2.m1.1.1.3.2.2.2.8">𝑖</ci><ci id="S3.E2.m1.1.1.3.2.2.2.9.cmml" xref="S3.E2.m1.1.1.3.2.2.2.9">𝑜</ci><ci id="S3.E2.m1.1.1.3.2.2.2.10.cmml" xref="S3.E2.m1.1.1.3.2.2.2.10">𝑛</ci></apply><ci id="S3.E2.m1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.3">𝑟</ci></apply><ci id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3">𝑒</ci><ci id="S3.E2.m1.1.1.3.2.4.cmml" xref="S3.E2.m1.1.1.3.2.4">𝑐</ci><ci id="S3.E2.m1.1.1.3.2.5.cmml" xref="S3.E2.m1.1.1.3.2.5">𝑎</ci><ci id="S3.E2.m1.1.1.3.2.6.cmml" xref="S3.E2.m1.1.1.3.2.6">𝑙</ci><ci id="S3.E2.m1.1.1.3.2.7.cmml" xref="S3.E2.m1.1.1.3.2.7">𝑙</ci></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><plus id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></plus><apply id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2"><times id="S3.E2.m1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.3.3.2.1"></times><ci id="S3.E2.m1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2">𝑝</ci><ci id="S3.E2.m1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.3.3.2.3">𝑟</ci><ci id="S3.E2.m1.1.1.3.3.2.4.cmml" xref="S3.E2.m1.1.1.3.3.2.4">𝑒</ci><ci id="S3.E2.m1.1.1.3.3.2.5.cmml" xref="S3.E2.m1.1.1.3.3.2.5">𝑐</ci><ci id="S3.E2.m1.1.1.3.3.2.6.cmml" xref="S3.E2.m1.1.1.3.3.2.6">𝑖</ci><ci id="S3.E2.m1.1.1.3.3.2.7.cmml" xref="S3.E2.m1.1.1.3.3.2.7">𝑠</ci><ci id="S3.E2.m1.1.1.3.3.2.8.cmml" xref="S3.E2.m1.1.1.3.3.2.8">𝑖</ci><ci id="S3.E2.m1.1.1.3.3.2.9.cmml" xref="S3.E2.m1.1.1.3.3.2.9">𝑜</ci><ci id="S3.E2.m1.1.1.3.3.2.10.cmml" xref="S3.E2.m1.1.1.3.3.2.10">𝑛</ci></apply><apply id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"><times id="S3.E2.m1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.2">𝑟</ci><ci id="S3.E2.m1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3">𝑒</ci><ci id="S3.E2.m1.1.1.3.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.3.4">𝑐</ci><ci id="S3.E2.m1.1.1.3.3.3.5.cmml" xref="S3.E2.m1.1.1.3.3.3.5">𝑎</ci><ci id="S3.E2.m1.1.1.3.3.3.6.cmml" xref="S3.E2.m1.1.1.3.3.3.6">𝑙</ci><ci id="S3.E2.m1.1.1.3.3.3.7.cmml" xref="S3.E2.m1.1.1.3.3.3.7">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">F_{1}=\frac{2\times precision\times recall}{precision+recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Therefore, the evaluation criterion for the Multi-Task-Learning Challenge is:</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{P}_{MTL}" display="inline"><semantics id="S3.Ex1.m1.1a"><msub id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">𝒫</mi><mrow id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.3.1" xref="S3.Ex1.m1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.3.1a" xref="S3.Ex1.m1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.1.1.3.4" xref="S3.Ex1.m1.1.1.3.4.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2">𝒫</ci><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><times id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3.1"></times><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">𝑀</ci><ci id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3">𝑇</ci><ci id="S3.Ex1.m1.1.1.3.4.cmml" xref="S3.Ex1.m1.1.1.3.4">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle\mathcal{P}_{MTL}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle=\mathcal{P}_{VA}+\mathcal{P}_{EXPR}+\mathcal{P}_{AU}" display="inline"><semantics id="S3.Ex1.m2.1a"><mrow id="S3.Ex1.m2.1.1" xref="S3.Ex1.m2.1.1.cmml"><mi id="S3.Ex1.m2.1.1.2" xref="S3.Ex1.m2.1.1.2.cmml"></mi><mo id="S3.Ex1.m2.1.1.1" xref="S3.Ex1.m2.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m2.1.1.3" xref="S3.Ex1.m2.1.1.3.cmml"><msub id="S3.Ex1.m2.1.1.3.2" xref="S3.Ex1.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.3.2.2" xref="S3.Ex1.m2.1.1.3.2.2.cmml">𝒫</mi><mrow id="S3.Ex1.m2.1.1.3.2.3" xref="S3.Ex1.m2.1.1.3.2.3.cmml"><mi id="S3.Ex1.m2.1.1.3.2.3.2" xref="S3.Ex1.m2.1.1.3.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.3.2.3.1" xref="S3.Ex1.m2.1.1.3.2.3.1.cmml">​</mo><mi id="S3.Ex1.m2.1.1.3.2.3.3" xref="S3.Ex1.m2.1.1.3.2.3.3.cmml">A</mi></mrow></msub><mo id="S3.Ex1.m2.1.1.3.1" xref="S3.Ex1.m2.1.1.3.1.cmml">+</mo><msub id="S3.Ex1.m2.1.1.3.3" xref="S3.Ex1.m2.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.3.3.2" xref="S3.Ex1.m2.1.1.3.3.2.cmml">𝒫</mi><mrow id="S3.Ex1.m2.1.1.3.3.3" xref="S3.Ex1.m2.1.1.3.3.3.cmml"><mi id="S3.Ex1.m2.1.1.3.3.3.2" xref="S3.Ex1.m2.1.1.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.3.3.3.1" xref="S3.Ex1.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.Ex1.m2.1.1.3.3.3.3" xref="S3.Ex1.m2.1.1.3.3.3.3.cmml">X</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.3.3.3.1a" xref="S3.Ex1.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.Ex1.m2.1.1.3.3.3.4" xref="S3.Ex1.m2.1.1.3.3.3.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.3.3.3.1b" xref="S3.Ex1.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.Ex1.m2.1.1.3.3.3.5" xref="S3.Ex1.m2.1.1.3.3.3.5.cmml">R</mi></mrow></msub><mo id="S3.Ex1.m2.1.1.3.1a" xref="S3.Ex1.m2.1.1.3.1.cmml">+</mo><msub id="S3.Ex1.m2.1.1.3.4" xref="S3.Ex1.m2.1.1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.3.4.2" xref="S3.Ex1.m2.1.1.3.4.2.cmml">𝒫</mi><mrow id="S3.Ex1.m2.1.1.3.4.3" xref="S3.Ex1.m2.1.1.3.4.3.cmml"><mi id="S3.Ex1.m2.1.1.3.4.3.2" xref="S3.Ex1.m2.1.1.3.4.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.3.4.3.1" xref="S3.Ex1.m2.1.1.3.4.3.1.cmml">​</mo><mi id="S3.Ex1.m2.1.1.3.4.3.3" xref="S3.Ex1.m2.1.1.3.4.3.3.cmml">U</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.1b"><apply id="S3.Ex1.m2.1.1.cmml" xref="S3.Ex1.m2.1.1"><eq id="S3.Ex1.m2.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.Ex1.m2.1.1.2.cmml" xref="S3.Ex1.m2.1.1.2">absent</csymbol><apply id="S3.Ex1.m2.1.1.3.cmml" xref="S3.Ex1.m2.1.1.3"><plus id="S3.Ex1.m2.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.3.1"></plus><apply id="S3.Ex1.m2.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.3.2.1.cmml" xref="S3.Ex1.m2.1.1.3.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.3.2.2.cmml" xref="S3.Ex1.m2.1.1.3.2.2">𝒫</ci><apply id="S3.Ex1.m2.1.1.3.2.3.cmml" xref="S3.Ex1.m2.1.1.3.2.3"><times id="S3.Ex1.m2.1.1.3.2.3.1.cmml" xref="S3.Ex1.m2.1.1.3.2.3.1"></times><ci id="S3.Ex1.m2.1.1.3.2.3.2.cmml" xref="S3.Ex1.m2.1.1.3.2.3.2">𝑉</ci><ci id="S3.Ex1.m2.1.1.3.2.3.3.cmml" xref="S3.Ex1.m2.1.1.3.2.3.3">𝐴</ci></apply></apply><apply id="S3.Ex1.m2.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.3.3.1.cmml" xref="S3.Ex1.m2.1.1.3.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.3.3.2.cmml" xref="S3.Ex1.m2.1.1.3.3.2">𝒫</ci><apply id="S3.Ex1.m2.1.1.3.3.3.cmml" xref="S3.Ex1.m2.1.1.3.3.3"><times id="S3.Ex1.m2.1.1.3.3.3.1.cmml" xref="S3.Ex1.m2.1.1.3.3.3.1"></times><ci id="S3.Ex1.m2.1.1.3.3.3.2.cmml" xref="S3.Ex1.m2.1.1.3.3.3.2">𝐸</ci><ci id="S3.Ex1.m2.1.1.3.3.3.3.cmml" xref="S3.Ex1.m2.1.1.3.3.3.3">𝑋</ci><ci id="S3.Ex1.m2.1.1.3.3.3.4.cmml" xref="S3.Ex1.m2.1.1.3.3.3.4">𝑃</ci><ci id="S3.Ex1.m2.1.1.3.3.3.5.cmml" xref="S3.Ex1.m2.1.1.3.3.3.5">𝑅</ci></apply></apply><apply id="S3.Ex1.m2.1.1.3.4.cmml" xref="S3.Ex1.m2.1.1.3.4"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.3.4.1.cmml" xref="S3.Ex1.m2.1.1.3.4">subscript</csymbol><ci id="S3.Ex1.m2.1.1.3.4.2.cmml" xref="S3.Ex1.m2.1.1.3.4.2">𝒫</ci><apply id="S3.Ex1.m2.1.1.3.4.3.cmml" xref="S3.Ex1.m2.1.1.3.4.3"><times id="S3.Ex1.m2.1.1.3.4.3.1.cmml" xref="S3.Ex1.m2.1.1.3.4.3.1"></times><ci id="S3.Ex1.m2.1.1.3.4.3.2.cmml" xref="S3.Ex1.m2.1.1.3.4.3.2">𝐴</ci><ci id="S3.Ex1.m2.1.1.3.4.3.3.cmml" xref="S3.Ex1.m2.1.1.3.4.3.3">𝑈</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.1c">\displaystyle=\mathcal{P}_{VA}+\mathcal{P}_{EXPR}+\mathcal{P}_{AU}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle=\frac{\rho_{a}+\rho_{v}}{2}+\frac{\sum_{expr}F_{1}^{expr}}{8}+\frac{\sum_{au}F_{1}^{au}}{12}" display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"></mi><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mfrac id="S3.E3.m1.1.1.3.2a" xref="S3.E3.m1.1.1.3.2.cmml"><mrow id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml"><msub id="S3.E3.m1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.2.cmml">ρ</mi><mi id="S3.E3.m1.1.1.3.2.2.2.3" xref="S3.E3.m1.1.1.3.2.2.2.3.cmml">a</mi></msub><mo id="S3.E3.m1.1.1.3.2.2.1" xref="S3.E3.m1.1.1.3.2.2.1.cmml">+</mo><msub id="S3.E3.m1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.3.2.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.2.3.2" xref="S3.E3.m1.1.1.3.2.2.3.2.cmml">ρ</mi><mi id="S3.E3.m1.1.1.3.2.2.3.3" xref="S3.E3.m1.1.1.3.2.2.3.3.cmml">v</mi></msub></mrow><mn id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml">2</mn></mfrac></mstyle><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mstyle displaystyle="true" id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mfrac id="S3.E3.m1.1.1.3.3a" xref="S3.E3.m1.1.1.3.3.cmml"><mrow id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml"><msub id="S3.E3.m1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.3.3.2.1.cmml"><mo id="S3.E3.m1.1.1.3.3.2.1.2" xref="S3.E3.m1.1.1.3.3.2.1.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.3.3.2.1.3" xref="S3.E3.m1.1.1.3.3.2.1.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.1.3.2" xref="S3.E3.m1.1.1.3.3.2.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1.3.1" xref="S3.E3.m1.1.1.3.3.2.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.1.3.3" xref="S3.E3.m1.1.1.3.3.2.1.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1.3.1a" xref="S3.E3.m1.1.1.3.3.2.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.1.3.4" xref="S3.E3.m1.1.1.3.3.2.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1.3.1b" xref="S3.E3.m1.1.1.3.3.2.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.1.3.5" xref="S3.E3.m1.1.1.3.3.2.1.3.5.cmml">r</mi></mrow></msub><msubsup id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2.2.2" xref="S3.E3.m1.1.1.3.3.2.2.2.2.cmml">F</mi><mn id="S3.E3.m1.1.1.3.3.2.2.2.3" xref="S3.E3.m1.1.1.3.3.2.2.2.3.cmml">1</mn><mrow id="S3.E3.m1.1.1.3.3.2.2.3" xref="S3.E3.m1.1.1.3.3.2.2.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2.3.2" xref="S3.E3.m1.1.1.3.3.2.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.2.3.1" xref="S3.E3.m1.1.1.3.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.2.3.3" xref="S3.E3.m1.1.1.3.3.2.2.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.2.3.1a" xref="S3.E3.m1.1.1.3.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.2.3.4" xref="S3.E3.m1.1.1.3.3.2.2.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.2.3.1b" xref="S3.E3.m1.1.1.3.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.2.3.5" xref="S3.E3.m1.1.1.3.3.2.2.3.5.cmml">r</mi></mrow></msubsup></mrow><mn id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">8</mn></mfrac></mstyle><mo id="S3.E3.m1.1.1.3.1a" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mstyle displaystyle="true" id="S3.E3.m1.1.1.3.4" xref="S3.E3.m1.1.1.3.4.cmml"><mfrac id="S3.E3.m1.1.1.3.4a" xref="S3.E3.m1.1.1.3.4.cmml"><mrow id="S3.E3.m1.1.1.3.4.2" xref="S3.E3.m1.1.1.3.4.2.cmml"><msub id="S3.E3.m1.1.1.3.4.2.1" xref="S3.E3.m1.1.1.3.4.2.1.cmml"><mo id="S3.E3.m1.1.1.3.4.2.1.2" xref="S3.E3.m1.1.1.3.4.2.1.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.3.4.2.1.3" xref="S3.E3.m1.1.1.3.4.2.1.3.cmml"><mi id="S3.E3.m1.1.1.3.4.2.1.3.2" xref="S3.E3.m1.1.1.3.4.2.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.4.2.1.3.1" xref="S3.E3.m1.1.1.3.4.2.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.4.2.1.3.3" xref="S3.E3.m1.1.1.3.4.2.1.3.3.cmml">u</mi></mrow></msub><msubsup id="S3.E3.m1.1.1.3.4.2.2" xref="S3.E3.m1.1.1.3.4.2.2.cmml"><mi id="S3.E3.m1.1.1.3.4.2.2.2.2" xref="S3.E3.m1.1.1.3.4.2.2.2.2.cmml">F</mi><mn id="S3.E3.m1.1.1.3.4.2.2.2.3" xref="S3.E3.m1.1.1.3.4.2.2.2.3.cmml">1</mn><mrow id="S3.E3.m1.1.1.3.4.2.2.3" xref="S3.E3.m1.1.1.3.4.2.2.3.cmml"><mi id="S3.E3.m1.1.1.3.4.2.2.3.2" xref="S3.E3.m1.1.1.3.4.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.4.2.2.3.1" xref="S3.E3.m1.1.1.3.4.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.4.2.2.3.3" xref="S3.E3.m1.1.1.3.4.2.2.3.3.cmml">u</mi></mrow></msubsup></mrow><mn id="S3.E3.m1.1.1.3.4.3" xref="S3.E3.m1.1.1.3.4.3.cmml">12</mn></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">absent</csymbol><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><divide id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2"></divide><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2"><plus id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2.1"></plus><apply id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2.2">𝜌</ci><ci id="S3.E3.m1.1.1.3.2.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.2.3">𝑎</ci></apply><apply id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.2.3.2">𝜌</ci><ci id="S3.E3.m1.1.1.3.2.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3.3">𝑣</ci></apply></apply><cn type="integer" id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3">2</cn></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><divide id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3"></divide><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2"><apply id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.1.1.cmml" xref="S3.E3.m1.1.1.3.3.2.1">subscript</csymbol><sum id="S3.E3.m1.1.1.3.3.2.1.2.cmml" xref="S3.E3.m1.1.1.3.3.2.1.2"></sum><apply id="S3.E3.m1.1.1.3.3.2.1.3.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3"><times id="S3.E3.m1.1.1.3.3.2.1.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.1.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3.2">𝑒</ci><ci id="S3.E3.m1.1.1.3.3.2.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3.3">𝑥</ci><ci id="S3.E3.m1.1.1.3.3.2.1.3.4.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3.4">𝑝</ci><ci id="S3.E3.m1.1.1.3.3.2.1.3.5.cmml" xref="S3.E3.m1.1.1.3.3.2.1.3.5">𝑟</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.2">superscript</csymbol><apply id="S3.E3.m1.1.1.3.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.2.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.2.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2.2.2">𝐹</ci><cn type="integer" id="S3.E3.m1.1.1.3.3.2.2.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.2.2.3">1</cn></apply><apply id="S3.E3.m1.1.1.3.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3"><times id="S3.E3.m1.1.1.3.3.2.2.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.2.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3.2">𝑒</ci><ci id="S3.E3.m1.1.1.3.3.2.2.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3.3">𝑥</ci><ci id="S3.E3.m1.1.1.3.3.2.2.3.4.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3.4">𝑝</ci><ci id="S3.E3.m1.1.1.3.3.2.2.3.5.cmml" xref="S3.E3.m1.1.1.3.3.2.2.3.5">𝑟</ci></apply></apply></apply><cn type="integer" id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">8</cn></apply><apply id="S3.E3.m1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.3.4"><divide id="S3.E3.m1.1.1.3.4.1.cmml" xref="S3.E3.m1.1.1.3.4"></divide><apply id="S3.E3.m1.1.1.3.4.2.cmml" xref="S3.E3.m1.1.1.3.4.2"><apply id="S3.E3.m1.1.1.3.4.2.1.cmml" xref="S3.E3.m1.1.1.3.4.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.2.1.1.cmml" xref="S3.E3.m1.1.1.3.4.2.1">subscript</csymbol><sum id="S3.E3.m1.1.1.3.4.2.1.2.cmml" xref="S3.E3.m1.1.1.3.4.2.1.2"></sum><apply id="S3.E3.m1.1.1.3.4.2.1.3.cmml" xref="S3.E3.m1.1.1.3.4.2.1.3"><times id="S3.E3.m1.1.1.3.4.2.1.3.1.cmml" xref="S3.E3.m1.1.1.3.4.2.1.3.1"></times><ci id="S3.E3.m1.1.1.3.4.2.1.3.2.cmml" xref="S3.E3.m1.1.1.3.4.2.1.3.2">𝑎</ci><ci id="S3.E3.m1.1.1.3.4.2.1.3.3.cmml" xref="S3.E3.m1.1.1.3.4.2.1.3.3">𝑢</ci></apply></apply><apply id="S3.E3.m1.1.1.3.4.2.2.cmml" xref="S3.E3.m1.1.1.3.4.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.2.2.1.cmml" xref="S3.E3.m1.1.1.3.4.2.2">superscript</csymbol><apply id="S3.E3.m1.1.1.3.4.2.2.2.cmml" xref="S3.E3.m1.1.1.3.4.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.2.2.2.1.cmml" xref="S3.E3.m1.1.1.3.4.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.4.2.2.2.2.cmml" xref="S3.E3.m1.1.1.3.4.2.2.2.2">𝐹</ci><cn type="integer" id="S3.E3.m1.1.1.3.4.2.2.2.3.cmml" xref="S3.E3.m1.1.1.3.4.2.2.2.3">1</cn></apply><apply id="S3.E3.m1.1.1.3.4.2.2.3.cmml" xref="S3.E3.m1.1.1.3.4.2.2.3"><times id="S3.E3.m1.1.1.3.4.2.2.3.1.cmml" xref="S3.E3.m1.1.1.3.4.2.2.3.1"></times><ci id="S3.E3.m1.1.1.3.4.2.2.3.2.cmml" xref="S3.E3.m1.1.1.3.4.2.2.3.2">𝑎</ci><ci id="S3.E3.m1.1.1.3.4.2.2.3.3.cmml" xref="S3.E3.m1.1.1.3.4.2.2.3.3">𝑢</ci></apply></apply></apply><cn type="integer" id="S3.E3.m1.1.1.3.4.3.cmml" xref="S3.E3.m1.1.1.3.4.3">12</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle=\frac{\rho_{a}+\rho_{v}}{2}+\frac{\sum_{expr}F_{1}^{expr}}{8}+\frac{\sum_{au}F_{1}^{au}}{12}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learning from Synthetic Data Challenge</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The performance measure is the average F1 Score of the 6 basic expression categories (i.e., macro F1 Score):</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{P}_{LSD}" display="inline"><semantics id="S3.E4.m1.1a"><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">𝒫</mi><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1a" xref="S3.E4.m1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.4" xref="S3.E4.m1.1.1.3.4.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">𝒫</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><times id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></times><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝐿</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝑆</ci><ci id="S3.E4.m1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.3.4">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle\mathcal{P}_{LSD}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{\sum_{expr}F_{1}^{expr}}{6}" display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml"><mi id="S3.E4.m2.1.1.2" xref="S3.E4.m2.1.1.2.cmml"></mi><mo id="S3.E4.m2.1.1.1" xref="S3.E4.m2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E4.m2.1.1.3" xref="S3.E4.m2.1.1.3.cmml"><mfrac id="S3.E4.m2.1.1.3a" xref="S3.E4.m2.1.1.3.cmml"><mrow id="S3.E4.m2.1.1.3.2" xref="S3.E4.m2.1.1.3.2.cmml"><msub id="S3.E4.m2.1.1.3.2.1" xref="S3.E4.m2.1.1.3.2.1.cmml"><mo id="S3.E4.m2.1.1.3.2.1.2" xref="S3.E4.m2.1.1.3.2.1.2.cmml">∑</mo><mrow id="S3.E4.m2.1.1.3.2.1.3" xref="S3.E4.m2.1.1.3.2.1.3.cmml"><mi id="S3.E4.m2.1.1.3.2.1.3.2" xref="S3.E4.m2.1.1.3.2.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.1.3.1" xref="S3.E4.m2.1.1.3.2.1.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.1.3.3" xref="S3.E4.m2.1.1.3.2.1.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.1.3.1a" xref="S3.E4.m2.1.1.3.2.1.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.1.3.4" xref="S3.E4.m2.1.1.3.2.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.1.3.1b" xref="S3.E4.m2.1.1.3.2.1.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.1.3.5" xref="S3.E4.m2.1.1.3.2.1.3.5.cmml">r</mi></mrow></msub><msubsup id="S3.E4.m2.1.1.3.2.2" xref="S3.E4.m2.1.1.3.2.2.cmml"><mi id="S3.E4.m2.1.1.3.2.2.2.2" xref="S3.E4.m2.1.1.3.2.2.2.2.cmml">F</mi><mn id="S3.E4.m2.1.1.3.2.2.2.3" xref="S3.E4.m2.1.1.3.2.2.2.3.cmml">1</mn><mrow id="S3.E4.m2.1.1.3.2.2.3" xref="S3.E4.m2.1.1.3.2.2.3.cmml"><mi id="S3.E4.m2.1.1.3.2.2.3.2" xref="S3.E4.m2.1.1.3.2.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.2.3.1" xref="S3.E4.m2.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.2.3.3" xref="S3.E4.m2.1.1.3.2.2.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.2.3.1a" xref="S3.E4.m2.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.2.3.4" xref="S3.E4.m2.1.1.3.2.2.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.3.2.2.3.1b" xref="S3.E4.m2.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E4.m2.1.1.3.2.2.3.5" xref="S3.E4.m2.1.1.3.2.2.3.5.cmml">r</mi></mrow></msubsup></mrow><mn id="S3.E4.m2.1.1.3.3" xref="S3.E4.m2.1.1.3.3.cmml">6</mn></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1"><eq id="S3.E4.m2.1.1.1.cmml" xref="S3.E4.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E4.m2.1.1.2.cmml" xref="S3.E4.m2.1.1.2">absent</csymbol><apply id="S3.E4.m2.1.1.3.cmml" xref="S3.E4.m2.1.1.3"><divide id="S3.E4.m2.1.1.3.1.cmml" xref="S3.E4.m2.1.1.3"></divide><apply id="S3.E4.m2.1.1.3.2.cmml" xref="S3.E4.m2.1.1.3.2"><apply id="S3.E4.m2.1.1.3.2.1.cmml" xref="S3.E4.m2.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.3.2.1.1.cmml" xref="S3.E4.m2.1.1.3.2.1">subscript</csymbol><sum id="S3.E4.m2.1.1.3.2.1.2.cmml" xref="S3.E4.m2.1.1.3.2.1.2"></sum><apply id="S3.E4.m2.1.1.3.2.1.3.cmml" xref="S3.E4.m2.1.1.3.2.1.3"><times id="S3.E4.m2.1.1.3.2.1.3.1.cmml" xref="S3.E4.m2.1.1.3.2.1.3.1"></times><ci id="S3.E4.m2.1.1.3.2.1.3.2.cmml" xref="S3.E4.m2.1.1.3.2.1.3.2">𝑒</ci><ci id="S3.E4.m2.1.1.3.2.1.3.3.cmml" xref="S3.E4.m2.1.1.3.2.1.3.3">𝑥</ci><ci id="S3.E4.m2.1.1.3.2.1.3.4.cmml" xref="S3.E4.m2.1.1.3.2.1.3.4">𝑝</ci><ci id="S3.E4.m2.1.1.3.2.1.3.5.cmml" xref="S3.E4.m2.1.1.3.2.1.3.5">𝑟</ci></apply></apply><apply id="S3.E4.m2.1.1.3.2.2.cmml" xref="S3.E4.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.3.2.2.1.cmml" xref="S3.E4.m2.1.1.3.2.2">superscript</csymbol><apply id="S3.E4.m2.1.1.3.2.2.2.cmml" xref="S3.E4.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.3.2.2.2.1.cmml" xref="S3.E4.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.E4.m2.1.1.3.2.2.2.2.cmml" xref="S3.E4.m2.1.1.3.2.2.2.2">𝐹</ci><cn type="integer" id="S3.E4.m2.1.1.3.2.2.2.3.cmml" xref="S3.E4.m2.1.1.3.2.2.2.3">1</cn></apply><apply id="S3.E4.m2.1.1.3.2.2.3.cmml" xref="S3.E4.m2.1.1.3.2.2.3"><times id="S3.E4.m2.1.1.3.2.2.3.1.cmml" xref="S3.E4.m2.1.1.3.2.2.3.1"></times><ci id="S3.E4.m2.1.1.3.2.2.3.2.cmml" xref="S3.E4.m2.1.1.3.2.2.3.2">𝑒</ci><ci id="S3.E4.m2.1.1.3.2.2.3.3.cmml" xref="S3.E4.m2.1.1.3.2.2.3.3">𝑥</ci><ci id="S3.E4.m2.1.1.3.2.2.3.4.cmml" xref="S3.E4.m2.1.1.3.2.2.3.4">𝑝</ci><ci id="S3.E4.m2.1.1.3.2.2.3.5.cmml" xref="S3.E4.m2.1.1.3.2.2.3.5">𝑟</ci></apply></apply></apply><cn type="integer" id="S3.E4.m2.1.1.3.3.cmml" xref="S3.E4.m2.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle=\frac{\sum_{expr}F_{1}^{expr}}{6}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baseline Networks and Performance</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">All baseline systems rely exclusively on existing open-source machine learning toolkits to ensure the reproducibility of the results. All systems have been implemented in TensorFlow; training time was around five hours on a Titan X GPU, with a learning rate of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.p1.1.m1.1a"><msup id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mn id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mo id="S4.p1.1.m1.1.1.3a" xref="S4.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">10</cn><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><minus id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">10^{-4}</annotation></semantics></math> and with a batch size of 128.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In this Section, we first describe the baseline systems developed for each Challenge and then report their achieved performance.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multi-Task Learning Challenge</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The baseline network is a VGG16 network with with fixed convolutional weights (only the 3 fully connected layers were trained), pre-trained on the VGGFACE dataset. The output layer consists of 22 units: 2 linear units that give the valence and arousal predictions; 8 units equipped with softmax activation function that give the expression predictions; 12 units equipped with sigmoid activation function that give the action unit predictions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Let us mention here that no data augmentation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> have been utilized when training this baseline network with the cropped-aligned version of s-Aff-Wild2 database. We just normalised all images’ pixel intensity values in the range <math id="S4.SS1.p2.1.m1.2" class="ltx_Math" alttext="[-1,1]" display="inline"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.2.1" xref="S4.SS1.p2.1.m1.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p2.1.m1.2.2.1.2" xref="S4.SS1.p2.1.m1.2.2.2.cmml">[</mo><mrow id="S4.SS1.p2.1.m1.2.2.1.1" xref="S4.SS1.p2.1.m1.2.2.1.1.cmml"><mo id="S4.SS1.p2.1.m1.2.2.1.1a" xref="S4.SS1.p2.1.m1.2.2.1.1.cmml">−</mo><mn id="S4.SS1.p2.1.m1.2.2.1.1.2" xref="S4.SS1.p2.1.m1.2.2.1.1.2.cmml">1</mn></mrow><mo id="S4.SS1.p2.1.m1.2.2.1.3" xref="S4.SS1.p2.1.m1.2.2.2.cmml">,</mo><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S4.SS1.p2.1.m1.2.2.1.4" xref="S4.SS1.p2.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><interval closure="closed" id="S4.SS1.p2.1.m1.2.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2.1"><apply id="S4.SS1.p2.1.m1.2.2.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1"><minus id="S4.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1"></minus><cn type="integer" id="S4.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1.2">1</cn></apply><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">[-1,1]</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Multi-Task Learning Challenge ‣ 4 Baseline Networks and Performance ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the performance of the baseline model on the validation set of s-Aff-Wild2.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Multi-Task Learning Challenge: Performance of baseline model on the validation set; evaluation criterion is the sum of each task’s independent
performance metric.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Baseline</td>
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{MTL}" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T4.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.m1.1.1.2.cmml">𝒫</mi><mrow id="S4.T4.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.m1.1.1.3.cmml"><mi id="S4.T4.1.1.1.m1.1.1.3.2" xref="S4.T4.1.1.1.m1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.T4.1.1.1.m1.1.1.3.1" xref="S4.T4.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T4.1.1.1.m1.1.1.3.3" xref="S4.T4.1.1.1.m1.1.1.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.T4.1.1.1.m1.1.1.3.1a" xref="S4.T4.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T4.1.1.1.m1.1.1.3.4" xref="S4.T4.1.1.1.m1.1.1.3.4.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.m1.1.1.2">𝒫</ci><apply id="S4.T4.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3"><times id="S4.T4.1.1.1.m1.1.1.3.1.cmml" xref="S4.T4.1.1.1.m1.1.1.3.1"></times><ci id="S4.T4.1.1.1.m1.1.1.3.2.cmml" xref="S4.T4.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S4.T4.1.1.1.m1.1.1.3.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3.3">𝑇</ci><ci id="S4.T4.1.1.1.m1.1.1.3.4.cmml" xref="S4.T4.1.1.1.m1.1.1.3.4">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\mathcal{P}_{MTL}</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_tt">VGGFACE</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">0.30</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Learning from Synthetic Data Challenge</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The baseline network is a ResNet with 50 layers, pre-trained on ImageNet (ResNet50); its output layer consists of 6 units and is equipped with softmax activation function that gives the basic expression predictions.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Let us mention here that no data augmentation techniques have been utilized when training this baseline network with the synthetic images. We just normalised all images’ pixel intensity values in the range <math id="S4.SS2.p2.1.m1.2" class="ltx_Math" alttext="[-1,1]" display="inline"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.2.1" xref="S4.SS2.p2.1.m1.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.2.2.1.2" xref="S4.SS2.p2.1.m1.2.2.2.cmml">[</mo><mrow id="S4.SS2.p2.1.m1.2.2.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.cmml"><mo id="S4.SS2.p2.1.m1.2.2.1.1a" xref="S4.SS2.p2.1.m1.2.2.1.1.cmml">−</mo><mn id="S4.SS2.p2.1.m1.2.2.1.1.2" xref="S4.SS2.p2.1.m1.2.2.1.1.2.cmml">1</mn></mrow><mo id="S4.SS2.p2.1.m1.2.2.1.3" xref="S4.SS2.p2.1.m1.2.2.2.cmml">,</mo><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S4.SS2.p2.1.m1.2.2.1.4" xref="S4.SS2.p2.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><interval closure="closed" id="S4.SS2.p2.1.m1.2.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1"><apply id="S4.SS2.p2.1.m1.2.2.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1"><minus id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1"></minus><cn type="integer" id="S4.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.2">1</cn></apply><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">[-1,1]</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Learning from Synthetic Data Challenge ‣ 4 Baseline Networks and Performance ‣ ABAW: Learning from Synthetic Data &amp; Multi-Task Learning Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the performance of the baseline model on the validation and test sets, which consist of only real data of the Aff-Wild2 database.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Learning from Synthetic Data Challenge: Performance of baseline model on the validation and test sets, which consist of only real data of the Aff-Wild2 database; evaluation criterion is the average F1 Score of the 6 basic expression
categories. The performance on the validation set is indicated inside the parenthesis.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Baseline</td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{LSD}" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><msub id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T5.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.m1.1.1.2.cmml">𝒫</mi><mrow id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3.cmml"><mi id="S4.T5.1.1.1.m1.1.1.3.2" xref="S4.T5.1.1.1.m1.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.m1.1.1.3.1" xref="S4.T5.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.1.1.1.m1.1.1.3.3" xref="S4.T5.1.1.1.m1.1.1.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.m1.1.1.3.1a" xref="S4.T5.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.1.1.1.m1.1.1.3.4" xref="S4.T5.1.1.1.m1.1.1.3.4.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2">𝒫</ci><apply id="S4.T5.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.m1.1.1.3"><times id="S4.T5.1.1.1.m1.1.1.3.1.cmml" xref="S4.T5.1.1.1.m1.1.1.3.1"></times><ci id="S4.T5.1.1.1.m1.1.1.3.2.cmml" xref="S4.T5.1.1.1.m1.1.1.3.2">𝐿</ci><ci id="S4.T5.1.1.1.m1.1.1.3.3.cmml" xref="S4.T5.1.1.1.m1.1.1.3.3">𝑆</ci><ci id="S4.T5.1.1.1.m1.1.1.3.4.cmml" xref="S4.T5.1.1.1.m1.1.1.3.4">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\mathcal{P}_{LSD}</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_tt">ResNet50</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">0.30 (0.50)</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we have presented the fourth Affective Behavior Analysis in-the-wild Competition (ABAW) 2022 held in conjunction with ECCV 2022. This Competition is a continuation of the first, second and third ABAW Competitions held in conjunction with IEEE FG 2020, ICCV 2021 and IEEE CVPR 2022, respectively. This Competition comprises two Challenges: i) the Multi-Task-
Learning (MTL) Challenge in which the goal is to create a system that learns at
the same time (i.e., in a multi-task learning setting) to estimate valence and
arousal, classify eight expressions (6 basic expressions plus the neutral state
plus a category ’other’ which denotes expressions/affective states other than the
6 basic ones) and detect twelve action units; ii) the Learning from Synthetic
Data (LSD) Challenge in which the goal is to create a system that learns to recognise
the six basic expressions (anger, disgust, fear, happiness, sadness, surprise) from
artificially generated data (i.e., synthetic data) and generalise its knowledge to
real-world (i.e., real) data.
Each Challenge’s corpora is derived from the Aff-Wild2 database.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Antoniadis, P., Pikoulis, I., Filntisis, P.P., Maragos, P.: An audiovisual and
contextual approach for categorical and continuous emotion recognition
in-the-wild. arXiv preprint arXiv:2107.03465 (2021)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Darwin, C., Prodger, P.: The expression of the emotions in man and animals.
Oxford University Press, USA (1998)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Deng, D.: Multiple emotion descriptors estimation at the abaw3 challenge. arXiv
preprint arXiv:2203.12845 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Deng, D., Chen, Z., Shi, B.E.: Fau, facial expressions, valence and arousal: A
multi-task solution. arXiv preprint arXiv:2002.03557 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Deng, D., Chen, Z., Shi, B.E.: Multitask emotion recognition with incomplete
labels. In: 2020 15th IEEE International Conference on Automatic Face and
Gesture Recognition (FG 2020). pp. 592–599. IEEE (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Deng, D., Shi, B.E.: Estimating multiple emotion descriptors by separating
description and inference. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2392–2400
(June 2022)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Deng, D., Wu, L., Shi, B.E.: Towards better uncertainty: Iterative training of
efficient networks for multitask emotion recognition. arXiv preprint
arXiv:2108.04228 (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: Retinaface:
Single-shot multi-level face localisation in the wild. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
5203–5212 (2020)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Do, N.T., Nguyen-Quynh, T.T., Kim, S.H.: Affective expression analysis
in-the-wild using multi-task temporal statistical deep learning model. In:
2020 15th IEEE International Conference on Automatic Face and Gesture
Recognition (FG 2020). pp. 624–628. IEEE (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dresvyanskiy, D., Ryumina, E., Kaya, H., Markitantov, M., Karpov, A., Minker,
W.: An audio-video deep and transfer learning framework for multimodal
emotion recognition in the wild. arXiv preprint arXiv:2010.03692 (2020)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ekman, P.: Facial action coding system (facs). A human face (2002)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Frijda, N.H., et al.: The emotions. Cambridge University Press (1986)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gera, D., Balasubramanian, S.: Affect expression behaviour analysis in the wild
using spatio-channel attention and complementary context information. arXiv
preprint arXiv:2009.14440 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gera, D., Balasubramanian, S.: Affect expression behaviour analysis in the wild
using consensual collaborative training. arXiv preprint arXiv:2107.05736
(2021)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Han, S., Meng, Z., Khan, A.S., Tong, Y.: Incremental boosting convolutional
neural network for facial action unit recognition. In: Advances in neural
information processing systems. pp. 109–117 (2016)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hoai, D.L., Lim, E., Choi, E., Kim, S., Pant, S., Lee, G.S., Kim, S.H., Yang,
H.J.: An attention-based method for action unit detection at the 3rd abaw
competition. arXiv preprint arXiv:2203.12428 (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jeong, E., Oh, G., Lim, S.: Multitask emotion recognition model with knowledge
distillation and task discriminator. arXiv preprint arXiv:2203.13072 (2022)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jeong, J.Y., Hong, Y.G., Kim, D., Jeong, J.W., Jung, Y., Kim, S.H.:
Classification of facial expression in-the-wild based on ensemble of
multi-head cross attention networks. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp.
2353–2358 (June 2022)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jeong, J.Y., Hong, Y.G., Kim, D., Jung, Y., Jeong, J.W.: Facial expression
recognition based on multi-head cross attention network. arXiv preprint
arXiv:2203.13235 (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Ji, X., Ding, Y., Li, L., Chen, Y., Fan, C.: Multi-label relation modeling in
facial action units detection. arXiv preprint arXiv:2002.01105 (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jiang, W., Wu, Y., Qiao, F., Meng, L., Deng, Y., Liu, C.: Facial action unit
recognition with multi-models ensembling. arXiv preprint arXiv:2203.13046
(2022)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jiang, W., Wu, Y., Qiao, F., Meng, L., Deng, Y., Liu, C.: Model level ensemble
for facial action unit recognition at the 3rd abaw challenge. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops. pp. 2337–2344 (June 2022)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jin, Y., Zheng, T., Gao, C., Xu, G.: A multi-modal and multi-task learning
method for action unit and expression recognition. arXiv preprint
arXiv:2107.04187 (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Karas, V., Tellamekala, M.K., Mallol-Ragolta, A., Valstar, M., Schuller, B.W.:
Continuous-time audiovisual fusion with recurrence vs. attention for
in-the-wild affect recognition. arXiv preprint arXiv:2203.13285 (2022)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Karas, V., Tellamekala, M.K., Mallol-Ragolta, A., Valstar, M., Schuller, B.W.:
Time-continuous audiovisual fusion with recurrence vs attention for
in-the-wild affect recognition. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2382–2391
(June 2022)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kim, J.H., Kim, N., Won, C.S.: Facial expression recognition with swin
transformer. arXiv preprint arXiv:2203.13472 (2022)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kollias, D.: Abaw: Valence-arousal estimation, expression recognition, action
unit detection &amp; multi-task learning challenges. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2328–2336 (2022)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kollias, D., Cheng, S., Pantic, M., Zafeiriou, S.: Photorealistic facial
synthesis in the dimensional affect space. In: European Conference on
Computer Vision. pp. 475–491. Springer (2018)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kollias, D., Cheng, S., Ververas, E., Kotsia, I., Zafeiriou, S.: Deep neural
network augmentation: Generating faces for affect analysis. International
Journal of Computer Vision pp. 1–30 (2020)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kollias, D., Nicolaou, M.A., Kotsia, I., Zhao, G., Zafeiriou, S.: Recognition
of affect in the wild using deep neural networks. In: Computer Vision and
Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on. pp.
1972–1979. IEEE (2017)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kollias, D., Schulc, A., Hajiyev, E., Zafeiriou, S.: Analysing affective
behavior in the first abaw 2020 competition. In: 2020 15th IEEE International
Conference on Automatic Face and Gesture Recognition (FG 2020)(FG). pp.
794–800. IEEE Computer Society (2020)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kollias, D., Sharmanska, V., Zafeiriou, S.: Face behavior a la carte:
Expressions, affect and action units in a single network. arXiv preprint
arXiv:1910.11111 (2019)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching for
heterogeneous multi-task learning: a large-scale face study. arXiv preprint
arXiv:2105.03790 (2021)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller,
B., Kotsia, I., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-wild
database and challenge, deep architectures, and beyond. International Journal
of Computer Vision <span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">127</span>(6-7), 907–929 (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: Aff-wild2: Extending the aff-wild database for
affect recognition. arXiv preprint arXiv:1811.07770 (2018)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: A multi-task learning &amp; generation framework:
Valence-arousal, action units &amp; primary expressions. arXiv preprint
arXiv:1811.07771 (2018)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: Expression, affect, action unit recognition:
Aff-wild2, multi-task learning and arcface. arXiv preprint arXiv:1910.04855
(2019)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: Va-stargan: Continuous affect generation. In:
International Conference on Advanced Concepts for Intelligent Vision Systems.
pp. 227–238. Springer (2020)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: Affect analysis in-the-wild: Valence-arousal,
expressions, action units and a unified framework. arXiv preprint
arXiv:2103.15792 (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Kollias, D., Zafeiriou, S.: Analysing affective behavior in the second abaw2
competition. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3652–3660 (2021)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Kuhnke, F., Rumberg, L., Ostermann, J.: Two-stream aural-visual affect analysis
in the wild. arXiv preprint arXiv:2002.03399 (2020)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Le Hoai, D., Lim, E., Choi, E., Kim, S., Pant, S., Lee, G.S., Kim, S.H., Yang,
H.J.: An attention-based method for multi-label facial action unit detection.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops. pp. 2454–2459 (June 2022)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Li, I., et al.: Technical report for valence-arousal estimation on affwild2
dataset. arXiv preprint arXiv:2105.01502 (2021)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Liu, H., Zeng, J., Shan, S., Chen, X.: Emotion recognition for in-the-wild
videos. arXiv preprint arXiv:2002.05447 (2020)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Mao, S., Fan, X., Peng, X.: Spatial and temporal networks for facial expression
recognition in the wild videos. arXiv preprint arXiv:2107.05160 (2021)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Meng, L., Liu, Y., Liu, X., Huang, Z., Jiang, W., Zhang, T., Deng, Y., Li, R.,
Wu, Y., Zhao, J., et al.: Multi-modal emotion estimation for in-the-wild
videos. arXiv preprint arXiv:2203.13032 (2022)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Meng, L., Liu, Y., Liu, X., Huang, Z., Jiang, W., Zhang, T., Liu, C., Jin, Q.:
Valence and arousal estimation based on multimodal temporal-aware features
for videos in the wild. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2345–2352
(June 2022)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Nguyen, H.H., Huynh, V.T., Kim, S.H.: An ensemble approach for facial behavior
analysis in-the-wild video. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2512–2517
(June 2022)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Nguyen, H.H., Huynh, V.T., Kim, S.H.: An ensemble approach for facial
expression analysis in video. arXiv preprint arXiv:2203.12891 (2022)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Oh, G., Jeong, E., Lim, S.: Causal affect prediction model using a facial image
sequence. arXiv preprint arXiv:2107.03886 (2021)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Pahl, J., Rieger, I., Seuss, D.: Multi-label class balancing algorithm for
action unit detection. arXiv preprint arXiv:2002.03238 (2020)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Phan, K.N., Nguyen, H.H., Huynh, V.T., Kim, S.H.: Expression classification
using concatenation of deep neural network for the 3rd abaw3 competition.
arXiv preprint arXiv:2203.12899 (2022)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Psaroudakis, A., Kollias, D.: Mixaugment &amp; mixup: Augmentation methods for
facial expression recognition. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 2367–2375 (2022)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Rajasekar, G.P., de Melo, W.C., Ullah, N., Aslam, H., Zeeshan, O., Denorme, T.,
Pedersoli, M., Koerich, A., Cardinal, P., Granger, E.: A joint
cross-attention model for audio-visual fusion in dimensional emotion
recognition. arXiv preprint arXiv:2203.14779 (2022)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Russell, J.A.: Evidence of convergent validity on the dimensions of affect.
Journal of personality and social psychology <span id="bib.bib55.1.1" class="ltx_text ltx_font_bold">36</span>(10),  1152 (1978)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Saito, J., Mi, X., Uchida, A., Youoku, S., Yamamoto, T., Murase, K.: Action
units recognition using improved pairwise deep architecture. arXiv preprint
arXiv:2107.03143 (2021)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Savchenko, A.V.: Frame-level prediction of facial expressions, valence, arousal
and action units for mobile devices. arXiv preprint arXiv:2203.13436 (2022)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Savchenko, A.V.: Video-based frame-level facial analysis of affective behavior
on mobile devices using efficientnets. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp.
2359–2366 (June 2022)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Tallec, G., Yvinec, E., Dapogny, A., Bailly, K.: Multi-label transformer for
action unit detection. arXiv preprint arXiv:2203.12531 (2022)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Vu, M.T., Beurton-Aimar, M.: Multitask multi-database emotion recognition.
arXiv preprint arXiv:2107.04127 (2021)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Wang, L., Qi, J., Cheng, J., Suzuki, K.: Action unit detection by exploiting
spatial-temporal and label-wise attention with transformer. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops. pp. 2470–2475 (June 2022)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Wang, L., Wang, S.: A multi-task mean teacher for semi-supervised facial
affective behavior analysis. arXiv preprint arXiv:2107.04225 (2021)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wang, L., Wang, S., Qi, J.: Multi-modal multi-label facial action unit
detection with transformer. arXiv preprint arXiv:2203.13301 (2022)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Wang, S., Chang, Y., Wang, J.: Facial action unit recognition based on transfer
learning. arXiv preprint arXiv:2203.14694 (2022)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Whissel, C.: The dictionary of affect in language, emotion: Theory, research
and experience: vol. 4, the measurement of emotions, r. Plutchik and H.
Kellerman, Eds., New York: Academic (1989)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Xie, H.X., Li, I., Lo, L., Shuai, H.H., Cheng, W.H., et al.: Technical report
for valence-arousal estimation in abaw2 challenge. arXiv preprint
arXiv:2107.03891 (2021)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Xue, F., Tan, Z., Zhu, Y., Ma, Z., Guo, G.: Coarse-to-fine cascaded networks
with smooth predicting for video facial expression recognition. arXiv
preprint arXiv:2203.13052 (2022)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Youoku, S., Toyoda, Y., Yamamoto, T., Saito, J., Kawamura, R., Mi, X., Murase,
K.: A multi-term and multi-task analyzing framework for affective analysis
in-the-wild. arXiv preprint arXiv:2009.13885 (2020)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Yu, J., Cai, Z., He, P., Xie, G., Ling, Q.: Multi-model ensemble learning
method for human expression recognition. arXiv preprint arXiv:2203.14466
(2022)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia,
I.: Aff-wild: Valence and arousal’in-the-wild’challenge. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp.
34–41 (2017)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhang, S., An, R., Ding, Y., Guan, C.: Continuous emotion recognition using
visual-audio-linguistic information: A technical report for abaw3. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops. pp. 2376–2381 (June 2022)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Zhang, S., An, R., Ding, Y., Guan, C.: Continuous emotion recognition using
visual-audio-linguistic information: A technical report for abaw3. arXiv
preprint arXiv:2203.13031 (2022)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Zhang, S., Ding, Y., Wei, Z., Guan, C.: Audio-visual attentive fusion for
continuous emotion recognition. arXiv preprint arXiv:2107.01175 (2021)

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Zhang, W., Guo, Z., Chen, K., Li, L., Zhang, Z., Ding, Y.: Prior aided
streaming network for multi-task affective recognitionat the 2nd abaw2
competition. arXiv preprint arXiv:2107.03708 (2021)

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Zhang, W., Guo, Z., Chen, K., Li, L., Zhang, Z., Ding, Y., Wu, R., Lv, T., Fan,
C.: Prior aided streaming network for multi-task affective analysis. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) Workshops. pp. 3539–3549 (October 2021)

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Zhang, W., Zhang, Z., Qiu, F., Wang, S., Ma, B., Zeng, H., An, R., Ding, Y.:
Transformer-based multimodal information fusion for facial expression
analysis. arXiv preprint arXiv:2203.12367 (2022)

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Zhang, Y.H., Huang, R., Zeng, J., Shan, S., Chen, X.: <math id="bib.bib77.1.m1.1" class="ltx_Math" alttext="m^{3}" display="inline"><semantics id="bib.bib77.1.m1.1a"><msup id="bib.bib77.1.m1.1.1" xref="bib.bib77.1.m1.1.1.cmml"><mi id="bib.bib77.1.m1.1.1.2" xref="bib.bib77.1.m1.1.1.2.cmml">m</mi><mn id="bib.bib77.1.m1.1.1.3" xref="bib.bib77.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib77.1.m1.1b"><apply id="bib.bib77.1.m1.1.1.cmml" xref="bib.bib77.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib77.1.m1.1.1.1.cmml" xref="bib.bib77.1.m1.1.1">superscript</csymbol><ci id="bib.bib77.1.m1.1.1.2.cmml" xref="bib.bib77.1.m1.1.1.2">𝑚</ci><cn type="integer" id="bib.bib77.1.m1.1.1.3.cmml" xref="bib.bib77.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib77.1.m1.1c">m^{3}</annotation></semantics></math> t: Multi-modal
continuous valence-arousal estimation in the wild. arXiv preprint
arXiv:2002.02957 (2020)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.01136" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.01138" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.01138">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.01138" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.01139" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 12:39:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
