<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1704.07121] Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets</title><meta property="og:description" content="Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1704.07121">

<!--Generated on Fri Mar 15 16:47:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Being Negative but Constructively: 
<br class="ltx_break">Lessons Learnt from Creating Better Visual Question Answering Datasets</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei-Lun Chao<sup id="id2.2.id1" class="ltx_sup">∗</sup>, Hexiang Hu , Fei Sha
<br class="ltx_break">University of Southern California
<br class="ltx_break">Los Angeles, California, USA
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_typewriter">weilunchao760414@gmail.com, hexiang.frank.hu@gmail.com, feisha@usc.edu</span>
</span><span class="ltx_author_notes">  Equal contributions</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via <a target="_blank" href="http://www.teds.usc.edu/website_vqa/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.teds.usc.edu/website_vqa/</a>.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/1704.07121/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="237" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration of how the shortcuts in the Visual7W dataset <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> should be remedied. In the original dataset, the correct answer “A train” is easily selected by a machine as it is far often used as the correct answer than the other decoy (negative) answers. (The numbers in the brackets are probability scores computed using eq. (<a href="#S3.E2" title="In Shortcuts are due to design deficiencies ‣ 3.2 Analysis results ‣ 3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)). Our two procedures — QoU and IoU (cf. Sect. <a href="#S4" title="4 Creating Better Visual QA Datasets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) — create alternative decoys such that both the correct answer and the decoys are highly likely by examining either the image or the question <span id="S0.F1.3.1" class="ltx_text ltx_font_bold">alone</span>. In these cases, machines make mistakes unless they consider all information <span id="S0.F1.4.2" class="ltx_text ltx_font_bold">together</span>. Thus, the alternative decoys suggested our procedures are better designed to gauge how well a learning algorithm can understand all information equally well.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multimodal information processing tasks such as image captioning <cite class="ltx_cite ltx_citemacro_cite">Farhadi et al. (<a href="#bib.bib9" title="" class="ltx_ref">2010</a>); Ordonez et al. (<a href="#bib.bib30" title="" class="ltx_ref">2011</a>); Xu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2015</a>)</cite> and visual question answering (Visual QA) <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> have gained a lot of attention recently. A number of significant advances in learning algorithms have been made, along with the development of nearly two dozens of datasets in this very active research domain. Among those datasets, popular ones include MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>); Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, and several others. The overarching objective is that a learning machine needs to go beyond understanding different modalities of information separately (such as image recognition alone) and to learn how to correlate them in order to perform well on those tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To evaluate the progress on those complex and more AI-like tasks is however a challenging topic. For tasks involving language generation, developing an automatic evaluation metric is itself an open problem <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>); Kilickaya et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2016</a>); Kafle and Kanan (<a href="#bib.bib18" title="" class="ltx_ref">2017b</a>)</cite>. Thus, many efforts have concentrated on tasks such as <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">multiple-choice</em> Visual QA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>); Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>); Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> or selecting the best caption <cite class="ltx_cite ltx_citemacro_cite">Hodosh et al. (<a href="#bib.bib14" title="" class="ltx_ref">2013</a>); Hodosh and Hockenmaier (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>); Ding et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>); Lin and Parikh (<a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>, where the selection accuracy is a natural evaluation metric.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we study how to design high-quality multiple choices for the Visual QA task. In this task, the machine (or the human annotator) is presented with an image, a question and a list of candidate answers. The goal is to select the correct answer through a consistent understanding of the image, the question and each of the candidate answers. As in any multiple-choice based tests (such as GRE), designing what should be presented as negative answers — we refer them as <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">decoys</em> — is as important as deciding the questions to ask. We all have had the experience of exploiting the elimination strategy: <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">This question is easy — none of the three answers could be right so the remaining one must be correct!</em></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While a clever strategy for taking exams, such “shortcuts” prevent us from studying faithfully how different learning algorithms comprehend the meanings in images and languages (e.g., the quality of the embeddings of both images and languages in a semantic space). It has been noted that machines can achieve very high accuracies of selecting the correct answer without the visual input (i.e., the image), the question, or both <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>); Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. Clearly, the learning algorithms have overfit on incidental statistics in the datasets. For instance, if the decoy answers have rarely been used as the correct answers (to any questions), then the machine can rule out a decoy answer with a binary classifier that determines whether the answers are in the set of the correct answers — note that this classifier does not need to examine the image and it just needs to memorize the list of the correct answers in the training dataset. See Fig. <a href="#S0.F1" title="Figure 1 ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for an example, and Sect. <a href="#S3" title="3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for more and detailed analysis.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We focus on minimizing the impacts of exploiting such shortcuts. We suggest a set of principles for creating decoy answers. In light of the amount of human efforts in curating existing datasets for the Visual QA task, we propose two procedures that revise those datasets such that the decoy answers are better designed. In contrast to some earlier works, the procedures are fully automatic and do not incur additional human annotator efforts. We apply the procedures to revise both Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> and VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. Additionally, we create new multiple-choice based datasets from COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite> and the recently released VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> and Visual Genome datasets <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>. The one based on Visual Genome becomes the largest multiple-choice dataset for the Visual QA task, with more than one million image-question-candidate answers triplets.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the Visual QA task. In particular, we show that machines need to use all three information (image, questions and answers) to perform well — any missing information induces a large drop in performance. Furthermore, we show that humans dominate machines in the task. However, given the revised datasets are likely reflecting the true gap between the human and the machine understanding of multimodal information, we expect that advances in learning algorithms likely focus more on the task itself instead of overfitting to the idiosyncrasies in the datasets.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The rest of the paper is organized as follows. In Sect. <a href="#S2" title="2 Related Work ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we describe related work. In Sect. <a href="#S3" title="3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we analyze and discuss the design deficiencies in existing datasets. In Sect. <a href="#S4" title="4 Creating Better Visual QA Datasets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we describe our automatic procedures for remedying those deficiencies. In Sect. <a href="#S5" title="5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we conduct experiments and analysis. We conclude the paper in Sect. <a href="#S6" title="6 Conclusion ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Wu et al. <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> and Kafle and Kanan <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib18" title="" class="ltx_ref">2017b</a>)</cite> provide recent overviews of the status quo of the Visual QA task. There are about two dozens of datasets for the task. Most of them use real-world images, while some are based on synthetic ones. Usually, for each image, multiple questions and their corresponding answers are generated. This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed image annotations. We concentrate on
3 datasets: VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>. All of them use images from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2015</a>)</cite> provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy. <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">What decoy answers to use</em> is the focus of our work.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets. In Visual7W, the decoys are all human-generated plausible ones. Note that, humans generate those decoys by <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">only looking at the questions and the correct answers but <span id="S2.p3.1.1.1" class="ltx_text ltx_font_bold">not</span> the images</em>. Thus, the decoys might be unrelated to the corresponding images. A learning algorithm can potentially examine the image alone and be able to identify the correct answer.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In visual Madlibs, the questions are generated with a limited set of question templates and the detailed annotations (e.g., objects) of the images. Thus, similarly, a learning model can examine the image alone and deduce the correct answer.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">We propose automatic procedures to revise VQA and Visual7W (and to create new datasets based on COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>, VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>, and Visual Genome) such that the decoy generation is carefully orchestrated to prevent learning algorithms from exploiting the shortcuts in the datasets by overfitting on incidental statistics. In particular, our design goal is that a learning machine needs to understand all the 3 components of an image-question-candidate answers triplet in order to make the right choice — ignoring either one or two components will result in drastic degradation in performance.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Our work is inspired by the experiments in <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> where they observe that machines without looking at images or questions can still perform well on the Visual QA task. Others have also reported similar issues <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>); Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2016</a>); Johnson et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>); Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>); Kafle and Kanan (<a href="#bib.bib17" title="" class="ltx_ref">2017a</a>); Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, though not in the multiple-choice setting. Our work extends theirs by providing more detailed analysis <em id="S2.p6.1.1" class="ltx_emph ltx_font_italic">as well as automatic procedures</em> to remedy those design deficiencies.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Besides Visual QA, VisDial <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> and Ding et al. <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> also propose automatic ways to generate decoys for the tasks of multiple-choice visual captioning and dialog, respectively.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Recently, Lin and Parikh <cite class="ltx_cite ltx_citemacro_cite">Lin and Parikh (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> study active learning for Visual QA: i.e., how to select informative image-question pairs (for acquiring annotations) or image-question-answer triplets for machines to “learn” from. On the other hand, our work further focuses on designing better datasets for “evaluating” a machine.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Analysis of Decoy Answers’ Effects</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we examine in detail the dataset Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>, a popular choice for the Visual QA task. We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p">In multiple-choice Visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A. The set A contains a target T (the correct answer) and <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">K</annotation></semantics></math> decoys (incorrect answers) denoted by D. An IQA triplet is thus <math id="S3.p2.2.m2.6" class="ltx_Math" alttext="\{\text{I},\text{Q},\text{A}=\{\text{T},\text{D}_{1},\cdots,\text{D}_{K}\}\}" display="inline"><semantics id="S3.p2.2.m2.6a"><mrow id="S3.p2.2.m2.6.6.1" xref="S3.p2.2.m2.6.6.2.cmml"><mo stretchy="false" id="S3.p2.2.m2.6.6.1.2" xref="S3.p2.2.m2.6.6.2.cmml">{</mo><mrow id="S3.p2.2.m2.6.6.1.1" xref="S3.p2.2.m2.6.6.1.1.cmml"><mrow id="S3.p2.2.m2.6.6.1.1.4.2" xref="S3.p2.2.m2.6.6.1.1.4.1.cmml"><mtext id="S3.p2.2.m2.3.3" xref="S3.p2.2.m2.3.3a.cmml">I</mtext><mo id="S3.p2.2.m2.6.6.1.1.4.2.1" xref="S3.p2.2.m2.6.6.1.1.4.1.cmml">,</mo><mtext id="S3.p2.2.m2.4.4" xref="S3.p2.2.m2.4.4a.cmml">Q</mtext><mo id="S3.p2.2.m2.6.6.1.1.4.2.2" xref="S3.p2.2.m2.6.6.1.1.4.1.cmml">,</mo><mtext id="S3.p2.2.m2.5.5" xref="S3.p2.2.m2.5.5a.cmml">A</mtext></mrow><mo id="S3.p2.2.m2.6.6.1.1.3" xref="S3.p2.2.m2.6.6.1.1.3.cmml">=</mo><mrow id="S3.p2.2.m2.6.6.1.1.2.2" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml"><mo stretchy="false" id="S3.p2.2.m2.6.6.1.1.2.2.3" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml">{</mo><mtext id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1a.cmml">T</mtext><mo id="S3.p2.2.m2.6.6.1.1.2.2.4" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml">,</mo><msub id="S3.p2.2.m2.6.6.1.1.1.1.1" xref="S3.p2.2.m2.6.6.1.1.1.1.1.cmml"><mtext id="S3.p2.2.m2.6.6.1.1.1.1.1.2" xref="S3.p2.2.m2.6.6.1.1.1.1.1.2a.cmml">D</mtext><mn id="S3.p2.2.m2.6.6.1.1.1.1.1.3" xref="S3.p2.2.m2.6.6.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.p2.2.m2.6.6.1.1.2.2.5" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">⋯</mi><mo id="S3.p2.2.m2.6.6.1.1.2.2.6" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml">,</mo><msub id="S3.p2.2.m2.6.6.1.1.2.2.2" xref="S3.p2.2.m2.6.6.1.1.2.2.2.cmml"><mtext id="S3.p2.2.m2.6.6.1.1.2.2.2.2" xref="S3.p2.2.m2.6.6.1.1.2.2.2.2a.cmml">D</mtext><mi id="S3.p2.2.m2.6.6.1.1.2.2.2.3" xref="S3.p2.2.m2.6.6.1.1.2.2.2.3.cmml">K</mi></msub><mo stretchy="false" id="S3.p2.2.m2.6.6.1.1.2.2.7" xref="S3.p2.2.m2.6.6.1.1.2.3.cmml">}</mo></mrow></mrow><mo stretchy="false" id="S3.p2.2.m2.6.6.1.3" xref="S3.p2.2.m2.6.6.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.6b"><set id="S3.p2.2.m2.6.6.2.cmml" xref="S3.p2.2.m2.6.6.1"><apply id="S3.p2.2.m2.6.6.1.1.cmml" xref="S3.p2.2.m2.6.6.1.1"><eq id="S3.p2.2.m2.6.6.1.1.3.cmml" xref="S3.p2.2.m2.6.6.1.1.3"></eq><list id="S3.p2.2.m2.6.6.1.1.4.1.cmml" xref="S3.p2.2.m2.6.6.1.1.4.2"><ci id="S3.p2.2.m2.3.3a.cmml" xref="S3.p2.2.m2.3.3"><mtext id="S3.p2.2.m2.3.3.cmml" xref="S3.p2.2.m2.3.3">I</mtext></ci><ci id="S3.p2.2.m2.4.4a.cmml" xref="S3.p2.2.m2.4.4"><mtext id="S3.p2.2.m2.4.4.cmml" xref="S3.p2.2.m2.4.4">Q</mtext></ci><ci id="S3.p2.2.m2.5.5a.cmml" xref="S3.p2.2.m2.5.5"><mtext id="S3.p2.2.m2.5.5.cmml" xref="S3.p2.2.m2.5.5">A</mtext></ci></list><set id="S3.p2.2.m2.6.6.1.1.2.3.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2"><ci id="S3.p2.2.m2.1.1a.cmml" xref="S3.p2.2.m2.1.1"><mtext id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">T</mtext></ci><apply id="S3.p2.2.m2.6.6.1.1.1.1.1.cmml" xref="S3.p2.2.m2.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.6.6.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.2.m2.6.6.1.1.1.1.1.2a.cmml" xref="S3.p2.2.m2.6.6.1.1.1.1.1.2"><mtext id="S3.p2.2.m2.6.6.1.1.1.1.1.2.cmml" xref="S3.p2.2.m2.6.6.1.1.1.1.1.2">D</mtext></ci><cn type="integer" id="S3.p2.2.m2.6.6.1.1.1.1.1.3.cmml" xref="S3.p2.2.m2.6.6.1.1.1.1.1.3">1</cn></apply><ci id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">⋯</ci><apply id="S3.p2.2.m2.6.6.1.1.2.2.2.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.p2.2.m2.6.6.1.1.2.2.2.1.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2.2">subscript</csymbol><ci id="S3.p2.2.m2.6.6.1.1.2.2.2.2a.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2.2.2"><mtext id="S3.p2.2.m2.6.6.1.1.2.2.2.2.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2.2.2">D</mtext></ci><ci id="S3.p2.2.m2.6.6.1.1.2.2.2.3.cmml" xref="S3.p2.2.m2.6.6.1.1.2.2.2.3">𝐾</ci></apply></set></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.6c">\{\text{I},\text{Q},\text{A}=\{\text{T},\text{D}_{1},\cdots,\text{D}_{K}\}\}</annotation></semantics></math>. We use C to denote either the target or a decoy.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Visual QA models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We investigate how well a learning algorithm can perform when supplied with different modalities of information. We concentrate on the one hidden-layer MLP model proposed in <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>, which has achieved state-of-the-art results on the dataset Visual7W. The model computes a scoring function <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="f(c,i)" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.2.3.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.2.3.3.2" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.3.2.1" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">c</mi><mo id="S3.SS1.p1.1.m1.2.3.3.2.2" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.3.2.3" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><times id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.1"></times><ci id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2">𝑓</ci><interval closure="open" id="S3.SS1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.3.2"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑐</ci><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">f(c,i)</annotation></semantics></math></p>
<table id="A7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\displaystyle f(c,i)=\sigma(\bm{U}\max(0,\bm{W}g(c,i))+b)" display="inline"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml"><mrow id="S3.E1.m1.7.7.3" xref="S3.E1.m1.7.7.3.cmml"><mi id="S3.E1.m1.7.7.3.2" xref="S3.E1.m1.7.7.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.3.1" xref="S3.E1.m1.7.7.3.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.3.3.2" xref="S3.E1.m1.7.7.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.3.3.2.1" xref="S3.E1.m1.7.7.3.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">c</mi><mo id="S3.E1.m1.7.7.3.3.2.2" xref="S3.E1.m1.7.7.3.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.7.7.3.3.2.3" xref="S3.E1.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.2" xref="S3.E1.m1.7.7.2.cmml">=</mo><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.cmml"><mi id="S3.E1.m1.7.7.1.3" xref="S3.E1.m1.7.7.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.3.cmml">𝑼</mi><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">max</mi><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1a" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">0</mn><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml">𝑾</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.2.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">c</mi><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.2.cmml">+</mo><mi id="S3.E1.m1.7.7.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7"><eq id="S3.E1.m1.7.7.2.cmml" xref="S3.E1.m1.7.7.2"></eq><apply id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.3"><times id="S3.E1.m1.7.7.3.1.cmml" xref="S3.E1.m1.7.7.3.1"></times><ci id="S3.E1.m1.7.7.3.2.cmml" xref="S3.E1.m1.7.7.3.2">𝑓</ci><interval closure="open" id="S3.E1.m1.7.7.3.3.1.cmml" xref="S3.E1.m1.7.7.3.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑐</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑖</ci></interval></apply><apply id="S3.E1.m1.7.7.1.cmml" xref="S3.E1.m1.7.7.1"><times id="S3.E1.m1.7.7.1.2.cmml" xref="S3.E1.m1.7.7.1.2"></times><ci id="S3.E1.m1.7.7.1.3.cmml" xref="S3.E1.m1.7.7.1.3">𝜎</ci><apply id="S3.E1.m1.7.7.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"><plus id="S3.E1.m1.7.7.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2"></plus><apply id="S3.E1.m1.7.7.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2"></times><ci id="S3.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.3">𝑼</ci><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1"><max id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"></max><cn type="integer" id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">0</cn><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2">𝑾</ci><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3">𝑔</ci><interval closure="open" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.4.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑐</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑖</ci></interval></apply></apply></apply><ci id="S3.E1.m1.7.7.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\displaystyle f(c,i)=\sigma(\bm{U}\max(0,\bm{W}g(c,i))+b)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.7" class="ltx_p">over a candidate answer <math id="S3.SS1.p1.2.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p1.2.m1.1a"><mi id="S3.SS1.p1.2.m1.1.1" xref="S3.SS1.p1.2.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m1.1b"><ci id="S3.SS1.p1.2.m1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m1.1c">c</annotation></semantics></math> and the multimodal information <math id="S3.SS1.p1.3.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.3.m2.1a"><mi id="S3.SS1.p1.3.m2.1.1" xref="S3.SS1.p1.3.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m2.1b"><ci id="S3.SS1.p1.3.m2.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m2.1c">i</annotation></semantics></math>, where <math id="S3.SS1.p1.4.m3.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p1.4.m3.1a"><mi id="S3.SS1.p1.4.m3.1.1" xref="S3.SS1.p1.4.m3.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m3.1b"><ci id="S3.SS1.p1.4.m3.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m3.1c">g</annotation></semantics></math> is the joint feature of <math id="S3.SS1.p1.5.m4.2" class="ltx_Math" alttext="(c,i)" display="inline"><semantics id="S3.SS1.p1.5.m4.2a"><mrow id="S3.SS1.p1.5.m4.2.3.2" xref="S3.SS1.p1.5.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m4.2.3.2.1" xref="S3.SS1.p1.5.m4.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.5.m4.1.1" xref="S3.SS1.p1.5.m4.1.1.cmml">c</mi><mo id="S3.SS1.p1.5.m4.2.3.2.2" xref="S3.SS1.p1.5.m4.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.5.m4.2.2" xref="S3.SS1.p1.5.m4.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p1.5.m4.2.3.2.3" xref="S3.SS1.p1.5.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m4.2b"><interval closure="open" id="S3.SS1.p1.5.m4.2.3.1.cmml" xref="S3.SS1.p1.5.m4.2.3.2"><ci id="S3.SS1.p1.5.m4.1.1.cmml" xref="S3.SS1.p1.5.m4.1.1">𝑐</ci><ci id="S3.SS1.p1.5.m4.2.2.cmml" xref="S3.SS1.p1.5.m4.2.2">𝑖</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m4.2c">(c,i)</annotation></semantics></math> and <math id="S3.SS1.p1.6.m5.3" class="ltx_Math" alttext="\sigma(x)=1/(1+\exp(-x))" display="inline"><semantics id="S3.SS1.p1.6.m5.3a"><mrow id="S3.SS1.p1.6.m5.3.3" xref="S3.SS1.p1.6.m5.3.3.cmml"><mrow id="S3.SS1.p1.6.m5.3.3.3" xref="S3.SS1.p1.6.m5.3.3.3.cmml"><mi id="S3.SS1.p1.6.m5.3.3.3.2" xref="S3.SS1.p1.6.m5.3.3.3.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m5.3.3.3.1" xref="S3.SS1.p1.6.m5.3.3.3.1.cmml">​</mo><mrow id="S3.SS1.p1.6.m5.3.3.3.3.2" xref="S3.SS1.p1.6.m5.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.3.3.2.1" xref="S3.SS1.p1.6.m5.3.3.3.cmml">(</mo><mi id="S3.SS1.p1.6.m5.1.1" xref="S3.SS1.p1.6.m5.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.3.3.2.2" xref="S3.SS1.p1.6.m5.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.6.m5.3.3.2" xref="S3.SS1.p1.6.m5.3.3.2.cmml">=</mo><mrow id="S3.SS1.p1.6.m5.3.3.1" xref="S3.SS1.p1.6.m5.3.3.1.cmml"><mn id="S3.SS1.p1.6.m5.3.3.1.3" xref="S3.SS1.p1.6.m5.3.3.1.3.cmml">1</mn><mo id="S3.SS1.p1.6.m5.3.3.1.2" xref="S3.SS1.p1.6.m5.3.3.1.2.cmml">/</mo><mrow id="S3.SS1.p1.6.m5.3.3.1.1.1" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.1.1.1.2" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.6.m5.3.3.1.1.1.1" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.cmml"><mn id="S3.SS1.p1.6.m5.3.3.1.1.1.1.3" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.3.cmml">1</mn><mo id="S3.SS1.p1.6.m5.3.3.1.1.1.1.2" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.2.cmml">+</mo><mrow id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.6.m5.2.2" xref="S3.SS1.p1.6.m5.2.2.cmml">exp</mi><mo id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1a" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1a" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.2.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.p1.6.m5.3.3.1.1.1.3" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m5.3b"><apply id="S3.SS1.p1.6.m5.3.3.cmml" xref="S3.SS1.p1.6.m5.3.3"><eq id="S3.SS1.p1.6.m5.3.3.2.cmml" xref="S3.SS1.p1.6.m5.3.3.2"></eq><apply id="S3.SS1.p1.6.m5.3.3.3.cmml" xref="S3.SS1.p1.6.m5.3.3.3"><times id="S3.SS1.p1.6.m5.3.3.3.1.cmml" xref="S3.SS1.p1.6.m5.3.3.3.1"></times><ci id="S3.SS1.p1.6.m5.3.3.3.2.cmml" xref="S3.SS1.p1.6.m5.3.3.3.2">𝜎</ci><ci id="S3.SS1.p1.6.m5.1.1.cmml" xref="S3.SS1.p1.6.m5.1.1">𝑥</ci></apply><apply id="S3.SS1.p1.6.m5.3.3.1.cmml" xref="S3.SS1.p1.6.m5.3.3.1"><divide id="S3.SS1.p1.6.m5.3.3.1.2.cmml" xref="S3.SS1.p1.6.m5.3.3.1.2"></divide><cn type="integer" id="S3.SS1.p1.6.m5.3.3.1.3.cmml" xref="S3.SS1.p1.6.m5.3.3.1.3">1</cn><apply id="S3.SS1.p1.6.m5.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1"><plus id="S3.SS1.p1.6.m5.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.2"></plus><cn type="integer" id="S3.SS1.p1.6.m5.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.3">1</cn><apply id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1"><exp id="S3.SS1.p1.6.m5.2.2.cmml" xref="S3.SS1.p1.6.m5.2.2"></exp><apply id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1"><minus id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1"></minus><ci id="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m5.3.3.1.1.1.1.1.1.1.1.2">𝑥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m5.3c">\sigma(x)=1/(1+\exp(-x))</annotation></semantics></math>. The information <math id="S3.SS1.p1.7.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.7.m6.1a"><mi id="S3.SS1.p1.7.m6.1.1" xref="S3.SS1.p1.7.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m6.1b"><ci id="S3.SS1.p1.7.m6.1.1.cmml" xref="S3.SS1.p1.7.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m6.1c">i</annotation></semantics></math> can be null, the image (I) alone, the question (Q) alone, or the combination of both (I+Q).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">Given an IQA triplet, we use the penultimate layer of ResNet-200 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> as visual features to represent I and the average <span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_smallcaps">word2vec</span> embeddings <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite> as text features to represent Q and C. To form the joint feature <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="g(c,i)" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.3" xref="S3.SS1.p2.1.m1.2.3.cmml"><mi id="S3.SS1.p2.1.m1.2.3.2" xref="S3.SS1.p2.1.m1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.2.3.1" xref="S3.SS1.p2.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.p2.1.m1.2.3.3.2" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.3.2.1" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">c</mi><mo id="S3.SS1.p2.1.m1.2.3.3.2.2" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.3.2.3" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.3.cmml" xref="S3.SS1.p2.1.m1.2.3"><times id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.1"></times><ci id="S3.SS1.p2.1.m1.2.3.2.cmml" xref="S3.SS1.p2.1.m1.2.3.2">𝑔</ci><interval closure="open" id="S3.SS1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.3.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑐</ci><ci id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">g(c,i)</annotation></semantics></math>, we just concatenate the features together. The candidate <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="c\in\text{A}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">c</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">∈</mo><mtext id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3a.cmml">A</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><in id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></in><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑐</ci><ci id="S3.SS1.p2.2.m2.1.1.3a.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><mtext id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">A</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">c\in\text{A}</annotation></semantics></math> that has the highest <math id="S3.SS1.p2.3.m3.2" class="ltx_Math" alttext="f(c,i)" display="inline"><semantics id="S3.SS1.p2.3.m3.2a"><mrow id="S3.SS1.p2.3.m3.2.3" xref="S3.SS1.p2.3.m3.2.3.cmml"><mi id="S3.SS1.p2.3.m3.2.3.2" xref="S3.SS1.p2.3.m3.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.2.3.1" xref="S3.SS1.p2.3.m3.2.3.1.cmml">​</mo><mrow id="S3.SS1.p2.3.m3.2.3.3.2" xref="S3.SS1.p2.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.2.3.3.2.1" xref="S3.SS1.p2.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">c</mi><mo id="S3.SS1.p2.3.m3.2.3.3.2.2" xref="S3.SS1.p2.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.3.m3.2.2" xref="S3.SS1.p2.3.m3.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p2.3.m3.2.3.3.2.3" xref="S3.SS1.p2.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.2b"><apply id="S3.SS1.p2.3.m3.2.3.cmml" xref="S3.SS1.p2.3.m3.2.3"><times id="S3.SS1.p2.3.m3.2.3.1.cmml" xref="S3.SS1.p2.3.m3.2.3.1"></times><ci id="S3.SS1.p2.3.m3.2.3.2.cmml" xref="S3.SS1.p2.3.m3.2.3.2">𝑓</ci><interval closure="open" id="S3.SS1.p2.3.m3.2.3.3.1.cmml" xref="S3.SS1.p2.3.m3.2.3.3.2"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑐</ci><ci id="S3.SS1.p2.3.m3.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.2c">f(c,i)</annotation></semantics></math> score in prediction is selected as the model output.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">We use the standard training, validation, and test splits of Visual7W, where each contains 69,817, 28,020, and 42,031 examples respectively. Each question has 4 candidate answers. The parameters of <math id="S3.SS1.p3.1.m1.2" class="ltx_Math" alttext="f(c,i)" display="inline"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.3" xref="S3.SS1.p3.1.m1.2.3.cmml"><mi id="S3.SS1.p3.1.m1.2.3.2" xref="S3.SS1.p3.1.m1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.1.m1.2.3.1" xref="S3.SS1.p3.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.p3.1.m1.2.3.3.2" xref="S3.SS1.p3.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.2.3.3.2.1" xref="S3.SS1.p3.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">c</mi><mo id="S3.SS1.p3.1.m1.2.3.3.2.2" xref="S3.SS1.p3.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p3.1.m1.2.2" xref="S3.SS1.p3.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p3.1.m1.2.3.3.2.3" xref="S3.SS1.p3.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><apply id="S3.SS1.p3.1.m1.2.3.cmml" xref="S3.SS1.p3.1.m1.2.3"><times id="S3.SS1.p3.1.m1.2.3.1.cmml" xref="S3.SS1.p3.1.m1.2.3.1"></times><ci id="S3.SS1.p3.1.m1.2.3.2.cmml" xref="S3.SS1.p3.1.m1.2.3.2">𝑓</ci><interval closure="open" id="S3.SS1.p3.1.m1.2.3.3.1.cmml" xref="S3.SS1.p3.1.m1.2.3.3.2"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑐</ci><ci id="S3.SS1.p3.1.m1.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">f(c,i)</annotation></semantics></math> are learned by minimizing the binary logistic loss of predicting whether or not a candidate <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">c</annotation></semantics></math> is the target of an IQA triplet. Details are in Sect. <a href="#S5" title="5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and the Supplementary Material.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Information used</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Machine</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Human</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">random</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.0</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">A</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">52.9</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">I + A</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">62.4</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center">75.3</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Q + A</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">58.2</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center">36.4</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">I + Q + A</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">65.7</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b">88.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy of selecting the right answers out of 4 choices (%) on the Visual QA task on Visual7W.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysis results</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Machines find shortcuts</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Visual QA models ‣ 3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the performance of the learning models, together with the human studies we performed on a subset of 1,000 triplets (c.f. Sect. <a href="#S5" title="5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for details). There are a few interesting observations.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">First, in the row of “A” where only the candidate answers (and whether they are right or wrong) are used to train a learning model, the model performs significantly better than random guessing and humans (52.9% vs. 25%) — humans will deem each of the answers equally likely <em id="S3.SS2.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">without</em> looking at both the image and the question! Note that in this case, the information <math id="S3.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.1c">i</annotation></semantics></math> in eq. (<a href="#S3.E1" title="In 3.1 Visual QA models ‣ 3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) contains nothing. The model learns the specific statistics of the candidate answers in the dataset and exploits those.
Adding the information about the image (i.e., the row of “I+A”), the machine improves significantly and gets close to the performance when all information is used (62.4% vs. 65.7%). There is a weaker correlation between the question and the answers as “Q+A” improves over “A” only modestly. This is expected. In the Visual7W dataset, the decoys are generated by human annotators as plausible answers to the questions without being shown the images — thus, many decoy answers do not have visual groundings. For instance, a question of “what animal is running?” elicits equally likely answers such as “dog”, “tiger”, “lion”, or “cat”, while an image of a dog running in the park will immediately rule out all 3 but the “dog”, see Fig. <a href="#S0.F1" title="Figure 1 ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for a similar example. Thus, the performance of “I+A” implies that many IQA triplets can be solved by object, attribute or concept detection on the image, without understanding the questions. This is indeed the case also for humans — humans can achieve 75.3% by considering “I+A” and not “Q”. Note that the difference between machine and human on “I+A” are likely due to their difference in understanding visual information.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p3.1" class="ltx_p">Note that human improves significantly from “I+A” to “I+Q+A” with “Q” added, while the machine does so only marginally. The difference can be attributed to the difference in understanding the question and correlating with the answers between the two. Since each image corresponds to multiple questions or have multiple objects, solely relying on the image itself will not work well in principle. Such difference clearly indicates that in the Visual QA model, the language component is weak as the model cannot fully exploit the information in “Q”, making a smaller relative improvement 5.3% (from 62.4% to 65.7%) where humans improved relatively 17.4%.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Shortcuts are due to design deficiencies</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We probe deeper on how the decoy answers have impacted the performance of learning models.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">As explained above, the decoys are drawn from all plausible answers to a question, irrespective of whether they are visually grounded or not. We have also discovered that the targets (i.e., correct answers) are infrequently used as decoys.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.2" class="ltx_p">Specifically, among the 69,817 training samples, there are 19,503 unique correct answers and each one of them is used about 3.6 times as correct answers to a question. However, among all the <math id="S3.SS2.SSS0.Px2.p3.1.m1.2" class="ltx_Math" alttext="69,817\times 3\approx 210K" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.1.m1.2a"><mrow id="S3.SS2.SSS0.Px2.p3.1.m1.2.2" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.2.cmml"><mn id="S3.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml">69</mn><mo id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.2.cmml">,</mo><mrow id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.2.cmml">817</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.3.cmml">3</mn></mrow></mrow><mo id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.2" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.2.cmml">≈</mo><mrow id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.cmml"><mn id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.2" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.2.cmml">210</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.3" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.1.m1.2b"><apply id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2"><approx id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.2"></approx><list id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1"><cn type="integer" id="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1">69</cn><apply id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1"><times id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.2">817</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.1.1.1.3">3</cn></apply></list><apply id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3"><times id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.2">210</cn><ci id="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.2.2.3.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.1.m1.2c">69,817\times 3\approx 210K</annotation></semantics></math> decoys, each correct answer appears 7.2 times on average, far below a chance level of 10.7 times (<math id="S3.SS2.SSS0.Px2.p3.2.m2.2" class="ltx_Math" alttext="210K\div 19,503\approx 10.7" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.2.m2.2a"><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.2.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.cmml"><mn id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.2.cmml">210</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.3.cmml">K</mi></mrow><mo id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.1.cmml">÷</mo><mn id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.3.cmml">19</mn></mrow><mo id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.2.cmml">,</mo><mn id="S3.SS2.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml">503</mn></mrow><mo id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.2.cmml">≈</mo><mn id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.3.cmml">10.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.2.m2.2b"><apply id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2"><approx id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.2"></approx><list id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1"><apply id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1"><divide id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.1"></divide><apply id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2"><times id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.2">210</cn><ci id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.2.3">𝐾</ci></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.1.1.1.3">19</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1">503</cn></list><cn type="float" id="S3.SS2.SSS0.Px2.p3.2.m2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.2.2.3">10.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.2.m2.2c">210K\div 19,503\approx 10.7</annotation></semantics></math>). This disparity exists in the test samples too. Consequently, the following rule, computing each answer’s likelihood of being correct,</p>
<table id="A7.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle P(\text{correct}|\text{C})=" display="inline"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mtext id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2a.cmml">correct</mtext><mo fence="false" id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml">|</mo><mtext id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3a.cmml">C</mtext></mrow><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">=</mo><mi id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"></eq><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.2"></times><ci id="S3.Ex1.m1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.3">𝑃</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.2a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2"><mtext id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2">correct</mtext></ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3"><mtext id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">C</mtext></ci></apply></apply><csymbol cd="latexml" id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle P(\text{correct}|\text{C})=</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\displaystyle\begin{cases}0.5,&amp;\text{if C is never seen in training,}\\
\frac{\text{\# times C as target}}{\text{\# times C as target}+(\text{\# times C as decoys})/K},&amp;\text{otherwise,}\end{cases}" display="inline"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4a" xref="S3.E2.m1.4.5.1.cmml"><mo id="S3.E2.m1.4.4a.5" xref="S3.E2.m1.4.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E2.m1.4.4.4a" xref="S3.E2.m1.4.5.1.cmml"><mtr id="S3.E2.m1.4.4.4aa" xref="S3.E2.m1.4.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4ab" xref="S3.E2.m1.4.5.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.5.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">0.5</mn><mo id="S3.E2.m1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.4.5.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4ac" xref="S3.E2.m1.4.5.1.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.1a.cmml">if C is never seen in training,</mtext></mtd></mtr><mtr id="S3.E2.m1.4.4.4ad" xref="S3.E2.m1.4.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4ae" xref="S3.E2.m1.4.5.1.cmml"><mrow id="S3.E2.m1.3.3.3.3.1.1.3" xref="S3.E2.m1.3.3.3.3.1.1.1.cmml"><mfrac id="S3.E2.m1.3.3.3.3.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.cmml"><mtext id="S3.E2.m1.3.3.3.3.1.1.1.3" xref="S3.E2.m1.3.3.3.3.1.1.1.3a.cmml"># times C as target</mtext><mrow id="S3.E2.m1.3.3.3.3.1.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.cmml"><mtext id="S3.E2.m1.3.3.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.3.3.1.1.1.1.3a.cmml"># times C as target</mtext><mo id="S3.E2.m1.3.3.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.3.3.3.3.1.1.1.1.4" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.cmml"><mrow id="S3.E2.m1.3.3.3.3.1.1.1.1.4.2.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1a.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.3.3.1.1.1.1.4.2.2.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1a.cmml">(</mo><mtext id="S3.E2.m1.3.3.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1.cmml"># times C as decoys</mtext><mo stretchy="false" id="S3.E2.m1.3.3.3.3.1.1.1.1.4.2.2.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1a.cmml">)</mo></mrow><mo id="S3.E2.m1.3.3.3.3.1.1.1.1.4.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.1.cmml">/</mo><mi id="S3.E2.m1.3.3.3.3.1.1.1.1.4.3" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.3.cmml">K</mi></mrow></mrow></mfrac><mo id="S3.E2.m1.3.3.3.3.1.1.3.1" xref="S3.E2.m1.3.3.3.3.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4af" xref="S3.E2.m1.4.5.1.cmml"><mtext id="S3.E2.m1.4.4.4.4.2.1" xref="S3.E2.m1.4.4.4.4.2.1a.cmml">otherwise,</mtext></mtd></mtr></mtable></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.4a"><csymbol cd="latexml" id="S3.E2.m1.4.5.1.1.cmml" xref="S3.E2.m1.4.4a.5">cases</csymbol><cn type="float" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">0.5</cn><ci id="S3.E2.m1.2.2.2.2.2.1a.cmml" xref="S3.E2.m1.2.2.2.2.2.1"><mtext id="S3.E2.m1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1">if C is never seen in training,</mtext></ci><apply id="S3.E2.m1.3.3.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.3"><divide id="S3.E2.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.3"></divide><ci id="S3.E2.m1.3.3.3.3.1.1.1.3a.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3"># times C as target</mtext></ci><apply id="S3.E2.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1"><plus id="S3.E2.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2"></plus><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.3a.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.3"># times C as target</mtext></ci><apply id="S3.E2.m1.3.3.3.3.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4"><divide id="S3.E2.m1.3.3.3.3.1.1.1.1.4.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.1"></divide><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.1a.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.2.2"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1"># times C as decoys</mtext></ci><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.4.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.4.3">𝐾</ci></apply></apply></apply><ci id="S3.E2.m1.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.4.4.4.4.2.1"><mtext id="S3.E2.m1.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.2.1">otherwise,</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\displaystyle\begin{cases}0.5,&amp;\text{if C is never seen in training,}\\
\frac{\text{\# times C as target}}{\text{\# times C as target}+(\text{\# times C as decoys})/K},&amp;\text{otherwise,}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px2.p3.3" class="ltx_p">should perform well. Essentially, it measures how unbiased C is used as the target and the decoys. Indeed, it attains an accuracy of 48.73% on the test data, far better than the random guess and is close to the learning model using the answers’ information only (the “A” row in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Visual QA models ‣ 3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Good rules for designing decoys</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Based on our analysis, we summarize the following guidance rules to design decoys: (1) <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">Question only Unresolvable (QoU)</span>. The decoys need to be equally plausible to the question. Otherwise, machines can rely on the correlation between the question and candidate answers to tell the target from decoys, even without the images. Note that this is a principle that is being followed by most datasets. (2) <span id="S3.SS2.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_bold">Neutrality</span>. The decoys answers should be equally likely used as the correct answers. (3) <span id="S3.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_bold">Image only Unresolvable (IoU)</span>. The decoys need to be plausible to the image. That is, they should appear in the image, or there exist questions so that the decoys can be treated as targets to the image. Otherwise, Visual QA can be resolved by objects, attributes, or concepts detection in images, even without the questions.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p2.1" class="ltx_p">Ideally, each decoy in an IQA triplet should meet the three principles. <span id="S3.SS2.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">Neutrality</span> is comparably easier to achieve by <em id="S3.SS2.SSS0.Px3.p2.1.2" class="ltx_emph ltx_font_italic">reusing terms in the whole set of targets as decoys.</em> On the contrary, a decoy may hardly meet <span id="S3.SS2.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_bold">QoU</span> and <span id="S3.SS2.SSS0.Px3.p2.1.4" class="ltx_text ltx_font_bold">IoU</span> simultaneously<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>E.g., in Fig <a href="#S0.F1" title="Figure 1 ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, for the question “What vehicle is pictured?”, the only answer that meets both principles is “train”, which is the correct answer instead of being a decoy.</span></span></span>. However, as long as all decoys of an IQA triplet meet <span id="S3.SS2.SSS0.Px3.p2.1.5" class="ltx_text ltx_font_bold">Neutrality</span> and some meet <span id="S3.SS2.SSS0.Px3.p2.1.6" class="ltx_text ltx_font_bold">QoU</span> and others meet <span id="S3.SS2.SSS0.Px3.p2.1.7" class="ltx_text ltx_font_bold">IoU</span>, the triplet as a whole still achieves the three principles — a machine ignoring either images or questions will likely perform poorly.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Creating Better Visual QA Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe our approaches of remedying design deficiencies in the existing datasets for the Visual QA task. We introduce two automatic and widely-applicable procedures to create new decoys that can prevent learning models from exploiting incident statistics in the datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Methods</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Main ideas</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Our procedures operate on a dataset that already contains image-question-target (IQT) triplets, i.e., we do not assume it has decoys already. For instance, we have used our procedures to create a multiple-choice dataset from the Visual Genome dataset which has no decoy. We assume that each image in the dataset is coupled with “multiple” QT pairs, which is the case in nearly all the existing datasets. Given an IQT triplet (I, Q, T), we create two sets of decoy answers.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">QoU-decoys</span>. We search among all other triplets that have similar questions to Q. The <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">targets</span> of those triplets are then collected as the decoys for T. As the targets to similar questions are likely plausible for the question Q, QoU-decoys likely follow the rules of <span id="S4.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">Neutrality</span> and <span id="S4.I1.i1.p1.1.4" class="ltx_text ltx_font_bold">Question only Unresolvable (QoU)</span>. We compute the average <span id="S4.I1.i1.p1.1.5" class="ltx_text ltx_font_smallcaps">word2vec</span> <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite> to represent a question, and use the cosine similarity to measure the similarity between questions.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">IoU-decoys</span>. We collect the <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">targets</span> from other triplets of the <em id="S4.I1.i2.p1.1.3" class="ltx_emph ltx_font_italic">same</em> image to be the decoys for T. The resulting decoys thus definitely follow the rules of <span id="S4.I1.i2.p1.1.4" class="ltx_text ltx_font_bold">Neutrality</span> and <span id="S4.I1.i2.p1.1.5" class="ltx_text ltx_font_bold">Image only Unresolvable (IoU)</span>.</p>
</div>
</li>
</ul>
<p id="S4.SS1.SSS0.Px1.p1.2" class="ltx_p">We then combine the triplet (I, Q, T) with QoU-decoys and IoU-decoys to form an IQA triplet as a training or test sample.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Resolving ambiguous decoys</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">One potential drawback of automatically selected decoys is that they may be semantically similar, ambiguous, or rephrased terms to the target <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>. We utilize two filtering steps to alleviate it. First, we perform string matching between a decoy and the target, deleting those decoys that contain or are covered by the target (e.g., “daytime” vs. “during the daytime” and “ponytail” vs. “pony tail”).</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">Secondly, we utilize the WordNet hierarchy and the Wu-Palmer (WUP) score <cite class="ltx_cite ltx_citemacro_cite">Wu and Palmer (<a href="#bib.bib35" title="" class="ltx_ref">1994</a>)</cite> to eliminate semantically similar decoys. The WUP score measures how similar two <em id="S4.SS1.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">word senses</em> are (in the range of <math id="S4.SS1.SSS0.Px2.p2.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S4.SS1.SSS0.Px2.p2.1.m1.2a"><mrow id="S4.SS1.SSS0.Px2.p2.1.m1.2.3.2" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS0.Px2.p2.1.m1.2.3.2.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">0</mn><mo id="S4.SS1.SSS0.Px2.p2.1.m1.2.3.2.2" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px2.p2.1.m1.2.2" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S4.SS1.SSS0.Px2.p2.1.m1.2.3.2.3" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.1.m1.2b"><interval closure="closed" id="S4.SS1.SSS0.Px2.p2.1.m1.2.3.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1">0</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p2.1.m1.2.2.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.1.m1.2c">[0,1]</annotation></semantics></math>), based on the depth of them in the taxonomy and that of their least common subsumer. We compute the similarity of two strings according to the WUP scores in a similar manner to <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite>, in which the WUP score is used to evaluate Visual QA performance. We eliminate decoys that have higher WUP-based similarity to the target. We use the NLTK toolkit <cite class="ltx_cite ltx_citemacro_cite">Bird et al. (<a href="#bib.bib5" title="" class="ltx_ref">2009</a>)</cite> to compute the similarity. See the Supplementary Material for more details.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other details</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.2" class="ltx_p">For QoU-decoys, we sort and keep for each triplet the top <math id="S4.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">N</annotation></semantics></math> (e.g., 10,000) similar triplets from the entire dataset according to the question similarity. Then for each triplet, we compute the WUP-based similarity of each potential decoy to the target successively, and accept those with similarity below 0.9 until we have <math id="S4.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.2.m2.1c">K</annotation></semantics></math> decoys. We choose 0.9 according to <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite>. We also perform such a check among selected decoys to ensure they are not very similar to each other. For IoU-decoys, the potential decoys are sorted randomly. The WUP-based similarity with a threshold of 0.9 is then applied to remove ambiguous decoys.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison to other datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Several authors have noticed the design deficiencies in the existing databases and have proposed “fixes” <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>); Yu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2015</a>); Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>); Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. No dataset has used a procedure to generate IoU-decoys. We empirically show that how the IoU-decoys significantly remedy the design deficiencies in the datasets.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Several previous efforts have generated decoys that are similar in spirit to our <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">QoU</span>-decoys. Yu et al. <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2015</a>)</cite>, Das et al. <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, and Ding et al. <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">GloVe</span> embeddings <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite>, and paragraph vectors <cite class="ltx_cite ltx_citemacro_cite">Le and Mikolov (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> and linguistic surface similarity, respectively. The later two are for different tasks from Visual QA, and only Ding et al. <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> consider removing semantically ambiguous decoys like ours. Antol et al. <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> and Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> ask humans to create decoys, given the questions and targets. As shown earlier, such decoys may disobey the rule of <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_bold">Neutrality</span>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> augment the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> (by human efforts) with additional IQT triplets to eliminate the shortcuts (language prior) in the open-ended setting. Their effort is complementary to ours on the multiple-choice setting. Note that an extended task of Visual QA, visual dialog <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, also adopts the latter setting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Empirical Studies</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We examine our automatic procedures for creating decoys on five datasets. Table <a href="#S5.T2" title="Table 2 ‣ Creating decoys ‣ 5.1 Dataset ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the characteristics of the three datasets we focus on.</p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA Real <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>
</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">The dataset uses images from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite> under the same training/validation/testing splits to construct IQA triplets. Totally 614,163 IQA triplets are generated for 204,721 images.
Each question has 18 candidate answers: in general 3 decoys are human-generated, 4 are randomly sampled, and 10 are randomly sampled frequent-occurring targets. <em id="S5.SS1.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">As the test set does not indicate the targets, our studies focus on the training and validation sets.</em></p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual7W Telling (Visual7W) <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>
</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">The dataset uses 47,300 images from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite> and contains 139,868 IQA triplets. Each has 3 decoys generated by humans.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>
</h4>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">The dataset uses 101,174 images from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite> and contains 1,445,322 IQT triplets. No decoys are provided. Human annotators are asked to write diverse pairs of questions and answers freely about an image or with respect to some regions of it. On average an image is coupled with 14 question-answer pairs. We divide the dataset into non-overlapping 50%/20%/30% for training/validation/testing. Additionally, we partition such that each portion is a “superset” of the corresponding one in Visual7W, respectively.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> and COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>
</h4>

<div id="S5.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px4.p1.1" class="ltx_p">We describe the datasets and experimental results in the Supplementary Material.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Creating decoys</h4>

<div id="S5.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px5.p1.1" class="ltx_p">We create 3 QoU-decoys and 3 IoU-decoys for every IQT triplet in each dataset, following the steps in Sect. <a href="#S4.SS1" title="4.1 Methods ‣ 4 Creating Better Visual QA Datasets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. In the cases that we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for other datasets, we randomly include those from the top 10 frequently-occurring targets.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">Dataset</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="3"># of Images</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="3"># of triplets</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;"># of decoys</th>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">Name</th>
<th id="S5.T2.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">train</th>
<th id="S5.T2.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">val</th>
<th id="S5.T2.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">test</th>
<th id="S5.T2.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">train</th>
<th id="S5.T2.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">val</th>
<th id="S5.T2.1.2.2.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">test</th>
<th id="S5.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">per triplet</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.3.1" class="ltx_tr">
<td id="S5.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">VQA</td>
<td id="S5.T2.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">83k</td>
<td id="S5.T2.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">41k</td>
<td id="S5.T2.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">81k</td>
<td id="S5.T2.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">248k</td>
<td id="S5.T2.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">121k</td>
<td id="S5.T2.1.3.1.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">244k</td>
<td id="S5.T2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">17</td>
</tr>
<tr id="S5.T2.1.4.2" class="ltx_tr">
<td id="S5.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">Visual7W</td>
<td id="S5.T2.1.4.2.2" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">14k</td>
<td id="S5.T2.1.4.2.3" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">5k</td>
<td id="S5.T2.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">8k</td>
<td id="S5.T2.1.4.2.5" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">69k</td>
<td id="S5.T2.1.4.2.6" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">28k</td>
<td id="S5.T2.1.4.2.7" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">42k</td>
<td id="S5.T2.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">3</td>
</tr>
<tr id="S5.T2.1.5.3" class="ltx_tr">
<td id="S5.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">VG</td>
<td id="S5.T2.1.5.3.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:2.5pt;padding-right:2.5pt;">49k</td>
<td id="S5.T2.1.5.3.3" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:2.5pt;padding-right:2.5pt;">19k</td>
<td id="S5.T2.1.5.3.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">29k</td>
<td id="S5.T2.1.5.3.5" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:2.5pt;padding-right:2.5pt;">727k</td>
<td id="S5.T2.1.5.3.6" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:2.5pt;padding-right:2.5pt;">283k</td>
<td id="S5.T2.1.5.3.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">433k</td>
<td id="S5.T2.1.5.3.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.5pt;padding-right:2.5pt;">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of Visual QA datasets.</figcaption>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Setup</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual QA models</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">We utilize the MLP models mentioned in Sect. <a href="#S3" title="3 Analysis of Decoy Answers’ Effects ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for all the experiments. We denote <span id="S5.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">MLP-A</span>, <span id="S5.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold">MLP-QA</span>, <span id="S5.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_bold">MLP-IA</span>, <span id="S5.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_bold">MLP-IQA</span> as the models using A (Answers only), Q+A (Question plus Answers), I+A (Image plus Answers), and I+Q+A (Image, Question and Answers) for multimodal information, respectively. The hidden-layer has 8,192 neurons. We use a 200-layer ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> to compute visual features which are 2,048-dimensional. The ResNet is pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">Russakovsky et al. (<a href="#bib.bib33" title="" class="ltx_ref">2015</a>)</cite>. The <span id="S5.SS2.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_smallcaps">word2vec</span> feature <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite> for questions and answers are 300-dimensional, pre-trained on Google News<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We experiment on using different features in the Supplementary Material.</span></span></span>. The parameters of the MLP models are learned by minimizing the binary logistic loss of predicting whether or not a candidate answer is the target of the corresponding IQA triplet. Please see the Supplementary Material for details on optimization.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">We further experiment with a variant of the spatial memory network (denoted as <span id="S5.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">Attention</span>) <cite class="ltx_cite ltx_citemacro_cite">Xu and Saenko (<a href="#bib.bib36" title="" class="ltx_ref">2016</a>)</cite> and the <span id="S5.SS2.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_bold">HieCoAtt</span> model <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> adjusted for the multiple-choice setting. Both models utilize the attention mechanism. Details are listed in the Supplementary Material.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metric</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">For VQA and VQA2, we follow their protocols by comparing the picked answer to 10 human-generated targets. The accuracy is computed based on the number of exactly matched targets (divided by 3 and clipped at 1). For others, we compute the accuracy of picking the target from multiple choices.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Decoy sets to compare</h4>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p1.1" class="ltx_p">For each dataset, we derive several variants: (1) <span id="S5.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">Orig</span>: the original decoys from the datasets, (2) <span id="S5.SS2.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_bold">QoU</span>: <span id="S5.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_bold">Orig</span> replaced with ones selected by our QoU-decoys generating procedure, (3) <span id="S5.SS2.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_bold">IoU</span>: <span id="S5.SS2.SSS0.Px3.p1.1.5" class="ltx_text ltx_font_bold">Orig</span> replaced with ones selected by our IoU-decoys generating procedure, (4) <span id="S5.SS2.SSS0.Px3.p1.1.6" class="ltx_text ltx_font_bold">QoU +IoU</span>: <span id="S5.SS2.SSS0.Px3.p1.1.7" class="ltx_text ltx_font_bold">Orig</span> replaced with ones combining <span id="S5.SS2.SSS0.Px3.p1.1.8" class="ltx_text ltx_font_bold">QoU</span> and <span id="S5.SS2.SSS0.Px3.p1.1.9" class="ltx_text ltx_font_bold">IoU</span>, (5) <span id="S5.SS2.SSS0.Px3.p1.1.10" class="ltx_text ltx_font_bold">All</span>: combining <span id="S5.SS2.SSS0.Px3.p1.1.11" class="ltx_text ltx_font_bold">Orig</span>, <span id="S5.SS2.SSS0.Px3.p1.1.12" class="ltx_text ltx_font_bold">QoU</span>, and <span id="S5.SS2.SSS0.Px3.p1.1.13" class="ltx_text ltx_font_bold">IoU</span>.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">User studies</h4>

<div id="S5.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px4.p1.1" class="ltx_p">Automatic decoy generation may lead to ambiguous decoys as mentioned in Sect. <a href="#S4" title="4 Creating Better Visual QA Datasets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>. We conduct a user study via Amazon Mechanic Turk (AMT) to test humans’ performance on the datasets after they are remedied by our automatic procedures. We select 1,000 IQA triplets from each dataset. Each triplet is answered by three workers and in total 169 workers get involved. The total cost is $215 — the rate for every 20 triplets is $0.25. We report the average human performance and compare it to the learning models’. See the Supplementary Material for more details.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The performances of learning models and humans on the 3 datasets are reported in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S5.T4" title="Table 4 ‣ Effectiveness of new decoys ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and <a href="#S5.T5" title="Table 5 ‣ Differences across datasets ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We note that in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the 4.3% drop of the human performance on IoU +QoU, compared to Orig, is likely due to that IoU +QoU has more candidates (7 per question). Besides, the human performance on qaVG cannot be directly compared to that on the other datasets, since the questions on qaVG tend to focus on local image regions and are considered harder.</span></span></span>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.2.3.1" class="ltx_tr">
<th id="S5.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<td id="S5.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Orig</td>
<td id="S5.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S5.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</td>
<td id="S5.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</td>
<td id="S5.T3.2.3.1.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
</tr>
<tr id="S5.T3.2.4.2" class="ltx_tr">
<th id="S5.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="S5.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">52.9</td>
<td id="S5.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.0</td>
<td id="S5.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">34.1</td>
<td id="S5.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">17.7</td>
<td id="S5.T3.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15.6</td>
</tr>
<tr id="S5.T3.2.5.3" class="ltx_tr">
<th id="S5.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="S5.T3.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">62.4</td>
<td id="S5.T3.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">27.3</td>
<td id="S5.T3.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">55.0</td>
<td id="S5.T3.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">23.6</td>
<td id="S5.T3.2.5.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.2</td>
</tr>
<tr id="S5.T3.2.6.4" class="ltx_tr">
<th id="S5.T3.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="S5.T3.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">58.2</td>
<td id="S5.T3.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">84.1</td>
<td id="S5.T3.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">40.7</td>
<td id="S5.T3.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">37.8</td>
<td id="S5.T3.2.6.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.9</td>
</tr>
<tr id="S5.T3.2.7.5" class="ltx_tr">
<th id="S5.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="S5.T3.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">65.7</td>
<td id="S5.T3.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">84.1</td>
<td id="S5.T3.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">57.6</td>
<td id="S5.T3.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
<td id="S5.T3.2.7.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.1</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">HieCoAtt<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><times id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">63.9</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">51.5</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<th id="S5.T3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Attntion<math id="S5.T3.2.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T3.2.2.1.m1.1a"><mo id="S5.T3.2.2.1.m1.1.1" xref="S5.T3.2.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.m1.1b"><times id="S5.T3.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">65.9</td>
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">52.8</td>
<td id="S5.T3.2.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T3.2.8.6" class="ltx_tr">
<th id="S5.T3.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Human</th>
<td id="S5.T3.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">88.4</td>
<td id="S5.T3.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T3.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">84.1</td>
<td id="S5.T3.2.8.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T3.2.9.7" class="ltx_tr">
<th id="S5.T3.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="S5.T3.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T3.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T3.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T3.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
<td id="S5.T3.2.9.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.0</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T3.3" class="ltx_p ltx_figure_panel ltx_align_center"><math id="S5.T3.3.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T3.3.m1.1a"><mo id="S5.T3.3.m1.1.1" xref="S5.T3.3.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.m1.1b"><times id="S5.T3.3.m1.1.1.cmml" xref="S5.T3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.m1.1c">*</annotation></semantics></math>: based on our implementation or modification</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Test accuracy (%) on Visual7W.</figcaption>
</figure>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effectiveness of new decoys</h4>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">A better set of decoys will force learning models to integrate all 3 pieces of information — images, questions and answers — to make the correct selection from multiple-choices. In particular, they should prevent learning algorithms from exploiting shortcuts such that partial information is sufficient for performing well on the Visual QA task.</p>
</div>
<div id="S5.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p2.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> clearly indicates that those goals have been achieved. With the Orig decoys, the relatively small gain from MLP-IA to MLP-IQA suggests that the question information can be ignored to attain good performance. However, with the IoU-decoys which require questions to help to resolve (as image itself is inadequate to resolve), the gain is substantial (from 27.3% to 84.1%). Likewise, with the QoU-decoys (question itself is not adequate to resolve), including images information improves from 40.7% (MLP-QA) substantially to 57.6% (MLP-IQA). Note that with the Orig decoys, this gain is smaller (58.2% vs. 65.7%).</p>
</div>
<div id="S5.SS3.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p3.1" class="ltx_p">It is expected that MLP-IA matches better QoU-decoys but not IoU-decoys, and MLP-QA is the other way around. Thus it is natural to combine these two decoys. What is particularly appealing is that MLP-IQA improves noticeably over models learned with partial information on the combined IoU +QoU-decoys (and “All” decoys<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We note that the decoys in Orig are not trivial, which can be seen from the gap between All and IoU +QoU. Our main concern on Orig is that for those questions that machines can accurately answer, they mostly rely on only partial information. This will thus hinder designing machines to fully comprehend and reason from multimodal information. We further experiment on random decoys, which can achieve <span id="footnote4.1" class="ltx_text ltx_font_bold">Neutrality</span> but not the other two principles, to demonstrate the effectiveness of our methods in the Supplementary Material.</span></span></span>). Furthermore, using answer information only (MLP-A) attains about the chance-level accuracy.</p>
</div>
<div id="S5.SS3.SSS0.Px1.p4" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p4.1" class="ltx_p">On the VQA dataset (Table <a href="#S5.T4" title="Table 4 ‣ Effectiveness of new decoys ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), the same observations hold, though to a lesser degree. On any of the IoU or QoU columns, we observe substantial gains when the complementary information is added to the model (such as MLP-IA to MLP-IQA). All these improvements are much more visible than those observed on the original decoy sets.</p>
</div>
<div id="S5.SS3.SSS0.Px1.p5" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p5.1" class="ltx_p">Combining both Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.T4" title="Table 4 ‣ Effectiveness of new decoys ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we notice that the improvements from MLP-QA to MLP-IQA tend to be lower when facing IoU-decoys. This is also expected as it is difficult to have decoys that are simultaneously both IoU and QoU — such answers tend to be the target answers. Nonetheless, we deem this as a future direction to explore.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T4.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.3.4.1" class="ltx_tr">
<th id="S5.T4.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<td id="S5.T4.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Orig</td>
<td id="S5.T4.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S5.T4.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</td>
<td id="S5.T4.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</td>
<td id="S5.T4.3.4.1.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
</tr>
<tr id="S5.T4.3.5.2" class="ltx_tr">
<th id="S5.T4.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="S5.T4.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.2</td>
<td id="S5.T4.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">39.9</td>
<td id="S5.T4.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">45.7</td>
<td id="S5.T4.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.2</td>
<td id="S5.T4.3.5.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.4</td>
</tr>
<tr id="S5.T4.3.6.3" class="ltx_tr">
<th id="S5.T4.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="S5.T4.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">42.0</td>
<td id="S5.T4.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">39.8</td>
<td id="S5.T4.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">55.1</td>
<td id="S5.T4.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">34.1</td>
<td id="S5.T4.3.6.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.7</td>
</tr>
<tr id="S5.T4.3.7.4" class="ltx_tr">
<th id="S5.T4.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="S5.T4.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">58.0</td>
<td id="S5.T4.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">84.7</td>
<td id="S5.T4.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">55.1</td>
<td id="S5.T4.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">54.4</td>
<td id="S5.T4.3.7.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.0</td>
</tr>
<tr id="S5.T4.3.8.5" class="ltx_tr">
<th id="S5.T4.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="S5.T4.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">64.6</td>
<td id="S5.T4.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">85.2</td>
<td id="S5.T4.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">65.4</td>
<td id="S5.T4.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">63.7</td>
<td id="S5.T4.3.8.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">58.9</td>
</tr>
<tr id="S5.T4.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">HieCoAtt<math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><times id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">63.0</td>
<td id="S5.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">63.7</td>
<td id="S5.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T4.2.2" class="ltx_tr">
<th id="S5.T4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Attntion<math id="S5.T4.2.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T4.2.2.1.m1.1a"><mo id="S5.T4.2.2.1.m1.1.1" xref="S5.T4.2.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.1.m1.1b"><times id="S5.T4.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">66.0</td>
<td id="S5.T4.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">66.7</td>
<td id="S5.T4.2.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T4.3.3" class="ltx_tr">
<th id="S5.T4.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Human</th>
<td id="S5.T4.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">88.5<sup id="S5.T4.3.3.1.1" class="ltx_sup">†</sup>
</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T4.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">89.0</td>
<td id="S5.T4.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T4.3.9.6" class="ltx_tr">
<th id="S5.T4.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="S5.T4.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.6</td>
<td id="S5.T4.3.9.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T4.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T4.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
<td id="S5.T4.3.9.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.2</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T4.5" class="ltx_p ltx_figure_panel ltx_align_center"><math id="S5.T4.4.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T4.4.m1.1a"><mo id="S5.T4.4.m1.1.1" xref="S5.T4.4.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.m1.1b"><times id="S5.T4.4.m1.1.1.cmml" xref="S5.T4.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.m1.1c">*</annotation></semantics></math>: based on our implementation or modification
<br class="ltx_break"><sup id="S5.T4.5.1" class="ltx_sup">†</sup>: taken from <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy (%) on the validation set in VQA.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Differences across datasets</h4>

<div id="S5.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px2.p1.1" class="ltx_p">Contrasting Visual7W to VQA (on the column IoU +QoU), we notice that Visual7W tends to have bigger improvements in general. This is due to the fact that VQA has many questions with “Yes” or “No” as the targets — the only valid decoy to the target “Yes” is “No”, and vice versa. As such decoys are already captured by Orig of VQA (‘Yes” and “No” are both top frequently-occurring targets), adding other decoy answers will not make any noticeable improvement. In Supplementary Material, however, we show that once we remove such questions/answers pairs, the degree of improvements increases substantially.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T5.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.2.3.1" class="ltx_tr">
<th id="S5.T5.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<td id="S5.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S5.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</td>
<td id="S5.T5.2.3.1.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</td>
</tr>
<tr id="S5.T5.2.4.2" class="ltx_tr">
<th id="S5.T5.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="S5.T5.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">29.1</td>
<td id="S5.T5.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">36.2</td>
<td id="S5.T5.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">19.5</td>
</tr>
<tr id="S5.T5.2.5.3" class="ltx_tr">
<th id="S5.T5.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="S5.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">29.5</td>
<td id="S5.T5.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">60.2</td>
<td id="S5.T5.2.5.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.2</td>
</tr>
<tr id="S5.T5.2.6.4" class="ltx_tr">
<th id="S5.T5.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="S5.T5.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">89.3</td>
<td id="S5.T5.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">45.6</td>
<td id="S5.T5.2.6.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">43.9</td>
</tr>
<tr id="S5.T5.2.7.5" class="ltx_tr">
<th id="S5.T5.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="S5.T5.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">89.2</td>
<td id="S5.T5.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">64.3</td>
<td id="S5.T5.2.7.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">58.5</td>
</tr>
<tr id="S5.T5.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">HieCoAtt<math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><times id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">57.5</td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<th id="S5.T5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Attntion<math id="S5.T5.2.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T5.2.2.1.m1.1a"><mo id="S5.T5.2.2.1.m1.1.1" xref="S5.T5.2.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.m1.1b"><times id="S5.T5.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">60.1</td>
</tr>
<tr id="S5.T5.2.8.6" class="ltx_tr">
<th id="S5.T5.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Human</th>
<td id="S5.T5.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T5.2.8.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">82.5</td>
</tr>
<tr id="S5.T5.2.9.7" class="ltx_tr">
<th id="S5.T5.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="S5.T5.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T5.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S5.T5.2.9.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T5.3" class="ltx_p ltx_figure_panel ltx_align_center"><math id="S5.T5.3.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S5.T5.3.m1.1a"><mo id="S5.T5.3.m1.1.1" xref="S5.T5.3.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.m1.1b"><times id="S5.T5.3.m1.1.1.cmml" xref="S5.T5.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.m1.1c">*</annotation></semantics></math>: based on our implementation or modification</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Test accuracy (%) on qaVG.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison on Visual QA models</h4>

<div id="S5.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p1.1" class="ltx_p">As presented in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.T4" title="Table 4 ‣ Effectiveness of new decoys ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, MLP-IQA is on par with or even outperforms <span id="S5.SS3.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">Attention</span> and <span id="S5.SS3.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_bold">HieCoAtt</span> on the Orig decoys, showing how the shortcuts make it difficult to compare different models. By eliminating the shortcuts (i.e., on the combined IoU +QoU-decoys), the advantage of using sophisticated models becomes obvious (<span id="S5.SS3.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_bold">Attention</span> outperforms MLP-IQA by 3% in Table <a href="#S5.T4" title="Table 4 ‣ Effectiveness of new decoys ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), indicating the importance to design advanced models for achieving human-level performance on Visual QA.</p>
</div>
<div id="S5.SS3.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p2.1" class="ltx_p">For completeness, we include the results on the Visual Genome dataset in Table <a href="#S5.T5" title="Table 5 ‣ Differences across datasets ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This dataset has no “Orig” decoys, and we have created a multiple-choice based dataset <span id="S5.SS3.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">qaVG</span> from it for the task — it has over 1 million triplets, the largest dataset on this task to our knowledge. On the combined IoU +QoU-decoys, we again clearly see that machines need to use all the information to succeed.</p>
</div>
<div id="S5.SS3.SSS0.Px3.p3" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p3.1" class="ltx_p">With <span id="S5.SS3.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_bold">qaVG</span>, we also investigate whether it can help improve the multiple-choice performances on the other two datasets. We use the MLP-IQA trained on qaVG with both IoU and QoU decoys to initialize the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets. As shown in Table <a href="#S5.T6" title="Table 6 ‣ Comparison on Visual QA models ‣ 5.3 Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, fine-tuning largely improves the performance, justifying the finding by Fukui et al. <cite class="ltx_cite ltx_citemacro_cite">Fukui et al. (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Datasets</th>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Decoys</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Best w/o</td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">qaVG model</td>
</tr>
<tr id="S5.T6.1.2.2" class="ltx_tr">
<th id="S5.T6.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<td id="S5.T6.1.2.2.2" class="ltx_td ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td id="S5.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">using qaVG</td>
<td id="S5.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">initial</td>
<td id="S5.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">fine-tuned</td>
</tr>
<tr id="S5.T6.1.3.3" class="ltx_tr">
<th id="S5.T6.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="3"><span id="S5.T6.1.3.3.1.1" class="ltx_text">Visual7W</span></th>
<td id="S5.T6.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">Orig</td>
<td id="S5.T6.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">65.7</td>
<td id="S5.T6.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">60.5</td>
<td id="S5.T6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.3.3.5.1" class="ltx_text ltx_font_bold">69.1</span></td>
</tr>
<tr id="S5.T6.1.4.4" class="ltx_tr">
<td id="S5.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">IoU +QoU</td>
<td id="S5.T6.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">52.0</td>
<td id="S5.T6.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">58.1</td>
<td id="S5.T6.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.4.4.4.1" class="ltx_text ltx_font_bold">58.7</span></td>
</tr>
<tr id="S5.T6.1.5.5" class="ltx_tr">
<td id="S5.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">All</td>
<td id="S5.T6.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">45.1</td>
<td id="S5.T6.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">48.9</td>
<td id="S5.T6.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.5.5.4.1" class="ltx_text ltx_font_bold">51.0</span></td>
</tr>
<tr id="S5.T6.1.6.6" class="ltx_tr">
<th id="S5.T6.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="3"><span id="S5.T6.1.6.6.1.1" class="ltx_text">VQA</span></th>
<td id="S5.T6.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">Orig</td>
<td id="S5.T6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">64.6</td>
<td id="S5.T6.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">42.2</td>
<td id="S5.T6.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.6.6.5.1" class="ltx_text ltx_font_bold">65.6</span></td>
</tr>
<tr id="S5.T6.1.7.7" class="ltx_tr">
<td id="S5.T6.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">IoU +QoU</td>
<td id="S5.T6.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">63.7</td>
<td id="S5.T6.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">47.9</td>
<td id="S5.T6.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.7.7.4.1" class="ltx_text ltx_font_bold">64.1</span></td>
</tr>
<tr id="S5.T6.1.8.8" class="ltx_tr">
<td id="S5.T6.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">All</td>
<td id="S5.T6.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">58.9</td>
<td id="S5.T6.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">37.5</td>
<td id="S5.T6.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T6.1.8.8.4.1" class="ltx_text ltx_font_bold">59.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Using models trained on qaVG to improve Visual7W and VQA (Accuracy in %).</figcaption>
</figure>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Qualitative Results</h3>

<figure id="S5.F2" class="ltx_figure"><img src="/html/1704.07121/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="472" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example image-question-target triplets from Visual7W, VQA, and VG, together with our IoU-decoys (A, B, C.) and QoU-decoys (D, E, F). G is the target. Machine’s selections are denoted by green ticks (correct) or red crosses (wrong).</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/1704.07121/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="232" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Ambiguous examples by our IoU-decoys (A, B, C) and QoU-decoys (D, E, F). G is the target. Ambiguous decoys F are marked.</figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In Fig. <a href="#S5.F2" title="Figure 2 ‣ 5.4 Qualitative Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present examples of image-question-target triplets from <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_bold">V7W</span>, <span id="S5.SS4.p1.1.2" class="ltx_text ltx_font_bold">VQA</span>, and <span id="S5.SS4.p1.1.3" class="ltx_text ltx_font_bold">VG</span>, together with our IoU-decoys (A, B, C) and QoU-decoys (D, E, F). G is the target. The predictions by the corresponding MLP-IQA are also included. Ignoring information from images or questions makes it extremely challenging to answer the triplet correctly, even for humans.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Our automatic procedures do fail at some triplets, resulting in ambiguous decoys to the targets. See Fig. <a href="#S5.F3" title="Figure 3 ‣ 5.4 Qualitative Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for examples. We categorized those failure cases into two situations.</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Our filtering steps in Sect. <a href="#S4" title="4 Creating Better Visual QA Datasets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> fail, as observed in the top example. The WUP-based similarity relies on the WordNet hierarchy. For some semantically similar words like “lady” and “woman”, the similarity is only 0.632, much lower than that of 0.857 between “cat” and “dog”. This issue can be alleviated by considering alternative semantic measures by <span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">word2vec</span> or by those used in <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>); Ding et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> for searching similar questions.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">The question is ambiguous to answer. In the bottom example in Fig. <a href="#S5.F3" title="Figure 3 ‣ 5.4 Qualitative Results ‣ 5 Empirical Studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, both candidates D and F seem valid as a target. Another representative case is when asked about the background of a image. In images that contain sky and mountains in the distance, both terms can be valid.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We perform detailed analysis on existing datasets for multiple-choice Visual QA. We found that the design of decoys can inadvertently provide “shortcuts” for machines to exploit to perform well on the task. We describe several principles of constructing good decoys and propose automatic procedures to remedy existing datasets and create new ones. We conduct extensive empirical studies to demonstrate the effectiveness of our methods in creating better Visual QA datasets. The remedied datasets and the newly created ones are released and available at <a target="_blank" href="http://www.teds.usc.edu/website_vqa/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.teds.usc.edu/website_vqa/</a>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is partially supported by USC Graduate Fellowship, NSF IIS-1065243, 1451412, 1513966/1632803, 1208500, CCF-1139148, a Google Research Award, an Alfred. P. Sloan Research Fellowship and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2016)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2016)</span>
<span class="ltx_bibblock">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.

</span>
<span class="ltx_bibblock">Spice: Semantic propositional image caption evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bird et al. (2009)</span>
<span class="ltx_bibblock">
Steven Bird, Ewan Klein, and Edward Loper. 2009.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Natural language processing with Python: analyzing text with the
natural language toolkit</span>.

</span>
<span class="ltx_bibblock">” O’Reilly Media, Inc.”.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick. 2015.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1504.00325</span> .

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,
José MF Moura, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock">Visual dialog.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2016)</span>
<span class="ltx_bibblock">
Nan Ding, Sebastian Goodman, Fei Sha, and Radu Soricut. 2016.

</span>
<span class="ltx_bibblock">Understanding image and text simultaneously: a dual vision-language
machine comprehension task.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.07833</span> .

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farhadi et al. (2010)</span>
<span class="ltx_bibblock">
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus
Rashtchian, Julia Hockenmaier, and David Forsyth. 2010.

</span>
<span class="ltx_bibblock">Every picture tells a story: Generating sentences from images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al. (2016)</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach. 2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodosh and Hockenmaier (2016)</span>
<span class="ltx_bibblock">
Micah Hodosh and Julia Hockenmaier. 2016.

</span>
<span class="ltx_bibblock">Focused evaluation for image description with binary forced-choice
tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ACL Workshop</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodosh et al. (2013)</span>
<span class="ltx_bibblock">
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013.

</span>
<span class="ltx_bibblock">Framing image description as a ranking task: Data, models and
evaluation metrics.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Journal of Artificial Intelligence Research</span> .

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri et al. (2016)</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens van der Maaten. 2016.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017a)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017a.

</span>
<span class="ltx_bibblock">An analysis of visual question answering algorithms.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017b)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017b.

</span>
<span class="ltx_bibblock">Visual question answering: Datasets, algorithms, and future
challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span> .

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kilickaya et al. (2017)</span>
<span class="ltx_bibblock">
Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, and Erkut Erdem. 2017.

</span>
<span class="ltx_bibblock">Re-evaluating automatic metrics for image captioning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">EACL</span>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma,
Michael Bernstein, and Li Fei-Fei. 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IJCV</span> .

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Mikolov (2014)</span>
<span class="ltx_bibblock">
Quoc V Le and Tomas Mikolov. 2014.

</span>
<span class="ltx_bibblock">Distributed representations of sentences and documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">ICML</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Parikh (2016)</span>
<span class="ltx_bibblock">
Xiao Lin and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Leveraging visual question answering for image-caption ranking.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Parikh (2017)</span>
<span class="ltx_bibblock">
Xiao Lin and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Active learning for visual question answering: An empirical study.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.01732</span> .

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2016)</span>
<span class="ltx_bibblock">
Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin,
and Joelle Pineau. 2016.

</span>
<span class="ltx_bibblock">How not to evaluate your dialogue system: An empirical study of
unsupervised evaluation metrics for dialogue response generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCann et al. (2017)</span>
<span class="ltx_bibblock">
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Learned in translation: Contextualized word vectors.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. (2011)</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. 2011.

</span>
<span class="ltx_bibblock">Im2text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2015)</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015.

</span>
<span class="ltx_bibblock">ImageNet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">IJCV</span> .

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2017)</span>
<span class="ltx_bibblock">
Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den
Hengel. 2017.

</span>
<span class="ltx_bibblock">Visual question answering: A survey of methods and datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span> .

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Palmer (1994)</span>
<span class="ltx_bibblock">
Zhibiao Wu and Martha Palmer. 1994.

</span>
<span class="ltx_bibblock">Verbs semantics and lexical selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ACL</span>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Saenko (2016)</span>
<span class="ltx_bibblock">
Huijuan Xu and Kate Saenko. 2016.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2015)</span>
<span class="ltx_bibblock">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan
Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">ICML</span>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2015)</span>
<span class="ltx_bibblock">
Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. 2015.

</span>
<span class="ltx_bibblock">Visual madlibs: Fill in the blank description generation and question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2016)</span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2016.

</span>
<span class="ltx_bibblock">Yin and yang: Balancing and answering binary visual questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text ltx_font_bold">Supplementary Material</span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">In this Supplementary Material, we provide details omitted in the main text.</p>
<ul id="A0.I1" class="ltx_itemize">
<li id="A0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i1.p1" class="ltx_para">
<p id="A0.I1.i1.p1.1" class="ltx_p">Sect. <a href="#A1" title="Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>: Details on the MLP-based models and the attention-based models (Sect. 3.1 and 5.2 of the main text).</p>
</div>
</li>
<li id="A0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i2.p1" class="ltx_para">
<p id="A0.I1.i2.p1.1" class="ltx_p">Sect. <a href="#A2" title="Appendix B WUP-based similarity for filtering out ambiguous decoys ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>: WUPS-based similarity for filtering out ambiguous decoys (Sect. 4.1 of the main text).</p>
</div>
</li>
<li id="A0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i3.p1" class="ltx_para">
<p id="A0.I1.i3.p1.1" class="ltx_p">Sect. <a href="#A3" title="Appendix C Detailed results on VQA w/o QA pairs that have Yes/No as the targets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>: Detailed results on VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> w/o question-answer (QA) pairs that have Yes/No as the targets (Sect. 5.3 of the main text).</p>
</div>
</li>
<li id="A0.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i4.p1" class="ltx_para">
<p id="A0.I1.i4.p1.1" class="ltx_p">Sect. <a href="#A4" title="Appendix D More experimental results on VQA2 and COCOQA ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>: Experiments on VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> and COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite> (Sect. 5 of the main text).</p>
</div>
</li>
<li id="A0.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i5.p1" class="ltx_para">
<p id="A0.I1.i5.p1.1" class="ltx_p">Sect. <a href="#A5" title="Appendix E Details on user studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>: Details on user studies (Sect. 5.2 of the main text).</p>
</div>
</li>
<li id="A0.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i6.p1" class="ltx_para">
<p id="A0.I1.i6.p1.1" class="ltx_p">Sect. <a href="#A6" title="Appendix F Analysis on different question and answer embeddings ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>: Analysis on different question and answer embeddings (Sect. 5.2 of the main text).</p>
</div>
</li>
<li id="A0.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i7.p1" class="ltx_para">
<p id="A0.I1.i7.p1.1" class="ltx_p">Sect. <a href="#A7" title="Appendix G Analysis on random decoys ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>: Analysis on random decoys (Sect. 5.3 of the main text).</p>
</div>
</li>
</ul>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details on the MLP-based models and the attention-based models</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">As mentioned in the main text, we benchmark the performance of popular Visual QA models on our remedied dataset. Here we provide the details about those models we experimented and its corresponding training configurations.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>The simple MLP-based model</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">The one hidden-layer MLP model used in our experiments has 8,192 hidden units, exactly following <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>. It contains a batch normalization layer before ReLU, and a dropout layer after ReLU. We set the dropout rate to be 0.5.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.2" class="ltx_p">The input to the model is the concatenated features of images, questions, and answers, as shown in Fig. <a href="#A1.F4" title="Figure 4 ‣ A.1 The simple MLP-based model ‣ Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We change all characters to lowercases and all integer numbers within <math id="A1.SS1.p2.1.m1.2" class="ltx_Math" alttext="[0,10]" display="inline"><semantics id="A1.SS1.p2.1.m1.2a"><mrow id="A1.SS1.p2.1.m1.2.3.2" xref="A1.SS1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="A1.SS1.p2.1.m1.2.3.2.1" xref="A1.SS1.p2.1.m1.2.3.1.cmml">[</mo><mn id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml">0</mn><mo id="A1.SS1.p2.1.m1.2.3.2.2" xref="A1.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="A1.SS1.p2.1.m1.2.2" xref="A1.SS1.p2.1.m1.2.2.cmml">10</mn><mo stretchy="false" id="A1.SS1.p2.1.m1.2.3.2.3" xref="A1.SS1.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.2b"><interval closure="closed" id="A1.SS1.p2.1.m1.2.3.1.cmml" xref="A1.SS1.p2.1.m1.2.3.2"><cn type="integer" id="A1.SS1.p2.1.m1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1">0</cn><cn type="integer" id="A1.SS1.p2.1.m1.2.2.cmml" xref="A1.SS1.p2.1.m1.2.2">10</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.2c">[0,10]</annotation></semantics></math> to words before computing <span id="A1.SS1.p2.2.1" class="ltx_text ltx_font_smallcaps">word2vec</span>. We perform <math id="A1.SS1.p2.2.m2.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="A1.SS1.p2.2.m2.1a"><msub id="A1.SS1.p2.2.m2.1.1" xref="A1.SS1.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="A1.SS1.p2.2.m2.1.1.2" xref="A1.SS1.p2.2.m2.1.1.2.cmml">ℓ</mi><mn id="A1.SS1.p2.2.m2.1.1.3" xref="A1.SS1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.2.m2.1b"><apply id="A1.SS1.p2.2.m2.1.1.cmml" xref="A1.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS1.p2.2.m2.1.1.1.cmml" xref="A1.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="A1.SS1.p2.2.m2.1.1.2.cmml" xref="A1.SS1.p2.2.m2.1.1.2">ℓ</ci><cn type="integer" id="A1.SS1.p2.2.m2.1.1.3.cmml" xref="A1.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.2.m2.1c">\ell_{2}</annotation></semantics></math> normalization to features of each information before concatenation.</p>
</div>
<figure id="A1.F4" class="ltx_figure"><img src="/html/1704.07121/assets/x4.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="235" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of MLP-based models.</figcaption>
</figure>
<figure id="A1.F5" class="ltx_figure">
<table id="A1.F5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.F5.2.2" class="ltx_tr">
<td id="A1.F5.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/1704.07121/assets/x5.png" id="A1.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="235" height="156" alt="Refer to caption"></td>
<td id="A1.F5.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><img src="/html/1704.07121/assets/x6.png" id="A1.F5.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="235" height="156" alt="Refer to caption"></td>
</tr>
<tr id="A1.F5.2.3.1" class="ltx_tr">
<td id="A1.F5.2.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">(a) Attention* (SMem + MLP)</td>
<td id="A1.F5.2.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">(b) HieCoAtt* (HieCoAtt + MLP)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of attention-based Visual QA models.</figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>A variant of SMem (Attention*)</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">In the main text we experiment with a straightforward attention model similar to the spatial memory network (SMem) <cite class="ltx_cite ltx_citemacro_cite">Xu and Saenko (<a href="#bib.bib36" title="" class="ltx_ref">2016</a>)</cite>, as shown in Fig. <a href="#A1.F5" title="Figure 5 ‣ A.1 The simple MLP-based model ‣ Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a). Instead of computing the visual attention for each word in the question, we directly compute the visual attention for the entire question using its average <span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">word2vec</span> embedding. We then concatenate the resulting visual features with the feature of the question and a candidate answer (in the same way as the MLP-based model in Sect. <a href="#A1.SS1" title="A.1 The simple MLP-based model ‣ Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>) as the input to train a one-hidden-layer MLP for binary classification.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>A variant of HieCoAtten (HieCoAtten*)</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">Beyond the Attention*, we also experimented HieCoAtt*, a variant of the model proposed by Lu et al. <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> (as shown in Fig. <a href="#A1.F5" title="Figure 5 ‣ A.1 The simple MLP-based model ‣ Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b)). Our model inherits all components in <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> that are related to computing the joint multi-modal embedding (from images and questions). To adapt to the multiple-choice setting, we discard the multi-way classifier in the original HieCoAtt and use its penultimate activations as feature for images. Similarly, we then concatenate this together with the features of questions and candidate answers, and input it to a one-hidden-layer MLP, following exact the configuration as Sect. <a href="#A1.SS1" title="A.1 The simple MLP-based model ‣ Appendix A Details on the MLP-based models and the attention-based models ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Optimization</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.2" class="ltx_p">We train all our models using stochastic gradient based optimization method with mini-batch size of 100, momentum of 0.9, and the stepped learning rate policy: the learning rate is divided by 10 after every <math id="A1.SS4.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="A1.SS4.p1.1.m1.1a"><mi id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><ci id="A1.SS4.p1.1.m1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">M</annotation></semantics></math> mini-batches. We set the initial learning rate to be 0.01 (we further consider 0.001 for the case of fine-tuning in Sect. 5.3 of the main text). For each model, we train with at most 600,000 iterations. We treat <math id="A1.SS4.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="A1.SS4.p1.2.m2.1a"><mi id="A1.SS4.p1.2.m2.1.1" xref="A1.SS4.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.2.m2.1b"><ci id="A1.SS4.p1.2.m2.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.2.m2.1c">M</annotation></semantics></math> and the number of iterations as hyper-parameters of training. We tune the hyper-parameters on the validation set.</p>
</div>
<div id="A1.SS4.p2" class="ltx_para">
<p id="A1.SS4.p2.1" class="ltx_p">Within each mini-batch, we sample 100 IQA triplets. For each triplet, we randomly choose to use QoU-decoys or IoU-decoys when training on IoU +QoU, or QoU-decoys or IoU-decoys or Orig when training on All. We then take the target and <span id="A1.SS4.p2.1.1" class="ltx_text ltx_font_bold">3</span> decoys for each triplet to train the binary classifier (i.e., minimize the logistic loss). Specifically on VQA, which has 17 Orig decoys for a triplet, we randomly choose 3 decoys out of them. That is, 100 triplets in the mini-batch corresponds to 400 examples with binary labels. This procedure is to prevent <em id="A1.SS4.p2.1.2" class="ltx_emph ltx_font_italic">unbalanced training</em>, where machines simply learn to predict the dominant label, as suggested by Jabri et al. <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="A1.SS4.p3" class="ltx_para">
<p id="A1.SS4.p3.1" class="ltx_p">We note that in all the experiments in the main text, we use the <em id="A1.SS4.p3.1.1" class="ltx_emph ltx_font_italic">same type of decoy sets for training and testing</em>.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>WUP-based similarity for filtering out ambiguous decoys</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.5" class="ltx_p">We use the Wu-Palmer (WUP) score <cite class="ltx_cite ltx_citemacro_cite">Wu and Palmer (<a href="#bib.bib35" title="" class="ltx_ref">1994</a>)</cite>, which characterizes the <em id="A2.p1.5.1" class="ltx_emph ltx_font_italic">word sense</em> similarity, to filter out ambiguous decoys to the target (correct answer). The WUP score is computed based on the WordNet hierarchy. Essentially, it measures the similarity of two <em id="A2.p1.5.2" class="ltx_emph ltx_font_italic">nodes</em> (i.e., synsets) in the hierarchy. As a <em id="A2.p1.5.3" class="ltx_emph ltx_font_italic">word</em> might correspond to multiple nodes, we measure the word similarity as follows:</p>
<table id="A7.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.E3.m1.3" class="ltx_Math" alttext="\displaystyle WUP(w_{1},w_{2})=\max_{(n_{1},n_{2})\in N_{1}\times N_{2}}WUP(n_{1},n_{2})," display="inline"><semantics id="A2.E3.m1.3a"><mrow id="A2.E3.m1.3.3.1" xref="A2.E3.m1.3.3.1.1.cmml"><mrow id="A2.E3.m1.3.3.1.1" xref="A2.E3.m1.3.3.1.1.cmml"><mrow id="A2.E3.m1.3.3.1.1.2" xref="A2.E3.m1.3.3.1.1.2.cmml"><mi id="A2.E3.m1.3.3.1.1.2.4" xref="A2.E3.m1.3.3.1.1.2.4.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.2.3" xref="A2.E3.m1.3.3.1.1.2.3.cmml">​</mo><mi id="A2.E3.m1.3.3.1.1.2.5" xref="A2.E3.m1.3.3.1.1.2.5.cmml">U</mi><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.2.3a" xref="A2.E3.m1.3.3.1.1.2.3.cmml">​</mo><mi id="A2.E3.m1.3.3.1.1.2.6" xref="A2.E3.m1.3.3.1.1.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.2.3b" xref="A2.E3.m1.3.3.1.1.2.3.cmml">​</mo><mrow id="A2.E3.m1.3.3.1.1.2.2.2" xref="A2.E3.m1.3.3.1.1.2.2.3.cmml"><mo stretchy="false" id="A2.E3.m1.3.3.1.1.2.2.2.3" xref="A2.E3.m1.3.3.1.1.2.2.3.cmml">(</mo><msub id="A2.E3.m1.3.3.1.1.1.1.1.1" xref="A2.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi id="A2.E3.m1.3.3.1.1.1.1.1.1.2" xref="A2.E3.m1.3.3.1.1.1.1.1.1.2.cmml">w</mi><mn id="A2.E3.m1.3.3.1.1.1.1.1.1.3" xref="A2.E3.m1.3.3.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="A2.E3.m1.3.3.1.1.2.2.2.4" xref="A2.E3.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="A2.E3.m1.3.3.1.1.2.2.2.2" xref="A2.E3.m1.3.3.1.1.2.2.2.2.cmml"><mi id="A2.E3.m1.3.3.1.1.2.2.2.2.2" xref="A2.E3.m1.3.3.1.1.2.2.2.2.2.cmml">w</mi><mn id="A2.E3.m1.3.3.1.1.2.2.2.2.3" xref="A2.E3.m1.3.3.1.1.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="A2.E3.m1.3.3.1.1.2.2.2.5" xref="A2.E3.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="A2.E3.m1.3.3.1.1.5" xref="A2.E3.m1.3.3.1.1.5.cmml">=</mo><mrow id="A2.E3.m1.3.3.1.1.4" xref="A2.E3.m1.3.3.1.1.4.cmml"><mrow id="A2.E3.m1.3.3.1.1.4.4" xref="A2.E3.m1.3.3.1.1.4.4.cmml"><munder id="A2.E3.m1.3.3.1.1.4.4.1" xref="A2.E3.m1.3.3.1.1.4.4.1.cmml"><mi id="A2.E3.m1.3.3.1.1.4.4.1.2" xref="A2.E3.m1.3.3.1.1.4.4.1.2.cmml">max</mi><mrow id="A2.E3.m1.2.2.2" xref="A2.E3.m1.2.2.2.cmml"><mrow id="A2.E3.m1.2.2.2.2.2" xref="A2.E3.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="A2.E3.m1.2.2.2.2.2.3" xref="A2.E3.m1.2.2.2.2.3.cmml">(</mo><msub id="A2.E3.m1.1.1.1.1.1.1" xref="A2.E3.m1.1.1.1.1.1.1.cmml"><mi id="A2.E3.m1.1.1.1.1.1.1.2" xref="A2.E3.m1.1.1.1.1.1.1.2.cmml">n</mi><mn id="A2.E3.m1.1.1.1.1.1.1.3" xref="A2.E3.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="A2.E3.m1.2.2.2.2.2.4" xref="A2.E3.m1.2.2.2.2.3.cmml">,</mo><msub id="A2.E3.m1.2.2.2.2.2.2" xref="A2.E3.m1.2.2.2.2.2.2.cmml"><mi id="A2.E3.m1.2.2.2.2.2.2.2" xref="A2.E3.m1.2.2.2.2.2.2.2.cmml">n</mi><mn id="A2.E3.m1.2.2.2.2.2.2.3" xref="A2.E3.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="A2.E3.m1.2.2.2.2.2.5" xref="A2.E3.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="A2.E3.m1.2.2.2.3" xref="A2.E3.m1.2.2.2.3.cmml">∈</mo><mrow id="A2.E3.m1.2.2.2.4" xref="A2.E3.m1.2.2.2.4.cmml"><msub id="A2.E3.m1.2.2.2.4.2" xref="A2.E3.m1.2.2.2.4.2.cmml"><mi id="A2.E3.m1.2.2.2.4.2.2" xref="A2.E3.m1.2.2.2.4.2.2.cmml">N</mi><mn id="A2.E3.m1.2.2.2.4.2.3" xref="A2.E3.m1.2.2.2.4.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="A2.E3.m1.2.2.2.4.1" xref="A2.E3.m1.2.2.2.4.1.cmml">×</mo><msub id="A2.E3.m1.2.2.2.4.3" xref="A2.E3.m1.2.2.2.4.3.cmml"><mi id="A2.E3.m1.2.2.2.4.3.2" xref="A2.E3.m1.2.2.2.4.3.2.cmml">N</mi><mn id="A2.E3.m1.2.2.2.4.3.3" xref="A2.E3.m1.2.2.2.4.3.3.cmml">2</mn></msub></mrow></mrow></munder><mo lspace="0.167em" id="A2.E3.m1.3.3.1.1.4.4a" xref="A2.E3.m1.3.3.1.1.4.4.cmml">⁡</mo><mrow id="A2.E3.m1.3.3.1.1.4.4.2" xref="A2.E3.m1.3.3.1.1.4.4.2.cmml"><mi id="A2.E3.m1.3.3.1.1.4.4.2.2" xref="A2.E3.m1.3.3.1.1.4.4.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.4.4.2.1" xref="A2.E3.m1.3.3.1.1.4.4.2.1.cmml">​</mo><mi id="A2.E3.m1.3.3.1.1.4.4.2.3" xref="A2.E3.m1.3.3.1.1.4.4.2.3.cmml">U</mi><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.4.4.2.1a" xref="A2.E3.m1.3.3.1.1.4.4.2.1.cmml">​</mo><mi id="A2.E3.m1.3.3.1.1.4.4.2.4" xref="A2.E3.m1.3.3.1.1.4.4.2.4.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="A2.E3.m1.3.3.1.1.4.3" xref="A2.E3.m1.3.3.1.1.4.3.cmml">​</mo><mrow id="A2.E3.m1.3.3.1.1.4.2.2" xref="A2.E3.m1.3.3.1.1.4.2.3.cmml"><mo stretchy="false" id="A2.E3.m1.3.3.1.1.4.2.2.3" xref="A2.E3.m1.3.3.1.1.4.2.3.cmml">(</mo><msub id="A2.E3.m1.3.3.1.1.3.1.1.1" xref="A2.E3.m1.3.3.1.1.3.1.1.1.cmml"><mi id="A2.E3.m1.3.3.1.1.3.1.1.1.2" xref="A2.E3.m1.3.3.1.1.3.1.1.1.2.cmml">n</mi><mn id="A2.E3.m1.3.3.1.1.3.1.1.1.3" xref="A2.E3.m1.3.3.1.1.3.1.1.1.3.cmml">1</mn></msub><mo id="A2.E3.m1.3.3.1.1.4.2.2.4" xref="A2.E3.m1.3.3.1.1.4.2.3.cmml">,</mo><msub id="A2.E3.m1.3.3.1.1.4.2.2.2" xref="A2.E3.m1.3.3.1.1.4.2.2.2.cmml"><mi id="A2.E3.m1.3.3.1.1.4.2.2.2.2" xref="A2.E3.m1.3.3.1.1.4.2.2.2.2.cmml">n</mi><mn id="A2.E3.m1.3.3.1.1.4.2.2.2.3" xref="A2.E3.m1.3.3.1.1.4.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="A2.E3.m1.3.3.1.1.4.2.2.5" xref="A2.E3.m1.3.3.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E3.m1.3.3.1.2" xref="A2.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E3.m1.3b"><apply id="A2.E3.m1.3.3.1.1.cmml" xref="A2.E3.m1.3.3.1"><eq id="A2.E3.m1.3.3.1.1.5.cmml" xref="A2.E3.m1.3.3.1.1.5"></eq><apply id="A2.E3.m1.3.3.1.1.2.cmml" xref="A2.E3.m1.3.3.1.1.2"><times id="A2.E3.m1.3.3.1.1.2.3.cmml" xref="A2.E3.m1.3.3.1.1.2.3"></times><ci id="A2.E3.m1.3.3.1.1.2.4.cmml" xref="A2.E3.m1.3.3.1.1.2.4">𝑊</ci><ci id="A2.E3.m1.3.3.1.1.2.5.cmml" xref="A2.E3.m1.3.3.1.1.2.5">𝑈</ci><ci id="A2.E3.m1.3.3.1.1.2.6.cmml" xref="A2.E3.m1.3.3.1.1.2.6">𝑃</ci><interval closure="open" id="A2.E3.m1.3.3.1.1.2.2.3.cmml" xref="A2.E3.m1.3.3.1.1.2.2.2"><apply id="A2.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="A2.E3.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="A2.E3.m1.3.3.1.1.1.1.1.1.2">𝑤</ci><cn type="integer" id="A2.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="A2.E3.m1.3.3.1.1.1.1.1.1.3">1</cn></apply><apply id="A2.E3.m1.3.3.1.1.2.2.2.2.cmml" xref="A2.E3.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="A2.E3.m1.3.3.1.1.2.2.2.2.1.cmml" xref="A2.E3.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="A2.E3.m1.3.3.1.1.2.2.2.2.2.cmml" xref="A2.E3.m1.3.3.1.1.2.2.2.2.2">𝑤</ci><cn type="integer" id="A2.E3.m1.3.3.1.1.2.2.2.2.3.cmml" xref="A2.E3.m1.3.3.1.1.2.2.2.2.3">2</cn></apply></interval></apply><apply id="A2.E3.m1.3.3.1.1.4.cmml" xref="A2.E3.m1.3.3.1.1.4"><times id="A2.E3.m1.3.3.1.1.4.3.cmml" xref="A2.E3.m1.3.3.1.1.4.3"></times><apply id="A2.E3.m1.3.3.1.1.4.4.cmml" xref="A2.E3.m1.3.3.1.1.4.4"><apply id="A2.E3.m1.3.3.1.1.4.4.1.cmml" xref="A2.E3.m1.3.3.1.1.4.4.1"><csymbol cd="ambiguous" id="A2.E3.m1.3.3.1.1.4.4.1.1.cmml" xref="A2.E3.m1.3.3.1.1.4.4.1">subscript</csymbol><max id="A2.E3.m1.3.3.1.1.4.4.1.2.cmml" xref="A2.E3.m1.3.3.1.1.4.4.1.2"></max><apply id="A2.E3.m1.2.2.2.cmml" xref="A2.E3.m1.2.2.2"><in id="A2.E3.m1.2.2.2.3.cmml" xref="A2.E3.m1.2.2.2.3"></in><interval closure="open" id="A2.E3.m1.2.2.2.2.3.cmml" xref="A2.E3.m1.2.2.2.2.2"><apply id="A2.E3.m1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E3.m1.1.1.1.1.1.1.2.cmml" xref="A2.E3.m1.1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="A2.E3.m1.1.1.1.1.1.1.3.cmml" xref="A2.E3.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="A2.E3.m1.2.2.2.2.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.E3.m1.2.2.2.2.2.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.2">subscript</csymbol><ci id="A2.E3.m1.2.2.2.2.2.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.2.2">𝑛</ci><cn type="integer" id="A2.E3.m1.2.2.2.2.2.2.3.cmml" xref="A2.E3.m1.2.2.2.2.2.2.3">2</cn></apply></interval><apply id="A2.E3.m1.2.2.2.4.cmml" xref="A2.E3.m1.2.2.2.4"><times id="A2.E3.m1.2.2.2.4.1.cmml" xref="A2.E3.m1.2.2.2.4.1"></times><apply id="A2.E3.m1.2.2.2.4.2.cmml" xref="A2.E3.m1.2.2.2.4.2"><csymbol cd="ambiguous" id="A2.E3.m1.2.2.2.4.2.1.cmml" xref="A2.E3.m1.2.2.2.4.2">subscript</csymbol><ci id="A2.E3.m1.2.2.2.4.2.2.cmml" xref="A2.E3.m1.2.2.2.4.2.2">𝑁</ci><cn type="integer" id="A2.E3.m1.2.2.2.4.2.3.cmml" xref="A2.E3.m1.2.2.2.4.2.3">1</cn></apply><apply id="A2.E3.m1.2.2.2.4.3.cmml" xref="A2.E3.m1.2.2.2.4.3"><csymbol cd="ambiguous" id="A2.E3.m1.2.2.2.4.3.1.cmml" xref="A2.E3.m1.2.2.2.4.3">subscript</csymbol><ci id="A2.E3.m1.2.2.2.4.3.2.cmml" xref="A2.E3.m1.2.2.2.4.3.2">𝑁</ci><cn type="integer" id="A2.E3.m1.2.2.2.4.3.3.cmml" xref="A2.E3.m1.2.2.2.4.3.3">2</cn></apply></apply></apply></apply><apply id="A2.E3.m1.3.3.1.1.4.4.2.cmml" xref="A2.E3.m1.3.3.1.1.4.4.2"><times id="A2.E3.m1.3.3.1.1.4.4.2.1.cmml" xref="A2.E3.m1.3.3.1.1.4.4.2.1"></times><ci id="A2.E3.m1.3.3.1.1.4.4.2.2.cmml" xref="A2.E3.m1.3.3.1.1.4.4.2.2">𝑊</ci><ci id="A2.E3.m1.3.3.1.1.4.4.2.3.cmml" xref="A2.E3.m1.3.3.1.1.4.4.2.3">𝑈</ci><ci id="A2.E3.m1.3.3.1.1.4.4.2.4.cmml" xref="A2.E3.m1.3.3.1.1.4.4.2.4">𝑃</ci></apply></apply><interval closure="open" id="A2.E3.m1.3.3.1.1.4.2.3.cmml" xref="A2.E3.m1.3.3.1.1.4.2.2"><apply id="A2.E3.m1.3.3.1.1.3.1.1.1.cmml" xref="A2.E3.m1.3.3.1.1.3.1.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.3.3.1.1.3.1.1.1.1.cmml" xref="A2.E3.m1.3.3.1.1.3.1.1.1">subscript</csymbol><ci id="A2.E3.m1.3.3.1.1.3.1.1.1.2.cmml" xref="A2.E3.m1.3.3.1.1.3.1.1.1.2">𝑛</ci><cn type="integer" id="A2.E3.m1.3.3.1.1.3.1.1.1.3.cmml" xref="A2.E3.m1.3.3.1.1.3.1.1.1.3">1</cn></apply><apply id="A2.E3.m1.3.3.1.1.4.2.2.2.cmml" xref="A2.E3.m1.3.3.1.1.4.2.2.2"><csymbol cd="ambiguous" id="A2.E3.m1.3.3.1.1.4.2.2.2.1.cmml" xref="A2.E3.m1.3.3.1.1.4.2.2.2">subscript</csymbol><ci id="A2.E3.m1.3.3.1.1.4.2.2.2.2.cmml" xref="A2.E3.m1.3.3.1.1.4.2.2.2.2">𝑛</ci><cn type="integer" id="A2.E3.m1.3.3.1.1.4.2.2.2.3.cmml" xref="A2.E3.m1.3.3.1.1.4.2.2.2.3">2</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E3.m1.3c">\displaystyle WUP(w_{1},w_{2})=\max_{(n_{1},n_{2})\in N_{1}\times N_{2}}WUP(n_{1},n_{2}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="A2.p1.4" class="ltx_p">where <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="N_{1}" display="inline"><semantics id="A2.p1.1.m1.1a"><msub id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mi id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">N</mi><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="A2.p1.1.m1.1.1.3.cmml" xref="A2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">N_{1}</annotation></semantics></math> and <math id="A2.p1.2.m2.1" class="ltx_Math" alttext="N_{2}" display="inline"><semantics id="A2.p1.2.m2.1a"><msub id="A2.p1.2.m2.1.1" xref="A2.p1.2.m2.1.1.cmml"><mi id="A2.p1.2.m2.1.1.2" xref="A2.p1.2.m2.1.1.2.cmml">N</mi><mn id="A2.p1.2.m2.1.1.3" xref="A2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.2.m2.1b"><apply id="A2.p1.2.m2.1.1.cmml" xref="A2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A2.p1.2.m2.1.1.1.cmml" xref="A2.p1.2.m2.1.1">subscript</csymbol><ci id="A2.p1.2.m2.1.1.2.cmml" xref="A2.p1.2.m2.1.1.2">𝑁</ci><cn type="integer" id="A2.p1.2.m2.1.1.3.cmml" xref="A2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.2.m2.1c">N_{2}</annotation></semantics></math> are the sets of nodes that words <math id="A2.p1.3.m3.1" class="ltx_Math" alttext="w_{1}" display="inline"><semantics id="A2.p1.3.m3.1a"><msub id="A2.p1.3.m3.1.1" xref="A2.p1.3.m3.1.1.cmml"><mi id="A2.p1.3.m3.1.1.2" xref="A2.p1.3.m3.1.1.2.cmml">w</mi><mn id="A2.p1.3.m3.1.1.3" xref="A2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.3.m3.1b"><apply id="A2.p1.3.m3.1.1.cmml" xref="A2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A2.p1.3.m3.1.1.1.cmml" xref="A2.p1.3.m3.1.1">subscript</csymbol><ci id="A2.p1.3.m3.1.1.2.cmml" xref="A2.p1.3.m3.1.1.2">𝑤</ci><cn type="integer" id="A2.p1.3.m3.1.1.3.cmml" xref="A2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.3.m3.1c">w_{1}</annotation></semantics></math> and <math id="A2.p1.4.m4.1" class="ltx_Math" alttext="w_{2}" display="inline"><semantics id="A2.p1.4.m4.1a"><msub id="A2.p1.4.m4.1.1" xref="A2.p1.4.m4.1.1.cmml"><mi id="A2.p1.4.m4.1.1.2" xref="A2.p1.4.m4.1.1.2.cmml">w</mi><mn id="A2.p1.4.m4.1.1.3" xref="A2.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.4.m4.1b"><apply id="A2.p1.4.m4.1.1.cmml" xref="A2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A2.p1.4.m4.1.1.1.cmml" xref="A2.p1.4.m4.1.1">subscript</csymbol><ci id="A2.p1.4.m4.1.1.2.cmml" xref="A2.p1.4.m4.1.1.2">𝑤</ci><cn type="integer" id="A2.p1.4.m4.1.1.3.cmml" xref="A2.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.4.m4.1c">w_{2}</annotation></semantics></math> correspond to, respectively. That is, the word similarity is based on the most similar pair of nodes from both words. We consider only the NOUN and ADJ nodes for tractable computation.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.2" class="ltx_p">Since a candidate answer may contain more than one word (i.e., a word sequence), we compute the similarity between two word sequences <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="WS_{1}" display="inline"><semantics id="A2.p2.1.m1.1a"><mrow id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml"><mi id="A2.p2.1.m1.1.1.2" xref="A2.p2.1.m1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.p2.1.m1.1.1.1" xref="A2.p2.1.m1.1.1.1.cmml">​</mo><msub id="A2.p2.1.m1.1.1.3" xref="A2.p2.1.m1.1.1.3.cmml"><mi id="A2.p2.1.m1.1.1.3.2" xref="A2.p2.1.m1.1.1.3.2.cmml">S</mi><mn id="A2.p2.1.m1.1.1.3.3" xref="A2.p2.1.m1.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><apply id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1"><times id="A2.p2.1.m1.1.1.1.cmml" xref="A2.p2.1.m1.1.1.1"></times><ci id="A2.p2.1.m1.1.1.2.cmml" xref="A2.p2.1.m1.1.1.2">𝑊</ci><apply id="A2.p2.1.m1.1.1.3.cmml" xref="A2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.p2.1.m1.1.1.3.1.cmml" xref="A2.p2.1.m1.1.1.3">subscript</csymbol><ci id="A2.p2.1.m1.1.1.3.2.cmml" xref="A2.p2.1.m1.1.1.3.2">𝑆</ci><cn type="integer" id="A2.p2.1.m1.1.1.3.3.cmml" xref="A2.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">WS_{1}</annotation></semantics></math> and <math id="A2.p2.2.m2.1" class="ltx_Math" alttext="WS_{2}" display="inline"><semantics id="A2.p2.2.m2.1a"><mrow id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml"><mi id="A2.p2.2.m2.1.1.2" xref="A2.p2.2.m2.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.p2.2.m2.1.1.1" xref="A2.p2.2.m2.1.1.1.cmml">​</mo><msub id="A2.p2.2.m2.1.1.3" xref="A2.p2.2.m2.1.1.3.cmml"><mi id="A2.p2.2.m2.1.1.3.2" xref="A2.p2.2.m2.1.1.3.2.cmml">S</mi><mn id="A2.p2.2.m2.1.1.3.3" xref="A2.p2.2.m2.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><apply id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1"><times id="A2.p2.2.m2.1.1.1.cmml" xref="A2.p2.2.m2.1.1.1"></times><ci id="A2.p2.2.m2.1.1.2.cmml" xref="A2.p2.2.m2.1.1.2">𝑊</ci><apply id="A2.p2.2.m2.1.1.3.cmml" xref="A2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="A2.p2.2.m2.1.1.3.1.cmml" xref="A2.p2.2.m2.1.1.3">subscript</csymbol><ci id="A2.p2.2.m2.1.1.3.2.cmml" xref="A2.p2.2.m2.1.1.3.2">𝑆</ci><cn type="integer" id="A2.p2.2.m2.1.1.3.3.cmml" xref="A2.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">WS_{2}</annotation></semantics></math> as follows</p>
<table id="A7.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex2.m2.2" class="ltx_Math" alttext="\displaystyle WUP(WS_{1},WS_{2})=" display="inline"><semantics id="A2.Ex2.m2.2a"><mrow id="A2.Ex2.m2.2.2" xref="A2.Ex2.m2.2.2.cmml"><mrow id="A2.Ex2.m2.2.2.2" xref="A2.Ex2.m2.2.2.2.cmml"><mi id="A2.Ex2.m2.2.2.2.4" xref="A2.Ex2.m2.2.2.2.4.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.Ex2.m2.2.2.2.3" xref="A2.Ex2.m2.2.2.2.3.cmml">​</mo><mi id="A2.Ex2.m2.2.2.2.5" xref="A2.Ex2.m2.2.2.2.5.cmml">U</mi><mo lspace="0em" rspace="0em" id="A2.Ex2.m2.2.2.2.3a" xref="A2.Ex2.m2.2.2.2.3.cmml">​</mo><mi id="A2.Ex2.m2.2.2.2.6" xref="A2.Ex2.m2.2.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="A2.Ex2.m2.2.2.2.3b" xref="A2.Ex2.m2.2.2.2.3.cmml">​</mo><mrow id="A2.Ex2.m2.2.2.2.2.2" xref="A2.Ex2.m2.2.2.2.2.3.cmml"><mo stretchy="false" id="A2.Ex2.m2.2.2.2.2.2.3" xref="A2.Ex2.m2.2.2.2.2.3.cmml">(</mo><mrow id="A2.Ex2.m2.1.1.1.1.1.1" xref="A2.Ex2.m2.1.1.1.1.1.1.cmml"><mi id="A2.Ex2.m2.1.1.1.1.1.1.2" xref="A2.Ex2.m2.1.1.1.1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.Ex2.m2.1.1.1.1.1.1.1" xref="A2.Ex2.m2.1.1.1.1.1.1.1.cmml">​</mo><msub id="A2.Ex2.m2.1.1.1.1.1.1.3" xref="A2.Ex2.m2.1.1.1.1.1.1.3.cmml"><mi id="A2.Ex2.m2.1.1.1.1.1.1.3.2" xref="A2.Ex2.m2.1.1.1.1.1.1.3.2.cmml">S</mi><mn id="A2.Ex2.m2.1.1.1.1.1.1.3.3" xref="A2.Ex2.m2.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="A2.Ex2.m2.2.2.2.2.2.4" xref="A2.Ex2.m2.2.2.2.2.3.cmml">,</mo><mrow id="A2.Ex2.m2.2.2.2.2.2.2" xref="A2.Ex2.m2.2.2.2.2.2.2.cmml"><mi id="A2.Ex2.m2.2.2.2.2.2.2.2" xref="A2.Ex2.m2.2.2.2.2.2.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="A2.Ex2.m2.2.2.2.2.2.2.1" xref="A2.Ex2.m2.2.2.2.2.2.2.1.cmml">​</mo><msub id="A2.Ex2.m2.2.2.2.2.2.2.3" xref="A2.Ex2.m2.2.2.2.2.2.2.3.cmml"><mi id="A2.Ex2.m2.2.2.2.2.2.2.3.2" xref="A2.Ex2.m2.2.2.2.2.2.2.3.2.cmml">S</mi><mn id="A2.Ex2.m2.2.2.2.2.2.2.3.3" xref="A2.Ex2.m2.2.2.2.2.2.2.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="A2.Ex2.m2.2.2.2.2.2.5" xref="A2.Ex2.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="A2.Ex2.m2.2.2.3" xref="A2.Ex2.m2.2.2.3.cmml">=</mo><mi id="A2.Ex2.m2.2.2.4" xref="A2.Ex2.m2.2.2.4.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex2.m2.2b"><apply id="A2.Ex2.m2.2.2.cmml" xref="A2.Ex2.m2.2.2"><eq id="A2.Ex2.m2.2.2.3.cmml" xref="A2.Ex2.m2.2.2.3"></eq><apply id="A2.Ex2.m2.2.2.2.cmml" xref="A2.Ex2.m2.2.2.2"><times id="A2.Ex2.m2.2.2.2.3.cmml" xref="A2.Ex2.m2.2.2.2.3"></times><ci id="A2.Ex2.m2.2.2.2.4.cmml" xref="A2.Ex2.m2.2.2.2.4">𝑊</ci><ci id="A2.Ex2.m2.2.2.2.5.cmml" xref="A2.Ex2.m2.2.2.2.5">𝑈</ci><ci id="A2.Ex2.m2.2.2.2.6.cmml" xref="A2.Ex2.m2.2.2.2.6">𝑃</ci><interval closure="open" id="A2.Ex2.m2.2.2.2.2.3.cmml" xref="A2.Ex2.m2.2.2.2.2.2"><apply id="A2.Ex2.m2.1.1.1.1.1.1.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1"><times id="A2.Ex2.m2.1.1.1.1.1.1.1.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.1"></times><ci id="A2.Ex2.m2.1.1.1.1.1.1.2.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.2">𝑊</ci><apply id="A2.Ex2.m2.1.1.1.1.1.1.3.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.Ex2.m2.1.1.1.1.1.1.3.1.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.Ex2.m2.1.1.1.1.1.1.3.2.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.3.2">𝑆</ci><cn type="integer" id="A2.Ex2.m2.1.1.1.1.1.1.3.3.cmml" xref="A2.Ex2.m2.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="A2.Ex2.m2.2.2.2.2.2.2.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2"><times id="A2.Ex2.m2.2.2.2.2.2.2.1.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.1"></times><ci id="A2.Ex2.m2.2.2.2.2.2.2.2.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.2">𝑊</ci><apply id="A2.Ex2.m2.2.2.2.2.2.2.3.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="A2.Ex2.m2.2.2.2.2.2.2.3.1.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.3">subscript</csymbol><ci id="A2.Ex2.m2.2.2.2.2.2.2.3.2.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.3.2">𝑆</ci><cn type="integer" id="A2.Ex2.m2.2.2.2.2.2.2.3.3.cmml" xref="A2.Ex2.m2.2.2.2.2.2.2.3.3">2</cn></apply></apply></interval></apply><csymbol cd="latexml" id="A2.Ex2.m2.2.2.4.cmml" xref="A2.Ex2.m2.2.2.4">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex2.m2.2c">\displaystyle WUP(WS_{1},WS_{2})=</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.E4.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\max\{\prod_{w_{1}\in WS_{1}}\max_{w_{2}\in WS_{2}}WUP(w_{1},w_{2})," display="inline"><semantics id="A2.E4.m1.1a"><mrow id="A2.E4.m1.1b"><mi id="A2.E4.m1.1.1">max</mi><mrow id="A2.E4.m1.1.2"><mo stretchy="false" id="A2.E4.m1.1.2.1">{</mo><mstyle displaystyle="true" id="A2.E4.m1.1.2.2"><munder id="A2.E4.m1.1.2.2a"><mo movablelimits="false" id="A2.E4.m1.1.2.2.2">∏</mo><mrow id="A2.E4.m1.1.2.2.3"><msub id="A2.E4.m1.1.2.2.3.2"><mi id="A2.E4.m1.1.2.2.3.2.2">w</mi><mn id="A2.E4.m1.1.2.2.3.2.3">1</mn></msub><mo id="A2.E4.m1.1.2.2.3.1">∈</mo><mrow id="A2.E4.m1.1.2.2.3.3"><mi id="A2.E4.m1.1.2.2.3.3.2">W</mi><mo lspace="0em" rspace="0em" id="A2.E4.m1.1.2.2.3.3.1">​</mo><msub id="A2.E4.m1.1.2.2.3.3.3"><mi id="A2.E4.m1.1.2.2.3.3.3.2">S</mi><mn id="A2.E4.m1.1.2.2.3.3.3.3">1</mn></msub></mrow></mrow></munder></mstyle><munder id="A2.E4.m1.1.2.3"><mi id="A2.E4.m1.1.2.3.2">max</mi><mrow id="A2.E4.m1.1.2.3.3"><msub id="A2.E4.m1.1.2.3.3.2"><mi id="A2.E4.m1.1.2.3.3.2.2">w</mi><mn id="A2.E4.m1.1.2.3.3.2.3">2</mn></msub><mo id="A2.E4.m1.1.2.3.3.1">∈</mo><mrow id="A2.E4.m1.1.2.3.3.3"><mi id="A2.E4.m1.1.2.3.3.3.2">W</mi><mo lspace="0em" rspace="0em" id="A2.E4.m1.1.2.3.3.3.1">​</mo><msub id="A2.E4.m1.1.2.3.3.3.3"><mi id="A2.E4.m1.1.2.3.3.3.3.2">S</mi><mn id="A2.E4.m1.1.2.3.3.3.3.3">2</mn></msub></mrow></mrow></munder><mi id="A2.E4.m1.1.2.4">W</mi><mi id="A2.E4.m1.1.2.5">U</mi><mi id="A2.E4.m1.1.2.6">P</mi><mrow id="A2.E4.m1.1.2.7"><mo stretchy="false" id="A2.E4.m1.1.2.7.1">(</mo><msub id="A2.E4.m1.1.2.7.2"><mi id="A2.E4.m1.1.2.7.2.2">w</mi><mn id="A2.E4.m1.1.2.7.2.3">1</mn></msub><mo id="A2.E4.m1.1.2.7.3">,</mo><msub id="A2.E4.m1.1.2.7.4"><mi id="A2.E4.m1.1.2.7.4.2">w</mi><mn id="A2.E4.m1.1.2.7.4.3">2</mn></msub><mo stretchy="false" id="A2.E4.m1.1.2.7.5">)</mo></mrow><mo id="A2.E4.m1.1.2.8">,</mo></mrow></mrow><annotation encoding="application/x-tex" id="A2.E4.m1.1c">\displaystyle\max\{\prod_{w_{1}\in WS_{1}}\max_{w_{2}\in WS_{2}}WUP(w_{1},w_{2}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="A2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex3.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\hskip 24.0pt\prod_{w_{2}\in WS_{2}}\max_{w_{1}\in WS_{1}}WUP(w_{1},w_{2})\}." display="inline"><semantics id="A2.Ex3.m1.1a"><mrow id="A2.Ex3.m1.1b"><mstyle displaystyle="true" id="A2.Ex3.m1.1.1"><munder id="A2.Ex3.m1.1.1a"><mo movablelimits="false" id="A2.Ex3.m1.1.1.2">∏</mo><mrow id="A2.Ex3.m1.1.1.3"><msub id="A2.Ex3.m1.1.1.3.2"><mi id="A2.Ex3.m1.1.1.3.2.2">w</mi><mn id="A2.Ex3.m1.1.1.3.2.3">2</mn></msub><mo id="A2.Ex3.m1.1.1.3.1">∈</mo><mrow id="A2.Ex3.m1.1.1.3.3"><mi id="A2.Ex3.m1.1.1.3.3.2">W</mi><mo lspace="0em" rspace="0em" id="A2.Ex3.m1.1.1.3.3.1">​</mo><msub id="A2.Ex3.m1.1.1.3.3.3"><mi id="A2.Ex3.m1.1.1.3.3.3.2">S</mi><mn id="A2.Ex3.m1.1.1.3.3.3.3">2</mn></msub></mrow></mrow></munder></mstyle><munder id="A2.Ex3.m1.1.2"><mi id="A2.Ex3.m1.1.2.2">max</mi><mrow id="A2.Ex3.m1.1.2.3"><msub id="A2.Ex3.m1.1.2.3.2"><mi id="A2.Ex3.m1.1.2.3.2.2">w</mi><mn id="A2.Ex3.m1.1.2.3.2.3">1</mn></msub><mo id="A2.Ex3.m1.1.2.3.1">∈</mo><mrow id="A2.Ex3.m1.1.2.3.3"><mi id="A2.Ex3.m1.1.2.3.3.2">W</mi><mo lspace="0em" rspace="0em" id="A2.Ex3.m1.1.2.3.3.1">​</mo><msub id="A2.Ex3.m1.1.2.3.3.3"><mi id="A2.Ex3.m1.1.2.3.3.3.2">S</mi><mn id="A2.Ex3.m1.1.2.3.3.3.3">1</mn></msub></mrow></mrow></munder><mi id="A2.Ex3.m1.1.3">W</mi><mi id="A2.Ex3.m1.1.4">U</mi><mi id="A2.Ex3.m1.1.5">P</mi><mrow id="A2.Ex3.m1.1.6"><mo stretchy="false" id="A2.Ex3.m1.1.6.1">(</mo><msub id="A2.Ex3.m1.1.6.2"><mi id="A2.Ex3.m1.1.6.2.2">w</mi><mn id="A2.Ex3.m1.1.6.2.3">1</mn></msub><mo id="A2.Ex3.m1.1.6.3">,</mo><msub id="A2.Ex3.m1.1.6.4"><mi id="A2.Ex3.m1.1.6.4.2">w</mi><mn id="A2.Ex3.m1.1.6.4.3">2</mn></msub><mo stretchy="false" id="A2.Ex3.m1.1.6.5">)</mo></mrow><mo stretchy="false" id="A2.Ex3.m1.1.7">}</mo><mo lspace="0em" id="A2.Ex3.m1.1.8">.</mo></mrow><annotation encoding="application/x-tex" id="A2.Ex3.m1.1c">\displaystyle\hskip 24.0pt\prod_{w_{2}\in WS_{2}}\max_{w_{1}\in WS_{1}}WUP(w_{1},w_{2})\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="A2.p2.3" class="ltx_p">This formulations is highly similar to the one proposed by Malinowski and Fritz et al. <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite> for evaluating open-ended Visual QA. The main difference is that we use “max” rather than “min” to compute the final score. Note that our purpose of using the WUP score is to filter out ambiguous decoys to the target. For example, we consider “a cute cat” to be ambiguous to “cat”. Using eq. (<a href="#A2.Ex2" title="Appendix B WUP-based similarity for filtering out ambiguous decoys ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>) gives a similarity 1, which can not be achieved by taking “min”.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Analysis on the coverage</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Among all the 139,868 IQA triplets in Visual7W, the target answers of 137,557 of them ( 98%) can find corresponding nodes in the WordNet hierarchy, so the scores can be computed. For VQA, the ratio is  97%. For qaVG, the ratio is  99%.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed results on VQA w/o QA pairs that have Yes/No as the targets</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">As mentioned in Sect. 5.3 of the main text, the validation set of VQA contains 45,478 QA pairs (out of totally 12,1512 pairs) that have Yes or No as the correct answers. The only reasonable decoy to Yes is No, and vice versa — any other decoy could be easily recognized in principle. Since both of them are among top 10 frequently-occurring answers, they are already included in the Orig decoys — our IoU-decoys and QoU-decoys can hardly make noticeable improvement. We thus remove all those pairs (denoted as Yes/No QA pairs) to investigate the improvement on the remaining pairs, for which having multiple choices makes sense. We denote the subset of VQA as VQA<sup id="A3.p1.1.1" class="ltx_sup">-</sup> (we remove Yes/No pairs in both training and validation set).</p>
</div>
<figure id="A3.T7" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A3.T7.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.T7.2.3.1" class="ltx_tr">
<th id="A3.T7.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<td id="A3.T7.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Orig</td>
<td id="A3.T7.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A3.T7.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</td>
<td id="A3.T7.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</td>
<td id="A3.T7.2.3.1.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
</tr>
<tr id="A3.T7.2.4.2" class="ltx_tr">
<th id="A3.T7.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A3.T7.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">28.8</td>
<td id="A3.T7.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">42.9</td>
<td id="A3.T7.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">34.5</td>
<td id="A3.T7.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">23.6</td>
<td id="A3.T7.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15.8</td>
</tr>
<tr id="A3.T7.2.5.3" class="ltx_tr">
<th id="A3.T7.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A3.T7.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">43.0</td>
<td id="A3.T7.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">44.8</td>
<td id="A3.T7.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">53.2</td>
<td id="A3.T7.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">35.5</td>
<td id="A3.T7.2.5.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.5</td>
</tr>
<tr id="A3.T7.2.6.4" class="ltx_tr">
<th id="A3.T7.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A3.T7.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">45.8</td>
<td id="A3.T7.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">80.7</td>
<td id="A3.T7.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">39.3</td>
<td id="A3.T7.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">38.2</td>
<td id="A3.T7.2.6.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.9</td>
</tr>
<tr id="A3.T7.2.7.5" class="ltx_tr">
<th id="A3.T7.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A3.T7.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">55.6</td>
<td id="A3.T7.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">81.8</td>
<td id="A3.T7.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">56.6</td>
<td id="A3.T7.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">53.7</td>
<td id="A3.T7.2.7.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">46.5</td>
</tr>
<tr id="A3.T7.1.1" class="ltx_tr">
<th id="A3.T7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">HieCoAtt<math id="A3.T7.1.1.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="A3.T7.1.1.1.m1.1a"><mo id="A3.T7.1.1.1.m1.1.1" xref="A3.T7.1.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="A3.T7.1.1.1.m1.1b"><times id="A3.T7.1.1.1.m1.1.1.cmml" xref="A3.T7.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.1.1.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="A3.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">54.8</td>
<td id="A3.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">55.6</td>
<td id="A3.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="A3.T7.2.2" class="ltx_tr">
<th id="A3.T7.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Attention<math id="A3.T7.2.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="A3.T7.2.2.1.m1.1a"><mo id="A3.T7.2.2.1.m1.1.1" xref="A3.T7.2.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="A3.T7.2.2.1.m1.1b"><times id="A3.T7.2.2.1.m1.1.1.cmml" xref="A3.T7.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.2.2.1.m1.1c">*</annotation></semantics></math>
</th>
<td id="A3.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">58.5</td>
<td id="A3.T7.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">58.6</td>
<td id="A3.T7.2.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="A3.T7.2.8.6" class="ltx_tr">
<th id="A3.T7.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Human-IQA</th>
<td id="A3.T7.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="A3.T7.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">85.5</td>
<td id="A3.T7.2.8.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="A3.T7.2.9.7" class="ltx_tr">
<th id="A3.T7.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A3.T7.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.6</td>
<td id="A3.T7.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A3.T7.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A3.T7.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
<td id="A3.T7.2.9.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.2</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.T7.3" class="ltx_p ltx_figure_panel ltx_align_center"><math id="A3.T7.3.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="A3.T7.3.m1.1a"><mo id="A3.T7.3.m1.1.1" xref="A3.T7.3.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="A3.T7.3.m1.1b"><times id="A3.T7.3.m1.1.1.cmml" xref="A3.T7.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.3.m1.1c">*</annotation></semantics></math>: based on our implementation or modification</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Accuracy (%) on VQA<sup id="A3.T7.7.1" class="ltx_sup">-</sup>-2014val, which contains 76,034 triplets.</figcaption>
</figure>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.2" class="ltx_p">We conduct the same experiments as in Sect. 5.3 of the main text on VQA<sup id="A3.p2.2.1" class="ltx_sup">-</sup>. Table <a href="#A3.T7" title="Table 7 ‣ Appendix C Detailed results on VQA w/o QA pairs that have Yes/No as the targets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes the machines’ as well as humans’ results. Compared to Table 4 of the main text, most of the results drop, which is expected as those removed Yes/No pairs are considered simpler and easier ones — their <em id="A3.p2.2.2" class="ltx_emph ltx_font_italic">effective</em> random chance is 50%. The exception is for the MLP-IA models, which performs roughly the same or even better on VQA<sup id="A3.p2.2.3" class="ltx_sup">-</sup>, suggesting that Yes/No pairs are somehow difficult to MLP-IA. This, however, makes sense since without the questions (e.g., those start with “Is there a …” or “Does the person …”), a machine cannot directly tell if the correct answer falls into Yes or No, or others.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">We see that on VQA<sup id="A3.p3.1.1" class="ltx_sup">-</sup>, the improvement by our IoU-decoys and QoU-decoys becomes significant. The gain brought by images on QoU (from 39.3% to 56.6%) is much larger than that on Orig (from 45.8% to 55.6%). Similarly, the gain brought by questions on IoU (from 44.8% to 81.8%) is much larger than that on Orig (from 43.0% to 55.6%). After combining IoU-decoys and QoU-decoys as in IoU +QoU and All, the improvement by either including images to MLP-QA or including questions to MLP-IA is noticeable higher than that on Orig. Moreover, even with only 6 decoys, the performance by MLP-A on IoU +QoU is already lower than that on Orig, which has 17 decoys, demonstrating the effectiveness of our decoys in preventing machines from overfitting to the incidental statistics. These observations together demonstrate how our proposed ways for creating decoys improve the quality of multiple-choice Visual QA datasets.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More experimental results on VQA2 and COCOQA</h2>

<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Dataset descriptions</h3>

<section id="A4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite> </h4>

<div id="A4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A4.SS1.SSS0.Px1.p1.1" class="ltx_p">This dataset contains in total 117,684 auto-generated IQT triplets with no decoy answers. Therefore, we create decoys using our proposed approach and follow the original data split, leading to a training set and a testing set with 78,736 IQA triplets and 38,948 IQA triplets, respectfully.</p>
</div>
</section>
<section id="A4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>
</h4>

<div id="A4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A4.SS1.SSS0.Px2.p1.1" class="ltx_p">VQA2 is a successive dataset of VQA, which pairs each IQT triplet with a complementary one to reduce the correlation between questions and answers. There are 443,757 training IQT triplets and 214,354 validation IQT triplets, with no decoys. We generate decoys using our approach and follow the original data split to organize the data. We do not consider the test split as it does not indicate the targets (correct answers).</p>
</div>
</section>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Experimental results</h3>

<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.1" class="ltx_p">For both datasets, we conduct the same experiments as in Sect. 5.3 of the main text using the MLP-based models. As shown in Table <a href="#A4.T8" title="Table 8 ‣ D.2 Experimental results ‣ Appendix D More experimental results on VQA2 and COCOQA ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we clearly see that with only answers being visible to the model (MLP-A), the performance is close to random (on the column of IoU +QoU-decoys), and far from observing all three sources of information (MLP-IQA). Meanwhile, models that can observe either images and answers (MLP-IA) or questions and answers (MLP-QA) fail to predict as good as the model that observe all three sources of information. Results in Table <a href="#A4.T9" title="Table 9 ‣ D.2 Experimental results ‣ Appendix D More experimental results on VQA2 and COCOQA ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> also shows a similar trend. These empirical observations meet our expectation and again verify the effectiveness of our proposed methods for creating decoys.</p>
</div>
<figure id="A4.T8" class="ltx_table">
<table id="A4.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T8.1.1.1" class="ltx_tr">
<th id="A4.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="A4.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</th>
<th id="A4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</th>
<th id="A4.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T8.1.2.1" class="ltx_tr">
<th id="A4.T8.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A4.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">70.3</td>
<td id="A4.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.7</td>
<td id="A4.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">26.6</td>
</tr>
<tr id="A4.T8.1.3.2" class="ltx_tr">
<th id="A4.T8.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A4.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">73.4</td>
<td id="A4.T8.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">73.3</td>
<td id="A4.T8.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">60.7</td>
</tr>
<tr id="A4.T8.1.4.3" class="ltx_tr">
<th id="A4.T8.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A4.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">91.5</td>
<td id="A4.T8.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">52.5</td>
<td id="A4.T8.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
</tr>
<tr id="A4.T8.1.5.4" class="ltx_tr">
<th id="A4.T8.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A4.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">93.1</td>
<td id="A4.T8.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">78.3</td>
<td id="A4.T8.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">75.9</td>
</tr>
<tr id="A4.T8.1.6.5" class="ltx_tr">
<th id="A4.T8.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A4.T8.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T8.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T8.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Test accuracy (%) on COCOQA.</figcaption>
</figure>
<figure id="A4.T9" class="ltx_table">
<table id="A4.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T9.1.1.1" class="ltx_tr">
<th id="A4.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="A4.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</th>
<th id="A4.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</th>
<th id="A4.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T9.1.2.1" class="ltx_tr">
<th id="A4.T9.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A4.T9.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">37.7</td>
<td id="A4.T9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">41.9</td>
<td id="A4.T9.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.7</td>
</tr>
<tr id="A4.T9.1.3.2" class="ltx_tr">
<th id="A4.T9.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A4.T9.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">37.9</td>
<td id="A4.T9.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">54.4</td>
<td id="A4.T9.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.5</td>
</tr>
<tr id="A4.T9.1.4.3" class="ltx_tr">
<th id="A4.T9.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A4.T9.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">84.2</td>
<td id="A4.T9.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">48.3</td>
<td id="A4.T9.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">48.1</td>
</tr>
<tr id="A4.T9.1.5.4" class="ltx_tr">
<th id="A4.T9.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A4.T9.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">86.3</td>
<td id="A4.T9.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">63.0</td>
<td id="A4.T9.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">61.1</td>
</tr>
<tr id="A4.T9.1.6.5" class="ltx_tr">
<th id="A4.T9.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A4.T9.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T9.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T9.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Test accuracy (%) on VQA2-2017val.</figcaption>
</figure>
<div id="A4.SS2.p2" class="ltx_para">
<p id="A4.SS2.p2.3" class="ltx_p">Besides, we also perform a more in-depth experiment on VQA2, removing triplets with Yes/No as the target. We name this subset as VQA2<sup id="A4.SS2.p2.3.1" class="ltx_sup">-</sup>. Table <a href="#A4.T10" title="Table 10 ‣ D.2 Experimental results ‣ Appendix D More experimental results on VQA2 and COCOQA ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the experimental results on VQA2<sup id="A4.SS2.p2.3.2" class="ltx_sup">-</sup>. Comparing to Table <a href="#A4.T9" title="Table 9 ‣ D.2 Experimental results ‣ Appendix D More experimental results on VQA2 and COCOQA ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we see that the overall performance for each model decreases as the dataset becomes more challenging on average. Specifically, the model that observes question and answer on VQA2<sup id="A4.SS2.p2.3.3" class="ltx_sup">-</sup> performs much worse than that on VQA2 (37.2% vs. 48.1%).</p>
</div>
<figure id="A4.T10" class="ltx_table">
<table id="A4.T10.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T10.3.1.1" class="ltx_tr">
<th id="A4.T10.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="A4.T10.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</th>
<th id="A4.T10.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QoU</th>
<th id="A4.T10.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;">IoU +QoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T10.3.2.1" class="ltx_tr">
<th id="A4.T10.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A4.T10.3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">39.8</td>
<td id="A4.T10.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">33.7</td>
<td id="A4.T10.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">21.3</td>
</tr>
<tr id="A4.T10.3.3.2" class="ltx_tr">
<th id="A4.T10.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A4.T10.3.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">40.3</td>
<td id="A4.T10.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">53.0</td>
<td id="A4.T10.3.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.0</td>
</tr>
<tr id="A4.T10.3.4.3" class="ltx_tr">
<th id="A4.T10.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A4.T10.3.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">84.8</td>
<td id="A4.T10.3.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">37.6</td>
<td id="A4.T10.3.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.2</td>
</tr>
<tr id="A4.T10.3.5.4" class="ltx_tr">
<th id="A4.T10.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A4.T10.3.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">85.9</td>
<td id="A4.T10.3.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">56.1</td>
<td id="A4.T10.3.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">53.8</td>
</tr>
<tr id="A4.T10.3.6.5" class="ltx_tr">
<th id="A4.T10.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A4.T10.3.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T10.3.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="A4.T10.3.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Test accuracy (%) on VQA2<sup id="A4.T10.5.1" class="ltx_sup">-</sup>-2017val, which contains 134,813 triplets.</figcaption>
</figure>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Details on user studies</h2>

<figure id="A5.F6" class="ltx_figure"><img src="/html/1704.07121/assets/graphics/user-interface-iqa.jpg" id="A5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="289" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>User interface for human evaluation on Visual7W (IoU-decoys+QoU-decoys).</figcaption>
</figure>
<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">As mentioned in Sect. 5.2 of the main text, we provide details on user studies. Fig. <a href="#A5.F6" title="Figure 6 ‣ Appendix E Details on user studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows our user interface. We perform the studies using Amazon Mechanic Turk (AMT) on Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>, VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> and Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>. We mainly evaluate on our IoU-decoys and QoU-decoys (combined together).</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">For each dataset, we randomly sample 1,000 image-question-target triplets together with the corresponding IoU-decoys and QoU-decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate answer according to the image and the question. We compute the average accuracy of these workers and report them in Table 3, 4 and 5 of the main text and Table <a href="#A3.T7" title="Table 7 ‣ Appendix C Detailed results on VQA w/o QA pairs that have Yes/No as the targets ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="A5.p3" class="ltx_para">
<p id="A5.p3.1" class="ltx_p">We also conduct human evaluation using the Orig decoys of Visual7W so as to investigate the difference between human-generated and automatically generated decoys. We also study how humans will perform given only partial information (i.e., images + candidate answers or questions + candidate answers), again using the Orig decoys of Visual7W. The corresponding interfaces are shown in Fig. <a href="#A5.F7" title="Figure 7 ‣ Appendix E Details on user studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#A5.F8" title="Figure 8 ‣ Appendix E Details on user studies ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. For these studies, we use the same set of 1,000 triplets used to evaluate our created decoys for fair comparison. We make sure that no worker works on the same triplet across the four studies on Visual7W. Results are reported in Table 1 of the main text.</p>
</div>
<div id="A5.p4" class="ltx_para">
<p id="A5.p4.1" class="ltx_p">In summary, 169 workers are involved in our studies. The total cost is $215 — the rate for every 20 triplets is $0.25. On our IoU-decoys and QoU-decoys, humans achieve 84.1%, 89.0%, and 82.5% on Visual7W, VQA, and VG, respectively. Compared to the human performance on the Orig decoys that involve human effort in creation (i.e., 88.4% on Visual7W, and 88.5% on VQA as reported in <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>), these results suggest that the ways we create the decoys and the filtering steps mentioned in Sect. 4.2 lead to high-quality datasets with limited ambiguity.</p>
</div>
<figure id="A5.F7" class="ltx_figure"><img src="/html/1704.07121/assets/graphics/user-interface-ia.jpg" id="A5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="278" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>User interface for human evaluation on Visual7W (Orig decoys), where questions are blocked.</figcaption>
</figure>
<figure id="A5.F8" class="ltx_figure"><img src="/html/1704.07121/assets/graphics/user-interface-qa.jpg" id="A5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>User interface for human evaluation on Visual7W (Orig decoys), where images are not blocked.</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Analysis on different question and answer embeddings</h2>

<figure id="A6.T11" class="ltx_table">
<table id="A6.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T11.1.1.1" class="ltx_tr">
<th id="A6.T11.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="A6.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T11.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">GloVe</span></th>
<th id="A6.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Translation</th>
<th id="A6.T11.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T11.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">word2vec</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T11.1.2.1" class="ltx_tr">
<th id="A6.T11.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A6.T11.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">18.0</td>
<td id="A6.T11.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">18.0</td>
<td id="A6.T11.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">17.7</td>
</tr>
<tr id="A6.T11.1.3.2" class="ltx_tr">
<th id="A6.T11.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A6.T11.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">23.6</td>
<td id="A6.T11.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">23.2</td>
<td id="A6.T11.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.6</td>
</tr>
<tr id="A6.T11.1.4.3" class="ltx_tr">
<th id="A6.T11.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A6.T11.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">38.1</td>
<td id="A6.T11.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">38.3</td>
<td id="A6.T11.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.8</td>
</tr>
<tr id="A6.T11.1.5.4" class="ltx_tr">
<th id="A6.T11.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A6.T11.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">52.5</td>
<td id="A6.T11.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
<td id="A6.T11.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
</tr>
<tr id="A6.T11.1.6.5" class="ltx_tr">
<th id="A6.T11.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A6.T11.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
<td id="A6.T11.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
<td id="A6.T11.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Test accuracy (%) on Visual7W, comparing different embeddings for questions and answers. The results are reported for the IoU +QoU-decoys.</figcaption>
</figure>
<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">We consider <span id="A6.p1.1.1" class="ltx_text ltx_font_smallcaps">GloVe</span> <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite> and the embedding learned from translation <cite class="ltx_cite ltx_citemacro_cite">McCann et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite> on both question and answer embeddings. The results on Visual7W (IoU + QoU, compared to Table 3 of the main text that uses <span id="A6.p1.1.2" class="ltx_text ltx_font_smallcaps">word2vec</span>) are in Table <a href="#A6.T11" title="Table 11 ‣ Appendix F Analysis on different question and answer embeddings ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. We do not observe significant difference among different embeddings, which is likely due to that both the questions and answers are short (averagely 7 words for questions and 2 for answers).</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Analysis on random decoys</h2>

<figure id="A7.T12" class="ltx_table">
<table id="A7.T12.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A7.T12.1.1.1" class="ltx_tr">
<th id="A7.T12.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="A7.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">(A)</th>
<th id="A7.T12.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">(B)</th>
<th id="A7.T12.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A7.T12.1.2.1" class="ltx_tr">
<th id="A7.T12.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-A</th>
<td id="A7.T12.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">39.6</td>
<td id="A7.T12.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">11.6</td>
<td id="A7.T12.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15.6</td>
</tr>
<tr id="A7.T12.1.3.2" class="ltx_tr">
<th id="A7.T12.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IA</th>
<td id="A7.T12.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">53.4</td>
<td id="A7.T12.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">40.3</td>
<td id="A7.T12.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.2</td>
</tr>
<tr id="A7.T12.1.4.3" class="ltx_tr">
<th id="A7.T12.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-QA</th>
<td id="A7.T12.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">52.3</td>
<td id="A7.T12.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">50.3</td>
<td id="A7.T12.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.9</td>
</tr>
<tr id="A7.T12.1.5.4" class="ltx_tr">
<th id="A7.T12.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MLP-IQA</th>
<td id="A7.T12.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">61.5</td>
<td id="A7.T12.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">60.2</td>
<td id="A7.T12.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.1</td>
</tr>
<tr id="A7.T12.1.6.5" class="ltx_tr">
<th id="A7.T12.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td id="A7.T12.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.0</td>
<td id="A7.T12.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.0</td>
<td id="A7.T12.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Test accuracy (%) on Visual7W, comparing different random decoy strategies to our methods: (A) Orig + uniformly random decoys from unique correct answers, (B) Orig + weighted random decoys w.r.t. their frequencies, and All (Orig+IoU +QoU).</figcaption>
</figure>
<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">We conduct the analysis on sampling random decoys, instead of our IoU-decoys and QoU-decoys, on Visual7W. We collect 6 additional random decoys for each <span id="A7.p1.1.1" class="ltx_text ltx_font_bold">Orig</span> IQA triplet so the answer set will contain 10 candidates, the same as <span id="A7.p1.1.2" class="ltx_text ltx_font_bold">All</span> in Table 3 of the main text. We consider two strategies: (A) uniformly random decoys from unique correct answers, and (B) weighted random decoys w.r.t. their frequencies. The results are in Table <a href="#A7.T12" title="Table 12 ‣ Appendix G Analysis on random decoys ‣ Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We see that different random strategies lead to drastically different results. Moreover, compared to the <span id="A7.p1.1.3" class="ltx_text ltx_font_bold">All</span> column in Table 3 of the main text, we see that our methods lead to a larger relative gap between MLP-IQA to MLP-IA and MLP-QA than both random strategies, demonstrating the effectiveness of our methods in creating decoys.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1704.07120" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1704.07121" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1704.07121">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1704.07121" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1704.07122" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar 15 16:47:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
