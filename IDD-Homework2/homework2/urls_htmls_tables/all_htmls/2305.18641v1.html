<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.18641] Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs</title><meta property="og:description" content="Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.18641">

<!--Generated on Thu Feb 29 03:43:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NewDocumentCommand</span><span id="p1.2" class="ltx_ERROR undefined">\heng</span>
<p id="p1.3" class="ltx_p">mO <sup id="p1.3.1" class="ltx_sup"><span id="p1.3.1.1" class="ltx_text ltx_font_italic" style="color:#00FFFF;">Heng</span></sup><span id="p1.3.2" class="ltx_text ltx_font_sansserif ltx_font_bold" style="color:#00FFFF;">[#1]</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mingyang Zhou<sup id="id11.10.id1" class="ltx_sup">1</sup>, Yi R. Fung<sup id="id12.11.id2" class="ltx_sup">2</sup>, Long Chen<sup id="id13.12.id3" class="ltx_sup">1</sup>, Christopher Thomas<sup id="id14.13.id4" class="ltx_sup">3</sup>, 
<br class="ltx_break"><span id="id6.6.2" class="ltx_text ltx_font_bold">Heng Ji<sup id="id6.6.2.1" class="ltx_sup"><span id="id6.6.2.1.1" class="ltx_text ltx_font_medium">2</span></sup>, Shih-Fu Chang<sup id="id6.6.2.2" class="ltx_sup"><span id="id6.6.2.2.1" class="ltx_text ltx_font_medium">1</span></sup></span>

<br class="ltx_break"><sup id="id15.14.id5" class="ltx_sup">1</sup>Columbia University  <sup id="id16.15.id6" class="ltx_sup">2</sup>University of Illinois at Urbana-Champaign  <sup id="id17.16.id7" class="ltx_sup">3</sup>Virginia Tech 
<br class="ltx_break"><span id="id18.17.id8" class="ltx_text ltx_font_typewriter">{mz2974, cl3695, sc250}@columbia.edu, {yifung2,hengji}@illinois.edu</span>, 
<br class="ltx_break"><span id="id19.18.id9" class="ltx_text ltx_font_typewriter">chris@cs.vt.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.1" class="ltx_p">Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over <math id="id10.1.m1.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="id10.1.m1.1a"><mrow id="id10.1.m1.1.1" xref="id10.1.m1.1.1.cmml"><mn id="id10.1.m1.1.1.2" xref="id10.1.m1.1.1.2.cmml">8</mn><mo id="id10.1.m1.1.1.1" xref="id10.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id10.1.m1.1b"><apply id="id10.1.m1.1.1.cmml" xref="id10.1.m1.1.1"><csymbol cd="latexml" id="id10.1.m1.1.1.1.cmml" xref="id10.1.m1.1.1.1">percent</csymbol><cn type="integer" id="id10.1.m1.1.1.2.cmml" xref="id10.1.m1.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.1.m1.1c">8\%</annotation></semantics></math> performance gains.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient’s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering (<span id="S1.p1.1.1" class="ltx_text ltx_font_bold">CQA</span>) <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>); Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> and Chart Summarization (<span id="S1.p1.1.2" class="ltx_text ltx_font_bold">CS</span>) <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> are introduced.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> and Chart-to-Text <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.18641/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner. </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methods <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Kato et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illustrated in Fig <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, to answer the question <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">"What is the value of India Bar"</span>, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Therefore, in this paper, we propose <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">ChartT5</span>, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model’s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least 8<math id="S1.p5.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><csymbol cd="latexml" id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\%</annotation></semantics></math> performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics <cite class="ltx_cite ltx_citemacro_cite">Barzilay and Lapata (<a href="#bib.bib2" title="" class="ltx_ref">2005</a>)</cite> which values the model’s capability to extract the critical information from the chart.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions are summarized below:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose chart-to-table pre-training for V+L model to learn the capability of interpreting table data from the chart.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate that the pre-trained model consistently outperforms table-based methods on two chart understanding tasks.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct comprehensive ablation studies to validate the effectiveness of chart-to-table pre-training and the proposed pre-training objectives.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2305.18641/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An Overview of ChartT5. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output. </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision and Language Research on Charts</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Researching chart understanding in V+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) <cite class="ltx_cite ltx_citemacro_cite">Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>); Kahou et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>); Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Chaudhry et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>); Obeid and Hoque (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite>, which requires machine learning models to create a summary of key insights conveyed by a chart. <cite class="ltx_cite ltx_citemacro_citet">Hsu et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> collected a large-scale scientific figures captioning dataset from research papers where many images are chart plots.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images <cite class="ltx_cite ltx_citemacro_cite">Kafle et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>); Singh and Shekhar (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>); Chaudhry et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>); Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>); Hsu et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Spreafico and Carenini (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>. The other approach assumes the table data of charts is accessible from the dataset <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>); Masry (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> or can be extracted from the chart images using vision to table techniques <cite class="ltx_cite ltx_citemacro_cite">Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>); Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>. Then, the researchers will either use a table-to-text generation model <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>); Masry (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>); Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> or combine the embedding of tables and charts via a multi-modal fusion method to generate the text output <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>. It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model’s capability to understand and interpret chart information.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Following the table-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre-training on chart-table pairs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vision and Language Pre-training</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Vision and language pre-training has received growing interest over the past few years. Researchers build transformer-based multi-modal fusion models and perform self-supervised learning on a large-scale corpus of image-text pairs to learn robust cross-modal representations that can benefit the performance of various downstream tasks <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>); Lu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>); Tan and Bansal (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Su et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>); Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>); Zhang et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">While the pre-trained models achieve great success on tasks like VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> and Image Captioning <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as “bars” and “lines”.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">To address the limitation of conventional vision and language pre-training, TAP <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> and PreSTU <cite class="ltx_cite ltx_citemacro_cite">Kil et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> propose OCR-based vision and language pre-training frameworks that focus on scene text understanding in natural images where they design various pre-training objectives around the extracted OCR texts. Most recently, Donut <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> and Pix2Struct <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite> propose OCR-free pre-training frameworks, where the pre-trained model directly generates a text output from a raw image input. Donut focuses on document image (<span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually-situated texts such as infographics and user interfaces via parsing web-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre-training that focuses on chart image understanding. Specifically, we propose novel pre-training objectives to parse charts to their underlined tables.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first introduce the dataset for pre-training. We then go over our ChartT5 model architecture and pre-training objectives to predict masked tables from the chart and OCR information.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pre-training Dataset Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To collect large-scale pairs of chart-table data, we collect synthetic data from existing chart question-answering corpora, including PlotQA <cite class="ltx_cite ltx_citemacro_cite">Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, DVQA <cite class="ltx_cite ltx_citemacro_cite">Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, and FigureQA <cite class="ltx_cite ltx_citemacro_cite">Kahou et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>. Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre-training corpus consists of 495K chart-table pairs, which cover a diverse range of chart types. Our pre-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question-answering benchmarks is summarized in table <a href="#S3.T1" title="Table 1 ‣ 3.1 Pre-training Dataset Collection ‣ 3 Method ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Type</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PlotQA</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DVQA</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">FigureQA</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Bar</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">142,587</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">204,514</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40,000</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">387,101</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Line</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center">48,133</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">0</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">40,000</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">88,133</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Pie</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">20,001</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">20,001</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Distribution of the three chart types: bar, line, and pie from different resources in the pre-training corpus.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Overview</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">ChartT5 is an extension of the existing V+L Pre-training framework, VLT5 <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, an encoder-decoder architecture that unifies the vision-language tasks as text generation conditioned on multi-modal inputs.
Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA <cite class="ltx_cite ltx_citemacro_cite">Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, FigureQA <cite class="ltx_cite ltx_citemacro_cite">Kahou et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>, and PlotQA <cite class="ltx_cite ltx_citemacro_cite">Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, the ground-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi-modal features are then concatenated and fused via the multi-layer encoder, and the output hidden vectors can then be used for various pre-training tasks.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Chart Image Encoder</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.6" class="ltx_p">Given an input chart image, to recognize the critical marks (<span id="S3.SS2.SSS1.p1.6.1" class="ltx_text ltx_font_italic">e.g.</span>, bars and lines) of chart images, we first utilize a pre-trained Mask R-CNN object detector from <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> to extract the visual region features <math id="S3.SS2.SSS1.p1.1.m1.4" class="ltx_Math" alttext="\boldsymbol{v}=\{v_{1},v_{2},\cdots,v_{l^{v}}\}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.4a"><mrow id="S3.SS2.SSS1.p1.1.m1.4.4" xref="S3.SS2.SSS1.p1.1.m1.4.4.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.4.4.5" xref="S3.SS2.SSS1.p1.1.m1.4.4.5.cmml">𝒗</mi><mo id="S3.SS2.SSS1.p1.1.m1.4.4.4" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS2.SSS1.p1.1.m1.4.4.3.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.4" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml">v</mi><mn id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.5" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml">v</mi><mn id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.3" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.6" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.7" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml">v</mi><msup id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2.cmml">l</mi><mi id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3.cmml">v</mi></msup></msub><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.8" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.4b"><apply id="S3.SS2.SSS1.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4"><eq id="S3.SS2.SSS1.p1.1.m1.4.4.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.4"></eq><ci id="S3.SS2.SSS1.p1.1.m1.4.4.5.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.5">𝒗</ci><set id="S3.SS2.SSS1.p1.1.m1.4.4.3.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3"><apply id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.2">𝑣</ci><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.2">𝑣</ci><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">⋯</ci><apply id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.2">𝑣</ci><apply id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2">𝑙</ci><ci id="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3">𝑣</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.4c">\boldsymbol{v}=\{v_{1},v_{2},\cdots,v_{l^{v}}\}</annotation></semantics></math>. Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets <cite class="ltx_cite ltx_citemacro_cite">Kahou et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Kafle et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>); Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> which is defined to identify 15 chart-related objects<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.</span></span></span>. For each detected object region, we also extract location features as a 5-d vector: [<math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\frac{x_{1}}{W}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mfrac id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml">x</mi><mn id="S3.SS2.SSS1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml">1</mn></msub><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><divide id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"></divide><apply id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2">𝑥</ci><cn type="integer" id="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3">1</cn></apply><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">\frac{x_{1}}{W}</annotation></semantics></math>,<math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\frac{y_{1}}{H}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mfrac id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><msub id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml">y</mi><mn id="S3.SS2.SSS1.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml">1</mn></msub><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml">H</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><divide id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"></divide><apply id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2">𝑦</ci><cn type="integer" id="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3">1</cn></apply><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">\frac{y_{1}}{H}</annotation></semantics></math>,<math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\frac{x_{2}}{W}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mfrac id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml">x</mi><mn id="S3.SS2.SSS1.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml">2</mn></msub><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><divide id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"></divide><apply id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2">𝑥</ci><cn type="integer" id="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3">2</cn></apply><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">\frac{x_{2}}{W}</annotation></semantics></math>,<math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\frac{y_{2}}{H}" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mfrac id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml"><msub id="S3.SS2.SSS1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.5.m5.1.1.2.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.2.cmml">y</mi><mn id="S3.SS2.SSS1.p1.5.m5.1.1.2.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.3.cmml">2</mn></msub><mi id="S3.SS2.SSS1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml">H</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"><divide id="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"></divide><apply id="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.2">𝑦</ci><cn type="integer" id="S3.SS2.SSS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.3">2</cn></apply><ci id="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\frac{y_{2}}{H}</annotation></semantics></math>,<math id="S3.SS2.SSS1.p1.6.m6.4" class="ltx_Math" alttext="\frac{(y_{2}-y_{1})(x_{2}-x_{1})}{W.H}" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.4a"><mfrac id="S3.SS2.SSS1.p1.6.m6.4.4" xref="S3.SS2.SSS1.p1.6.m6.4.4.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml"><msub id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.2.cmml">y</mi><mn id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.2.cmml">y</mi><mn id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.6.m6.2.2.2.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.3.cmml">​</mo><mrow id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.cmml"><msub id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.2.cmml">x</mi><mn id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.1.cmml">−</mo><msub id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.2.cmml">x</mi><mn id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.SS2.SSS1.p1.6.m6.4.4.4.4" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.3.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.3.3.3.1" xref="S3.SS2.SSS1.p1.6.m6.3.3.3.1.cmml">W</mi><mo lspace="0em" rspace="0.167em" id="S3.SS2.SSS1.p1.6.m6.4.4.4.4.1" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.3a.cmml">.</mo><mi id="S3.SS2.SSS1.p1.6.m6.4.4.4.2" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.2.cmml">H</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.4b"><apply id="S3.SS2.SSS1.p1.6.m6.4.4.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4"><divide id="S3.SS2.SSS1.p1.6.m6.4.4.5.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4"></divide><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2"><times id="S3.SS2.SSS1.p1.6.m6.2.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.3"></times><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1"><minus id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.1"></minus><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.2">𝑦</ci><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.2.3">2</cn></apply><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.2">𝑦</ci><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1"><minus id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.1"></minus><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.2">𝑥</ci><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.2.3">2</cn></apply><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.2">𝑥</ci><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.1.3.3">1</cn></apply></apply></apply><apply id="S3.SS2.SSS1.p1.6.m6.4.4.4.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.4.4.4.3a.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.4.1">formulae-sequence</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3.3.1">𝑊</ci><ci id="S3.SS2.SSS1.p1.6.m6.4.4.4.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.4.2">𝐻</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.4c">\frac{(y_{2}-y_{1})(x_{2}-x_{1})}{W.H}</annotation></semantics></math>], which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>OCR Encoder</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">After extracting the list of the OCR words from the chart image, we obtain a set of OCR text embeddings <math id="S3.SS2.SSS2.p1.1.m1.4" class="ltx_Math" alttext="\boldsymbol{o}=\{o_{1},o_{2},\cdots,o_{l^{o}}\}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.4a"><mrow id="S3.SS2.SSS2.p1.1.m1.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.5" xref="S3.SS2.SSS2.p1.1.m1.4.4.5.cmml">𝒐</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS2.SSS2.p1.1.m1.4.4.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml">o</mi><mn id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.5" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml">o</mi><mn id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.6" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.7" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml">o</mi><msup id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.2.cmml">l</mi><mi id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.3.cmml">o</mi></msup></msub><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.8" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.4b"><apply id="S3.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4"><eq id="S3.SS2.SSS2.p1.1.m1.4.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4"></eq><ci id="S3.SS2.SSS2.p1.1.m1.4.4.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.5">𝒐</ci><set id="S3.SS2.SSS2.p1.1.m1.4.4.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3"><apply id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.2">𝑜</ci><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.2">𝑜</ci><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">⋯</ci><apply id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.2">𝑜</ci><apply id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3">superscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.2">𝑙</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.3">𝑜</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.4c">\boldsymbol{o}=\{o_{1},o_{2},\cdots,o_{l^{o}}\}</annotation></semantics></math> via a learned word embedding layer. We also get each OCR token’s 5-d position vector similar to the visual position vector from the OCR token’s detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x-axis label should share a similar x-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Language Encoder</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Following the setting of the original VLT5 <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, we add a prefix to the flattened underlying table to indicate different pre-training tasks. We then get the table token embeddings <math id="S3.SS2.SSS3.p1.1.m1.4" class="ltx_Math" alttext="\boldsymbol{t}=\{t_{1},t_{2},\cdots,t_{l^{t}}\}" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.4a"><mrow id="S3.SS2.SSS3.p1.1.m1.4.4" xref="S3.SS2.SSS3.p1.1.m1.4.4.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.4.4.5" xref="S3.SS2.SSS3.p1.1.m1.4.4.5.cmml">𝒕</mi><mo id="S3.SS2.SSS3.p1.1.m1.4.4.4" xref="S3.SS2.SSS3.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS2.SSS3.p1.1.m1.4.4.3.3" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.4" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.2.cmml">t</mi><mn id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.5" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.2" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.2.cmml">t</mi><mn id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.3" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.6" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.7" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.2" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.2.cmml">t</mi><msup id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.2" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.2.cmml">l</mi><mi id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.3" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.3.cmml">t</mi></msup></msub><mo stretchy="false" id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.8" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.4b"><apply id="S3.SS2.SSS3.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4"><eq id="S3.SS2.SSS3.p1.1.m1.4.4.4.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.4"></eq><ci id="S3.SS2.SSS3.p1.1.m1.4.4.5.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.5">𝒕</ci><set id="S3.SS2.SSS3.p1.1.m1.4.4.3.4.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3"><apply id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.2">𝑡</ci><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.2">𝑡</ci><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">⋯</ci><apply id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.2">𝑡</ci><apply id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.2">𝑙</ci><ci id="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.4.4.3.3.3.3.3">𝑡</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.4c">\boldsymbol{t}=\{t_{1},t_{2},\cdots,t_{l^{t}}\}</annotation></semantics></math> with a shared word embedding layer. We apply the original T5’s <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>); <span class="ltx_ref ltx_missing_citation ltx_ref_self">herzi-2020-tapas</span></cite>. We leave the exploration of the specialized table position embedding for chart table pre-training in the future.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p"><span id="S3.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Scene Text Copy Mechanism.</span> A critical ingredient to the success of chart-to-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary.
To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens <math id="S3.SS2.SSS3.p2.1.m1.4" class="ltx_Math" alttext="\{&lt;\text{ocr}\textunderscore 1&gt;,&lt;\text{ocr}\textunderscore 2&gt;,\cdots,&lt;\text{ocr}\textunderscore{l^{o}}&gt;\}" display="inline"><semantics id="S3.SS2.SSS3.p2.1.m1.4a"><mrow id="S3.SS2.SSS3.p2.1.m1.4.4.3" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p2.1.m1.4.4.3.4" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml">{</mo><mrow id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.cmml"><mtext id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2a.cmml">ocr</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1a" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1.cmml">​</mo><mn id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.4" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.4.cmml">1</mn></mrow><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.2.1.cmml">&gt;</mo></mrow><mo id="S3.SS2.SSS3.p2.1.m1.4.4.3.5" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml">,</mo><mrow id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.2" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.2.1.cmml">&lt;</mo><mrow id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.cmml"><mtext id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2a.cmml">ocr</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1a" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1.cmml">​</mo><mn id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.4" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.4.cmml">2</mn></mrow><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.3" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.2.1.cmml">&gt;</mo></mrow><mo id="S3.SS2.SSS3.p2.1.m1.4.4.3.6" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS2.SSS3.p2.1.m1.4.4.3.7" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml">,</mo><mrow id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.2" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.2.1.cmml">&lt;</mo><mrow id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.cmml"><mtext id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2a.cmml">ocr</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1a" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.cmml"><mi id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.2" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.2.cmml">l</mi><mi id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.3" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.3.cmml">o</mi></msup></mrow><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.3" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.2.1.cmml">&gt;</mo></mrow><mo stretchy="false" id="S3.SS2.SSS3.p2.1.m1.4.4.3.8" xref="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.4b"><set id="S3.SS2.SSS3.p2.1.m1.4.4.4.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3"><apply id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.2">expectation</csymbol><apply id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1"><times id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.1"></times><ci id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2a.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2"><mtext id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.2">ocr</mtext></ci><ci id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.3">_</ci><cn type="integer" id="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.4.cmml" xref="S3.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.4">1</cn></apply></apply><apply id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.2">expectation</csymbol><apply id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1"><times id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.1"></times><ci id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2a.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2"><mtext id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.2">ocr</mtext></ci><ci id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.3">_</ci><cn type="integer" id="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.4.cmml" xref="S3.SS2.SSS3.p2.1.m1.3.3.2.2.1.1.4">2</cn></apply></apply><ci id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">⋯</ci><apply id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.2.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.2">expectation</csymbol><apply id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1"><times id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.1"></times><ci id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2a.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2"><mtext id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.2">ocr</mtext></ci><ci id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.3">_</ci><apply id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.2">𝑙</ci><ci id="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.4.4.3.3.1.1.4.3">𝑜</ci></apply></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.4c">\{&lt;\text{ocr}\textunderscore 1&gt;,&lt;\text{ocr}\textunderscore 2&gt;,\cdots,&lt;\text{ocr}\textunderscore{l^{o}}&gt;\}</annotation></semantics></math>, which corresponds to the detected OCR texts.
As illustrated in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre-training, as all the plot images are synthesized from a paired table, the one-to-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table prediction from the relevant resources.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pre-training Objectives</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">Given the chart-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="t_{m}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑡</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">t_{m}</annotation></semantics></math> with the remaining table info <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="t_{\backslash m}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">t</mi><mrow id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.3.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.3.1" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">\</mo><mi id="S3.SS3.p1.2.m2.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑡</ci><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><ci id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.1">\</ci><csymbol cd="latexml" id="S3.SS3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2">absent</csymbol><ci id="S3.SS3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">t_{\backslash m}</annotation></semantics></math> as well as the chart image region <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\boldsymbol{v}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\boldsymbol{v}</annotation></semantics></math> and the scene text <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="\boldsymbol{o}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">𝒐</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝒐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">\boldsymbol{o}</annotation></semantics></math>. Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre-training objectives. (2) We increase the masking rate from 15<math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mo id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><csymbol cd="latexml" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\%</annotation></semantics></math> to 45<math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mo id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><csymbol cd="latexml" id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">\%</annotation></semantics></math>, as the masked table token has less dependence on the surrounding table values.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart-table pre-training and the two pre-training objectives.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Chart Question Answering.</span> Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>, which collects question-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1) Human-written question-answer pairs; and (2) machine-generated question-answer pairs derived from the human-written chart summaries. In total 32.7K question-answer pairs are collected on 21.9K scraped chart images, where about 9.6K question-and-answer pairs are human-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>); Methani et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><csymbol cd="latexml" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\%</annotation></semantics></math> of the gold answer). For non-numerical answers, the prediction needs to be exactly matched to the gold-standard answer.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Chart Summarization.</span> Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart-to-Text benchmark <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>, which collects roughly 36.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human-written which is well grounded on the chart image.
Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> to split the two sets for training and testing. We adopt BLEU-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.2" class="ltx_p"><span id="S4.p4.2.1" class="ltx_text ltx_font_bold">Implementation details.</span>
We initialized our ChartT5 from <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\text{T5}_{\text{base}}" display="inline"><semantics id="S4.p4.1.m1.1a"><msub id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mtext id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2a.cmml">T5</mtext><mtext id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.1.1.2a.cmml" xref="S4.p4.1.m1.1.1.2"><mtext id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">T5</mtext></ci><ci id="S4.p4.1.m1.1.1.3a.cmml" xref="S4.p4.1.m1.1.1.3"><mtext mathsize="70%" id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\text{T5}_{\text{base}}</annotation></semantics></math> and pre-trained on our self-collected corpus for 30 epochs with a batch size of 60. We used Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite> with a linear warm-up for the first 5<math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.p4.2.m2.1a"><mo id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><csymbol cd="latexml" id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\%</annotation></semantics></math> training steps, and the peak learning rate is set as 1e-4. After warming up, a linear decayed learning-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre-training run as our final checkpoint for fine-tuning.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">We also applied warming-up for downstream fine-tuning to gradually increase the learning rate to the pick value during the first 5<math id="S4.p5.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.p5.1.m1.1a"><mo id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><csymbol cd="latexml" id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\%</annotation></semantics></math> of training epochs. After that, a linear decayed learning-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine-tune ChartT5 for 60 epochs with a peak learning rate 2e-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation split. On the CS task, we use batch size 20 and a peak learning rate 5e-5. On the Pew split, we fine-tune ChartT5for 20 epochs, and on Statista, we fine-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one-time runs.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We first compare ChartT5 to various state-of-the-art methods with or without pre-training on the two downstream tasks.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.2.3.1.1.1" class="ltx_text">Model</span></th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">ChartQA</td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<td id="S4.T2.2.4.2.1" class="ltx_td ltx_align_center">Human</td>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center">Augment</td>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_align_center">Overall</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">T5</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">25.12</td>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">56.96</td>
<td id="S4.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">41.56</td>
</tr>
<tr id="S4.T2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Tapas</th>
<td id="S4.T2.2.6.4.2" class="ltx_td ltx_align_center">28.72</td>
<td id="S4.T2.2.6.4.3" class="ltx_td ltx_align_center">53.84</td>
<td id="S4.T2.2.6.4.4" class="ltx_td ltx_align_center">41.28</td>
</tr>
<tr id="S4.T2.2.7.5" class="ltx_tr">
<th id="S4.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VLT5</th>
<td id="S4.T2.2.7.5.2" class="ltx_td ltx_align_center">26.24</td>
<td id="S4.T2.2.7.5.3" class="ltx_td ltx_align_center">56.88</td>
<td id="S4.T2.2.7.5.4" class="ltx_td ltx_align_center">41.56</td>
</tr>
<tr id="S4.T2.2.8.6" class="ltx_tr">
<th id="S4.T2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VisionTapas</th>
<td id="S4.T2.2.8.6.2" class="ltx_td ltx_align_center">29.60</td>
<td id="S4.T2.2.8.6.3" class="ltx_td ltx_align_center">61.44</td>
<td id="S4.T2.2.8.6.4" class="ltx_td ltx_align_center">45.52</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\text{VLT5}_{pre}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mtext id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2a.cmml">VLT5</mtext><mrow id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.3.1a" xref="S4.T2.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.m1.1.1.3.4" xref="S4.T2.1.1.1.m1.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.m1.1.1.2"><mtext id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">VLT5</mtext></ci><apply id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.m1.1.1.3.2">𝑝</ci><ci id="S4.T2.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3.3">𝑟</ci><ci id="S4.T2.1.1.1.m1.1.1.3.4.cmml" xref="S4.T2.1.1.1.m1.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\text{VLT5}_{pre}</annotation></semantics></math></th>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.1" class="ltx_text ltx_font_bold">40.08</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">63.60</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t">51.84</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\text{VisionTapas}_{pre}" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><msub id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml"><mtext id="S4.T2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.1.m1.1.1.2a.cmml">VisionTapas</mtext><mrow id="S4.T2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.1.m1.1.1.3.cmml"><mi id="S4.T2.2.2.1.m1.1.1.3.2" xref="S4.T2.2.2.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.1.m1.1.1.3.1" xref="S4.T2.2.2.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.2.2.1.m1.1.1.3.3" xref="S4.T2.2.2.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.1.m1.1.1.3.1a" xref="S4.T2.2.2.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.2.2.1.m1.1.1.3.4" xref="S4.T2.2.2.1.m1.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><apply id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.1.m1.1.1.2a.cmml" xref="S4.T2.2.2.1.m1.1.1.2"><mtext id="S4.T2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.1.m1.1.1.2">VisionTapas</mtext></ci><apply id="S4.T2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.1.m1.1.1.3"><times id="S4.T2.2.2.1.m1.1.1.3.1.cmml" xref="S4.T2.2.2.1.m1.1.1.3.1"></times><ci id="S4.T2.2.2.1.m1.1.1.3.2.cmml" xref="S4.T2.2.2.1.m1.1.1.3.2">𝑝</ci><ci id="S4.T2.2.2.1.m1.1.1.3.3.cmml" xref="S4.T2.2.2.1.m1.1.1.3.3">𝑟</ci><ci id="S4.T2.2.2.1.m1.1.1.3.4.cmml" xref="S4.T2.2.2.1.m1.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\text{VisionTapas}_{pre}</annotation></semantics></math></th>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center">32.56</td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center">61.60</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center">47.08</td>
</tr>
<tr id="S4.T2.2.9.7" class="ltx_tr">
<th id="S4.T2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Pix2Struct</th>
<td id="S4.T2.2.9.7.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.9.7.3" class="ltx_td ltx_align_center"> -</td>
<td id="S4.T2.2.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.9.7.4.1" class="ltx_text ltx_font_bold">56.00</span></td>
</tr>
<tr id="S4.T2.2.10.8" class="ltx_tr">
<th id="S4.T2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">ChartT5</th>
<td id="S4.T2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_bb">31.8</td>
<td id="S4.T2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.8.3.1" class="ltx_text ltx_font_bold">74.4</span></td>
<td id="S4.T2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_bb">53.16</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.
</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Evaluation on CQA</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.2" class="ltx_p">We compare ChartT5 with SOTA non-pretraining and pre-training methods on CQA tasks. The best-performed non-pretraining baselines are introduced in <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>. The authors first predict the table data from the chart image via an automatic data extraction tool <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>. Then they extend various language-only models (T5, Tapas) and multi-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre-training baselines, we compare to <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\text{VLT5}_{pre}" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mtext id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2a.cmml">VLT5</mtext><mrow id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.1.m1.1.1.3.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.1.m1.1.1.3.1a" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p1.1.m1.1.1.3.4" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2a.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2"><mtext id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">VLT5</mtext></ci><apply id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3"><times id="S4.SS1.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S4.SS1.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.2">𝑝</ci><ci id="S4.SS1.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.3">𝑟</ci><ci id="S4.SS1.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\text{VLT5}_{pre}</annotation></semantics></math> and <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\text{VisionTapas}_{pre}" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><msub id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml"><mtext id="S4.SS1.SSS1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2a.cmml">VisionTapas</mtext><mrow id="S4.SS1.SSS1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.2.m2.1.1.3.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.2.m2.1.1.3.1a" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3.4" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.2.m2.1.1.2a.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2"><mtext id="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2">VisionTapas</mtext></ci><apply id="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3"><times id="S4.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.2">𝑝</ci><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3">𝑟</ci><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.4.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">\text{VisionTapas}_{pre}</annotation></semantics></math> which pre-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Main Results ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p2.2" class="ltx_p"><span id="S4.SS1.SSS1.p2.2.1" class="ltx_text ltx_font_bold">Comparison to Non-Pretraining Method</span>
Even without access to the predicted tables, ChartT5 has outperformed all non-pretraining methods by a large margin (a minimum 7.3<math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mo id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\%</annotation></semantics></math> gain on the overall performance). ChartT5 also outperforms all non-pretraining baselines on the human-written questions and machine-generated questions. Although the predicted table covers 54<math id="S4.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mo id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">\%</annotation></semantics></math> of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre-training to help the model to obtain the relevant table information for chart understanding.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p3.6" class="ltx_p"><span id="S4.SS1.SSS1.p3.6.1" class="ltx_text ltx_font_bold">Comparison to Pre-training Method</span>
Although the performance of VLT5 and VisionTapas is improved significantly by pre-training on additional CQA data, ChartT5 still outperform them by at least 1.3<math id="S4.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mo id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">\%</annotation></semantics></math>. Specifically, on machine-augmented questions, ChartT5 outperforms <math id="S4.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\text{VLT5}_{pre}" display="inline"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><msub id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml"><mtext id="S4.SS1.SSS1.p3.2.m2.1.1.2" xref="S4.SS1.SSS1.p3.2.m2.1.1.2a.cmml">VLT5</mtext><mrow id="S4.SS1.SSS1.p3.2.m2.1.1.3" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS1.p3.2.m2.1.1.3.2" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.2.m2.1.1.3.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.2.m2.1.1.3.3" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.2.m2.1.1.3.1a" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.2.m2.1.1.3.4" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><apply id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p3.2.m2.1.1.2a.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.2"><mtext id="S4.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.2">VLT5</mtext></ci><apply id="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3"><times id="S4.SS1.SSS1.p3.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.1"></times><ci id="S4.SS1.SSS1.p3.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.2">𝑝</ci><ci id="S4.SS1.SSS1.p3.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.3">𝑟</ci><ci id="S4.SS1.SSS1.p3.2.m2.1.1.3.4.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">\text{VLT5}_{pre}</annotation></semantics></math> by 8<math id="S4.SS1.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><mo id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">\%</annotation></semantics></math>. However, both <math id="S4.SS1.SSS1.p3.4.m4.1" class="ltx_Math" alttext="\text{visionTapas}_{pre}" display="inline"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><msub id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml"><mtext id="S4.SS1.SSS1.p3.4.m4.1.1.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.2a.cmml">visionTapas</mtext><mrow id="S4.SS1.SSS1.p3.4.m4.1.1.3" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.cmml"><mi id="S4.SS1.SSS1.p3.4.m4.1.1.3.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.4.m4.1.1.3.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.4.m4.1.1.3.3" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.4.m4.1.1.3.1a" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.4.m4.1.1.3.4" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><apply id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p3.4.m4.1.1.2a.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.2"><mtext id="S4.SS1.SSS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.2">visionTapas</mtext></ci><apply id="S4.SS1.SSS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3"><times id="S4.SS1.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.1"></times><ci id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2">𝑝</ci><ci id="S4.SS1.SSS1.p3.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.3">𝑟</ci><ci id="S4.SS1.SSS1.p3.4.m4.1.1.3.4.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">\text{visionTapas}_{pre}</annotation></semantics></math> and <math id="S4.SS1.SSS1.p3.5.m5.1" class="ltx_Math" alttext="\text{VLT5}_{pre}" display="inline"><semantics id="S4.SS1.SSS1.p3.5.m5.1a"><msub id="S4.SS1.SSS1.p3.5.m5.1.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.cmml"><mtext id="S4.SS1.SSS1.p3.5.m5.1.1.2" xref="S4.SS1.SSS1.p3.5.m5.1.1.2a.cmml">VLT5</mtext><mrow id="S4.SS1.SSS1.p3.5.m5.1.1.3" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.cmml"><mi id="S4.SS1.SSS1.p3.5.m5.1.1.3.2" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.5.m5.1.1.3.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.5.m5.1.1.3.3" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p3.5.m5.1.1.3.1a" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.5.m5.1.1.3.4" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.5.m5.1b"><apply id="S4.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p3.5.m5.1.1.2a.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.2"><mtext id="S4.SS1.SSS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.2">VLT5</mtext></ci><apply id="S4.SS1.SSS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.3"><times id="S4.SS1.SSS1.p3.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.1"></times><ci id="S4.SS1.SSS1.p3.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.2">𝑝</ci><ci id="S4.SS1.SSS1.p3.5.m5.1.1.3.3.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.3">𝑟</ci><ci id="S4.SS1.SSS1.p3.5.m5.1.1.3.4.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.5.m5.1c">\text{VLT5}_{pre}</annotation></semantics></math> achieve better accuracy on the human split, which means that the in-domain question answering objectives helps the model to improve the numerical reasoning capability.
ChartT5 underperforms Pix2Struct by 2.3<math id="S4.SS1.SSS1.p3.6.m6.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p3.6.m6.1a"><mo id="S4.SS1.SSS1.p3.6.m6.1.1" xref="S4.SS1.SSS1.p3.6.m6.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.6.m6.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.6.m6.1c">\%</annotation></semantics></math> on the overall test split. However, pix2struct is pre-trained on a more than 100 times larger pre-training corpus than the rest of the pre-training methods. Given the same scale of the pre-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3">Statista</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Pew</th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">CS</th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">CIDER</th>
<th id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">CS</th>
<th id="S4.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">CIDER</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">T5-OCR</th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">35.29</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">73.77</td>
<td id="S4.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.43</td>
<td id="S4.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.5.1" class="ltx_text ltx_font_bold">10.49</span></td>
<td id="S4.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">40.87</td>
<td id="S4.T3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.1.7.1" class="ltx_text ltx_font_bold">2.20</span></td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BART-OCR</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.4.2.5" class="ltx_td ltx_align_center">9.09</td>
<td id="S4.T3.1.4.2.6" class="ltx_td ltx_align_center">39.99</td>
<td id="S4.T3.1.4.2.7" class="ltx_td ltx_align_center">1.97</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">T5-TAB</th>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center">37.01</td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center">75.72</td>
<td id="S4.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.5.3.4.1" class="ltx_text ltx_font_bold">4.68</span></td>
<td id="S4.T3.1.5.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.5.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.5.3.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<th id="S4.T3.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BART-TAB</th>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center">36.36</td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center">77.14</td>
<td id="S4.T3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">4.40</td>
<td id="S4.T3.1.6.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<th id="S4.T3.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">ChartT5</th>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.7.5.2.1" class="ltx_text ltx_font_bold">37.51</span></td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.7.5.3.1" class="ltx_text ltx_font_bold">82.16</span></td>
<td id="S4.T3.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">3.45</td>
<td id="S4.T3.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb">9.05</td>
<td id="S4.T3.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.7.5.6.1" class="ltx_text ltx_font_bold">55.1</span></td>
<td id="S4.T3.1.7.5.7" class="ltx_td ltx_align_center ltx_border_bb">1.23</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table-based method does not have results on Pew split.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Evaluation on Chart Summarization</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">For the chart summarization task, we compare ChartT5 to the best non-pretraining approaches introduced in <cite class="ltx_cite ltx_citemacro_cite">Kantharaj et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>. Given a chart image, The authors build the chart summarization models by extending the pre-trained language generation model T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> and BART<cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> whose generation processes are conditioned on: (1) a set of scene texts extracted by a trained OCR detector. (2) the ground truth table that is paired with the chart. The evaluation result is summarized in Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1 Evaluation on CQA ‣ 4.1 Main Results ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">From Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1 Evaluation on CQA ‣ 4.1 Main Results ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5-OCR by almost 1.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores across both Statista and Pew sets.
The under-performance on BLEU and CIDER indicates that Chart-table pre-training is limited to benefit high-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart-table pre-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre-training objectives, such as chart-to-text pre-training and chart-table pre-training goals, to facilitate the model with diverse strengths.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text">Pretraining?</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Question Types</th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Table</th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Human</th>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Augment</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<th id="S4.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">No</th>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">60.7</td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">30.8</td>
<td id="S4.T4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">66.7</td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<th id="S4.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Yes</th>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.2.2.1" class="ltx_text ltx_font_bold">64.7</span></td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.2.3.1" class="ltx_text ltx_font_bold">31.8</span></td>
<td id="S4.T4.1.4.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.2.4.1" class="ltx_text ltx_font_bold">74.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation Study on Chart Table Pre-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human-written questions, and machine-generated questions. </figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We conduct ablation experiments to validate the effectiveness of chart-table pre-training and the pre-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Chart-Table Pre-training</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We conduct detailed analyses on the effectiveness of chart-table pre-training. First, we measure the performance gain from the chart-table pre-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart-table pre-training by picking three subsets of questions that measure different capabilities of the model: (1) Human-written questions, (2) Machine-generated questions, and (3) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table <a href="#S4.T4" title="Table 4 ‣ 4.1.2 Evaluation on Chart Summarization ‣ 4.1 Main Results ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. From Table <a href="#S4.T4" title="Table 4 ‣ 4.1.2 Evaluation on Chart Summarization ‣ 4.1 Main Results ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we find that after chart-table pre-training the model’s performance on these three sets of questions is all improved. The most significant gain is obtained on machine-generated questions, which mainly focus on extractive-type questions. This indicates that chart-table pre-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table-cover questions, where the model demonstrates significant improvement in the capability of chart-to-table interpretation.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Question Types</th>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<th id="S4.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Human</th>
<th id="S4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Augment</th>
<th id="S4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.3.1" class="ltx_tr">
<th id="S4.T5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Full</th>
<td id="S4.T5.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">31.8</td>
<td id="S4.T5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">74.4</td>
<td id="S4.T5.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">53.1</td>
</tr>
<tr id="S4.T5.1.4.2" class="ltx_tr">
<th id="S4.T5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">- MVP</th>
<td id="S4.T5.1.4.2.2" class="ltx_td ltx_align_center">30.9</td>
<td id="S4.T5.1.4.2.3" class="ltx_td ltx_align_center">73.7</td>
<td id="S4.T5.1.4.2.4" class="ltx_td ltx_align_center">52.3</td>
</tr>
<tr id="S4.T5.1.5.3" class="ltx_tr">
<th id="S4.T5.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">- MHP</th>
<td id="S4.T5.1.5.3.2" class="ltx_td ltx_align_center">31.2</td>
<td id="S4.T5.1.5.3.3" class="ltx_td ltx_align_center">68.3</td>
<td id="S4.T5.1.5.3.4" class="ltx_td ltx_align_center">49.7</td>
</tr>
<tr id="S4.T5.1.6.4" class="ltx_tr">
<th id="S4.T5.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">- STC</th>
<td id="S4.T5.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">30.8</td>
<td id="S4.T5.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">72.4</td>
<td id="S4.T5.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">51.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation Study on the two proposed pre-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre-training objectives and the scene-text-copy mechanism. We report the results of different ablation experiments on both human and machine-generated splits as well as the overall performance.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2305.18641/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An error prediction from our model due to noisy OCR prediction</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2305.18641/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An error prediction from our model due to complex multi-hop reasoning</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Pre-training Objectives</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We validate the effectiveness of the two pre-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre-training objective at a time and pre-train the ChartT5with only one table prediction task. The pre-trained model is then fine-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table <a href="#S4.T5" title="Table 5 ‣ 4.2.1 Chart-Table Pre-training ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model’s mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine-generated question-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Scene Text Copy</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">We also validate the effectiveness of the scene-text-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5. The result is displayed in Table <a href="#S4.T5" title="Table 5 ‣ 4.2.1 Chart-Table Pre-training ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Disabling the scene-text-copy mechanism leads to a 1.5<math id="S4.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mo id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">\%</annotation></semantics></math> overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene-text-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Error Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi-hop reasoning.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Noisy OCR Prediction.</span> As an OCR-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2.1 Chart-Table Pre-training ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>; the model localizes the right scene text “1.18” to answer the question, but the OCR text is mistakenly detected as “1:18”. To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5% performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene-text detection benchmark. Another promising direction is to attempt OCR-free end-to-end plot recognition method like Pix2Struct <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Multi-Hop Reasoning.</span> Our model is also quite vulnerable to handling complex questions requiring multi-hop reasoning. An example is shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2.1 Chart-Table Pre-training ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre-training on the mathematic reasoning datasets to address this limitation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose ChartT5 to enhance the vision language model’s ability to understand chart images via chart-table pre-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table-based non-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart-to-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart-table pre-training, and we find that the proposed pre-training is extremely helpful to extract accurate information from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data-efficient settings <cite class="ltx_cite ltx_citemacro_cite">Hsu et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Zeng et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> and for evidence retrieval tasks <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>); Ji et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although introducing chart value prediction objective, it only provides minor improvement to the model’s performance on doing complex reasoning. There is still a large room to improve the model’s capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities.
Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Ethics Statement</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">When we collect the pre-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public’s understanding of chart content due to the potential bias from our training corpus. Therefore, we don’t recommend using our model for any real-world decision on chart images.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary-related data and baseline model outputs.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barzilay and Lapata (2005)</span>
<span class="ltx_bibblock">
Regina Barzilay and Mirella Lapata. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/H05-1042" title="" class="ltx_ref ltx_href">Collective content
selection for concept-to-text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Language Processing</em>, pages
331–338, Vancouver, British Columbia, Canada. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhry et al. (2020)</span>
<span class="ltx_bibblock">
Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal,
and Ajay Joshi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV45572.2020.9093269" title="" class="ltx_ref ltx_href">LEAF-QA:
locate, encode &amp; attend for figure question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Winter Conference on Applications of Computer Vision,
WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020</em>, pages 3501–3510.
IEEE.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick. 2015.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2021)</span>
<span class="ltx_bibblock">
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.

</span>
<span class="ltx_bibblock">Unifying vision-and-language tasks via text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2022)</span>
<span class="ltx_bibblock">
I-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee, Scott Miller, Prem Natarajan,
Kai-Wei Chang, and Nanyun Peng. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.138" title="" class="ltx_ref ltx_href">DEGREE: A
data-efficient generation-based event extraction model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1890–1908, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Ting-Yao Hsu, C Lee Giles, and Ting-Hao Huang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-emnlp.277" title="" class="ltx_ref ltx_href">SciCap: Generating captions for scientific figures</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 3258–3264, Punta Cana, Dominican Republic. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Wei Ji, Yinwei Wei, Zhedong Zheng, Hao Fei, and Tat-seng Chua. 2023.

</span>
<span class="ltx_bibblock">Deep multimodal learning for information retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ACM International Conference on Multimedia</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle et al. (2018)</span>
<span class="ltx_bibblock">
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018.

</span>
<span class="ltx_bibblock">Dvqa: Understanding data visualizations via question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle et al. (2020)</span>
<span class="ltx_bibblock">
Kushal Kafle, Robik Shrestha, Scott Cohen, Brian Price, and Christopher Kanan.
2020.

</span>
<span class="ltx_bibblock">Answering questions about data visualizations using efficient bimodal
fusion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">The IEEE Winter Conference on Applications of Computer
Vision</em>, pages 1498–1507.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahou et al. (2018)</span>
<span class="ltx_bibblock">
Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Ákos Kádár, Adam
Trischler, and Yoshua Bengio. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=SyunbfbAb" title="" class="ltx_ref ltx_href">FigureQA: An
annotated figure dataset for visual reasoning</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantharaj et al. (2022)</span>
<span class="ltx_bibblock">
Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar,
Enamul Hoque, and Shafiq Joty. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.277" title="" class="ltx_ref ltx_href">Chart-to-text:
A large-scale benchmark for chart summarization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 4005–4023,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kato et al. (2022)</span>
<span class="ltx_bibblock">
Hajime Kato, Mitsuru Nakazawa, Hsuan-Kung Yang, Mark Chen, and Björn Stenger.
2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV51458.2022.00261" title="" class="ltx_ref ltx_href">Parsing line
chart images using linear programming</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV)</em>, pages 2553–2562.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kil et al. (2022)</span>
<span class="ltx_bibblock">
Jihyung Kil, Soravit Changpinyo, Xi Chen, Hexiang Hu, Sebastian Goodman,
Wei-Lun Chao, and Radu Soricut. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2209.05534" title="" class="ltx_ref ltx_href">Prestu:
Pre-training for scene-text understanding</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2020)</span>
<span class="ltx_bibblock">
Dae Hyun Kim, Enamul Hoque, and Maneesh Agrawala. 2020.

</span>
<span class="ltx_bibblock">Answering questions about charts and generating visual explanations.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 CHI Conference on Human Factors in
Computing Systems</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong
Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022.

</span>
<span class="ltx_bibblock">Ocr-free document understanding transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_href">Adam: A method for
stochastic optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian
Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina
Toutanova. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2210.03347" title="" class="ltx_ref ltx_href">Pix2struct:
Screenshot parsing as pretraining for visual language understanding</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational
Linguistics</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.02265</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Keming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, and Muhao Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.10786" title="" class="ltx_ref ltx_href">Multi-hop evidence retrieval
for cross-document relation extraction</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2021)</span>
<span class="ltx_bibblock">
Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.microsoft.com/en-us/research/publication/chartocr-data-extraction-from-charts-images-via-a-deep-hybrid-framework/" title="" class="ltx_ref ltx_href">Chartocr: Data extraction from charts images via a deep hybrid framework</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Winter Conference on Applications of Computer
Vision (WACV)</em>. The Computer Vision Foundation.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry (2021)</span>
<span class="ltx_bibblock">
Ahmed Masry. 2021.

</span>
<span class="ltx_bibblock">Integrating image data extraction and table parsing methods for chart
question answering.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al. (2022)</span>
<span class="ltx_bibblock">
Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.177" title="" class="ltx_ref ltx_href">ChartQA: A benchmark for question answering about charts with visual and
logical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 2263–2279, Dublin, Ireland. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Methani et al. (2020)</span>
<span class="ltx_bibblock">
Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. 2020.

</span>
<span class="ltx_bibblock">Plotqa: Reasoning over scientific plots.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">The IEEE Winter Conference on Applications of Computer
Vision (WACV)</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Obeid and Hoque (2020)</span>
<span class="ltx_bibblock">
Jason Obeid and Enamul Hoque. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.inlg-1.20" title="" class="ltx_ref ltx_href">Chart-to-text:
Generating natural language descriptions for charts by adapting the
transformer model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Conference on Natural
Language Generation</em>, pages 138–147, Dublin, Ireland. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the limits
of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh and Shekhar (2020)</span>
<span class="ltx_bibblock">
Hrituraj Singh and Sumit Shekhar. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.264" title="" class="ltx_ref ltx_href">STL-CQA:
Structure-based transformers with localization and encoding for chart
question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 3275–3284, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spreafico and Carenini (2020)</span>
<span class="ltx_bibblock">
Andrea Spreafico and Giuseppe Carenini. 2020.

</span>
<span class="ltx_bibblock">Neural data-driven captioning of time-series line charts.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Advanced Visual
Interfaces</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2019)</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
2019.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.08530</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang,
Cha Zhang, Lei Zhang, and Jiebo Luo. 2021.

</span>
<span class="ltx_bibblock">Tap: Text-aware pre-training for text-vqa and text-caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2020)</span>
<span class="ltx_bibblock">
Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. 2020.

</span>
<span class="ltx_bibblock">TaBERT: Pretraining for joint understanding of textual and tabular
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Annual Conference of the Association for Computational
Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2023)</span>
<span class="ltx_bibblock">
Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian
Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas
Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=G2Q2Mh3avow" title="" class="ltx_ref ltx_href">Socratic models:
Composing zero-shot multimodal reasoning with language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao. 2021.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 5579–5588.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.18640" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.18641" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.18641">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.18641" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.18642" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 03:43:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
