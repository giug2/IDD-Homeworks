<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation</title>
<!--Generated on Mon Sep 30 22:30:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.00266v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2.SS1" title="In 2 Related Work ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Sketch Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2.SS2" title="In 2 Related Work ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Sketch Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.SS1" title="In 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Class-Agnostic Visio-Temporal Detector</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.SS2" title="In 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Post-Processing Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.SS3" title="In 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Dataset Preparation for Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>The FrISS Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4.SS1" title="In 4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Sketch Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4.SS2" title="In 4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Sketch Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4.SS3" title="In 4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Statistics and Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS1" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS2" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Sketch Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS3" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS4" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS5" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Comparison Against State-of-the-art (SOTA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS6" title="In 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S6" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S7" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1a" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S1 </span>Details on Post-Processing Module</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.SS1" title="In S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S1.1 </span>Hyperparameter Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.SS2" title="In S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S1.2 </span>Post-Processing Time &amp; Memory Footprint</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2a" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S2 </span>Additional Details on RGB Coloring Technique</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3a" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S3 </span>Additional Analysis on External Classifiers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4a" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S4 </span>Additional Visual Results on Scene Sketch Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5a" title="In Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5 </span>Additional Details on FrISS Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS1a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.1 </span>UI of Data Collection Web Application</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS2a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.2 </span>Visual Comparison of FrISS to Other Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS3a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.3 </span>Details of Textual Scene Descriptions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS4a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.4 </span>Detailed Analysis of FrISS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS5a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.5 </span>Common Categories of FrISS and Other Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS6a" title="In S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">S5.6 </span>Ethical Considerations in Data Collection</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aleyna Kütük          Tevfik Metin Sezgin
<br class="ltx_break"/>Department of Computer Engineering, KUIS AI Center, Koç University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">{akutuk21, mtsezgin}@ku.edu.tr</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Scene sketch semantic segmentation is a crucial task for various applications including sketch-to-image retrieval and scene understanding. Existing sketch segmentation methods treat sketches as bitmap images, leading to the loss of temporal order among strokes due to the shift from vector to image format. Moreover, these methods struggle to segment objects from categories absent in the training data. In this paper, we propose a Class-Agnostic Visio-Temporal Network (CAVT) for scene sketch semantic segmentation. CAVT employs a class-agnostic object detector to detect individual objects in a scene and groups the strokes of instances through its post-processing module. This is the first approach that performs segmentation at both the instance and stroke levels within scene sketches. Furthermore, there is a lack of free-hand scene sketch datasets with both instance and stroke-level class annotations. To fill this gap, we collected the largest Free-hand Instance- and Stroke-level Scene Sketch Dataset (FrISS) that contains 1K scene sketches and covers 403 object classes with dense annotations. Extensive experiments on FrISS and other datasets demonstrate the superior performance of our method over state-of-the-art scene sketch segmentation models. The code and dataset will be made public after acceptance.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="575" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Sample scene sketches from FrISS dataset, each paired with corresponding textual scene descriptions. For each pair, the left image shows the black-and-white sketch, while the right image highlights the instance and stroke-level class annotations.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Sketching is a rapid and widely adopted way for humans to visually express ideas. Especially with the rise of touchscreen technology, understanding hand-drawn sketches has become an essential task in the field of human-computer interaction. The field of sketch understanding includes various tasks such as sketch recognition, sketch-based image retrieval, and sketch segmentation. Sketch semantic segmentation stands out as a pivotal task, offering broad applicability in the analysis of sketches and facilitating tasks like sketch-based image retrieval. Despite the considerable attention given to semantic segmentation in natural images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib21" title="">21</a>]</cite>, this task
remains relatively underexplored in sketches. Earlier studies on sketch segmentation have mostly concentrated on segmenting single-object sketches into semantically meaningful parts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib18" title="">18</a>]</cite>. On the other hand, recent attention has shifted towards scene-level sketch semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.2" style="width:422.3pt;height:178.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.1pt,44.1pt) scale(0.669715704171594,0.669715704171594) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.2.1">
<tr class="ltx_tr" id="S1.T1.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.2.1.1.1">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.2"># of Sketches</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.3"># of Cat.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.4">Vector</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.5">Free-hand</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.6">Scene-level</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.7">Publicly Available</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.1.8">Annot. Type</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.2.1.2.1">QMUL Shoe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.2.2">419</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.2.3">1</td>
<td class="ltx_td ltx_border_t" id="S1.T1.2.1.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.2.5">✓</td>
<td class="ltx_td ltx_border_t" id="S1.T1.2.1.2.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.2.7">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.2.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.3.1">QMUL Chair <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.3.2">297</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.3.3">1</td>
<td class="ltx_td" id="S1.T1.2.1.3.4"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.3.5">✓</td>
<td class="ltx_td" id="S1.T1.2.1.3.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.3.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.3.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.4.1">Sketchy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.2">75K</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.3">125</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.5">✓</td>
<td class="ltx_td" id="S1.T1.2.1.4.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.4.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.5.1">TU-Berlin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib7" title="">7</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.2">20K</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.3">250</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.5">✓</td>
<td class="ltx_td" id="S1.T1.2.1.5.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.5.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.6.1">QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.2">50M+</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.3">345</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.5">✓</td>
<td class="ltx_td" id="S1.T1.2.1.6.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.6.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.2.1.7.1">SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.7.2">7K+</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.7.3">45</td>
<td class="ltx_td ltx_border_t" id="S1.T1.2.1.7.4"></td>
<td class="ltx_td ltx_border_t" id="S1.T1.2.1.7.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.7.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.7.7">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.1.7.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.8.1">SketchyCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib8" title="">8</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.8.2">14K+</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.8.3">17</td>
<td class="ltx_td" id="S1.T1.2.1.8.4"></td>
<td class="ltx_td" id="S1.T1.2.1.8.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.8.6">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.8.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.8.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.9.1">SKY-Scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.9.2">7K+</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.9.3">30</td>
<td class="ltx_td" id="S1.T1.2.1.9.4"></td>
<td class="ltx_td" id="S1.T1.2.1.9.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.9.6">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.9.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.9.8">C</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.10.1">TUB-Scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.10.2">7K+</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.10.3">35</td>
<td class="ltx_td" id="S1.T1.2.1.10.4"></td>
<td class="ltx_td" id="S1.T1.2.1.10.5"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.10.6">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.10.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.10.8">C</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.11.1">CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.2">331</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.3">74</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.5">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.6">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.11.8">C, I</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.12.1">FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.2">10K</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.3">92-150</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.5">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.6">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.7">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.12.8">D</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.2.1.13.1">SFSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.2">12K+</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.3">40</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.4">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.5">✓</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.6">✓</td>
<td class="ltx_td" id="S1.T1.2.1.13.7"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.1.13.8">C</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.1.14">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.2.1.14.1">FrISS (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.2">1K</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.3">403</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.7">✓*</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.2.1.14.8">C, D, I</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.4.2" style="font-size:90%;">Summary of the sketch datasets. C, I, and D denote class-level annotations, instance-level annotations, and scene sketch textual descriptions, respectively. ✓*: the dataset will be publicly available after acceptance.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Sketches are processed either as stroke sequences or bitmap images. Many methodologies treat sketches as images and address sketch segmentation similarly to image segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib32" title="">32</a>]</cite>. However, this direct approach often leads to the loss of temporal stroke information. As sketches consist of stroke sequences, capturing the stroke order can significantly enhance semantic segmentation performance. Moreover, current research on scene sketch segmentation mainly focuses on assigning a class to each pixel or stroke within a scene, thus segmenting scene sketches at the class level. Unfortunately, these methods cannot distinguish between individual objects that belong to the same class, such as two zebra instances in the same scene. To overcome these limitations, we introduce the Class-Agnostic Visio-Temporal Network (CAVT) that processes scene sketches and generates stroke-level groupings of instances without relying on predefined class labels. Our approach leverages visual information via an object detector and incorporates the temporal order of strokes using both a post-processing module and an RGB coloring technique.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The primary challenge for scene sketch semantic segmentation lies in the absence of large-scale scene sketch datasets. Existing scene sketch datasets are typically constructed by inserting pre-defined clip-art or free-hand single-instance sketches into the layouts of reference images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite>. These datasets preserve the scene sketches in image format, limiting their utilization in stroke-based sketch methods. More recently, scene datasets have been collected by instructing participants to draw scenes based on reference natural images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite>. However, this often results in the loss of participants’ natural drawing behavior, as individuals tend to replicate the object positions and postures from the reference images.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we collected the largest Free-hand Instance- and Stroke-level Scene Sketch Dataset (FrISS), consisting of free-hand scene sketches in vector format, accompanied by textual descriptions, verbal audio recordings, and annotations at both the stroke and instance levels. To capture natural drawing behavior, participants were provided only with textual scene descriptions during the drawing process, without being shown any reference images. This approach ensures that FrISS features a diverse range of scene sketches that are not mere copies of reference images. Moreover, we avoided prolonged drawing sessions or multiple attempts, thus preventing artificially polished scene sketches. In summary, our main contributions are highlighted as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose CAVT, a novel scene sketch semantic segmentation pipeline, that utilizes both visual and temporal information in the scene. This is the first study on scene sketch semantic segmentation that works at both instance and stroke levels.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce FrISS, a densely annotated dataset that includes 1K free-hand scene sketches covering 403 object categories. FrISS can promote future stroke-based scene-level studies.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct extensive experiments on FrISS and other free-hand scene sketch datasets and show that our approach achieves state-of-the-art performance.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sketch Semantic Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Existing works on sketch semantic segmentation mostly focus on single-object sketch datasets and divide an object into its semantically valid parts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib18" title="">18</a>]</cite>. On the other hand, scene-level sketch semantic segmentation aims to distinguish individual object instances within the scene. Regarding the processing of sketches, these studies can be divided into two main groups: image-based and sequence-based. Image-based methods typically treat sketches as raster images and output pixel-level segmentation predictions; whereas sequence-based methods utilize stroke-level information and assign semantic labels to each stroke in a sketch. Even if the majority of studies on single-object sketch semantic segmentation lie in the sequence-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib29" title="">29</a>]</cite>, there are not many studies conducted on stroke-level scene sketch semantic segmentation. This is mostly due to the lack of large-scale scene sketch datasets with stroke-level class annotations.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S2.F2.g1" src="x2.png" width="713"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">The overall pipeline of CAVT</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Prior works on scene sketch semantic segmentation treat the task as a semantic image segmentation problem, disregarding the stroke order <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib32" title="">32</a>]</cite>. SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite> is the pioneering study that assigns object categories at the pixel level. Ge <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">et al</em>.<span class="ltx_text" id="S2.SS1.p2.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> proposed a deep-shallow feature fusion network based on DeepLab-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib4" title="">4</a>]</cite>, examining the influence of local details on scene sketch segmentation. Bourouis <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.3">et al</em>.<span class="ltx_text" id="S2.SS1.p2.1.4"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite> introduced the first language-supervised scene sketch segmentation method by utilizing sketch captions. In contrast, Zhang <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.5">et al</em>.<span class="ltx_text" id="S2.SS1.p2.1.6"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite> developed an RNN-GCN-based architecture, marking the first stroke-level approach to scene sketch semantic segmentation. Their study is the most relevant to ours since they also utilize visual, sequential, and spatial information on stroke sequences. However, their code is not publicly available for comparison.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Scene sketch segmentation works mostly focus on assigning each pixel or stroke to a specific class in a given scene. Therefore, different objects belonging to the same category cannot be distinguished at the instance level. In contrast, we propose a novel class-agnostic scene segmentation pipeline that can differentiate object instances in a given scene, regardless of their classes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sketch Datasets</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Sketch datasets can be categorized into two primary types: single-object and scene sketch datasets. Single-object sketch datasets feature one object instance per sketch, while scene sketch datasets encompass drawings with multiple objects. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> provides a summary of the sketch datasets and our proposed scene sketch dataset, FrISS.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">QMUL Shoe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib34" title="">34</a>]</cite>, QMUL Chair <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib34" title="">34</a>]</cite>, and Sketchy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib24" title="">24</a>]</cite> are multi-modal single-object sketch datasets that contain corresponding natural images paired with each sketch. TU Berlin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib7" title="">7</a>]</cite> is the first large-scale free-hand single-object sketch dataset, that is collected via crowdsourcing. QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite> is the largest free-hand single sketch dataset, and it is gathered through an online game.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">A growing number of large-scale scene-level sketch datasets have been proposed due to the importance of higher-level sketch understanding. SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite> pioneered this field, assembling clip art-like single-object sketches onto reference images as layout templates. SketchyCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib8" title="">8</a>]</cite> is another synthetically generated scene sketch dataset that integrates free-hand single-object datasets into the corresponding mask area of COCO-Stuff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib3" title="">3</a>]</cite> real images. Ge <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.1">et al</em>.<span class="ltx_text" id="S2.SS2.p3.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> introduced two more semi-synthetic scene datasets, called as SKY-Scene and TUB-Scene. Although synthetic scene sketch data generation offers a quick solution to the scarcity of large-scale scene datasets, it lacks the authenticity of human drawing behavior. Moreover, none of these synthetic datasets are available in vector storage formats, rendering them unsuitable for our stroke-based approach. FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite> stands out as the first free-hand scene sketch dataset collected in vector format, accompanied by scene captions. However, it lacks stroke- or object-level annotations, hindering semantic segmentation experiments. SFSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite> is another free-hand scene sketch dataset, offering both vector storage format and stroke-level class annotations, but it is not publicly available. Lastly, CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite> emerges as the sole publicly accessible free-hand scene sketch dataset with instance-level class annotation in the vector storage format. Thus, we leverage CBSC to test our network. To address the lack of free-hand scene sketch datasets, we introduce FrISS, which contains free-hand scene sketches annotated at both instance and stroke levels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, the architecture of CAVT and the generation process of its training dataset are explained. As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2.F2" title="Figure 2 ‣ 2.1 Sketch Semantic Segmentation ‣ 2 Related Work ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, CAVT consists of two sub-modules: (i) the Class-Agnostic Visio-Temporal object detector and (ii) the Post-Processing module. First, each scene sketch is pre-processed using an RGB coloring technique to preserve the temporal stroke order. These color-coded sketches are then passed through the Class-Agnostic Object Detector to generate prediction boxes. Subsequently, the Post-Processing module refines the detector’s outputs using a set of rules for stroke-level instance grouping by leveraging temporal stroke order and spatial features. Finally, CAVT produces stroke groups belonging to object instances in the scene.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Class-Agnostic Visio-Temporal Detector</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To proceed with an appropriate object detector, we investigated the cross-domain object detection studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib28" title="">28</a>]</cite> in the literature. DASS-Detector <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib28" title="">28</a>]</cite> leverages YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib10" title="">10</a>]</cite> and stands out for its high performance within its domain. Inspired by their work, we also utilize YOLOX in our study. Fully-supervised detectors are typically trained to recognize specific predefined classes, restricting their ability to detect objects beyond these predetermined categories. To address this constraint, YOLOX is trained in a class-agnostic manner, in which the detector solely predicts potential object areas without the need for classification. We conduct an ablation study to evaluate the impact of our approach and discuss it in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS6" title="5.6 Ablation Study ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">5.6</span></a>. Our trained detector offers predictions concerning potential object regions within sketch scenes. These predictions solely approximate object-bounding boxes on the coordinate plane. Therefore, we introduce a post-processing module designed to group object strokes by leveraging the bounding box predictions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Post-Processing Module</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">This module performs stroke-level segmentation for individual sketches by utilizing the output from the object detector. The full algorithm for the post-processing module is provided in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#algorithm1" title="Algorithm 1 ‣ S1.1 Hyperparameter Optimization ‣ S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> in the Supplementary Material. The steps involved in this module are as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The predicted bounding boxes are sorted in ascending order based on their area, from smallest to largest.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">IoU-Based Stroke-to-Box Assignment:</span> Starting with the smallest bounding box, the stroke sequence with the highest Intersection over Union (IoU) compared to the selected box is identified. If the IoU value surpasses a threshold called <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">IoU_threshold</span>, the corresponding stroke set is assigned to that bounding box.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">Assigning Neighboring Strokes to Boxes:</span> The unassigned strokes are then evaluated based on their overlap ratio. For each of the remaining longest stroke sequences, if the overlap ratio between the sequence and the nearest bounding box exceeds a threshold called <span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.2">OR_threshold</span>, the stroke set is assigned to that box. The overlap ratio is calculated by dividing the area of intersection between the bounding box and the stroke set by the total area of the stroke set.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i4.p1.1.1">Grouping Unassigned Strokes:</span> Strokes that remain unassigned to any bounding box after these steps are considered separate objects, and their coordinates are added to the list of predicted boxes.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">The coordinates of each bounding box are updated based on the latest stroke assignments. Each box’s dimensions are adjusted to become the smallest bounding box enclosing its assigned stroke set.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">These steps are repeated until no further changes occur in stroke groupings, ensuring that each stroke is assigned to a corresponding object bounding box.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Both the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">IoU_threshold</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">OR_threshold</span> are determined using a grid-search algorithm (see in Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1a" title="S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S1</span></a>). The object detector produces bounding boxes without class predictions, so strokes are grouped without class information. This enables the utilization of an external sketch object classifier, offering several advantages: (1) Both stroke- and image-based single sketch classifiers can be employed, each capable of identifying broader or narrower object categories, or sketches with varying complexities; (2) Inference time and required memory can be adjusted based on the chosen classifiers.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Dataset Preparation for Training</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Object detection models are widely used in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib30" title="">30</a>]</cite>. However, their direct application to the sketch domain faces challenges due to the domain shift from real-life images to scene sketches. Achieving fully supervised detector training on sketches necessitates a large-scale instance-level scene sketch dataset. Furthermore, the training dataset should maintain strokes in vector storage format to utilize temporal cues effectively. Unfortunately, none of the existing large-scale datasets offer both instance-level annotation and vector storage format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.2" style="width:447.2pt;height:111.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-88.6pt,22.1pt) scale(0.716294746584022,0.716294746584022) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.2.1">
<tr class="ltx_tr" id="S3.T2.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.2.1.1.1" rowspan="2"><span class="ltx_text" id="S3.T2.2.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.1.1.2" rowspan="2"><span class="ltx_text" id="S3.T2.2.1.1.2.1">Cats.</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T2.2.1.1.4">Category per sketch</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.2.1.1.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T2.2.1.1.7">Objects per sketch</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.2.1.1.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.1.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T2.2.1.1.10">Strokes per sketch</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.2">
<td class="ltx_td" id="S3.T2.2.1.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.2">Max</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.3">Min</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.4">Mean</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.2.5"></td>
<td class="ltx_td" id="S3.T2.2.1.2.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.7">Max</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.8">Min</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.9">Mean</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.2.10"></td>
<td class="ltx_td" id="S3.T2.2.1.2.11"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.12">Max</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.13">Min</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.2.14">Mean</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.2.1.3.1">SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.1.3.2">45</td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.4">19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.5">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.6">6.88</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.2.1.3.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.3.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.9">94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.10">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.11">16.71</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.2.1.3.12"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.2.1.3.13"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.14">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.15">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.1.3.16">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.2.1.4.1">SketchyCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib8" title="">8</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.1.4.2">17</td>
<td class="ltx_td" id="S3.T2.2.1.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.4">6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.5">1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.6">2.33</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.4.7"></td>
<td class="ltx_td" id="S3.T2.2.1.4.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.9">35</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.10">2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.11">10.93</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.4.12"></td>
<td class="ltx_td" id="S3.T2.2.1.4.13"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.14">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.15">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.4.16">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.2.1.5.1">CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.1.5.2">74</td>
<td class="ltx_td" id="S3.T2.2.1.5.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.4">10</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.5">3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.6">4.23</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.5.7"></td>
<td class="ltx_td" id="S3.T2.2.1.5.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.9">16</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.10">3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.11">4.72</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.5.12"></td>
<td class="ltx_td" id="S3.T2.2.1.5.13"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.14">185</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.15">6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.5.16">33.14</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.2.1.6.1">FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.1.6.2">92 / 150</td>
<td class="ltx_td" id="S3.T2.2.1.6.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.4">5 / 25</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.5">1 / 1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.6">1.37 / 7.17</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.6.7"></td>
<td class="ltx_td" id="S3.T2.2.1.6.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.9">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.10">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.11">-</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.6.12"></td>
<td class="ltx_td" id="S3.T2.2.1.6.13"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.14">561</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.15">5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.6.16">75.86</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.2.1.7.1">SFSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.1.7.2">40</td>
<td class="ltx_td" id="S3.T2.2.1.7.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.4">11</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.5">1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.6">4.46</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.7.7"></td>
<td class="ltx_td" id="S3.T2.2.1.7.8"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.9">43</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.10">2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.11">7.76</td>
<td class="ltx_td ltx_border_r" id="S3.T2.2.1.7.12"></td>
<td class="ltx_td" id="S3.T2.2.1.7.13"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.14">699</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.15">9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.1.7.16">146.64</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.1.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.1.8.1">FrISS (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.1.8.2">403</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S3.T2.2.1.8.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.4">10</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.6">4.33</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.1.8.7"></td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S3.T2.2.1.8.8"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.9">30</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.10">1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.11">6.04</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.1.8.12"></td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S3.T2.2.1.8.13"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.14">186</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.15">4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.1.8.16">35.81</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.4.2" style="font-size:90%;">Comparison and statistics of scene sketch datasets</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To train an object detector for the sketch domain, we created a large-scale, synthetically generated scene sketch dataset. To ensure our object detector’s robustness across various categories and drawing styles, we utilized QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>, which offers a wide range of categories and diverse sketch styles. Each scene is composed of a minimum of 2 and a maximum of 8 randomly chosen objects from a pool of 345 categories, with 70K drawing instances per category. Objects are randomly scaled to have a large side length ranging from 50 to 700 pixels and positioned randomly within the scene. To prevent extreme overlapping between objects, we ensure that the intersection-over-union (IOU) value between them remains below 0.35. Scenes are created in two potential sizes: 720x1280 or 1280x720 pixels. To capture the temporal order, each stroke is assigned a color from a spectrum spanning blue to red based on its order (see Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2a" title="S2 Additional Details on RGB Coloring Technique ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S2</span></a>). In total, we generated 11.5K synthetic drawing scenes under these settings, allocating 10K for training and 1.5K for validation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>The FrISS Dataset</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We propose the largest Free-hand Instance- and Stroke-level Scene sketch dataset (FrISS) that includes scene sketches in vector format, stroke-level class and instance annotations, sketch-text pairs, and verbal audio clips paired with each scene. The data construction process involves two primary stages: (i) sketch collection and (ii) sketch annotation. This section elaborates on these stages and provides statistics and analysis on the FrISS dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Sample scenes taken from FrISS that are drawn by three individuals by referring to the same textual scene description</span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Sketch Collection</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We developed a web application to collect scene sketches, following similar data collection methods as in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite>. Visuals of the web application are provided in Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS1a" title="S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5.1</span></a>. We recruited 100 volunteer participants with varying levels of drawing skills, each tasked with creating 10 distinct scene sketches based on textual scene descriptions. The textual scene descriptions provided during the drawing phase are either sourced from captions within the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib20" title="">20</a>]</cite> or constructed by us. Details on the generation of scene descriptions, along with examples, are provided in Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS3a" title="S5.3 Details of Textual Scene Descriptions ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5.3</span></a>. To avoid influencing participants with predefined layouts or poses, no visual references were provided. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4.F3" title="Figure 3 ‣ 4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>, the arrangement and diversity of objects in the scenes varied significantly when participants sketched scenes without visual guidance.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Each participant was given 1.5 minutes to complete each scene. The time limit was determined through pilot studies with a group of volunteers. These studies showed that a shorter time often led to incomplete drawings, while a longer time resulted in excessively detailed sketches. Participants were allowed to redraw objects within the time limit but were not permitted to restart the scene with extra time. Allowing multiple attempts could lead to unrealistically polished sketches. Additionally, participants were asked to verbally describe their scenes as they drew. To ensure comfort and clarity, they were encouraged to speak in their native language. The verbal explanations were recorded during the drawing process, enabling FrISS to support research on tasks such as speech-based sketch studies.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Sketch Annotation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the second phase of data collection, participants were presented with scenes they had previously drawn. They annotated each stroke with both instance and category information. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> shows sample sketch-text pairs from the FrISS dataset and their colored annotations. Different colors are used to visualize instance-level annotations of the objects from the same category (e.g., pizzas, mountains).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To avoid interrupting the natural drawing process, we collected sketch annotations separately from the drawing phase. This phase was conducted under our supervision to ensure accurate annotations. Each stroke in a scene was assigned to its corresponding object category, with incomplete or ambiguous strokes labeled as <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">’incomplete’</span> and subsequently excluded from the scene. Additionally, we manually reviewed the annotations for accuracy and assessed the quality of the scenes. Any mislabeled object strokes were either corrected or eliminated from the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Statistics and Analysis</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.T2" title="Table 2 ‣ 3.3 Synthetic Dataset Preparation for Training ‣ 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> provides a statistical comparison of various scene sketch datasets, focusing on category, object, and stroke counts per sketch. Our dataset covers a wider range of object categories compared to previous scene datasets. Additionally, each scene sketch was collected within a 1.5-minute timeframe, resulting in simpler sketches resembling participants’ daily drawings. Other free-hand scene sketch datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite> allow more time for drawings and multiple drawing attempts, which results in extremely detailed scene sketches. On the other hand, our scene sketches contain an average of approximately 36 strokes per scene, significantly fewer than other datasets in terms of complexity. Refer to Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> for sample scenes from our FrISS dataset. Thus, FrISS stands out by including both instance- and stroke-level class annotations. Additional scene samples and comparisons are available in Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5a" title="S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We utilize temporal stroke information in our pipeline, thus it limits the range of applicable datasets for evaluation. Therefore, we assessed our approach using only the test partitions of FrISS and CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>. FrISS comprises 1K free-hand scene sketches spanning 403 object categories, with 236 categories overlapping with the QuickDraw classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>. We reserved 500 scene sketches for testing, while the remaining sketches were divided into validation (145 sketches) and training (355 sketches) sets. CBSC dataset consists of 222 free-hand scene sketches in its test partition, covering 74 object categories and these categories fully align with QuickDraw, except for the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">’person’</span> class. However, the visual characteristics of the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">’yoga’</span> class of the QuickDraw closely resemble those of the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">’person’</span> class in other scene sketch datasets. Therefore, we map the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">’person’</span> class to <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.5">’yoga’</span> class during the evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Sketch Classification</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">As discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3" title="3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>, CAVT generates segmented stroke groups without any category assignments. Thus, we utilized one stroke-based and one image-based sketch classifier. First, we investigated the performances of state-of-the-art stroke-based sketch classifiers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite>. Since Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite> achieves superior performance, it was selected as the external classifier for categorizing sketches segmented by CAVT. Secondly, we trained various CNN-based classifiers using the training sets of QuickDraw and FrISS. Among these, Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>
outperforms others. Hence, we further utilize our trained Inception-V3 as a second external classifier. In the following sections, we call the end-to-end CAVT + Sketchformer pipeline as <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">CAVT-S</span>, and CAVT + pre-trained-Inception-V3 pipeline as <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">CAVT-I</span>. A detailed analysis of classifiers can be found in Supplementary Material Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3a" title="S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Earlier works utilize metrics that are commonly used to evaluate image segmentation models. Hence, we follow the standard four metrics that are used in our competitor models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite> for fair comparison. These metrics are listed as follows: Overall Pixel Accuracy (OVAcc), Mean Pixel Accuracy (MeanAcc), Mean Intersection over Union (MIoU), and Frequency Weighted Intersection over Union (FWIoU). Still, there is no available metric specifically designed for stroke-level scene sketch semantic segmentation. Thus, we propose two additional metrics for stroke-level evaluation:</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">All or Nothing (AoN):</span> evaluates the ratio of correctly predicted stroke groups. If a single stroke of an object is mislabeled, then the result becomes incorrect.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Stroke-level Intersection over Union (S-IoU):</span> calculates the largest overlap ratio of the actual and the predicted stroke groups, and averages the overlap ratio for all ground truth stroke groups.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Our competitor models perform class-level segmentation and require bitmap images as input. Therefore, we could not compare our results with earlier works on these metrics.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.2">
<tr class="ltx_tr" id="S5.T3.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.1.1" rowspan="2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S5.T3.2.1.1.1">Model</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T3.2.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T3.2.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CBSC-SKY</td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T3.2.1.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CBSC-SS</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.2">
<td class="ltx_td ltx_border_r" id="S5.T3.2.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td" id="S5.T3.2.2.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
<td class="ltx_td" id="S5.T3.2.2.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">LDP</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T3.2.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">54.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">52.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">33.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">37.96</td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.3.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">47.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">36.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">23.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">32.93</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.4">
<td class="ltx_td ltx_align_left" id="S5.T3.2.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-S</td>
<td class="ltx_td ltx_border_r" id="S5.T3.2.4.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td" id="S5.T3.2.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">70.24</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">73.89</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">51.21</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">59.22</td>
<td class="ltx_td" id="S5.T3.2.4.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">71.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">73.29</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">51.92</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.4.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">60.30</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.5">
<td class="ltx_td ltx_align_left" id="S5.T3.2.5.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-I</td>
<td class="ltx_td ltx_border_r" id="S5.T3.2.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td" id="S5.T3.2.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.4.1">73.76</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.5.1">74.08</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.6.1">53.38</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.7.1">61.89</span></td>
<td class="ltx_td" id="S5.T3.2.5.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.9.1">73.13</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.10.1">75.26</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.11.1">52.45</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.5.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.12.1">60.56</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.6">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.6.1" rowspan="2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S5.T3.2.6.1.1">Model</span></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T3.2.6.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_border_tt" id="S5.T3.2.6.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T3.2.6.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FrISS-SKY</td>
<td class="ltx_td ltx_border_tt" id="S5.T3.2.6.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T3.2.6.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FrISS-SS</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.7">
<td class="ltx_td ltx_border_r" id="S5.T3.2.7.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td" id="S5.T3.2.7.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
<td class="ltx_td" id="S5.T3.2.7.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">LDP</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T3.2.8.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.8.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">44.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">27.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">14.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">31.89</td>
<td class="ltx_td ltx_border_t" id="S5.T3.2.8.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">41.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">29.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">15.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">27.82</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.9">
<td class="ltx_td ltx_align_left" id="S5.T3.2.9.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-S</td>
<td class="ltx_td ltx_border_r" id="S5.T3.2.9.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td" id="S5.T3.2.9.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">65.39</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.9.5.1">62.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.9.6.1">34.88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.9.7.1">54.86</span></td>
<td class="ltx_td" id="S5.T3.2.9.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">60.02</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.9.10.1">60.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.9.11.1">33.09</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.9.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">48.11</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.10">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.2.10.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-I</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S5.T3.2.10.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_border_b" id="S5.T3.2.10.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.10.4.1">66.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">62.08</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">34.18</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">54.40</td>
<td class="ltx_td ltx_border_b" id="S5.T3.2.10.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.10.9.1">61.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">55.07</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">31.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.2.10.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.2.10.12.1">48.19</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Comparison of CAVT against LDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> on the CBSC-SS CBSC-SKY, FrISS-SS, and FrISS-SKY datasets.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.2" style="width:496.9pt;height:82.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.2pt,10.2pt) scale(0.799696089673709,0.799696089673709) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.2.1">
<tr class="ltx_tr" id="S5.T4.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.2.1.1.1" rowspan="2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text" id="S5.T4.2.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T4.2.1.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CBSC</td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T4.2.1.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FrISS-QD</td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.1.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T4.2.1.1.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FrISS</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.1.2">
<td class="ltx_td" id="S5.T4.2.1.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
<td class="ltx_td" id="S5.T4.2.1.2.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
<td class="ltx_td" id="S5.T4.2.1.2.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MeanAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.2.15" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">FWIoU</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.2.1.3.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">OV</td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">62.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">62.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">45.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">49.34</td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.3.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">64.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">54.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">38.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">50.68</td>
<td class="ltx_td ltx_border_t" id="S5.T4.2.1.3.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">41.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">41.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.15" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">25.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.1.3.16" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">29.92</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.2.1.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-S*</td>
<td class="ltx_td" id="S5.T4.2.1.4.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">81.21</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">81.87</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">68.71</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">70.13</td>
<td class="ltx_td" id="S5.T4.2.1.4.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">80.90</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.4.9.1">76.99</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">64.95</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">69.53</td>
<td class="ltx_td" id="S5.T4.2.1.4.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.15" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">-</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.1.4.16" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T4.2.1.5.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">CAVT-I*</td>
<td class="ltx_td ltx_border_b" id="S5.T4.2.1.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.3.1">83.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.4.1">82.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.5.1">71.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.6" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.6.1">73.14</span></td>
<td class="ltx_td ltx_border_b" id="S5.T4.2.1.5.7" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.8" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.8.1">81.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.9" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">75.50</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.10" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.10.1">65.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.11" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.11.1">70.97</span></td>
<td class="ltx_td ltx_border_b" id="S5.T4.2.1.5.12" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.13" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.13.1">72.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.14" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.14.1">46.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.15" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.15.1">37.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.2.1.5.16" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.5.16.1">58.05</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.4.2" style="font-size:90%;">Comparison of CAVT with Open Vocabulary (OV) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite> on the CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>, FrISS-QD, and FrISS datasets.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Implementation Details</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The sole trainable component of our network is the Class-Agnostic Visio-Temporal Object Detector, built upon the YOLOX framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib10" title="">10</a>]</cite>. During model training, we employed the MMDetection library, training YOLOX with default configurations while modifying only the total number of categories to 1. Our training process utilizes a single Tesla T4 GPU with a batch size of 16, spanning 600 epochs. We compared our results with the Local Detail Perception (LDP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and the Open Vocabulary (OV) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite>. However, when comparing CAVT with these models, several adjustments to the datasets and our evaluation process are necessary:</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">LDP is trained on categories from SKY-Scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite>. Additionally, we use Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite> as our sketch classifier, which only supports the 345 categories from QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>. To ensure a fair comparison, we created five distinct sub-datasets: <span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.1">FrISS-SKY</span> and <span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.2">CBSC-SKY</span> include objects from the common classes shared between QuickDraw, SKY-Scene, and FrISS/CBSC; <span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.3">FrISS-SS</span> and <span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.4">CBSC-SS</span> feature objects from the common categories of QuickDraw, SketchyScene, and FrISS/CBSC; <span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.5">FrISS-QD</span> comprises objects from the common classes of FrISS and QuickDraw.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">The OV model operates without relying on pixel or stroke-level annotations, instead, it uses sketch-caption pairs. During inference, captions are generated by concatenating ground truth object categories, and OV predicts the correct class label from the given set of object classes. To ensure a fair comparison with OV, we developed alternative versions of our pipelines (CAVT-S* and CAVT-I*) that restrict the possible object classes to those present in the ground truth scene.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="526" id="S5.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Visual comparison of our method with LDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and OV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite> models that are evaluated on the FrISS-SS dataset.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:496.9pt;height:94.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-69.6pt,13.1pt) scale(0.781151246484552,0.781151246484552) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.1">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S5.T5.1.1.1.2">Components:</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="6" id="S5.T5.1.1.1.3">CBSC</td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S5.T5.1.1.1.5">FrISS</td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T5.1.1.1.1">FrISS<sup class="ltx_sup" id="S5.T5.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.1.1.1.1.1.1">sub</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.2">
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.2.1">T</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.2.2">CA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.2.3">PP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.4">AoN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.5">S-IoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.6">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.7">MAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.8">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.9">FWIoU</td>
<td class="ltx_td" id="S5.T5.1.1.2.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.11">AoN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.12">S-IoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.13">OVAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.14">MAcc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.15">MIoU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.16">FWIoU</td>
<td class="ltx_td" id="S5.T5.1.1.2.17"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.18">AoN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.19">S-IoU</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3">
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.3.1"></td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.3.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T5.1.1.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.4">39.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.5">68.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.6">39.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.7">38.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.8">23.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.9">28.53</td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.3.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.11">28.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.12">57.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.13">25.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.14">14.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.15">6.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.16">17.32</td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.1.3.17"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.18">23.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.3.19">58.23</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4">
<td class="ltx_td" id="S5.T5.1.1.4.1"></td>
<td class="ltx_td" id="S5.T5.1.1.4.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.4.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.4">48.95</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.5">73.57</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.6">48.83</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.7">46.68</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.8">30.70</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.9">37.05</td>
<td class="ltx_td" id="S5.T5.1.1.4.10"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.11">33.40</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.12">60.07</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.13">29.77</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.14">18.21</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.15">8.83</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.16">20.74</td>
<td class="ltx_td" id="S5.T5.1.1.4.17"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.18">26.11</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.19">58.37</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.5">
<td class="ltx_td" id="S5.T5.1.1.5.1"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.5.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4">58.64</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.5">81.09</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.6">52.00</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.7">51.67</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.8">32.57</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.9">39.55</td>
<td class="ltx_td" id="S5.T5.1.1.5.10"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.11">47.09</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.12"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.5.12.1">72.89</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.13">32.89</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.14"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.5.14.1">22.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.15">9.98</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.16">23.17</td>
<td class="ltx_td" id="S5.T5.1.1.5.17"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.18">39.98</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.19"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.5.19.1">71.67</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.1">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.1.1.6.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.4.1">68.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.5.1">84.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.6.1">60.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.7.1">57.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.8.1">38.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.9"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.9.1">47.96</span></td>
<td class="ltx_td ltx_border_b" id="S5.T5.1.1.6.10"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.11"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.11.1">51.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.12">72.77</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.13"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.13.1">36.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.14">22.04</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.15"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.15.1">10.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.16"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.16.1">26.12</span></td>
<td class="ltx_td ltx_border_b" id="S5.T5.1.1.6.17"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.18"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.6.18.1">41.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.19">71.34</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.10.2.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.3.1" style="font-size:90%;">Ablation study for the impact of including or excluding three components: temporal stroke order (T), class-agnostic training (CA), and post-processing steps (PP). FrISS<sup class="ltx_sup" id="S5.T5.3.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.1.1.1">sub</span></sup>: calculates metrics for a subset of categories in FrISS that are not part of QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>. The metrics <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.2">OVAcc</span>, <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.3">MeanAcc</span>, <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.4">MIoU</span>, and <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.5">FWIoU</span> are evaluated using CAVT-I, since it also supports the complete class set of FrISS.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Comparison Against State-of-the-art (SOTA)</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The comparison results of our model with prior works on the different subsets of CBSC and FrISS datasets are shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.T3" title="Table 3 ‣ 5.3 Evaluation Metrics ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.T4" title="Table 4 ‣ 5.3 Evaluation Metrics ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>. Across all datasets and metric variations, under identical conditions, the gap between LDP and CAVT-S or CAVT-I is consistently between 15% - 39%, but it narrows to 6% - 31% with OV. Still, our pipeline outperforms previous SOTA by a significant margin.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F4" title="Figure 4 ‣ 5.4 Implementation Details ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> shows the qualitative comparison between our method, LDP, and OV models. Our pipeline leverages stroke information and does not assign different class labels to any point in a single stroke. This allows us to generate more coherent segmentation outputs. Moreover, we share our instance-level visual results in the rightmost column of the figure. Different from the SOTA models, we can segment different instances from the same category (2nd and 3rd rows). We provide additional visual comparisons in Supplementary Material.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">In this experiment, we examine the individual effects of each component of CAVT. The key components include the use of temporal stroke order, class-agnostic training, and the post-processing module. To evaluate the impact of the post-processing module, we implement a simple stroke grouping technique as a baseline for comparison. In this method, each stroke is assigned to the nearest predicted bounding box, and the strokes assigned to the same box are grouped as a single object.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.T5" title="Table 5 ‣ 5.4 Implementation Details ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the impact of each component on segmentation performance, with each one contributing a notable improvement. While the most significant component in CBSC is PP with a 7.48% average performance increase, CA has the least effect with a 4.96% increase. On the other hand, CA has the most effect on FrISS with an average of 6.22% performance enhancement, while T provides the least increase with 1.82% on average.</p>
</div>
<div class="ltx_para" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">As detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.SS1" title="3.1 Class-Agnostic Visio-Temporal Detector ‣ 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the object detector is trained using a synthetic dataset derived from QuickDraw classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>. We excluded objects belonging to QuickDraw categories from the FrISS dataset and denoted as FrISS<sup class="ltx_sup" id="S5.SS6.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.p3.1.1.1">sub</span></sup>. Later, we calculated AoN and S-IoU on this subset to evaluate the generalizability of CAVT to instances from unseen classes. Although the AoN score drops by approximately 10%, the decrease in S-IoU remains only around 1.5%. This indicates that CAVT can still generalize to sketch objects from unseen classes with minimal performance loss.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we proposed a novel pipeline for the scene sketch semantic segmentation task that identifies individual object instances at both stroke- and instance levels. We utilized both temporal information and the visual appearance of the sketches within a scene. Our approach allows us to assign a class label to each object instance without being constrained by a predefined category list. Furthermore, we introduced the FrISS dataset, comprising instance and stroke-level class annotations, sketch-text pairs, and verbal audio clips paired with each scene. We hope that FrISS facilitates a wide range of studies, including stroke-level scene sketch segmentation, speech-based sketch applications, and cross-modal research utilizing sketch-text pairs. Benefitting from FrISS, we conducted extensive experiments to show that our novel approach outperforms the state-of-the-art methods, yielding more coherent visual results in scene sketch semantic segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgement</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This study is written as a part of a Research Project supported by grants from the Scientific and Technological Research Council of Turkey (TUBITAK), Turkey (Project No. 120E489). We are also thankful for the support of the council and KUIS AI Center.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Segnet: A deep convolutional encoder-decoder architecture for image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 39(12):2481–2495, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Ahmed Bourouis, Judith Ellen Fan, and Yulia Gryaditskaya.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Open vocabulary semantic scene sketch understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.12463</span><span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Coco-stuff: Thing and stuff classes in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, pages 1209–1218, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 40(4):834–848, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhunia, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Fs-coco: Towards understanding of freehand sketches of common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pages 253–270. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Unbiased mean teacher for cross-domain object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, pages 4091–4101, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Mathias Eitz, James Hays, and Marc Alexa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">How do humans sketch objects?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">ACM Transactions on graphics (TOG)</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 31(4):1–10, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, and Changqing Zou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Sketchycoco: Image generation from freehand scene sketches.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 5174–5183, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Ce Ge, Haifeng Sun, Yi-Zhe Song, Zhanyu Ma, and Jianxin Liao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Exploring local detail perception for scene sketch semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">IEEE Transactions on Image Processing</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 31:1447–1461, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Yolox: Exceeding yolo series in 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">arXiv preprint arXiv:2107.08430</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
David Ha and Douglas Eck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">A neural representation of sketch drawings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">arXiv preprint arXiv:1704.03477</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
David Ha and Douglas Eck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">A neural representation of sketch drawings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">International Conference on Learning Representations</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib13.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Searching for mobilenetv3.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 1314–1324, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Junguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng Long.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Decoupled adaptation for cross-domain object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">arXiv preprint arXiv:2110.02578</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Kurmanbek Kaiyrbekov and Metin Sezgin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Deep stroke-based sketched symbol reconstruction and segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.3.1" style="font-size:90%;">IEEE computer graphics and applications</span><span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">, 40(1):112–126, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Yolov6 v3. 0: A full-scale reloading.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.3.1" style="font-size:90%;">arXiv preprint arXiv:2301.05586</span><span class="ltx_text" id="bib.bib17.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Lei Li, Hongbo Fu, and Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Fast sketch segmentation and labeling with deep learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">IEEE computer graphics and applications</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 39(2):38–51, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Lei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, and Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Sketch-r2cnn: an rnn-rasterization-cnn architecture for vector sketch recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">IEEE transactions on visualization and computer graphics</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 27(9):3745–3754, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Learning deconvolution network for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</span><span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">, pages 1520–1528, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and Moacir Ponti.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Sketchformer: Transformer-based representation for sketched structure.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 14153–14162, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 4510–4520, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">The sketchy database: learning to retrieve badly drawn bunnies.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 35(4):1–12, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Zhenbang Sun, Changhu Wang, Liqing Zhang, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Free hand-drawn sketch segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, pages 626–639. Springer, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Rethinking the inception architecture for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, pages 2818–2826, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Barış Batuhan Topal, Deniz Yuret, and Tevfik Metin Sezgin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Domain-adaptive self-supervised pre-training for face &amp; body detection in drawings, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Xingyuan Wu, Yonggang Qi, Jun Liu, and Jie Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Sketchsegnet: A rnn model for labeling sketch strokes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, pages 1–6. IEEE, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">End-to-end semi-supervised object detection with soft teacher.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, pages 3060–3069, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Peng Xu, Chaitanya K Joshi, and Xavier Bresson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Multigraph transformer for free-hand sketch recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 33(10):5150–5161, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Jie Yang, Aihua Ke, Yaoxiang Yu, and Bo Cai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Scene sketch semantic segmentation with hierarchical transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">Knowledge-Based Systems</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 280:110962, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun Zhou, and Youyi Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Sketchgnn: Semantic sketch segmentation with graph neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 40(3):1–13, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Sketch me that shoe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 799–807, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Dino: Detr with improved denoising anchor boxes for end-to-end object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">arXiv preprint arXiv:2203.03605</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Jianhui Zhang, Yilan Chen, Lei Li, Hongbo Fu, and Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Context-based sketch classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, pages 1–10, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Zhengming Zhang, Xiaoming Deng, Jinyao Li, Yukun Lai, Cuixia Ma, Yongjin Liu, and Hongan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Stroke-based semantic segmentation for scene-level free-hand sketches.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">The Visual Computer</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 39(12):6309–6321, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Yixiao Zheng, Jiyang Xie, Aneeshan Sain, Yi-Zhe Song, and Zhanyu Ma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Sketch-segformer: Transformer-based segmentation for figurative and creative sketches.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">IEEE transactions on image processing</span><span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Xianyi Zhu, Yi Xiao, and Yan Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Part-level sketch segmentation and labeling using dual-cnn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib39.4.2" style="font-size:90%;">Neural Information Processing: 25th International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13-16, 2018, Proceedings, Part I 25</span><span class="ltx_text" id="bib.bib39.5.3" style="font-size:90%;">, pages 374–384. Springer, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen, and Hao Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Sketchyscene: Richly-annotated scene sketches.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.2" style="font-size:90%;">ECCV</span><span class="ltx_text" id="bib.bib40.5.3" style="font-size:90%;">, pages 438–454. Springer International Publishing, 2018.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p ltx_align_center" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:144%;">Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation 
<br class="ltx_break"/>- Supplementary Material -</span></p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">The Supplementary Material is organized as follows. The details regarding the Post-Processing Module of CAVT are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1a" title="S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S1</span></a>. RGB Coloring Technique is detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2a" title="S2 Additional Details on RGB Coloring Technique ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S2</span></a>. Additional analysis on external classifiers is provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3a" title="S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S3</span></a>. Additional visual results are shared for scene sketch segmentation in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4a" title="S4 Additional Visual Results on Scene Sketch Semantic Segmentation ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S4</span></a>. Lastly, additional analysis and discussions regarding to FrISS dataset and UI of data collection web application are shared in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5a" title="S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5</span></a>.</p>
</div>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S1 </span>Details on Post-Processing Module</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S1.1 </span>Hyperparameter Optimization</h3>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.29">
<div class="ltx_listingline" id="algorithm1.29.30">
<span class="ltx_text" id="algorithm1.29.30.1"><span class="ltx_text ltx_font_bold" id="algorithm1.29.30.1.1">Input:</span> </span>boxes, IoU_threshold, OR_threshold
</div>
<div class="ltx_listingline" id="algorithm1.29.31">
<span class="ltx_text" id="algorithm1.29.31.1"><span class="ltx_text ltx_font_bold" id="algorithm1.29.31.1.1">Output:</span> </span>segmented stroke groups
</div>
<div class="ltx_listingline" id="algorithm1.29.32">
</div>
<div class="ltx_listingline" id="algorithm1.29.33">
<span class="ltx_text ltx_font_bold" id="algorithm1.29.33.1">while</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.29.33.2">there is alternation in stroke grouping</em> <span class="ltx_text ltx_font_bold" id="algorithm1.29.33.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.29.34">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Mark all strokes as unassigned. 
</div>
<div class="ltx_listingline" id="algorithm1.29.35">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Sort the boxes by area in ascending order.

</div>
<div class="ltx_listingline" id="algorithm1.1.1">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span class="ltx_text ltx_font_bold" id="algorithm1.1.1.2">for</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.1.1.1">each box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.1.1.1.m1.1"><semantics id="algorithm1.1.1.1.m1.1a"><msub id="algorithm1.1.1.1.m1.1.1" xref="algorithm1.1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.1.m1.1.1.2" xref="algorithm1.1.1.1.m1.1.1.2.cmml">b</mi><mi id="algorithm1.1.1.1.m1.1.1.3" xref="algorithm1.1.1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.1.m1.1b"><apply id="algorithm1.1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.1.m1.1.1">subscript</csymbol><ci id="algorithm1.1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.1.m1.1.1.2">𝑏</ci><ci id="algorithm1.1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.1.m1.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.1.1.1.m1.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in boxes</em> <span class="ltx_text ltx_font_bold" id="algorithm1.1.1.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.3.3">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Find the longest stroke sequence <math alttext="S" class="ltx_Math" display="inline" id="algorithm1.2.2.m1.1"><semantics id="algorithm1.2.2.m1.1a"><mi id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><ci id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="algorithm1.2.2.m1.1d">italic_S</annotation></semantics></math> that has the highest IoU with the box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.3.3.m2.1"><semantics id="algorithm1.3.3.m2.1a"><msub id="algorithm1.3.3.m2.1.1" xref="algorithm1.3.3.m2.1.1.cmml"><mi id="algorithm1.3.3.m2.1.1.2" xref="algorithm1.3.3.m2.1.1.2.cmml">b</mi><mi id="algorithm1.3.3.m2.1.1.3" xref="algorithm1.3.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m2.1b"><apply id="algorithm1.3.3.m2.1.1.cmml" xref="algorithm1.3.3.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.3.3.m2.1.1.1.cmml" xref="algorithm1.3.3.m2.1.1">subscript</csymbol><ci id="algorithm1.3.3.m2.1.1.2.cmml" xref="algorithm1.3.3.m2.1.1.2">𝑏</ci><ci id="algorithm1.3.3.m2.1.1.3.cmml" xref="algorithm1.3.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m2.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.3.3.m2.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. 
</div>
<div class="ltx_listingline" id="algorithm1.5.5">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span class="ltx_text ltx_font_bold" id="algorithm1.5.5.3">if</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.5.5.2">the overlap ratio between <math alttext="S" class="ltx_Math" display="inline" id="algorithm1.4.4.1.m1.1"><semantics id="algorithm1.4.4.1.m1.1a"><mi id="algorithm1.4.4.1.m1.1.1" xref="algorithm1.4.4.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.1.m1.1b"><ci id="algorithm1.4.4.1.m1.1.1.cmml" xref="algorithm1.4.4.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="algorithm1.4.4.1.m1.1d">italic_S</annotation></semantics></math> and <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.5.5.2.m2.1"><semantics id="algorithm1.5.5.2.m2.1a"><msub id="algorithm1.5.5.2.m2.1.1" xref="algorithm1.5.5.2.m2.1.1.cmml"><mi id="algorithm1.5.5.2.m2.1.1.2" xref="algorithm1.5.5.2.m2.1.1.2.cmml">b</mi><mi id="algorithm1.5.5.2.m2.1.1.3" xref="algorithm1.5.5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.2.m2.1b"><apply id="algorithm1.5.5.2.m2.1.1.cmml" xref="algorithm1.5.5.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.5.5.2.m2.1.1.1.cmml" xref="algorithm1.5.5.2.m2.1.1">subscript</csymbol><ci id="algorithm1.5.5.2.m2.1.1.2.cmml" xref="algorithm1.5.5.2.m2.1.1.2">𝑏</ci><ci id="algorithm1.5.5.2.m2.1.1.3.cmml" xref="algorithm1.5.5.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.2.m2.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.5.5.2.m2.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is more than IoU_threshold</em> <span class="ltx_text ltx_font_bold" id="algorithm1.5.5.4">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.7.7">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Assign stroke sequence <math alttext="S" class="ltx_Math" display="inline" id="algorithm1.6.6.m1.1"><semantics id="algorithm1.6.6.m1.1a"><mi id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><ci id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="algorithm1.6.6.m1.1d">italic_S</annotation></semantics></math> to bounding box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.7.7.m2.1"><semantics id="algorithm1.7.7.m2.1a"><msub id="algorithm1.7.7.m2.1.1" xref="algorithm1.7.7.m2.1.1.cmml"><mi id="algorithm1.7.7.m2.1.1.2" xref="algorithm1.7.7.m2.1.1.2.cmml">b</mi><mi id="algorithm1.7.7.m2.1.1.3" xref="algorithm1.7.7.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m2.1b"><apply id="algorithm1.7.7.m2.1.1.cmml" xref="algorithm1.7.7.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.7.7.m2.1.1.1.cmml" xref="algorithm1.7.7.m2.1.1">subscript</csymbol><ci id="algorithm1.7.7.m2.1.1.2.cmml" xref="algorithm1.7.7.m2.1.1.2">𝑏</ci><ci id="algorithm1.7.7.m2.1.1.3.cmml" xref="algorithm1.7.7.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m2.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.7.7.m2.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="algorithm1.29.36">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline" id="algorithm1.8.8">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span class="ltx_text ltx_font_bold" id="algorithm1.8.8.2">for</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.8.8.1">each unassigned longest stroke sequence <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.8.8.1.m1.1"><semantics id="algorithm1.8.8.1.m1.1a"><msub id="algorithm1.8.8.1.m1.1.1" xref="algorithm1.8.8.1.m1.1.1.cmml"><mi id="algorithm1.8.8.1.m1.1.1.2" xref="algorithm1.8.8.1.m1.1.1.2.cmml">S</mi><mi id="algorithm1.8.8.1.m1.1.1.3" xref="algorithm1.8.8.1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.1.m1.1b"><apply id="algorithm1.8.8.1.m1.1.1.cmml" xref="algorithm1.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.1.m1.1.1.1.cmml" xref="algorithm1.8.8.1.m1.1.1">subscript</csymbol><ci id="algorithm1.8.8.1.m1.1.1.2.cmml" xref="algorithm1.8.8.1.m1.1.1.2">𝑆</ci><ci id="algorithm1.8.8.1.m1.1.1.3.cmml" xref="algorithm1.8.8.1.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.1.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.8.8.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold" id="algorithm1.8.8.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.9.9">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Find the nearest bounding box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.9.9.m1.1"><semantics id="algorithm1.9.9.m1.1a"><msub id="algorithm1.9.9.m1.1.1" xref="algorithm1.9.9.m1.1.1.cmml"><mi id="algorithm1.9.9.m1.1.1.2" xref="algorithm1.9.9.m1.1.1.2.cmml">b</mi><mi id="algorithm1.9.9.m1.1.1.3" xref="algorithm1.9.9.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m1.1b"><apply id="algorithm1.9.9.m1.1.1.cmml" xref="algorithm1.9.9.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1">subscript</csymbol><ci id="algorithm1.9.9.m1.1.1.2.cmml" xref="algorithm1.9.9.m1.1.1.2">𝑏</ci><ci id="algorithm1.9.9.m1.1.1.3.cmml" xref="algorithm1.9.9.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m1.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.9.9.m1.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. 
</div>
<div class="ltx_listingline" id="algorithm1.11.11">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span class="ltx_text ltx_font_bold" id="algorithm1.11.11.3">if</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.11.11.2">the overlap ratio between <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.10.10.1.m1.1"><semantics id="algorithm1.10.10.1.m1.1a"><msub id="algorithm1.10.10.1.m1.1.1" xref="algorithm1.10.10.1.m1.1.1.cmml"><mi id="algorithm1.10.10.1.m1.1.1.2" xref="algorithm1.10.10.1.m1.1.1.2.cmml">S</mi><mi id="algorithm1.10.10.1.m1.1.1.3" xref="algorithm1.10.10.1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.1.m1.1b"><apply id="algorithm1.10.10.1.m1.1.1.cmml" xref="algorithm1.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.10.10.1.m1.1.1.1.cmml" xref="algorithm1.10.10.1.m1.1.1">subscript</csymbol><ci id="algorithm1.10.10.1.m1.1.1.2.cmml" xref="algorithm1.10.10.1.m1.1.1.2">𝑆</ci><ci id="algorithm1.10.10.1.m1.1.1.3.cmml" xref="algorithm1.10.10.1.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.1.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.10.10.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.11.11.2.m2.1"><semantics id="algorithm1.11.11.2.m2.1a"><msub id="algorithm1.11.11.2.m2.1.1" xref="algorithm1.11.11.2.m2.1.1.cmml"><mi id="algorithm1.11.11.2.m2.1.1.2" xref="algorithm1.11.11.2.m2.1.1.2.cmml">b</mi><mi id="algorithm1.11.11.2.m2.1.1.3" xref="algorithm1.11.11.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.2.m2.1b"><apply id="algorithm1.11.11.2.m2.1.1.cmml" xref="algorithm1.11.11.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.11.11.2.m2.1.1.1.cmml" xref="algorithm1.11.11.2.m2.1.1">subscript</csymbol><ci id="algorithm1.11.11.2.m2.1.1.2.cmml" xref="algorithm1.11.11.2.m2.1.1.2">𝑏</ci><ci id="algorithm1.11.11.2.m2.1.1.3.cmml" xref="algorithm1.11.11.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.2.m2.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.11.11.2.m2.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is more than OR_threshold</em> <span class="ltx_text ltx_font_bold" id="algorithm1.11.11.4">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.13.13">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Assign strokes in <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.12.12.m1.1"><semantics id="algorithm1.12.12.m1.1a"><msub id="algorithm1.12.12.m1.1.1" xref="algorithm1.12.12.m1.1.1.cmml"><mi id="algorithm1.12.12.m1.1.1.2" xref="algorithm1.12.12.m1.1.1.2.cmml">S</mi><mi id="algorithm1.12.12.m1.1.1.3" xref="algorithm1.12.12.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m1.1b"><apply id="algorithm1.12.12.m1.1.1.cmml" xref="algorithm1.12.12.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.1.cmml" xref="algorithm1.12.12.m1.1.1">subscript</csymbol><ci id="algorithm1.12.12.m1.1.1.2.cmml" xref="algorithm1.12.12.m1.1.1.2">𝑆</ci><ci id="algorithm1.12.12.m1.1.1.3.cmml" xref="algorithm1.12.12.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.12.12.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math> to bounding box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.13.13.m2.1"><semantics id="algorithm1.13.13.m2.1a"><msub id="algorithm1.13.13.m2.1.1" xref="algorithm1.13.13.m2.1.1.cmml"><mi id="algorithm1.13.13.m2.1.1.2" xref="algorithm1.13.13.m2.1.1.2.cmml">b</mi><mi id="algorithm1.13.13.m2.1.1.3" xref="algorithm1.13.13.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m2.1b"><apply id="algorithm1.13.13.m2.1.1.cmml" xref="algorithm1.13.13.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m2.1.1.1.cmml" xref="algorithm1.13.13.m2.1.1">subscript</csymbol><ci id="algorithm1.13.13.m2.1.1.2.cmml" xref="algorithm1.13.13.m2.1.1.2">𝑏</ci><ci id="algorithm1.13.13.m2.1.1.3.cmml" xref="algorithm1.13.13.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m2.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.13.13.m2.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="algorithm1.29.37">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline" id="algorithm1.14.14">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span class="ltx_text ltx_font_bold" id="algorithm1.14.14.2">for</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.14.14.1">each longest stroke sequence <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.14.14.1.m1.1"><semantics id="algorithm1.14.14.1.m1.1a"><msub id="algorithm1.14.14.1.m1.1.1" xref="algorithm1.14.14.1.m1.1.1.cmml"><mi id="algorithm1.14.14.1.m1.1.1.2" xref="algorithm1.14.14.1.m1.1.1.2.cmml">S</mi><mi id="algorithm1.14.14.1.m1.1.1.3" xref="algorithm1.14.14.1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.1.m1.1b"><apply id="algorithm1.14.14.1.m1.1.1.cmml" xref="algorithm1.14.14.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.14.14.1.m1.1.1.1.cmml" xref="algorithm1.14.14.1.m1.1.1">subscript</csymbol><ci id="algorithm1.14.14.1.m1.1.1.2.cmml" xref="algorithm1.14.14.1.m1.1.1.2">𝑆</ci><ci id="algorithm1.14.14.1.m1.1.1.3.cmml" xref="algorithm1.14.14.1.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.1.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.14.14.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math> that are unassigned</em> <span class="ltx_text ltx_font_bold" id="algorithm1.14.14.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.19.19">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Find the boundaries <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.15.15.m1.1"><semantics id="algorithm1.15.15.m1.1a"><msub id="algorithm1.15.15.m1.1.1" xref="algorithm1.15.15.m1.1.1.cmml"><mi id="algorithm1.15.15.m1.1.1.2" xref="algorithm1.15.15.m1.1.1.2.cmml">S</mi><mi id="algorithm1.15.15.m1.1.1.3" xref="algorithm1.15.15.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.m1.1b"><apply id="algorithm1.15.15.m1.1.1.cmml" xref="algorithm1.15.15.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1">subscript</csymbol><ci id="algorithm1.15.15.m1.1.1.2.cmml" xref="algorithm1.15.15.m1.1.1.2">𝑆</ci><ci id="algorithm1.15.15.m1.1.1.3.cmml" xref="algorithm1.15.15.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.15.15.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math>: <math alttext="x\_min" class="ltx_Math" display="inline" id="algorithm1.16.16.m2.1"><semantics id="algorithm1.16.16.m2.1a"><mrow id="algorithm1.16.16.m2.1.1" xref="algorithm1.16.16.m2.1.1.cmml"><mi id="algorithm1.16.16.m2.1.1.2" xref="algorithm1.16.16.m2.1.1.2.cmml">x</mi><mo id="algorithm1.16.16.m2.1.1.1" xref="algorithm1.16.16.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.16.16.m2.1.1.3" mathvariant="normal" xref="algorithm1.16.16.m2.1.1.3.cmml">_</mi><mo id="algorithm1.16.16.m2.1.1.1a" xref="algorithm1.16.16.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.16.16.m2.1.1.4" xref="algorithm1.16.16.m2.1.1.4.cmml">m</mi><mo id="algorithm1.16.16.m2.1.1.1b" xref="algorithm1.16.16.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.16.16.m2.1.1.5" xref="algorithm1.16.16.m2.1.1.5.cmml">i</mi><mo id="algorithm1.16.16.m2.1.1.1c" xref="algorithm1.16.16.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.16.16.m2.1.1.6" xref="algorithm1.16.16.m2.1.1.6.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.m2.1b"><apply id="algorithm1.16.16.m2.1.1.cmml" xref="algorithm1.16.16.m2.1.1"><times id="algorithm1.16.16.m2.1.1.1.cmml" xref="algorithm1.16.16.m2.1.1.1"></times><ci id="algorithm1.16.16.m2.1.1.2.cmml" xref="algorithm1.16.16.m2.1.1.2">𝑥</ci><ci id="algorithm1.16.16.m2.1.1.3.cmml" xref="algorithm1.16.16.m2.1.1.3">_</ci><ci id="algorithm1.16.16.m2.1.1.4.cmml" xref="algorithm1.16.16.m2.1.1.4">𝑚</ci><ci id="algorithm1.16.16.m2.1.1.5.cmml" xref="algorithm1.16.16.m2.1.1.5">𝑖</ci><ci id="algorithm1.16.16.m2.1.1.6.cmml" xref="algorithm1.16.16.m2.1.1.6">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.m2.1c">x\_min</annotation><annotation encoding="application/x-llamapun" id="algorithm1.16.16.m2.1d">italic_x _ italic_m italic_i italic_n</annotation></semantics></math>, <math alttext="y\_min" class="ltx_Math" display="inline" id="algorithm1.17.17.m3.1"><semantics id="algorithm1.17.17.m3.1a"><mrow id="algorithm1.17.17.m3.1.1" xref="algorithm1.17.17.m3.1.1.cmml"><mi id="algorithm1.17.17.m3.1.1.2" xref="algorithm1.17.17.m3.1.1.2.cmml">y</mi><mo id="algorithm1.17.17.m3.1.1.1" xref="algorithm1.17.17.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.17.17.m3.1.1.3" mathvariant="normal" xref="algorithm1.17.17.m3.1.1.3.cmml">_</mi><mo id="algorithm1.17.17.m3.1.1.1a" xref="algorithm1.17.17.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.17.17.m3.1.1.4" xref="algorithm1.17.17.m3.1.1.4.cmml">m</mi><mo id="algorithm1.17.17.m3.1.1.1b" xref="algorithm1.17.17.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.17.17.m3.1.1.5" xref="algorithm1.17.17.m3.1.1.5.cmml">i</mi><mo id="algorithm1.17.17.m3.1.1.1c" xref="algorithm1.17.17.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.17.17.m3.1.1.6" xref="algorithm1.17.17.m3.1.1.6.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m3.1b"><apply id="algorithm1.17.17.m3.1.1.cmml" xref="algorithm1.17.17.m3.1.1"><times id="algorithm1.17.17.m3.1.1.1.cmml" xref="algorithm1.17.17.m3.1.1.1"></times><ci id="algorithm1.17.17.m3.1.1.2.cmml" xref="algorithm1.17.17.m3.1.1.2">𝑦</ci><ci id="algorithm1.17.17.m3.1.1.3.cmml" xref="algorithm1.17.17.m3.1.1.3">_</ci><ci id="algorithm1.17.17.m3.1.1.4.cmml" xref="algorithm1.17.17.m3.1.1.4">𝑚</ci><ci id="algorithm1.17.17.m3.1.1.5.cmml" xref="algorithm1.17.17.m3.1.1.5">𝑖</ci><ci id="algorithm1.17.17.m3.1.1.6.cmml" xref="algorithm1.17.17.m3.1.1.6">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m3.1c">y\_min</annotation><annotation encoding="application/x-llamapun" id="algorithm1.17.17.m3.1d">italic_y _ italic_m italic_i italic_n</annotation></semantics></math>, <math alttext="x\_max" class="ltx_Math" display="inline" id="algorithm1.18.18.m4.1"><semantics id="algorithm1.18.18.m4.1a"><mrow id="algorithm1.18.18.m4.1.1" xref="algorithm1.18.18.m4.1.1.cmml"><mi id="algorithm1.18.18.m4.1.1.2" xref="algorithm1.18.18.m4.1.1.2.cmml">x</mi><mo id="algorithm1.18.18.m4.1.1.1" xref="algorithm1.18.18.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.18.18.m4.1.1.3" mathvariant="normal" xref="algorithm1.18.18.m4.1.1.3.cmml">_</mi><mo id="algorithm1.18.18.m4.1.1.1a" xref="algorithm1.18.18.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.18.18.m4.1.1.4" xref="algorithm1.18.18.m4.1.1.4.cmml">m</mi><mo id="algorithm1.18.18.m4.1.1.1b" xref="algorithm1.18.18.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.18.18.m4.1.1.5" xref="algorithm1.18.18.m4.1.1.5.cmml">a</mi><mo id="algorithm1.18.18.m4.1.1.1c" xref="algorithm1.18.18.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.18.18.m4.1.1.6" xref="algorithm1.18.18.m4.1.1.6.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m4.1b"><apply id="algorithm1.18.18.m4.1.1.cmml" xref="algorithm1.18.18.m4.1.1"><times id="algorithm1.18.18.m4.1.1.1.cmml" xref="algorithm1.18.18.m4.1.1.1"></times><ci id="algorithm1.18.18.m4.1.1.2.cmml" xref="algorithm1.18.18.m4.1.1.2">𝑥</ci><ci id="algorithm1.18.18.m4.1.1.3.cmml" xref="algorithm1.18.18.m4.1.1.3">_</ci><ci id="algorithm1.18.18.m4.1.1.4.cmml" xref="algorithm1.18.18.m4.1.1.4">𝑚</ci><ci id="algorithm1.18.18.m4.1.1.5.cmml" xref="algorithm1.18.18.m4.1.1.5">𝑎</ci><ci id="algorithm1.18.18.m4.1.1.6.cmml" xref="algorithm1.18.18.m4.1.1.6">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m4.1c">x\_max</annotation><annotation encoding="application/x-llamapun" id="algorithm1.18.18.m4.1d">italic_x _ italic_m italic_a italic_x</annotation></semantics></math>, <math alttext="y\_max" class="ltx_Math" display="inline" id="algorithm1.19.19.m5.1"><semantics id="algorithm1.19.19.m5.1a"><mrow id="algorithm1.19.19.m5.1.1" xref="algorithm1.19.19.m5.1.1.cmml"><mi id="algorithm1.19.19.m5.1.1.2" xref="algorithm1.19.19.m5.1.1.2.cmml">y</mi><mo id="algorithm1.19.19.m5.1.1.1" xref="algorithm1.19.19.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.19.19.m5.1.1.3" mathvariant="normal" xref="algorithm1.19.19.m5.1.1.3.cmml">_</mi><mo id="algorithm1.19.19.m5.1.1.1a" xref="algorithm1.19.19.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.19.19.m5.1.1.4" xref="algorithm1.19.19.m5.1.1.4.cmml">m</mi><mo id="algorithm1.19.19.m5.1.1.1b" xref="algorithm1.19.19.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.19.19.m5.1.1.5" xref="algorithm1.19.19.m5.1.1.5.cmml">a</mi><mo id="algorithm1.19.19.m5.1.1.1c" xref="algorithm1.19.19.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.19.19.m5.1.1.6" xref="algorithm1.19.19.m5.1.1.6.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.m5.1b"><apply id="algorithm1.19.19.m5.1.1.cmml" xref="algorithm1.19.19.m5.1.1"><times id="algorithm1.19.19.m5.1.1.1.cmml" xref="algorithm1.19.19.m5.1.1.1"></times><ci id="algorithm1.19.19.m5.1.1.2.cmml" xref="algorithm1.19.19.m5.1.1.2">𝑦</ci><ci id="algorithm1.19.19.m5.1.1.3.cmml" xref="algorithm1.19.19.m5.1.1.3">_</ci><ci id="algorithm1.19.19.m5.1.1.4.cmml" xref="algorithm1.19.19.m5.1.1.4">𝑚</ci><ci id="algorithm1.19.19.m5.1.1.5.cmml" xref="algorithm1.19.19.m5.1.1.5">𝑎</ci><ci id="algorithm1.19.19.m5.1.1.6.cmml" xref="algorithm1.19.19.m5.1.1.6">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.m5.1c">y\_max</annotation><annotation encoding="application/x-llamapun" id="algorithm1.19.19.m5.1d">italic_y _ italic_m italic_a italic_x</annotation></semantics></math>. 
</div>
<div class="ltx_listingline" id="algorithm1.24.24">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Define a new box <math alttext="b_{new}" class="ltx_Math" display="inline" id="algorithm1.20.20.m1.1"><semantics id="algorithm1.20.20.m1.1a"><msub id="algorithm1.20.20.m1.1.1" xref="algorithm1.20.20.m1.1.1.cmml"><mi id="algorithm1.20.20.m1.1.1.2" xref="algorithm1.20.20.m1.1.1.2.cmml">b</mi><mrow id="algorithm1.20.20.m1.1.1.3" xref="algorithm1.20.20.m1.1.1.3.cmml"><mi id="algorithm1.20.20.m1.1.1.3.2" xref="algorithm1.20.20.m1.1.1.3.2.cmml">n</mi><mo id="algorithm1.20.20.m1.1.1.3.1" xref="algorithm1.20.20.m1.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.20.20.m1.1.1.3.3" xref="algorithm1.20.20.m1.1.1.3.3.cmml">e</mi><mo id="algorithm1.20.20.m1.1.1.3.1a" xref="algorithm1.20.20.m1.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.20.20.m1.1.1.3.4" xref="algorithm1.20.20.m1.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m1.1b"><apply id="algorithm1.20.20.m1.1.1.cmml" xref="algorithm1.20.20.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.1.1.cmml" xref="algorithm1.20.20.m1.1.1">subscript</csymbol><ci id="algorithm1.20.20.m1.1.1.2.cmml" xref="algorithm1.20.20.m1.1.1.2">𝑏</ci><apply id="algorithm1.20.20.m1.1.1.3.cmml" xref="algorithm1.20.20.m1.1.1.3"><times id="algorithm1.20.20.m1.1.1.3.1.cmml" xref="algorithm1.20.20.m1.1.1.3.1"></times><ci id="algorithm1.20.20.m1.1.1.3.2.cmml" xref="algorithm1.20.20.m1.1.1.3.2">𝑛</ci><ci id="algorithm1.20.20.m1.1.1.3.3.cmml" xref="algorithm1.20.20.m1.1.1.3.3">𝑒</ci><ci id="algorithm1.20.20.m1.1.1.3.4.cmml" xref="algorithm1.20.20.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m1.1c">b_{new}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.20.20.m1.1d">italic_b start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> from values <math alttext="x\_min" class="ltx_Math" display="inline" id="algorithm1.21.21.m2.1"><semantics id="algorithm1.21.21.m2.1a"><mrow id="algorithm1.21.21.m2.1.1" xref="algorithm1.21.21.m2.1.1.cmml"><mi id="algorithm1.21.21.m2.1.1.2" xref="algorithm1.21.21.m2.1.1.2.cmml">x</mi><mo id="algorithm1.21.21.m2.1.1.1" xref="algorithm1.21.21.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.21.21.m2.1.1.3" mathvariant="normal" xref="algorithm1.21.21.m2.1.1.3.cmml">_</mi><mo id="algorithm1.21.21.m2.1.1.1a" xref="algorithm1.21.21.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.21.21.m2.1.1.4" xref="algorithm1.21.21.m2.1.1.4.cmml">m</mi><mo id="algorithm1.21.21.m2.1.1.1b" xref="algorithm1.21.21.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.21.21.m2.1.1.5" xref="algorithm1.21.21.m2.1.1.5.cmml">i</mi><mo id="algorithm1.21.21.m2.1.1.1c" xref="algorithm1.21.21.m2.1.1.1.cmml">⁢</mo><mi id="algorithm1.21.21.m2.1.1.6" xref="algorithm1.21.21.m2.1.1.6.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m2.1b"><apply id="algorithm1.21.21.m2.1.1.cmml" xref="algorithm1.21.21.m2.1.1"><times id="algorithm1.21.21.m2.1.1.1.cmml" xref="algorithm1.21.21.m2.1.1.1"></times><ci id="algorithm1.21.21.m2.1.1.2.cmml" xref="algorithm1.21.21.m2.1.1.2">𝑥</ci><ci id="algorithm1.21.21.m2.1.1.3.cmml" xref="algorithm1.21.21.m2.1.1.3">_</ci><ci id="algorithm1.21.21.m2.1.1.4.cmml" xref="algorithm1.21.21.m2.1.1.4">𝑚</ci><ci id="algorithm1.21.21.m2.1.1.5.cmml" xref="algorithm1.21.21.m2.1.1.5">𝑖</ci><ci id="algorithm1.21.21.m2.1.1.6.cmml" xref="algorithm1.21.21.m2.1.1.6">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m2.1c">x\_min</annotation><annotation encoding="application/x-llamapun" id="algorithm1.21.21.m2.1d">italic_x _ italic_m italic_i italic_n</annotation></semantics></math>, <math alttext="y\_min" class="ltx_Math" display="inline" id="algorithm1.22.22.m3.1"><semantics id="algorithm1.22.22.m3.1a"><mrow id="algorithm1.22.22.m3.1.1" xref="algorithm1.22.22.m3.1.1.cmml"><mi id="algorithm1.22.22.m3.1.1.2" xref="algorithm1.22.22.m3.1.1.2.cmml">y</mi><mo id="algorithm1.22.22.m3.1.1.1" xref="algorithm1.22.22.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.22.22.m3.1.1.3" mathvariant="normal" xref="algorithm1.22.22.m3.1.1.3.cmml">_</mi><mo id="algorithm1.22.22.m3.1.1.1a" xref="algorithm1.22.22.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.22.22.m3.1.1.4" xref="algorithm1.22.22.m3.1.1.4.cmml">m</mi><mo id="algorithm1.22.22.m3.1.1.1b" xref="algorithm1.22.22.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.22.22.m3.1.1.5" xref="algorithm1.22.22.m3.1.1.5.cmml">i</mi><mo id="algorithm1.22.22.m3.1.1.1c" xref="algorithm1.22.22.m3.1.1.1.cmml">⁢</mo><mi id="algorithm1.22.22.m3.1.1.6" xref="algorithm1.22.22.m3.1.1.6.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.22.22.m3.1b"><apply id="algorithm1.22.22.m3.1.1.cmml" xref="algorithm1.22.22.m3.1.1"><times id="algorithm1.22.22.m3.1.1.1.cmml" xref="algorithm1.22.22.m3.1.1.1"></times><ci id="algorithm1.22.22.m3.1.1.2.cmml" xref="algorithm1.22.22.m3.1.1.2">𝑦</ci><ci id="algorithm1.22.22.m3.1.1.3.cmml" xref="algorithm1.22.22.m3.1.1.3">_</ci><ci id="algorithm1.22.22.m3.1.1.4.cmml" xref="algorithm1.22.22.m3.1.1.4">𝑚</ci><ci id="algorithm1.22.22.m3.1.1.5.cmml" xref="algorithm1.22.22.m3.1.1.5">𝑖</ci><ci id="algorithm1.22.22.m3.1.1.6.cmml" xref="algorithm1.22.22.m3.1.1.6">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.22.22.m3.1c">y\_min</annotation><annotation encoding="application/x-llamapun" id="algorithm1.22.22.m3.1d">italic_y _ italic_m italic_i italic_n</annotation></semantics></math>, <math alttext="x\_max" class="ltx_Math" display="inline" id="algorithm1.23.23.m4.1"><semantics id="algorithm1.23.23.m4.1a"><mrow id="algorithm1.23.23.m4.1.1" xref="algorithm1.23.23.m4.1.1.cmml"><mi id="algorithm1.23.23.m4.1.1.2" xref="algorithm1.23.23.m4.1.1.2.cmml">x</mi><mo id="algorithm1.23.23.m4.1.1.1" xref="algorithm1.23.23.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.23.23.m4.1.1.3" mathvariant="normal" xref="algorithm1.23.23.m4.1.1.3.cmml">_</mi><mo id="algorithm1.23.23.m4.1.1.1a" xref="algorithm1.23.23.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.23.23.m4.1.1.4" xref="algorithm1.23.23.m4.1.1.4.cmml">m</mi><mo id="algorithm1.23.23.m4.1.1.1b" xref="algorithm1.23.23.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.23.23.m4.1.1.5" xref="algorithm1.23.23.m4.1.1.5.cmml">a</mi><mo id="algorithm1.23.23.m4.1.1.1c" xref="algorithm1.23.23.m4.1.1.1.cmml">⁢</mo><mi id="algorithm1.23.23.m4.1.1.6" xref="algorithm1.23.23.m4.1.1.6.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m4.1b"><apply id="algorithm1.23.23.m4.1.1.cmml" xref="algorithm1.23.23.m4.1.1"><times id="algorithm1.23.23.m4.1.1.1.cmml" xref="algorithm1.23.23.m4.1.1.1"></times><ci id="algorithm1.23.23.m4.1.1.2.cmml" xref="algorithm1.23.23.m4.1.1.2">𝑥</ci><ci id="algorithm1.23.23.m4.1.1.3.cmml" xref="algorithm1.23.23.m4.1.1.3">_</ci><ci id="algorithm1.23.23.m4.1.1.4.cmml" xref="algorithm1.23.23.m4.1.1.4">𝑚</ci><ci id="algorithm1.23.23.m4.1.1.5.cmml" xref="algorithm1.23.23.m4.1.1.5">𝑎</ci><ci id="algorithm1.23.23.m4.1.1.6.cmml" xref="algorithm1.23.23.m4.1.1.6">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m4.1c">x\_max</annotation><annotation encoding="application/x-llamapun" id="algorithm1.23.23.m4.1d">italic_x _ italic_m italic_a italic_x</annotation></semantics></math>, <math alttext="y\_max" class="ltx_Math" display="inline" id="algorithm1.24.24.m5.1"><semantics id="algorithm1.24.24.m5.1a"><mrow id="algorithm1.24.24.m5.1.1" xref="algorithm1.24.24.m5.1.1.cmml"><mi id="algorithm1.24.24.m5.1.1.2" xref="algorithm1.24.24.m5.1.1.2.cmml">y</mi><mo id="algorithm1.24.24.m5.1.1.1" xref="algorithm1.24.24.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.24.24.m5.1.1.3" mathvariant="normal" xref="algorithm1.24.24.m5.1.1.3.cmml">_</mi><mo id="algorithm1.24.24.m5.1.1.1a" xref="algorithm1.24.24.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.24.24.m5.1.1.4" xref="algorithm1.24.24.m5.1.1.4.cmml">m</mi><mo id="algorithm1.24.24.m5.1.1.1b" xref="algorithm1.24.24.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.24.24.m5.1.1.5" xref="algorithm1.24.24.m5.1.1.5.cmml">a</mi><mo id="algorithm1.24.24.m5.1.1.1c" xref="algorithm1.24.24.m5.1.1.1.cmml">⁢</mo><mi id="algorithm1.24.24.m5.1.1.6" xref="algorithm1.24.24.m5.1.1.6.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m5.1b"><apply id="algorithm1.24.24.m5.1.1.cmml" xref="algorithm1.24.24.m5.1.1"><times id="algorithm1.24.24.m5.1.1.1.cmml" xref="algorithm1.24.24.m5.1.1.1"></times><ci id="algorithm1.24.24.m5.1.1.2.cmml" xref="algorithm1.24.24.m5.1.1.2">𝑦</ci><ci id="algorithm1.24.24.m5.1.1.3.cmml" xref="algorithm1.24.24.m5.1.1.3">_</ci><ci id="algorithm1.24.24.m5.1.1.4.cmml" xref="algorithm1.24.24.m5.1.1.4">𝑚</ci><ci id="algorithm1.24.24.m5.1.1.5.cmml" xref="algorithm1.24.24.m5.1.1.5">𝑎</ci><ci id="algorithm1.24.24.m5.1.1.6.cmml" xref="algorithm1.24.24.m5.1.1.6">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m5.1c">y\_max</annotation><annotation encoding="application/x-llamapun" id="algorithm1.24.24.m5.1d">italic_y _ italic_m italic_a italic_x</annotation></semantics></math>. 
</div>
<div class="ltx_listingline" id="algorithm1.25.25">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Append <math alttext="b_{new}" class="ltx_Math" display="inline" id="algorithm1.25.25.m1.1"><semantics id="algorithm1.25.25.m1.1a"><msub id="algorithm1.25.25.m1.1.1" xref="algorithm1.25.25.m1.1.1.cmml"><mi id="algorithm1.25.25.m1.1.1.2" xref="algorithm1.25.25.m1.1.1.2.cmml">b</mi><mrow id="algorithm1.25.25.m1.1.1.3" xref="algorithm1.25.25.m1.1.1.3.cmml"><mi id="algorithm1.25.25.m1.1.1.3.2" xref="algorithm1.25.25.m1.1.1.3.2.cmml">n</mi><mo id="algorithm1.25.25.m1.1.1.3.1" xref="algorithm1.25.25.m1.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.25.25.m1.1.1.3.3" xref="algorithm1.25.25.m1.1.1.3.3.cmml">e</mi><mo id="algorithm1.25.25.m1.1.1.3.1a" xref="algorithm1.25.25.m1.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.25.25.m1.1.1.3.4" xref="algorithm1.25.25.m1.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.25.25.m1.1b"><apply id="algorithm1.25.25.m1.1.1.cmml" xref="algorithm1.25.25.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.25.25.m1.1.1.1.cmml" xref="algorithm1.25.25.m1.1.1">subscript</csymbol><ci id="algorithm1.25.25.m1.1.1.2.cmml" xref="algorithm1.25.25.m1.1.1.2">𝑏</ci><apply id="algorithm1.25.25.m1.1.1.3.cmml" xref="algorithm1.25.25.m1.1.1.3"><times id="algorithm1.25.25.m1.1.1.3.1.cmml" xref="algorithm1.25.25.m1.1.1.3.1"></times><ci id="algorithm1.25.25.m1.1.1.3.2.cmml" xref="algorithm1.25.25.m1.1.1.3.2">𝑛</ci><ci id="algorithm1.25.25.m1.1.1.3.3.cmml" xref="algorithm1.25.25.m1.1.1.3.3">𝑒</ci><ci id="algorithm1.25.25.m1.1.1.3.4.cmml" xref="algorithm1.25.25.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.25.25.m1.1c">b_{new}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.25.25.m1.1d">italic_b start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> to the boxes. 
</div>
<div class="ltx_listingline" id="algorithm1.27.27">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Assign each stroke in <math alttext="S_{u}" class="ltx_Math" display="inline" id="algorithm1.26.26.m1.1"><semantics id="algorithm1.26.26.m1.1a"><msub id="algorithm1.26.26.m1.1.1" xref="algorithm1.26.26.m1.1.1.cmml"><mi id="algorithm1.26.26.m1.1.1.2" xref="algorithm1.26.26.m1.1.1.2.cmml">S</mi><mi id="algorithm1.26.26.m1.1.1.3" xref="algorithm1.26.26.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.26.26.m1.1b"><apply id="algorithm1.26.26.m1.1.1.cmml" xref="algorithm1.26.26.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.26.26.m1.1.1.1.cmml" xref="algorithm1.26.26.m1.1.1">subscript</csymbol><ci id="algorithm1.26.26.m1.1.1.2.cmml" xref="algorithm1.26.26.m1.1.1.2">𝑆</ci><ci id="algorithm1.26.26.m1.1.1.3.cmml" xref="algorithm1.26.26.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.26.26.m1.1c">S_{u}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.26.26.m1.1d">italic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="b_{new}" class="ltx_Math" display="inline" id="algorithm1.27.27.m2.1"><semantics id="algorithm1.27.27.m2.1a"><msub id="algorithm1.27.27.m2.1.1" xref="algorithm1.27.27.m2.1.1.cmml"><mi id="algorithm1.27.27.m2.1.1.2" xref="algorithm1.27.27.m2.1.1.2.cmml">b</mi><mrow id="algorithm1.27.27.m2.1.1.3" xref="algorithm1.27.27.m2.1.1.3.cmml"><mi id="algorithm1.27.27.m2.1.1.3.2" xref="algorithm1.27.27.m2.1.1.3.2.cmml">n</mi><mo id="algorithm1.27.27.m2.1.1.3.1" xref="algorithm1.27.27.m2.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.27.27.m2.1.1.3.3" xref="algorithm1.27.27.m2.1.1.3.3.cmml">e</mi><mo id="algorithm1.27.27.m2.1.1.3.1a" xref="algorithm1.27.27.m2.1.1.3.1.cmml">⁢</mo><mi id="algorithm1.27.27.m2.1.1.3.4" xref="algorithm1.27.27.m2.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.27.27.m2.1b"><apply id="algorithm1.27.27.m2.1.1.cmml" xref="algorithm1.27.27.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.27.27.m2.1.1.1.cmml" xref="algorithm1.27.27.m2.1.1">subscript</csymbol><ci id="algorithm1.27.27.m2.1.1.2.cmml" xref="algorithm1.27.27.m2.1.1.2">𝑏</ci><apply id="algorithm1.27.27.m2.1.1.3.cmml" xref="algorithm1.27.27.m2.1.1.3"><times id="algorithm1.27.27.m2.1.1.3.1.cmml" xref="algorithm1.27.27.m2.1.1.3.1"></times><ci id="algorithm1.27.27.m2.1.1.3.2.cmml" xref="algorithm1.27.27.m2.1.1.3.2">𝑛</ci><ci id="algorithm1.27.27.m2.1.1.3.3.cmml" xref="algorithm1.27.27.m2.1.1.3.3">𝑒</ci><ci id="algorithm1.27.27.m2.1.1.3.4.cmml" xref="algorithm1.27.27.m2.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.27.27.m2.1c">b_{new}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.27.27.m2.1d">italic_b start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="algorithm1.28.28">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span class="ltx_text ltx_font_bold" id="algorithm1.28.28.2">for</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.28.28.1">each box <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.28.28.1.m1.1"><semantics id="algorithm1.28.28.1.m1.1a"><msub id="algorithm1.28.28.1.m1.1.1" xref="algorithm1.28.28.1.m1.1.1.cmml"><mi id="algorithm1.28.28.1.m1.1.1.2" xref="algorithm1.28.28.1.m1.1.1.2.cmml">b</mi><mi id="algorithm1.28.28.1.m1.1.1.3" xref="algorithm1.28.28.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.28.28.1.m1.1b"><apply id="algorithm1.28.28.1.m1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.28.28.1.m1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1">subscript</csymbol><ci id="algorithm1.28.28.1.m1.1.1.2.cmml" xref="algorithm1.28.28.1.m1.1.1.2">𝑏</ci><ci id="algorithm1.28.28.1.m1.1.1.3.cmml" xref="algorithm1.28.28.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.28.28.1.m1.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.28.28.1.m1.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in boxes</em> <span class="ltx_text ltx_font_bold" id="algorithm1.28.28.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.29.29">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Update the coordinates of each <math alttext="b_{i}" class="ltx_Math" display="inline" id="algorithm1.29.29.m1.1"><semantics id="algorithm1.29.29.m1.1a"><msub id="algorithm1.29.29.m1.1.1" xref="algorithm1.29.29.m1.1.1.cmml"><mi id="algorithm1.29.29.m1.1.1.2" xref="algorithm1.29.29.m1.1.1.2.cmml">b</mi><mi id="algorithm1.29.29.m1.1.1.3" xref="algorithm1.29.29.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.29.29.m1.1b"><apply id="algorithm1.29.29.m1.1.1.cmml" xref="algorithm1.29.29.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.29.29.m1.1.1.1.cmml" xref="algorithm1.29.29.m1.1.1">subscript</csymbol><ci id="algorithm1.29.29.m1.1.1.2.cmml" xref="algorithm1.29.29.m1.1.1.2">𝑏</ci><ci id="algorithm1.29.29.m1.1.1.3.cmml" xref="algorithm1.29.29.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.29.29.m1.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.29.29.m1.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> according to the most recent assignment of strokes.

</div>
<div class="ltx_listingline" id="algorithm1.29.38">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline" id="algorithm1.29.39">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.31.1.1">Algorithm 1</span> </span>Post-Processing Module</figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">The complete algorithm for the post-processing module is outlined in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#algorithm1" title="Algorithm 1 ‣ S1.1 Hyperparameter Optimization ‣ S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. Furthermore, we provide details of our grid-search approach used to determine the optimal hyperparameter combination for the post-processing module. We evaluated the AoN and S-IoU scores on the validation sets of both CBSC and FrISS and selected the top-performing parameter combination based on the average of all scores. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.T1a" title="Table 1 ‣ S1.1 Hyperparameter Optimization ‣ S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results for the top-performing parameter combination. The parameters in the ablation study are explained as follows:</p>
</div>
<figure class="ltx_table" id="S1.T1a">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1a.2" style="width:496.9pt;height:375.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.3pt,25.2pt) scale(0.881818193635445,0.881818193635445) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1a.2.1">
<tr class="ltx_tr" id="S1.T1a.2.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1a.2.1.1.1" rowspan="2"><span class="ltx_text" id="S1.T1a.2.1.1.1.1">Index</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.1.2" rowspan="2"><span class="ltx_text ltx_font_italic" id="S1.T1a.2.1.1.2.1">IoU_threshold</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.1.3" rowspan="2"><span class="ltx_text ltx_font_italic" id="S1.T1a.2.1.1.3.1">OR_threshold</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.1.4" rowspan="2"><span class="ltx_text ltx_font_italic" id="S1.T1a.2.1.1.4.1">num_repeats</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1a.2.1.1.5" rowspan="2"><span class="ltx_text ltx_font_italic" id="S1.T1a.2.1.1.5.1">stroke_thickness</span></td>
<td class="ltx_td ltx_border_t" id="S1.T1a.2.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S1.T1a.2.1.1.7">CBSC</td>
<td class="ltx_td ltx_border_t" id="S1.T1a.2.1.1.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S1.T1a.2.1.1.9">FrISS</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1a.2.1.1.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.1.11" rowspan="2"><span class="ltx_text" id="S1.T1a.2.1.1.11.1">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.2">
<td class="ltx_td" id="S1.T1a.2.1.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.2.2">AoN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.2.3">S-IoU</td>
<td class="ltx_td" id="S1.T1a.2.1.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.2.5">AoN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.2.6">S-IoU</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.2.7"></td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1a.2.1.3.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.2">65%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.3">60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1a.2.1.3.5">2</td>
<td class="ltx_td ltx_border_t" id="S1.T1a.2.1.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.7">74,17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.8">88,19</td>
<td class="ltx_td ltx_border_t" id="S1.T1a.2.1.3.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.10">57,96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.11">79,20</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1a.2.1.3.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1a.2.1.3.13"><span class="ltx_text ltx_font_bold" id="S1.T1a.2.1.3.13.1">74,88</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.4.1">2</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.2">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.3">45%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.4.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.4.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.7">73,53</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.8">87,56</td>
<td class="ltx_td" id="S1.T1a.2.1.4.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.10">59,37</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.11">78,98</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.4.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.4.13">74,86</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.5.1">3</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.2">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.3">60%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.5.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.5.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.7">74,17</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.8">88,19</td>
<td class="ltx_td" id="S1.T1a.2.1.5.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.10">57,71</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.11">79,32</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.5.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.5.13">74,85</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.6.1">4</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.2">65%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.6.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.6.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.7">73,56</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.8">87,94</td>
<td class="ltx_td" id="S1.T1a.2.1.6.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.10">58,08</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.11">79,25</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.6.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.6.13">74,71</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.7.1">5</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.2">65%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.3">70%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.7.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.7.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.7">73,56</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.8">87,94</td>
<td class="ltx_td" id="S1.T1a.2.1.7.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.10">58,10</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.11">79,16</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.7.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.7.13">74,69</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.8.1">6</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.2">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.3">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.8.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.8.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.7">74,07</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.8">88,11</td>
<td class="ltx_td" id="S1.T1a.2.1.8.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.10">57,38</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.11">78,99</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.8.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.8.13">74,64</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.9.1">7</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.2">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.3">70%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.9.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.9.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.7">73,56</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.8">87,94</td>
<td class="ltx_td" id="S1.T1a.2.1.9.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.10">57,85</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.11">79,18</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.9.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.9.13">74,63</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.10.1">8</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.2">25%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.3">60%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.10.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.10.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.7">74,40</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.8">88,47</td>
<td class="ltx_td" id="S1.T1a.2.1.10.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.10">56,67</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.11">78,99</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.10.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.10.13">74,63</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.11.1">9</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.2">45%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.3">60%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.11.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.11.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.7">74,17</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.8">88,27</td>
<td class="ltx_td" id="S1.T1a.2.1.11.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.10">56,79</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.11">79,11</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.11.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.11.13">74,59</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.12.1">10</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.2">65%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.3">70%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.12.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.12.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.7">73,43</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.8">88,02</td>
<td class="ltx_td" id="S1.T1a.2.1.12.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.10">57,75</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.11">79,05</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.12.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.12.13">74,56</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.13.1">11</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.2">25%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.13.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.13.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.7">73,79</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.8">88,20</td>
<td class="ltx_td" id="S1.T1a.2.1.13.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.10">57,03</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.11">78,96</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.13.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.13.13">74,49</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.14.1">12</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.2">45%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.14.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.14.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.7">73,56</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.8">88,00</td>
<td class="ltx_td" id="S1.T1a.2.1.14.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.10">57,14</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.11">79,09</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.14.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.14.13">74,45</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.15.1">13</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.2">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.3">70%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.15.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.15.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.7">72,69</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.8">87,64</td>
<td class="ltx_td" id="S1.T1a.2.1.15.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.10">58,45</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.11">78,97</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.15.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.15.13">74,44</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.16.1">14</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.2">25%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.3">70%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.16.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.16.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.7">73,79</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.8">88,20</td>
<td class="ltx_td" id="S1.T1a.2.1.16.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.10">56,81</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.11">78,85</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.16.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.16.13">74,41</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.17.1">15</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.2">35%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.17.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.17.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.7">73,34</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.8">87,97</td>
<td class="ltx_td" id="S1.T1a.2.1.17.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.10">57,14</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.11">79,09</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.17.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.17.13">74,38</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.18.1">16</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.2">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.3">50%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.18.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.18.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.7">72,89</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.8">87,61</td>
<td class="ltx_td" id="S1.T1a.2.1.18.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.10">58,32</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.11">78,58</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.18.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.18.13">74,35</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.19.1">17</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.2">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.3">50%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.19.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.19.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.7">73,76</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.8">87,84</td>
<td class="ltx_td" id="S1.T1a.2.1.19.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.10">57,02</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.11">78,75</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.19.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.19.13">74,34</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.20.1">18</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.2">35%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.20.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.20.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.7">73,21</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.8">88,05</td>
<td class="ltx_td" id="S1.T1a.2.1.20.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.10">56,68</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.11">79,17</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.20.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.20.13">74,28</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.21.1">19</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.2">55%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.3">50%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.21.5">2</td>
<td class="ltx_td" id="S1.T1a.2.1.21.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.7">73,63</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.8">87,89</td>
<td class="ltx_td" id="S1.T1a.2.1.21.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.10">56,67</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.11">78,81</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.21.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.21.13">74,25</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.22.1">20</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.2">85%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.3">75%</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1a.2.1.22.5">1</td>
<td class="ltx_td" id="S1.T1a.2.1.22.6"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.7">73,23</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.8">87,41</td>
<td class="ltx_td" id="S1.T1a.2.1.22.9"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.10">58,40</td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.11">77,78</td>
<td class="ltx_td ltx_border_r" id="S1.T1a.2.1.22.12"></td>
<td class="ltx_td ltx_align_center" id="S1.T1a.2.1.22.13">74,21</td>
</tr>
<tr class="ltx_tr" id="S1.T1a.2.1.23">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1a.2.1.23.1">Lowest</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.2">85%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.3">50%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.4">7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1a.2.1.23.5">3</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.6"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.7">66,97</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.8">83,00</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.9"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.10">53,17</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.11">75,74</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S1.T1a.2.1.23.12"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1a.2.1.23.13">69,72</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1a.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1a.4.2" style="font-size:90%;">The top-performing hyperparameter combinations for the post-processing module are presented in descending order.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.p2">
<ul class="ltx_itemize" id="S1.I1a">
<li class="ltx_item" id="S1.I1.i1a" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1a.p1">
<p class="ltx_p" id="S1.I1.i1a.p1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i1a.p1.3.1">IoU_threshold:</span> The threshold value determines the Intersection over Union (IoU) of stroke sequences to boxes. For each box, if the IoU between the box and the longest intersecting stroke sequence exceeds <span class="ltx_text ltx_font_italic" id="S1.I1.i1a.p1.3.2">IoU_threshold</span>, the sequence is assigned to that box. For the ablation study, we adjusted the threshold within a range of <math alttext="25\%" class="ltx_Math" display="inline" id="S1.I1.i1a.p1.1.m1.1"><semantics id="S1.I1.i1a.p1.1.m1.1a"><mrow id="S1.I1.i1a.p1.1.m1.1.1" xref="S1.I1.i1a.p1.1.m1.1.1.cmml"><mn id="S1.I1.i1a.p1.1.m1.1.1.2" xref="S1.I1.i1a.p1.1.m1.1.1.2.cmml">25</mn><mo id="S1.I1.i1a.p1.1.m1.1.1.1" xref="S1.I1.i1a.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1a.p1.1.m1.1b"><apply id="S1.I1.i1a.p1.1.m1.1.1.cmml" xref="S1.I1.i1a.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i1a.p1.1.m1.1.1.1.cmml" xref="S1.I1.i1a.p1.1.m1.1.1.1">percent</csymbol><cn id="S1.I1.i1a.p1.1.m1.1.1.2.cmml" type="integer" xref="S1.I1.i1a.p1.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1a.p1.1.m1.1c">25\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1a.p1.1.m1.1d">25 %</annotation></semantics></math> to <math alttext="85\%" class="ltx_Math" display="inline" id="S1.I1.i1a.p1.2.m2.1"><semantics id="S1.I1.i1a.p1.2.m2.1a"><mrow id="S1.I1.i1a.p1.2.m2.1.1" xref="S1.I1.i1a.p1.2.m2.1.1.cmml"><mn id="S1.I1.i1a.p1.2.m2.1.1.2" xref="S1.I1.i1a.p1.2.m2.1.1.2.cmml">85</mn><mo id="S1.I1.i1a.p1.2.m2.1.1.1" xref="S1.I1.i1a.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1a.p1.2.m2.1b"><apply id="S1.I1.i1a.p1.2.m2.1.1.cmml" xref="S1.I1.i1a.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i1a.p1.2.m2.1.1.1.cmml" xref="S1.I1.i1a.p1.2.m2.1.1.1">percent</csymbol><cn id="S1.I1.i1a.p1.2.m2.1.1.2.cmml" type="integer" xref="S1.I1.i1a.p1.2.m2.1.1.2">85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1a.p1.2.m2.1c">85\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1a.p1.2.m2.1d">85 %</annotation></semantics></math>, increasing by <math alttext="10\%" class="ltx_Math" display="inline" id="S1.I1.i1a.p1.3.m3.1"><semantics id="S1.I1.i1a.p1.3.m3.1a"><mrow id="S1.I1.i1a.p1.3.m3.1.1" xref="S1.I1.i1a.p1.3.m3.1.1.cmml"><mn id="S1.I1.i1a.p1.3.m3.1.1.2" xref="S1.I1.i1a.p1.3.m3.1.1.2.cmml">10</mn><mo id="S1.I1.i1a.p1.3.m3.1.1.1" xref="S1.I1.i1a.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1a.p1.3.m3.1b"><apply id="S1.I1.i1a.p1.3.m3.1.1.cmml" xref="S1.I1.i1a.p1.3.m3.1.1"><csymbol cd="latexml" id="S1.I1.i1a.p1.3.m3.1.1.1.cmml" xref="S1.I1.i1a.p1.3.m3.1.1.1">percent</csymbol><cn id="S1.I1.i1a.p1.3.m3.1.1.2.cmml" type="integer" xref="S1.I1.i1a.p1.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1a.p1.3.m3.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1a.p1.3.m3.1d">10 %</annotation></semantics></math> increments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2a" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2a.p1">
<p class="ltx_p" id="S1.I1.i2a.p1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i2a.p1.3.1">OR_threshold:</span> This is the threshold value that determines the assignment of remaining stroke sequences to boxes. If the overlap ratio of the longest unassigned stroke sequence with its nearest box exceeds <span class="ltx_text ltx_font_italic" id="S1.I1.i2a.p1.3.2">OR_threshold</span>, the sequence is assigned to that box. For the ablation study, we set the threshold ranges from <math alttext="30\%" class="ltx_Math" display="inline" id="S1.I1.i2a.p1.1.m1.1"><semantics id="S1.I1.i2a.p1.1.m1.1a"><mrow id="S1.I1.i2a.p1.1.m1.1.1" xref="S1.I1.i2a.p1.1.m1.1.1.cmml"><mn id="S1.I1.i2a.p1.1.m1.1.1.2" xref="S1.I1.i2a.p1.1.m1.1.1.2.cmml">30</mn><mo id="S1.I1.i2a.p1.1.m1.1.1.1" xref="S1.I1.i2a.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2a.p1.1.m1.1b"><apply id="S1.I1.i2a.p1.1.m1.1.1.cmml" xref="S1.I1.i2a.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i2a.p1.1.m1.1.1.1.cmml" xref="S1.I1.i2a.p1.1.m1.1.1.1">percent</csymbol><cn id="S1.I1.i2a.p1.1.m1.1.1.2.cmml" type="integer" xref="S1.I1.i2a.p1.1.m1.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2a.p1.1.m1.1c">30\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2a.p1.1.m1.1d">30 %</annotation></semantics></math> to <math alttext="80\%" class="ltx_Math" display="inline" id="S1.I1.i2a.p1.2.m2.1"><semantics id="S1.I1.i2a.p1.2.m2.1a"><mrow id="S1.I1.i2a.p1.2.m2.1.1" xref="S1.I1.i2a.p1.2.m2.1.1.cmml"><mn id="S1.I1.i2a.p1.2.m2.1.1.2" xref="S1.I1.i2a.p1.2.m2.1.1.2.cmml">80</mn><mo id="S1.I1.i2a.p1.2.m2.1.1.1" xref="S1.I1.i2a.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2a.p1.2.m2.1b"><apply id="S1.I1.i2a.p1.2.m2.1.1.cmml" xref="S1.I1.i2a.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i2a.p1.2.m2.1.1.1.cmml" xref="S1.I1.i2a.p1.2.m2.1.1.1">percent</csymbol><cn id="S1.I1.i2a.p1.2.m2.1.1.2.cmml" type="integer" xref="S1.I1.i2a.p1.2.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2a.p1.2.m2.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2a.p1.2.m2.1d">80 %</annotation></semantics></math> in <math alttext="5\%" class="ltx_Math" display="inline" id="S1.I1.i2a.p1.3.m3.1"><semantics id="S1.I1.i2a.p1.3.m3.1a"><mrow id="S1.I1.i2a.p1.3.m3.1.1" xref="S1.I1.i2a.p1.3.m3.1.1.cmml"><mn id="S1.I1.i2a.p1.3.m3.1.1.2" xref="S1.I1.i2a.p1.3.m3.1.1.2.cmml">5</mn><mo id="S1.I1.i2a.p1.3.m3.1.1.1" xref="S1.I1.i2a.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2a.p1.3.m3.1b"><apply id="S1.I1.i2a.p1.3.m3.1.1.cmml" xref="S1.I1.i2a.p1.3.m3.1.1"><csymbol cd="latexml" id="S1.I1.i2a.p1.3.m3.1.1.1.cmml" xref="S1.I1.i2a.p1.3.m3.1.1.1">percent</csymbol><cn id="S1.I1.i2a.p1.3.m3.1.1.2.cmml" type="integer" xref="S1.I1.i2a.p1.3.m3.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2a.p1.3.m3.1c">5\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2a.p1.3.m3.1d">5 %</annotation></semantics></math> increments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3a" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3a.p1">
<p class="ltx_p" id="S1.I1.i3a.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i3a.p1.1.1">num_repeats:</span> This refers to the total number of iterations the post-processing module undergoes to complete the stroke assignment process. The post-processing module continues until stroke group assignments reach a stable state. However, this approach can increase runtime, so we limited the number of iterations to evaluate the impact of different repetition counts. We tested the effect of the <span class="ltx_text ltx_font_italic" id="S1.I1.i3a.p1.1.2">num_repeats</span> parameter with values of 1, 3, 5, 7, and 9.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.I1.i4.p1.1.1">stroke_thickness:</span> We assessed the effect of stroke line thickness by evaluating the <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.2">stroke_thickness</span> parameter with values of 1, 2, 3, and 4, where higher values correspond to thicker stroke lines in the scene.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S1.T1a" title="Table 1 ‣ S1.1 Hyperparameter Optimization ‣ S1 Details on Post-Processing Module ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the impact of each parameter, revealing that the best-performing hyperparameter combination includes these values: <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.1">IoU_threshold</span> set to 65%, <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.2">OR_threshold</span> to 60%, <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.3">num_repeats</span> to 3, and <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.4">stroke_thickness</span> to 2. As demonstrated, using a value for <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.5">stroke_thickness</span> different than 2 degrades performance by distorting the features of the sketches. The <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.6">num_repeats</span> parameter does not significantly affect performance when increased, indicating that the stroke assignment operation completes effectively within a few iterations, minimizing the need for extended runtime. Setting <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.7">OR_threshold</span> to a low percentage can lead to incorrect stroke assignments, as some strokes that should be labeled as separate objects are merged with other stroke sequences. Therefore, setting <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.8">OR_threshold</span> higher than 50% generally results in better performance. A range of 55%-65% for <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.9">IoU_threshold</span> yields the best results. Lower <span class="ltx_text ltx_font_italic" id="S1.SS1.p3.1.10">IoU_threshold</span> values can lead to incorrect stroke-to-box assignments, while higher values may prevent the accurate stroke assignment.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S1.2 </span>Post-Processing Time &amp; Memory Footprint</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Our post-processor takes on average 345 milliseconds per scene on CPU and has the memory upper bound of 5 times the scene in vector format.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S2 </span>Additional Details on RGB Coloring Technique</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="420" id="S2.F1.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure S1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Sample scene sketch from the CBSC, which demonstrates the input for our object detector model. Each stroke within the scene is color-coded based on drawing order, utilizing a spectrum ranging from blue to red, as illustrated at the bottom.</span></figcaption>
</figure>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.2" style="width:447.2pt;height:81.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.9pt,19.2pt) scale(0.680734461486772,0.680734461486772) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.2.1">
<tr class="ltx_tr" id="S2.T2.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.1.1.1" rowspan="2"><span class="ltx_text" id="S2.T2.2.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.1.2"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S2.T2.2.1.1.4">Top-1 Accuracy</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.1.5"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S2.T2.2.1.1.7">Top-3 Accuracy</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.1.8"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.1.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S2.T2.2.1.1.10">Top-5 Accuracy</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.2">
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.2.1"></td>
<td class="ltx_td" id="S2.T2.2.1.2.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.3">CBSC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.4">FrISS-QD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.5">Avg.</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.2.6"></td>
<td class="ltx_td" id="S2.T2.2.1.2.7"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.8">CBSC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.9">FrISS-QD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.10">Avg.</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.2.11"></td>
<td class="ltx_td" id="S2.T2.2.1.2.12"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.13">CBSC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.14">FrISS-QD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.15">Avg.</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.1.3.1">SketchR2CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.3.2"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.4">63.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.5">48.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.6">55.85</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.3.7"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.3.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.9">71.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.10">59.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.11">65.35</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T2.2.1.3.12"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.2.1.3.13"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.14">74.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.15">63.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.3.16">68.59</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.4">
<td class="ltx_td ltx_align_left" id="S2.T2.2.1.4.1">MGT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib31" title="">31</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.4.2"></td>
<td class="ltx_td" id="S2.T2.2.1.4.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.4">65.29</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.5">51.78</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.6">58.54</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.4.7"></td>
<td class="ltx_td" id="S2.T2.2.1.4.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.9">79.22</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.10">67.85</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.11">73.54</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.4.12"></td>
<td class="ltx_td" id="S2.T2.2.1.4.13"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.14">83.63</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.15">73.40</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.4.16">78.52</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.5">
<td class="ltx_td ltx_align_left" id="S2.T2.2.1.5.1">Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.5.2"></td>
<td class="ltx_td" id="S2.T2.2.1.5.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.4">65.88</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.5">52.82</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.6">59.35</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.5.7"></td>
<td class="ltx_td" id="S2.T2.2.1.5.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.9">80.81</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.10">66.57</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.11">73.69</td>
<td class="ltx_td ltx_border_r" id="S2.T2.2.1.5.12"></td>
<td class="ltx_td" id="S2.T2.2.1.5.13"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.14">85.69</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.15">71.36</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.1.5.16">78.53</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.2.1.6.1">Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T2.2.1.6.2"></td>
<td class="ltx_td ltx_border_b" id="S2.T2.2.1.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.4.1">67.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.5.1">55.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.6.1">61.47</span></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T2.2.1.6.7"></td>
<td class="ltx_td ltx_border_b" id="S2.T2.2.1.6.8"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.9.1">82.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.10.1">70.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.11"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.11.1">76.56</span></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T2.2.1.6.12"></td>
<td class="ltx_td ltx_border_b" id="S2.T2.2.1.6.13"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.14"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.14.1">86.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.15"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.15.1">74.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.2.1.6.16"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.16.1">80.33</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.4.2" style="font-size:90%;">Analysis on state-of-the-art single sketch classifiers</span></figcaption>
</figure>
<div class="ltx_para" id="S2a.p1">
<p class="ltx_p" id="S2a.p1.1">We adopted an RGB coloring technique to maintain a 3-channel input and values ranging from 0 to 255 for the detector. In our design, the neighboring strokes are represented with colors closer in the spectrum that spans from blue to red. Therefore, the strokes of the same object are expected to contain similar colors. Although a single object may not be entirely drawn in one stroke sequence, individual sequences are expected to exhibit consistent patterns. Besides the shape and distance of strokes, we expect our detector to recognize groups of consecutively sketched strokes. An illustrative example of a scene colored according to stroke order is given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2.F1" title="Figure S1 ‣ S2 Additional Details on RGB Coloring Technique ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S1</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S3a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S3 </span>Additional Analysis on External Classifiers</h2>
<div class="ltx_para" id="S3a.p1">
<p class="ltx_p" id="S3a.p1.1">To develop a CNN-based sketch classifier, I first train several models, including Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>, VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib25" title="">25</a>]</cite>, ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib13" title="">13</a>]</cite>, ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib13" title="">13</a>]</cite>, MobileNet-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib14" title="">14</a>]</cite>, and MobileNet-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib23" title="">23</a>]</cite>, using only the QuickDraw dataset. Afterward, I select the top three performing models and conduct further training by incorporating the FrISS training set along with QuickDraw. In both phases of the experiment, Inception-V3 consistently outperforms the other classifiers. Additionally, including the FrISS training set improves overall performance across both datasets. The results are summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.T3" title="Table 3 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>. Based on these results, our pretrained Inception-V3 is selected as the external CNN-based classifier in our experiments.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.2" style="width:433.6pt;height:250.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(17.2pt,-10.0pt) scale(1.0862356857281,1.0862356857281) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.2.1">
<tr class="ltx_tr" id="S3.T3.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S3.T3.2.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Train Dataset</td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T3.2.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.2">
<td class="ltx_td" id="S3.T3.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">QD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">FrISS</td>
<td class="ltx_td" id="S3.T3.2.1.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">CBSC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">FrISS-QD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">Avg.</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">65.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">50.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.3.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.3.8.1" style="color:#00FF00;">57.88</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib25" title="">25</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">64.02</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">50.69</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.4.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.4.8.1" style="color:#0000FF;">57.36</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib13" title="">13</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.5.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">63.04</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.5.7" style="padding-left:4.0pt;padding-right:4.0pt;">48.79</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.5.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.5.8.1" style="color:#FF0000;">55.92</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib13" title="">13</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.6.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">62.84</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">48.03</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">55.44</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">MobileNetV3-Small <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib14" title="">14</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.7.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.7.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.7.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">61.37</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.7.7" style="padding-left:4.0pt;padding-right:4.0pt;">46.85</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.7.8" style="padding-left:4.0pt;padding-right:4.0pt;">54.11</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">MobileNetV3-Large <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib14" title="">14</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.8.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.8.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.8.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">60.88</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">48.89</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.8.8" style="padding-left:4.0pt;padding-right:4.0pt;">54.89</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">MobileNet-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td" id="S3.T3.2.1.9.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.9.4" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="S3.T3.2.1.9.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">62.55</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">47.75</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.9.8" style="padding-left:4.0pt;padding-right:4.0pt;">55.15</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.1.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">Inception-V3</td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.10.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="S3.T3.2.1.10.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">67.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">55.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.10.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.10.8.1" style="color:#00FF00;">61.47</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.1.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">VGG19</td>
<td class="ltx_td" id="S3.T3.2.1.11.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" id="S3.T3.2.1.11.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.11.6" style="padding-left:4.0pt;padding-right:4.0pt;">65.98</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.11.7" style="padding-left:4.0pt;padding-right:4.0pt;">55.24</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.11.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.11.8.1" style="color:#0000FF;">60.61</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.12">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T3.2.1.12.1" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet18</td>
<td class="ltx_td ltx_border_b" id="S3.T3.2.1.12.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.2.1.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.2.1.12.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_b" id="S3.T3.2.1.12.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.2.1.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">67.65</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.2.1.12.7" style="padding-left:4.0pt;padding-right:4.0pt;">53.11</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.2.1.12.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S3.T3.2.1.12.8.1" style="color:#FF0000;">60.38</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.6.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.7.2" style="font-size:90%;">The ablation study is performed to measure the effect of different backbone architectures and the effect of including the FrISS dataset in the training set. The highest average score is highlighted in <span class="ltx_text" id="S3.T3.7.2.1" style="color:#00FF00;">green</span>, the second highest in <span class="ltx_text" id="S3.T3.7.2.2" style="color:#0000FF;">blue</span>, and the third highest in <span class="ltx_text" id="S3.T3.7.2.3" style="color:#FF0000;">red</span> for each aspect (i.e., backbone type and FrISS contribution).</span></figcaption>
</figure>
<div class="ltx_para" id="S3a.p2">
<p class="ltx_p" id="S3a.p2.1">I evaluate the performance of several state-of-the-art stroke-based sketch classifiers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>, and results are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S2.T2" title="Table 2 ‣ S2 Additional Details on RGB Coloring Technique ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>. The highest-performing transformer-based classifier, Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite> is outperformed by our pretrained Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite>. To demonstrate the compatibility of CAVT with a stroke-based external classifier, Sketchformer is utilized in an end-to-end manner.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="745" id="S3.F2.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure S2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Visual comparison of our method with LDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and OV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite> models, tested on FrISS dataset. We utilize CAVT with the external classifier Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite> (CAVT-S) and our pre-trained Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite> (CAVT-I) in an end-to-end manner.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="745" id="S3.F3.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure S3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Visual comparison of our method with LDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and OV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite> models, tested on CBSC dataset. We utilize CAVT with the external classifier Sketchformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib22" title="">22</a>]</cite> (CAVT-S) and our pre-trained Inception-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib27" title="">27</a>]</cite> (CAVT-I) in an end-to-end manner.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1163" id="S3.F4.g1" src="x8.png" width="571"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure S4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Instance-level visual results of CAVT in FrISS and CBSC datasets combined.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S4 </span>Additional Visual Results on Scene Sketch Semantic Segmentation</h2>
<div class="ltx_para" id="S4a.p1">
<p class="ltx_p" id="S4a.p1.1">In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.SS5" title="5.5 Comparison Against State-of-the-art (SOTA) ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">5.5</span></a> of the main document, we provide a numerical comparison of the segmentation results obtained using our pipelines and two state-of-the-art methods: LDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite> and OV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib2" title="">2</a>]</cite>. Additionally, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F4" title="Figure 4 ‣ 5.4 Implementation Details ‣ 5 Experiments ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> from the main document, we present a visual comparison of our method against LDP and OV. Here, we provide additional visual results of our method against state-of-the-art models, assessed on FrISS and CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite> datasets in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F2" title="Figure S2 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F3" title="Figure S3 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S3</span></a>, respectively. To visualize class-level segmentation results, we colored each pixel or stroke within the scene regarding its predicted object category.</p>
</div>
<div class="ltx_para" id="S4a.p2">
<p class="ltx_p" id="S4a.p2.1">The additional visual outcomes depicted in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F2" title="Figure S2 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F3" title="Figure S3 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S3</span></a> demonstrate consistent segmentation results from both our primary pipelines (CAVT-S and CAVT-I) and its variant (CAVT-S* and CAVT-I). Therefore, we can observe that leveraging stroke representations of sketches and the temporal order of stroke sequences is a promising solution for the scene sketch segmentation problem. In some cases, although our class-agnostic approach successfully segments object instances, our adopted classifier may cause a performance drop due to its misclassification. For instance, in the 3rd row of Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F2" title="Figure S2 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S2</span></a>, our class-agnostic approach accurately segments the <span class="ltx_text ltx_font_italic" id="S4a.p2.1.1">’sheep’</span> object. However, our adopted classifiers mislabel <span class="ltx_text ltx_font_italic" id="S4a.p2.1.2">’sheep’</span> as <span class="ltx_text ltx_font_italic" id="S4a.p2.1.3">’horse’</span> and <span class="ltx_text ltx_font_italic" id="S4a.p2.1.4">’dog’</span>, thus impacting the segmentation results at the class level. This highlights the potential for our class-agnostic method’s improved performance when paired with a classifier offering more accurate object class predictions. A similar issue is observed for the <span class="ltx_text ltx_font_italic" id="S4a.p2.1.5">’cloud’</span> object in the 2nd row of Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F3" title="Figure S3 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S3</span></a>.</p>
</div>
<div class="ltx_para" id="S4a.p3">
<p class="ltx_p" id="S4a.p3.1">In addition to the class-level results, we share additional instance-level segmentation results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.F4" title="Figure S4 ‣ S3 Additional Analysis on External Classifiers ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S4</span></a>. In this figure, we can see that our pipelines successfully segment the objects from the same categories. While two houses are successfully differentiated in the 3rd row, the clouds are successfully detected and identified in the 6th row. However, there also exist some rare cases in which CAVT fails to segment (see individual birds and clouds in the 1st row).</p>
</div>
</section>
<section class="ltx_section" id="S5a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S5 </span>Additional Details on FrISS Dataset</h2>
<section class="ltx_subsection" id="S5.SS1a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.1 </span>UI of Data Collection Web Application</h3>
<div class="ltx_para" id="S5.SS1a.p1">
<p class="ltx_p" id="S5.SS1a.p1.1">In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4" title="4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> of the main document, we provide a detailed discussion of our data collection process. In Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F5" title="Figure S5 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F6" title="Figure S6 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S6</span></a>, we present visuals from the user interface of our data collection web application. As we discussed in the main document, our data collection consists of two distinct phases: sketch collection and sketch annotation. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F5" title="Figure S5 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S5</span></a> provides an example of the sketch collection phase, where participants are tasked with illustrating a scene within a time frame of 1.5 minutes, using a provided text description as a reference. Each participant sequentially draws 10 distinct scene sketches by referring to the corresponding descriptions. Upon completing the sketch collection phase, participants proceed to the second phase, where they annotate their previously drawn sketches.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="339" id="S5.F5.g1" src="extracted/5891231/figures/friss/ui_drawing.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure S5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">The screenshot from the UI of the data collection web application during the drawing phase</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="263" id="S5.F6.g1" src="extracted/5891231/figures/friss/ui_annotation.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="258" id="S5.F6.g2" src="extracted/5891231/figures/friss/ui_annotation2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.4.1.1" style="font-size:90%;">Figure S6</span>: </span><span class="ltx_text" id="S5.F6.5.2" style="font-size:90%;">The screenshot of data collection UI during the annotation phase. The upper image is taken while labeling the strokes corresponding to the initial object, <span class="ltx_text ltx_font_italic" id="S5.F6.5.2.1">’car’</span>. The lower image is taken before labeling the final drawn object, <span class="ltx_text ltx_font_italic" id="S5.F6.5.2.2">’tree’</span>. Annotated object classes are listed in the upper-right corner of the UI, in the order of labeling.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1a.p2">
<p class="ltx_p" id="S5.SS1a.p2.1">During the annotation phase, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F6" title="Figure S6 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S6</span></a>, selected strokes turn from <span class="ltx_text ltx_font_italic" id="S5.SS1a.p2.1.1">’gray’</span> to <span class="ltx_text ltx_font_italic" id="S5.SS1a.p2.1.2">’black’</span> and participants assign a category to each stroke that turns into <span class="ltx_text ltx_font_italic" id="S5.SS1a.p2.1.3">’black’</span>. The annotation process continues until each object instance within the scene is labeled (i.e., each stroke turns into <span class="ltx_text ltx_font_italic" id="S5.SS1a.p2.1.4">’black’</span>). In the process of assigning categories, participants have the option to select from a predetermined list or introduce new categories by entering them into a designated text box (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F6" title="Figure S6 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S6</span></a>). The predetermined list includes all QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite> classes and additional well-known categories not included in QuickDraw but likely to be sketched by participants (e.g., balloon, plate, carpet). This list is provided to ease the labeling process. Finally, strokes that are labeled as incompletely sketched or unrecognizable are marked as <span class="ltx_text ltx_font_italic" id="S5.SS1a.p2.1.5">’incomplete’</span> and excluded from the dataset. Upon acceptance, we will release our data collection web application to the public.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="872" id="S5.F7.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.1.1" style="font-size:90%;">Figure S7</span>: </span><span class="ltx_text" id="S5.F7.3.2" style="font-size:90%;">Sample scene sketches from our FrISS dataset paired with their textual scene descriptions</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="540" id="S5.F8.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure S8</span>: </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">A comparison of scene sketches from FrISS with those from FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite> and CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>. The visuals are selected to ensure that each set of scene sketches shares at least two object categories in common, with the common classes listed below each group of three.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T4a">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4a.2" style="width:472.0pt;height:132.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.0pt,19.1pt) scale(0.776447436000347,0.776447436000347) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4a.2.1">
<tr class="ltx_tr" id="S5.T4a.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.1.1" style="padding-left:1.6pt;padding-right:1.6pt;">Context</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.1.2" style="padding-left:1.6pt;padding-right:1.6pt;">Scene Description</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.1.3" style="padding-left:1.6pt;padding-right:1.6pt;">Expected Objects</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4a.2.1.1.4" style="padding-left:1.6pt;padding-right:1.6pt;">COCO Img Id</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.2.1" style="padding-left:1.6pt;padding-right:1.6pt;">bathroom</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.2.2" style="padding-left:1.6pt;padding-right:1.6pt;">In the bathroom, there is a toilet, a bathtub, and a hair dryer.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T4a.2.1.2.3" style="padding-left:1.6pt;padding-right:1.6pt;">toilet, bathtub, hair dryer</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4a.2.1.2.4" style="padding-left:1.6pt;padding-right:1.6pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.3.1" style="padding-left:1.6pt;padding-right:1.6pt;">beach</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.3.2" style="padding-left:1.6pt;padding-right:1.6pt;">A group of people stand on the beach and fly a kite.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.3.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, kite, beach</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.3.4" style="padding-left:1.6pt;padding-right:1.6pt;">92478</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.4.1" style="padding-left:1.6pt;padding-right:1.6pt;">outdoor</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.4.2" style="padding-left:1.6pt;padding-right:1.6pt;">A girl is standing next to a stop sign with an umbrella in her hand.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.4.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, umbrella, stop sign</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.4.4" style="padding-left:1.6pt;padding-right:1.6pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.5.1" style="padding-left:1.6pt;padding-right:1.6pt;">garden</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.5.2" style="padding-left:1.6pt;padding-right:1.6pt;">Four sheep are eating grass, and a child is approaching them.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.5.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, sheep, grass</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.5.4" style="padding-left:1.6pt;padding-right:1.6pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.6.1" style="padding-left:1.6pt;padding-right:1.6pt;">laboratory</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.6.2" style="padding-left:1.6pt;padding-right:1.6pt;">A computer workstation with a printer, computer, mouse, and keyboards.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.6.3" style="padding-left:1.6pt;padding-right:1.6pt;">printer, computer, mouse, keyboard</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.6.4" style="padding-left:1.6pt;padding-right:1.6pt;">102609</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.7.1" style="padding-left:1.6pt;padding-right:1.6pt;">park</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.7.2" style="padding-left:1.6pt;padding-right:1.6pt;">A skateboarder with a hat is riding his skateboard to walk his dog.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.7.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, skateboard, dog, hat</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.7.4" style="padding-left:1.6pt;padding-right:1.6pt;">304173</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.8.1" style="padding-left:1.6pt;padding-right:1.6pt;">living room</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.8.2" style="padding-left:1.6pt;padding-right:1.6pt;">A child eats ice cream and his eyeglasses fall on the carpet.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4a.2.1.8.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, ice cream, carpet, eyeglasses</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T4a.2.1.8.4" style="padding-left:1.6pt;padding-right:1.6pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T4a.2.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T4a.2.1.9.1" style="padding-left:1.6pt;padding-right:1.6pt;">hospital</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T4a.2.1.9.2" style="padding-left:1.6pt;padding-right:1.6pt;">A doctor is holding a syringe and test tube.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b" id="S5.T4a.2.1.9.3" style="padding-left:1.6pt;padding-right:1.6pt;">person, syringe, test tube, bed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T4a.2.1.9.4" style="padding-left:1.6pt;padding-right:1.6pt;">-</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4a.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4a.4.2" style="font-size:90%;">Sample scene descriptions paired with the expected objects to be drawn by participants during the drawing phase of FrISS. The corresponding real-life image id is provided if the textual description is taken from the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib20" title="">20</a>]</cite>.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="360" id="S5.F9.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.2.1.1" style="font-size:90%;">Figure S9</span>: </span><span class="ltx_text" id="S5.F9.3.2" style="font-size:90%;">The visualization of the number of scene sketches that each object category appears in. For visualization purposes, we selected the categories that have more than 15 appearances in the FrISS dataset.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S5.F10.g1" src="extracted/5891231/figures/friss/categories_per_scene.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1" style="font-size:90%;">Figure S10</span>: </span><span class="ltx_text" id="S5.F10.3.2" style="font-size:90%;">The visualization of the number of unique object categories per scene in the FrISS dataset</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.2 </span>Visual Comparison of FrISS to Other Datasets</h3>
<div class="ltx_para" id="S5.SS2a.p1">
<p class="ltx_p" id="S5.SS2a.p1.1">In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S4.SS3" title="4.3 Statistics and Analysis ‣ 4 The FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a> of the main document, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.T2" title="Table 2 ‣ 3.3 Synthetic Dataset Preparation for Training ‣ 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> provides a statistical comparison of various scene sketch datasets, focusing on category, object, and stroke counts per sketch. Among these datasets provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S3.T2" title="Table 2 ‣ 3.3 Synthetic Dataset Preparation for Training ‣ 3 Methodology ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>, FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite>, and SFSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib37" title="">37</a>]</cite> contain free-hand scene sketches stored in vector format. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F8" title="Figure S8 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S8</span></a>, we provide a detailed visual comparison between FrISS and these datasets. However, we could only share the visual comparisons between CBSC and FS-COCO, as SFSD is not publicly available. Additionally, we include extra sample scene sketches from FrISS along with their corresponding textual scene descriptions in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F7" title="Figure S7 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S7</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2a.p2">
<p class="ltx_p" id="S5.SS2a.p2.1">CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite> and FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite> are collected under similar conditions: participants are permitted multiple drawing attempts, with an average completion time of 3 minutes per scene. In contrast, we imposed a drawing time limit of 1.5 minutes for each scene in our dataset, allowing redraw attempts only within this constrained timeframe, without permitting complete redraws. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F8" title="Figure S8 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S8</span></a>, our free-hand scene sketches exhibit significantly fewer strokes per object compared to those in FS-COCO. Furthermore, in the creation of FS-COCO, participants were presented with natural images as references during the drawing process. This results in scene sketches with similar object positions and postures as those in the referenced images. Conversely, the CBSC dataset was collected by instructing participants to quickly draw simple scene sketches that convey semantic meaning to humans, without any time restrictions. Our scene sketches demonstrate comparable object complexities to those in CBSC. However, while CBSC comprises 331 scene sketches covering 74 object categories, FrISS consists of 1K free-hand scene sketches, spanning a broader spectrum of object categories, totaling 403.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.3 </span>Details of Textual Scene Descriptions</h3>
<div class="ltx_para" id="S5.SS3a.p1">
<p class="ltx_p" id="S5.SS3a.p1.1">Scene descriptions are sourced either from the MS COCO dataset image captions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib20" title="">20</a>]</cite> or manually created by us. Relying solely on MS COCO captions was insufficient to cover a wider range of object categories due to the dataset’s limited variety. To ensure a broader representation, we aimed to include descriptions with at least three objects per scene, making sure the prompts were simple and drawable by individuals without professional drawing skills.</p>
</div>
<div class="ltx_para" id="S5.SS3a.p2">
<p class="ltx_p" id="S5.SS3a.p2.1">To increase scene variety, most of the descriptions were manually constructed. We first gathered a list of environments likely to contain everyday objects. Then, we constructed scene descriptions featuring approximately 3 to 5 objects, ensuring they could be easily drawn within a specified time limit. In total, 180 unique scene descriptions were created, covering 403 object categories in FrISS. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.T4a" title="Table 4 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> presents a subset of our scene descriptions along with their environments. The list of contexts is as follows: <span class="ltx_text ltx_font_italic" id="S5.SS3a.p2.1.1">beach, zoo, sky, living room, ocean, kitchen, military base, stadium, concert hall, river, airport, hospital, jungle, graveyard, laboratory, camping site, restaurant, garden, gym, bedroom, gas station, battlefield, library, tower, school, cave, police station, space, museum, hotel, court, farm, hairdresser, park, bathroom, business center, music store, outdoor</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.4 </span>Detailed Analysis of FrISS</h3>
<div class="ltx_para" id="S5.SS4a.p1">
<p class="ltx_p" id="S5.SS4a.p1.1">Here, we provide additional analysis on our collected dataset in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F10" title="Figure S10 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S10</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F9" title="Figure S9 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S9</span></a>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F10" title="Figure S10 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S10</span></a>, we observe that the count of distinct object categories within a scene varies between 1 and 10, with a dominant accumulation between 3 and 6. Additionally, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#S5.F9" title="Figure S9 ‣ S5.1 UI of Data Collection Web Application ‣ S5 Additional Details on FrISS Dataset ‣ Class-Agnostic Visio-Temporal Scene Sketch Semantic Segmentation"><span class="ltx_text ltx_ref_tag">S9</span></a> reveals that the most frequently occurring object categories in FrISS are person, tree, table, flower, and cloud, with the remaining categories distributed more balanced throughout the dataset. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S5.SS4a.p2">
<p class="ltx_p" id="S5.SS4a.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4a.p2.1.1">List of Categories in FrISS:</span> airplane, alarm clock, ambulance, ant, apple, arm, asparagus, axe, backpack, banana, bandage, barn, baseball, baseball bat, basket, basketball, bathtub, beach, bear, bed, bee, belt, bench, bicycle, binoculars, bird, birthday cake, blackberry, book, boomerang, bowtie, bracelet, bread, bridge, broom, bucket, bus, bush, butterfly, cake, calendar, camera, campfire, candle, cannon, canoe, car, carrot, castle, cat, ceiling fan, cell phone, cello, chair, chandelier, clarinet, clock, cloud, coffee cup, compass, computer, cookie, cooler, couch, cow, crab, crayon, crown, cruise ship, cup, dishwasher, dog, dolphin, donut, door, dresser, drill, drums, duck, dumbbell, elephant, eraser, eyeglasses, face, fan, fence, fire hydrant, fireplace, fish, flamingo, flashlight, flip flops, floor lamp, flower, fork, garden, giraffe, grapes, grass, guitar, hammer, hand, harp, hat, headphones, helmet, horse, hot air balloon, hot dog, hourglass, house, ice cream, key, keyboard, knife, ladder, laptop, leaf, light bulb, lighter, lighthouse, lightning, lion, lollipop, mailbox, map, microphone, microwave, moon, motorbike, mountain, mouse, mug, mushroom, necklace, ocean, octopus, onion, oven, palm tree, panda, pants, paper clip, pear, peas, pencil, penguin, picture frame, pig, pillow, pizza, police car, pond, pool, popsicle, potato, purse, rabbit, radio, rain, rainbow, rake, remote control, rhinoceros, river, sailboat, sandwich, saw, saxophone, school bus, scissors, screwdriver, sea turtle, see saw, shark, sheep, shoe, shovel, sink, skateboard, skull, skyscraper, sleeping bag, smiley face, snake, snorkel, snowflake, snowman, soccer ball, sock, spider, spoon, squirrel, stairs, star, steak, stereo, stop sign, stove, strawberry, streetlight, string bean, submarine, suitcase, sun, swan, swing set, syringe, t-shirt, table, teddy-bear, telephone, television, tennis racquet, tent, toilet, toothbrush, toothpaste, tractor, traffic light, train, tree, truck, trumpet, umbrella, vase, washing machine, watermelon, waterslide, wheel, windmill, wine bottle, wine glass, wristwatch, yoga*, zebra, anchor, bag, ball, balloon, barrier, baseball field, basketball hoop, bee nest, bell, billboard, board, bone, bottle, bowl, box, branch, building, button, cabinet, cable, cage, candy, carpet, cave, ceiling, cheese, chicken, cockroach, coconut, computer case, container, coral, counter, crosswalk, cupboard, curly hair, curtain, dagger, desk, dirt, dog collar, drain, drawer, earth, egg, exhibition, field, fish tank, fishing net, fishing rod, flag, floor, football field, footprint, fridge, frisbee, gas pump, gas station, glass, glass shard, glove, goal, gun, hair, hair dryer, hair tie, hammock, handcuffs, hanger, heart, hook, ice, jellyfish, kite, lake, lamp, light effect, marshmallow, meat, mirror, monitor, moon crater, mousepad, mud, museum, music note, necktie, needles, net, notebook, notes, orange, paddle, paper, path, pathway, peach, pepper, phone box, picnic rug, pipe, plant, plate, plug, present, printer, propeller, rail, restaurant, ribbon, road, rocket, roof, room, rope, ruler, safe, salt, sand, sandcastle, sausage, scarecrow, scarf, sea, sea fish, sea goggles, sea horse, sea shell, seagull, serum, shelf, shower head, sidewalk, sign, slide, smoke, soccer field, speaker, spider web, stage, stage lights, stand, staple, station, stick, stone, stool, strainer, street, suit, sunflower, sunglasses, surfboard, swim goggles, tape player, tennis court, test tube, toilet paper, tomb, tower, toy, traffic cone, trash bin, tray, tribune, turnstile, wall, walnut, water, weapon, wind, window, wing, wood. 
<br class="ltx_break"/>Please note that in the FrISS dataset, <span class="ltx_text ltx_font_italic" id="S5.SS4a.p2.1.2">yoga*</span> denotes the <span class="ltx_text ltx_font_italic" id="S5.SS4a.p2.1.3">person</span> class. This mapping between the two classes is due to their visual similarity.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.5 </span>Common Categories of FrISS and Other Datasets</h3>
<div class="ltx_para" id="S5.SS5a.p1">
<ul class="ltx_itemize" id="S5.I1a">
<li class="ltx_item" id="S5.I1.i1a" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1a.p1">
<p class="ltx_p" id="S5.I1.i1a.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1a.p1.1.1">List of common categories between FrISS and SKY-Scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib9" title="">9</a>]</cite>:</span> airplane, apple, banana, bee, bench, bicycle, bird, butterfly, car, cat, chair, couch, cow, cup, dog, duck, flower, horse, house, mountain, pig, rabbit, sheep, strawberry, table, tree, truck, umbrella, wine bottle.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2a" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2a.p1">
<p class="ltx_p" id="S5.I1.i2a.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2a.p1.1.1">List of common categories between FrISS and SketchyScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib40" title="">40</a>]</cite>:</span> airplane, apple, banana, basket, bee, bench, bicycle, bird, bucket, bus, butterfly, car, cat, chair, cloud, couch, cow, cup, dog, duck, fence, flower, grass, horse, house, moon, mountain, pig, rabbit, sheep, star, streetlight, sun, table, tree, truck, umbrella, person.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">List of common categories between FrISS and QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib12" title="">12</a>]</cite>:</span> airplane, helicopter, alarm clock, clock, wristwatch, ambulance, firetruck, pickup truck, truck, leaf, van, apple, asparagus, onion, peas, potato, string bean, mushroom, backpack, banana, house, baseball, basketball, soccer ball, baseball bat, bear, panda, bed, bench, bicycle, bird, parrot, birthday cake, cake, blackberry, blueberry, grapes, pear, pineapple, strawberry, watermelon, book, bread, peanut, steak, bridge, broccoli, bus, school bus, bush, canoe, cruise ship, sailboat, speedboat, car, police car, carrot, cat, cell phone, chair, church, hospital, castle, cloud, coffee cup, cup, mug, computer, laptop, cooler, couch, cow, dog, donut, cookie, door, dresser, elephant, fence, fire hydrant, floor lamp, lantern, light bulb, flashlight, flower, fork, giraffe, hamburger, sandwich, horse, hot dog, house plant, jail, keyboard, knife, microwave, motorbike, mountain, mouse, ocean, oven, stove, dishwasher, washing machine, pillow, pizza, purse, rain, remote control, scissors, sheep, sink, skateboard, skyscraper, spoon, stairs, stop sign, suitcase, backpack, table, teddy-bear, television, tennis racquet, tent, toaster, toilet, toothbrush, traffic light, train, umbrella, vase, boomerang, basket, table, wine bottle, wine glass, person, zebra, stop sign, streetlight, hat, helmet, shoe, flip flops, eyeglasses, table, chandelier, ceiling fan, t-shirt, pants, dresser, pencil, eraser, grass, mountain, fence, river, sun, moon, star, snowflake, tree, palm tree</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">List of common categories between FrISS and CBSC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib36" title="">36</a>]</cite>:</span> candle, bus, backpack, keyboard, car, camera, clock, mug, television, truck, banana, couch, elephant, flower, oven, pillow, cow, helmet, sheep, bridge, bench, table, spoon, horse, sandwich, bread, ladder, skateboard, tree, suitcase, bed, giraffe, house, fence, train, laptop, hat, bird, zebra, eyeglasses, fork, carrot, toilet, cat, person, airplane, baseball, bicycle, computer, basket, tent, stairs, chair, cell phone, river, cloud, knife, vase, umbrella, leaf, mountain, pizza, bucket, bear, cup, dog, bush, apple, key, cake, book, mouse, ocean.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">List of common categories between FrISS and FS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00266v1#bib.bib5" title="">5</a>]</cite>:</span> cloud, orange, cow, net, hot dog, car, couch, laptop, frisbee, road, chair, wine glass, roof, bed, horse, fork, knife, pizza, bird, river, sandwich, fire hydrant, floor, banana, apple, counter, backpack, bear, plate, mud, toothbrush, shoe, cup, airplane, umbrella, mountain, book, scissors, window, donut, bush, spoon, stairs, keyboard, vase, grass, wood, fence, bottle, kite, plant, mirror, traffic light, cat, door, oven, dog, truck, bus, zebra, toilet, bridge, skateboard, bench, table, dirt, bicycle, cage, giraffe, tent, tree, cake, picnic rug, bowl, stop sign, branch, house, sand, elephant, clock, cell phone, paper, skyscraper, baseball bat, carrot, suitcase, field, train, stone, sheep, surfboard, flower, hat, sea, person, tennis racquet.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6a">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S5.6 </span>Ethical Considerations in Data Collection</h3>
<div class="ltx_para" id="S5.SS6a.p1">
<p class="ltx_p" id="S5.SS6a.p1.1">Our dataset contains free-hand scene sketches paired with their textual descriptions, audio clips of participants, and video recordings of drawing processes. During the drawing process, participants were asked to verbally explain their sketches in their native languages. At the beginning of the data collection, participants received detailed information regarding the following: the recording of their drawing screen in video format, the retention of their verbal descriptions as audio clips, and the potential release of their data in a research paper. Each participant was kindly requested to review and sign the consent form acknowledging our data collection procedures: 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S5.SS6a.p2">
<p class="ltx_p" id="S5.SS6a.p2.1"><span class="ltx_text ltx_font_italic" id="S5.SS6a.p2.1.1">’I confirm that I have thoroughly read and understood the instructions. I hereby authorize the utilization of my anonymized data (i.e., drawings, video, and audio recordings) for scientific research purposes.’</span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S5.SS6a.p3">
<p class="ltx_p" id="S5.SS6a.p3.1">Participants who consented to our data collection terms were assigned a random ID and proceeded with the data collection process. Additionally, we provided a contact address to allow participants to confidentially address any concerns regarding the release of their data.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 22:30:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
