<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.02280] Human-Adversarial Visual Question Answering</title><meta property="og:description" content="Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far fro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Human-Adversarial Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Human-Adversarial Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.02280">

<!--Generated on Sat Mar  2 05:05:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Human-Adversarial Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sasha Sheng<sup id="id9.9.id1" class="ltx_sup">‡</sup>  Amanpreet Singh<sup id="id10.10.id2" class="ltx_sup">‡</sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>  Vedanuj Goswami<sup id="id11.11.id3" class="ltx_sup">‡</sup>  Jose Alberto Lopez Magana  Wojciech Galuba<sup id="id12.12.id4" class="ltx_sup">‡</sup>  Devi Parikh<sup id="id13.13.id5" class="ltx_sup">‡</sup>  Douwe Kiela<sup id="id14.14.id6" class="ltx_sup">‡</sup>
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">‡</sup> Facebook AI Research  <sup id="id16.16.id8" class="ltx_sup">†</sup> Tecnológico de Monterrey 
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contribution. Correspondence to <a href="mailto:advqa@fb.com" title="" class="ltx_ref ltx_href">advqa@fb.com</a>.Work done as an intern at Facebook AI Research.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We plan to make an evaluation server available for community to evaluate on AdVQA.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) is widely recognized as an important evaluation task for vision and language research. Besides direct applications such as helping the visually impaired or multimodal content understanding on the web, it offers a mechanism for probing machine understanding of images via natural language queries. Making progress on VQA requires bringing together different subfields in AI – combining advances from natural language processing (NLP) and computer vision together with those in multimodal fusion – making it an exciting task in AI research.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Over the years, the performance of VQA models has started to plateau on the popular VQA v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> – approaching inter-human agreement – as evidenced by Fig. <a href="#S2.F1" title="Figure 1 ‣ Saturating prior work. ‣ 2 Related Work ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This raises important questions for the field: To what extent have we solved the problem? If we haven’t, what are we still missing? How good are we really?</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">An intriguing method for investigating these questions is dynamic data collection, where human annotators and state-of-the-art models are put “in the loop” together to collect data adversarially. Annotators are tasked with and rewarded for finding model-fooling examples, which are then verified by other humans. The easier it is to find such examples, the worse the model’s performance can be said to be. The collected data can be used to “stress test” current VQA models and can serve as the next iteration of the VQA benchmark that can help drive further progress.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The commonly used VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> dataset was collected by instructing annotators to “ask a question about this scene that [a]
smart robot probably can not answer” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. One way of thinking about our proposed human-adversarial data collection is that it explicitly ensures that the questions can not be answered by today’s “smartest” models.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This work is, to the best of our knowledge, the first to apply this human-adversarial approach to any multimodal problem. We introduce Adversarial VQA (AdVQA), a large evaluation dataset of 28,522 examples in total, all of which fooled the VQA 2020 challenge winner, MoViE+MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate a wide range of existing VQA models on AdVQA and find that their performance is significantly lower than on the commonly used VQA v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (see Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Furthermore, we conduct an extensive analysis of AdVQA characteristics, and contrast with the VQA v2 dataset.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We hope that this new benchmark can help advance the state of the art by shedding important light on the current model shortcomings. Our findings suggest that there is still considerable room for continued improvement, with much more work remaining to be done.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.8.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S1.T1.9.2" class="ltx_text ltx_font_bold" style="font-size:113%;">Contrastive examples from VQA and AdVQA<span id="S1.T1.9.2.1" class="ltx_text ltx_font_medium">. Predictions are given for the VisualBERT, ViLBERT and UniT models, respectively. Models can answer VQA questions accurately, but consistently fail on AdVQA questions.</span></span></figcaption>
<table id="S1.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.3.4.1" class="ltx_tr">
<th id="S1.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.3.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></th>
<th id="S1.T1.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.3.4.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.3.4.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></span>
</span>
</th>
<th id="S1.T1.3.4.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.3.4.1.3.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.3.4.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/1.jpeg" id="S1.T1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="118" height="89" alt="[Uncaptioned image]"></span></th>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: How many cats are in</span><span id="S1.T1.1.1.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.1.1.2.1.1.3" class="ltx_text" style="font-size:80%;">the image?</span><span id="S1.T1.1.1.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.1.1.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.2.1.1.5.1" class="ltx_text ltx_font_bold">A</span>: 2</span><span id="S1.T1.1.1.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.1.1.2.1.1.7" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.2.1.1.7.1" class="ltx_text ltx_font_bold">Model</span>: 2, 2, 2</span></span>
</span>
</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What brand is the tv?</span><span id="S1.T1.1.1.3.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.1.1.3.1.1.3" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.3.1.1.3.1" class="ltx_text ltx_font_bold">A</span>: lg</span><span id="S1.T1.1.1.3.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.1.1.3.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.1.1.3.1.1.5.1" class="ltx_text ltx_font_bold">Model</span>: sony, samsung, samsung</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<th id="S1.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.2.2.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/2.jpeg" id="S1.T1.2.2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="118" height="163" alt="[Uncaptioned image]"></span></th>
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.2.2.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Does the cat look happy?</span><span id="S1.T1.2.2.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.2.2.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.2.1.1.3.1" class="ltx_text ltx_font_bold">A</span>: no</span><span id="S1.T1.2.2.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.2.2.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.2.1.1.5.1" class="ltx_text ltx_font_bold">Model</span>: no, no, no</span></span>
</span>
</td>
<td id="S1.T1.2.2.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.2.2.3.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: How many cartoon drawings</span><span id="S1.T1.2.2.3.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.2.2.3.1.1.3" class="ltx_text" style="font-size:80%;">are present on the cat’s tie?</span><span id="S1.T1.2.2.3.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.2.2.3.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.3.1.1.5.1" class="ltx_text ltx_font_bold">A</span>: 4</span><span id="S1.T1.2.2.3.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.2.2.3.1.1.7" class="ltx_text" style="font-size:80%;"><span id="S1.T1.2.2.3.1.1.7.1" class="ltx_text ltx_font_bold">Model</span>: 1, 1, 2</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<th id="S1.T1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.3.3.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/3b.png" id="S1.T1.3.3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="118" height="157" alt="[Uncaptioned image]"></span></th>
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.3.3.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What kind of floor is the</span><span id="S1.T1.3.3.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.2.1.1.3" class="ltx_text" style="font-size:80%;">man sitting on?</span><span id="S1.T1.3.3.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.2.1.1.5.1" class="ltx_text ltx_font_bold">A</span>: wood</span><span id="S1.T1.3.3.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.2.1.1.7" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.2.1.1.7.1" class="ltx_text ltx_font_bold">Model</span>: wood, wood, wood</span></span>
</span>
</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S1.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.3.3.3.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.3.3.3.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Did someone else take this</span><span id="S1.T1.3.3.3.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.3.1.1.3" class="ltx_text" style="font-size:80%;">picture?</span><span id="S1.T1.3.3.3.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.3.1.1.5" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.3.1.1.5.1" class="ltx_text ltx_font_bold">A</span>: no</span><span id="S1.T1.3.3.3.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="S1.T1.3.3.3.1.1.7" class="ltx_text" style="font-size:80%;"><span id="S1.T1.3.3.3.1.1.7.1" class="ltx_text ltx_font_bold">Model</span>: yes, yes, yes</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stress testing VQA.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Several attempts exist for stress testing VQA models. Some examine to what extent VQA models’ predictions are grounded in the image content, as opposed to them relying primarily on language biases learned from the training dataset. The widely used VQA v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> was one attempt at this. The dataset contains pairs of similar images that have different answers to the same question, rendering a language-based prior inadequate. VQA under changing priors (VQA-CP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a more stringent test where the linguistic prior not only is weak in the test set, but is adversarial relative to the training dataset (<em id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_text"></span>, answers that are popular for a question type during training are rarer in the test set and vice versa). Datasets also exist that benchmark specific capabilities in VQA models, such as reading and reasoning about text in images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, leveraging external knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and spatial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or compositional reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Other vision and language datasets, such as Hateful Memes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, have also tried to make sure the task involves true multimodal reasoning, as opposed to any of the individual modalities sufficing for arriving at the correct label.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Saturating prior work.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> challenge has been running yearly since 2016 and has seen tremendous progress, as can be seen in Figure <a href="#S2.F1" title="Figure 1 ‣ Saturating prior work. ‣ 2 Related Work ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. From simple LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> fused models to more advanced fusion techniques (MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>) to better object detectors (MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, MoViE+MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>) to transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and self-supervised pretraining (MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, UNIMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>), the community has brought the performance of models on the VQA v2 dataset close to human accuracy, thus starting to saturate progress on dataset. As we show through AdVQA, however, saturation is far from achieved on the overall task, and hence there is a need for new datasets to continue benchmarking progress in vision and language reasoning.</p>
</div>
<figure id="S2.F1" class="ltx_figure ltx_align_floatright"><img src="/html/2106.02280/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Progress on the VQA v2 dataset over time.</span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adversarial datasets.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">As AI models are starting to work well enough in some narrow contexts for real world deployment, there are increasing concerns about the robustness of these models. This has led to fertile research both in designing adversarial examples to “attack” models (e.g., minor imperceptible noise added to image pixels that significantly changes the model’s prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), as well as in approaches to make models more robust to “defend” against these attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The human-in-the-loop adversarial setting that we consider in this paper is qualitatively different from the statistical perturbations typically explored when creating adversarial examples. This type of human-in-the-loop adversarial data collection have been explored in NLP e.g., natural language inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, hate speech detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and dialogue safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
To the best of our knowledge, ours is the first work to explore a multimodal human-adversarial benchmark.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">The aim of this work is to investigate state of the art VQA model performance via human-adversarial data collection. In this human-and-model-in-the-loop paradigm, human annotators are tasked with finding examples that fool the model. In this case, annotators are shown an image and are tasked with asking difficult but valid questions that the model answers incorrectly. We collected our dataset using Dynabench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, a platform for dynamic adversarial data collection and benchmarking. For details on our labeling user interface, please refer to the supplementary material.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p3.1" class="ltx_p">The VQA v2 dataset is based on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> images.
We collected adversarial questions for subsets of the val2017 COCO images (4,095) and test COCO images (22,259), respectively. The data collection involves three phrases ( Figure <a href="#S2.F2" title="Figure 2 ‣ Adversarial datasets. ‣ 2 Related Work ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>): First, in the <span id="S2.SS0.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_bold">question collection</span> phase, Amazon Mechanical Turk workers interact with a state-of-the-art VQA model and self-report image-question pairs that the model failed to answer correctly. Second, in the <span id="S2.SS0.SSS0.Px3.p3.1.2" class="ltx_text ltx_font_bold">question validation</span> phase, a new set of workers validates whether the answers provided by the VQA model for the image-question pairs collected in the first phrase are indeed incorrect. Finally, in the <span id="S2.SS0.SSS0.Px3.p3.1.3" class="ltx_text ltx_font_bold">answer collection</span> phase, we collect 10 ground truth answers for the image-question pairs validated in the second phase. In what follows, we provide further details for each of these steps.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2106.02280/assets/images/diagram.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Diagram of the AdVQA human-adversarial data collection flow.</span></figcaption>
</figure>
</section>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Question Collection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this phase, annotators on Amazon MTurk are shown an image and are tasked with asking questions about this image. The interface has a state-of-the-art VQA model in the loop. For each question the annotator asks, the model produces an answer. Annotators are asked to come up with questions that fool the model.
Annotators are done (<em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p1.1.2" class="ltx_text"></span>, can submit the HIT) once they successfully fooled the model or after they tried a minimum of 10 questions (whichever occurs first). To account for the fact that it may be hard to think of many questions for some images, annotators are given the option to skip an image after providing non-fooling three questions. We use the VQA Challenge 2020 winner – MoViE+MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> – trained on the COCO 2017 train set as the model in the loop. This is to ensure that the adversarial benchmark we collect is challenging for the current state-of-the-art in VQA.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The interface provides a digital magnifier to zoom into an area of the image when the mouse hovers over it, allowing workers to examine the image closely if necessary. See Figure <a href="#S2.F3.sf2" title="In Figure 3 ‣ 2.1 Question Collection ‣ 2 Related Work ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>. Keyboard shortcuts were provided for convenience.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/qc_censored.jpg" id="S2.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="453" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F3.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Main interface<span id="S2.F3.sf1.4.2.1" class="ltx_text ltx_font_medium">. The annotators follow the instructions provided to write the question in the text field provided which is then answered by the model-in-the-loop.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/new_magnifier.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="568" height="714" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F3.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Magnifier,<span id="S2.F3.sf2.4.2.1" class="ltx_text ltx_font_medium"> available both in question collection, validation and answer collection helps annotators ask and answer more specific questions.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question collection interface<span id="S2.F3.4.2.1" class="ltx_text ltx_font_medium"> showing the first stage of AdVQA collection setup.</span></span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Question Validation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Note that in the question collection phase, annotators self-report when they have identified a question that is incorrectly answered by the model. Whether or not this was actually the case is verified in the question validation phase. Two different annotators are shown the image, question, and answer predicted by the model, and asked whether the model’s answer is correct. As an additional quality control, the annotator is also asked whether the question is “valid”. A valid question is one where the image is necessary and sufficient to answer the question. Examples of invalid questions are: “What is the capital of USA?” (does not need an image) or “What is the person doing?” when the image does not contain any people or where there are multiple people doing multiple things. If the two validators disagree, a third annotator is used to break the tie. Examples are added to the dataset only if at least two annotators agree that the question is valid.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Answer Collection</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the final stage of data collection, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, we collect 10 answers per question providing instructions similar to those used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> making sure that no annotator sees the same question twice. In addition, as an extra step of caution, to filter bad questions that might have passed through last two stages and to account for ambiguity that can be present in questions, we allow annotators to select <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">“unanswerable”</span> as an answer.
Further, to ensure superior data quality, we occasionally provide annotators with hand-crafted questions for which we know the non-ambiguous single true answer, as a means to identify and filter out annotators providing poor quality responses.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Human-Adversarial Annotation Statistics</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The statistics for the first two stages (question collection and validation) are shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.4 Human-Adversarial Annotation Statistics ‣ 2 Related Work ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We find that annotators took about 5 tries and on average around 4 minutes to find a model-fooling example. For computing the model error rate, we can look at the instances where annotators claimed they had fooled the model, and where annotators were verified by other annotators to have fooled the model. The latter has been argued to be a particularly good metric for measuring model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We also further confirm that the model was indeed fooled by running the model-in-the-loop on a subset of examples in which human agreement was 100% and found the accuracy to be 0.08% on val, and 0.1% on test splits of AdVQA respectively.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">AdVQA human-adversarial question collection and dataset statistics.<span id="S2.T2.4.2.1" class="ltx_text ltx_font_medium"> The model error rate is the percentage of examples where the submitted questions fooled the model (either as claimed during question collection, or after validation). We also report the number of attempts (tries) needed before a validated model-fooling example was found, and how long this took, in seconds.</span></span></figcaption>
<table id="S2.T2.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.5.1.1" class="ltx_tr">
<td id="S2.T2.5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.5.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S2.T2.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S2.T2.5.1.1.2.1" class="ltx_text ltx_font_bold">Model error rate</span></td>
<td id="S2.T2.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.5.1.1.3.1" class="ltx_text ltx_font_bold">Tries</span></td>
<td id="S2.T2.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.5.1.1.4.1" class="ltx_text ltx_font_bold">Time in sec</span></td>
</tr>
<tr id="S2.T2.5.2.2" class="ltx_tr">
<td id="S2.T2.5.2.2.1" class="ltx_td"></td>
<td id="S2.T2.5.2.2.2" class="ltx_td ltx_align_center">claimed</td>
<td id="S2.T2.5.2.2.3" class="ltx_td ltx_align_center">validated</td>
<td id="S2.T2.5.2.2.4" class="ltx_td ltx_align_center" colspan="2">mean/median per ex.</td>
</tr>
<tr id="S2.T2.5.3.3" class="ltx_tr">
<td id="S2.T2.5.3.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">75,669</td>
<td id="S2.T2.5.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">43.71% (33,074)</td>
<td id="S2.T2.5.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">37.69% (28,522)</td>
<td id="S2.T2.5.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">5.25/4.0</td>
<td id="S2.T2.5.3.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">226.17/166.66</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Interestingly, the “claimed” model error rate, based on self-reported model-fooling questions, is similar to that of text-based adversarial question answering, which was at 44.0% for RoBERTa and 47.1% for BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The validated error rate for our task is much higher than e.g. for ANLI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, which was 9.52% overall, suggesting that fooling models is a lot easier for VQA than for NLI. It appears to be not too difficult for annotators to find examples that the model fails to predict correctly.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Evaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baselines and Methods</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We analyze the performance of several baselines and a wide variety of state-of-the-art VQA models on the AdVQA dataset. We evaluate the same set of models on VQA v2 dataset as a direct comparison.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Prior baselines.</span> We start by evaluating two prior baselines: answering based on the overall majority answer, or the per answer type majority answer in the validation dataset. We use the same answer types as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The overall majority answer in AdVQA is <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">no</span>. The majority answer is <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">no</span> for “yes/no”, <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_italic">2</span> for “numbers” and <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_italic">unanswerable</span> for “others”. See Section <a href="#S4" title="4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for more details on answer types.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Unimodal baselines.</span> Next, we evaluate two unimodal pretrained models: i) ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> pretrained on Imagenet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>; and ii) BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, both finetuned on the task using the visual (image) or textual (question) modality respectively, while ignoring the other modality. We observe that the unimodal text model performs better than unimodal image for both VQA and AdVQA.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Multimodal methods.</span> We evaluate two varieties of multimodal models: i) unimodal pretrained and ii) multimodal pretrained. In the unimodal pretrained category we explore MMBT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, MoViE+MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and UniT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. These models are initialized from unimodal pretrained weights: BERT pretraining for MMBT; Imagenet + Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> detection pretraining for MoViE+MCAN; and Imagenet + COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> detection pretraining for the image encoder part and BERT pretraining for the text encoder part in UniT. In the multimodal pretrained category, we explore VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, VilT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, UNITER<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and VILLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. These models are first initialized from pretrained unimodal models and then pretrained on different multimodal datasets on proxy self-supervised/semi-supervised tasks before finetuning on VQA. VisualBERT is pretrained on COCO Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; VilBERT is pretrained on Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>; ViLT, UNITER and VILLA models are pretrained on COCO Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> + Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> + Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> + SBU Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> datasets.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">We find that multimodal models in general perform better than unimodal models, as we would expect given that both modalities are important for the VQA task.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Multimodal OCR methods.</span> As we will see in Section <a href="#S4" title="4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, a significant amount of questions in AdVQA can be answered using scene text.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">We test a state-of-the-art TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> model, M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> on AdVQA. We evaluate two versions: (i) trained on VQA 2.0 dataset, and (ii) trained on TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and STVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In both cases, we use OCR tokens extracted using the Rosetta OCR system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We also use the same answer vocabulary used by other models for fair comparison.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.10.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S3.T3.11.2" class="ltx_text ltx_font_bold" style="font-size:113%;">Model performance on VQA v2 and AdVQA<span id="S3.T3.11.2.1" class="ltx_text ltx_font_medium"> val and test sets. * indicates that this model architecture (but not this model instance) was used in the data collection loop.</span></span></figcaption>
<table id="S3.T3.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.5.6.1" class="ltx_tr">
<td id="S3.T3.5.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2"><span id="S3.T3.5.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></td>
<td id="S3.T3.5.6.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.5.6.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></td>
<td id="S3.T3.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T3.5.6.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></td>
<td id="S3.T3.5.6.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.5.6.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></td>
<td id="S3.T3.5.6.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.5.6.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></td>
</tr>
<tr id="S3.T3.5.7.2" class="ltx_tr">
<td id="S3.T3.5.7.2.1" class="ltx_td"></td>
<td id="S3.T3.5.7.2.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T3.5.7.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.5.7.2.3.1" class="ltx_text" style="font-size:80%;">test-dev</span></td>
<td id="S3.T3.5.7.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.5.7.2.4.1" class="ltx_text" style="font-size:80%;">test</span></td>
<td id="S3.T3.5.7.2.5" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T3.5.7.2.5.1" class="ltx_text" style="font-size:80%;">val</span></td>
</tr>
<tr id="S3.T3.5.8.3" class="ltx_tr">
<td id="S3.T3.5.8.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><em id="S3.T3.5.8.3.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Human performance</em></td>
<td id="S3.T3.5.8.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.8.3.2.1" class="ltx_text" style="font-size:80%;">80.78</span></td>
<td id="S3.T3.5.8.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.8.3.3.1" class="ltx_text" style="font-size:80%;">91.18</span></td>
<td id="S3.T3.5.8.3.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.8.3.4.1" class="ltx_text" style="font-size:80%;">84.73</span></td>
<td id="S3.T3.5.8.3.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.8.3.5.1" class="ltx_text" style="font-size:80%;">87.53</span></td>
</tr>
<tr id="S3.T3.5.9.4" class="ltx_tr">
<td id="S3.T3.5.9.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><em id="S3.T3.5.9.4.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Majority answer (overall)</em></td>
<td id="S3.T3.5.9.4.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.9.4.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T3.5.9.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.9.4.3.1" class="ltx_text" style="font-size:80%;">13.38</span></td>
<td id="S3.T3.5.9.4.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.9.4.4.1" class="ltx_text" style="font-size:80%;">24.67</span></td>
<td id="S3.T3.5.9.4.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.9.4.5.1" class="ltx_text" style="font-size:80%;">11.65</span></td>
</tr>
<tr id="S3.T3.5.10.5" class="ltx_tr">
<td id="S3.T3.5.10.5.1" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><em id="S3.T3.5.10.5.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Majority answer (per answer type)</em></td>
<td id="S3.T3.5.10.5.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.10.5.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T3.5.10.5.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.10.5.3.1" class="ltx_text" style="font-size:80%;">27.39</span></td>
<td id="S3.T3.5.10.5.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.10.5.4.1" class="ltx_text" style="font-size:80%;">31.01</span></td>
<td id="S3.T3.5.10.5.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.10.5.5.1" class="ltx_text" style="font-size:80%;">29.24</span></td>
</tr>
<tr id="S3.T3.5.11.6" class="ltx_tr">
<td id="S3.T3.5.11.6.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.5.11.6.1.1" class="ltx_text" style="font-size:80%;">Model in loop</span></td>
<td id="S3.T3.5.11.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T3.5.11.6.2.1" class="ltx_text" style="font-size:80%;">MoViE+MCAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.11.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S3.T3.5.11.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.11.6.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.11.6.3.1" class="ltx_text" style="font-size:80%;">73.56</span></td>
<td id="S3.T3.5.11.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.11.6.4.1" class="ltx_text" style="font-size:80%;">10.33</span></td>
<td id="S3.T3.5.11.6.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.11.6.5.1" class="ltx_text" style="font-size:80%;">73.51</span></td>
<td id="S3.T3.5.11.6.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.11.6.6.1" class="ltx_text" style="font-size:80%;">10.24</span></td>
</tr>
<tr id="S3.T3.5.12.7" class="ltx_tr">
<td id="S3.T3.5.12.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T3.5.12.7.1.1" class="ltx_text" style="font-size:80%;">Unimodal</span></td>
<td id="S3.T3.5.12.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T3.5.12.7.2.1" class="ltx_text" style="font-size:80%;">ResNet-152 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.12.7.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.T3.5.12.7.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.12.7.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.12.7.3.1" class="ltx_text" style="font-size:80%;">26.37</span></td>
<td id="S3.T3.5.12.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.12.7.4.1" class="ltx_text" style="font-size:80%;">10.85</span></td>
<td id="S3.T3.5.12.7.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.12.7.5.1" class="ltx_text" style="font-size:80%;">24.82</span></td>
<td id="S3.T3.5.12.7.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.12.7.6.1" class="ltx_text" style="font-size:80%;">11.22</span></td>
</tr>
<tr id="S3.T3.5.13.8" class="ltx_tr">
<td id="S3.T3.5.13.8.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.13.8.1.1" class="ltx_text" style="font-size:80%;">BERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.13.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S3.T3.5.13.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.13.8.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.13.8.2.1" class="ltx_text" style="font-size:80%;">39.47</span></td>
<td id="S3.T3.5.13.8.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.13.8.3.1" class="ltx_text" style="font-size:80%;">26.9</span></td>
<td id="S3.T3.5.13.8.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.13.8.4.1" class="ltx_text" style="font-size:80%;">39.40</span></td>
<td id="S3.T3.5.13.8.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.13.8.5.1" class="ltx_text" style="font-size:80%;">23.81</span></td>
</tr>
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S3.T3.1.1.2.1" class="ltx_text" style="font-size:80%;">
<span id="S3.T3.1.1.2.1.1" class="ltx_inline-block">
<span id="S3.T3.1.1.2.1.1.1" class="ltx_p">Multimodal</span>
<span id="S3.T3.1.1.2.1.1.2" class="ltx_p">(unimodal pretrain)</span>
</span></span></td>
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">MoViE+MCAN</span><sup id="S3.T3.1.1.1.2" class="ltx_sup"><span id="S3.T3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">∗</span></sup><span id="S3.T3.1.1.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S3.T3.1.1.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.1.1.3.1" class="ltx_text" style="font-size:80%;">71.36</span></td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">26.64</span></td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">71.31</span></td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.1.1.6.1" class="ltx_text" style="font-size:80%;">26.37</span></td>
</tr>
<tr id="S3.T3.5.14.9" class="ltx_tr">
<td id="S3.T3.5.14.9.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.14.9.1.1" class="ltx_text" style="font-size:80%;">MMBT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.14.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.T3.5.14.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.14.9.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.14.9.2.1" class="ltx_text" style="font-size:80%;">58.00</span></td>
<td id="S3.T3.5.14.9.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.14.9.3.1" class="ltx_text" style="font-size:80%;">26.70</span></td>
<td id="S3.T3.5.14.9.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.14.9.4.1" class="ltx_text" style="font-size:80%;">57.32</span></td>
<td id="S3.T3.5.14.9.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.14.9.5.1" class="ltx_text" style="font-size:80%;">25.78</span></td>
</tr>
<tr id="S3.T3.5.15.10" class="ltx_tr">
<td id="S3.T3.5.15.10.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.15.10.1.1" class="ltx_text" style="font-size:80%;">UniT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.15.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.T3.5.15.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.15.10.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.15.10.2.1" class="ltx_text" style="font-size:80%;">64.36</span></td>
<td id="S3.T3.5.15.10.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.15.10.3.1" class="ltx_text" style="font-size:80%;">28.15</span></td>
<td id="S3.T3.5.15.10.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.15.10.4.1" class="ltx_text" style="font-size:80%;">64.32</span></td>
<td id="S3.T3.5.15.10.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.15.10.5.1" class="ltx_text" style="font-size:80%;">27.55</span></td>
</tr>
<tr id="S3.T3.5.16.11" class="ltx_tr">
<td id="S3.T3.5.16.11.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span id="S3.T3.5.16.11.1.1" class="ltx_text" style="font-size:80%;">
<span id="S3.T3.5.16.11.1.1.1" class="ltx_inline-block">
<span id="S3.T3.5.16.11.1.1.1.1" class="ltx_p">Multimodal</span>
<span id="S3.T3.5.16.11.1.1.1.2" class="ltx_p">(multimodal pretrain)</span>
</span></span></td>
<td id="S3.T3.5.16.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T3.5.16.11.2.1" class="ltx_text" style="font-size:80%;">VisualBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.16.11.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S3.T3.5.16.11.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.16.11.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.16.11.3.1" class="ltx_text" style="font-size:80%;">70.37</span></td>
<td id="S3.T3.5.16.11.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.16.11.4.1" class="ltx_text" style="font-size:80%;">28.70</span></td>
<td id="S3.T3.5.16.11.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.16.11.5.1" class="ltx_text" style="font-size:80%;">70.05</span></td>
<td id="S3.T3.5.16.11.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.16.11.6.1" class="ltx_text" style="font-size:80%;">28.03</span></td>
</tr>
<tr id="S3.T3.5.17.12" class="ltx_tr">
<td id="S3.T3.5.17.12.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.17.12.1.1" class="ltx_text" style="font-size:80%;">ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.17.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.T3.5.17.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.17.12.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.17.12.2.1" class="ltx_text" style="font-size:80%;">69.42</span></td>
<td id="S3.T3.5.17.12.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.17.12.3.1" class="ltx_text" style="font-size:80%;">27.36</span></td>
<td id="S3.T3.5.17.12.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.17.12.4.1" class="ltx_text" style="font-size:80%;">69.27</span></td>
<td id="S3.T3.5.17.12.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.17.12.5.1" class="ltx_text" style="font-size:80%;">27.36</span></td>
</tr>
<tr id="S3.T3.5.18.13" class="ltx_tr">
<td id="S3.T3.5.18.13.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.18.13.1.1" class="ltx_text" style="font-size:80%;">ViLT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.18.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S3.T3.5.18.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.18.13.2" class="ltx_td ltx_align_right"><span id="S3.T3.5.18.13.2.1" class="ltx_text" style="font-size:80%;">64.52</span></td>
<td id="S3.T3.5.18.13.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.18.13.3.1" class="ltx_text" style="font-size:80%;">27.11</span></td>
<td id="S3.T3.5.18.13.4" class="ltx_td ltx_align_right"><span id="S3.T3.5.18.13.4.1" class="ltx_text" style="font-size:80%;">65.43</span></td>
<td id="S3.T3.5.18.13.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.18.13.5.1" class="ltx_text" style="font-size:80%;">27.19</span></td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.2.2.1.1" class="ltx_text" style="font-size:80%;">UNITER</span><sub id="S3.T3.2.2.1.2" class="ltx_sub"><span id="S3.T3.2.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Base</span></sub><span id="S3.T3.2.2.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.2.2.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.T3.2.2.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_right"><span id="S3.T3.2.2.2.1" class="ltx_text" style="font-size:80%;">71.87</span></td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.2.2.3.1" class="ltx_text" style="font-size:80%;">25.16</span></td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_right"><span id="S3.T3.2.2.4.1" class="ltx_text" style="font-size:80%;">70.50</span></td>
<td id="S3.T3.2.2.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.2.2.5.1" class="ltx_text" style="font-size:80%;">25.20</span></td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.3.3.1.1" class="ltx_text" style="font-size:80%;">UNITER</span><sub id="S3.T3.3.3.1.2" class="ltx_sub"><span id="S3.T3.3.3.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Large</span></sub><span id="S3.T3.3.3.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.3.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.T3.3.3.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_right"><span id="S3.T3.3.3.2.1" class="ltx_text" style="font-size:80%;">73.57</span></td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">26.94</span></td>
<td id="S3.T3.3.3.4" class="ltx_td ltx_align_right"><span id="S3.T3.3.3.4.1" class="ltx_text" style="font-size:80%;">72.71</span></td>
<td id="S3.T3.3.3.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.3.3.5.1" class="ltx_text" style="font-size:80%;">28.03</span></td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.4.4.1.1" class="ltx_text" style="font-size:80%;">VILLA</span><sub id="S3.T3.4.4.1.2" class="ltx_sub"><span id="S3.T3.4.4.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Base</span></sub><span id="S3.T3.4.4.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.4.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T3.4.4.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_right"><span id="S3.T3.4.4.2.1" class="ltx_text" style="font-size:80%;">70.94</span></td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.4.4.3.1" class="ltx_text" style="font-size:80%;">25.14</span></td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_right"><span id="S3.T3.4.4.4.1" class="ltx_text" style="font-size:80%;">69.50</span></td>
<td id="S3.T3.4.4.5" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.4.4.5.1" class="ltx_text" style="font-size:80%;">25.17</span></td>
</tr>
<tr id="S3.T3.5.5" class="ltx_tr">
<td id="S3.T3.5.5.2" class="ltx_td"></td>
<td id="S3.T3.5.5.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S3.T3.5.5.1.1" class="ltx_text" style="font-size:80%;">VILLA</span><sub id="S3.T3.5.5.1.2" class="ltx_sub"><span id="S3.T3.5.5.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Large</span></sub><span id="S3.T3.5.5.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.5.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T3.5.5.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.5.3" class="ltx_td ltx_align_right"><span id="S3.T3.5.5.3.1" class="ltx_text" style="font-size:80%;">72.29</span></td>
<td id="S3.T3.5.5.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T3.5.5.4.1" class="ltx_text" style="font-size:80%;">25.79</span></td>
<td id="S3.T3.5.5.5" class="ltx_td ltx_align_right"><span id="S3.T3.5.5.5.1" class="ltx_text" style="font-size:80%;">71.40</span></td>
<td id="S3.T3.5.5.6" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S3.T3.5.5.6.1" class="ltx_text" style="font-size:80%;">26.18</span></td>
</tr>
<tr id="S3.T3.5.19.14" class="ltx_tr">
<td id="S3.T3.5.19.14.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S3.T3.5.19.14.1.1" class="ltx_text" style="font-size:80%;">
<span id="S3.T3.5.19.14.1.1.1" class="ltx_inline-block">
<span id="S3.T3.5.19.14.1.1.1.1" class="ltx_p">Multimodal</span>
<span id="S3.T3.5.19.14.1.1.1.2" class="ltx_p">(unimodal pretrain + OCR)</span>
</span></span></td>
<td id="S3.T3.5.19.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T3.5.19.14.2.1" class="ltx_text" style="font-size:80%;">M4C (TextVQA+STVQA) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.19.14.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.T3.5.19.14.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.19.14.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.19.14.3.1" class="ltx_text" style="font-size:80%;">32.89</span></td>
<td id="S3.T3.5.19.14.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T3.5.19.14.4.1" class="ltx_text" style="font-size:80%;">28.86</span></td>
<td id="S3.T3.5.19.14.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T3.5.19.14.5.1" class="ltx_text" style="font-size:80%;">31.44</span></td>
<td id="S3.T3.5.19.14.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="S3.T3.5.19.14.6.1" class="ltx_text" style="font-size:80%;">29.08</span></td>
</tr>
<tr id="S3.T3.5.20.15" class="ltx_tr">
<td id="S3.T3.5.20.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T3.5.20.15.1.1" class="ltx_text" style="font-size:80%;">M4C (VQA v2 train set) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.5.20.15.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.T3.5.20.15.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.5.20.15.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T3.5.20.15.2.1" class="ltx_text" style="font-size:80%;">67.66</span></td>
<td id="S3.T3.5.20.15.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r"><span id="S3.T3.5.20.15.3.1" class="ltx_text" style="font-size:80%;">33.52</span></td>
<td id="S3.T3.5.20.15.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T3.5.20.15.4.1" class="ltx_text" style="font-size:80%;">66.21</span></td>
<td id="S3.T3.5.20.15.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb"><span id="S3.T3.5.20.15.5.1" class="ltx_text" style="font-size:80%;">33.33</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Discussion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our model evaluation results show some surprising findings. We discuss our observations and hypotheses around these findings in this section.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Baseline comparison.</span> Surprisingly, most multimodal models are unable to outperform a unimodal baseline (BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) and simple majority answer (per answer type) prior baseline which only predicts the most-frequently occurring word for the question’s category (<span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">no</span> for “yes/no”, <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">2</span> for “numbers” and <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">unanswerable</span> for “others”). A detailed breakdown of category-wise performance provided in Table <a href="#S3.T4" title="Table 4 ‣ 3.2 Discussion ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> suggests that even though “yes/no” questions are often considered to be easy, the baseline multimodal models we evaluate are unable to beat the majority answer prediction. We also observe varied but close to majority answer performance in the “numbers” category, which is a question type known to be difficult for VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In contrast, the models outperform the majority answer in the “others” category even though “unanswerable” (the majority answer) is not in the answer vocabulary used. Interestingly, M4C outperforms all on “numbers” and “others” categories possibly thanks to its text reading capabilities. These trends showcase the difficulty of AdVQA and suggest that we have a long way to go still, given that we are as yet apparently unable to beat such simple baselines.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Model rankings.</span> First, M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> performs the best among the evaluated models. Interestingly, it is smaller than many of the more sophisticated model architectures that score higher on VQA. This is probably due to the importance of the ability to read and reason about text in the image for answering some AdVQA questions. Second, among models that can’t read text, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is the best model, despite (or perhaps because of?) it being simple and having fewer parameters compared to other models. Third, the adversarially-trained VILLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> model performs surprisingly poorly. While it may be more robust to statistically-generated adversarial examples, it appears to be less so against human-adversarial examples. Fourth, we find that all of these models perform poorly compared to humans, while model performance on VQA is much closer to that of humans.</p>
</div>
<figure id="S3.T4" class="ltx_table ltx_align_floatright">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.6.1.1" class="ltx_text" style="font-size:113%;">Table 4</span>: </span><span id="S3.T4.7.2" class="ltx_text ltx_font_bold" style="font-size:113%;">The category-wise performance of VQA models.<span id="S3.T4.7.2.1" class="ltx_text ltx_font_medium"> The state-of-the-art VQA models perform very close to the majority class prior, illustrating the challenge and difficulty of AdVQA.</span></span></figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<th id="S3.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;" rowspan="2"><span id="S3.T4.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;" colspan="3"><span id="S3.T4.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question Type</span></td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.3.2.1.1" class="ltx_text" style="font-size:80%;">yes/no</span></td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.3.2.2.1" class="ltx_text" style="font-size:80%;">numbers</span></td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.3.2.3.1" class="ltx_text" style="font-size:80%;">others</span></td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<th id="S3.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Majority Class</span></th>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.4.3.2.1" class="ltx_text" style="font-size:80%;">62.28</span></td>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.4.3.3.1" class="ltx_text" style="font-size:80%;">31.11</span></td>
<td id="S3.T4.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.4.3.4.1" class="ltx_text" style="font-size:80%;">9.29</span></td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<th id="S3.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.5.4.1.1" class="ltx_text" style="font-size:80%;">ResNet-152</span></th>
<td id="S3.T4.1.5.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.5.4.2.1" class="ltx_text" style="font-size:80%;">62.81</span></td>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.5.4.3.1" class="ltx_text" style="font-size:80%;">0.18</span></td>
<td id="S3.T4.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.5.4.4.1" class="ltx_text" style="font-size:80%;">0.51</span></td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<th id="S3.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.6.5.1.1" class="ltx_text" style="font-size:80%;">BERT</span></th>
<td id="S3.T4.1.6.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.6.5.2.1" class="ltx_text" style="font-size:80%;">67.58</span></td>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.6.5.3.1" class="ltx_text" style="font-size:80%;">26.87</span></td>
<td id="S3.T4.1.6.5.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.6.5.4.1" class="ltx_text" style="font-size:80%;">9.25</span></td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<th id="S3.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.7.6.1.1" class="ltx_text" style="font-size:80%;">VisualBERT</span></th>
<td id="S3.T4.1.7.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.7.6.2.1" class="ltx_text" style="font-size:80%;">55.51</span></td>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.7.6.3.1" class="ltx_text" style="font-size:80%;">32.29</span></td>
<td id="S3.T4.1.7.6.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.7.6.4.1" class="ltx_text" style="font-size:80%;">17.66</span></td>
</tr>
<tr id="S3.T4.1.8.7" class="ltx_tr">
<th id="S3.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.8.7.1.1" class="ltx_text" style="font-size:80%;">ViLBERT</span></th>
<td id="S3.T4.1.8.7.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.8.7.2.1" class="ltx_text" style="font-size:80%;">55.58</span></td>
<td id="S3.T4.1.8.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.8.7.3.1" class="ltx_text" style="font-size:80%;">29.49</span></td>
<td id="S3.T4.1.8.7.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.8.7.4.1" class="ltx_text" style="font-size:80%;">16.67</span></td>
</tr>
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<span id="S3.T4.1.1.1.1" class="ltx_text" style="font-size:80%;">MoViE+MCAN</span><sup id="S3.T4.1.1.1.2" class="ltx_sup"><span id="S3.T4.1.1.1.2.1" class="ltx_text" style="font-size:80%;">∗</span></sup>
</th>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.1.2.1" class="ltx_text" style="font-size:80%;">52.74</span></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.1.3.1" class="ltx_text" style="font-size:80%;">33.62</span></td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.1.4.1" class="ltx_text" style="font-size:80%;">14.56</span></td>
</tr>
<tr id="S3.T4.1.9.8" class="ltx_tr">
<th id="S3.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.9.8.1.1" class="ltx_text" style="font-size:80%;">M4C (VQA2)</span></th>
<td id="S3.T4.1.9.8.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.9.8.2.1" class="ltx_text" style="font-size:80%;">56.67</span></td>
<td id="S3.T4.1.9.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.9.8.3.1" class="ltx_text" style="font-size:80%;">38.04</span></td>
<td id="S3.T4.1.9.8.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S3.T4.1.9.8.4.1" class="ltx_text" style="font-size:80%;">22.73</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Human performance.</span> Another surprising finding is that inter-human agreement is higher on the AdVQA dataset than on VQA. This could be due to different data annotation procedures, requirements on annotators or just statistical noise. Human-adversarial questions may also be more specific due to annotators having to make crisp decisions about the model failing or not.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Model-in-the-loop’s performance.</span> Interestingly, the MoViE+MCAN model that was <em id="S3.SS2.p5.1.2" class="ltx_emph ltx_font_italic">not</em> used in the loop and trained with a different seed, performs very similarly to other models. This suggests that to some extent, annotators overfit to the model instance. An alternative explanation is that model selection for all evaluated models was done on the AdVQA validation set, which was (obviously) not possible for the model in the loop used to construct the dataset. In Adversarial NLI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, the entire model class of the in-the-loop model was affected. Note however that all VQA models perform poorly on AdVQA, suggesting that the examples are by and large representative of shortcomings of VQA techniques overall, and not of an individual model instance or class.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.4" class="ltx_p"><span id="S3.SS2.p6.4.1" class="ltx_text ltx_font_bold">Train vs Test Distribution.</span> We experiment with finetuning VisualBERT and VilBERT further on the AdVQA <span id="S3.SS2.p6.4.2" class="ltx_text ltx_font_italic">val</span> set, finding that this improved test accuracy from <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="28.70" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mn id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">28.70</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><cn type="float" id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">28.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">28.70</annotation></semantics></math> to <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="33.10" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mn id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">33.10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><cn type="float" id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">33.10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">33.10</annotation></semantics></math> and <math id="S3.SS2.p6.3.m3.1" class="ltx_Math" alttext="27.36" display="inline"><semantics id="S3.SS2.p6.3.m3.1a"><mn id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml">27.36</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><cn type="float" id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">27.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">27.36</annotation></semantics></math> to <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="32.85" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><mn id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">32.85</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><cn type="float" id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">32.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">32.85</annotation></semantics></math>, respectively, suggesting a difference in the VQA and AdVQA distributions, as we would expect.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We train all the models in Table <a href="#S3.T3" title="Table 3 ‣ 3.1 Baselines and Methods ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> on the VQA <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">train</span> + <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">val</span> split excluding the COCO 2017 validation images. We also add questions from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> corresponding to the images that overlap with the VQA training set to our training split. The VQA <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">val</span> set in our results contains all questions associated with the images in the COCO 2017 validation split. This is also consistent with the training split used for the model in the loop. We collect our AdVQA validation set on the images from the COCO 2017 <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">val</span> split, ensuring there is no overlap with the training set images. We choose the best checkpoint for each model by validating on the AdVQA validation set.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For all our experiments, we use the standard architecture for the models as provided by their authors. For the models that are initialized from pretrained models (whether unimodal or multimodal pretrained), we use off-the-shelf pretrained model weights and then finetune on our training set. We do not do any hyperparamter search for these models and use the best hyperparams as provided by respective authors. We finetune each model with three different seeds and report average accuracy.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We run most of our experiments on NVIDIA V100 GPUs. The maximum number of GPUs used for training is 8 for larger models. Maximum training time is 2 days. More details about hyperparameters, number of devices and time for training each model are provided in the supplementary material.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/x2.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/x3.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/x4.png" id="S3.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Quantitative statistics for AdVQA val set questions and answers<span id="S3.F4.4.2.1" class="ltx_text ltx_font_medium"> showing longer question length, more diversity and better human-agreement. (a) Percentage of questions solvable with a particular question length in AdVQA val set.
We see that the average question length (7.82) in AdVQA is higher than the prior work. (b) Percentage of questions solvable when a particular number of top k most occurring answers are selected from each dataset. The plot suggests that AdVQA has a much more diverse answer vocabulary compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> but also not quite as challenging as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. (c) Cumulative human agreement scores. We see that human agreement is better and higher on AdVQA compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Questions.</span> We first analyze the question diversity in AdVQA and compare them with popular VQA datasets. AdVQA contains 28,522 questions (5,123 in val and 23,399 in test) each with 10 answers.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Fig. <a href="#S3.F4.sf1" title="In Figure 4 ‣ 3.3 Training Details ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> shows the distribution of question length compared with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. The average question length in AdVQA is 7.8, which is higher than the VQA v2 (6.3), and TextVQA (7.2). The workers often need to get creative and more specific to fool the model-in-the-loop, leading to somewhat longer questions (<em id="S4.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p2.1.2" class="ltx_text"></span> specifying a particular person to ask about). Fig. <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a> shows the top 15 most occurring questions from the val set, showcasing that questions involving text (<em id="S4.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p2.1.4" class="ltx_text"></span> time, sign) and counting (<em id="S4.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p2.1.6" class="ltx_text"></span> how many) are major failures for current state-of-the-art VQA models, corroborating the findings of prior work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a sunburst plot for the first 4 words in the AdVQA val set questions. We can observe that questions in AdVQA often start with “what” or “how” frequently inquiring about aspects like “many” (count) and “brand” (text).</p>
</div>
<figure id="S4.F5" class="ltx_figure ltx_align_floatright"><img src="/html/2106.02280/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_img_square" width="576" height="576" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Sunburst distribution for the first four words in the AdVQA val set questions.<span id="S4.F5.4.2.1" class="ltx_text ltx_font_medium"> Most questions start with “what” or “how”.</span></span></figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Answers.</span> In AdVQA val set, only 66.6% (3,856) answers occur more than twice compared to 94.8% in VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, suggesting that the diversity of possible answers is much larger in AdVQA compared to VQA v2. Fig. <a href="#S3.F4.sf2" title="In Figure 4 ‣ 3.3 Training Details ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> shows the percentage of questions that are solvable with a vocabulary of top k most occurring answers. We observe that more questions in VQA v2 while fewer question in TextVQA are solvable with smaller vocabulary compare to AdVQA. This suggests that AdVQA is more diverse and difficult than VQA v2 but not as narrowly focused as TextVQA, making it a great testbed for future VQA models. We also showcase more qualitative examples in our supplementary to demonstrate that AdVQA’s diversity doesn’t lead to unnatural questions. Fig. <a href="#S3.F4.sf3" title="In Figure 4 ‣ 3.3 Training Details ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(c)</span></a> shows the cumulative percentage of questions where more than a particular number of annotators agree with each other. Fig. <a href="#S4.F6.sf2" title="In Figure 6 ‣ 4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a> shows the top 500 most occurring answers in the AdVQA val set; starting from very common answers such as “no” and counts (“1”, “9”), to gradually more specific and targeted answers like “21”, “teddy bear” and “center”.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/x6.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/x7.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative statistics for AdVQA val set questions and answers<span id="S4.F6.6.2.1" class="ltx_text ltx_font_medium"> showing top questions, answers and word distribution. (a) </span>Top 15 most occurring questions in AdVQA val.<span id="S4.F6.6.2.2" class="ltx_text ltx_font_medium"> Most of the top questions start with “what”. (b) </span>Total occurrences for 500 most common answers
<span id="S4.F6.6.2.3" class="ltx_text ltx_font_medium">
with markers for particular ranks.</span></span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Answer Vocabulary.</span> To showcase the challenge of AdVQA, we take the original VQA v2 vocabulary used in the Pythia v0.3 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. We find that 77.2% of the AdVQA val set’s questions are answerable using this vocabulary, suggesting that a model with powerful reasoning capability won’t be heavily limited by vocabulary on AdVQA. But, we also note that for high performance on AdVQA, a model will need to understand and reason about rare concepts, as 50.9% of the answers in AdVQA val and test sets don’t occur in VQA v2 train set.</p>
</div>
<figure id="S4.T5" class="ltx_table ltx_align_floatright">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.5.1.1" class="ltx_text" style="font-size:113%;">Table 5</span>: </span><span id="S4.T5.6.2" class="ltx_text ltx_font_bold" style="font-size:113%;">The category-wise distribution of answers.<span id="S4.T5.6.2.1" class="ltx_text ltx_font_medium"> Compared to VQA, AdVQA contains more “number”based and lesser “yes/no” questions supporting the prior work’s observations around failure of VQA models to count and read text.</span></span></figcaption>
<table id="S4.T5.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.7.1.1" class="ltx_tr">
<th id="S4.T5.7.1.1.1" class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;" rowspan="2"><span id="S4.T5.7.1.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S4.T5.7.1.1.1.1.1" class="ltx_inline-block">
<span id="S4.T5.7.1.1.1.1.1.1" class="ltx_p"><span id="S4.T5.7.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
<span id="S4.T5.7.1.1.1.1.1.2" class="ltx_p"><span id="S4.T5.7.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Type</span></span>
</span></span></th>
<th id="S4.T5.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></th>
<th id="S4.T5.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></th>
<th id="S4.T5.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></th>
<th id="S4.T5.7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></th>
</tr>
<tr id="S4.T5.7.2.2" class="ltx_tr">
<th id="S4.T5.7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.2.2.1.1" class="ltx_text" style="font-size:80%;">test-dev</span></th>
<th id="S4.T5.7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.2.2.2.1" class="ltx_text" style="font-size:80%;">test</span></th>
<th id="S4.T5.7.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.2pt;padding-right:3.2pt;" colspan="2"><span id="S4.T5.7.2.2.3.1" class="ltx_text" style="font-size:80%;">val</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.7.3.1" class="ltx_tr">
<th id="S4.T5.7.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.3.1.1.1" class="ltx_text" style="font-size:80%;">yes/no</span></th>
<td id="S4.T5.7.3.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.3.1.2.1" class="ltx_text" style="font-size:80%;">38.36</span></td>
<td id="S4.T5.7.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.3.1.3.1" class="ltx_text" style="font-size:80%;">17.89</span></td>
<td id="S4.T5.7.3.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.3.1.4.1" class="ltx_text" style="font-size:80%;">37.70</span></td>
<td id="S4.T5.7.3.1.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.3.1.5.1" class="ltx_text" style="font-size:80%;">17.90</span></td>
</tr>
<tr id="S4.T5.7.4.2" class="ltx_tr">
<th id="S4.T5.7.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.4.2.1.1" class="ltx_text" style="font-size:80%;">number</span></th>
<td id="S4.T5.7.4.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.4.2.2.1" class="ltx_text" style="font-size:80%;">12.31</span></td>
<td id="S4.T5.7.4.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.4.2.3.1" class="ltx_text" style="font-size:80%;">41.91</span></td>
<td id="S4.T5.7.4.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.4.2.4.1" class="ltx_text" style="font-size:80%;">11.48</span></td>
<td id="S4.T5.7.4.2.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.4.2.5.1" class="ltx_text" style="font-size:80%;">31.80</span></td>
</tr>
<tr id="S4.T5.7.5.3" class="ltx_tr">
<th id="S4.T5.7.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.5.3.1.1" class="ltx_text" style="font-size:80%;">others</span></th>
<td id="S4.T5.7.5.3.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.5.3.2.1" class="ltx_text" style="font-size:80%;">49.33</span></td>
<td id="S4.T5.7.5.3.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.5.3.3.1" class="ltx_text" style="font-size:80%;">40.20</span></td>
<td id="S4.T5.7.5.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.5.3.4.1" class="ltx_text" style="font-size:80%;">50.82</span></td>
<td id="S4.T5.7.5.3.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span id="S4.T5.7.5.3.5.1" class="ltx_text" style="font-size:80%;">50.30</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Question Types.</span> Table <a href="#S4.T5" title="Table 5 ‣ 4 Dataset Analysis ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the category-wise distribution for AdVQA questions compared with VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. We can observe a shift from more easy questions of the “yes/no” category in VQA v2 dataset to more difficult questions in “numbers” category (as suggested in prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>) in AdVQA.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Human Agreement.</span> In the AdVQA val set, 27.2% human annotators agree on all 10 answers while 3 or more annotators agree an answer for 97.7%, which is higher compared to 93.4% on VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, even though as discussed AdVQA contains a large number of rare concepts.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">Relationship to TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</span> To understand if the ability to read text is crucial for AdVQA, we extract Rosetta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> tokens on the AdVQA val set and determine how many questions can be answered using OCR tokens at an edit distance of 1 to account for OCR errors. We find that 15.4% of the AdVQA val questions are solvable using OCR tokens suggesting that ability to read scene text would play a crucial role in solving AdVQA.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion, Limitations &amp; Outlook</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduced the AdVQA dataset, a novel human-adversarial multimodal dataset designed for accelerating progress on Visual Question Answering (VQA). Current VQA datasets have started plateauing and are approaching saturation with respect to human performance. In this work, we demonstrate that the problem is far from solved.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In particular, our analysis and model evaluation results suggest that current state-of-the-art models underperform on AdVQA due to a reasoning gap incurred from a combination of (i) inability to read text; (ii) inability to count; (iii) heavy bias towards the VQA v2 question and answer distribution; (iv) external knowledge; (v) rare unseen concepts; and (vi) weak multimodal understanding. We’ve shown the gap is unlikely to be due to (i) limited answer vocabulary; (ii) language representation (BERT performance compared to other); (iii) no pretraining (UniT); or (iv) lack of adversarial training (VILLA performance).
We will provide an evaluation benchmark for community to evaluate on AdVQA and hope that AdVQA will help bridge the gap by serving as a dynamic new benchmark for visual reasoning with a large amount of headroom for further progress in the field.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In future work, it would be interesting to continue AdVQA as a dynamic benchmark. If a new state of the art emerges, those models can be put in the loop to examine how we can improve even further.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Broader Impact</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work analyzed state-of-the-art Visual Question Answering (VQA) models via a dataset constructed using a dynamic human-adversarial approach. We hope that this work can help make VQA models more robust.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">VQA datasets contain biases, both in the distribution of images represented in the datasets, as well as the corresponding questions and answers. These biases are likely amplified by the VQA models trained on these datasets. Biases studied in the context of image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> are also relevant for VQA. English is the only language represented in this work.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">VQA models can be useful for aiding visually impaired users. The commonly used VQA datasets are not representative of the needs of visually impaired users – both in terms of the distribution of images (typically consumer photographs from the web), and in terms of the questions contained in the datasets (typically asked by sighted individuals while looking at the image). In contrast, the VizWiz dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> contains questions asked by visually impaired users on images taken by them to accomplish day-to-day tasks. It would be beneficial for the community to collect larger datasets in this context to enable progress towards relevant technology. It would be important to do this with input from all relevant stakeholders, and in a responsible and privacy-preserving manner. VQA models are currently far from being accurate enough to be uesful or safe in these contexts.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We thank our collaborators in the Dynabench team for their support and discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 6077–6086, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF International Conference on Computer
Vision</span>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus
Stenetorp.

</span>
<span class="ltx_bibblock">Beat the ai: Investigating adversarial human annotation for reading
comprehension.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>,
8:662–678, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol,
Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4291–4301, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Fedor Borisyuk, Albert Gordo, and Viswanath Sivakumar.

</span>
<span class="ltx_bibblock">Rosetta: Large scale system for text detection and recognition in
images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</span>, pages 71–79, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kaylee Burns, Lisa Anne Hendricks, Trevor Darrell, and Anna Rohrbach.

</span>
<span class="ltx_bibblock">Women also snowboard: Overcoming bias in captioning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of European Conference on Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Nicholas Carlini and David Wagner.

</span>
<span class="ltx_bibblock">Towards evaluating the robustness of neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2017 ieee symposium on security and privacy (sp)</span>, pages
39–57. IEEE, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1504.00325</span>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of European Conference on Computer Vision</span>, pages
104–120. Springer, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas
Usunier.

</span>
<span class="ltx_bibblock">Parseval networks: Improving robustness to adversarial examples.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings on International Conference on Machine Learning</span>,
pages 854–863. PMLR, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2009.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston.

</span>
<span class="ltx_bibblock">Build it break it fix it for dialogue safety: Robustness from
adversarial human attack.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language
Processing</span>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language
Processing</span>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Large-scale adversarial training for vision-and-language
representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of Conference on Advances in Neural Information
Processing Systems</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.

</span>
<span class="ltx_bibblock">Explaining and harnessing adversarial examples.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations</span>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P. Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 9(8):1735–1780, 1997.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ronghang Hu and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Unit: Multimodal multitask learning with a unified transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.10772</span>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Iterative answer prediction with pointer-augmented multimodal
transformers for textvqa.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 9992–10002, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh.

</span>
<span class="ltx_bibblock">Pythia v0. 1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1807.09956</span>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C. Lawrence Zitnick, and Ross B. Girshick.

</span>
<span class="ltx_bibblock">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,
Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia,
Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp,
Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams.

</span>
<span class="ltx_bibblock">Dynabench: Rethinking benchmarking in nlp.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Conference of the North American
Chapter of the Association for Computational Linguistics</span>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, and Davide Testuggine.

</span>
<span class="ltx_bibblock">Supervised multimodal bitransformers for classifying images and text.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.02950</span>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,
Pratik Ringshia, and Davide Testuggine.

</span>
<span class="ltx_bibblock">The hateful memes challenge: Detecting hate speech in multimodal
memes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of Conference on Advances in Neural Information
Processing Systems</span>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wonjae Kim, Bokyung Son, and Ildoo Kim.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region
supervision.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.03334</span>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations</span>, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of International Journal of Computer Vision</span>,
123(1):32–73, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Meeting of the Association for
Computational Linguistics</span>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and
Haifeng Wang.

</span>
<span class="ltx_bibblock">Unimo: Towards unified-modal understanding and generation via
cross-modal contrastive learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.15409</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of European Conference on Computer Vision</span>, pages
121–137. Springer, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of European Conference on Computer Vision</span>, pages
740–755. Springer, 2014.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations</span>, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.02265</span>, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">12-in-1: Multi-task vision and language representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10437–10446, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen.

</span>
<span class="ltx_bibblock">Revisiting modulated convolutions for visual counting and beyond.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations</span>, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
Kiela.

</span>
<span class="ltx_bibblock">Adversarial nli: A new benchmark for natural language understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Meeting of the Association for
Computational Linguistics</span>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.

</span>
<span class="ltx_bibblock">Im2text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of Conference on Advances in Neural Information
Processing Systems</span>, 24:1143–1151, 2011.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proceedings of Conference on Advances in Neural Information
Processing Systems</span>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Dynasent: A dynamic benchmark for sentiment analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Meeting of the Association for
Computational Linguistics</span>, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Pedro Rodriguez, Shi Feng, Mohit Iyyer, He He, and Jordan Boyd-Graber.

</span>
<span class="ltx_bibblock">Quizbowl: The case for incremental question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.04792</span>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Meeting of the Association for
Computational Linguistics</span>, pages 2556–2565, 2018.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>, 2014.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vedanuj Goswami, Vivek Natarajan, Yu Jiang, Xinlei Chen, Meet
Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Mmf: A multimodal framework for vision and language research.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/mmf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/mmf</a>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vedanuj Goswami, and Devi Parikh.

</span>
<span class="ltx_bibblock">Are we pretraining it right? digging deeper into visio-linguistic
pretraining.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.08744</span>, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Alexander Trott, Caiming Xiong, and Richard Socher.

</span>
<span class="ltx_bibblock">Interpretable counting for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations</span>, 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Proceedings of Conference on Advances in Neural Information
Processing Systems</span>, 2017.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Learning from the worst: Dynamically generated datasets to improve
online hate detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings of Annual Meeting of the Association for
Computational Linguistics</span>, 2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 6281–6290, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Vinvl: Making visual representations matter in vision-language
models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.

</span>
<span class="ltx_bibblock">Men also like shopping: Reducing gender bias amplification using
corpus-level constraints.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language
Processing</span>, 2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Human-Adversarial Visual Question Answering<span id="p1.1.1.1" class="ltx_text ltx_font_medium">

<br class="ltx_break">(Supplementary Material)</span></span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training Details</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.2" class="ltx_p">Except UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and VILLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we trained all models using MMF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
The evaluation results with the standard deviation over three different runs with three different seeds are provided in Table <a href="#A1.T1" title="Table A.1 ‣ Appendix A Training Details ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. Below we detail the finetuning setup on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> train 2017 split evaluated on the AdVQA validation split. Unless otherwise specified, we used the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> optimizer with an initial learning rate of <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mrow id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mn id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A1.p1.1.m1.1.1.2.1" xref="A1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">−</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><minus id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></minus><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><times id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">5</cn><ci id="A1.p1.1.m1.1.1.2.3.cmml" xref="A1.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">5e-5</annotation></semantics></math>, epsilon of <math id="A1.p1.2.m2.1" class="ltx_Math" alttext="1e-8" display="inline"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mrow id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml"><mn id="A1.p1.2.m2.1.1.2.2" xref="A1.p1.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p1.2.m2.1.1.2.1" xref="A1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="A1.p1.2.m2.1.1.2.3" xref="A1.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p1.2.m2.1.1.1" xref="A1.p1.2.m2.1.1.1.cmml">−</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><minus id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></minus><apply id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2"><times id="A1.p1.2.m2.1.1.2.1.cmml" xref="A1.p1.2.m2.1.1.2.1"></times><cn type="integer" id="A1.p1.2.m2.1.1.2.2.cmml" xref="A1.p1.2.m2.1.1.2.2">1</cn><ci id="A1.p1.2.m2.1.1.2.3.cmml" xref="A1.p1.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p1.2.m2.1.1.3.cmml" xref="A1.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">1e-8</annotation></semantics></math>, cosine schedule and a warmup of 2000 steps. All jobs are trained in distributed fashion using PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>› on NVIDIA V100 GPUs.</p>
</div>
<figure id="A1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A1.T1.14.2.1" class="ltx_text" style="font-size:113%;">Table A.1</span>: </span><span id="A1.T1.2.1" class="ltx_text ltx_font_bold" style="font-size:113%;">Model performance on VQA v2 and AdVQA<span id="A1.T1.2.1.1" class="ltx_text ltx_font_medium"> val and test sets. <sup id="A1.T1.2.1.1.1" class="ltx_sup">∗</sup> indicates that this model architecture (but not this model instance) was used in the data collection loop.</span></span></figcaption>
<table id="A1.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T1.7.6.1" class="ltx_tr">
<th id="A1.T1.7.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.9pt;padding-right:2.9pt;" colspan="2"><span id="A1.T1.7.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<td id="A1.T1.7.6.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.6.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></td>
<td id="A1.T1.7.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.6.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></td>
<td id="A1.T1.7.6.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.6.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span></td>
<td id="A1.T1.7.6.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.6.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></td>
</tr>
<tr id="A1.T1.7.7.2" class="ltx_tr">
<th id="A1.T1.7.7.2.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:2.9pt;padding-right:2.9pt;"></th>
<th id="A1.T1.7.7.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"></th>
<td id="A1.T1.7.7.2.3" class="ltx_td ltx_align_center" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.7.2.3.1" class="ltx_text" style="font-size:80%;">test-dev</span></td>
<td id="A1.T1.7.7.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.7.2.4.1" class="ltx_text" style="font-size:80%;">test</span></td>
<td id="A1.T1.7.7.2.5" class="ltx_td ltx_align_center" style="padding-left:2.9pt;padding-right:2.9pt;" colspan="2"><span id="A1.T1.7.7.2.5.1" class="ltx_text" style="font-size:80%;">val</span></td>
</tr>
<tr id="A1.T1.7.8.3" class="ltx_tr">
<th id="A1.T1.7.8.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" colspan="2"><em id="A1.T1.7.8.3.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Human performance</em></th>
<td id="A1.T1.7.8.3.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.8.3.2.1" class="ltx_text" style="font-size:80%;">80.78</span></td>
<td id="A1.T1.7.8.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.8.3.3.1" class="ltx_text" style="font-size:80%;">91.18</span></td>
<td id="A1.T1.7.8.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.8.3.4.1" class="ltx_text" style="font-size:80%;">84.73</span></td>
<td id="A1.T1.7.8.3.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.8.3.5.1" class="ltx_text" style="font-size:80%;">87.53</span></td>
</tr>
<tr id="A1.T1.7.9.4" class="ltx_tr">
<th id="A1.T1.7.9.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" colspan="2"><em id="A1.T1.7.9.4.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Majority answer (overall)</em></th>
<td id="A1.T1.7.9.4.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.9.4.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="A1.T1.7.9.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.9.4.3.1" class="ltx_text" style="font-size:80%;">13.38</span></td>
<td id="A1.T1.7.9.4.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.9.4.4.1" class="ltx_text" style="font-size:80%;">24.67</span></td>
<td id="A1.T1.7.9.4.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.9.4.5.1" class="ltx_text" style="font-size:80%;">11.65</span></td>
</tr>
<tr id="A1.T1.7.10.5" class="ltx_tr">
<th id="A1.T1.7.10.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;" colspan="2"><em id="A1.T1.7.10.5.1.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">Majority answer (per answer type)</em></th>
<td id="A1.T1.7.10.5.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.10.5.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="A1.T1.7.10.5.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.10.5.3.1" class="ltx_text" style="font-size:80%;">27.39</span></td>
<td id="A1.T1.7.10.5.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.10.5.4.1" class="ltx_text" style="font-size:80%;">31.01</span></td>
<td id="A1.T1.7.10.5.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.10.5.5.1" class="ltx_text" style="font-size:80%;">29.24</span></td>
</tr>
<tr id="A1.T1.7.11.6" class="ltx_tr">
<th id="A1.T1.7.11.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.11.6.1.1" class="ltx_text" style="font-size:80%;">Model in loop</span></th>
<th id="A1.T1.7.11.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.11.6.2.1" class="ltx_text" style="font-size:80%;">MoViE+MCAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.11.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="A1.T1.7.11.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.11.6.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.11.6.3.1" class="ltx_text" style="font-size:80%;">73.56</span></td>
<td id="A1.T1.7.11.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.11.6.4.1" class="ltx_text" style="font-size:80%;">10.33</span></td>
<td id="A1.T1.7.11.6.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.11.6.5.1" class="ltx_text" style="font-size:80%;">73.51</span></td>
<td id="A1.T1.7.11.6.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.11.6.6.1" class="ltx_text" style="font-size:80%;">10.24</span></td>
</tr>
<tr id="A1.T1.7.12.7" class="ltx_tr">
<th id="A1.T1.7.12.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" rowspan="2"><span id="A1.T1.7.12.7.1.1" class="ltx_text" style="font-size:80%;">Unimodal</span></th>
<th id="A1.T1.7.12.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.12.7.2.1" class="ltx_text" style="font-size:80%;">ResNet-152 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.12.7.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="A1.T1.7.12.7.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.12.7.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.12.7.3.1" class="ltx_text" style="font-size:80%;">26.37±0.38</span></td>
<td id="A1.T1.7.12.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.12.7.4.1" class="ltx_text" style="font-size:80%;">10.85±0.37</span></td>
<td id="A1.T1.7.12.7.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.12.7.5.1" class="ltx_text" style="font-size:80%;">24.82±0.27</span></td>
<td id="A1.T1.7.12.7.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.12.7.6.1" class="ltx_text" style="font-size:80%;">11.22±0.23</span></td>
</tr>
<tr id="A1.T1.7.13.8" class="ltx_tr">
<th id="A1.T1.7.13.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.13.8.1.1" class="ltx_text" style="font-size:80%;">BERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.13.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="A1.T1.7.13.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.13.8.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.13.8.2.1" class="ltx_text" style="font-size:80%;">39.47±2.92</span></td>
<td id="A1.T1.7.13.8.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.13.8.3.1" class="ltx_text" style="font-size:80%;">26.90±0.36</span></td>
<td id="A1.T1.7.13.8.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.13.8.4.1" class="ltx_text" style="font-size:80%;">39.40±3.23</span></td>
<td id="A1.T1.7.13.8.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.13.8.5.1" class="ltx_text" style="font-size:80%;">23.81±0.86</span></td>
</tr>
<tr id="A1.T1.3.1" class="ltx_tr">
<th id="A1.T1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" rowspan="3"><span id="A1.T1.3.1.2.1" class="ltx_text" style="font-size:80%;">
<span id="A1.T1.3.1.2.1.1" class="ltx_inline-block">
<span id="A1.T1.3.1.2.1.1.1" class="ltx_p">Multimodal</span>
<span id="A1.T1.3.1.2.1.1.2" class="ltx_p">(unimodal pretrain)</span>
</span></span></th>
<th id="A1.T1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.3.1.1.1" class="ltx_text" style="font-size:80%;">MoViE+MCAN</span><sup id="A1.T1.3.1.1.2" class="ltx_sup"><span id="A1.T1.3.1.1.2.1" class="ltx_text" style="font-size:80%;">∗</span></sup><span id="A1.T1.3.1.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.3.1.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="A1.T1.3.1.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.3.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.3.1.3.1" class="ltx_text" style="font-size:80%;">71.36±0.27</span></td>
<td id="A1.T1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.3.1.4.1" class="ltx_text" style="font-size:80%;">26.64±0.45</span></td>
<td id="A1.T1.3.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.3.1.5.1" class="ltx_text" style="font-size:80%;">71.31±0.13</span></td>
<td id="A1.T1.3.1.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.3.1.6.1" class="ltx_text" style="font-size:80%;">26.37±0.49</span></td>
</tr>
<tr id="A1.T1.7.14.9" class="ltx_tr">
<th id="A1.T1.7.14.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.14.9.1.1" class="ltx_text" style="font-size:80%;">MMBT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.14.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="A1.T1.7.14.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.14.9.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.14.9.2.1" class="ltx_text" style="font-size:80%;">58.00±4.10</span></td>
<td id="A1.T1.7.14.9.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.14.9.3.1" class="ltx_text" style="font-size:80%;">26.70±0.24</span></td>
<td id="A1.T1.7.14.9.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.14.9.4.1" class="ltx_text" style="font-size:80%;">57.32±3.75</span></td>
<td id="A1.T1.7.14.9.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.14.9.5.1" class="ltx_text" style="font-size:80%;">25.78±0.34</span></td>
</tr>
<tr id="A1.T1.7.15.10" class="ltx_tr">
<th id="A1.T1.7.15.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.15.10.1.1" class="ltx_text" style="font-size:80%;">UniT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.15.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="A1.T1.7.15.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.15.10.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.15.10.2.1" class="ltx_text" style="font-size:80%;">64.36±0.13</span></td>
<td id="A1.T1.7.15.10.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.15.10.3.1" class="ltx_text" style="font-size:80%;">28.15±0.21</span></td>
<td id="A1.T1.7.15.10.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.15.10.4.1" class="ltx_text" style="font-size:80%;">64.32±0.08</span></td>
<td id="A1.T1.7.15.10.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.15.10.5.1" class="ltx_text" style="font-size:80%;">27.55±0.16</span></td>
</tr>
<tr id="A1.T1.7.16.11" class="ltx_tr">
<th id="A1.T1.7.16.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" rowspan="6"><span id="A1.T1.7.16.11.1.1" class="ltx_text" style="font-size:80%;">
<span id="A1.T1.7.16.11.1.1.1" class="ltx_inline-block">
<span id="A1.T1.7.16.11.1.1.1.1" class="ltx_p">Multimodal</span>
<span id="A1.T1.7.16.11.1.1.1.2" class="ltx_p">(multimodal pretrain)</span>
</span></span></th>
<th id="A1.T1.7.16.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.16.11.2.1" class="ltx_text" style="font-size:80%;">VisualBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.16.11.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="A1.T1.7.16.11.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.16.11.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.16.11.3.1" class="ltx_text" style="font-size:80%;">70.37±0.05</span></td>
<td id="A1.T1.7.16.11.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.16.11.4.1" class="ltx_text" style="font-size:80%;">28.70±0.36</span></td>
<td id="A1.T1.7.16.11.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.16.11.5.1" class="ltx_text" style="font-size:80%;">70.05±0.11</span></td>
<td id="A1.T1.7.16.11.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.16.11.6.1" class="ltx_text" style="font-size:80%;">28.03±0.33</span></td>
</tr>
<tr id="A1.T1.7.17.12" class="ltx_tr">
<th id="A1.T1.7.17.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.17.12.1.1" class="ltx_text" style="font-size:80%;">ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.17.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="A1.T1.7.17.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.17.12.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.17.12.2.1" class="ltx_text" style="font-size:80%;">69.42±0.30</span></td>
<td id="A1.T1.7.17.12.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.17.12.3.1" class="ltx_text" style="font-size:80%;">27.36±0.18</span></td>
<td id="A1.T1.7.17.12.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.17.12.4.1" class="ltx_text" style="font-size:80%;">69.27±0.14</span></td>
<td id="A1.T1.7.17.12.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.17.12.5.1" class="ltx_text" style="font-size:80%;">27.36±0.32</span></td>
</tr>
<tr id="A1.T1.7.18.13" class="ltx_tr">
<th id="A1.T1.7.18.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.18.13.1.1" class="ltx_text" style="font-size:80%;">ViLT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.18.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="A1.T1.7.18.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.18.13.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.18.13.2.1" class="ltx_text" style="font-size:80%;">64.52±0.42</span></td>
<td id="A1.T1.7.18.13.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.18.13.3.1" class="ltx_text" style="font-size:80%;">27.11±0.14</span></td>
<td id="A1.T1.7.18.13.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.18.13.4.1" class="ltx_text" style="font-size:80%;">65.43±2.43</span></td>
<td id="A1.T1.7.18.13.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.18.13.5.1" class="ltx_text" style="font-size:80%;">27.19±0.50</span></td>
</tr>
<tr id="A1.T1.4.2" class="ltx_tr">
<th id="A1.T1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.4.2.1.1" class="ltx_text" style="font-size:80%;">UNITER</span><sub id="A1.T1.4.2.1.2" class="ltx_sub"><span id="A1.T1.4.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Base</span></sub><span id="A1.T1.4.2.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.4.2.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="A1.T1.4.2.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.4.2.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.4.2.2.1" class="ltx_text" style="font-size:80%;">71.87±0.01</span></td>
<td id="A1.T1.4.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.4.2.3.1" class="ltx_text" style="font-size:80%;">25.16±0.49</span></td>
<td id="A1.T1.4.2.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.4.2.4.1" class="ltx_text" style="font-size:80%;">70.50±0.08</span></td>
<td id="A1.T1.4.2.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.4.2.5.1" class="ltx_text" style="font-size:80%;">25.20±0.19</span></td>
</tr>
<tr id="A1.T1.5.3" class="ltx_tr">
<th id="A1.T1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.5.3.1.1" class="ltx_text" style="font-size:80%;">UNITER</span><sub id="A1.T1.5.3.1.2" class="ltx_sub"><span id="A1.T1.5.3.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Large</span></sub><span id="A1.T1.5.3.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.5.3.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="A1.T1.5.3.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.5.3.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.5.3.2.1" class="ltx_text" style="font-size:80%;">73.57±0.21</span></td>
<td id="A1.T1.5.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.5.3.3.1" class="ltx_text" style="font-size:80%;">26.94±0.31</span></td>
<td id="A1.T1.5.3.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.5.3.4.1" class="ltx_text" style="font-size:80%;">72.71±0.22</span></td>
<td id="A1.T1.5.3.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.5.3.5.1" class="ltx_text" style="font-size:80%;">28.03±0.33</span></td>
</tr>
<tr id="A1.T1.6.4" class="ltx_tr">
<th id="A1.T1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.6.4.1.1" class="ltx_text" style="font-size:80%;">VILLA</span><sub id="A1.T1.6.4.1.2" class="ltx_sub"><span id="A1.T1.6.4.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Base</span></sub><span id="A1.T1.6.4.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.6.4.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T1.6.4.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.6.4.2" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.6.4.2.1" class="ltx_text" style="font-size:80%;">70.94±1.25</span></td>
<td id="A1.T1.6.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.6.4.3.1" class="ltx_text" style="font-size:80%;">25.14±0.93</span></td>
<td id="A1.T1.6.4.4" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.6.4.4.1" class="ltx_text" style="font-size:80%;">69.50±1.44</span></td>
<td id="A1.T1.6.4.5" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.6.4.5.1" class="ltx_text" style="font-size:80%;">25.17±0.67</span></td>
</tr>
<tr id="A1.T1.7.5" class="ltx_tr">
<th id="A1.T1.7.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:2.9pt;padding-right:2.9pt;"></th>
<th id="A1.T1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.5.1.1" class="ltx_text" style="font-size:80%;">VILLA</span><sub id="A1.T1.7.5.1.2" class="ltx_sub"><span id="A1.T1.7.5.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Large</span></sub><span id="A1.T1.7.5.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.5.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T1.7.5.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.5.3" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.5.3.1" class="ltx_text" style="font-size:80%;">72.29±0.39</span></td>
<td id="A1.T1.7.5.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.5.4.1" class="ltx_text" style="font-size:80%;">25.79±0.46</span></td>
<td id="A1.T1.7.5.5" class="ltx_td ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.5.5.1" class="ltx_text" style="font-size:80%;">71.40±0.37</span></td>
<td id="A1.T1.7.5.6" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.5.6.1" class="ltx_text" style="font-size:80%;">26.18±0.15</span></td>
</tr>
<tr id="A1.T1.7.19.14" class="ltx_tr">
<th id="A1.T1.7.19.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;" rowspan="2"><span id="A1.T1.7.19.14.1.1" class="ltx_text" style="font-size:80%;">
<span id="A1.T1.7.19.14.1.1.1" class="ltx_inline-block">
<span id="A1.T1.7.19.14.1.1.1.1" class="ltx_p">Multimodal</span>
<span id="A1.T1.7.19.14.1.1.1.2" class="ltx_p">(unimodal pretrain + OCR)</span>
</span></span></th>
<th id="A1.T1.7.19.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.19.14.2.1" class="ltx_text" style="font-size:80%;">M4C (TextVQA+STVQA) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.19.14.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="A1.T1.7.19.14.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.19.14.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.19.14.3.1" class="ltx_text" style="font-size:80%;">32.89±0.57</span></td>
<td id="A1.T1.7.19.14.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.19.14.4.1" class="ltx_text" style="font-size:80%;">28.86±0.35</span></td>
<td id="A1.T1.7.19.14.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.19.14.5.1" class="ltx_text" style="font-size:80%;">31.44±0.59</span></td>
<td id="A1.T1.7.19.14.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.19.14.6.1" class="ltx_text" style="font-size:80%;">29.08±0.33</span></td>
</tr>
<tr id="A1.T1.7.20.15" class="ltx_tr">
<th id="A1.T1.7.20.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;">
<span id="A1.T1.7.20.15.1.1" class="ltx_text" style="font-size:80%;">M4C (VQA v2 train set) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T1.7.20.15.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="A1.T1.7.20.15.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="A1.T1.7.20.15.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.20.15.2.1" class="ltx_text" style="font-size:80%;">67.66±0.34</span></td>
<td id="A1.T1.7.20.15.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.20.15.3.1" class="ltx_text" style="font-size:80%;">33.52±0.47</span></td>
<td id="A1.T1.7.20.15.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.20.15.4.1" class="ltx_text" style="font-size:80%;">66.21±0.38</span></td>
<td id="A1.T1.7.20.15.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:2.9pt;padding-right:2.9pt;"><span id="A1.T1.7.20.15.5.1" class="ltx_text" style="font-size:80%;">33.33±0.51</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">MoViE+MCAN</span> We use a batch size of 64 for 236K updates using a multi-step learning rate scheduler with steps at 180K and 216K, learning rate ratio of 0.2 and a warmup for 54K updates. Training takes an average of 2 days.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Unimodal</span> We train the models with a batch size of 64 for 88K updates with linear learning rate schedule starting from <math id="A1.p3.1.m1.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="A1.p3.1.m1.1a"><mrow id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml"><mrow id="A1.p3.1.m1.1.1.2" xref="A1.p3.1.m1.1.1.2.cmml"><mn id="A1.p3.1.m1.1.1.2.2" xref="A1.p3.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p3.1.m1.1.1.2.1" xref="A1.p3.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.p3.1.m1.1.1.2.3" xref="A1.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p3.1.m1.1.1.1" xref="A1.p3.1.m1.1.1.1.cmml">−</mo><mn id="A1.p3.1.m1.1.1.3" xref="A1.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><apply id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1"><minus id="A1.p3.1.m1.1.1.1.cmml" xref="A1.p3.1.m1.1.1.1"></minus><apply id="A1.p3.1.m1.1.1.2.cmml" xref="A1.p3.1.m1.1.1.2"><times id="A1.p3.1.m1.1.1.2.1.cmml" xref="A1.p3.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p3.1.m1.1.1.2.2.cmml" xref="A1.p3.1.m1.1.1.2.2">1</cn><ci id="A1.p3.1.m1.1.1.2.3.cmml" xref="A1.p3.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p3.1.m1.1.1.3.cmml" xref="A1.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">1e-5</annotation></semantics></math> with a warmup for 2000 updates. We used a linear learning rate schedule with 2000 warm up steps. The training takes an average of 8 hours.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p"><span id="A1.p4.1.1" class="ltx_text ltx_font_bold">MMBT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span> We trained MMBT from scratch with a batch size 64 without any pretraining following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for 88K updates. The training takes an average of 17 hours.</p>
</div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.1" class="ltx_p"><span id="A1.p5.1.1" class="ltx_text ltx_font_bold">UniT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span> We initialized from the model pretrained on all 8 datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with COCO initialization. We set the batch size to 8, weight decay as <math id="A1.p5.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="A1.p5.1.m1.1a"><mrow id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml"><mrow id="A1.p5.1.m1.1.1.2" xref="A1.p5.1.m1.1.1.2.cmml"><mn id="A1.p5.1.m1.1.1.2.2" xref="A1.p5.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p5.1.m1.1.1.2.1" xref="A1.p5.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.p5.1.m1.1.1.2.3" xref="A1.p5.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p5.1.m1.1.1.1" xref="A1.p5.1.m1.1.1.1.cmml">−</mo><mn id="A1.p5.1.m1.1.1.3" xref="A1.p5.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><apply id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1"><minus id="A1.p5.1.m1.1.1.1.cmml" xref="A1.p5.1.m1.1.1.1"></minus><apply id="A1.p5.1.m1.1.1.2.cmml" xref="A1.p5.1.m1.1.1.2"><times id="A1.p5.1.m1.1.1.2.1.cmml" xref="A1.p5.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p5.1.m1.1.1.2.2.cmml" xref="A1.p5.1.m1.1.1.2.2">1</cn><ci id="A1.p5.1.m1.1.1.2.3.cmml" xref="A1.p5.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p5.1.m1.1.1.3.cmml" xref="A1.p5.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">1e-4</annotation></semantics></math> and train the model on 8 GPUs for 2 days.</p>
</div>
<div id="A1.p6" class="ltx_para">
<p id="A1.p6.1" class="ltx_p"><span id="A1.p6.1.1" class="ltx_text ltx_font_bold">VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span> We trained VisualBERT from the best pretrained model on COCO using MLM loss using a batch size of 64 which takes an average of 8 hours.</p>
</div>
<div id="A1.p7" class="ltx_para">
<p id="A1.p7.1" class="ltx_p"><span id="A1.p7.1.1" class="ltx_text ltx_font_bold">ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite></span> We trained ViLBERT from the best pretrained model on Conceptual Captions using MLM loss using a batch size of 64 which takes an average of 13 hours.</p>
</div>
<div id="A1.p8" class="ltx_para">
<p id="A1.p8.2" class="ltx_p"><span id="A1.p8.2.1" class="ltx_text ltx_font_bold">ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></span> We trained ViLT with 44K updates, initial learning rate of <math id="A1.p8.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="A1.p8.1.m1.1a"><mrow id="A1.p8.1.m1.1.1" xref="A1.p8.1.m1.1.1.cmml"><mrow id="A1.p8.1.m1.1.1.2" xref="A1.p8.1.m1.1.1.2.cmml"><mn id="A1.p8.1.m1.1.1.2.2" xref="A1.p8.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p8.1.m1.1.1.2.1" xref="A1.p8.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.p8.1.m1.1.1.2.3" xref="A1.p8.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p8.1.m1.1.1.1" xref="A1.p8.1.m1.1.1.1.cmml">−</mo><mn id="A1.p8.1.m1.1.1.3" xref="A1.p8.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p8.1.m1.1b"><apply id="A1.p8.1.m1.1.1.cmml" xref="A1.p8.1.m1.1.1"><minus id="A1.p8.1.m1.1.1.1.cmml" xref="A1.p8.1.m1.1.1.1"></minus><apply id="A1.p8.1.m1.1.1.2.cmml" xref="A1.p8.1.m1.1.1.2"><times id="A1.p8.1.m1.1.1.2.1.cmml" xref="A1.p8.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p8.1.m1.1.1.2.2.cmml" xref="A1.p8.1.m1.1.1.2.2">1</cn><ci id="A1.p8.1.m1.1.1.2.3.cmml" xref="A1.p8.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p8.1.m1.1.1.3.cmml" xref="A1.p8.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p8.1.m1.1c">1e-4</annotation></semantics></math>, eps as <math id="A1.p8.2.m2.1" class="ltx_Math" alttext="1e-8" display="inline"><semantics id="A1.p8.2.m2.1a"><mrow id="A1.p8.2.m2.1.1" xref="A1.p8.2.m2.1.1.cmml"><mrow id="A1.p8.2.m2.1.1.2" xref="A1.p8.2.m2.1.1.2.cmml"><mn id="A1.p8.2.m2.1.1.2.2" xref="A1.p8.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p8.2.m2.1.1.2.1" xref="A1.p8.2.m2.1.1.2.1.cmml">​</mo><mi id="A1.p8.2.m2.1.1.2.3" xref="A1.p8.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p8.2.m2.1.1.1" xref="A1.p8.2.m2.1.1.1.cmml">−</mo><mn id="A1.p8.2.m2.1.1.3" xref="A1.p8.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p8.2.m2.1b"><apply id="A1.p8.2.m2.1.1.cmml" xref="A1.p8.2.m2.1.1"><minus id="A1.p8.2.m2.1.1.1.cmml" xref="A1.p8.2.m2.1.1.1"></minus><apply id="A1.p8.2.m2.1.1.2.cmml" xref="A1.p8.2.m2.1.1.2"><times id="A1.p8.2.m2.1.1.2.1.cmml" xref="A1.p8.2.m2.1.1.2.1"></times><cn type="integer" id="A1.p8.2.m2.1.1.2.2.cmml" xref="A1.p8.2.m2.1.1.2.2">1</cn><ci id="A1.p8.2.m2.1.1.2.3.cmml" xref="A1.p8.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p8.2.m2.1.1.3.cmml" xref="A1.p8.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p8.2.m2.1c">1e-8</annotation></semantics></math> and weight decay as 0.01 for an average of 7 hours.</p>
</div>
<div id="A1.p9" class="ltx_para">
<p id="A1.p9.1" class="ltx_p"><span id="A1.p9.1.1" class="ltx_text ltx_font_bold">UNITER/VILLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span> We used the author-provided pretrained checkpoint for UNITER<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>UNITER Code: https://github.com/ChenRocks/UNITER</span></span></span> and VILLA<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>VILLA Code: https://github.com/zhegan27/VILLA</span></span></span> with a confidence threshold of 0.075. The rest of the hyper parameters were consistent with the configuration provided in the repository. The training takes about 2 hours.</p>
</div>
<div id="A1.p10" class="ltx_para">
<p id="A1.p10.1" class="ltx_p"><span id="A1.p10.1.1" class="ltx_text ltx_font_bold">M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></span> We used the same training schedule and hyper parameters as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> used for TextVQA training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> for both TextVQA + STVQA and VQA v2. We didn’t use extra Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> QA data due to lack of OCR text and used the OCR data from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for both COCO and TextVQA.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Off-the-shelf models</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In addition to the main results in Table <a href="#S3.T3" title="Table 3 ‣ 3.1 Baselines and Methods ‣ 3 Model Evaluation ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we also evaluated existing OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> / VinVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> models on AdVQA, since both models are known to perform well on VQA. Note that the training set of the off-the-shelf OSCAR and VinVL models includes COCO val2014 data, which overlaps with our validation set (COCO 2017). Even then, we find that the validation performance is still much lower (more than 2x) compared to its performance on VQA v2. See Table <a href="#A2.T1" title="Table B.1 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>.</p>
</div>
<figure id="A2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table B.1</span>: </span><span id="A2.T1.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Validation performance of off-the-shelf OSCAR and VinVL models on AdVQA<span id="A2.T1.8.2.1" class="ltx_text ltx_font_medium">. Note that AdVQA validation set has some overlap with the off-the-shelf models’ training set.</span></span></figcaption>
<table id="A2.T1.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T1.4.4.5.1" class="ltx_tr">
<th id="A2.T1.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Model</th>
<th id="A2.T1.4.4.5.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">VQA</th>
<th id="A2.T1.4.4.5.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AdVQA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T1.1.1.1" class="ltx_tr">
<th id="A2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">OSCAR<sub id="A2.T1.1.1.1.1.1" class="ltx_sub"><span id="A2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">Base</span></sub>
</th>
<td id="A2.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">72.59</td>
<td id="A2.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">29.17</td>
</tr>
<tr id="A2.T1.2.2.2" class="ltx_tr">
<th id="A2.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OSCAR<sub id="A2.T1.2.2.2.1.1" class="ltx_sub"><span id="A2.T1.2.2.2.1.1.1" class="ltx_text ltx_font_italic">Large</span></sub>
</th>
<td id="A2.T1.2.2.2.2" class="ltx_td ltx_align_left">92.64</td>
<td id="A2.T1.2.2.2.3" class="ltx_td ltx_align_left">31.60</td>
</tr>
<tr id="A2.T1.3.3.3" class="ltx_tr">
<th id="A2.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VinVL<sub id="A2.T1.3.3.3.1.1" class="ltx_sub"><span id="A2.T1.3.3.3.1.1.1" class="ltx_text ltx_font_italic">Base</span></sub>
</th>
<td id="A2.T1.3.3.3.2" class="ltx_td ltx_align_left">75.13</td>
<td id="A2.T1.3.3.3.3" class="ltx_td ltx_align_left">33.59</td>
</tr>
<tr id="A2.T1.4.4.4" class="ltx_tr">
<th id="A2.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">VinVL<sub id="A2.T1.4.4.4.1.1" class="ltx_sub"><span id="A2.T1.4.4.4.1.1.1" class="ltx_text ltx_font_italic">Large</span></sub>
</th>
<td id="A2.T1.4.4.4.2" class="ltx_td ltx_align_left ltx_border_bb">76.23</td>
<td id="A2.T1.4.4.4.3" class="ltx_td ltx_align_left ltx_border_bb">37.12</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/examples_instr.png" id="A2.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="278" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A2.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Instructions for Examples Stage</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/example_1.png" id="A2.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="390" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A2.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Invalid example</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/example_2.png" id="A2.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="439" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A2.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">Valid example</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure B.1</span>: </span><span id="A2.F1.3.2" class="ltx_text" style="font-size:90%;">Qualification Phase Stage 1: View Examples</span></figcaption>
</figure>
<figure id="A2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/quiz_instr.png" id="A2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="283" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A2.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Instructions for Quiz Stage</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/quiz_1.png" id="A2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="390" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A2.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Determine if the answer is correct</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2106.02280/assets/images/quiz_2.png" id="A2.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="367" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A2.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Determine if the question is valid</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure B.2</span>: </span><span id="A2.F2.3.2" class="ltx_text" style="font-size:90%;">Qualification Phase Stage 2 - quiz. Annotators are allowed access to the main task only if they passed the quiz. </span></figcaption>
</figure>
<figure id="A2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2106.02280/assets/images/qc_instr.png" id="A2.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="491" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A2.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Question Collection Instructions</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2106.02280/assets/images/val_instr.png" id="A2.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="447" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A2.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Question Validation Instructions</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure B.3</span>: </span><span id="A2.F3.3.2" class="ltx_text" style="font-size:90%;">Main Labeling Task Instructions</span></figcaption>
</figure>
<figure id="A2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2106.02280/assets/images/prev_qc.png" id="A2.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="167" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A2.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Preview for Question Collection</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2106.02280/assets/images/prev_val.png" id="A2.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A2.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Preview for Question Validations</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure B.4</span>: </span><span id="A2.F4.3.2" class="ltx_text" style="font-size:90%;">Preview - landing page on MTurk interface.</span></figcaption>
</figure>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2106.02280/assets/images/val_interface.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="590" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure B.5</span>: </span><span id="A2.F5.3.2" class="ltx_text" style="font-size:90%;">Validation Interface</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Annotation details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Before annotators were able to proceed with the main task, they had to pass an on-boarding phase (Section <a href="#A3.SS1" title="C.1 Qualification Phase ‣ Appendix C Annotation details ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>). We restricted the interface to be only accessible on non-mobile devices, to annotators in the US with at least 100 approved hits and with an approval rate higher than 98%. The annotators were paid a bonus if their self-claimed fooling question was verified to be fooling and valid by two other annotators.</p>
</div>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Qualification Phase</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">The annotators were asked to go through a two-stage qualification phase. In the first stage, they were shown 11 examples that include both valid and invalid questions. Figure <a href="#A2.F1" title="Figure B.1 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a> shows valid and invalid examples in the Example Stage. After scrolling through those 11 examples, the annotators proceeded to the second stage, in which we ask them to complete a quiz. The annotators passed only if they got more than 6 out of 7 correct, after which they qualified for the main task. There are two types of quiz questions: 1) determine if the provided answer is correct for the specific image question pair; and 2) determine if a given question is valid with the image as context. See Figure <a href="#A2.F2" title="Figure B.2 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> for examples of those two question types. If an annotator failed the first time, they were given an explanation on the correct choice before being allowed a second try on a different (but similar) set of questions.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Main Labeling Task</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">Figure <a href="#A2.F3.sf1" title="In Figure B.3 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> and <a href="#A2.F3.sf2" title="In Figure B.3 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> show the instructions given to first-time annotators for the “question collection” and “question validation” tasks. The instructions were hidden for the non-first-time annotators, but remained accessible via a button at the top. Figure  <a href="#A2.F4.sf1" title="In Figure B.4 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a href="#A2.F4.sf2" title="In Figure B.4 ‣ Appendix B Off-the-shelf models ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> show the preview landing pages on Mechanical Turk.</p>
</div>
<figure id="A3.F1" class="ltx_figure"><img src="/html/2106.02280/assets/images/answer_1.png" id="A3.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="314" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure C.1</span>: </span><span id="A3.F1.3.2" class="ltx_text" style="font-size:90%;">Preview for Answer Collection</span></figcaption>
</figure>
<figure id="A3.F2" class="ltx_figure"><img src="/html/2106.02280/assets/images/answer_2.png" id="A3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure C.2</span>: </span><span id="A3.F2.3.2" class="ltx_text" style="font-size:90%;">Answer collection interface</span></figcaption>
</figure>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Answer collection task</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">Figure <a href="#A3.F1" title="Figure C.1 ‣ C.2 Main Labeling Task ‣ Appendix C Annotation details ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a> and <a href="#A3.F2" title="Figure C.2 ‣ C.2 Main Labeling Task ‣ Appendix C Annotation details ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.2</span></a> show the preview and main interface of the answer collection stage. For each question, we collect ten answers from ten different annotators using this interface. We provide explicit instructions following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> to avoid ambiguity and collect short relevant answers. The annotators are also provided a checkbox to select “unanswerable” in case the question is ambiguous or can’t be answered which annotators are suggested to use sparingly. Finally, we use and show a set of hand-crafted already annotated questions without ambiguity randomly to filter out bad annotators by comparing their answers to ground truth. An annotator is prevented from doing the task if they fail the test three times.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Random Samples</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We show 10 randomly selected samples in Figure <a href="#A4.T1" title="Table D.1 ‣ Appendix D Random Samples ‣ Human-Adversarial Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a>.</p>
</div>
<figure id="A4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A4.T1.7.1.1" class="ltx_text" style="font-size:113%;">Table D.1</span>: </span><span id="A4.T1.8.2" class="ltx_text ltx_font_bold" style="font-size:113%;">Random examples from AdVQA<span id="A4.T1.8.2.1" class="ltx_text ltx_font_medium">. </span></span></figcaption>
<table id="A4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T1.2.3.1" class="ltx_tr">
<th id="A4.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.T1.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></th>
<th id="A4.T1.2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.T1.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T1.2.3.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T1.2.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T1.1.1" class="ltx_tr">
<th id="A4.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.T1.1.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r10_censored.png" id="A4.T1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="197" height="294" alt="[Uncaptioned image]"></span></th>
<td id="A4.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T1.1.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.T1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Which hands does he have bracelets on?</span><span id="A4.T1.1.1.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.1.1.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.T1.1.1.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: both (count: 9)</span><span id="A4.T1.1.1.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.1.1.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.T1.1.1.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’both’, ’both hands’, ’both’, ’both’</span><span id="A4.T1.1.1.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.1.1.2.1.1.7" class="ltx_text" style="font-size:80%;">’both’, ’both’, ’both’, ’both’, ’both’, ’both’</span></span>
</span>
</td>
</tr>
<tr id="A4.T1.2.2" class="ltx_tr">
<th id="A4.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.T1.2.2.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r9_censored.png" id="A4.T1.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="131" alt="[Uncaptioned image]"></span></th>
<td id="A4.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T1.2.2.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T1.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>:What is the baby wearing?</span><span id="A4.T1.2.2.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.2.2.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.T1.2.2.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: overalls (count: 6)</span><span id="A4.T1.2.2.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.2.2.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.T1.2.2.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’shirt &amp; overall’, ’overalls’, ’overalls’, ’overalls’</span><span id="A4.T1.2.2.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.T1.2.2.2.1.1.7" class="ltx_text" style="font-size:80%;">’jumper’, ’overalls’, ’coverals ’, ’dungerees’, ’overalls’, ’overalls’</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A4.4" class="ltx_table">
<table id="A4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.4.4.5.1" class="ltx_tr">
<th id="A4.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.4.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></th>
<th id="A4.4.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.4.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.4.4.5.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.4.4.5.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.1.1.1" class="ltx_tr">
<th id="A4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.1.1.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r1_censored.png" id="A4.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="131" alt="[Uncaptioned image]"></span></th>
<td id="A4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.1.1.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: How many people can be seen in the room?</span><span id="A4.1.1.1.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.1.1.1.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.1.1.1.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: 6 (count: 5), 7 (count: 3)</span><span id="A4.1.1.1.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.1.1.1.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.1.1.1.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’7’, ’5’, ’6’, ’8’, ’6’, ’7’, ’6’, ’6’, ’7’, ’6’</span></span>
</span>
</td>
</tr>
<tr id="A4.2.2.2" class="ltx_tr">
<th id="A4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.2.2.2.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r2.jpeg" id="A4.2.2.2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="197" height="262" alt="[Uncaptioned image]"></span></th>
<td id="A4.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.2.2.2.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.2.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.2.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What is on the stovetop?</span><span id="A4.2.2.2.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.2.2.2.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.2.2.2.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: kettle (count: 4)</span><span id="A4.2.2.2.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.2.2.2.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.2.2.2.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’tea kettle’, ’kettle’, ’teapot’, ’teapot’, ’right’</span><span id="A4.2.2.2.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.2.2.2.2.1.1.7" class="ltx_text" style="font-size:80%;">’tea kettle’, ’kettle’, ’teapot’, ’kettle’, ’kettle’</span></span>
</span>
</td>
</tr>
<tr id="A4.3.3.3" class="ltx_tr">
<th id="A4.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.3.3.3.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r3.jpeg" id="A4.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="131" alt="[Uncaptioned image]"></span></th>
<td id="A4.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.3.3.3.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.3.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.3.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Whats does the three letters spell?</span><span id="A4.3.3.3.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.3.3.3.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.3.3.3.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: unknown (not in vocab)</span><span id="A4.3.3.3.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.3.3.3.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.3.3.3.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’pub’, ’pub’, ’pub’, ’pub’, ’pub’</span><span id="A4.3.3.3.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.3.3.3.2.1.1.7" class="ltx_text" style="font-size:80%;">’pub’, ’pub’, ’pub’, ’pub’, ’pub’</span></span>
</span>
</td>
</tr>
<tr id="A4.4.4.4" class="ltx_tr">
<th id="A4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.4.4.4.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r4.jpeg" id="A4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_portrait" width="197" height="294" alt="[Uncaptioned image]"></span></th>
<td id="A4.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.4.4.4.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.4.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.4.4.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Are the windows all the same size?</span><span id="A4.4.4.4.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.4.4.4.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.4.4.4.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: no (count: 10)</span><span id="A4.4.4.4.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.4.4.4.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.4.4.4.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’no’, ’no’, ’no’, ’no’, ’no’, ’no’</span><span id="A4.4.4.4.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.4.4.4.2.1.1.7" class="ltx_text" style="font-size:80%;">’no’, ’no’, ’no’, ’no’</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A4.8" class="ltx_table">
<table id="A4.8.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.8.4.5.1" class="ltx_tr">
<th id="A4.8.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.8.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></th>
<th id="A4.8.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.8.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.8.4.5.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.8.4.5.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AdVQA</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.5.1.1" class="ltx_tr">
<th id="A4.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.5.1.1.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r5.jpeg" id="A4.5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="131" alt="[Uncaptioned image]"></span></th>
<td id="A4.5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.5.1.1.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.5.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.5.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: how many pieces of meat?</span><span id="A4.5.1.1.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.5.1.1.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.5.1.1.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: 2 (count: 7), 1 (count: 3)</span><span id="A4.5.1.1.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.5.1.1.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.5.1.1.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’2’, ’2’, ’2’, ’2’, ’2’, ’2’, ’1’, ’1’, ’1’, ’2’</span></span>
</span>
</td>
</tr>
<tr id="A4.6.2.2" class="ltx_tr">
<th id="A4.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.6.2.2.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r6.jpeg" id="A4.6.2.2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="197" height="298" alt="[Uncaptioned image]"></span></th>
<td id="A4.6.2.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.6.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.6.2.2.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.6.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.6.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: Which is the largest window?</span><span id="A4.6.2.2.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.6.2.2.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.6.2.2.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: middle (count: 3)</span><span id="A4.6.2.2.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.6.2.2.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.6.2.2.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’center’, ’bottom middle’, ’bottom middle’, </span><span id="A4.6.2.2.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.6.2.2.2.1.1.7" class="ltx_text" style="font-size:80%;">’middle’, ’middle’, ’bottom middle’, ’middle lower ’</span><span id="A4.6.2.2.2.1.1.8" class="ltx_text" style="font-size:80%;">
</span><span id="A4.6.2.2.2.1.1.9" class="ltx_text" style="font-size:80%;">’where a cat sits’, ’middle one’, ’the middle’</span></span>
</span>
</td>
</tr>
<tr id="A4.7.3.3" class="ltx_tr">
<th id="A4.7.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.7.3.3.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r7.jpeg" id="A4.7.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="148" alt="[Uncaptioned image]"></span></th>
<td id="A4.7.3.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.7.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.7.3.3.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.7.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.7.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What color is the checkerboard background?</span><span id="A4.7.3.3.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.7.3.3.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.7.3.3.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: black and white (count: 5)</span><span id="A4.7.3.3.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.7.3.3.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.7.3.3.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’unanswerable’, ’black and white’, ’gray’</span><span id="A4.7.3.3.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.7.3.3.2.1.1.7" class="ltx_text" style="font-size:80%;">’black and white’, ’black’, ’black white’, ’black and white’</span><span id="A4.7.3.3.2.1.1.8" class="ltx_text" style="font-size:80%;">
</span><span id="A4.7.3.3.2.1.1.9" class="ltx_text" style="font-size:80%;">’black and white’, ’black white’, ’black and white’</span></span>
</span>
</td>
</tr>
<tr id="A4.8.4.4" class="ltx_tr">
<th id="A4.8.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A4.8.4.4.1.1" class="ltx_text" style="font-size:80%;position:relative; bottom:-0.5pt;"><img src="/html/2106.02280/assets/images/r8.jpeg" id="A4.8.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="131" alt="[Uncaptioned image]"></span></th>
<td id="A4.8.4.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="A4.8.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.8.4.4.2.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.8.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A4.8.4.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What does it say on the side of the boat closest in the foreground?</span><span id="A4.8.4.4.2.1.1.2" class="ltx_text" style="font-size:80%;">
</span><span id="A4.8.4.4.2.1.1.3" class="ltx_text" style="font-size:80%;"><span id="A4.8.4.4.2.1.1.3.1" class="ltx_text ltx_font_bold">Processed Answers</span>: unknown (not in vocab)</span><span id="A4.8.4.4.2.1.1.4" class="ltx_text" style="font-size:80%;">
</span><span id="A4.8.4.4.2.1.1.5" class="ltx_text" style="font-size:80%;"><span id="A4.8.4.4.2.1.1.5.1" class="ltx_text ltx_font_bold">Raw Answers</span>: ’sanssouci’, ’sanssouci’, ’sanssouci’</span><span id="A4.8.4.4.2.1.1.6" class="ltx_text" style="font-size:80%;">
</span><span id="A4.8.4.4.2.1.1.7" class="ltx_text" style="font-size:80%;">’sanssouci’, ’sanssouci’, ’sanssouci’, ’sanssouci’, ’sanssouci’</span><span id="A4.8.4.4.2.1.1.8" class="ltx_text" style="font-size:80%;">
</span><span id="A4.8.4.4.2.1.1.9" class="ltx_text" style="font-size:80%;">’sanssouci’, ’sanssouci’</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.02279" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.02280" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.02280">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.02280" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.02281" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 05:05:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
