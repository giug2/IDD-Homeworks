<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.12990] A Survey on Drowsiness Detection – Modern Applications and Methods</title><meta property="og:description" content="Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey on Drowsiness Detection – Modern Applications and Methods">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey on Drowsiness Detection – Modern Applications and Methods">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.12990">

<!--Generated on Thu Sep  5 13:47:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Drowsiness detection,  Fatigue detection,  Drowsiness and safety,  Public Transportation,  Sleep analysis and drowsiness
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Drowsiness Detection – Modern Applications and Methods</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Biying Fu, Fadi Boutros, Chin-Teng Lin and Naser Damer
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection plays a critical role in preventing accidents, enhancing safety, and ultimately saving lives across various sectors and scenarios. This comprehensive review explores the significance of drowsiness detection in various areas of application, transcending the conventional focus solely on driver drowsiness detection. We delve into the current methodologies, challenges, and technological advancements in drowsiness detection schemes, considering diverse contexts such as public transportation, healthcare, workplace safety, and beyond. By examining the multifaceted implications of drowsiness, this work contributes to a holistic understanding of its impact and the crucial role of accurate and real-time detection techniques in enhancing safety and performance. We identified weaknesses in current algorithms and limitations in existing research such as accurate and real-time detection, stable data transmission, and building bias-free systems. Our survey frames existing works and leads to practical recommendations like mitigating the bias issue by using synthetic data, overcoming the hardware limitations with model compression, and leveraging fusion to boost model performance. This is a pioneering work to survey the topic of drowsiness detection in such an entirely and not only focusing on one single aspect. We consider the topic of drowsiness detection as a dynamic and evolving field, presenting numerous opportunities for further exploration.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Drowsiness detection, Fatigue detection, Drowsiness and safety, Public Transportation, Sleep analysis and drowsiness

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Drowsiness detection aims at detecting the early symptoms of individual drowsiness using physiological (EEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, ECG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>) and visible behavioural (eye blinking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and eye closure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>) indications. The accurate and real-time identification of drowsiness detection is crucial for multiple reasons. One of the most prominent use-cases is driver and road safety; drowsy driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is a dominant cause of accidents, injuries, and fatalities on the road. The National Highway Traffic Safety Administration estimated that up to 20% of the annual traffic deaths were attributed to driver drowsiness in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. An early alert can help prevent accidents caused by impaired reaction times of a drowsy driver. Under the setting of workplace safety, such as employees working in jobs like e.g., operating heavy machinery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, industrial equipment, or in medical establishment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, drowsiness can lead to accidents that jeopardize worker’s safety or patient safety. In addition, drowsiness could negatively impact the cognitive function and productivity of shift workers with long working hours. Even in healthcare, monitoring the drowsiness of patients, especially those with sleep disorders or undergoing medical treatments, ensures their well-being and helps healthcare providers make informed decisions about accurate treatment plans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Finally, drowsiness detection is crucial in sectors such as aviation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and public transportation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, where a drowsy operator can compromise passenger safety.
The attentiveness of the driver significantly impacts railway safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Incidents involving high-speed trains can result in severe consequences, as evidenced by the most catastrophic railway accident in Chinese history occurring on 23 July 2011, resulting in 40 fatalities and at least 192 injuries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Therefore, finding ways to unobtrusively detect driver drowsiness operating these high-speed trains is important. Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the application areas that can benefit greatly from accurate and real-time drowsiness detection.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.12990/assets/drowsiness_detection_areas.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Depicts areas of applications in urgent need of accurate and real-time drowsiness detection task.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Drowsiness detection is thus vital for safety, preventing accidents caused by impaired attention and reaction times. It improves health by identifying sleep disorders and enhances productivity in various settings. After motivating the importance of drowsiness detection, our work preliminary focused on showing modern applications and methods on drowsiness detection. We are the first work concluding multiple aspects and areas of drowsiness detection not only focusing especially on driver drowsiness detection. Current surveys on drowsiness detection mainly focused on presenting either techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on driver drowsiness detection. Less focus was put on the general use-cases of drowsiness detection in such a broad application area and its faced challenges.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2408.12990/assets/categorization.jpg" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="309" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall structure of the surveyed advancements in drowsiness detection. This primary framework illustrates the three measuring techniques utilized in drowsiness detection, along with their respective attributes and methodologies. From the considered topology, we first distinguish between drowsiness detection based on physiological signals and monitoring through vision. Based on these classification, we assign different attributes to individual measuring techniques. The sensing modalities for EEG and ECG are not mutually exclusive and can be divided under wired and wireless application. While under the vision-approach, we now consider static investigation for single frame and dynamic investigation for multiple frames and the unobtrusiveness for the general setup. Abbreviations like HRV stands for heartrate variability, PRV for pulserate variability, QRS for morphological components of a heartbeat, LF/HF for certain frequency bands, and finally PERCLOS for percentage of eye closure. The sensing modality demonstrates the potential realization of applications. The bottom row outlines the typical areas of application for drowsiness detection across all measuring techniques.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts a primary framework of this survey, illustrating the three measuring techniques and their respective attributes used for detection, sensing modalities, and typical application areas across all measuring techniques. and provides a clear view of the structure of our work. Based on our proposed topology, we differentiate between drowsiness detection relying on precise physiological signals and monitoring through vision-based approaches. Biological processes manifest themselves in brain and heart activities, that can lead to distinct changes in specific attributes induced by drowsiness. Consequently, we can assign different attributes to individual measuring techniques based on our classification. The sensing modality for physiological signals typically involves electrode-based methods. However, there is a shift towards wearable and wireless communication technologies in this regard to enhance user acceptance. Monitoring through vision is in general more unobtrusive and user friendlier, focusing on relevant attributes such as facial features, eye closures, and head poses, among others. We view single frame as static and consecutive frames as dynamic sensing modality in this context.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The structure of our work is outline in the following. In Section <a href="#S2" title="II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we discuss recent works within each of the application areas. In Section <a href="#S3" title="III Measuring Technology ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> we first introduce three general techniques typically used for drowsiness detection under two main categories, i.e. physiological signal- and vision-based approaches. In Section <a href="#S4" title="IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we extend the techniques with relevant works. In Section <a href="#S5" title="V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> we provide a list of public benchmarks used to evaluate or design algorithms for drowsiness detection both in form of time series and image- or video-sequence-based data. In Section <a href="#S6" title="VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we exclusively explained the performance and evaluation metrics used for assessing the algorithm’s effectiveness in state-of-the-art (SOTA) works. We then discuss and reveal the existing limitations within this research area categorized under physiological and vision-based approaches and present potential solutions and actionable suggestions in Section <a href="#S7" title="VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>. In Section <a href="#S8" title="VIII Potential Directions for Research ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>, we summarize the current research gaps and provide potential future research directions. Finally, we conclude our work by summarizing the major findings in Section <a href="#S9" title="IX Conclusion ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Areas of Applications</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first start with relevant works for drowsiness detection grouped by applications. Beyond the scope of driver’s drowsiness detection, drowsiness detection can take place in several other aspects of our daily life, as it has far-reaching effects ranging from affecting our quality at work, influencing our healthcare, or clouding our ability at working on cognitive tasks.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection in workplace and secured areas</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Drowsiness and sleepiness in the workplace are two of the major risks of modern society <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Well-rested and alert employees are fundamental for better productivity and creativity, while excessive fatigue not only reduces efficiency but also can pose a risk at workplaces, thus fatigue management in the workplace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> aims to enhance worker health and well-being both on and off the workspace.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">One example of a mitigating safety issue caused by drowsiness at the workplace is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The work by Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> focused on improving the performance of drowsiness detection for crane operators by using hybrid deep neural networks connecting both spatial and temporal features from videos. The primary contributions of this study involved extending drowsiness detection beyond vehicle drivers to crane operators, introducing relevant facial features as indicators for detection. In addition to their customized dataset on simulated crane operation scenarios, the research provided recommendations for gathering extensive and publicly accessible realistic drowsiness datasets tailored to crane operators.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">Sectors engaged in safety-critical endeavors, including the oil and gas industry, exhibit a vested interest in monitoring biological markers to avert human errors and enhance process safety, thereby enhancing their readiness for emergency situations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Within this context, the reliability of human performance assumes a pivotal role in preventing potential catastrophic incidents stemming from human factors, such as worker fatigue. Ramos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> introduced an ensemble approach for fully automated drowsiness detection utilizing Electroencephalogram (EEG) signals.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p4.1" class="ltx_p">Even in office environments across various industries, persistent drowsiness poses a significant challenge, resulting in occupational fatigue due to excessive work demands, particularly when precise operational output is essential. Presently, companies are increasingly dedicating resources to monitor their workplaces, aiming to uphold employees’ optimal working states and sustain a desirable level of productivity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Natnithikarat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> proposed one approach to detect the drowsiness state of an office worker that combines Biometric characteristics like keyboard keystrokes and mouse movement along with eye tracking during office-related tasks. The objective was to identify and assess drowsiness based on the self-reported Karolinska sleepiness scale (KSS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> questionnaire. The study identified a correlation between the anticipated level of drowsiness inferred from the biometric data and the estimated KSS score provided by the users. These findings suggest the viability of using the proposed method to effectively detect the level of drowsiness among office workers.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection in healthcare sectors</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Modern industries, hospitals, and many other essential sectors require shift work to maintain productivity and profit<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. However shift work with its induced drowsiness and fatigue in shift workers has proved to be the source of human error and can lead to a number of accidents, catastrophes, and health-related problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">In healthcare sectors, Geiger-Brown et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> revealed in their study that nurses frequently express fatigue and dissatisfaction with the quality of sleep when engaged in 12-hour shifts. This study examines the sleep patterns, levels of sleepiness, fatigue, and neuro-behavioral performance across three consecutive 12-hour shifts (both day and night) for hospital nurses. The findings indicate that nurses accumulate a substantial sleep deficit during successive 12-hour shifts, leading to increased fatigue, sleepiness, and decreased attention. Other studies investigating the subjective sleep quality and daytime sleepiness among nursing staff can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As a result, it becomes imperative to promptly and accurately identify fatigue in real-time to mitigate potential instances of malpractice stemming from fatigue-related issues in the medical domain.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p3.1" class="ltx_p">Chervin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> conducted an investigation in 2000, focusing on the relation of sleepiness, fatigue, tiredness, and lack of energy in individuals to obstructive sleep apnea. The study encompassed a comprehensive analysis involving 190 participants, including 117 males (M) and 73 females (F), within a university-affiliated sleep laboratory. Data were derived from both sleep studies and questionnaires. The study’s findings suggested that complaints regarding fatigue, tiredness, and reduced energy levels could hold comparable significance to reports of sleepiness among obstructive sleep apnea patients. Notably, female patients seemed to express such concerns more frequently than their male counterparts.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection at public transportation and aviation</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">In the public transportation sector, accurate and real-time drivers’ drowsiness detection can save human lives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposed a non-parametric solution for detecting the cognitive state of pilots by utilizing a 64-channel brain EEG signal. They developed 2D brain maps from these spatially distributed 3D multichannel EEG signals and extracted useful feature maps for developing algorithms to detect fatigue in pilots. Another work by Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> showcased drowsiness detection within the aviation industry too. They utilized EEG signals obtained from a standard aviation headset to identify the drowsiness levels of pilots. The researchers integrated the Seeing Machines driver monitoring system
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Seeing Machines driver monitoring system: https://seeingmachines.com/</span></span></span> with electrooculogram (EOG) data to localize microsleep events and investigated unique characteristics in EEG spectral patterns during these events. Simultaneous recordings of EEG, EOG, and facial behavior data were taken from 16 pilots during simulated flights. Their study demonstrated useful features from the EEG signals and confirmed the effectiveness of drowsiness detection by embedding EEG electrodes within the commonly used aviation headset.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> targeted drivers operating high-speed trains. They proposed a driver’s drowsiness detection system for high-speed train safety based on monitoring train driver’s vigilance using a wireless wearable EEG. The proposed system includes three stages ranging from wireless data collection and driver vigilance detection to pushing early alert messages to drivers. An 8-channel wireless wearable brain-computer interface is used to unobtrusively collect the locomotive driver’s brain EEG signal, while the driver is simultaneously operating a high-speed train.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p3.1" class="ltx_p">Multi-modal fusion of multiple physiological signals and leveraging deep learning techniques for driver’s drowsiness detection for high-speed rail operators can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. One research specifically deals with drowsiness detection from facial clues with occluded face images of railway drivers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This study was conducted in the post-covid phase to work with the autonomous-rail rapid transit system in China railway. This research was built upon facial thermal imaging and also included environmental information for detection.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness Detection in smart home context</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Beyond workspace applications, modern homes with integrated smart technologies also intend to enhance the user’s living experience in domestic areas. The smart home application with the smart mirror is used as an example to assess residents’ well-being over time to improve their lifestyle through user-centered guidance. Facial clues indicating user’s emotional states like stress, fatigue and anxiety are targeted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p2.1" class="ltx_p">Elderly fall due to drowsiness was studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Based on facial landmarks and eye openness, Kumar et al. further analyzed sleep patterns in order to help to predict the physical condition of the elderly and to avoid emergency situations such as falls. The main contribution of this work is to predict the health condition of the elderly by leveraging machine learning models and their results were verified on real-world scenarios while maintaining good accuracy (Acc).</p>
</div>
<div id="S2.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p3.1" class="ltx_p">Another proof of concept work focusing on real-time drowsiness detection for elderly care is in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. The study is based on video feeds to extract visible facial clues, such as yawning, eyelid and head movement over time which are related to drowsiness detection. Classification simply based on eyelid and mouth status already achieved an accuracy between 94.3%-97.2%.</p>
</div>
<div id="S2.SS0.SSS0.Px4.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p4.1" class="ltx_p">In this survey, we focus on showing modern application areas and methods in terms of drowsiness detection not only limited to driver’s drowsiness detection but extended to cover much broader possible scenarios and detection under more general and diverse purposes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Measuring Technology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the three most popular measuring techniques for drowsiness detection. These measuring techniques can be categorized under physiological sensing (which uses either EEG or ECG) and vision-based sensing. Under physiological sensing, researchers aim to capture numerous biological signals of each individual, such as heart rate variability, muscle movement, and brain wave activities. From these biological signals, unique biological markers are extracted to detect drowsiness. Under vision-based capture, visual feeds are leveraged to derive facial expressions and eye blinking status, among other clues, to give an indication of drowsiness.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">How does Electroencephalogram work for drowsiness detection?</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Electroencephalogram (EEG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> measures the electrical activity of the brain. It involves the recording of the brain’s electrical signals using electrodes placed on the scalp. EEG is a non-invasive and painless procedure used to study brain activity, diagnose certain brain disorders, and monitor brain health during medical treatments.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">EEG-based drowsiness detection systems are just one component of an overall driver safety system. During a driving simulation or driving a vehicle in a controlled environment, the EEG system continuously records brain activity throughout the driving session. From the raw multi-channel EEG data, relevant features are extracted that correlate with drowsiness. These features often include changes in different brainwave frequencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, asymmetry in frequency bands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, or power spectral density <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Similarly, these features are used to train a machine learning model detecting the real-time state of the driver as in either awake or drowsy state. Mardi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> demonstrated that the brain exhibits its lowest levels of activity and complexity when a driver experiences drowsiness. Consequently, individuals in such a state lose their concentration and control, thereby hindering their ability to respond promptly to stimuli.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">EEG is a valuable tool in neuroscience and clinical settings, providing insights into brain function and helping diagnose and manage various neurological conditions. Its non-invasiveness and ability to capture real-time brain activity make it a widely used method for studying brain health and understanding brain-related disorders. EEG also serves as the ’gold standard’ and is extensively applied to indicate the transition between wakefulness and sleepiness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Spontaneous alpha activity detected in EEG signals can indicate different underlying physiological process. Spontaneous alpha activity refers to rhythmic electrical oscillations in the brain that occur predominantly in the alpha frequency range, typically between 8 and 13 Hz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. According to Cantero et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, alpha activity in this range can be related to processes like wakefulness, drowsiness period, and REM sleep phase. Ogilvie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> investigated the process of falling sleep in relation to the alpha activities being recorded and associated the alpha wave disappearance to be related to stage I sleep. Kleitman and his students <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> also examined the correlation between muscle relaxation and EEG activity, observing a drop of a hand-held spool between 0.5 and 25 seconds after the alpha wave had vanished.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">How does Electrocardiogram work for drowsiness detection?</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The most effective outcomes in driver drowsiness detection have been attained through these electrode based instruments including EEG and EOG measurements to date<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, thus establishing them as the prevailing standard in this research domain. Nevertheless, implementing driver monitoring using EEG measurement proves impractical. In contrast, Electrocardiogram (ECG) recording offers a more feasible alternative, given its ease of capture, significantly larger magnitude, and lower susceptibility to noise interference.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Sleep is a complex state marked by important changes in the autonomic modulation of the cardiovascular activity as investigated by Viola et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Heart rate variability (HRV) undergoes substantial alterations across different sleep stages, reflecting a prevailing parasympathetic influence on the heart during non-rapid eye movement (NREM) sleep, while displaying heightened sympathetic activity during rapid eye movement (REM) sleep. In addition, respiration also undergoes notable changes, deepening and becoming more regular during deep sleep and shallower and more frequent during REM sleep. These effects were thoroughly investigated by Cabiddu et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and thus making the ECG measurement a well-suited technique for investigating sleep analysis and drowsiness detection.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">ECG is a medical instrument to record the electrical activity of the heart over a period of time. The ECG provides valuable information about the heart’s rhythm, rate, and overall electrical activity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. To perform an ECG, a set of electrodes is placed on the patient’s skin at specific locations. The electrodes are connected to an electrocardiograph which detects and amplifies the electric signals generated by the heart. The recorded electrical impulses are time series representing the movement of the heart’s chambers during each heartbeat. A cardiologist or a trained technician can analyze the ECG recording to identify abnormalities or irregularities that can indicate various heart conditions or problems. ECG signals can exhibit various abnormalities indicative of heart related deceases, including arrhythmia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, ST-segment abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> as indicators for hear attack, T-wave abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and QT interval prolongation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> indicative of electrolyte imbalances, QRS complex abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> indicative for bundle branch blocks, and other artifact or noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> among others.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The ECG is a non-invasive and painless measuring procedure, making it a widely used tool for diagnosing heart-related issues. In sleep studies, ECG can be used to record heart rate variations providing hints for sleep disorders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> or sleep apnea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> detection. Nowadays, simplified versions of ECGs exist, such as portable ECG devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> based on Bluetooth transmission or wearable sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> for ECG measurement, making the application of ECG in everyday life possible. There are several studies linking ECG-based features to drowsiness detection as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Most of these features are based on detecting heart rate variability and are commonly derived from the frequency and spectrum domains.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">How does a vision-based drowsiness detection system work?</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Both previous methods, which rely on biosignals, are well-suited for laboratory conditions but are not very practical in real-world driving scenarios. This is because, while driving on the road, both the movement and the dynamic environment significantly impact performance and pose challenges to extracting features from captured biosignals. Both measures would require individuals sitting still and wearing cables or head-mounted devices. Thus, we motivate for vision-based approaches overcoming the restrictions of wired installation or head-mounted devices, which might effect usability in some use-cases. Vision-based drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> refers to a technique that uses visual information, such as facial expressions and eye movements, to identify signs of drowsiness in individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. This method is commonly employed in various domains, including driver fatigue monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, operator alertness assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and other safety scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> where detecting drowsiness is crucial for safety and performance. Research linking facial muscle movement to different levels of muscle fatigue can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The pipeline of vision-based drowsiness detection often includes the following steps. During the image/video acquisition stage, a camera mostly installed on dashboards captures real-time visual data from the occupant, such as facial images or eye movement. Relevant facial features are extracted from the acquired images or video frames. Typical features involve facial landmarks, eye closures, head movements, and changes in gaze direction. These features are used to train a machine learning model often leading to a binary classification model to determine the binary states of awake or drowsiness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Vision-based drowsiness detection systems offer real-time monitoring capabilities and can be integrated into various applications, such as in-vehicle driver assistance systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> or workplace safety monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. They play a crucial role in enhancing safety and reducing the risk of accidents caused by drowsy or fatigued individuals.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Modern Applications and Methods</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we provide a comprehensive survey of recent relevant research, which is divided into three main methods for detecting drowsiness using either physiological signals or visual sampling. We distinguish between these methods based on the variables measured. ECG signals use low-dimensional heartbeat signals, EEG signals use multichannel brain activity signals, and visual feeds provide image data. Each method is summarized with a table containing the investigated research works, which is later discussed. Thereby, we consider various aspects, such as year of publication, area of use, specific algorithms developed, evaluation database and its properties, performance, and conclude with special remarks.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">EEG-based Drowsiness Detection</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">EEG signals are often used to detect the mental stress of patients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> but are also one of the physiological signals used to derive the drowsiness state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. For detecting the EEG signals, electrodes are detached from the skin directly thus allowing clear signal acquisition. Earlier works as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> applied traditional machine learning approaches with handcrafted features extracted from these physiological signals. The development in the last few years moved to more advanced approaches based on Deep Q-Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> or deep learning in general <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Traditional machine learning uses handcrafted feature extracted from the EEG power spectrum density to build efficient models for drowsiness detection. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> applied a support vector machine (SVM) on Fast Fourier Transformation (FFT) features as a binary classifier. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> investigated unique characteristics from EEG spectral patterns during micro-sleep events. Ramo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> leveraged data from five diverse EEG signal channels and employed ensemble learning techniques such as bagging to construct a robust and more precise drowsiness detection system. The efficacy of the system was validated using the DROZY database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Spatio-temporal convolution served as the cornerstone for successive deep learning-based approaches to derive both the sequential and spatial characteristic features from this physiological signal. Jeong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> extended the binary classification task of the drowsiness state to a more fine-grained classification of five drowsiness levels from EEG signals. They stated to be the first work providing such a detailed classification of drowsiness levels using only EEG signals. They acquired EEG data from ten pilots in a simulated night flight environment. They proposed a deep spatio-temporal convolutional bidirectional long short-term memory network (DSTCLN) model. The classification performance is evaluated using the Karolinska sleepiness scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> for two mental states and five drowsiness levels. Results demonstrated the feasibility of their proposed fine-grained drowsiness classification.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Similarly, Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> improved the subject-independent drowsiness recognition from single-channel EEG with an interpretable CNN-LSTM model. In this work, authors put more effort on the explainability of the proposed deep learning models by revealing the network’s decision with respect to the input data. Results showed a model average accuracy of 72.97% on 11 subjects for leave-one-out subject-independent detection on a public dataset <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Project page with access to data: https://figshare.com/articles/dataset/Multi-channel_EEG_recordings_during_a_sustained-attention_driving_task/6427334</span></span></span>. They stated that their proposed method surpasses conventional baseline methods and other SOTA deep learning methods till publication. Similar network architecture is leveraged by Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> in a more recent work. This work investigated the optimal length of input time series for more accurate detection of drowsiness at multiple levels (awake, sleep, and drowsy), while few studies have seriously considered this feature before.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Paulo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> investigated two approaches for drowsiness detection using EEG signals during a sustained-attention driving task. The study focused on pre-event time windows and addressed the challenge of cross-subject zero calibration. EEG signals are known for their low signal-to-noise ratio and individual differences between subjects, thus requiring individual calibration cycles. To tackle this issue, the researchers employed spatio-temporal image encoding representations in the form of recurrence plots for classification using deep CNN. The results obtained from a public dataset of 27 subjects showed the effectiveness of their cross-subject zero calibration approach, highlighting its success in drowsiness detection. Similarly, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> targeted the same issue by only requiring a few subject-specific calibrations to adjust for a new subject. They provided an online and multiview setup to enforce the consistencies across different views in both source and target domains, and thus, making the system in general more robust. In addition, online training makes the proposed application more suitable for practical requirements. Recent follow-up work focusing on cross-subject investigations was presented by Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. In this work, the focus further lies in the interpretability of drowsiness detection schemes and automatic feature selections from EEG signals.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">A reinforcement learning-based method for the task of drowsiness detection is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. Ming et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> leveraged deep Q-learning to analyze EEG dataset collected during simulated driving to estimate driver drowsiness state. The main research is to relate certain characteristics of the EEG data to better derive the response time in order to indirectly estimate the driver’s drowsiness state. Their results showed superior performance compared to supervised learning and is promising for real applications.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">The most current research by Zhuang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> leveraged Graph Neural Networks (GNNs) for EEG-based driver drowsiness detection in real-time. Their results surpassed the accuracy of other CNNs and graph generation methods based on drowsiness detection schemes. They proposed a GNN-based network with a self-attention mechanism that can focus on developing task-relevant connectivity networks via end-to-end learning. In addition, the authors leveraged a squeeze-and-excitation (SE) block to select the most relevant features and feature bands for drivers’ drowsiness detection. This block is shown both to improve the classification accuracy and the model’s interpretability.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">From the data augmentation point of view, Chaabene et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> introduced an EEG-based drowsiness detection system based on deep learning networks. The system follows a two-stage framework, encompassing (i) data acquisition and (ii) model analysis. For data collection, the authors employed a wearable Emotiv EPOC + headset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, recording EEG signals from 14 channels along with signal annotations. Data augmentation techniques were implemented to prevent the proposed model from overfitting. The chosen deep learning architecture in this study used a CNN network. A self-collected dataset containing 42 records of six men and eight women aged between 14 and 64 with normal mental health are used for evaluation. The outcomes exhibited a noteworthy accuracy of 90.42% in binary classification for distinguishing drowsy and awake states. Compared to alternative research, the proposed approach demonstrated its efficacy and efficiency.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">From the distributed system point of view, Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> proposed a driver’s drowsiness state detection system using EEG signals. They increased the accuracy of their system by using Federated Learning (FL) and CNN. FL is used to accumulate knowledge from the data of different clients under privacy protecting mechanism and CNN is used to identify and explain the driver’s drowsiness state. However, they evaluated their method only on a relatively small amount of private database consisting of 11 subjects.</p>
</div>
<div id="S4.SS1.p10" class="ltx_para">
<p id="S4.SS1.p10.1" class="ltx_p">Another more difficult and pressing issue is the detection of microsleep events (MSE). MSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> refers to abrupt and non-anticipated lapses of attention experienced by individuals, typically resulting from drowsiness and monotony. MSE can serve as objective indicators of excessive daytime sleepiness and can be characterized by a non-anticipated brief period of sleep lasting between 2 and 30 seconds, occurring amidst ongoing wakefulness as investigated by Carskadon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> in the Encyclopedia of sleep and dreaming. Microsleep accounts for an annual loss of nearly 150 million dollars due to diminished daily work performance and vehicular accidents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. Assessing an individual’s level of sleepiness and detecting the onset of microsleep is thus crucial for tasks demanding sustained focus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, such as driving or operating machinery during nighttime hours, where falling asleep poses high risks. In recent years, this subject has received widespread attention from governmental bodies, the public, and the research community alike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>.</p>
</div>
<div id="S4.SS1.p11" class="ltx_para">
<p id="S4.SS1.p11.1" class="ltx_p">Such subtle events like microsleep episodes are very hard to recognize. The group of Golz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> worked extensively on detecting microsleep episodes from EEG and EOG data. To achieve detection, signals coming from heterogeneous sources are processed, such as the brain electric activity captured by EEG data, variation in the pupil size, and eye and eyelid movements captured by EOG data. By combining the spectral and the state space, both linear and non-linear features are considered. The binary decision networks between MSE and non-MSE are based on a support vector machines (SMV) and a learning vector quantization (LVQ) scheme. However, pupil adaptation through light stimuli could affect the accuracy of detection.</p>
</div>
<div id="S4.SS1.p12" class="ltx_para">
<p id="S4.SS1.p12.1" class="ltx_p">Pham et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> proposed a more flexible and mobile application by introducing WAKE, which is a behind-the-ear wearable device for microsleep detection. They utilized bone-conduction headphones to gather biosignals including brain activity, eye movements, facial muscle contractions, and galvanic responses from the area behind the user’s ears. Their findings demonstrated that WAKE effectively suppressed motion and environmental noise in real-time by 9.74-19.74 dB during activities such as walking, driving, or being in various environments, ensuring reliable capture of the biosignals. A preliminary training conducted on 19 sleep-deprived and narcoleptic subjects demonstrated an average precision and recall of 76% and 85% on an unseen subject with leave one subject out cross validation technique.</p>
</div>
<div id="S4.SS1.p13" class="ltx_para">
<p id="S4.SS1.p13.1" class="ltx_p">A way to detect microsleep with deep learning architectures even with less training data is proposed by Chougule et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. Their model uses the attention-based mechanism, which combines the advantages of the wavelet transform with the Short Time Fourier Transform (STFT) Spectogram. By separating ”time-dependent” and ”time-independent” parts, the deep learning model is more robust to capture both the sequence features and simultaneously learn the relationships between epochs. Only a single electrode EEG signal was used to achieve greater social acceptability. The training and evaluation are performed on the public Maintenance of Wakefulness Test (MWT) dataset <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>MWT dataset: https://sites.google.com/view/utarldd/home</span></span></span>.</p>
</div>
<div id="S4.SS1.p14" class="ltx_para">
<p id="S4.SS1.p14.1" class="ltx_p">Table <a href="#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the investigated works using EEG signals for drowsiness detection in various application scenarios. Sensing modality covers both wired and wireless wearable devices targeting non-intrusive applications. References to the public databases are provided in the footnote. From Table <a href="#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we notice that all recent works starting from 2021 utilized deep learning-based approaches to mitigate the investigation of handcrafted features. We further observed that the most predominant evaluation dataset for physiological signal-based drowsiness detection is the DROZY dataset.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>summarizes recent works performing drowsiness detection based on EEG signals.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:337.1pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-325.6pt,252.9pt) scale(0.399681303831824,0.399681303831824) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">work</span></td>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">year</span></td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">area of use</span></td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">algorithm</span></td>
<td id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">database</span></td>
<td id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">subjects/session</span></td>
<td id="S4.T1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">performance</span></td>
<td id="S4.T1.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.2.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">remarks</span></td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Golz et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib85" title="" class="ltx_ref">85</a><span id="S4.T1.1.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">2007</span></td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">microsleep events</span></td>
<td id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">support vector machine</span></td>
</tr>
<tr id="S4.T1.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.4.1.2.1.1" class="ltx_text" style="font-size:90%;">Learning vector quantization</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.6.1" class="ltx_text" style="font-size:90%;">23 subjects</span></td>
<td id="S4.T1.1.1.3.2.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.7.1" class="ltx_text" style="font-size:90%;">test errors = 9%</span></td>
<td id="S4.T1.1.1.3.2.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.3.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.3.2.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.3.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Biosignals from EEG and variations in</span></td>
</tr>
<tr id="S4.T1.1.1.3.2.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.8.1.2.1.1" class="ltx_text" style="font-size:90%;">pupil size and eye movements</span></td>
</tr>
<tr id="S4.T1.1.1.3.2.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.3.2.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.3.2.8.1.3.1.1" class="ltx_text" style="font-size:90%;">simulated driving, sleep deprivated subjects</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.T1.1.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">2017</span></td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.4.3.3.1.1" class="ltx_tr">
<td id="S4.T1.1.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">high-speed train</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.3.1.2" class="ltx_tr">
<td id="S4.T1.1.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.3.1.2.1.1" class="ltx_text" style="font-size:90%;">operators</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">SVM builds on FFT</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.4.1.2.1.1" class="ltx_text" style="font-size:90%;">features extracted from</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.4.1.3" class="ltx_tr">
<td id="S4.T1.1.1.4.3.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.4.1.3.1.1" class="ltx_text" style="font-size:90%;">EEG power spectrum</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.6.1" class="ltx_text" style="font-size:90%;">10 drivers</span></td>
<td id="S4.T1.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.4.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.4.3.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.4.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.7.1.1.1.1" class="ltx_text" style="font-size:90%;">precision=90.79%,</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.4.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.7.1.2.1.1" class="ltx_text" style="font-size:90%;">sensitivity=86.80%</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.4.3.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.4.3.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.4.3.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.4.3.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.8.1.1.1.1" class="ltx_text" style="font-size:90%;">awake/drowsy, wearable</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.4.3.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.8.1.2.1.1" class="ltx_text" style="font-size:90%;">BCI model for EEGs,</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.4.3.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.8.1.3.1.1" class="ltx_text" style="font-size:90%;">wireless data transmission,</span></td>
</tr>
<tr id="S4.T1.1.1.4.3.8.1.4" class="ltx_tr">
<td id="S4.T1.1.1.4.3.8.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.4.3.8.1.4.1.1" class="ltx_text" style="font-size:90%;">virtual driving environment</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Jeong et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S4.T1.1.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">aviation, pilots</span></td>
<td id="S4.T1.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.5.4.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">deep spatio-temporal</span></td>
</tr>
<tr id="S4.T1.1.1.5.4.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.4.1.2.1.1" class="ltx_text" style="font-size:90%;">convolution</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.5.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.6.1" class="ltx_text" style="font-size:90%;">9 M, 1 F</span></td>
<td id="S4.T1.1.1.5.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.5.4.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.5.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.7.1.1.1.1" class="ltx_text" style="font-size:90%;">2 states Acc = 0.87</span></td>
</tr>
<tr id="S4.T1.1.1.5.4.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.5.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.7.1.2.1.1" class="ltx_text" style="font-size:90%;">5 levels Acc = 0.69</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.5.4.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.5.4.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.5.4.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.5.4.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.8.1.1.1.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.5.4.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.5.4.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.5.4.8.1.2.1.1" class="ltx_text" style="font-size:90%;">more fine-grained classification</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T1.1.1.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S4.T1.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.6.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.6.5.3.1.1" class="ltx_tr">
<td id="S4.T1.1.1.6.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.3.1.1.1.1" class="ltx_text" style="font-size:90%;">aviation, public</span></td>
</tr>
<tr id="S4.T1.1.1.6.5.3.1.2" class="ltx_tr">
<td id="S4.T1.1.1.6.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.3.1.2.1.1" class="ltx_text" style="font-size:90%;">transportation</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.6.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.6.5.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.6.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">handcrafted features</span></td>
</tr>
<tr id="S4.T1.1.1.6.5.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.6.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.4.1.2.1.1" class="ltx_text" style="font-size:90%;">from EEG, EOG, and</span></td>
</tr>
<tr id="S4.T1.1.1.6.5.4.1.3" class="ltx_tr">
<td id="S4.T1.1.1.6.5.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.4.1.3.1.1" class="ltx_text" style="font-size:90%;">facial data</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.6.1" class="ltx_text" style="font-size:90%;">16 pilots</span></td>
<td id="S4.T1.1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.7.1" class="ltx_text" style="font-size:90%;">find operational features</span></td>
<td id="S4.T1.1.1.6.5.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.6.5.8.1" class="ltx_text" style="font-size:90%;">aviation headset equipped with sensors</span></td>
</tr>
<tr id="S4.T1.1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.7.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.7.6.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.7.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Natnithikarat</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.1.1.2" class="ltx_tr">
<td id="S4.T1.1.1.7.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.7.6.1.1.2.1.1" class="ltx_text" style="font-size:90%;">et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.7.6.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S4.T1.1.1.7.6.1.1.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S4.T1.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">office employees</span></td>
<td id="S4.T1.1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.7.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.7.6.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.7.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.4.1.1.1.1" class="ltx_text" style="font-size:90%;">linear regression task,</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.7.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.4.1.2.1.1" class="ltx_text" style="font-size:90%;">PCA, SVM</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.7.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.7.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.7.6.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.7.6.6.1.1" class="ltx_tr">
<td id="S4.T1.1.1.7.6.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.6.1.1.1.1" class="ltx_text" style="font-size:90%;">18 (15 M, 3 F),</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.6.1.2" class="ltx_tr">
<td id="S4.T1.1.1.7.6.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.6.1.2.1.1" class="ltx_text" style="font-size:90%;">1h office work</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.7.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.7.6.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.7.1.1.1.1" class="ltx_text" style="font-size:90%;">keystroke, mouse move</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.7.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.7.1.2.1.1" class="ltx_text" style="font-size:90%;">and self-evaluated KSS</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.7.1.3" class="ltx_tr">
<td id="S4.T1.1.1.7.6.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.7.1.3.1.1" class="ltx_text" style="font-size:90%;">correlation to drowsiness</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.7.6.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.7.6.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.7.6.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.7.6.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.8.1.1.1.1" class="ltx_text" style="font-size:90%;">keyboard, mouse events,</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.7.6.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.8.1.2.1.1" class="ltx_text" style="font-size:90%;">eye-tracking, EEG+ECG</span></td>
</tr>
<tr id="S4.T1.1.1.7.6.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.7.6.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.7.6.8.1.3.1.1" class="ltx_text" style="font-size:90%;">as reference</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Pham et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib90" title="" class="ltx_ref">90</a><span id="S4.T1.1.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">2020</span></td>
<td id="S4.T1.1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">microsleep events</span></td>
<td id="S4.T1.1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.8.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.8.7.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.8.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.4.1.1.1.1" class="ltx_text" style="font-size:90%;">feature engineering + classifiers</span></td>
</tr>
<tr id="S4.T1.1.1.8.7.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.8.7.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.4.1.2.1.1" class="ltx_text" style="font-size:90%;">deep learnning on raw data</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.8.7.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.8.7.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.6.1" class="ltx_text" style="font-size:90%;">19 subjects</span></td>
<td id="S4.T1.1.1.8.7.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.8.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.8.7.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.8.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Avg precision = 76%</span></td>
</tr>
<tr id="S4.T1.1.1.8.7.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.8.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.7.1.2.1.1" class="ltx_text" style="font-size:90%;">recall=85%</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.8.7.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.8.7.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.8.7.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.8.7.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Headphones as wearable design</span></td>
</tr>
<tr id="S4.T1.1.1.8.7.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.8.7.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.8.7.8.1.2.1.1" class="ltx_text" style="font-size:90%;">noise mitigation and uses EEG+EOG+EMG</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Paulo et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.9.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="S4.T1.1.1.9.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T1.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.9.8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.9.8.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.9.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.4.1.1.1.1" class="ltx_text" style="font-size:90%;">spatio-temporal en-</span></td>
</tr>
<tr id="S4.T1.1.1.9.8.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.9.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.4.1.2.1.1" class="ltx_text" style="font-size:90%;">coding CNN classifier</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.6.1" class="ltx_text" style="font-size:90%;">27 subjects</span></td>
<td id="S4.T1.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.7.1" class="ltx_text" style="font-size:90%;">LOO-CV Acc=75.87%</span></td>
<td id="S4.T1.1.1.9.8.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.9.8.8.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.10.9.1.1" class="ltx_text" style="font-size:90%;">Chaabene et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.10.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a><span id="S4.T1.1.1.10.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T1.1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.10.9.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.10.9.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.10.9.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.10.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.4.1.1.1.1" class="ltx_text" style="font-size:90%;">DL-based two-stage,</span></td>
</tr>
<tr id="S4.T1.1.1.10.9.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.10.9.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.4.1.2.1.1" class="ltx_text" style="font-size:90%;">networks</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.10.9.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.10.9.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.10.9.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.10.9.6.1.1" class="ltx_tr">
<td id="S4.T1.1.1.10.9.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.6.1.1.1.1" class="ltx_text" style="font-size:90%;">6 M, 8 F,</span></td>
</tr>
<tr id="S4.T1.1.1.10.9.6.1.2" class="ltx_tr">
<td id="S4.T1.1.1.10.9.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.6.1.2.1.1" class="ltx_text" style="font-size:90%;">42 records</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.10.9.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.7.1" class="ltx_text" style="font-size:90%;">Acc=90.42%</span></td>
<td id="S4.T1.1.1.10.9.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.10.9.8.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.11.10.1.1" class="ltx_text" style="font-size:90%;">Cui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.11.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="S4.T1.1.1.11.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.11.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T1.1.1.11.10.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.11.10.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.4.1" class="ltx_text" style="font-size:90%;">CNN-LSTM model</span></td>
<td id="S4.T1.1.1.11.10.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.11.10.5.1" class="ltx_text" style="font-size:90%;">public</span><span id="S4.T1.1.1.11.10.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T1.1.1.11.10.5.3" class="ltx_text" style="font-size:90%;">fn:eeg</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.11.10.5.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib92" title="" class="ltx_ref">92</a><span id="S4.T1.1.1.11.10.5.5.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.11.10.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.6.1" class="ltx_text" style="font-size:90%;">11 subjects</span></td>
<td id="S4.T1.1.1.11.10.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.7.1" class="ltx_text" style="font-size:90%;">Avg Acc=72.97% (LOO)</span></td>
<td id="S4.T1.1.1.11.10.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.11.10.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.11.10.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.11.10.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.8.1.1.1.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.11.10.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.11.10.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.11.10.8.1.2.1.1" class="ltx_text" style="font-size:90%;">cross subject recognition</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.12.11" class="ltx_tr">
<td id="S4.T1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.12.11.1.1" class="ltx_text" style="font-size:90%;">Cui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.12.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S4.T1.1.1.12.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.12.11.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T1.1.1.12.11.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.12.11.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.4.1" class="ltx_text" style="font-size:90%;">interpretable CNN</span></td>
<td id="S4.T1.1.1.12.11.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.12.11.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.6.1" class="ltx_text" style="font-size:90%;">11 subjects</span></td>
<td id="S4.T1.1.1.12.11.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.7.1" class="ltx_text" style="font-size:90%;">Avg Acc=78.35% (LOO)</span></td>
<td id="S4.T1.1.1.12.11.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.12.11.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.12.11.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.12.11.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.8.1.1.1.1" class="ltx_text" style="font-size:90%;">automatic feature selection from EEG features;</span></td>
</tr>
<tr id="S4.T1.1.1.12.11.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.12.11.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.12.11.8.1.2.1.1" class="ltx_text" style="font-size:90%;">cross subject recognition</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.13.12" class="ltx_tr">
<td id="S4.T1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.13.12.1.1" class="ltx_text" style="font-size:90%;">Ming et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.13.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib75" title="" class="ltx_ref">75</a><span id="S4.T1.1.1.13.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.13.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T1.1.1.13.12.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.13.12.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.4.1" class="ltx_text" style="font-size:90%;">Deep Q-Learning</span></td>
<td id="S4.T1.1.1.13.12.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.13.12.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.6.1" class="ltx_text" style="font-size:90%;">37 subjects</span></td>
<td id="S4.T1.1.1.13.12.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.13.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.13.12.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.13.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.7.1.1.1.1" class="ltx_text" style="font-size:90%;">use DQN to derive</span></td>
</tr>
<tr id="S4.T1.1.1.13.12.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.13.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.7.1.2.1.1" class="ltx_text" style="font-size:90%;">response time from</span></td>
</tr>
<tr id="S4.T1.1.1.13.12.7.1.3" class="ltx_tr">
<td id="S4.T1.1.1.13.12.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.7.1.3.1.1" class="ltx_text" style="font-size:90%;">EEG data</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.13.12.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.13.12.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.13.12.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.13.12.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Relate EEG characteristics to</span></td>
</tr>
<tr id="S4.T1.1.1.13.12.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.13.12.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.8.1.2.1.1" class="ltx_text" style="font-size:90%;">response time to indrectly</span></td>
</tr>
<tr id="S4.T1.1.1.13.12.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.13.12.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.13.12.8.1.3.1.1" class="ltx_text" style="font-size:90%;">estimate drowsiness states.</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Ramos et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T1.1.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">sectors engaged in</span></td>
</tr>
<tr id="S4.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.4.1.2.1.1" class="ltx_text" style="font-size:90%;">safety critical end-</span></td>
</tr>
<tr id="S4.T1.1.1.1.4.1.3" class="ltx_tr">
<td id="S4.T1.1.1.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.4.1.3.1.1" class="ltx_text" style="font-size:90%;">eavors, oil&amp;gas</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1.5.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">multiple channel EEGs,</span></td>
</tr>
<tr id="S4.T1.1.1.1.5.1.2" class="ltx_tr">
<td id="S4.T1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.5.1.2.1.1" class="ltx_text" style="font-size:90%;">ensemble machine</span></td>
</tr>
<tr id="S4.T1.1.1.1.5.1.3" class="ltx_tr">
<td id="S4.T1.1.1.1.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.5.1.3.1.1" class="ltx_text" style="font-size:90%;">learning</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.1.6.1" class="ltx_text" style="font-size:90%;">DROZY</span><span id="S4.T1.1.1.1.6.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T1.1.1.1.6.3" class="ltx_text" style="font-size:90%;">fn:drozy</span>
</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.7.1" class="ltx_text" style="font-size:90%;">14 subjects</span></td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Accuracy</span><math id="S4.T1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.1.m1.1a"><mo mathsize="90%" id="S4.T1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.1.m1.1b"><geq id="S4.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.m1.1c">\geq</annotation></semantics></math><span id="S4.T1.1.1.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;"> 90% for</span>
</td>
</tr>
<tr id="S4.T1.1.1.1.1.1.2" class="ltx_tr">
<td id="S4.T1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.1.1.2.1.1" class="ltx_text" style="font-size:90%;">specific subjects and</span></td>
</tr>
<tr id="S4.T1.1.1.1.1.1.3" class="ltx_tr">
<td id="S4.T1.1.1.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.1.1.3.1.1" class="ltx_text" style="font-size:90%;">dedicated models</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.8.1.1.1.1" class="ltx_text" style="font-size:90%;">awake/drowsy,</span></td>
</tr>
<tr id="S4.T1.1.1.1.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.8.1.2.1.1" class="ltx_text" style="font-size:90%;">considered different setups</span></td>
</tr>
<tr id="S4.T1.1.1.1.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.1.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.1.8.1.3.1.1" class="ltx_text" style="font-size:90%;">for evaluation.</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T1.1.1.14.13" class="ltx_tr">
<td id="S4.T1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.14.13.1.1" class="ltx_text" style="font-size:90%;">Chougule et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.14.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib91" title="" class="ltx_ref">91</a><span id="S4.T1.1.1.14.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.14.13.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T1.1.1.14.13.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.3.1" class="ltx_text" style="font-size:90%;">microsleep events</span></td>
<td id="S4.T1.1.1.14.13.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.14.13.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.14.13.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.14.13.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.4.1.1.1.1" class="ltx_text" style="font-size:90%;">attention-based method</span></td>
</tr>
<tr id="S4.T1.1.1.14.13.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.14.13.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.4.1.2.1.1" class="ltx_text" style="font-size:90%;">combine STFT+Wavelets</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.14.13.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.14.13.5.1" class="ltx_text" style="font-size:90%;">MWT dataset </span><span id="S4.T1.1.1.14.13.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T1.1.1.14.13.5.3" class="ltx_text" style="font-size:90%;">fn:mwt</span>
</td>
<td id="S4.T1.1.1.14.13.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.6.1" class="ltx_text" style="font-size:90%;">64 (27M,37F)</span></td>
<td id="S4.T1.1.1.14.13.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.14.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.14.13.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.14.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.7.1.1.1.1" class="ltx_text" style="font-size:90%;">train acc=92%</span></td>
</tr>
<tr id="S4.T1.1.1.14.13.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.14.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.7.1.2.1.1" class="ltx_text" style="font-size:90%;">test acc=89.9%</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.14.13.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.14.13.8.1" class="ltx_text" style="font-size:90%;">One electrode EEG for more user acceptance</span></td>
</tr>
<tr id="S4.T1.1.1.15.14" class="ltx_tr">
<td id="S4.T1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.15.14.1.1" class="ltx_text" style="font-size:90%;">Qin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.15.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T1.1.1.15.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.15.14.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T1.1.1.15.14.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.15.14.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.15.14.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.15.14.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.15.14.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.4.1.1.1.1" class="ltx_text" style="font-size:90%;">federated learning</span></td>
</tr>
<tr id="S4.T1.1.1.15.14.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.15.14.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.4.1.2.1.1" class="ltx_text" style="font-size:90%;">CNN classifier</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.15.14.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.15.14.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.6.1" class="ltx_text" style="font-size:90%;">11 subjects</span></td>
<td id="S4.T1.1.1.15.14.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.15.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.15.14.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.15.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.7.1.1.1.1" class="ltx_text" style="font-size:90%;">avg Acc=73.56%</span></td>
</tr>
<tr id="S4.T1.1.1.15.14.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.15.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.7.1.2.1.1" class="ltx_text" style="font-size:90%;">F1-score=73.26%</span></td>
</tr>
<tr id="S4.T1.1.1.15.14.7.1.3" class="ltx_tr">
<td id="S4.T1.1.1.15.14.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.7.1.3.1.1" class="ltx_text" style="font-size:90%;">AUC=78.23%</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.15.14.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.15.14.8.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.16.15" class="ltx_tr">
<td id="S4.T1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.16.15.1.1" class="ltx_text" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.16.15.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="S4.T1.1.1.16.15.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.16.15.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T1.1.1.16.15.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T1.1.1.16.15.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.16.15.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.16.15.4.1.1" class="ltx_tr">
<td id="S4.T1.1.1.16.15.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Graph Neural network</span></td>
</tr>
<tr id="S4.T1.1.1.16.15.4.1.2" class="ltx_tr">
<td id="S4.T1.1.1.16.15.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.4.1.2.1.1" class="ltx_text" style="font-size:90%;">with attention</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.16.15.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.5.1" class="ltx_text" style="font-size:90%;">public</span></td>
<td id="S4.T1.1.1.16.15.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.16.15.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.16.15.6.1.1" class="ltx_tr">
<td id="S4.T1.1.1.16.15.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.6.1.1.1.1" class="ltx_text" style="font-size:90%;">27 subjects</span></td>
</tr>
<tr id="S4.T1.1.1.16.15.6.1.2" class="ltx_tr">
<td id="S4.T1.1.1.16.15.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.6.1.2.1.1" class="ltx_text" style="font-size:90%;">62+ sessions</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.16.15.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.16.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.16.15.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.16.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Acc=72.6%</span></td>
</tr>
<tr id="S4.T1.1.1.16.15.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.16.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.7.1.2.1.1" class="ltx_text" style="font-size:90%;">F1 = 70.7%</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.16.15.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.16.15.8.1" class="ltx_text" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr id="S4.T1.1.1.17.16" class="ltx_tr">
<td id="S4.T1.1.1.17.16.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T1.1.1.17.16.1.1" class="ltx_text" style="font-size:90%;">Lee et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.17.16.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a><span id="S4.T1.1.1.17.16.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.1.1.17.16.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T1.1.1.17.16.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.3.1" class="ltx_text" style="font-size:90%;">drowsiness detection</span></td>
<td id="S4.T1.1.1.17.16.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.4.1" class="ltx_text" style="font-size:90%;">LSTM-CNN model</span></td>
<td id="S4.T1.1.1.17.16.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T1.1.1.17.16.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.6.1" class="ltx_text" style="font-size:90%;">19 subjects</span></td>
<td id="S4.T1.1.1.17.16.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.17.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.17.16.7.1.1" class="ltx_tr">
<td id="S4.T1.1.1.17.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.7.1.1.1.1" class="ltx_text" style="font-size:90%;">F1=95% (4000ms)</span></td>
</tr>
<tr id="S4.T1.1.1.17.16.7.1.2" class="ltx_tr">
<td id="S4.T1.1.1.17.16.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Acc=85.6% (500ms)</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.1.17.16.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T1.1.1.17.16.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.17.16.8.1.1" class="ltx_tr">
<td id="S4.T1.1.1.17.16.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.8.1.1.1.1" class="ltx_text" style="font-size:90%;">multistage consciousness</span></td>
</tr>
<tr id="S4.T1.1.1.17.16.8.1.2" class="ltx_tr">
<td id="S4.T1.1.1.17.16.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.8.1.2.1.1" class="ltx_text" style="font-size:90%;">(awake, sleep, drowsiness)</span></td>
</tr>
<tr id="S4.T1.1.1.17.16.8.1.3" class="ltx_tr">
<td id="S4.T1.1.1.17.16.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T1.1.1.17.16.8.1.3.1.1" class="ltx_text" style="font-size:90%;">auditory stimuli and button responses</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">ECG-based Drowsiness Detection</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The ECG is another measurement on the skin to record the heartbeat variability. This physiological signal is also often applied for driver drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. Different to multi-channel EEG signals, ECG does not need to be placed on the scalp, thus can be more suitable for drowsiness detection under more relaxed and natural conditions. Interestingly, the trend goes beyond the development of the algorithms and also affects the design of the sensors. The latest trend shows a shift from traditional, stationary medical placement to a more convenient, and wearable design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Takalokastari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> carried out real-time drowsiness detection utilizing a wireless sensor node connected to a wearable ECG sensor. They build a binary classification method based on extracted features from the ECG signal, which was sampled at 100Hz, to distinguish between awake and drowsy states. The wireless transmission of data facilitated the forwarding of information to a server PC. Notably, the QRS complex in the ECG signal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> offered valuable features that could aid in the diagnosis of various cardiovascular conditions. The process of drowsiness detection often involves the analysis of R peaks, R-R intervals, the interval between R and S peaks, and the duration of the QRS complex. Another work by Martins et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> conducted a comprehensive review that examined the latest research on fatigue detection and monitoring using wearable devices. Wearable devices offer a significant advantage by facilitating continuous and long-term monitoring of biomedical signals with comfort and non-intrusiveness. However, the study also identified distinct challenges associated with using wearable devices for fatigue monitoring.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Later, Shebakova focused on the inter-person variability of the detection scheme. Sherbakova et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> performed a thorough analysis of ECG signal for driver drowsiness detection. The authors stated that the threshold of drowsiness based purely on individual’s heart rate can vary for different people. Possible causes that could affect the threshold of drowsiness include the individual’s current postures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> or different cognitive states <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> among other factors. Relevant works showed that the heart rate is significantly different when individuals are supine or upright <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. Difference in heart rate is also observable during the sleep phase in a high or low worrier state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. However, research demonstrated that parameters of HRV change over time depend on the current state (i.e. during wakefulness, drowsiness, and stress). Therefore, the authors suggested using the analysis of three ECG parameters, including heart rate (HR), LF/HF, and the Baevsky stress index <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> as robust indications for drowsiness detection. GPRS data transmission allows the processing and storage of ECG signals on a powerful server.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.2" class="ltx_p">Ke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> proposed a drowsiness detection system using heartbeat detection from Android-based handheld devices. ECG signal acquired from a sensor is first transferred via Bluetooth to an Android device. The system extracted meaningful information from the ECG signal and indicative features are calculated from the power ratio after applying the hamming window and the Fourier transformation. Data was collected from a male and female test subject both in the awake state as well as in the asleep state. Evaluation results revealed a correlation between the state of drowsiness with a decreasing trend in the ratio (LF/HF). LF band stands for low-frequency component ranging from 0.04<math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mo id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\sim</annotation></semantics></math>0.15Hz and HF band stands for high-frequency component ranging from 0.15<math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mo id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\sim</annotation></semantics></math>0.4Hz. Each controls certain functionalities in the vegetative nervous system.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">A more extended investigation of ECG-based drowsiness detection study was conducted by Fujiwara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>. They proposed a detection algorithm based on heart rate variability (HRV) analysis and validated their method by comparing it with EEG-based sleep assessment. Eight features of heart rate variability are monitored to detect known abnormalities in the signal. During the experimental phase, data were collected from 34 participants in a driving simulator and their sleep stages were labeled by a sleep specialist. Results show that sleepiness was detected in 12 of 13 pre-N1 episodes before sleep onset.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">The work by Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> introduced a deep learning based approach for drowsiness detection. The authors investigated the robust and deterministic pattern of HRV signals collected from wearable ECG or photolethysmogram (PPG) sensors for driver drowsiness detection. Challenges of using wearable adds additional moving artifacts to collected time series. These motion artefacts can be alleviated. Three types of recurrence plots are generated as input features to a CNN for the binary classification of drowsy and awake state. An experimental dataset was collected under a virtual driving environment to evaluate the proposed measures.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">With a notable surge in motorcycle traffic accidents, frequently leading to serious consequences and a significant loss of life, research on driver drowsiness detection for motorcyclists becomes more relevant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>. Motorcyclists are often more vulnerable compared to car drivers in case of accidents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>. To address this concern, Fahrurrasyid et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> introduced an innovative solution: a smart helmet integrated with various sensors. These sensors monitor psychological signals, including heartbeats, alongside a GPS module, GSM module, and alert push notifications. The study’s experiment, involving 10 participants, demonstrated the helmet’s capability to identify drowsiness and send alerts when the heart-rate drops below 60 bpm. This data is accessible in real-time, while the helmet also employs the Google Maps application to track the precise location of the incident.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">Latest interesting work by Heydari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> introduced a technique to identify driver drowsiness by monitoring the pulse rate variability (PRV) measured on a finger. They analyzed finger pulse data which are derived from PPG signals, focusing on features within the pulse rate variability that exhibit notable changes during drowsiness. Findings reveal that the variability values, along with their averages, increased before the onset of sleepiness. Additionally, it was observed that the standard deviation of all peak-to-peak intervals notably decreases during drowsiness. Also, an increase in the values of the Root Mean Square of Successive Differences (RMSSD) is observed during the drowsiness stage. The authors suggested a purely conceptual design to integrate PPG sensors into a steering wheel to detect the driver’s finger pulse rate, offering a viable and non-invasive means for detecting driver drowsiness.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">Due to the subtlety of microsleeps, these events are usually recorded using EEG or EOG signals from subtle eye muscle movements. Towards that, Lenis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> proposed a work investigating MSEs during a car driving simulation using ECG features. In this work, morphological and rhythmical features before and after a MSE are extracted from the ECG signals and analyzed towards baseline. The findings suggested that detecting (or predicting) MSE solely based on the ECG is not feasible. However, when MSE is present, noticeable differences in both the rhythmic and morphological features were observed compared to those calculated for the reference signal in the absence of sleepiness.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">Table <a href="#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> summarizes the investigated works using ECG signals for drowsiness detection in various application scenarios. Sensing modality covers both wired and wireless wearable devices targeting non-intrusive applications. References to the public databases are provided in the footnote if available. From Table <a href="#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we notice that beyond the development of algorithmic choices, i.e., starting from a more heuristic pattern generation to more advanced recurrent and CNN-based methods, the trend also goes to the evaluation of larger groups with more subjects and in the design choices of more flexible and wearable sensors introduce their own individual pros and cons.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>summarizes recent works performing drowsiness detection based on ECG signals.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:199.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-305.5pt,140.5pt) scale(0.41513531239335,0.41513531239335) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">work</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">year</span></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">area of use</span></th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">algorithm</span></th>
<th id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">database</span></th>
<th id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">subjects/session</span></th>
<th id="S4.T2.1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">performance</span></th>
<th id="S4.T2.1.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">remarks</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Takalokastari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S4.T2.1.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">2011</span></td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.2.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.2.1.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.2.1.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.4.1.2.1.1" class="ltx_text" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.7.1" class="ltx_text" style="font-size:90%;">find operation thresholds</span></td>
<td id="S4.T2.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.2.1.8.1" class="ltx_text" style="font-size:90%;">wireless sensor node, wearable ECG</span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Sherbakova et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T2.1.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">2015</span></td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">public transportation</span></td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.4.1.2.1.1" class="ltx_text" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.1.3.2.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.7.1" class="ltx_text" style="font-size:90%;">find operation thresholds</span></td>
<td id="S4.T2.1.1.3.2.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.3.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.3.2.8.1.1" class="ltx_tr">
<td id="S4.T2.1.1.3.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.8.1.1.1.1" class="ltx_text" style="font-size:90%;">portable device for 1 lead ECG,</span></td>
</tr>
<tr id="S4.T2.1.1.3.2.8.1.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.3.2.8.1.2.1.1" class="ltx_text" style="font-size:90%;">wireless data transmission</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Ke et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib101" title="" class="ltx_ref">101</a><span id="S4.T2.1.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">2016</span></td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">HRV, FFT</span></td>
</tr>
<tr id="S4.T2.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.4.1.2.1.1" class="ltx_text" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.4.3.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.4.3.6.1.1" class="ltx_tr">
<td id="S4.T2.1.1.4.3.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.6.1.1.1.1" class="ltx_text" style="font-size:90%;">1 M, 1 F,</span></td>
</tr>
<tr id="S4.T2.1.1.4.3.6.1.2" class="ltx_tr">
<td id="S4.T2.1.1.4.3.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.6.1.2.1.1" class="ltx_text" style="font-size:90%;">120 minutes</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.7.1" class="ltx_text" style="font-size:90%;">find operation thresholds</span></td>
<td id="S4.T2.1.1.4.3.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.4.3.8.1" class="ltx_text" style="font-size:90%;">handheld devices, wireless transmission</span></td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Lenis et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib107" title="" class="ltx_ref">107</a><span id="S4.T2.1.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">2016</span></td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">microsleep event</span></td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.5.4.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">ECG features before</span></td>
</tr>
<tr id="S4.T2.1.1.5.4.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.4.1.2.1.1" class="ltx_text" style="font-size:90%;">and after MSE</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.5.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.5.4.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.5.4.6.1.1" class="ltx_tr">
<td id="S4.T2.1.1.5.4.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.6.1.1.1.1" class="ltx_text" style="font-size:90%;">7 subjects;</span></td>
</tr>
<tr id="S4.T2.1.1.5.4.6.1.2" class="ltx_tr">
<td id="S4.T2.1.1.5.4.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.6.1.2.1.1" class="ltx_text" style="font-size:90%;">14 records</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.5.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.1.5.4.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.5.4.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.5.4.8.1.1" class="ltx_tr">
<td id="S4.T2.1.1.5.4.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.8.1.1.1.1" class="ltx_text" style="font-size:90%;">finding significant changes</span></td>
</tr>
<tr id="S4.T2.1.1.5.4.8.1.2" class="ltx_tr">
<td id="S4.T2.1.1.5.4.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.8.1.2.1.1" class="ltx_text" style="font-size:90%;">in heart rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.5.4.8.1.3" class="ltx_tr">
<td id="S4.T2.1.1.5.4.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.5.4.8.1.3.1.1" class="ltx_text" style="font-size:90%;">around MSE</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Fujiwara et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib102" title="" class="ltx_ref">102</a><span id="S4.T2.1.1.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">2018</span></td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.6.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.6.5.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.6.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.6.5.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.6.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.4.1.2.1.1" class="ltx_text" style="font-size:90%;">handcrafted features</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.6.1" class="ltx_text" style="font-size:90%;">34 subjects</span></td>
<td id="S4.T2.1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.6.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.6.5.7.1.1" class="ltx_tr">
<td id="S4.T2.1.1.6.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.7.1.1.1.1" class="ltx_text" style="font-size:90%;">12 out of 13 pre-N1</span></td>
</tr>
<tr id="S4.T2.1.1.6.5.7.1.2" class="ltx_tr">
<td id="S4.T2.1.1.6.5.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.7.1.2.1.1" class="ltx_text" style="font-size:90%;">episodes prior sleep</span></td>
</tr>
<tr id="S4.T2.1.1.6.5.7.1.3" class="ltx_tr">
<td id="S4.T2.1.1.6.5.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.7.1.3.1.1" class="ltx_text" style="font-size:90%;">onsets detected</span></td>
</tr>
<tr id="S4.T2.1.1.6.5.7.1.4" class="ltx_tr">
<td id="S4.T2.1.1.6.5.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.7.1.4.1.1" class="ltx_text" style="font-size:90%;">FP=1.7 times per hour</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.6.5.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.6.5.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.6.5.8.1.1" class="ltx_tr">
<td id="S4.T2.1.1.6.5.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.8.1.1.1.1" class="ltx_text" style="font-size:90%;">data labeled by sleep specialist</span></td>
</tr>
<tr id="S4.T2.1.1.6.5.8.1.2" class="ltx_tr">
<td id="S4.T2.1.1.6.5.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.6.5.8.1.2.1.1" class="ltx_text" style="font-size:90%;">detection of sleep onsets</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Lee et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.7.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.T2.1.1.7.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.7.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.7.6.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.7.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Recurrent methods</span></td>
</tr>
<tr id="S4.T2.1.1.7.6.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.7.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.4.1.2.1.1" class="ltx_text" style="font-size:90%;">CNN classification</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.7.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.7.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.7.6.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.7.6.6.1.1" class="ltx_tr">
<td id="S4.T2.1.1.7.6.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.6.1.1.1.1" class="ltx_text" style="font-size:90%;">6 subjects,</span></td>
</tr>
<tr id="S4.T2.1.1.7.6.6.1.2" class="ltx_tr">
<td id="S4.T2.1.1.7.6.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.6.1.2.1.1" class="ltx_text" style="font-size:90%;">22 recordings</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.7.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.7.6.7.1.1" class="ltx_tr">
<td id="S4.T2.1.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.7.1.1.1.1" class="ltx_text" style="font-size:90%;">ReLU-RP CNN:</span></td>
</tr>
<tr id="S4.T2.1.1.7.6.7.1.2" class="ltx_tr">
<td id="S4.T2.1.1.7.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.7.1.2.1.1" class="ltx_text" style="font-size:90%;">ECG Acc = 70%</span></td>
</tr>
<tr id="S4.T2.1.1.7.6.7.1.3" class="ltx_tr">
<td id="S4.T2.1.1.7.6.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.7.1.3.1.1" class="ltx_text" style="font-size:90%;">PPG Acc = 64%</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.7.6.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.7.6.8.1" class="ltx_text" style="font-size:90%;">wearable ECG + PPG sensors</span></td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<td id="S4.T2.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Fahrurrasyid et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib105" title="" class="ltx_ref">105</a><span id="S4.T2.1.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">motorcycle driver</span></td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.8.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.8.7.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.8.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.4.1.1.1.1" class="ltx_text" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.8.7.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.8.7.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.4.1.2.1.1" class="ltx_text" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.8.7.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.8.7.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.6.1" class="ltx_text" style="font-size:90%;">10 subjects</span></td>
<td id="S4.T2.1.1.8.7.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.7.1" class="ltx_text" style="font-size:90%;">find operation thresholds</span></td>
<td id="S4.T2.1.1.8.7.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.8.7.8.1" class="ltx_text" style="font-size:90%;">a smart helmet equipped with sensors</span></td>
</tr>
<tr id="S4.T2.1.1.9.8" class="ltx_tr">
<td id="S4.T2.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Heydari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.9.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib106" title="" class="ltx_ref">106</a><span id="S4.T2.1.1.9.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T2.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.9.8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.9.8.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.9.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.4.1.1.1.1" class="ltx_text" style="font-size:90%;">pulse rate variability</span></td>
</tr>
<tr id="S4.T2.1.1.9.8.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.9.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.4.1.2.1.1" class="ltx_text" style="font-size:90%;">of a finger from PPG</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.6.1" class="ltx_text" style="font-size:90%;">10 subjects</span></td>
<td id="S4.T2.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.7.1" class="ltx_text" style="font-size:90%;">relevant features selection</span></td>
<td id="S4.T2.1.1.9.8.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.9.8.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.9.8.8.1.1" class="ltx_tr">
<td id="S4.T2.1.1.9.8.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.8.1.1.1.1" class="ltx_text" style="font-size:90%;">awake/drowsy, heuristic features from time,</span></td>
</tr>
<tr id="S4.T2.1.1.9.8.8.1.2" class="ltx_tr">
<td id="S4.T2.1.1.9.8.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.9.8.8.1.2.1.1" class="ltx_text" style="font-size:90%;">frequency domain, and nonlinear analysis</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.1.10.9" class="ltx_tr">
<td id="S4.T2.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T2.1.1.10.9.1.1" class="ltx_text" style="font-size:90%;">Hasan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.10.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib108" title="" class="ltx_ref">108</a><span id="S4.T2.1.1.10.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.2.1" class="ltx_text" style="font-size:90%;">2024</span></td>
<td id="S4.T2.1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T2.1.1.10.9.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.10.9.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.10.9.4.1.1" class="ltx_tr">
<td id="S4.T2.1.1.10.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.4.1.1.1.1" class="ltx_text" style="font-size:90%;">explainable ML in</span></td>
</tr>
<tr id="S4.T2.1.1.10.9.4.1.2" class="ltx_tr">
<td id="S4.T2.1.1.10.9.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.4.1.2.1.1" class="ltx_text" style="font-size:90%;">multimodal system</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.10.9.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T2.1.1.10.9.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.6.1" class="ltx_text" style="font-size:90%;">35 subjects</span></td>
<td id="S4.T2.1.1.10.9.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.10.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.10.9.7.1.1" class="ltx_tr">
<td id="S4.T2.1.1.10.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.7.1.1.1.1" class="ltx_text" style="font-size:90%;">sensitivity=70.3%</span></td>
</tr>
<tr id="S4.T2.1.1.10.9.7.1.2" class="ltx_tr">
<td id="S4.T2.1.1.10.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.7.1.2.1.1" class="ltx_text" style="font-size:90%;">specificity=82.2%</span></td>
</tr>
<tr id="S4.T2.1.1.10.9.7.1.3" class="ltx_tr">
<td id="S4.T2.1.1.10.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.7.1.3.1.1" class="ltx_text" style="font-size:90%;">Acc=80.1%</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.1.10.9.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T2.1.1.10.9.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.10.9.8.1.1" class="ltx_tr">
<td id="S4.T2.1.1.10.9.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.8.1.1.1.1" class="ltx_text" style="font-size:90%;">validation techniques for black box model</span></td>
</tr>
<tr id="S4.T2.1.1.10.9.8.1.2" class="ltx_tr">
<td id="S4.T2.1.1.10.9.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T2.1.1.10.9.8.1.2.1.1" class="ltx_text" style="font-size:90%;">combining EEG+EOG+ECG signals</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Vision-based Drowsiness Detection</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Vision-based drowsiness detection is intended to be non-invasive and non-intrusive. Unlike physiological signals, this method does not require close contact with the subject. It operates remotely and does not necessitate physical attachment or direct interaction with the individual being monitored. Most relevant features for vision-based drowsiness detection are focused on the facial attributes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, such as eye blinking, eye aspect ratio or facial expressions<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> such as yawning or mouth opening which indicates the level of drowsiness. In contrast to the works developed based on physiological signals from previous sub-sections, it exists more official databases for the development and evaluation of vision-based drowsiness detection schemes.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Earlier work by Garcia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed a vision-based drowsiness detector under real driving conditions. An infrared camera is placed in front to capture the driver’s face and to obtain drowsiness clues from their eyes closure. Three stages of processing include face and eye detection, pupil position detection, and illumination adaptation. Finally, the PERCLOS features are extracted to relate them to the drowsiness state. An outdoor database of several experiments over 25 driving hours was generated as the evaluation dataset. Results of the binary classification for awake and fatigue states showed a specificity, sensitivity, and recall of 92.23%, 79.84%, and 90.68% respectively.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In this research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>, Yu et al. introduced an innovative approach for drowsiness detection, utilizing three main steps for simultaneous representation learning, scene understanding, and feature fusion. They extracted and learned spatio-temporal representations from consecutive frames and employed scene conditional understanding and fusion techniques to enhance the accuracy of drowsiness detection. To evaluate their method’s performance, they tested it on the NTHU-DDD dataset. Results showed a validation accuracy of 88% and an F1-score of 0.712. However, the limitation of the proposed model is its generalizability. Since it is trained on NTHU-DDD dataset, it may not be directly applicable to scenarios that deviate from the trained conditions.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Deng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed a system called DriCare which unobtrusively detects the driver’s fatigue status clues, such as yawning, blinking, and duration of eye closure, based on video images. They introduced a face-tracking algorithm to improve the tracking accuracy and designed a new detection scheme for facial regions based on 68 facial landmarks which are leveraged to access the driver’s state. By fusion features of the eyes and mouth, DriCare achieved an accuracy of 92% on the YawDD database.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">To mitigate the problem of changing illumination under real driving conditions in a car, Bakheet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> proposed an improved histogram of oriented gradients (HOG) features combined with a naive Bayesian classification to detect driver drowsiness. The experimental outcomes on the publicly accessible NTHU-DDD dataset demonstrated that the proposed framework has the potential to compete strongly with several SOTA baselines. Results showed an average accuracy of 85.62%. However, the model could have the same shortcomings of missing generalizability.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">Vijay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> presented a vision-based, two-stage pipeline for real-time driver drowsiness detection using Facial Action Units (FAUs). FAUs can represent facial expression-related movements in facial muscle groups. In the first stage, they employed CNN for detecting FAUs. The second stage utilized an Extreme Gradient Boosting (XGBoost) classifier for drowsiness detection. To model user-specific behavior, individual classifiers were trained. This approach achieved high accuracy in real-time using only a small amount of data and short training time.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">Liu’s work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> focused on the drowsiness detection of crane operators by using deep neural networks leveraging both spatial features and temporal features. They combined the spatial feature extraction with CNN and temporal feature extraction with a Long-short-term-memory (LSTM) network. The authors collected facial videos from licensed crane operators under simulated crane operation scenarios and created a large and public fatigue dataset especially tailored for crane operators. They trained their model on three public vehicle driver datasets, NTHU-DDD, UTA-RLDD, and YawnDD, with human-verified labels at the frame and minute segment levels.</p>
</div>
<div id="S4.SS3.p8" class="ltx_para">
<p id="S4.SS3.p8.1" class="ltx_p">Ahmed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> proposed an approach combining visual features from both eyes and mouth regions extracted from two separate CNNs for driver drowsiness detection. The weights of each stream are further trained on a single-layer perceptron to output the final prediction of drowsiness or non-drowsiness detection. The strength of the ensemble structure is demonstrated over single-stream processing using one of the two face areas. This model is evaluated on the NTHU-DDD video dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> with an accuracy of 97.1 % and showed robustness over variations in pose and illumination. More recent multi-stream classification networks combining both spatial and spatio-temporal features is proposed by Pandey et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>.</p>
</div>
<div id="S4.SS3.p9" class="ltx_para">
<p id="S4.SS3.p9.1" class="ltx_p">Krishna et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> proposed to build a driver drowsiness detection framework by fusion object detection and using global attention. Thus, the authors leveraged vision transformers and YoloV5 detectors in their proposed framework. This work aims to capture more complicated driver behaviour features from images compared to current CNN-based methods in this field. The framework is evaluated on the public dataset UTA-RLDD and further validated on a custom dataset of 39 participants collected under various light conditions. On both datasets, the proposed method showcased promising results in terms of high accuracy. The authors claimed the significance of their proposed framework for practical applications in smart transportation systems.</p>
</div>
<div id="S4.SS3.p10" class="ltx_para">
<p id="S4.SS3.p10.1" class="ltx_p">Tamanani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> proposed a new driver’s vigilance detection system based on deep learning methods on facial region diagnosis using the Haar-cascade method and CNN for classification. Evaluation is performed on the UTA-RLDD dataset with five-fold cross-validation. Results showed an accuracy of 96.8% which is higher than most previously reported algorithms. Another customized dataset with 10 subjects under different light conditions was collected to evaluate the generalizability of the proposed method.</p>
</div>
<div id="S4.SS3.p11" class="ltx_para">
<p id="S4.SS3.p11.1" class="ltx_p">In addition to the detection of eye blinking and yawning in a visual image, Khan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> further considered another visual feature as an indicator of ’distraction’ by detecting the driver looking sideways for a certain duration of times, i.e. more than 3 seconds. This feature is calculated by determining the Euclidean distances from both ears to nose tip, which builds a triangle, and the difference of both distances is related to side looking. Side looking face will cause an increase in this difference measure compared to frontal view. To evaluate the proposed method, experiments were conducted on a self-collected dataset containing 50 subjects. Results showed a precision, recall, and F1-score of 0.89, 0.98, and 0.93 respectively.</p>
</div>
<div id="S4.SS3.p12" class="ltx_para">
<p id="S4.SS3.p12.1" class="ltx_p">Most vision-based detection methods primarily focus on frontal faces and struggle to handle various head poses encountered in real driving scenarios. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> dealt with the challenge by proposing a network to accurately detect driver drowsiness from various viewing angles combining transfer learning and population-based sampling strategy (TLPSN). The population-based sampling strategy is adopted to curate a new training set from data captured in a driver-in-the-loop platform. The results demonstrated that the proposed method has strong robustness to the variation of pose while maintaining high accuracy. In addition, transfer learning significantly improves the generalizability of the model.</p>
</div>
<div id="S4.SS3.p13" class="ltx_para">
<p id="S4.SS3.p13.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> recently introduced a novel network also designed for detecting driver yawning across arbitrary poses in video. The network comprises three key components: a Geometric-based Key-frame Selection Module (GK-Module), a Face Frontalization with Warp Attention Module (FF-Module), and a dual-channel classifier for Head Pose &amp; Facial Action Fusion Module (HF-Module). Extensive experiments demonstrate that the proposed JHPFA-Net achieves SOTA performance compared to several representative methods on the public YawDD benchmark. Moreover, it exhibits excellent performance in real-time applications.</p>
</div>
<div id="S4.SS3.p14" class="ltx_para">
<p id="S4.SS3.p14.1" class="ltx_p">Table <a href="#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes the works using images or video data for drowsiness detection in various application scenarios. Vision-based drowsiness detection approaches are non-intrusive and remote, focusing mostly on facial clues, eye blink rate, or head positions. References to the public databases used as benchmarks are provided in the footnote. From Table <a href="#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we noticed that there exist more public databases for the development and evaluation of vision-based drowsiness detection schemes compared to physiological signals. A similar trend from traditional machine learning to deep learning-based methods for drowsiness detection can be observed over time. Sequence models and attention-based vision transformers represent the latest advancements in deep learning-based drowsiness detection schemes.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>summarizes recent works performing drowsiness detection based on vision.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:335.5pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-286.3pt,221.2pt) scale(0.430968698515999,0.430968698515999) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">work</span></td>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">year</span></td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">area of use</span></td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">algorithm</span></td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">database</span></td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">subjects/session</span></td>
<td id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">performance</span></td>
<td id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">remarks</span></td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Garcia et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.2.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S4.T3.1.1.2.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.2.1" class="ltx_text" style="font-size:90%;">2012</span></td>
<td id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.2.2.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">face, eye, pupil detection</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.4.1.2.1.1" class="ltx_text" style="font-size:90%;">illumination adaptation</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.4.1.3" class="ltx_tr">
<td id="S4.T3.1.1.2.2.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.4.1.3.1.1" class="ltx_text" style="font-size:90%;">heuristic</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.2.2.6.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.6.1.1.1.1" class="ltx_text" style="font-size:90%;">10 subjects, 30 h</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.6.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.6.1.2.1.1" class="ltx_text" style="font-size:90%;">driving, 1296 awake</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.6.1.3" class="ltx_tr">
<td id="S4.T3.1.1.2.2.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.6.1.3.1.1" class="ltx_text" style="font-size:90%;">min., 504 fatigue min.</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.2.2.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Specificity=92.23%</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Sensitivity=79.84%</span></td>
</tr>
<tr id="S4.T3.1.1.2.2.7.1.3" class="ltx_tr">
<td id="S4.T3.1.1.2.2.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.7.1.3.1.1" class="ltx_text" style="font-size:90%;">Recall=90.68%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.2.2.8.1" class="ltx_text" style="font-size:90%;">under real drive conditions</span></td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<td id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Yu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib111" title="" class="ltx_ref">111</a><span id="S4.T3.1.1.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.2.1" class="ltx_text" style="font-size:90%;">2017</span></td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.3.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.3.3.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.3.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">representation learning,</span></td>
</tr>
<tr id="S4.T3.1.1.3.3.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.3.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.4.1.2.1.1" class="ltx_text" style="font-size:90%;">scene understanding,</span></td>
</tr>
<tr id="S4.T3.1.1.3.3.4.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.3.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.4.1.3.1.1" class="ltx_text" style="font-size:90%;">feature fusion</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.3.3.5.1" class="ltx_text" style="font-size:90%;">NTHU-DDD </span><span id="S4.T3.1.1.3.3.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.3.3.5.3" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
<td id="S4.T3.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.6.1" class="ltx_text" style="font-size:90%;">36 subjects</span></td>
<td id="S4.T3.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.3.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.3.3.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.3.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.7.1.1.1.1" class="ltx_text" style="font-size:90%;">validation Acc=88%</span></td>
</tr>
<tr id="S4.T3.1.1.3.3.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.3.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.7.1.2.1.1" class="ltx_text" style="font-size:90%;">F1-score = 0.712</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.3.3.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.3.3.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.3.3.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.8.1.1.1.1" class="ltx_text" style="font-size:90%;">feature sparsity in fusion model,</span></td>
</tr>
<tr id="S4.T3.1.1.3.3.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.3.3.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.3.3.8.1.2.1.1" class="ltx_text" style="font-size:90%;">framework may not generalize</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.4.4" class="ltx_tr">
<td id="S4.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Deng et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S4.T3.1.1.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S4.T3.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.4.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.4.4.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.4.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">imroved face tracking,</span></td>
</tr>
<tr id="S4.T3.1.1.4.4.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.4.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.4.1.2.1.1" class="ltx_text" style="font-size:90%;">features from face regions</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.4.4.5.1" class="ltx_text" style="font-size:90%;">YawDD</span><span id="S4.T3.1.1.4.4.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.4.4.5.3" class="ltx_text" style="font-size:90%;">fn:yawdd</span>
</td>
<td id="S4.T3.1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.6.1" class="ltx_text" style="font-size:90%;">107 subjects</span></td>
<td id="S4.T3.1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.7.1" class="ltx_text" style="font-size:90%;">Acc = 92%</span></td>
<td id="S4.T3.1.1.4.4.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.4.4.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.4.4.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.4.4.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.8.1.1.1.1" class="ltx_text" style="font-size:90%;">real-time system,</span></td>
</tr>
<tr id="S4.T3.1.1.4.4.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.4.4.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.4.4.8.1.2.1.1" class="ltx_text" style="font-size:90%;">applicable to different circumstances</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.5.5" class="ltx_tr">
<td id="S4.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.5.5.1.1" class="ltx_text" style="font-size:90%;">Bakheet et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib109" title="" class="ltx_ref">109</a><span id="S4.T3.1.1.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T3.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.5.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.5.5.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.5.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">HOG feature</span></td>
</tr>
<tr id="S4.T3.1.1.5.5.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.5.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.4.1.2.1.1" class="ltx_text" style="font-size:90%;">Naive Bayesian Classifier</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.5.5.5.1" class="ltx_text" style="font-size:90%;">NTHU-DDD </span><span id="S4.T3.1.1.5.5.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.5.5.5.3" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
<td id="S4.T3.1.1.5.5.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.6.1" class="ltx_text" style="font-size:90%;">36 subjects</span></td>
<td id="S4.T3.1.1.5.5.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.7.1" class="ltx_text" style="font-size:90%;">Acc = 85.62%</span></td>
<td id="S4.T3.1.1.5.5.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.5.5.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.5.5.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.5.5.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.8.1.1.1.1" class="ltx_text" style="font-size:90%;">limitation of generalizability,</span></td>
</tr>
<tr id="S4.T3.1.1.5.5.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.5.5.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.5.5.8.1.2.1.1" class="ltx_text" style="font-size:90%;">need more diverse datasets</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.6.6" class="ltx_tr">
<td id="S4.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.6.6.1.1" class="ltx_text" style="font-size:90%;">Vijay et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S4.T3.1.1.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T3.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.6.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.6.6.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.6.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.4.1.1.1.1" class="ltx_text" style="font-size:90%;">CNN for Facial Action</span></td>
</tr>
<tr id="S4.T3.1.1.6.6.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.6.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.4.1.2.1.1" class="ltx_text" style="font-size:90%;">Units, Extreme Gradient</span></td>
</tr>
<tr id="S4.T3.1.1.6.6.4.1.3" class="ltx_tr">
<td id="S4.T3.1.1.6.6.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.4.1.3.1.1" class="ltx_text" style="font-size:90%;">Boosting Classifier</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.6.6.5.1" class="ltx_text" style="font-size:90%;">NTHU-DDD </span><span id="S4.T3.1.1.6.6.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.6.6.5.3" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
<td id="S4.T3.1.1.6.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.6.1" class="ltx_text" style="font-size:90%;">36 subjects</span></td>
<td id="S4.T3.1.1.6.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.7.1" class="ltx_text" style="font-size:90%;">Acc =96%</span></td>
<td id="S4.T3.1.1.6.6.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.6.6.8.1" class="ltx_text" style="font-size:90%;">subject-specific classification</span></td>
</tr>
<tr id="S4.T3.1.1.7.7" class="ltx_tr">
<td id="S4.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Liu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.T3.1.1.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T3.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.3.1" class="ltx_text" style="font-size:90%;">crane operator</span></td>
<td id="S4.T3.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.4.1" class="ltx_text" style="font-size:90%;">LSTM + CNN</span></td>
<td id="S4.T3.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.7.7.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.7.7.5.1.1" class="ltx_tr">
<td id="S4.T3.1.1.7.7.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.7.7.5.1.1.1.1" class="ltx_text" style="font-size:90%;">NTHU</span><span id="S4.T3.1.1.7.7.5.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.7.7.5.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:nthu, UTA</span><span id="S4.T3.1.1.7.7.5.1.1.1.4" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.7.7.5.1.1.1.5" class="ltx_text" style="font-size:90%;">fn:uta,</span>
</td>
</tr>
<tr id="S4.T3.1.1.7.7.5.1.2" class="ltx_tr">
<td id="S4.T3.1.1.7.7.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.7.7.5.1.2.1.1" class="ltx_text" style="font-size:90%;">YawnDD</span><span id="S4.T3.1.1.7.7.5.1.2.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.7.7.5.1.2.1.3" class="ltx_text" style="font-size:90%;">fn:yawdd, Custom</span>
</td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T3.1.1.7.7.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T3.1.1.7.7.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.7.7.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.7.7.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.7.7.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.8.1.1.1.1" class="ltx_text" style="font-size:90%;">simulated crane operation,</span></td>
</tr>
<tr id="S4.T3.1.1.7.7.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.7.7.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.7.7.8.1.2.1.1" class="ltx_text" style="font-size:90%;">made their database public</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.8.8" class="ltx_tr">
<td id="S4.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.8.8.1.1" class="ltx_text" style="font-size:90%;">Ahmed et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.8.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S4.T3.1.1.8.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T3.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.4.1" class="ltx_text" style="font-size:90%;">two streams CNNs</span></td>
<td id="S4.T3.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.8.8.5.1" class="ltx_text" style="font-size:90%;">NTHU-DDD </span><span id="S4.T3.1.1.8.8.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.8.8.5.3" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
<td id="S4.T3.1.1.8.8.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.6.1" class="ltx_text" style="font-size:90%;">36 subjects</span></td>
<td id="S4.T3.1.1.8.8.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.8.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.8.8.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.8.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Evaluation dataset</span></td>
</tr>
<tr id="S4.T3.1.1.8.8.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.8.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Acc=97.1%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.8.8.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.8.8.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.8.8.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.8.8.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.8.1.1.1.1" class="ltx_text" style="font-size:90%;">robust over variations</span></td>
</tr>
<tr id="S4.T3.1.1.8.8.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.8.8.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.8.8.8.1.2.1.1" class="ltx_text" style="font-size:90%;">in pose and illumination</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.9.9" class="ltx_tr">
<td id="S4.T3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.9.9.1.1" class="ltx_text" style="font-size:90%;">Mou et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.9.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib118" title="" class="ltx_ref">118</a><span id="S4.T3.1.1.9.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S4.T3.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.9.9.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.9.9.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.9.9.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.9.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.4.1.1.1.1" class="ltx_text" style="font-size:90%;">IsoSSL-MoCo</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.9.9.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.9.9.5.1.1" class="ltx_tr">
<td id="S4.T3.1.1.9.9.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.9.9.5.1.1.1.1" class="ltx_text" style="font-size:90%;">NTHU-DDD</span><span id="S4.T3.1.1.9.9.5.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.9.9.5.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
</tr>
<tr id="S4.T3.1.1.9.9.5.1.2" class="ltx_tr">
<td id="S4.T3.1.1.9.9.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.9.9.5.1.2.1.1" class="ltx_text" style="font-size:90%;">YawDD</span><span id="S4.T3.1.1.9.9.5.1.2.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.9.9.5.1.2.1.3" class="ltx_text" style="font-size:90%;">fn:yawdd</span>
</td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.9.9.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.9.9.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.9.9.6.1.1" class="ltx_tr">
<td id="S4.T3.1.1.9.9.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.6.1.1.1.1" class="ltx_text" style="font-size:90%;">36 subjects</span></td>
</tr>
<tr id="S4.T3.1.1.9.9.6.1.2" class="ltx_tr">
<td id="S4.T3.1.1.9.9.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.6.1.2.1.1" class="ltx_text" style="font-size:90%;">107 subjects</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.9.9.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.9.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.9.9.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.9.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Acc=93.71%</span></td>
</tr>
<tr id="S4.T3.1.1.9.9.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.9.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Acc=98.65%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.9.9.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.9.9.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.9.9.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.9.9.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.9.9.8.1.1.1.1" class="ltx_text" style="font-size:90%;">pretrain on MRL dataset</span><span id="S4.T3.1.1.9.9.8.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.9.9.8.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:mrl + NTHU-DDD</span><span id="S4.T3.1.1.9.9.8.1.1.1.4" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.9.9.8.1.1.1.5" class="ltx_text" style="font-size:90%;">fn:nthu</span>
</td>
</tr>
<tr id="S4.T3.1.1.9.9.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.9.9.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.9.9.8.1.2.1.1" class="ltx_text" style="font-size:90%;">leveraging self-supervised learning</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.10.10" class="ltx_tr">
<td id="S4.T3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.10.10.1.1" class="ltx_text" style="font-size:90%;">Krishna et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.10.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib114" title="" class="ltx_ref">114</a><span id="S4.T3.1.1.10.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T3.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.3.1" class="ltx_text" style="font-size:90%;">smart transportation</span></td>
<td id="S4.T3.1.1.10.10.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.4.1" class="ltx_text" style="font-size:90%;">vision transformers + Yolov5</span></td>
<td id="S4.T3.1.1.10.10.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.10.10.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.10.10.5.1.1" class="ltx_tr">
<td id="S4.T3.1.1.10.10.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.10.10.5.1.1.1.1" class="ltx_text" style="font-size:90%;">UTA-RLDD</span><span id="S4.T3.1.1.10.10.5.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.10.10.5.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:uta</span>
</td>
</tr>
<tr id="S4.T3.1.1.10.10.5.1.2" class="ltx_tr">
<td id="S4.T3.1.1.10.10.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.5.1.2.1.1" class="ltx_text" style="font-size:90%;">Custom</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.10.10.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.10.10.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.10.10.6.1.1" class="ltx_tr">
<td id="S4.T3.1.1.10.10.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.6.1.1.1.1" class="ltx_text" style="font-size:90%;">60 subjects,</span></td>
</tr>
<tr id="S4.T3.1.1.10.10.6.1.2" class="ltx_tr">
<td id="S4.T3.1.1.10.10.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.6.1.2.1.1" class="ltx_text" style="font-size:90%;">39 subjects</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.10.10.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.10.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.10.10.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.10.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.7.1.1.1.1" class="ltx_text" style="font-size:90%;">train Acc = 96.2%</span></td>
</tr>
<tr id="S4.T3.1.1.10.10.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.10.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.7.1.2.1.1" class="ltx_text" style="font-size:90%;">valid Acc = 97.4%</span></td>
</tr>
<tr id="S4.T3.1.1.10.10.7.1.3" class="ltx_tr">
<td id="S4.T3.1.1.10.10.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.7.1.3.1.1" class="ltx_text" style="font-size:90%;">custom Acc = 95.5%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.10.10.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.10.10.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.10.10.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.10.10.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.8.1.1.1.1" class="ltx_text" style="font-size:90%;">attention-based model + object detection</span></td>
</tr>
<tr id="S4.T3.1.1.10.10.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.10.10.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.10.10.8.1.2.1.1" class="ltx_text" style="font-size:90%;">vigilent/drowsy detection</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.11.11" class="ltx_tr">
<td id="S4.T3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.11.11.1.1" class="ltx_text" style="font-size:90%;">Chen et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.11.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib116" title="" class="ltx_ref">116</a><span id="S4.T3.1.1.11.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.2.1" class="ltx_text" style="font-size:90%;">2022</span></td>
<td id="S4.T3.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.11.11.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.11.11.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.11.11.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.11.11.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.4.1.1.1.1" class="ltx_text" style="font-size:90%;">multiple viewing angle</span></td>
</tr>
<tr id="S4.T3.1.1.11.11.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.11.11.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.4.1.2.1.1" class="ltx_text" style="font-size:90%;">sampling strategy for</span></td>
</tr>
<tr id="S4.T3.1.1.11.11.4.1.3" class="ltx_tr">
<td id="S4.T3.1.1.11.11.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.4.1.3.1.1" class="ltx_text" style="font-size:90%;">data augmentation</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.11.11.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T3.1.1.11.11.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.6.1" class="ltx_text" style="font-size:90%;">5 subjects</span></td>
<td id="S4.T3.1.1.11.11.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.11.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.11.11.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.11.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Acc = 97.5%</span></td>
</tr>
<tr id="S4.T3.1.1.11.11.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.11.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.7.1.2.1.1" class="ltx_text" style="font-size:90%;">F1 = 97.5%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.11.11.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.11.11.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.11.11.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.11.11.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.8.1.1.1.1" class="ltx_text" style="font-size:90%;">simulated driving,</span></td>
</tr>
<tr id="S4.T3.1.1.11.11.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.11.11.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.8.1.2.1.1" class="ltx_text" style="font-size:90%;">sleep deprivated subjects</span></td>
</tr>
<tr id="S4.T3.1.1.11.11.8.1.3" class="ltx_tr">
<td id="S4.T3.1.1.11.11.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.11.11.8.1.3.1.1" class="ltx_text" style="font-size:90%;">small-scale dataset</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.12.12" class="ltx_tr">
<td id="S4.T3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.12.12.1.1" class="ltx_text" style="font-size:90%;">Tamanani et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.12.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib115" title="" class="ltx_ref">115</a><span id="S4.T3.1.1.12.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.12.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T3.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.3.1" class="ltx_text" style="font-size:90%;">public transportation</span></td>
<td id="S4.T3.1.1.12.12.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.12.12.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.12.12.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.12.12.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.4.1.1.1.1" class="ltx_text" style="font-size:90%;">facial region diagnosis</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.12.12.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.4.1.2.1.1" class="ltx_text" style="font-size:90%;">(with Haar-cascade),</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.4.1.3" class="ltx_tr">
<td id="S4.T3.1.1.12.12.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.4.1.3.1.1" class="ltx_text" style="font-size:90%;">CNN classification</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.12.12.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.12.12.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.12.12.5.1.1" class="ltx_tr">
<td id="S4.T3.1.1.12.12.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.12.12.5.1.1.1.1" class="ltx_text" style="font-size:90%;">UTA-RLDD</span><span id="S4.T3.1.1.12.12.5.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.12.12.5.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:uta ,</span>
</td>
</tr>
<tr id="S4.T3.1.1.12.12.5.1.2" class="ltx_tr">
<td id="S4.T3.1.1.12.12.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.5.1.2.1.1" class="ltx_text" style="font-size:90%;">private</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.12.12.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.12.12.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.12.12.6.1.1" class="ltx_tr">
<td id="S4.T3.1.1.12.12.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.6.1.1.1.1" class="ltx_text" style="font-size:90%;">60 subjects,</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.6.1.2" class="ltx_tr">
<td id="S4.T3.1.1.12.12.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.6.1.2.1.1" class="ltx_text" style="font-size:90%;">10 subjects</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.12.12.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.12.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.12.12.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.12.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.7.1.1.1.1" class="ltx_text" style="font-size:90%;">avg Acc = 0.918</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.12.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.7.1.2.1.1" class="ltx_text" style="font-size:90%;">precision = 0.928</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.7.1.3" class="ltx_tr">
<td id="S4.T3.1.1.12.12.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.7.1.3.1.1" class="ltx_text" style="font-size:90%;">recall = 0.920</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.7.1.4" class="ltx_tr">
<td id="S4.T3.1.1.12.12.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.7.1.4.1.1" class="ltx_text" style="font-size:90%;">F1-score = 0.920</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.12.12.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.12.12.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.12.12.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.12.12.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.8.1.1.1.1" class="ltx_text" style="font-size:90%;">authors evaluated their method</span></td>
</tr>
<tr id="S4.T3.1.1.12.12.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.12.12.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.12.12.8.1.2.1.1" class="ltx_text" style="font-size:90%;">on a customized dataset</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.13.13" class="ltx_tr">
<td id="S4.T3.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.13.13.1.1" class="ltx_text" style="font-size:90%;">Khan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.13.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib72" title="" class="ltx_ref">72</a><span id="S4.T3.1.1.13.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T3.1.1.13.13.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.3.1" class="ltx_text" style="font-size:90%;">public transportation</span></td>
<td id="S4.T3.1.1.13.13.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.13.13.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.13.13.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.13.13.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.4.1.1.1.1" class="ltx_text" style="font-size:90%;">handcrafted features,</span></td>
</tr>
<tr id="S4.T3.1.1.13.13.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.13.13.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.4.1.2.1.1" class="ltx_text" style="font-size:90%;">heuristics</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.13.13.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.5.1" class="ltx_text" style="font-size:90%;">private</span></td>
<td id="S4.T3.1.1.13.13.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.13.13.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.13.13.6.1.1" class="ltx_tr">
<td id="S4.T3.1.1.13.13.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.6.1.1.1.1" class="ltx_text" style="font-size:90%;">50 subjects</span></td>
</tr>
<tr id="S4.T3.1.1.13.13.6.1.2" class="ltx_tr">
<td id="S4.T3.1.1.13.13.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.6.1.2.1.1" class="ltx_text" style="font-size:90%;">(33 M, 17 F)</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.13.13.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.13.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.13.13.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.13.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Precision = 0.89</span></td>
</tr>
<tr id="S4.T3.1.1.13.13.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.13.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Recall = 0.98</span></td>
</tr>
<tr id="S4.T3.1.1.13.13.7.1.3" class="ltx_tr">
<td id="S4.T3.1.1.13.13.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.7.1.3.1.1" class="ltx_text" style="font-size:90%;">F1-score =0.93</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.13.13.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.13.13.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.13.13.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.13.13.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.8.1.1.1.1" class="ltx_text" style="font-size:90%;">IoT-based Non-Intrusive</span></td>
</tr>
<tr id="S4.T3.1.1.13.13.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.13.13.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.13.13.8.1.2.1.1" class="ltx_text" style="font-size:90%;">to enhance Road Safety</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.14.14" class="ltx_tr">
<td id="S4.T3.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.14.14.1.1" class="ltx_text" style="font-size:90%;">Pandey et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.14.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib113" title="" class="ltx_ref">113</a><span id="S4.T3.1.1.14.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.14.14.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T3.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.14.14.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.14.14.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.14.14.4.1.1" class="ltx_tr">
<td id="S4.T3.1.1.14.14.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.4.1.1.1.1" class="ltx_text" style="font-size:90%;">mutlistream classifcation</span></td>
</tr>
<tr id="S4.T3.1.1.14.14.4.1.2" class="ltx_tr">
<td id="S4.T3.1.1.14.14.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.4.1.2.1.1" class="ltx_text" style="font-size:90%;">YoLov3 + LSTM</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.14.14.5.1" class="ltx_text" style="font-size:90%;">UTA-RLDD</span><span id="S4.T3.1.1.14.14.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.14.14.5.3" class="ltx_text" style="font-size:90%;">fn:uta</span>
</td>
<td id="S4.T3.1.1.14.14.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.6.1" class="ltx_text" style="font-size:90%;">60 subjects</span></td>
<td id="S4.T3.1.1.14.14.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.14.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.14.14.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.14.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Acc=97.5%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.14.14.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.14.14.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.14.14.8.1.1" class="ltx_tr">
<td id="S4.T3.1.1.14.14.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Spatio-temporal feature;</span></td>
</tr>
<tr id="S4.T3.1.1.14.14.8.1.2" class="ltx_tr">
<td id="S4.T3.1.1.14.14.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.8.1.2.1.1" class="ltx_text" style="font-size:90%;">TransGAN; YOLOv3</span></td>
</tr>
<tr id="S4.T3.1.1.14.14.8.1.3" class="ltx_tr">
<td id="S4.T3.1.1.14.14.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.14.14.8.1.3.1.1" class="ltx_text" style="font-size:90%;">Temporal feature; LSTM</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.1.1.15.15" class="ltx_tr">
<td id="S4.T3.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.15.15.1.1" class="ltx_text" style="font-size:90%;">Lu et al </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.15.15.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib117" title="" class="ltx_ref">117</a><span id="S4.T3.1.1.15.15.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T3.1.1.15.15.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.2.1" class="ltx_text" style="font-size:90%;">2023</span></td>
<td id="S4.T3.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.3.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
<td id="S4.T3.1.1.15.15.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.4.1" class="ltx_text" style="font-size:90%;">DL with 3 modules</span></td>
<td id="S4.T3.1.1.15.15.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T3.1.1.15.15.5.1" class="ltx_text" style="font-size:90%;">YawDD</span><span id="S4.T3.1.1.15.15.5.2" class="ltx_ERROR undefined">\footref</span><span id="S4.T3.1.1.15.15.5.3" class="ltx_text" style="font-size:90%;">fn:yawdd</span>
</td>
<td id="S4.T3.1.1.15.15.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.6.1" class="ltx_text" style="font-size:90%;">107 subjects</span></td>
<td id="S4.T3.1.1.15.15.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T3.1.1.15.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.15.15.7.1.1" class="ltx_tr">
<td id="S4.T3.1.1.15.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Acc=86.7%</span></td>
</tr>
<tr id="S4.T3.1.1.15.15.7.1.2" class="ltx_tr">
<td id="S4.T3.1.1.15.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.7.1.2.1.1" class="ltx_text" style="font-size:90%;">Precision = 91%</span></td>
</tr>
<tr id="S4.T3.1.1.15.15.7.1.3" class="ltx_tr">
<td id="S4.T3.1.1.15.15.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.7.1.3.1.1" class="ltx_text" style="font-size:90%;">F1-score =88.8%</span></td>
</tr>
</table>
</td>
<td id="S4.T3.1.1.15.15.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.1.1.15.15.8.1" class="ltx_text" style="font-size:90%;">targets arbitrary head poses</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Widely Used Databases</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section summarizes popular databases used for drowsiness detection based on visual, mutli-channel EEG, and ECG signals. This discussion stressed on available databases and put less focus on small-scale private databases. Table <a href="#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> contains an overview of these publicly available databases for drowsiness detection research. A link to the individual database is given in the footnote for easier access. In Table <a href="#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we target this tabular representation from various aspects, including the database name, year of publication (chronologically ordered), detection modality, number of subjects/sessions included, its label annotation, application area, and conclude with specific remarks. The majority of the cited databases here are used for driver drowsiness detection but can be extended to general drowsiness detection tasks because of the diversity of the capture environments.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The University of Texas at Arlington created the Real-Life Drowsiness Dataset<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>UTA-RLDD: https://sites.google.com/view/utarldd/home</span></span></span> (UTA-RLDD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> targeting the task of multi-stage drowsiness detection. The dataset contains both easily visible cases and subtle cases where the drowsiness level is at an early stage and the detection is strongly related to subtle micro-expressions. The creators claimed that the UTA-RLDD dataset is the largest to date realistic drowsiness dataset. It consists of around 30 hours of RGB videos of 60 healthy participants, where each participant collected three different classes including alertness, low vigilance, and drowsiness. It contains variations in gender, ethnicity, age, acquisition device, and also accessories like glasses, facial hair and different viewing angles in different real-life environments and backgrounds.
The position of the acquisition device is placed such that both eyes are visible and the device is within one arm length away from the subject. This database is often used to evaluate algorithms for driver’s drowsiness detection with vision-based approaches<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The academic NTHU-DDD dataset<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>NTHU-DDD: http://cv.cs.nthu.edu.tw/php/callforpaper/datasets/DDD/</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> was collected by NTHU Computer Vision Lab at National Tsing Hua University. This is another video dataset for detecting driver’s drowsiness. They used active infrared illumination to alleviate the poor illumination issue. All videos were captured by a stand-alone surveillance camera D-Link DCS-932L with a resolution of 640x480 pixels. The capture device is placed on the top left to emulate the position in the car without blocking the driver’s view. Each subject recorded two different sessions including day-time and night-time sessions. Subjects are asked to perform actions indicating different drowsiness levels while playing a plain driving game, such as normal driving, slow blink rate, yawning, falling asleep and bursting out laughing. The dataset includes variations in illumination, scenarios, skin color, gender, age and also differences in hairstyles, clothing and glasses/sunglasses to cover the most realistic driving scenarios. Compared to UTA-RLDD, although this database was also recorded under indoor settings, its applications are much more limited, because the use-case collected here is more controlled.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The YawDD dataset<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>YawDD: https://traces.cs.umass.edu/index.php/Mmsys/Mmsys</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> is recorded by an in-car camera capturing the driver’s facial characteristics in an actual car during talking, singing, being silent, and yawning. It contains two different camera installations: (1) under the front mirror and (2) on the driver’s dashboard. In total 322 RGB videos were recorded with large variations in illumination, gender, ethnicity, and with and without glasses/sunglasses. Additional 29 videos, one for each subject, are added under second setup containing all sequences of performed actions. Compared to UTA-RLDD or NTHU-DDD this database is more realistic in the data acquisition, as the videos are collected in a real vehicle with individuals sitting behind the wheel. This allows the design of drowsiness detection algorithm under more realistic scenarios. The challenge in this database accounts for the placement of the camera under the mirror. Under this viewing angle face detection is much harder.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Media Research Lab from the Technical University of Ostrava proposed the MRL Eye Dataset<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Project page MRL Eye Dataset: http://mrl.cs.vsb.cz/eyedataset</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> which could be used to detect eye blinking and relate this to drowsiness detection by combining eye blinking rate to drowsiness levels. This database was not originally designed for drowsiness detection. But it can be leveraged to detect visual features of the human eyes related to drowsiness. It is a large-scale dataset of human eye images and consists of 84,898 images from 37 individuals, while 33 male and 4 female subjects are included. This dataset further modulated variations in capture devices, capture spectra (RGB + IR), lightning conditions, image resolutions and eye openness levels. Unlike previous databases, this database restricts the area of faces to only the eye regions making the detection of subtle facial micro-expressions impossible.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">The FatigueView dataset<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Project page FatigueView Dataset: https://fatigueview.github.io/#/</span></span></span>, introduced by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, is a large-scaled dataset of videos designed for driver’s drowsiness detection. This dataset boasts practicality, diversity, and a wide range of environments. It encompasses images captured by both RGB and infrared cameras, positioned in five different angles. The dataset includes genuine instances of drowsy driving and displays various visual indications of different drowsiness levels. Notably, the dataset features a substantial 17,403 instances of yawning, significantly surpassing current widely used datasets. The authors evaluated the dataset using SOTA algorithms, therefore establishing numerous baseline results that can guide future algorithm advancements. Unlike the NTHU-DDD, this database is collected in a single office environment by sitting the participants on an upholstered office chair. While the acquisition environment is not diverse enough compared to NTHU-DDD or UTA-RLDD, it covers a much broader viewing angle.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">The National Cheng Kung University Driver Drowsiness Dataset (NCKUDD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> consists of videos from a total of 25 participants captured during a normal driving condition. The camera is placed in front of the driver to unobtrusively capture the driver’s facial expression. Recordings contain both daylight and dark environments and include normal, sleepy, distracted, talking/eating scenarios while driving, talking on the phone while driving, and other abnormal driving patterns. Compared to other vision-based databases so far, this database may contain footage of participants maneuvering the car for real. While this fact might be considered advantages, it may include situations that endanger the driver’s safety.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">A novel database is introduced by Martin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> called the Driver&amp;Act dataset<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Project page Drive&amp;Act: https://driveandact.com/</span></span></span>, uniquely designated for fine-grained recognition of driver behavior in autonomous vehicles. This comprehensive dataset encompasses more than 9.6 million frames spanning 12 hours, capturing individuals engaged in distracting actions during both manual and automated driving scenarios. The dataset utilizes a capture device that integrates RGB, IR, depth, and 3D body pose data from six different perspectives. Among its objectives, this database particularly aims to excel in the identification of intricate actions and features a multi-modal approach to activity recognition. As opposed to other drowsiness detection databases, this database specifically target the use-case of autonomous driving and was not originally addressed for drowsiness detection only.</p>
</div>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">Ortega et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> proposed DMD, which is another large-scale multi-modal driver monitoring dataset<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Project page DMD dataset: https://dmd.vicomtech.org/</span></span></span> for attention and alertness analysis. This is an extensive dataset with 37 drivers which includes real and simulated driving scenarios. Targeted tasks contain levels of distraction, gaze allocation, drowsiness detection, hands-wheel interaction and context data. It contains in total 41 hours of RGB, depth and infrared videos from 3 cameras capturing face, body, and hands. Compared to other existing similar datasets, the authors motivated their proposed database to be more extensive, diverse, and multi-purpose. Unlike UTA-RLDD, NTHU-DDD or YawDD, this database does not consider drowsiness as the main focus of distraction and considers much more diverse activities causing the drivers to shift their attention from the road, such as talking on the phone, playing with the car interface, or typing into a car navigation system.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.1" class="ltx_p">Recently, Yilmaz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> introduced a novel dataset, termed SUST-DDD<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>SUST-DDD: https://www.kaggle.com/datasets/esrakavalci/sust-ddd</span></span></span>, aimed at driver drowsiness detection. This dataset is meticulously curated and benchmarked using a range of DL techniques to effectively predict driver drowsiness. The dataset compilation involved 19 participants who were instructed to record themselves while driving using their personal cell phones positioned in front of the driver’s seat. To closely replicate real-world driving scenarios, participants were required to operate their own vehicles and utilize their individual phones. Notably, this dataset encapsulates genuine driving situations encompassing diverse lighting conditions, distinct video sizes, and varying resolutions due to each participant’s unique phone specifications. This database is most similar to YawDD and NCKUDD, however with the difference that both YawDD and SUST-DDD use user-specific cell phones, while only SUST-DDD includes real driving scenarios. The difference between NCKUDD and SUST-DDD is that NCKUDD uses a camera installed in front of the driver, while in SUST-DDD custom cell phones are used as recording devices within custom vehicles to simulate a more natural driving experience.</p>
</div>
<div id="S5.p11" class="ltx_para">
<p id="S5.p11.1" class="ltx_p">The ULg Multimodality Drowsiness Database<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Project page DROZY: http://www.drozy.ulg.ac.be/</span></span></span> (DROZY) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> is a database which is not only based on vision capture to detect drowsiness. Here, a total of 14 subjects (3 males, 11 females) have participated in the data collection process. Experiments were conducted in a quite and isolated laboratory environment. Participants have no sleeping disorders and a sleep diary was kept individually. Tea and coffee were avoided during the acquisition process to keep the subjects really drowsy instead of faking it. In this database, four different electrical bio-signals such as electroencephalogram (EEG), electroculography (EOG), electrocardiogram (EKG) and electromyogram (EMG) were acquired. Similar to UTA-RLDD, this database was also not specifically designated to target driver’s drowsiness detection but includes much more sensory inputs especially measuring physiological entities compared to UTA-RLDD.</p>
</div>
<div id="S5.p12" class="ltx_para">
<p id="S5.p12.1" class="ltx_p">Last but not least, another EEG-based database is proposed by Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> in Multichannel EEG<span id="S5.p12.1.1" class="ltx_ERROR undefined">\footref</span>fn:eeg. They collected 32-channel EEG data during a sustained-attention driving task for drivers’ drowsiness detection. In Table <a href="#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we referred this database to the Multi-channel EEG recordings. It consisted of more than 62 sessions for 27 subjects. Each session includes a 90-minute sustained-attention task in which an immersive driving scenario is simulated. The participants were asked to drive on a four line highway and keep the car in the center of the lane. Random car drifts are induced expecting the participants to respond accordingly. The driver’s drowsiness is inferred from the required response time in this lane-keeping task. This dataset is specifically collected for targeting the driver’s drowsiness detection task.</p>
</div>
<div id="S5.p13" class="ltx_para">
<p id="S5.p13.1" class="ltx_p">In this section, we have seen a wide range of databases that motivate research on drowsiness detection. Both visual and biosignal-based databases are investigated here. While most databases, such as NTHU-DDD, YawDD, FatigueView, NCKUDD, Drive &amp; Act, DMD, and SUST-DDD, are intended for driver drowsiness detection, other databases, such as UTA-RLDD, MRL Eye Dataset, and ULg Dataset, are targeting the more general drowsiness detection tasks. Especially the UTA-RLDD database is collected under unconstrained real-world indoor scenarios, which can be used to develop drowsiness detection applications more general for smart environments. But also NHU-DDD collected under indoor scenarios while sitting can be extended to build general drowsiness detection applications for smart environments as well.</p>
</div>
<div id="S5.T4" class="ltx_table ltx_transformed_outer" style="width:13297.2pt;height:450.3pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:450.3pt;transform:translate(6423.46pt,6424.46pt) rotate(-90deg) ;"><figure>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>summarizes databases used for drowsiness detection. Most of them are used for driver drowsiness detection but can be easily extended to general drowsiness detection framework for smart environments, as they are collected under various indoor scenarios.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:13296.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.5pt,46.0pt) scale(0.993129036614105,0.993129036614105) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.1.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">database</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.2.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">year</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.3.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">measrung technique</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.4.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">attributes</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.5.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">sensing modality</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.6.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">area of application</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.7.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">subjects</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.8.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">sessions</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.9.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.9.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">classes</span></span>
</span>
</th>
<th id="S5.T4.1.1.1.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.1.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.1.10.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">specific remarks</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.2.1" class="ltx_tr">
<td id="S5.T4.1.1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.1.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">YawDD</span><span id="S5.T4.1.1.2.1.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.2.1.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:yawdd </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.2.1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib120" title="" class="ltx_ref">120</a><span id="S5.T4.1.1.2.1.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.2.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">2014</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.3.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.2.1.4.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.2.1.4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.2.1.4.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.2.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">yawning,</span></td>
</tr>
<tr id="S5.T4.1.1.2.1.4.1.2" class="ltx_tr">
<td id="S5.T4.1.1.2.1.4.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.2.1.4.1.2.1.1" class="ltx_text" style="font-size:90%;">open mouth</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.2.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.5.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.6.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.7.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.7.1.1.1" class="ltx_text" style="font-size:90%;">29</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.8.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.8.1.1.1" class="ltx_text" style="font-size:90%;">342 RGB videos</span></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.9.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.9.1.1.1" class="ltx_text" style="font-size:90%;">normal, talking/singing, yawing, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.2.1.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a><span id="S5.T4.1.1.2.1.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.2.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.2.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.2.1.10.1.1" class="ltx_p"><span id="S5.T4.1.1.2.1.10.1.1.1" class="ltx_text" style="font-size:90%;">recorded in an actual parked car</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.3.2" class="ltx_tr">
<td id="S5.T4.1.1.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.1.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">NTHU-DDD</span><span id="S5.T4.1.1.3.2.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.3.2.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:nthu </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.3.2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib112" title="" class="ltx_ref">112</a><span id="S5.T4.1.1.3.2.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.2.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">2017</span></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.3.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.3.2.4.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">head pose,</span></td>
</tr>
<tr id="S5.T4.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S5.T4.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.4.1.2.1.1" class="ltx_text" style="font-size:90%;">yawning,</span></td>
</tr>
<tr id="S5.T4.1.1.3.2.4.1.3" class="ltx_tr">
<td id="S5.T4.1.1.3.2.4.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.4.1.3.1.1" class="ltx_text" style="font-size:90%;">eye closure</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.3.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.5.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.3.2.6.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.3.2.6.1.1" class="ltx_tr">
<td id="S5.T4.1.1.3.2.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.6.1.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness,</span></td>
</tr>
<tr id="S5.T4.1.1.3.2.6.1.2" class="ltx_tr">
<td id="S5.T4.1.1.3.2.6.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.6.1.2.1.1" class="ltx_text" style="font-size:90%;">while sitting on a</span></td>
</tr>
<tr id="S5.T4.1.1.3.2.6.1.3" class="ltx_tr">
<td id="S5.T4.1.1.3.2.6.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.3.2.6.1.3.1.1" class="ltx_text" style="font-size:90%;">chair indoor</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.3.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.7.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.7.1.1.1" class="ltx_text" style="font-size:90%;">22</span></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.8.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.8.1.1.1" class="ltx_text" style="font-size:90%;">360 RGB videos</span></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.9.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.9.1.1.1" class="ltx_text" style="font-size:90%;">drowsiness and non-drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.3.2.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S5.T4.1.1.3.2.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.3.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.3.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.3.2.10.1.1" class="ltx_p"><span id="S5.T4.1.1.3.2.10.1.1.1" class="ltx_text" style="font-size:90%;">controlled indoor scenarios, under simulations, contains variations for most realistic driving scenarios</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.4.3" class="ltx_tr">
<td id="S5.T4.1.1.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.1.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">MRL Eye Dataset</span><span id="S5.T4.1.1.4.3.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.4.3.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:mrl </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.4.3.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib121" title="" class="ltx_ref">121</a><span id="S5.T4.1.1.4.3.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.2.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">2018</span></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.3.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.4.3.4.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">eye blink,</span></td>
</tr>
<tr id="S5.T4.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S5.T4.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.4.1.2.1.1" class="ltx_text" style="font-size:90%;">eye openess</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.4.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.5.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.6.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.6.1.1.1" class="ltx_text" style="font-size:90%;">general drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.7.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.7.1.1.1" class="ltx_text" style="font-size:90%;">37</span></span>
</span>
</td>
<td id="S5.T4.1.1.4.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.4.3.8.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.4.3.8.1.1" class="ltx_tr">
<td id="S5.T4.1.1.4.3.8.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.8.1.1.1.1" class="ltx_text" style="font-size:90%;">85000 different</span></td>
</tr>
<tr id="S5.T4.1.1.4.3.8.1.2" class="ltx_tr">
<td id="S5.T4.1.1.4.3.8.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.8.1.2.1.1" class="ltx_text" style="font-size:90%;">eye regions</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.4.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.4.3.9.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.4.3.9.1.1" class="ltx_tr">
<td id="S5.T4.1.1.4.3.9.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.9.1.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness,</span></td>
</tr>
<tr id="S5.T4.1.1.4.3.9.1.2" class="ltx_tr">
<td id="S5.T4.1.1.4.3.9.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.4.3.9.1.2.1.1" class="ltx_text" style="font-size:90%;">gaze direction,</span></td>
</tr>
<tr id="S5.T4.1.1.4.3.9.1.3" class="ltx_tr">
<td id="S5.T4.1.1.4.3.9.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.9.1.3.1.1" class="ltx_text" style="font-size:90%;">used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.4.3.9.1.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a><span id="S5.T4.1.1.4.3.9.1.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.4.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.4.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.4.3.10.1.1" class="ltx_p"><span id="S5.T4.1.1.4.3.10.1.1.1" class="ltx_text" style="font-size:90%;">contains only eye regions not faces, impossible to detect facial expressions</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.5.4" class="ltx_tr">
<td id="S5.T4.1.1.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.1.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">UTA-RLDD</span><span id="S5.T4.1.1.5.4.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.5.4.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:uta </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.5.4.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib119" title="" class="ltx_ref">119</a><span id="S5.T4.1.1.5.4.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.2.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.3.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.4.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expressions extreme to subtle</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.5.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.5.4.6.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.5.4.6.1.1" class="ltx_tr">
<td id="S5.T4.1.1.5.4.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.5.4.6.1.1.1.1" class="ltx_text" style="font-size:90%;">general drowsiness</span></td>
</tr>
<tr id="S5.T4.1.1.5.4.6.1.2" class="ltx_tr">
<td id="S5.T4.1.1.5.4.6.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.5.4.6.1.2.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.5.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.7.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.7.1.1.1" class="ltx_text" style="font-size:90%;">60</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.8.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.8.1.1.1" class="ltx_text" style="font-size:90%;">180 RGB videos</span></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.9.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.9.1.1.1" class="ltx_text" style="font-size:90%;">alert, low vigilance, drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.5.4.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib115" title="" class="ltx_ref">115</a><span id="S5.T4.1.1.5.4.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.5.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.5.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.5.4.10.1.1" class="ltx_p"><span id="S5.T4.1.1.5.4.10.1.1.1" class="ltx_text" style="font-size:90%;">contains variations in gender, ethnicity, age, acquisition device and accessories, can be used for general drowsiness detection</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.6.5" class="ltx_tr">
<td id="S5.T4.1.1.6.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.6.5.1.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.6.5.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.6.5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.6.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">NCK-</span></td>
</tr>
<tr id="S5.T4.1.1.6.5.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.6.5.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.1.1.2.1.1" class="ltx_text" style="font-size:90%;">UDD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.6.5.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib123" title="" class="ltx_ref">123</a><span id="S5.T4.1.1.6.5.1.1.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.6.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.2.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.3.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.4.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expression, eye openess, mouth open</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.5.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.6.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.7.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.7.1.1.1" class="ltx_text" style="font-size:90%;">25</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.8.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.8.1.1.1" class="ltx_text" style="font-size:90%;">643 total events</span></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.9.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.9.1.1.1" class="ltx_text" style="font-size:90%;">normal, sleepy, distracted, and various other activities while driving, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.6.5.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a><span id="S5.T4.1.1.6.5.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.6.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.6.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.6.5.10.1.1" class="ltx_p"><span id="S5.T4.1.1.6.5.10.1.1.1" class="ltx_text" style="font-size:90%;">contains real driving conditions possibly endangering the drivers</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.7.6" class="ltx_tr">
<td id="S5.T4.1.1.7.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.7.6.1.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.7.6.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.7.6.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.7.6.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Fatigue-</span></td>
</tr>
<tr id="S5.T4.1.1.7.6.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.7.6.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.1.1.2.1.1" class="ltx_text" style="font-size:90%;">View</span><span id="S5.T4.1.1.7.6.1.1.2.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.7.6.1.1.2.1.3" class="ltx_text" style="font-size:90%;">fn:fatigue </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.7.6.1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib122" title="" class="ltx_ref">122</a><span id="S5.T4.1.1.7.6.1.1.2.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.7.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.2.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.2.1.1.1" class="ltx_text" style="font-size:90%;">2022</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.3.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.4.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.4.1.1.1" class="ltx_text" style="font-size:90%;">head pose, eye openess, mouth closed</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.5.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.6.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.7.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.7.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.8.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.8.1.1.1" class="ltx_text" style="font-size:90%;">17403 yawning sets</span></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.9.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.9.1.1.1" class="ltx_text" style="font-size:90%;">drowsy, normal, nodding, and stretching, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.7.6.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib134" title="" class="ltx_ref">134</a><span id="S5.T4.1.1.7.6.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.7.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.7.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.7.6.10.1.1" class="ltx_p"><span id="S5.T4.1.1.7.6.10.1.1.1" class="ltx_text" style="font-size:90%;">office environment indoor, sitting on office chairs, simulation</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.8.7" class="ltx_tr">
<td id="S5.T4.1.1.8.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.1.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Drive &amp; Act</span><span id="S5.T4.1.1.8.7.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.8.7.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:driveAct </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.8.7.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib124" title="" class="ltx_ref">124</a><span id="S5.T4.1.1.8.7.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.2.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.3.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.4.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expression, head pose, eye open, mouth open</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.5.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.6.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.6.1.1.1" class="ltx_text" style="font-size:90%;">autonomous driving, includes driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.7.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.7.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.8.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.8.1.1.1" class="ltx_text" style="font-size:90%;">12 hours of RGB, depth, IR videos, 3D body pose from 6 different views</span></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.9.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.9.1.1.1" class="ltx_text" style="font-size:90%;">diverse distracting activities during both manual and automated driving conditions, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.8.7.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib135" title="" class="ltx_ref">135</a><span id="S5.T4.1.1.8.7.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.8.7.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.8.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.8.7.10.1.1" class="ltx_p"><span id="S5.T4.1.1.8.7.10.1.1.1" class="ltx_text" style="font-size:90%;">specific for autonomous driving not specific for driver drowsiness. Database is more extensive, diverse, and multi-purpose.</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.9.8" class="ltx_tr">
<td id="S5.T4.1.1.9.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.1.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.1.1.1.1" class="ltx_text" style="font-size:90%;">DMD</span><span id="S5.T4.1.1.9.8.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.9.8.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:dmd</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.9.8.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib125" title="" class="ltx_ref">125</a><span id="S5.T4.1.1.9.8.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.2.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.3.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.4.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expression, mouth open, eyes open, head pose, yawning</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.5.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.6.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.6.1.1.1" class="ltx_text" style="font-size:90%;">driver distraction, recognition includes but not only for drowsiness detection</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.7.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.7.1.1.1" class="ltx_text" style="font-size:90%;">37</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.8.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.8.1.1.1" class="ltx_text" style="font-size:90%;">41 hours of RGB, depth, IR videos</span></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.9.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.9.1.1.1" class="ltx_text" style="font-size:90%;">levels of distraction, gaze allocation, hands-wheel interaction, and context data, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.9.8.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib136" title="" class="ltx_ref">136</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a><span id="S5.T4.1.1.9.8.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.9.8.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.9.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.9.8.10.1.1" class="ltx_p"><span id="S5.T4.1.1.9.8.10.1.1.1" class="ltx_text" style="font-size:90%;">multimodal dataset for different driver scenarios. Not considering drowsiness as the only distraction factor for drivers</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.10.9" class="ltx_tr">
<td id="S5.T4.1.1.10.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.1.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.1.1.1.1" class="ltx_text" style="font-size:90%;">SUST-DDD</span><span id="S5.T4.1.1.10.9.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.10.9.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:sust </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.10.9.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib126" title="" class="ltx_ref">126</a><span id="S5.T4.1.1.10.9.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.2.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.2.1.1.1" class="ltx_text" style="font-size:90%;">2022</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.3.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.3.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.4.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expression, eye openess, mouth open</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.5.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.5.1.1.1" class="ltx_text" style="font-size:90%;">remote</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.6.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.7.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.7.1.1.1" class="ltx_text" style="font-size:90%;">19</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.8.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.8.1.1.1" class="ltx_text" style="font-size:90%;">2074 RGB videos</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.9.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.9.1.1.1" class="ltx_text" style="font-size:90%;">drowsiness and non-drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.10.9.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.10.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.10.9.10.1.1" class="ltx_p"><span id="S5.T4.1.1.10.9.10.1.1.1" class="ltx_text" style="font-size:90%;">recorded under real driving scenarios, participants use their own vehicle and phones to mimic most natural and comfortable driving condition</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.11.10" class="ltx_tr">
<td id="S5.T4.1.1.11.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.1.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.1.1.1.1" class="ltx_text" style="font-size:90%;">DROZY</span><span id="S5.T4.1.1.11.10.1.1.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.11.10.1.1.1.3" class="ltx_text" style="font-size:90%;">fn:drozy </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.11.10.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="S5.T4.1.1.11.10.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.2.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.2.1.1.1" class="ltx_text" style="font-size:90%;">2016</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.11.10.3.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.11.10.3.1.1" class="ltx_tr">
<td id="S5.T4.1.1.11.10.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.11.10.3.1.1.1.1" class="ltx_text" style="font-size:90%;">vision-based</span></td>
</tr>
<tr id="S5.T4.1.1.11.10.3.1.2" class="ltx_tr">
<td id="S5.T4.1.1.11.10.3.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.11.10.3.1.2.1.1" class="ltx_text" style="font-size:90%;">physiological</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.11.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.4.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.4.1.1.1" class="ltx_text" style="font-size:90%;">facial expression, heartbeat variability</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.5.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.5.1.1.1" class="ltx_text" style="font-size:90%;">remote, wired</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.6.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.7.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.7.1.1.1" class="ltx_text" style="font-size:90%;">14</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.8.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.8.1.1.1" class="ltx_text" style="font-size:90%;">3064 examples</span></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.9.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.9.1.1.1" class="ltx_text" style="font-size:90%;">drowsiness and non-drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.11.10.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a><span id="S5.T4.1.1.11.10.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.11.10.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.11.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.11.10.10.1.1" class="ltx_p"><span id="S5.T4.1.1.11.10.10.1.1.1" class="ltx_text" style="font-size:90%;">recorded in real driving scenarios, participants use their own vehicle and phones to mimic most natural and comfortable driving condition</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.1.12.11" class="ltx_tr">
<td id="S5.T4.1.1.12.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.12.11.1.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.12.11.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.12.11.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.12.11.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Multi</span></td>
</tr>
<tr id="S5.T4.1.1.12.11.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.12.11.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.12.11.1.1.2.1.1" class="ltx_text" style="font-size:90%;">channel</span></td>
</tr>
<tr id="S5.T4.1.1.12.11.1.1.3" class="ltx_tr">
<td id="S5.T4.1.1.12.11.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.1.1.3.1.1" class="ltx_text" style="font-size:90%;">EEG</span><span id="S5.T4.1.1.12.11.1.1.3.1.2" class="ltx_ERROR undefined">\footref</span><span id="S5.T4.1.1.12.11.1.1.3.1.3" class="ltx_text" style="font-size:90%;">fn:eeg </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.12.11.1.1.3.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib92" title="" class="ltx_ref">92</a><span id="S5.T4.1.1.12.11.1.1.3.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.12.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.2.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.3.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.3.1.1.1" class="ltx_text" style="font-size:90%;">physiological</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.4.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.4.1.1.1" class="ltx_text" style="font-size:90%;">EEG-based deviation in RT during lane-keeping task</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.5.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.5.1.1.1" class="ltx_text" style="font-size:90%;">wired</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.6.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.6.1.1.1" class="ltx_text" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.7.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.7.1.1.1" class="ltx_text" style="font-size:90%;">27</span></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<table id="S5.T4.1.1.12.11.8.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T4.1.1.12.11.8.1.1" class="ltx_tr">
<td id="S5.T4.1.1.12.11.8.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.12.11.8.1.1.1.1" class="ltx_text" style="font-size:90%;">62 sessions derived</span></td>
</tr>
<tr id="S5.T4.1.1.12.11.8.1.2" class="ltx_tr">
<td id="S5.T4.1.1.12.11.8.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T4.1.1.12.11.8.1.2.1.1" class="ltx_text" style="font-size:90%;">from 90 min. task</span></td>
</tr>
</table>
</td>
<td id="S5.T4.1.1.12.11.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.9.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.9.1.1.1" class="ltx_text" style="font-size:90%;">deviation onset, response onset, response offset, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.1.1.12.11.9.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="S5.T4.1.1.12.11.9.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S5.T4.1.1.12.11.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T4.1.1.12.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.12.11.10.1.1" class="ltx_p"><span id="S5.T4.1.1.12.11.10.1.1.1" class="ltx_text" style="font-size:90%;">23-channel EEG under simulated driving condition</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure></div></div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Performance and Evaluation Metrics</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we present common evaluation metrics used to compare SOTA drowsiness detection algorithms. Popular evaluation metrics for this classification task include accuracy (Acc) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>, precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, and F1-score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> defined as follows:</p>
<table id="S6.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E1.m1.1" class="ltx_Math" alttext="Precision=\cfrac{TP}{TP+FP}," display="block"><semantics id="S6.E1.m1.1a"><mrow id="S6.E1.m1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml"><mrow id="S6.E1.m1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml"><mrow id="S6.E1.m1.1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.2.cmml"><mi id="S6.E1.m1.1.1.1.1.2.2" xref="S6.E1.m1.1.1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.3" xref="S6.E1.m1.1.1.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1a" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.4" xref="S6.E1.m1.1.1.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1b" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.5" xref="S6.E1.m1.1.1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1c" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.6" xref="S6.E1.m1.1.1.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1d" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.7" xref="S6.E1.m1.1.1.1.1.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1e" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.8" xref="S6.E1.m1.1.1.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1f" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.9" xref="S6.E1.m1.1.1.1.1.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.2.1g" xref="S6.E1.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.2.10" xref="S6.E1.m1.1.1.1.1.2.10.cmml">n</mi></mrow><mo id="S6.E1.m1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E1.m1.1.1.1.1.3" xref="S6.E1.m1.1.1.1.1.3.cmml"><mrow id="S6.E1.m1.1.1.1.1.3.2" xref="S6.E1.m1.1.1.1.1.3.2.cmml"><mi id="S6.E1.m1.1.1.1.1.3.2.2" xref="S6.E1.m1.1.1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.3.2.1" xref="S6.E1.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.3.2.3" xref="S6.E1.m1.1.1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S6.E1.m1.1.1.1.1.3.3" xref="S6.E1.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E1.m1.1.1.1.1.3.3.2" xref="S6.E1.m1.1.1.1.1.3.3.2.cmml"><mi id="S6.E1.m1.1.1.1.1.3.3.2.2" xref="S6.E1.m1.1.1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.3.3.2.1" xref="S6.E1.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.3.3.2.3" xref="S6.E1.m1.1.1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S6.E1.m1.1.1.1.1.3.3.1" xref="S6.E1.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E1.m1.1.1.1.1.3.3.3" xref="S6.E1.m1.1.1.1.1.3.3.3.cmml"><mi id="S6.E1.m1.1.1.1.1.3.3.3.2" xref="S6.E1.m1.1.1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.1.1.3.3.3.1" xref="S6.E1.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S6.E1.m1.1.1.1.1.3.3.3.3" xref="S6.E1.m1.1.1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><mo id="S6.E1.m1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.1b"><apply id="S6.E1.m1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1"><eq id="S6.E1.m1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1"></eq><apply id="S6.E1.m1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.2"><times id="S6.E1.m1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.1.1.2.1"></times><ci id="S6.E1.m1.1.1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.1.1.2.2">𝑃</ci><ci id="S6.E1.m1.1.1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.1.1.2.3">𝑟</ci><ci id="S6.E1.m1.1.1.1.1.2.4.cmml" xref="S6.E1.m1.1.1.1.1.2.4">𝑒</ci><ci id="S6.E1.m1.1.1.1.1.2.5.cmml" xref="S6.E1.m1.1.1.1.1.2.5">𝑐</ci><ci id="S6.E1.m1.1.1.1.1.2.6.cmml" xref="S6.E1.m1.1.1.1.1.2.6">𝑖</ci><ci id="S6.E1.m1.1.1.1.1.2.7.cmml" xref="S6.E1.m1.1.1.1.1.2.7">𝑠</ci><ci id="S6.E1.m1.1.1.1.1.2.8.cmml" xref="S6.E1.m1.1.1.1.1.2.8">𝑖</ci><ci id="S6.E1.m1.1.1.1.1.2.9.cmml" xref="S6.E1.m1.1.1.1.1.2.9">𝑜</ci><ci id="S6.E1.m1.1.1.1.1.2.10.cmml" xref="S6.E1.m1.1.1.1.1.2.10">𝑛</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.cmml" xref="S6.E1.m1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E1.m1.1.1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E1.m1.1.1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.2"><times id="S6.E1.m1.1.1.1.1.3.2.1.cmml" xref="S6.E1.m1.1.1.1.1.3.2.1"></times><ci id="S6.E1.m1.1.1.1.1.3.2.2.cmml" xref="S6.E1.m1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E1.m1.1.1.1.1.3.2.3.cmml" xref="S6.E1.m1.1.1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3"><plus id="S6.E1.m1.1.1.1.1.3.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.1"></plus><apply id="S6.E1.m1.1.1.1.1.3.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2"><times id="S6.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.1"></times><ci id="S6.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3"><times id="S6.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.1"></times><ci id="S6.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.1c">Precision=\cfrac{TP}{TP+FP},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S6.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E2.m1.1" class="ltx_Math" alttext="Recall=\cfrac{TP}{TP+FN}," display="block"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.2.cmml"><mi id="S6.E2.m1.1.1.1.1.2.2" xref="S6.E2.m1.1.1.1.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.2.1" xref="S6.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.2.3" xref="S6.E2.m1.1.1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.2.1a" xref="S6.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.2.4" xref="S6.E2.m1.1.1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.2.1b" xref="S6.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.2.5" xref="S6.E2.m1.1.1.1.1.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.2.1c" xref="S6.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.2.6" xref="S6.E2.m1.1.1.1.1.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.2.1d" xref="S6.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.2.7" xref="S6.E2.m1.1.1.1.1.2.7.cmml">l</mi></mrow><mo id="S6.E2.m1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E2.m1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.3.cmml"><mrow id="S6.E2.m1.1.1.1.1.3.2" xref="S6.E2.m1.1.1.1.1.3.2.cmml"><mi id="S6.E2.m1.1.1.1.1.3.2.2" xref="S6.E2.m1.1.1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.3.2.1" xref="S6.E2.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.3.2.3" xref="S6.E2.m1.1.1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S6.E2.m1.1.1.1.1.3.3" xref="S6.E2.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E2.m1.1.1.1.1.3.3.2" xref="S6.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S6.E2.m1.1.1.1.1.3.3.2.2" xref="S6.E2.m1.1.1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.3.3.2.1" xref="S6.E2.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.3.3.2.3" xref="S6.E2.m1.1.1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S6.E2.m1.1.1.1.1.3.3.1" xref="S6.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E2.m1.1.1.1.1.3.3.3" xref="S6.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S6.E2.m1.1.1.1.1.3.3.3.2" xref="S6.E2.m1.1.1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.1.1.3.3.3.1" xref="S6.E2.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S6.E2.m1.1.1.1.1.3.3.3.3" xref="S6.E2.m1.1.1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mrow><mo id="S6.E2.m1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"><eq id="S6.E2.m1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1"></eq><apply id="S6.E2.m1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.2"><times id="S6.E2.m1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.1.2.1"></times><ci id="S6.E2.m1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.1.1.2.2">𝑅</ci><ci id="S6.E2.m1.1.1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.1.1.2.3">𝑒</ci><ci id="S6.E2.m1.1.1.1.1.2.4.cmml" xref="S6.E2.m1.1.1.1.1.2.4">𝑐</ci><ci id="S6.E2.m1.1.1.1.1.2.5.cmml" xref="S6.E2.m1.1.1.1.1.2.5">𝑎</ci><ci id="S6.E2.m1.1.1.1.1.2.6.cmml" xref="S6.E2.m1.1.1.1.1.2.6">𝑙</ci><ci id="S6.E2.m1.1.1.1.1.2.7.cmml" xref="S6.E2.m1.1.1.1.1.2.7">𝑙</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E2.m1.1.1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E2.m1.1.1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.2"><times id="S6.E2.m1.1.1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.1.1.3.2.1"></times><ci id="S6.E2.m1.1.1.1.1.3.2.2.cmml" xref="S6.E2.m1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E2.m1.1.1.1.1.3.2.3.cmml" xref="S6.E2.m1.1.1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3"><plus id="S6.E2.m1.1.1.1.1.3.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.1"></plus><apply id="S6.E2.m1.1.1.1.1.3.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2"><times id="S6.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.1"></times><ci id="S6.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3"><times id="S6.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.1"></times><ci id="S6.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">Recall=\cfrac{TP}{TP+FN},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S6.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E3.m1.1" class="ltx_Math" alttext="F1-score=2\times\cfrac{precision\times recall}{precision+recall}," display="block"><semantics id="S6.E3.m1.1a"><mrow id="S6.E3.m1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml"><mrow id="S6.E3.m1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml"><mrow id="S6.E3.m1.1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.2.2" xref="S6.E3.m1.1.1.1.1.2.2.cmml"><mi id="S6.E3.m1.1.1.1.1.2.2.2" xref="S6.E3.m1.1.1.1.1.2.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.2.2.1" xref="S6.E3.m1.1.1.1.1.2.2.1.cmml">​</mo><mn id="S6.E3.m1.1.1.1.1.2.2.3" xref="S6.E3.m1.1.1.1.1.2.2.3.cmml">1</mn></mrow><mo id="S6.E3.m1.1.1.1.1.2.1" xref="S6.E3.m1.1.1.1.1.2.1.cmml">−</mo><mrow id="S6.E3.m1.1.1.1.1.2.3" xref="S6.E3.m1.1.1.1.1.2.3.cmml"><mi id="S6.E3.m1.1.1.1.1.2.3.2" xref="S6.E3.m1.1.1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.2.3.1" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.2.3.3" xref="S6.E3.m1.1.1.1.1.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.2.3.1a" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.2.3.4" xref="S6.E3.m1.1.1.1.1.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.2.3.1b" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.2.3.5" xref="S6.E3.m1.1.1.1.1.2.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.2.3.1c" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.2.3.6" xref="S6.E3.m1.1.1.1.1.2.3.6.cmml">e</mi></mrow></mrow><mo id="S6.E3.m1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S6.E3.m1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.3.cmml"><mn id="S6.E3.m1.1.1.1.1.3.2" xref="S6.E3.m1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S6.E3.m1.1.1.1.1.3.1" xref="S6.E3.m1.1.1.1.1.3.1.cmml">×</mo><mfrac id="S6.E3.m1.1.1.1.1.3.3" xref="S6.E3.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.4" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.5" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.6" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.7" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1e" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.8" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1f" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.9" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1g" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.10" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S6.E3.m1.1.1.1.1.3.3.2.2.1" xref="S6.E3.m1.1.1.1.1.3.3.2.2.1.cmml">×</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.2.3.cmml">r</mi></mrow><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.1" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.4" xref="S6.E3.m1.1.1.1.1.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.5" xref="S6.E3.m1.1.1.1.1.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.6" xref="S6.E3.m1.1.1.1.1.3.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.7" xref="S6.E3.m1.1.1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S6.E3.m1.1.1.1.1.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.3.2.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.2" xref="S6.E3.m1.1.1.1.1.3.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.3" xref="S6.E3.m1.1.1.1.1.3.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.4" xref="S6.E3.m1.1.1.1.1.3.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.5" xref="S6.E3.m1.1.1.1.1.3.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.6" xref="S6.E3.m1.1.1.1.1.3.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.7" xref="S6.E3.m1.1.1.1.1.3.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1e" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.8" xref="S6.E3.m1.1.1.1.1.3.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1f" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.9" xref="S6.E3.m1.1.1.1.1.3.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.2.1g" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.10" xref="S6.E3.m1.1.1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S6.E3.m1.1.1.1.1.3.3.3.1" xref="S6.E3.m1.1.1.1.1.3.3.3.1.cmml">+</mo><mrow id="S6.E3.m1.1.1.1.1.3.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.3.1" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.3.1a" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.4" xref="S6.E3.m1.1.1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.3.1b" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.5" xref="S6.E3.m1.1.1.1.1.3.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.3.1c" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.6" xref="S6.E3.m1.1.1.1.1.3.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.1.1.3.3.3.3.1d" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.7" xref="S6.E3.m1.1.1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><mo id="S6.E3.m1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.1b"><apply id="S6.E3.m1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1"><eq id="S6.E3.m1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1"></eq><apply id="S6.E3.m1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1.2"><minus id="S6.E3.m1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.1.2.1"></minus><apply id="S6.E3.m1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.1.1.2.2"><times id="S6.E3.m1.1.1.1.1.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.2.2.1"></times><ci id="S6.E3.m1.1.1.1.1.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.2.2.2">𝐹</ci><cn type="integer" id="S6.E3.m1.1.1.1.1.2.2.3.cmml" xref="S6.E3.m1.1.1.1.1.2.2.3">1</cn></apply><apply id="S6.E3.m1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.1.1.2.3"><times id="S6.E3.m1.1.1.1.1.2.3.1.cmml" xref="S6.E3.m1.1.1.1.1.2.3.1"></times><ci id="S6.E3.m1.1.1.1.1.2.3.2.cmml" xref="S6.E3.m1.1.1.1.1.2.3.2">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.2.3.3.cmml" xref="S6.E3.m1.1.1.1.1.2.3.3">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.2.3.4.cmml" xref="S6.E3.m1.1.1.1.1.2.3.4">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.2.3.5.cmml" xref="S6.E3.m1.1.1.1.1.2.3.5">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.2.3.6.cmml" xref="S6.E3.m1.1.1.1.1.2.3.6">𝑒</ci></apply></apply><apply id="S6.E3.m1.1.1.1.1.3.cmml" xref="S6.E3.m1.1.1.1.1.3"><times id="S6.E3.m1.1.1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.1"></times><cn type="integer" id="S6.E3.m1.1.1.1.1.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.2">2</cn><apply id="S6.E3.m1.1.1.1.1.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3"><csymbol cd="latexml" id="S6.E3.m1.1.1.1.1.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3">continued-fraction</csymbol><apply id="S6.E3.m1.1.1.1.1.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.1"></times><apply id="S6.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.1"></times><apply id="S6.E3.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.2">𝑝</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.3">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.4">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.5">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.6">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.7">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.8.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.8">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.9.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.9">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.10.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.10">𝑛</ci></apply><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.3">𝑟</ci></apply><ci id="S6.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.3">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.4">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.5">𝑎</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.6">𝑙</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.7">𝑙</ci></apply><apply id="S6.E3.m1.1.1.1.1.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3"><plus id="S6.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.1"></plus><apply id="S6.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2"><times id="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.2">𝑝</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.3">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.4">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.5">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.6">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.7">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.8.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.8">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.9.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.9">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.10.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.10">𝑛</ci></apply><apply id="S6.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3"><times id="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.2">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.3">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.4">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.5">𝑎</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.6">𝑙</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.7">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.1c">F1-score=2\times\cfrac{precision\times recall}{precision+recall},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S6.p1.2" class="ltx_p">where TP stands for true positive accounting for the number of correct drowsiness predictions, FP stands for false positive accounting for the number of incorrect drowsiness predictions (type I error), TN stands for true negative accounting for the number of correct non-drowsiness predictions and finally, FN stands for false negative accounting the number of incorrect non-drowsiness prediction (type II errors). Detailed results in terms of accuracy and F1-score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> are calculated for assessing the detection performance. The mathematical formulation of accuracy is given as:</p>
<table id="S6.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E4.m1.2" class="ltx_Math" alttext="Accuracy=\left(\cfrac{TP+TN}{TP+FN+TN+FP}\right)." display="block"><semantics id="S6.E4.m1.2a"><mrow id="S6.E4.m1.2.2.1" xref="S6.E4.m1.2.2.1.1.cmml"><mrow id="S6.E4.m1.2.2.1.1" xref="S6.E4.m1.2.2.1.1.cmml"><mrow id="S6.E4.m1.2.2.1.1.2" xref="S6.E4.m1.2.2.1.1.2.cmml"><mi id="S6.E4.m1.2.2.1.1.2.2" xref="S6.E4.m1.2.2.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.3" xref="S6.E4.m1.2.2.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1a" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.4" xref="S6.E4.m1.2.2.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1b" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.5" xref="S6.E4.m1.2.2.1.1.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1c" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.6" xref="S6.E4.m1.2.2.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1d" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.7" xref="S6.E4.m1.2.2.1.1.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1e" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.8" xref="S6.E4.m1.2.2.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.2.2.1.1.2.1f" xref="S6.E4.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E4.m1.2.2.1.1.2.9" xref="S6.E4.m1.2.2.1.1.2.9.cmml">y</mi></mrow><mo id="S6.E4.m1.2.2.1.1.1" xref="S6.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S6.E4.m1.2.2.1.1.3.2" xref="S6.E4.m1.1.1.cmml"><mo id="S6.E4.m1.2.2.1.1.3.2.1" xref="S6.E4.m1.1.1.cmml">(</mo><mfrac id="S6.E4.m1.1.1" xref="S6.E4.m1.1.1.cmml"><mrow id="S6.E4.m1.1.1.2" xref="S6.E4.m1.1.1.2.cmml"><mrow id="S6.E4.m1.1.1.2.2" xref="S6.E4.m1.1.1.2.2.cmml"><mi id="S6.E4.m1.1.1.2.2.2" xref="S6.E4.m1.1.1.2.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.2.2.1" xref="S6.E4.m1.1.1.2.2.1.cmml">​</mo><mi id="S6.E4.m1.1.1.2.2.3" xref="S6.E4.m1.1.1.2.2.3.cmml">P</mi></mrow><mo id="S6.E4.m1.1.1.2.1" xref="S6.E4.m1.1.1.2.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.2.3" xref="S6.E4.m1.1.1.2.3.cmml"><mi id="S6.E4.m1.1.1.2.3.2" xref="S6.E4.m1.1.1.2.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.2.3.1" xref="S6.E4.m1.1.1.2.3.1.cmml">​</mo><mi id="S6.E4.m1.1.1.2.3.3" xref="S6.E4.m1.1.1.2.3.3.cmml">N</mi></mrow></mrow><mrow id="S6.E4.m1.1.1.3" xref="S6.E4.m1.1.1.3.cmml"><mrow id="S6.E4.m1.1.1.3.2" xref="S6.E4.m1.1.1.3.2.cmml"><mi id="S6.E4.m1.1.1.3.2.2" xref="S6.E4.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.3.2.1" xref="S6.E4.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.E4.m1.1.1.3.2.3" xref="S6.E4.m1.1.1.3.2.3.cmml">P</mi></mrow><mo id="S6.E4.m1.1.1.3.1" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.3" xref="S6.E4.m1.1.1.3.3.cmml"><mi id="S6.E4.m1.1.1.3.3.2" xref="S6.E4.m1.1.1.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.3.3.1" xref="S6.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S6.E4.m1.1.1.3.3.3" xref="S6.E4.m1.1.1.3.3.3.cmml">N</mi></mrow><mo id="S6.E4.m1.1.1.3.1a" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.4" xref="S6.E4.m1.1.1.3.4.cmml"><mi id="S6.E4.m1.1.1.3.4.2" xref="S6.E4.m1.1.1.3.4.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.3.4.1" xref="S6.E4.m1.1.1.3.4.1.cmml">​</mo><mi id="S6.E4.m1.1.1.3.4.3" xref="S6.E4.m1.1.1.3.4.3.cmml">N</mi></mrow><mo id="S6.E4.m1.1.1.3.1b" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.5" xref="S6.E4.m1.1.1.3.5.cmml"><mi id="S6.E4.m1.1.1.3.5.2" xref="S6.E4.m1.1.1.3.5.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E4.m1.1.1.3.5.1" xref="S6.E4.m1.1.1.3.5.1.cmml">​</mo><mi id="S6.E4.m1.1.1.3.5.3" xref="S6.E4.m1.1.1.3.5.3.cmml">P</mi></mrow></mrow></mfrac><mo id="S6.E4.m1.2.2.1.1.3.2.2" xref="S6.E4.m1.1.1.cmml">)</mo></mrow></mrow><mo lspace="0em" id="S6.E4.m1.2.2.1.2" xref="S6.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E4.m1.2b"><apply id="S6.E4.m1.2.2.1.1.cmml" xref="S6.E4.m1.2.2.1"><eq id="S6.E4.m1.2.2.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.1"></eq><apply id="S6.E4.m1.2.2.1.1.2.cmml" xref="S6.E4.m1.2.2.1.1.2"><times id="S6.E4.m1.2.2.1.1.2.1.cmml" xref="S6.E4.m1.2.2.1.1.2.1"></times><ci id="S6.E4.m1.2.2.1.1.2.2.cmml" xref="S6.E4.m1.2.2.1.1.2.2">𝐴</ci><ci id="S6.E4.m1.2.2.1.1.2.3.cmml" xref="S6.E4.m1.2.2.1.1.2.3">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.4.cmml" xref="S6.E4.m1.2.2.1.1.2.4">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.5.cmml" xref="S6.E4.m1.2.2.1.1.2.5">𝑢</ci><ci id="S6.E4.m1.2.2.1.1.2.6.cmml" xref="S6.E4.m1.2.2.1.1.2.6">𝑟</ci><ci id="S6.E4.m1.2.2.1.1.2.7.cmml" xref="S6.E4.m1.2.2.1.1.2.7">𝑎</ci><ci id="S6.E4.m1.2.2.1.1.2.8.cmml" xref="S6.E4.m1.2.2.1.1.2.8">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.9.cmml" xref="S6.E4.m1.2.2.1.1.2.9">𝑦</ci></apply><apply id="S6.E4.m1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.3.2"><csymbol cd="latexml" id="S6.E4.m1.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.3.2">continued-fraction</csymbol><apply id="S6.E4.m1.1.1.2.cmml" xref="S6.E4.m1.1.1.2"><plus id="S6.E4.m1.1.1.2.1.cmml" xref="S6.E4.m1.1.1.2.1"></plus><apply id="S6.E4.m1.1.1.2.2.cmml" xref="S6.E4.m1.1.1.2.2"><times id="S6.E4.m1.1.1.2.2.1.cmml" xref="S6.E4.m1.1.1.2.2.1"></times><ci id="S6.E4.m1.1.1.2.2.2.cmml" xref="S6.E4.m1.1.1.2.2.2">𝑇</ci><ci id="S6.E4.m1.1.1.2.2.3.cmml" xref="S6.E4.m1.1.1.2.2.3">𝑃</ci></apply><apply id="S6.E4.m1.1.1.2.3.cmml" xref="S6.E4.m1.1.1.2.3"><times id="S6.E4.m1.1.1.2.3.1.cmml" xref="S6.E4.m1.1.1.2.3.1"></times><ci id="S6.E4.m1.1.1.2.3.2.cmml" xref="S6.E4.m1.1.1.2.3.2">𝑇</ci><ci id="S6.E4.m1.1.1.2.3.3.cmml" xref="S6.E4.m1.1.1.2.3.3">𝑁</ci></apply></apply><apply id="S6.E4.m1.1.1.3.cmml" xref="S6.E4.m1.1.1.3"><plus id="S6.E4.m1.1.1.3.1.cmml" xref="S6.E4.m1.1.1.3.1"></plus><apply id="S6.E4.m1.1.1.3.2.cmml" xref="S6.E4.m1.1.1.3.2"><times id="S6.E4.m1.1.1.3.2.1.cmml" xref="S6.E4.m1.1.1.3.2.1"></times><ci id="S6.E4.m1.1.1.3.2.2.cmml" xref="S6.E4.m1.1.1.3.2.2">𝑇</ci><ci id="S6.E4.m1.1.1.3.2.3.cmml" xref="S6.E4.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E4.m1.1.1.3.3.cmml" xref="S6.E4.m1.1.1.3.3"><times id="S6.E4.m1.1.1.3.3.1.cmml" xref="S6.E4.m1.1.1.3.3.1"></times><ci id="S6.E4.m1.1.1.3.3.2.cmml" xref="S6.E4.m1.1.1.3.3.2">𝐹</ci><ci id="S6.E4.m1.1.1.3.3.3.cmml" xref="S6.E4.m1.1.1.3.3.3">𝑁</ci></apply><apply id="S6.E4.m1.1.1.3.4.cmml" xref="S6.E4.m1.1.1.3.4"><times id="S6.E4.m1.1.1.3.4.1.cmml" xref="S6.E4.m1.1.1.3.4.1"></times><ci id="S6.E4.m1.1.1.3.4.2.cmml" xref="S6.E4.m1.1.1.3.4.2">𝑇</ci><ci id="S6.E4.m1.1.1.3.4.3.cmml" xref="S6.E4.m1.1.1.3.4.3">𝑁</ci></apply><apply id="S6.E4.m1.1.1.3.5.cmml" xref="S6.E4.m1.1.1.3.5"><times id="S6.E4.m1.1.1.3.5.1.cmml" xref="S6.E4.m1.1.1.3.5.1"></times><ci id="S6.E4.m1.1.1.3.5.2.cmml" xref="S6.E4.m1.1.1.3.5.2">𝐹</ci><ci id="S6.E4.m1.1.1.3.5.3.cmml" xref="S6.E4.m1.1.1.3.5.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E4.m1.2c">Accuracy=\left(\cfrac{TP+TN}{TP+FN+TN+FP}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In the works investigated in section <a href="#S4" title="IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, accuracy is used as the primary performance measure. However, caution should be taken with unbalanced class distributions between drowsiness and alert labels. In cases where the evaluation data is imbalanced with respect to these labels, we advocate the use of the balanced accuracy that takes this imbalance in the class distribution into account. The mathematical equation is given by:</p>
<table id="S6.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E5.m1.1" class="ltx_Math" alttext="Balanced-Accuracy=0.5*\left(\cfrac{TP}{TP+FN}+\cfrac{TN}{TN+FP}\right)." display="block"><semantics id="S6.E5.m1.1a"><mrow id="S6.E5.m1.1.1.1" xref="S6.E5.m1.1.1.1.1.cmml"><mrow id="S6.E5.m1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.cmml"><mrow id="S6.E5.m1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.3.2" xref="S6.E5.m1.1.1.1.1.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.3.2.2" xref="S6.E5.m1.1.1.1.1.3.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.3" xref="S6.E5.m1.1.1.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1a" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.4" xref="S6.E5.m1.1.1.1.1.3.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1b" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.5" xref="S6.E5.m1.1.1.1.1.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1c" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.6" xref="S6.E5.m1.1.1.1.1.3.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1d" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.7" xref="S6.E5.m1.1.1.1.1.3.2.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1e" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.8" xref="S6.E5.m1.1.1.1.1.3.2.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.2.1f" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.2.9" xref="S6.E5.m1.1.1.1.1.3.2.9.cmml">d</mi></mrow><mo id="S6.E5.m1.1.1.1.1.3.1" xref="S6.E5.m1.1.1.1.1.3.1.cmml">−</mo><mrow id="S6.E5.m1.1.1.1.1.3.3" xref="S6.E5.m1.1.1.1.1.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.3.3.2" xref="S6.E5.m1.1.1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.3" xref="S6.E5.m1.1.1.1.1.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1a" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.4" xref="S6.E5.m1.1.1.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1b" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.5" xref="S6.E5.m1.1.1.1.1.3.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1c" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.6" xref="S6.E5.m1.1.1.1.1.3.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1d" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.7" xref="S6.E5.m1.1.1.1.1.3.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1e" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.8" xref="S6.E5.m1.1.1.1.1.3.3.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.3.3.1f" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.3.3.9" xref="S6.E5.m1.1.1.1.1.3.3.9.cmml">y</mi></mrow></mrow><mo id="S6.E5.m1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S6.E5.m1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.cmml"><mn id="S6.E5.m1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.3.cmml">0.5</mn><mo lspace="0.222em" rspace="0.222em" id="S6.E5.m1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.2.cmml">∗</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S6.E5.m1.1.1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S6.E5.m1.1.1.1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">P</mi></mrow><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">P</mi></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">+</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">N</mi></mrow></mrow></mfrac><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S6.E5.m1.1.1.1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">N</mi></mrow><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml">N</mi></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S6.E5.m1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E5.m1.1b"><apply id="S6.E5.m1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1"><eq id="S6.E5.m1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.2"></eq><apply id="S6.E5.m1.1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.1.3"><minus id="S6.E5.m1.1.1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.1.1.3.1"></minus><apply id="S6.E5.m1.1.1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.1.1.3.2"><times id="S6.E5.m1.1.1.1.1.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.3.2.2">𝐵</ci><ci id="S6.E5.m1.1.1.1.1.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.3.2.3">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.2.4.cmml" xref="S6.E5.m1.1.1.1.1.3.2.4">𝑙</ci><ci id="S6.E5.m1.1.1.1.1.3.2.5.cmml" xref="S6.E5.m1.1.1.1.1.3.2.5">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.2.6.cmml" xref="S6.E5.m1.1.1.1.1.3.2.6">𝑛</ci><ci id="S6.E5.m1.1.1.1.1.3.2.7.cmml" xref="S6.E5.m1.1.1.1.1.3.2.7">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.2.8.cmml" xref="S6.E5.m1.1.1.1.1.3.2.8">𝑒</ci><ci id="S6.E5.m1.1.1.1.1.3.2.9.cmml" xref="S6.E5.m1.1.1.1.1.3.2.9">𝑑</ci></apply><apply id="S6.E5.m1.1.1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.1.1.3.3"><times id="S6.E5.m1.1.1.1.1.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.3.3.2">𝐴</ci><ci id="S6.E5.m1.1.1.1.1.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.3.3.3">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.4.cmml" xref="S6.E5.m1.1.1.1.1.3.3.4">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.5.cmml" xref="S6.E5.m1.1.1.1.1.3.3.5">𝑢</ci><ci id="S6.E5.m1.1.1.1.1.3.3.6.cmml" xref="S6.E5.m1.1.1.1.1.3.3.6">𝑟</ci><ci id="S6.E5.m1.1.1.1.1.3.3.7.cmml" xref="S6.E5.m1.1.1.1.1.3.3.7">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.3.8.cmml" xref="S6.E5.m1.1.1.1.1.3.3.8">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.9.cmml" xref="S6.E5.m1.1.1.1.1.3.3.9">𝑦</ci></apply></apply><apply id="S6.E5.m1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1"><times id="S6.E5.m1.1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.1.2"></times><cn type="float" id="S6.E5.m1.1.1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.1.1.3">0.5</cn><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S6.E5.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2">continued-fraction</csymbol><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3">𝑃</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3">𝑃</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2">𝐹</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3">𝑁</ci></apply></apply></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3">𝑁</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3">𝑁</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E5.m1.1c">Balanced-Accuracy=0.5*\left(\cfrac{TP}{TP+FN}+\cfrac{TN}{TN+FP}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Further metrics include area under the receiver operating characteristic (ROC) Curve (AUC-ROC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> which evaluates the classifier’s ability to distinguish between drowsy and non-drowsy instances across different threshold values and Mean Squared Error (MSE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> or Root Mean Squared Error (RMSE)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> which quantifies the average squared differences between predicted drowsiness levels and actual levels. Dua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> reported their results further in terms of a confusion matrix. This tabular representation comprises all four metrics (TP, TN, FP, and FN) representing the performance of the binary drowsiness detection classifier. Based on the four metrics, the authors also provide evaluation metrics such as <span id="S6.p3.1.1" class="ltx_text ltx_font_italic">sensitivity, specificity, precision</span>, and <span id="S6.p3.1.2" class="ltx_text ltx_font_italic">F1-score</span>. The mathematical formulation for <span id="S6.p3.1.3" class="ltx_text ltx_font_italic">specificity</span> is given in equation (<a href="#S6.E6" title="In VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) as:</p>
<table id="S6.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E6.m1.2" class="ltx_Math" alttext="Specificity=\left(\cfrac{TN}{TN+FP}\right)." display="block"><semantics id="S6.E6.m1.2a"><mrow id="S6.E6.m1.2.2.1" xref="S6.E6.m1.2.2.1.1.cmml"><mrow id="S6.E6.m1.2.2.1.1" xref="S6.E6.m1.2.2.1.1.cmml"><mrow id="S6.E6.m1.2.2.1.1.2" xref="S6.E6.m1.2.2.1.1.2.cmml"><mi id="S6.E6.m1.2.2.1.1.2.2" xref="S6.E6.m1.2.2.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.3" xref="S6.E6.m1.2.2.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1a" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.4" xref="S6.E6.m1.2.2.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1b" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.5" xref="S6.E6.m1.2.2.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1c" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.6" xref="S6.E6.m1.2.2.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1d" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.7" xref="S6.E6.m1.2.2.1.1.2.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1e" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.8" xref="S6.E6.m1.2.2.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1f" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.9" xref="S6.E6.m1.2.2.1.1.2.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1g" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.10" xref="S6.E6.m1.2.2.1.1.2.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1h" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.11" xref="S6.E6.m1.2.2.1.1.2.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.2.2.1.1.2.1i" xref="S6.E6.m1.2.2.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.2.2.1.1.2.12" xref="S6.E6.m1.2.2.1.1.2.12.cmml">y</mi></mrow><mo id="S6.E6.m1.2.2.1.1.1" xref="S6.E6.m1.2.2.1.1.1.cmml">=</mo><mrow id="S6.E6.m1.2.2.1.1.3.2" xref="S6.E6.m1.1.1.cmml"><mo id="S6.E6.m1.2.2.1.1.3.2.1" xref="S6.E6.m1.1.1.cmml">(</mo><mfrac id="S6.E6.m1.1.1" xref="S6.E6.m1.1.1.cmml"><mrow id="S6.E6.m1.1.1.2" xref="S6.E6.m1.1.1.2.cmml"><mi id="S6.E6.m1.1.1.2.2" xref="S6.E6.m1.1.1.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.1.1.2.1" xref="S6.E6.m1.1.1.2.1.cmml">​</mo><mi id="S6.E6.m1.1.1.2.3" xref="S6.E6.m1.1.1.2.3.cmml">N</mi></mrow><mrow id="S6.E6.m1.1.1.3" xref="S6.E6.m1.1.1.3.cmml"><mrow id="S6.E6.m1.1.1.3.2" xref="S6.E6.m1.1.1.3.2.cmml"><mi id="S6.E6.m1.1.1.3.2.2" xref="S6.E6.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.1.1.3.2.1" xref="S6.E6.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.E6.m1.1.1.3.2.3" xref="S6.E6.m1.1.1.3.2.3.cmml">N</mi></mrow><mo id="S6.E6.m1.1.1.3.1" xref="S6.E6.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E6.m1.1.1.3.3" xref="S6.E6.m1.1.1.3.3.cmml"><mi id="S6.E6.m1.1.1.3.3.2" xref="S6.E6.m1.1.1.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.1.1.3.3.1" xref="S6.E6.m1.1.1.3.3.1.cmml">​</mo><mi id="S6.E6.m1.1.1.3.3.3" xref="S6.E6.m1.1.1.3.3.3.cmml">P</mi></mrow></mrow></mfrac><mo id="S6.E6.m1.2.2.1.1.3.2.2" xref="S6.E6.m1.1.1.cmml">)</mo></mrow></mrow><mo lspace="0em" id="S6.E6.m1.2.2.1.2" xref="S6.E6.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E6.m1.2b"><apply id="S6.E6.m1.2.2.1.1.cmml" xref="S6.E6.m1.2.2.1"><eq id="S6.E6.m1.2.2.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1"></eq><apply id="S6.E6.m1.2.2.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.2"><times id="S6.E6.m1.2.2.1.1.2.1.cmml" xref="S6.E6.m1.2.2.1.1.2.1"></times><ci id="S6.E6.m1.2.2.1.1.2.2.cmml" xref="S6.E6.m1.2.2.1.1.2.2">𝑆</ci><ci id="S6.E6.m1.2.2.1.1.2.3.cmml" xref="S6.E6.m1.2.2.1.1.2.3">𝑝</ci><ci id="S6.E6.m1.2.2.1.1.2.4.cmml" xref="S6.E6.m1.2.2.1.1.2.4">𝑒</ci><ci id="S6.E6.m1.2.2.1.1.2.5.cmml" xref="S6.E6.m1.2.2.1.1.2.5">𝑐</ci><ci id="S6.E6.m1.2.2.1.1.2.6.cmml" xref="S6.E6.m1.2.2.1.1.2.6">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.7.cmml" xref="S6.E6.m1.2.2.1.1.2.7">𝑓</ci><ci id="S6.E6.m1.2.2.1.1.2.8.cmml" xref="S6.E6.m1.2.2.1.1.2.8">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.9.cmml" xref="S6.E6.m1.2.2.1.1.2.9">𝑐</ci><ci id="S6.E6.m1.2.2.1.1.2.10.cmml" xref="S6.E6.m1.2.2.1.1.2.10">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.11.cmml" xref="S6.E6.m1.2.2.1.1.2.11">𝑡</ci><ci id="S6.E6.m1.2.2.1.1.2.12.cmml" xref="S6.E6.m1.2.2.1.1.2.12">𝑦</ci></apply><apply id="S6.E6.m1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.3.2"><csymbol cd="latexml" id="S6.E6.m1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.3.2">continued-fraction</csymbol><apply id="S6.E6.m1.1.1.2.cmml" xref="S6.E6.m1.1.1.2"><times id="S6.E6.m1.1.1.2.1.cmml" xref="S6.E6.m1.1.1.2.1"></times><ci id="S6.E6.m1.1.1.2.2.cmml" xref="S6.E6.m1.1.1.2.2">𝑇</ci><ci id="S6.E6.m1.1.1.2.3.cmml" xref="S6.E6.m1.1.1.2.3">𝑁</ci></apply><apply id="S6.E6.m1.1.1.3.cmml" xref="S6.E6.m1.1.1.3"><plus id="S6.E6.m1.1.1.3.1.cmml" xref="S6.E6.m1.1.1.3.1"></plus><apply id="S6.E6.m1.1.1.3.2.cmml" xref="S6.E6.m1.1.1.3.2"><times id="S6.E6.m1.1.1.3.2.1.cmml" xref="S6.E6.m1.1.1.3.2.1"></times><ci id="S6.E6.m1.1.1.3.2.2.cmml" xref="S6.E6.m1.1.1.3.2.2">𝑇</ci><ci id="S6.E6.m1.1.1.3.2.3.cmml" xref="S6.E6.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S6.E6.m1.1.1.3.3.cmml" xref="S6.E6.m1.1.1.3.3"><times id="S6.E6.m1.1.1.3.3.1.cmml" xref="S6.E6.m1.1.1.3.3.1"></times><ci id="S6.E6.m1.1.1.3.3.2.cmml" xref="S6.E6.m1.1.1.3.3.2">𝐹</ci><ci id="S6.E6.m1.1.1.3.3.3.cmml" xref="S6.E6.m1.1.1.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E6.m1.2c">Specificity=\left(\cfrac{TN}{TN+FP}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S6.p3.2" class="ltx_p">It is noteworthy, that the term <span id="S6.p3.2.1" class="ltx_text ltx_font_italic">sensitivity</span> is commonly used as a synonym to the term <span id="S6.p3.2.2" class="ltx_text ltx_font_italic">recall</span> and thus there is no need to duplicate the equation. Some works also simply reported the ”Accuracy” of their algorithm. <span id="S6.p3.2.3" class="ltx_text ltx_font_italic">Accuracy</span> in this sense is dependent on the FN and FP, but unlike <span id="S6.p3.2.4" class="ltx_text ltx_font_italic">F1-score</span>, does not consider the possible imbalance in the data. Even though it is not reported in detail in most works, the accuracy can be given simply by the ratio of misclassified samples to all evaluated samples, regardless of their ground-truth label balance.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.4" class="ltx_p">Another performance metric, when driver drowsiness detection is considered as a regression task instead of a classification task, is the case to predict the accuracy of continuous drowsiness level. Such a performance measure proposed by Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> is called the drowsiness index (DI). They measured the difference between the individual response time (RT) and the true alert state (alert RT) in a simulated lane-keeping task. The formulation is given by equation (<a href="#S6.E7" title="In VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) as:</p>
<table id="S6.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E7.m1.4" class="ltx_Math" alttext="DI=max\biggl{(}0,\cfrac{1-e^{-\alpha(\tau-\tau_{0})}}{1+e^{-\alpha(\tau-\tau_{0})}}\biggr{)}," display="block"><semantics id="S6.E7.m1.4a"><mrow id="S6.E7.m1.4.4.1" xref="S6.E7.m1.4.4.1.1.cmml"><mrow id="S6.E7.m1.4.4.1.1" xref="S6.E7.m1.4.4.1.1.cmml"><mrow id="S6.E7.m1.4.4.1.1.2" xref="S6.E7.m1.4.4.1.1.2.cmml"><mi id="S6.E7.m1.4.4.1.1.2.2" xref="S6.E7.m1.4.4.1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.4.4.1.1.2.1" xref="S6.E7.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S6.E7.m1.4.4.1.1.2.3" xref="S6.E7.m1.4.4.1.1.2.3.cmml">I</mi></mrow><mo id="S6.E7.m1.4.4.1.1.1" xref="S6.E7.m1.4.4.1.1.1.cmml">=</mo><mrow id="S6.E7.m1.4.4.1.1.3" xref="S6.E7.m1.4.4.1.1.3.cmml"><mi id="S6.E7.m1.4.4.1.1.3.2" xref="S6.E7.m1.4.4.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.4.4.1.1.3.1" xref="S6.E7.m1.4.4.1.1.3.1.cmml">​</mo><mi id="S6.E7.m1.4.4.1.1.3.3" xref="S6.E7.m1.4.4.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.4.4.1.1.3.1a" xref="S6.E7.m1.4.4.1.1.3.1.cmml">​</mo><mi id="S6.E7.m1.4.4.1.1.3.4" xref="S6.E7.m1.4.4.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.4.4.1.1.3.1b" xref="S6.E7.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S6.E7.m1.4.4.1.1.3.5.2" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml"><mo maxsize="210%" minsize="210%" id="S6.E7.m1.4.4.1.1.3.5.2.1" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">(</mo><mn id="S6.E7.m1.3.3" xref="S6.E7.m1.3.3.cmml">0</mn><mo id="S6.E7.m1.4.4.1.1.3.5.2.2" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">,</mo><mfrac id="S6.E7.m1.2.2" xref="S6.E7.m1.2.2.cmml"><mrow id="S6.E7.m1.1.1.1" xref="S6.E7.m1.1.1.1.cmml"><mn id="S6.E7.m1.1.1.1.3" xref="S6.E7.m1.1.1.1.3.cmml">1</mn><mo id="S6.E7.m1.1.1.1.2" xref="S6.E7.m1.1.1.1.2.cmml">−</mo><msup id="S6.E7.m1.1.1.1.4" xref="S6.E7.m1.1.1.1.4.cmml"><mi id="S6.E7.m1.1.1.1.4.2" xref="S6.E7.m1.1.1.1.4.2.cmml">e</mi><mrow id="S6.E7.m1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.cmml"><mo id="S6.E7.m1.1.1.1.1.1a" xref="S6.E7.m1.1.1.1.1.1.cmml">−</mo><mrow id="S6.E7.m1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.3" xref="S6.E7.m1.1.1.1.1.1.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.1.1.1.1.1.1.2" xref="S6.E7.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S6.E7.m1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.E7.m1.1.1.1.1.1.1.1.1.2" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E7.m1.1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.1.1.1.2" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml">τ</mi><mo id="S6.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">τ</mi><mn id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S6.E7.m1.1.1.1.1.1.1.1.1.3" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow><mrow id="S6.E7.m1.2.2.2" xref="S6.E7.m1.2.2.2.cmml"><mn id="S6.E7.m1.2.2.2.3" xref="S6.E7.m1.2.2.2.3.cmml">1</mn><mo id="S6.E7.m1.2.2.2.2" xref="S6.E7.m1.2.2.2.2.cmml">+</mo><msup id="S6.E7.m1.2.2.2.4" xref="S6.E7.m1.2.2.2.4.cmml"><mi id="S6.E7.m1.2.2.2.4.2" xref="S6.E7.m1.2.2.2.4.2.cmml">e</mi><mrow id="S6.E7.m1.2.2.2.1.1" xref="S6.E7.m1.2.2.2.1.1.cmml"><mo id="S6.E7.m1.2.2.2.1.1a" xref="S6.E7.m1.2.2.2.1.1.cmml">−</mo><mrow id="S6.E7.m1.2.2.2.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.3" xref="S6.E7.m1.2.2.2.1.1.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S6.E7.m1.2.2.2.1.1.1.2" xref="S6.E7.m1.2.2.2.1.1.1.2.cmml">​</mo><mrow id="S6.E7.m1.2.2.2.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.E7.m1.2.2.2.1.1.1.1.1.2" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E7.m1.2.2.2.1.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.1.1.1.2" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.2.cmml">τ</mi><mo id="S6.E7.m1.2.2.2.1.1.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml">τ</mi><mn id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S6.E7.m1.2.2.2.1.1.1.1.1.3" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo maxsize="210%" minsize="210%" id="S6.E7.m1.4.4.1.1.3.5.2.3" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">)</mo></mrow></mrow></mrow><mo id="S6.E7.m1.4.4.1.2" xref="S6.E7.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E7.m1.4b"><apply id="S6.E7.m1.4.4.1.1.cmml" xref="S6.E7.m1.4.4.1"><eq id="S6.E7.m1.4.4.1.1.1.cmml" xref="S6.E7.m1.4.4.1.1.1"></eq><apply id="S6.E7.m1.4.4.1.1.2.cmml" xref="S6.E7.m1.4.4.1.1.2"><times id="S6.E7.m1.4.4.1.1.2.1.cmml" xref="S6.E7.m1.4.4.1.1.2.1"></times><ci id="S6.E7.m1.4.4.1.1.2.2.cmml" xref="S6.E7.m1.4.4.1.1.2.2">𝐷</ci><ci id="S6.E7.m1.4.4.1.1.2.3.cmml" xref="S6.E7.m1.4.4.1.1.2.3">𝐼</ci></apply><apply id="S6.E7.m1.4.4.1.1.3.cmml" xref="S6.E7.m1.4.4.1.1.3"><times id="S6.E7.m1.4.4.1.1.3.1.cmml" xref="S6.E7.m1.4.4.1.1.3.1"></times><ci id="S6.E7.m1.4.4.1.1.3.2.cmml" xref="S6.E7.m1.4.4.1.1.3.2">𝑚</ci><ci id="S6.E7.m1.4.4.1.1.3.3.cmml" xref="S6.E7.m1.4.4.1.1.3.3">𝑎</ci><ci id="S6.E7.m1.4.4.1.1.3.4.cmml" xref="S6.E7.m1.4.4.1.1.3.4">𝑥</ci><interval closure="open" id="S6.E7.m1.4.4.1.1.3.5.1.cmml" xref="S6.E7.m1.4.4.1.1.3.5.2"><cn type="integer" id="S6.E7.m1.3.3.cmml" xref="S6.E7.m1.3.3">0</cn><apply id="S6.E7.m1.2.2.cmml" xref="S6.E7.m1.2.2"><csymbol cd="latexml" id="S6.E7.m1.2.2.3.cmml" xref="S6.E7.m1.2.2">continued-fraction</csymbol><apply id="S6.E7.m1.1.1.1.cmml" xref="S6.E7.m1.1.1.1"><minus id="S6.E7.m1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.2"></minus><cn type="integer" id="S6.E7.m1.1.1.1.3.cmml" xref="S6.E7.m1.1.1.1.3">1</cn><apply id="S6.E7.m1.1.1.1.4.cmml" xref="S6.E7.m1.1.1.1.4"><csymbol cd="ambiguous" id="S6.E7.m1.1.1.1.4.1.cmml" xref="S6.E7.m1.1.1.1.4">superscript</csymbol><ci id="S6.E7.m1.1.1.1.4.2.cmml" xref="S6.E7.m1.1.1.1.4.2">𝑒</ci><apply id="S6.E7.m1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1"><minus id="S6.E7.m1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1"></minus><apply id="S6.E7.m1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1"><times id="S6.E7.m1.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.2"></times><ci id="S6.E7.m1.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.1.1.1.1.1.1.3">𝛼</ci><apply id="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1"><minus id="S6.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S6.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.2">𝜏</ci><apply id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2">𝜏</ci><cn type="integer" id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply></apply></apply></apply><apply id="S6.E7.m1.2.2.2.cmml" xref="S6.E7.m1.2.2.2"><plus id="S6.E7.m1.2.2.2.2.cmml" xref="S6.E7.m1.2.2.2.2"></plus><cn type="integer" id="S6.E7.m1.2.2.2.3.cmml" xref="S6.E7.m1.2.2.2.3">1</cn><apply id="S6.E7.m1.2.2.2.4.cmml" xref="S6.E7.m1.2.2.2.4"><csymbol cd="ambiguous" id="S6.E7.m1.2.2.2.4.1.cmml" xref="S6.E7.m1.2.2.2.4">superscript</csymbol><ci id="S6.E7.m1.2.2.2.4.2.cmml" xref="S6.E7.m1.2.2.2.4.2">𝑒</ci><apply id="S6.E7.m1.2.2.2.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1"><minus id="S6.E7.m1.2.2.2.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1"></minus><apply id="S6.E7.m1.2.2.2.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1"><times id="S6.E7.m1.2.2.2.1.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.2"></times><ci id="S6.E7.m1.2.2.2.1.1.1.3.cmml" xref="S6.E7.m1.2.2.2.1.1.1.3">𝛼</ci><apply id="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1"><minus id="S6.E7.m1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.1"></minus><ci id="S6.E7.m1.2.2.2.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.2">𝜏</ci><apply id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2">𝜏</ci><cn type="integer" id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply></apply></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E7.m1.4c">DI=max\biggl{(}0,\cfrac{1-e^{-\alpha(\tau-\tau_{0})}}{1+e^{-\alpha(\tau-\tau_{0})}}\biggr{)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S6.p4.3" class="ltx_p">where <math id="S6.p4.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S6.p4.1.m1.1a"><mi id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><ci id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">\tau</annotation></semantics></math> denotes the RT of the given lane-departure event, <math id="S6.p4.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S6.p4.2.m2.1a"><mi id="S6.p4.2.m2.1.1" xref="S6.p4.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S6.p4.2.m2.1b"><ci id="S6.p4.2.m2.1.1.cmml" xref="S6.p4.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.2.m2.1c">a</annotation></semantics></math> is a constant, and <math id="S6.p4.3.m3.1" class="ltx_Math" alttext="\tau_{0}" display="inline"><semantics id="S6.p4.3.m3.1a"><msub id="S6.p4.3.m3.1.1" xref="S6.p4.3.m3.1.1.cmml"><mi id="S6.p4.3.m3.1.1.2" xref="S6.p4.3.m3.1.1.2.cmml">τ</mi><mn id="S6.p4.3.m3.1.1.3" xref="S6.p4.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p4.3.m3.1b"><apply id="S6.p4.3.m3.1.1.cmml" xref="S6.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S6.p4.3.m3.1.1.1.cmml" xref="S6.p4.3.m3.1.1">subscript</csymbol><ci id="S6.p4.3.m3.1.1.2.cmml" xref="S6.p4.3.m3.1.1.2">𝜏</ci><cn type="integer" id="S6.p4.3.m3.1.1.3.cmml" xref="S6.p4.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.3.m3.1c">\tau_{0}</annotation></semantics></math> denotes the true alert RT. In their work, they continuously predict this score and related this measure to the continuous driver’s drowsiness level.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Paulo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> also used a leave-one-subject-out cross-validation strategy to validate their proposed classification method. Because the authors intended to perform classification without individual-dependent calibration from EEG signals, they first carried out the validation at the subject level to understand the individual contribution. By further selecting groups of subjects with major individual contributions, a better cross-subject generalizable model is created.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">These metrics in this section help to assess the effectiveness, accuracy, and reliability of drowsiness detection systems and algorithms, enabling researchers and developers to improve their models for better real-world applicability and safety. The performance metrics used are also reported in Table <a href="#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, <a href="#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, which show the achieved performance on bench-marking datasets for each of the three measurement techniques considered.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Technical and Practical Limitations</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Application areas of drowsiness detection are broad. It is of paramount importance across diverse domains due to its profound impact on safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, productivity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and healthcare<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As extensively studied in this survey, whether on the road, in workplaces, or in critical operational environments, the ability to accurately identify and mitigate drowsiness can have far-reaching implications. In this section, we focus on identifying certain weaknesses in current algorithms and uncovering limitations in existing research. We do this with respect to two main categories, i.e., vision-based and physiological signal-based approaches.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.4.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.5.2" class="ltx_text ltx_font_italic">Limitations on vision-based technique</span>
</h3>

<section id="S7.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From the database perspective</h4>

<div id="S7.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px1.p1.1" class="ltx_p">The increasing number of public benchmark databases for driver drowsiness detection is partially due to the rising number of fatal traffic accidents, but also following the trend towards autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>. The National Highway Traffic Safety Administration (NHTSA) published the Drowsy Driving Research and Program Plan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> in 2016 estimating that 2% to 20% of annual traffic deaths are attributable to driver drowsiness. As we have observed in previous Section <a href="#S5" title="V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, there are several vision-based databases that can be leveraged as benchmarking systems for developing driver drowsiness detection solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>. However, a major limitation is that these databases often contain only simulation data or were collected under strictly controlled environments, such as indoors or in parked vehicles as reviewed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This makes developing solutions that would work in real life scenarios more challenging. Additionally, it makes estimating the performance in real deployment questionable as the data does not represent all the variations in such scenarios. To alleviate this problem, the trend moves from simulation data to real-world data. Additional databases have been curated that have been collected under real driving conditions and incorporated various aspects of the real environment, such as different lightening and road physics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. However, it is not only tedious but also risky to collect data in a moving vehicle and it is difficult to capture all variability mimicking a real environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. In addition, problems such as occlusion-free capturing and accurate labeling also play an important role, which lead to works as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite> dealing with capturing of realistic and diverse drowsiness data.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From the algorithmic perspective</h4>

<div id="S7.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p1.1" class="ltx_p">Most vision-based driver drowsiness detection schemes focus on observing the individual’s facial attributes, such as face expression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>, head position <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, pupil diameter state, eye blink and eye movement (PERCLOS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>. With the advancement of deep learning, more accurate and efficient extraction of face and facial landmarks becomes possible, making the drowsiness detection on facial features more accurate and real-time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Popular face detection methods include MTCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> and RetinaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite> that allow more precise and accurate face detection under more challenging environments. Other approaches like accurate facial landmark detection in the wild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> also help to improve the detection of fine-grained facial expressions under varying and challenging situations.
For integrating face recognition on devices with limited hardware resources, previous works leveraged extremely lightweight face recognition networks from knowledge distillation or model quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. To overcome the limitation of occlusion in vision-based drowsiness detection, research can benefit from methods developed for improving face detection performance under masked faces as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>. Working on robust vision-based algorithms coping with variety of challenges faced under real life scenarios is thus a very promising future research direction. Multi angles processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> and key frames selection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> are also the upcoming challenges for video processing in vision-based drowsiness detection.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From the biased data perspective</h4>

<div id="S7.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px3.p1.1" class="ltx_p">Proper datasets play a pivotal role in the training of deep neural networks. When datasets lack representativeness, trained models can become biased and struggle to generalize to real-world scenarios. This concern is particularly pronounced in models trained within specific cultural contexts, potentially leading to inadequate generalization due to limited racial diversity representation. This challenge is amplified in the context of driver drowsiness detection, where publicly available vision-based datasets often focus on specific ethnic groups, resulting in an incomplete picture. Ngxande et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> addressed this issue by utilizing a GAN-based method for data augmentation. They used a population bias visualization strategy to group similar facial attributes and highlight the model weaknesses in such samples. The approach involved fine-tuning the CNN model using a sampling technique for faces with subpar performance. Experimental outcomes demonstrated the effectiveness of this approach in enhancing driver drowsiness detection for ethnic groups that were underrepresented. Under the same context of data, studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> dealing with how to induce drowsiness and collecting realistic drowsy driving data both in real traffic and under simulation are thus very important for developing robust detection algorithms.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From on-site hardware limitation perspective</h4>

<div id="S7.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px4.p1.1" class="ltx_p">The requirement for a real-time and on-site driver monitoring system is crucial to avert motor vehicle accidents attributed to driver inattentiveness or drowsiness. However, onboard hardware often possesses limited computational resources. Recent advancements in deep learning, particularly model compression and distillation techniques, have made it feasible to construct compact yet highly accurate models on embedded systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, such as those integrated into vehicles. Reddy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposed employing model compression to transition from a resource-intensive baseline model to a lightweight model suitable for deployment on an embedded board device. The suggested model based on facial landmarks achieved an accuracy of 89.5% for a 3-classes classification task, operating at a speed of 14.9 frames per second (FPS) on the Jetson TK1 platform. With the European Union (EU) mandating the introduction of driver drowsiness and alertness warning (DDAW) systems for all new vehicles from 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, the development and installation of accurate and resource-efficient algorithms to detect drowsiness in real time in the vehicle is becoming an urgent issue.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From data synthesis perspective</h4>

<div id="S7.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px5.p1.1" class="ltx_p">Because of the considerable expenses associated with dataset acquisition and the lack of adequate datasets discussed earlier, we propose to use synthetic data to study the common characteristics and the various hidden impacts in data for drowsiness detection. Inspired by the insights gained from the synthetic data, we can further extend to other downstream tasks. This also allows us to uncover different causal aspects for drowsiness detection. Most works focus on finding the correlations in signal variability to the different states of drowsiness, but less on the causality aspects. We consider this to be a very promising research direction with regard to explainability. Kong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> proposed to use Granger Causality Network to investigate driver fatigue and alertness state in EEG signals in 2015. The whole experiment included twelve young and healthy participants by recording their mental states under different simulated driving conditions and the data was analyzed by using Granger-Causality-based brain effective networks. Using such foundation models for other tasks, such as training face recognition solutions, have already gained increased interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.4.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.5.2" class="ltx_text ltx_font_italic">Limitations on physiological signal based technique</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Physiological signal based techniques shares, to some degree, most of the limitations related to vision based techniques discussed above. Here, we focus on these limitations specifically linked to the nature of physiological signal based technique.</p>
</div>
<section id="S7.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From motion artifacts perspective</h4>

<div id="S7.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS2.SSS0.Px1.p1.1" class="ltx_p">Multiple studies have investigated the correlation between EEG and driving behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, consistently highlighting EEG as the most predictive physiological indicator of drowsiness. The fluctuation in band activities within EEG signals in the spectral-frequency domain offers dominant insights into varying levels of drowsiness. In contrast to vision-based detection methods that often identify drowsiness after the onset of actual sleep or during advanced drowsiness stages, technologies based on physiological signals enable the early alerting of drivers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, averting potentially catastrophic accidents. However, a challenge arises from relevant signal extraction from motion artifacts by capturing biological signals either due to the motion of vehicles, individuals, or remote capture. This makes developing robust solutions what would work in real driving scenarios more challenging. Therefore, there are extensive approaches dealing with the removal of motion artifacts from EEG recordings. Relevant works include using methods like signal decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>, wavelet decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>, <a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>, and detrended fluctuation analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>.</p>
</div>
</section>
<section id="S7.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From data transmission perspective</h4>

<div id="S7.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS2.SSS0.Px2.p1.1" class="ltx_p">Physiological signals have demonstrated their stability, reliability and precision, as they are less influenced by external factors like e.g., occlusions or variability in lightening, resulting in fewer false positive detections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>, <a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>. Nevertheless, such physiological sensing methods involving cables or wired electrodes can be obtrusive and inconvenient for signal capture. Consequently, there is a noticeable trend towards wireless sensing and communication to alleviate these issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. The hurdle lies in maintaining consistent connectivity and addressing weak signal strength in wireless setups. These challenges even exist under laboratory setup and become even more pronounced in uncontrolled environments, particularly when utilizing wearable platforms with a limited count of dry electrodes as stated by Gerwin Schalk<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, a neuroscientist at New York State Department of Health’s Wadsworth Center. Stable wireless data transmission and communication are thus a necessary requirement for developing high performance solutions. Signal imputation in case of data leakage is another solution to address the issue besides implementing fusion techniques. It is known that deep learning based models are often used to detect outliers or handle missing data both in handling image data or time-series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib183" title="" class="ltx_ref">183</a>, <a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>.</p>
</div>
</section>
<section id="S7.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">From the multimodal perspective</h4>

<div id="S7.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS2.SSS0.Px3.p1.1" class="ltx_p">Often drowsiness detection scheme based on only one modality is not robust enough and thus a fusion of several complementary modalities or methods would lead to a better overall system performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>. Samiee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> introduced data fusion using image-based features and driver-vehicle interaction as a strategy to address the issue of signal loss and boost the overall resilience of the driver drowsiness detection system. The outcome underlined the system’s primary strengths, which encompass dependable and reliable detection and a capacity to withstand input signal losses effectively. Sedik et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> investigated sensor fusion techniques in their research. Their approach involves integrating EEG, EOG, ECG, and EMG signals, resulting in enhanced system accuracy, faster detection time, and a robust drowsiness detection scheme. Signal processing techniques fuse FFT and Discrete Wavelet Transform that are applied for feature extraction and noise reduction. Various machine learning and deep learning classifiers are utilized for both multi-class and binary-class classifications. The proposed methodologies are validated through simulations in two scenarios addressing these classification tasks. The outcomes demonstrate that the proposed models exhibit high performance in detecting drowsiness state from medical signals, achieving detection accuracy of 90% and 96% for the multi-class and binary-class scenarios, respectively.
Recent works on multi-modal drowsiness detection systems also focused on the explainability of the algorithms it used as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. The interpretability of such models would strengthen confidence in the detection systems and increase their reliability, which is particularly important as the EU will make the installation of DDAW systems in new vehicles a legal requirement from 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>.</p>
</div>
<figure id="S7.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Benchmark drowsiness detection system with respect to features. Label annotations: x negative, o neutral, and + positive. We note that this assessment is based on current existing research and the negativity in any aspect of any category does not represent the potential of this category but is seen as a future research challenge. Therefore, this table is dynamic and can be changed with future research efforts.</figcaption>
<table id="S7.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.T5.1.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S7.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.2.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">data</span></td>
</tr>
<tr id="S7.T5.1.1.1.2.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">accessability</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">unobstrusive</span></td>
<td id="S7.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.4.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">processing</span></td>
</tr>
<tr id="S7.T5.1.1.1.4.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">complexity</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.5.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">calibration</span></td>
</tr>
<tr id="S7.T5.1.1.1.5.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">complexity</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.6.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">noise</span></td>
</tr>
<tr id="S7.T5.1.1.1.6.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">coupling</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.7.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">motion</span></td>
</tr>
<tr id="S7.T5.1.1.1.7.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">artifect</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.8.1" class="ltx_text ltx_font_bold">occlusion</span></td>
<td id="S7.T5.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.9.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.9.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.9.1.1.1.1" class="ltx_text ltx_font_bold">transmission</span></td>
</tr>
<tr id="S7.T5.1.1.1.9.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.9.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.9.1.2.1.1" class="ltx_text ltx_font_bold">stability</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<table id="S7.T5.1.1.1.10.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T5.1.1.1.10.1.1" class="ltx_tr">
<td id="S7.T5.1.1.1.10.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.10.1.1.1.1" class="ltx_text ltx_font_bold">detection</span></td>
</tr>
<tr id="S7.T5.1.1.1.10.1.2" class="ltx_tr">
<td id="S7.T5.1.1.1.10.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.10.1.2.1.1" class="ltx_text ltx_font_bold">accuracy</span></td>
</tr>
</table>
</td>
<td id="S7.T5.1.1.1.11" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T5.1.1.1.11.1" class="ltx_text ltx_font_bold">practability</span></td>
</tr>
<tr id="S7.T5.1.2.2" class="ltx_tr">
<td id="S7.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EEG</td>
<td id="S7.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.2.2.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.2.2.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
</tr>
<tr id="S7.T5.1.3.3" class="ltx_tr">
<td id="S7.T5.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">-wearable</td>
<td id="S7.T5.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.3.3.10" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.3.3.11" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
</tr>
<tr id="S7.T5.1.4.4" class="ltx_tr">
<td id="S7.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">ECG</td>
<td id="S7.T5.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.4.4.11" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
</tr>
<tr id="S7.T5.1.5.5" class="ltx_tr">
<td id="S7.T5.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">-wearable</td>
<td id="S7.T5.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td id="S7.T5.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.5.5.11" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
</tr>
<tr id="S7.T5.1.6.6" class="ltx_tr">
<td id="S7.T5.1.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Vision</td>
<td id="S7.T5.1.6.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td id="S7.T5.1.6.6.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.10" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td id="S7.T5.1.6.6.11" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S7.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>The diagram highlights the sensor types utilized for various drowsiness detection applications reported in previous works. This allows us to promptly pinpoint areas where specific sensor types are absent, promoting future research in those domains. This table does not limit the research in the field of drowsiness detection and is thus dynamic and can be changed with future research efforts.</figcaption>
<table id="S7.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.T6.1.1.1" class="ltx_tr">
<td id="S7.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Application areas</span></td>
<td id="S7.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Public transportation</span></td>
<td id="S7.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Aviation transportation</span></td>
<td id="S7.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Driver Monitoring</span></td>
<td id="S7.T6.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">Workplace Safety</span></td>
<td id="S7.T6.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">Healthcare</span></td>
<td id="S7.T6.1.1.1.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S7.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">Smart Homes</span></td>
</tr>
<tr id="S7.T6.1.2.2" class="ltx_tr">
<td id="S7.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EEG</td>
<td id="S7.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.2.2.7" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S7.T6.1.3.3" class="ltx_tr">
<td id="S7.T6.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">ECG</td>
<td id="S7.T6.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.3.3.5" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S7.T6.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.3.3.7" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S7.T6.1.4.4" class="ltx_tr">
<td id="S7.T6.1.4.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Vision</td>
<td id="S7.T6.1.4.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.4.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.4.4.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.4.4.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S7.T6.1.4.4.6" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S7.T6.1.4.4.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Potential Directions for Research</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Table <a href="#S7.T5" title="TABLE V ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> summarizes the limitations described in section <a href="#S7" title="VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> and provides a better overview of the relative strengths and weaknesses of the current sensing modalities and algorithms for drowsiness detection. It’s important to emphasize that this assessment relies on investigated research works in this survey. Any negative aspect identified within a category should not be interpreted as a reflection of its potential; rather, it’s viewed as a future research challenge.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Biosignals captured with EEG and EOG measurements are more precise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>, but less appropriate for use-cases as driver’s drowsiness detection. Both vehicle and subject motion would strongly affect the performance and the accuracy of the feature extraction process of most proposed methods in the literature. Trends towards wearable sensors reduced the limitation of electrode-based applications of the EEG and EOG approaches to certain extent. However, headphone-based devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> including the biosignal measurements are also not practicable for real world driving scenarios. Drivers are not safe wearing headphone devices while driving. Therefore, future research directions involve the development of more robust algorithms mitigating these motion artifacts (e.g., by using de-trending, or signal decomposition techniques) or require more appropriate setups without affecting the drivers while driving under real world scenarios. This can include automatic learnable augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite> techniques that would enrich the training data with more realistic variation.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Confusion may arise between the terms driver fatigue detection and driver drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>.
Fatigue state describes the degree of fatigue, which may not necessarily involve drowsiness or subject falling asleep. Many individuals can experience fatigue while still remaining cognitive vigilant and are still capable of driving safely. Therefore, it is essential to prioritize the detection of drowsiness, as drivers in this state are unconscious while driving. To tackle this issue, heart rate variability (HRV) derived from ECG-signals are often employed as a feature for detection. However, ECG signals in relation towards drowsiness are often hard to draw. Changes in the parasympathetic and sympathetic activity of the body can be related to different biological processes beyond drowsiness detection. Several works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> investigated the discriminative power of ECG-based features towards drowsiness and future trends hint towards more complex fusion methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> or more generalized self-learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">Table <a href="#S7.T5" title="TABLE V ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> indicates a clear advantage of vision-based solutions over a wide range of challenges faced by other two investigated measures, but also emphasized that such solutions are strongly affected by any application scenario where the measured subject might be occluded or under less favorable capture conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>. Features considered like the PERCLOS indicates the percentage of eye closures. This features strongly relates to the accuracy and efficiency of face detection and facial landmark detection. Video-based approaches of drowsiness detection often combine techniques for keyframe selections and final drowsiness state classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>. Other works intend to face the challenge of variation in head poses as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. Therefore, we believe that future research directions in vision-based approaches should consider these relevant topics, such as accurate and fast keyframe selections and creating more robust algorithms dealing with the dynamic challenges posed in a real-world setting.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">Finally, Table <a href="#S7.T6" title="TABLE VI ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the investigated measuring modalities successfully associated with different application areas. This allows researchers to have a clearer view to position their future research in the missing areas of drowsiness detection. However, this table should not limit the research in the field of drowsiness detection and is thus dynamic and can be changed with future research efforts.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Detecting drowsiness is of high significance in guaranteeing safety in various domains such as in workplaces requiring high concentration of employees, under real driving situations, in public transportation, and for aviation. Alert and well-rested employees also lead to enhanced productivity and better personal healthcare. This work is a pioneering work covering a wide application area of drowsiness detection with its modern applications and methods. We categorized our researched works into three measuring techniques, with multi-channel EEG signals, ECG signals, and vision-based detection schemes. We summarized and compared popular benchmarking databases and common evaluation metrics used to assess the performance of the developed drowsiness detection algorithms.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">We identified strengths and weaknesses in current algorithms and discussed the limitations of current research categorized under both physiological-based and vision-based approaches. We pinpointed challenges in accurate and real-time detection, in stable data transmission using wireless sensing technologies, and in building a bias-free system among others. We provide possible solutions like mitigating the bias by using synthetic, adversarial data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> or data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> techniques. Overcoming the hardware limitations requires model compression techniques to build small-scale but still highly accurate models, and leveraging the fusion of complementary modalities, methods, or sensors to lead to more robust and accurate detection resilient to noise or data loss. Finally, we believe that drowsiness detection remains an actively evolving field with abundant opportunities to explore, both when it comes to sensing technology and algorithmic development. The primary goal of this work is to provide an initial comprehensive survey of drowsiness detection within contemporary applications and methodologies.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Qin, Y. Niu, H. Zhou, X. Li, W. Jia, and Y. Zheng, “Driver drowsiness EEG detection based on tree federated learning and interpretable network,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Int. J. Neural Syst.</span>, vol. 33, no. 3, pp. 2350009:1–2350009:17, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Lee, J. Lee, and M. Shin, “Using wearable ECG/PPG sensors for driver drowsiness detection based on distinguishable pattern of recurrence plots,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Electronics</span>, vol. 8, no. 2, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W. Deng and R. Wu, “Real-time driver-drowsiness detection system using facial features,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 7, pp. 118727–118738, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
I. G. Daza, S. Bronte, L. M. Bergasa, J. Almazán, and J. J. Y. Torres, “Vision-based drowsiness detector for real driving conditions,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2012 IEEE Intelligent Vehicles Symposium, IV 2012, Alcal de Henares, Madrid, Spain, June 3-7, 2012</span>, pp. 618–623, IEEE, 2012.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Sahayadhas, K. Sundaraj, and M. Murugappan, “Detecting driver drowsiness based on sensors: A review,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 12, no. 12, pp. 16937–16953, 2012.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
National Highway Traffic Safety Administration, “NHTSA drowsy driving research and program plan.” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nhtsa.gov/sites/nhtsa.gov/files/drowsydriving_strategicplan_030316.pdf</span>, 2016.

</span>
<span class="ltx_bibblock">DOT publication HS 812 252. Accessed: (Februar 2, 2024).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Liu, H.-L. Chi, X. Li, and J. Guo, “Effects of dataset characteristics on the performance of fatigue detection for crane operators using hybrid deep neural networks,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Automation in Construction</span>, vol. 132, p. 103901, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Geiger-Brown, V. E. Rogers, A. M. Trinkoff, R. L. Kane, R. B. Bausell, and S. M. Scharf, “Sleep, sleepiness, fatigue, and performance of 12-hour-shift nurses,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Chronobiology international</span>, vol. 29, no. 2, pp. 211–219, 2012.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Mantzanas, D. Chrysikos, I. Giannakodimos, G. Chelidonis, P. Drymousi, E. Pechlivanidou, E. Pikoulis, N. Basios, C. G. Zografos, G. C. Zografos, <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Subjective sleep quality and daytime sleepiness among greek nursing staff: A multicenter cross-sectional study,” <span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Health &amp; Research Journal</span>, vol. 8, no. 3, pp. 214–224, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Juan-García, M. Plaza-Carmona, and N. Fernández-Martínez, “Sleep analysis in emergency nurses’ department,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Revista da Associação Médica Brasileira</span>, vol. 67, pp. 862–867, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Woodard, J. Adornetti, J. M. Nogales, M. Foster, L. Leask, R. McGee, M. Carlucci, S. Crowley, and A. Wolfson, “0064 youth sleep-wake experience in juvenile justice facilities: A descriptive analysis,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Sleep</span>, vol. 45, no. Supplement_1, pp. A29–A30, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. A. Kamran, M. M. N. Mannan, and M. Y. Jeong, “Drowsiness, fatigue and poor sleep’s causes and detection: A comprehensive study,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 7, pp. 167172–167186, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Papadelis, Z. Chen, C. Kourtidou-Papadeli, P. D. Bamidis, I. Chouvarda, E. Bekiaris, and N. Maglaveras, “Monitoring sleepiness with on-board electrophysiological recordings for preventing sleep-deprived traffic accidents,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Clinical Neurophysiology</span>, vol. 118, no. 9, pp. 1906–1922, 2007.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. N. Boyle, J. Tippin, A. Paul, and M. Rizzo, “Driver performance in the moments surrounding a microsleep,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Transportation research part F: traffic psychology and behaviour</span>, vol. 11, no. 2, pp. 126–136, 2008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Sharma and K. K. Sharma, “An algorithm for sleep apnea detection from single-lead ECG using hermite basis functions,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Comput. Biol. Medicine</span>, vol. 77, pp. 116–124, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Wang, B. Guragain, A. K. Verma, L. Archer, S. Majumder, A. Mohamud, E. Flaherty-Woods, G. Shapiro, M. Almashor, M. Lenné, R. Myers, J. Kuo, S. Yang, N. Wilson, and K. Tavakolian, “Spectral analysis of EEG during microsleep events annotated via driver monitoring system to characterize drowsiness,” <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Aerosp. Electron. Syst.</span>, vol. 56, no. 2, pp. 1346–1356, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.-H. Jeong, B.-W. Yu, D.-H. Lee, and S.-W. Lee, “Classification of drowsiness levels based on a deep spatio-temporal convolutional bidirectional lstm network using electroencephalography signals,” <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Brain sciences</span>, vol. 9, no. 12, p. 348, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Zhang, J. Li, Y. Liu, Z. Zhang, Z. Wang, D. Luo, X. Zhou, M. Zhu, W. Salman, G. Hu, and C. Wang, “Design of a fatigue detection system for high-speed trains based on driver vigilance using a wireless wearable EEG,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 17, no. 3, p. 486, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Chen, H. Li, L. Han, J. Wu, A. Azam, and Z. Zhang, “Driver vigilance detection for high-speed rail using fusion of multiple physiological signals and deep learning,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Appl. Soft Comput.</span>, vol. 123, p. 108982, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Zhou, Z. Fang, J. Wang, J. Chen, H. Li, L. Han, and Z. Zhang, “Driver vigilance detection based on deep learning with fused thermal image information for public transportation,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Eng. Appl. Artif. Intell.</span>, vol. 124, p. 106604, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Fan, Z. Li, J. Pei, H. Li, and J. Sun, “Applying systems thinking approach to accident analysis in china: Case study of “7.23” yong-tai-wen high-speed train accident,” <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Safety science</span>, vol. 76, pp. 190–201, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Ramzan, H. U. Khan, S. M. Awan, A. Ismail, M. Ilyas, and A. Mahmood, “A survey on state-of-the-art drowsiness detection techniques,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 7, pp. 61904–61919, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. S. D. Pandilwar and M. More, “Survey paper for real time car driver drowsiness detection using machine learning approach,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International Journal for Research in Applied Science and Engineering Technology</span>, p. 4318–4323, Jul 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Reddy, Y. Kim, S. Yun, C. Seo, and J. Jang, “Real-time driver drowsiness detection for embedded system using model compression of deep neural networks,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017</span>, pp. 438–445, IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Othmani, A. Q. M. Sabri, S. Aslan, F. Chaieb, H. Rameh, R. Alfred, and D. Cohen, “EEG-based neural networks approaches for fatigue and drowsiness detection: A survey,” <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, p. 126709, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Sahayadhas, K. Sundaraj, and M. Murugappan, “Detecting driver drowsiness based on sensors: A review,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 12, no. 12, pp. 16937–16953, 2012.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. A. El-Nabi, W. El-Shafai, E.-S. M. El-Rabaie, K. F. Ramadan, F. E. Abd El-Samie, and S. Mohsen, “Machine learning and deep learning techniques for driver fatigue and drowsiness detection: a review,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, vol. 83, no. 3, pp. 9441–9477, 2024.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. A. Caldwell, J. L. Caldwell, L. A. Thompson, and H. R. Lieberman, “Fatigue and its management in the workplace,” <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Neuroscience &amp; Biobehavioral Reviews</span>, vol. 96, pp. 272–289, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. E. Lerman, E. Eskin, D. J. Flower, E. C. George, B. Gerson, N. Hartenbaum, S. R. Hursh, and M. Moore-Ede, “Fatigue risk management in the workplace,” vol. 54, no. 2, pp. 231–258, 2012.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K. Sadeghniiat-Haghighi and Z. Yazdi, “Fatigue management in the workplace,” <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Industrial psychiatry journal</span>, vol. 24, no. 1, p. 12, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Astaras, P. D. Bamidis, C. Kourtidou-Papadeli, and N. Maglaveras, “Biomedical real-time monitoring in restricted and safety-critical environments,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Hippokratia</span>, vol. 12, no. Suppl 1, p. 10, 2008.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
P. M. Ramos, C. B. Maior, M. C. Moura, and I. D. Lins, “Automatic drowsiness detection for safety-critical operations using ensemble models and EEG signals,” <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Process Safety and Environmental Protection</span>, vol. 164, pp. 566–581, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Natnithikarat, S. Lamyai, P. Leelaarporn, N. Kunaseth, P. Autthasan, T. Wisutthisen, and T. Wilaiprasitporn, “Drowsiness detection for office-based workload with mouse and keyboard data,” in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2019 12th Biomedical Engineering International Conference (BMEiCON)</span>, pp. 1–4, IEEE, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Kaida, M. Takahashi, T. Åkerstedt, A. Nakata, Y. Otsuka, T. Haratani, and K. Fukasawa, “Validation of the karolinska sleepiness scale against performance and EEG variables,” <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Clinical neurophysiology</span>, vol. 117, no. 7, pp. 1574–1581, 2006.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Ummul and K. Rao, “Shift work and fatigue,” <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Journal of Environ Sci Toxicol Food Technol</span>, vol. 1, no. 3, pp. 17–21, 2012.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. J. Thompson, M. S. Stock, and V. K. Banuelas, “Effects of accumulating work shifts on performance-based fatigue using multiple strength measurements in day and night shift nurses and aides,” <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Hum. Factors</span>, vol. 59, no. 3, pp. 346–356, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T. Åkerstedt, “Sleepiness as a consequence of shift work,” <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Sleep</span>, vol. 11, no. 1, pp. 17–34, 1988.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. D. Chervin, “Sleepiness, fatigue, tiredness, and lack of energy in obstructive sleep apnea,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Chest</span>, vol. 118, no. 2, pp. 372–379, 2000.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. Q. Wu, C. Lin, L. Zhu, Z. Tang, Y. Jie, and G. Zhou, “Fatigue detection of pilots’ brain through brains cognitive map and multilayer latent incremental learning model,” <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Cybern.</span>, vol. 52, no. 11, pp. 12302–12314, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. Henríquez, B. J. Matuszewski, Y. Andreu, L. Bastiani, S. Colantonio, G. Coppini, M. D’Acunto, R. Favilla, D. Germanese, D. Giorgi, P. Marraccini, M. Martinelli, M. Morales, M. A. Pascali, M. Righi, O. Salvetti, M. Larsson, T. Strömberg, L. Randeberg, A. Bjorgan, G. A. Giannakakis, M. Pediaditis, F. Chiarugi, E. Christinaki, K. Marias, and M. Tsiknakis, “Mirror mirror on the wall… an unobtrusive intelligent multisensory mirror for well-being status self-assessment and visualization,” <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Multim.</span>, vol. 19, no. 7, pp. 1467–1481, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
V. Kumar, N. Badal, and R. Mishra, “Elderly fall due to drowsiness: Detection and prevention using machine learning and iot,” <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Modern Physics Letters B</span>, vol. 35, no. 07, p. 2150120, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
B. Bačić and J. Zhang, “Towards real-time drowsiness detection for elderly care,” in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)</span>, pp. 1–6, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Teplan <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Fundamentals of EEG measurement,” <span id="bib.bib43.2.2" class="ltx_text ltx_font_italic">Measurement science review</span>, vol. 2, no. 2, pp. 1–11, 2002.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Jiang, Y. Lu, Y. Ma, and Y. Wang, “Robust sleep stage classification with single-channel EEG signals using multimodal decomposition and hmm-based refinement,” <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Expert Syst. Appl.</span>, vol. 121, pp. 188–203, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. L. T. da Silveira, A. J. Kozakevicius, and C. R. Rodrigues, “Automated drowsiness detection through wavelet packet analysis of a single EEG channel,” <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Expert Syst. Appl.</span>, vol. 55, pp. 559–565, 2016.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B. A. GEERING, P. ACHERMANN, F. EGGIMANN, and A. A. BORBÉLY, “Period-amplitude analysis and power spectral analysis: a comparison based on all-night sleep eeg recordings,” <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Journal of sleep research</span>, vol. 2, no. 3, pp. 121–129, 1993.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
F. K. Onay and C. Köse, “Power spectral density analysis in alfa, beta and gamma frequency bands for classification of motor EEG signals,” in <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">27th Signal Processing and Communications Applications Conference, SIU 2019, Sivas, Turkey, April 24-26, 2019</span>, pp. 1–4, IEEE, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Z. Mardi, S. N. M. Ashtiani, and M. Mikaili, “Eeg-based drowsiness detection for safe driving using chaotic features and statistical tests,” <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Journal of medical signals and sensors</span>, vol. 1, no. 2, p. 130, 2011.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Jiao, Y. Deng, Y. Luo, and B.-L. Lu, “Driver sleepiness detection from EEG and EOG signals using gan and lstm networks,” <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, vol. 408, pp. 100–111, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. L. Cantero, M. Atienza, and R. M. Salas, “Human alpha oscillations in wakefulness, drowsiness period, and rem sleep: different electroencephalographic phenomena within the alpha band,” <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Neurophysiologie Clinique/Clinical Neurophysiology</span>, vol. 32, no. 1, pp. 54–71, 2002.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
R. D. Ogilvie, “The process of falling asleep,” <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Sleep medicine reviews</span>, vol. 5, no. 3, pp. 247–270, 2001.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
H. Blake, R. W. Gerard, and N. Kleitman, “Factors influencing brain potentials during sleep,” <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Journal of Neurophysiology</span>, vol. 2, no. 1, pp. 48–60, 1939.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C. Heinze, C. Hütterer, T. Schnupp, G. Lenis, and M. Golz, “Drowsiness discrimination in an overnight driving simulation on the basis of rr and qt intervals,” <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Current Directions in Biomedical Engineering</span>, vol. 3, no. 2, pp. 563–567, 2017.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. U. Viola, E. Tobaldini, S. L. Chellappa, K. R. Casali, A. Porta, and N. Montano, “Short-term complexity of cardiac autonomic control during sleep: Rem as a potential risk factor for cardiovascular system in aging,” <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">PloS one</span>, vol. 6, no. 4, p. e19002, 2011.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
R. Cabiddu, S. Cerutti, G. Viardot, S. Werner, and A. M. Bianchi, “Modulation of the sympatho-vagal balance during sleep: frequency domain study of heart rate variability and respiration,” <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Frontiers in physiology</span>, vol. 3, p. 45, 2012.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M. AlGhatrif and J. Lindsay, “A brief review: history to understand fundamentals of electrocardiography,” <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Journal of community hospital internal medicine perspectives</span>, vol. 2, no. 1, p. 14383, 2012.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Hammad, A. Maher, K. Wang, F. Jiang, and M. Amrani, “Detection of abnormal heart conditions based on characteristics of ECG signals,” <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Measurement</span>, vol. 125, pp. 634–644, 2018.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
W. J. Brady, A. D. Perron, M. L. Martin, C. Beagle, and T. P. Aufderheide, “Cause of ST segment abnormality in ED chest pain patients,” <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">The American journal of emergency medicine</span>, vol. 19, no. 1, pp. 25–28, 2001.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
A. Kumar and D. M. Lloyd-Jones, “Clinical significance of minor nonspecific ST-segment and T-wave abnormalities in asymptomatic subjects: a systematic review,” <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Cardiology in review</span>, vol. 15, no. 3, pp. 133–142, 2007.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. E. Trinkley, R. Lee Page, H. Lien, K. Yamanouye, and J. E. Tisdale, “QT interval prolongation and the risk of torsades de pointes: essentials for clinicians,” <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Current medical research and opinion</span>, vol. 29, no. 12, pp. 1719–1726, 2013.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
D. G. Strauss and R. H. Selvester, “The QRS complex—a biomarker that “images” the heart: Qrs scores to quantify myocardial scar in the presence of normal and abnormal ventricular conduction,” <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Journal of electrocardiology</span>, vol. 42, no. 1, pp. 85–96, 2009.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
R. Kher <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Signal processing techniques for removing noise from ECG signals,” <span id="bib.bib62.2.2" class="ltx_text ltx_font_italic">J. Biomed. Eng. Res</span>, vol. 3, no. 101, pp. 1–9, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
P. K. Stein and Y. Pu, “Heart rate variability, sleep and sleep disorders,” <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Sleep medicine reviews</span>, vol. 16, no. 1, pp. 47–66, 2012.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
T. Penzel, J. W. Kantelhardt, R. P. Bartsch, M. Riedl, J. F. Kraemer, N. Wessel, C. Garcia, M. Glos, I. Fietze, and C. Schöbel, “Modulations of heart rate, ECG, and cardio-respiratory coupling observed in polysomnography,” <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Frontiers in physiology</span>, vol. 7, p. 460, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. H. Khandoker, M. Palaniswami, and C. K. Karmakar, “Support vector machines for automated recognition of obstructive sleep apnea syndrome from ECG recordings,” <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Inf. Technol. Biomed.</span>, vol. 13, no. 1, pp. 37–48, 2009.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
G. B. Papini, P. Fonseca, J. Margarito, M. M. van Gilst, S. Overeem, J. W. M. Bergmans, and R. Vullings, “On the generalizability of ECG-based obstructive sleep apnea monitoring: merits and limitations of the apnea-ecg database,” in <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2018, Honolulu, HI, USA, July 18-21, 2018</span>, pp. 6022–6025, IEEE, 2018.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
A. Dharma, P. Sihombing, H. Mawengkang, S. Efendi, and A. R. Crispin, “Development and evaluation of a portable ECG monitoring system for automated classification of normal and abnormal ECG signal using random forest with xgboost,” in <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">29th International Conference on Telecommunications, ICT 2023, Toba, Indonesia, November 8-9, 2023</span>, pp. 1–5, IEEE, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
S. Ramasamy and A. Balan, “Wearable sensors for ECG measurement: a review,” <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Sensor Review</span>, vol. 38, no. 4, pp. 412–419, 2018.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
H. Li, Z. An, S. Zuo, W. Zhu, Z. Zhang, S. Zhang, C. Zhang, W. Song, Q. Mao, Y. Mu, E. Li, and J. D. P. García, “Artificial intelligence-enabled ECG algorithm based on improved residual network for wearable ECG,” <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 18, p. 6043, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
M. Ahmed, S. Masood, M. Ahmad, and A. A. A. El-Latif, “Intelligent driver drowsiness detection for traffic safety based on multi CNN deep model and facial subsampling,” <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 10, pp. 19743–19752, 2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
M. C. Uchida, R. Carvalho, V. D. Tessutti, R. F. P. Bacurau, H. J. Coelho-Júnior, L. P. Capelo, H. P. Ramos, M. C. d. Santos, L. F. M. Teixeira, and P. H. Marchetti, “Identification of muscle fatigue by tracking facial expressions,” <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">PLoS One</span>, vol. 13, no. 12, p. e0208834, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. A. Khan, T. Nawaz, U. S. Khan, A. Hamza, and N. Rashid, “IoT-Based Non-Intrusive automated driver drowsiness monitoring framework for logistics and public transport applications to enhance road safety,” <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 11, pp. 14385–14397, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
R. Katmah, F. Al-Shargie, U. Tariq, F. Babiloni, F. Al-Mughairbi, and H. Al-Nashash, “A review on mental stress assessment methods using EEG signals,” <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 15, p. 5043, 2021.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
I. Stancin, M. Cifrek, and A. Jovic, “A review of EEG signal features and their application in driver drowsiness detection systems,” <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 11, p. 3786, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. Ming, D. Wu, Y. Wang, Y. Shi, and C. Lin, “EEG-based drowsiness estimation for driving safety using Deep Q-Learning,” <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Emerg. Top. Comput. Intell.</span>, vol. 5, no. 4, pp. 583–594, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
S. Chaabene, B. Bouaziz, A. Boudaya, A. Hökelmann, A. Ammar, and L. Chaâri, “Convolutional neural network for drowsiness detection using EEG signals,” <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 5, p. 1734, 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Q. Massoz, T. Langohr, C. François, and J. G. Verly, “The ULg multimodality drowsiness database (called DROZY) and examples of use,” in <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016</span>, pp. 1–7, IEEE Computer Society, 2016.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
J. Cui, Z. Lan, T. Zheng, Y. Liu, O. Sourina, L. Wang, and W. Müller-Wittig, “Subject-independent drowsiness recognition from single-channel EEG with an interpretable CNN-LSTM model,” in <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">International Conference on Cyberworlds, CW 2021, Caen, France, September 28-30, 2021</span> (A. Sourin, C. Rosenberger, and O. Sourina, eds.), pp. 201–208, IEEE, 2021.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
C. Lee and J. An, “LSTM-CNN model of drowsiness detection from multiple consciousness states acquired by eeg,” <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, vol. 213, p. 119032, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
J. R. Paulo, G. Pires, and U. J. Nunes, “Cross-subject zero calibration driver’s drowsiness detection: Exploring spatiotemporal image encoding of EEG signals for convolutional neural network classification,” <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Systems and Rehabilitation Engineering</span>, vol. 29, pp. 905–915, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Y. Jiang, Y. Zhang, C. Lin, D. Wu, and C. Lin, “EEG-based driver drowsiness estimation using an online multi-view and transfer TSK fuzzy system,” <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 22, no. 3, pp. 1752–1764, 2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
J. Cui, Z. Lan, O. Sourina, and W. Müller-Wittig, “EEG-based cross-subject driver drowsiness recognition with an interpretable convolutional neural network,” <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>, 2022.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Z. Zhuang, Y.-K. Wang, Y.-C. Chang, J. Liu, and C.-T. Lin, “A connectivity-aware graph neural network for real-time drowsiness classification,” <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Systems and Rehabilitation Engineering</span>, vol. PP, pp. 1–1, 11 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
M. Duvinage, T. Castermans, M. Petieau, T. Hoellinger, G. Cheron, and T. Dutoit, “Performance of the emotiv epoc headset for p300-based applications,” <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">Biomedical engineering online</span>, vol. 12, pp. 1–15, 2013.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, M. Chen, U. Trutschel, and D. Mandic, “Feature fusion for the detection of microsleep events,” <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology</span>, vol. 49, pp. 329–342, 2007.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
M. A. Carskadon, <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Encyclopedia of sleep and dreaming.</span>

</span>
<span class="ltx_bibblock">Macmillan Publishing Co, Inc, 1993.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
A. A. of Sleep Medicine <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Economic burden of undiagnosed sleep apnea in us is nearly $150 b per year,” 2016.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
R. L. DeHart, “The twenty-four hour society: Understanding human limits in a world that never sleeps,” <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">JAMA</span>, vol. 270, no. 18, pp. 2230–2230, 1993.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, A. Seyfarth, U. Trutschel, and M. Moore-Ede, “Application of vector-based neural networks for the recognition of beginning microsleep episodes with an eyetracking system,” <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Computational Intelligence: Methods &amp; Applications</span>, pp. 130–134, 2001.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
N. Pham, T. Dinh, Z. Raghebi, T. Kim, N. Bui, P. Nguyen, H. Truong, F. Banaei-Kashani, A. Halbower, T. Dinh, <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Wake: a behind-the-ear wearable system for microsleep detection,” in <span id="bib.bib90.2.2" class="ltx_text ltx_font_italic">Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services</span>, pp. 404–418, 2020.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
A. Chougule, J. Shah, V. Chamola, and S. Kanhere, “Enabling safe ITS: EEG-Based microsleep detection in vanets,” <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 12, pp. 15773–15783, 2023.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Z. Cao, C.-H. Chuang, J.-K. King, and C.-T. Lin, “Multi-channel EEG recordings during a sustained-attention driving task,” <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Scientific data</span>, vol. 6, no. 1, p. 19, 2019.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
K. T. Chui, K. F. Tsang, H. R. Chi, B. W. Ling, and C. K. Wu, “An accurate ECG-based transportation safety drowsiness detection scheme,” <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Ind. Informatics</span>, vol. 12, no. 4, pp. 1438–1452, 2016.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
T. Takalokastari, S.-J. Jung, D.-D. Lee, and W.-Y. Chung, “Real time drowsiness detection by a WSN based wearable ecg measurement system,” <span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">Journal of Sensor Science and Technology</span>, vol. 20, 11 2011.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
C. Meyer, J. F. Gavela, and M. Harris, “Combining algorithms in automatic detection of QRS complexes in ECG signals,” <span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Inf. Technol. Biomed.</span>, vol. 10, no. 3, pp. 468–475, 2006.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
N. R. Adão Martins, S. Annaheim, C. M. Spengler, and R. M. Rossi, “Fatigue monitoring through wearables: A state-of-the-art review,” <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Frontiers in Physiology</span>, vol. 12, 2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
T. Sherbakova and O. Osipova, “The analysis of an electrocardiosignal in a system of data transmission in control office,” in <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">2015 International Conference on Antenna Theory and Techniques (ICATT)</span>, pp. 1–2, 2015.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
J. Achten and A. E. Jeukendrup, “Heart rate monitoring: applications and limitations,” <span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">Sports medicine</span>, vol. 33, pp. 517–538, 2003.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
S. Weise, J. Ong, N. A. Tesler, S. Kim, and W. T. Roth, “Worried sleep: 24-h monitoring in high and low worriers,” <span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">Biological psychology</span>, vol. 94, no. 1, pp. 61–70, 2013.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
V. A. Zernov, E. V. Lobanova, E. V. Likhacheva, L. R. Nikolaeva, D. D. Dymarchuk, D. S. Yesenin, N. V. Mizin, A. S. Ognev, and M. Y. Rudenko, “Cardiometric fingerprints of various human ego states,” <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Cardiometry</span>, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
K. Ke, M. R. Zulman, H. Wu, Y. Huang, and J. Thiagarajan, “Drowsiness detection system using heartbeat rate in android-based handheld devices,” in <span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">First International Conference on Multimedia and Image Processing, ICMIP 2016, Bandar Seri Begawan, Brunei Darussalam, June 1-3, 2016</span>, pp. 99–103, IEEE, 2016.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
K. Fujiwara, E. Abe, K. Kamata, C. Nakayama, Y. Suzuki, T. Yamakawa, T. Hiraoka, M. Kano, Y. Sumi, F. Masuda, M. Matsuo, and H. Kadotani, “Heart rate variability-based driver drowsiness detection and its validation with EEG,” <span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Biomed. Eng.</span>, vol. 66, no. 6, pp. 1769–1778, 2019.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
F. L. Mannering and L. L. Grodsky, “Statistical analysis of motorcyclists’ perceived accident risk,” <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Accident Analysis &amp; Prevention</span>, vol. 27, no. 1, pp. 21–31, 1995.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
H. Ospina-Mateus, L. A. Q. Jiménez, F. J. López-Valdés, S. B. Garcia, L. H. Barrero, and S. S. Sana, “Extraction of decision rules using genetic algorithms and simulated annealing for prediction of severity of traffic accidents by motorcyclists,” <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">J. Ambient Intell. Humaniz. Comput.</span>, vol. 12, no. 11, pp. 10051–10072, 2021.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
F. Fahrurrasyid, G. I. Hapsari, L. Meisaroh, and G. A. Mutiara, “Smart helmet GPS-based for heartbeat drowsiness detection and location tracking,” <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">Journal of Biomimetics, Biomaterials and Biomedical Engineering</span>, vol. 55, pp. 226–235, 4 2022.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
S. Heydari, A. Ayatollahi, A. Najafi, and M. Poorjafari, “Detection of drowsiness using the pulse rate variability of finger,” <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">SN Comput. Sci.</span>, vol. 3, no. 5, p. 359, 2022.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
G. Lenis, P. Reichensperger, D. Sommer, C. Heinze, M. Golz, and O. Dössel, “Detection of microsleep events in a car driving simulation study using electrocardiographic features,” <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Current Directions in Biomedical Engineering</span>, vol. 2, no. 1, pp. 283–287, 2016.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
M. M. Hasan, C. N. Watling, and G. S. Larue, “Validation and interpretation of a multimodal drowsiness detection system using explainable machine learning,” <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">Computer Methods and Programs in Biomedicine</span>, vol. 243, p. 107925, 2024.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
S. Bakheet and A. Al-Hamadi, “A framework for instantaneous driver drowsiness detection based on improved HOG features and naïve bayesian classification,” <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">Brain Sciences</span>, vol. 11, no. 2, 2021.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
M. Vijay, N. N. Vinayak, M. Nunna, and N. Subramanyam, “Real-time driver drowsiness detection using facial action units,” in <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">25th International Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15, 2021</span>, pp. 10113–10119, IEEE, 2020.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
J. Yu, S. Park, S. Lee, and M. Jeon, “Representation learning, scene understanding, and feature fusion for drowsiness detection,” in <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Computer Vision - ACCV 2016 Workshops - ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part III</span> (C. Chen, J. Lu, and K. Ma, eds.), vol. 10118 of <span id="bib.bib111.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 165–177, Springer, 2016.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
C. Weng, Y. Lai, and S. Lai, “Driver drowsiness detection via a hierarchical temporal deep belief network,” in <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">Computer Vision - ACCV 2016 Workshops - ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part III</span> (C. Chen, J. Lu, and K. Ma, eds.), vol. 10118 of <span id="bib.bib112.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 117–133, Springer, 2016.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
N. N. Pandey and N. B. Muppalaneni, “Dumodds: Dual modeling approach for drowsiness detection based on spatial and spatio-temporal features,” <span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">Engineering Applications of Artificial Intelligence</span>, vol. 119, p. 105759, 2023.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
G. S. Krishna, K. Supriya, J. Vardhan, and M. R. K, “Vision transformers and YoloV5 based driver drowsiness detection framework,” <span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2209.01401, 2022.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
R. Tamanani, R. Muresan, and A. Al-Dweik, “Estimation of driver vigilance status using real-time facial expression and deep learning,” <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">IEEE Sensors Letters</span>, vol. 5, no. 5, pp. 1–4, 2021.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J. Chen, Z. Fang, J. Wang, J. Chen, and G. Yin, “A multi-view driver drowsiness detection method using transfer learning and population-based sampling strategy,” in <span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</span>, pp. 3386–3391, 2022.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y. Lu, C. Liu, F. Chang, H. Liu, and H. Huan, “JHPFA-Net: Joint head pose and facial action network for driver yawning detection across arbitrary poses in videos,” <span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 11, pp. 11850–11863, 2023.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
L. Mou, C. Zhou, P. Xie, P. Zhao, R. Jain, W. Gao, and B. Yin, “Isotropic self-supervised learning for driver drowsiness detection with attention-based multimodal fusion,” <span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia</span>, vol. 25, pp. 529–542, 2021.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
R. Ghoddoosian, M. Galib, and V. Athitsos, “A realistic dataset and baseline temporal model for early drowsiness detection,” in <span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019</span>, pp. 178–187, Computer Vision Foundation / IEEE, 2019.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
S. Abtahi, M. Omidyeganeh, S. Shirmohammadi, and B. Hariri, “YawDD: a yawning detection dataset,” in <span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Multimedia Systems Conference 2014, MMSys ’14, Singapore, March 19-21, 2014</span> (R. Zimmermann, ed.), pp. 24–28, ACM, 2014.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
R. Fusek, “Pupil localization using geodesic distance,” in <span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">Advances in Visual Computing - 13th International Symposium, ISVC 2018, Las Vegas, NV, USA, November 19-21, 2018, Proceedings</span> (G. Bebis, R. Boyle, B. Parvin, D. Koracin, M. Turek, S. Ramalingam, K. Xu, S. Lin, B. Alsallakh, J. Yang, E. Cuervo, and J. Ventura, eds.), vol. 11241 of <span id="bib.bib121.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 433–444, Springer, 2018.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
C. Yang, Z. Yang, W. Li, and J. See, “Fatigueview: A multi-camera video dataset for vision-based drowsiness detection,” <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 24, no. 1, pp. 233–246, 2023.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
C. Chiou, W. Wang, S. Lu, C. Huang, P. Chung, and Y. Lai, “Driver monitoring using sparse representation with part-based temporal face descriptors,” <span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 21, no. 1, pp. 346–361, 2020.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
M. Martin, A. Roitberg, M. Haurilet, M. Horne, S. Reiß, M. Voit, and R. Stiefelhagen, “Drive&amp;act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles,” in <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</span>, pp. 2801–2810, IEEE, 2019.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
J. D. Ortega, N. Kose, P. Cañas, M. Chao, A. Unnervik, M. Nieto, O. Otaegui, and L. Salgado, “DMD: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis,” in <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2020 Workshops - Glasgow, UK, August 23-28, 2020, Proceedings, Part IV</span> (A. Bartoli and A. Fusiello, eds.), vol. 12538 of <span id="bib.bib125.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 387–405, Springer, 2020.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
E. K. Yılmaz and M. A. Akcayol, “Sust-ddd: A real-drive dataset for driver drowsiness detection,” in <span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">Conference of Open Innovations Association (FRUCT)</span>, vol. 6, 2022.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
S. A. El-Nabi, W. El-Shafai, E. M. El-Rabaie, K. F. Ramadan, F. E. A. El-Samie, and S. Mohsen, “Machine learning and deep learning techniques for driver fatigue and drowsiness detection: a review,” <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Multim. Tools Appl.</span>, vol. 83, no. 3, pp. 9441–9477, 2024.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
S. Saurav, P. S. Gidde, R. Saini, and S. Singh, “Real-time eye state recognition using dual convolutional neural network ensemble,” <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Journal of Real Time Image Process.</span>, vol. 19, no. 3, pp. 607–622, 2022.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Z. Hu, C. Lv, P. Hang, C. Huang, and Y. Xing, “Data-driven estimation of driver attention using calibration-free eye gaze and scene features,” <span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Ind. Electron.</span>, vol. 69, no. 2, pp. 1800–1808, 2022.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Y. Ed-Doughmi, N. Idrissi, and Y. Hbali, “Real-time system for driver fatigue detection based on a recurrent neuronal network,” <span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">J. Imaging</span>, vol. 6, no. 3, p. 8, 2020.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
F. Song, X. Tan, X. Liu, and S. Chen, “Eyes closeness detection from still images with multi-scale histograms of principal oriented gradients,” <span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">Pattern Recognit.</span>, vol. 47, no. 9, pp. 2825–2838, 2014.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
T. Drutarovsky and A. Fogelton, “Eye blink detection using variance of motion vectors,” in <span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2014 Workshops - Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part III</span> (L. Agapito, M. M. Bronstein, and C. Rother, eds.), vol. 8927 of <span id="bib.bib132.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 436–448, Springer, 2014.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Y. Xie, K. Chen, and Y. L. Murphey, “Real-time and robust driver yawning detection with deep neural networks,” in <span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">IEEE Symposium Series on Computational Intelligence, SSCI 2018, Bangalore, India, November 18-21, 2018</span>, pp. 532–538, IEEE, 2018.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
S. Kawato and J. Ohya, “Real-time detection of nodding and head-shaking by directly detecting and tracking the ”between-eyes”,” in <span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">4th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2000), 26-30 March 2000, Grenoble, France</span>, pp. 40–45, IEEE Computer Society, 2000.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
M. Tan, G. Ni, X. Liu, S. Zhang, X. Wu, Y. Wang, and R. Zeng, “Bidirectional posture-appearance interaction network for driver behavior recognition,” <span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 8, pp. 13242–13254, 2022.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
T. Abbas, S. F. Ali, M. A. Mohammed, A. Z. Khan, M. J. Awan, A. Majumdar, and O. Thinnukool, “Deep learning approach based on residual neural network and svm classifier for driver’s distraction detection,” <span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol. 12, no. 13, p. 6626, 2022.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
I. Kotseruba and J. K. Tsotsos, “Attention for vision-based assistive and automated driving: A review of algorithms and datasets,” <span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 11, pp. 19907–19928, 2022.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
C. B. S. Maior, M. J. das Chagas Moura, J. M. M. Santana, and I. D. Lins, “Real-time classification for autonomous drowsiness detection using eye aspect ratio,” <span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Expert Syst. Appl.</span>, vol. 158, p. 113505, 2020.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
M. Papakostas, K. Das, M. Abouelenien, R. Mihalcea, and M. Burzo, “Distracted and drowsy driving modeling using deep physiological representations and multitask learning,” <span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol. 11, no. 1, 2021.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Y. Albadawi, A. A. Redhaei, and M. Takruri, “Real-time machine learning-based driver drowsiness detection using visual features,” <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">J. Imaging</span>, vol. 9, no. 5, p. 91, 2023.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
C. J. de Naurois, C. Bourdin, A. Stratulat, E. Diaz, and J.-L. Vercher, “Detection and prediction of driver drowsiness using artificial neural network models,” <span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">Accident Analysis &amp; Prevention</span>, vol. 126, pp. 95–104, 2019.

</span>
<span class="ltx_bibblock">10th International Conference on Managing Fatigue: Managing Fatigue to Improve Safety, Wellness, and Effectiveness”.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
G. Li and W. Chung, “Estimation of eye closure degree using EEG sensors and its application in driver drowsiness detection,” <span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 14, no. 9, pp. 17491–17515, 2014.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y. Cui and D. Wu, “EEG-based driver drowsiness estimation using convolutional neural networks,” in <span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">Neural Information Processing: 24th International Conference, ICONIP 2017, Guangzhou, China, November 14-18, 2017, Proceedings, Part II 24</span>, pp. 822–832, Springer, 2017.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
S. Hachisuka, “Human and vehicle-driver drowsiness detection by facial expression,” in <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">2013 International Conference on Biometrics and Kansei Engineering</span>, pp. 320–326, 2013.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
M. Dua, Shakshi, R. Singla, S. Raj, and A. Jangra, “Deep CNN models-based ensemble approach to driver drowsiness detection,” <span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">Neural Comput. Appl.</span>, vol. 33, no. 8, pp. 3155–3168, 2021.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
C. Wei, Y. Lin, Y. Wang, C. Lin, and T. Jung, “A subject-transfer framework for obviating inter- and intra-subject variability in EEG-based drowsiness detection,” <span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">NeuroImage</span>, vol. 174, pp. 407–419, 2018.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
E. Perkins, C. Sitaula, M. Burke, and F. Marzbanrad, “Challenges of driver drowsiness prediction: The remaining steps to implementation,” <span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2022.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
S. Abtahi, B. Hariri, and S. Shirmohammadi, “Driver drowsiness monitoring based on yawning detection,” in <span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">2011 IEEE International Instrumentation and Measurement Technology Conference</span>, pp. 1–4, IEEE, 2011.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
A. Ghourabi, H. Ghazouani, and W. Barhoumi, “Driver drowsiness detection based on joint monitoring of yawning, blinking and nodding,” in <span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">16th IEEE International Conference on Intelligent Computer Communication and Processing, ICCP 2020, Cluj-Napoca, Romania, September 3-5, 2020</span> (S. Nedevschi, R. Potolea, and R. R. Slavescu, eds.), pp. 407–414, IEEE, 2020.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
E. Vural, M. Çetin, A. Erçil, G. Littlewort, M. S. Bartlett, and J. R. Movellan, “Drowsy driver detection through facial movement analysis,” in <span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">Human-Computer Interaction, IEEE International Workshop, HCI 2007, Rio de Janeiro, Brazil, October 20, 2007, Proceedings</span> (M. S. Lew, N. Sebe, T. S. Huang, and E. M. Bakker, eds.), vol. 4796 of <span id="bib.bib150.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 6–18, Springer, 2007.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
H. Yang, L. Liu, W. Min, X. Yang, and X. Xiong, “Driver yawning detection based on subtle facial action recognition,” <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Multim.</span>, vol. 23, pp. 572–583, 2021.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
B. Yin, X. Fan, and Y. Sun, “Multiscale dynamic features based driver fatigue detection,” <span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">Int. J. Pattern Recognit. Artif. Intell.</span>, vol. 23, no. 3, pp. 575–589, 2009.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
X. Fan, B. Yin, and Y. Sun, “Yawning detection based on gabor wavelets and LDA,” <span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">Journal of Beijing Univ Technol</span>, vol. 35, no. 3, pp. 409–413, 2009.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
T. Brandt, R. Stemmer, and A. Rakotonirainy, “Affordable visual driver monitoring system for fatigue and monotony,” in <span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Systems, Man &amp; Cybernetics: The Hague, Netherlands, 10-13 October 2004</span>, pp. 6451–6456, IEEE, 2004.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
R. C. Chang, C. Wang, W. Chen, and C. Chiu, “Drowsiness detection system based on PERCLOS and facial physiological signal,” <span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 22, no. 14, p. 5380, 2022.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
L. Wang, X. Wu, B. Ba, and W. Dong, “A vision-based method to detect perclos features,” <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">Computer Engineering &amp; Science</span>, vol. 6, p. 017, 2006.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Z. Zhao, N. Zhou, L. Zhang, H. Yan, Y. Xu, and Z. Zhang, “Driver fatigue detection based on convolutional neural networks using EM-CNN,” <span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">Comput. Intell. Neurosci.</span>, vol. 2020, pp. 7251280:1–7251280:11, 2020.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
L. Chen, G. Xin, Y. Liu, and J. Huang, “Driver fatigue detection based on facial key points and LSTM,” <span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">Secur. Commun. Networks</span>, vol. 2021, pp. 5383573:1–5383573:9, 2021.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment using multitask cascaded convolutional networks,” <span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">IEEE Signal Process. Lett.</span>, vol. 23, no. 10, pp. 1499–1503, 2016.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, “Retinaface: Single-shot multi-level face localisation in the wild,” in <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</span>, pp. 5202–5211, Computer Vision Foundation / IEEE, 2020.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in <span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI</span> (D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, eds.), vol. 8694 of <span id="bib.bib161.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp. 94–108, Springer, 2014.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, and A. Kuijper, “Quantface: Towards lightweight face recognition by synthetic data low-bit quantization,” in <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">26th International Conference on Pattern Recognition, ICPR 2022, Montreal, QC, Canada, August 21-25, 2022</span>, pp. 855–862, IEEE, 2022.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
F. Boutros, P. Siebke, M. Klemt, N. Damer, F. Kirchbuchner, and A. Kuijper, “Pocketnet: Extreme lightweight face recognition network using neural architecture search and multistep knowledge distillation,” <span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 10, pp. 46823–46833, 2022.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
N. Damer, F. Boutros, M. Süßmilch, M. Fang, F. Kirchbuchner, and A. Kuijper, “Masked face recognition: Human versus machine,” <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">IET Biom.</span>, vol. 11, no. 5, pp. 512–528, 2022.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper, “Self-restrained triplet loss for accurate masked face recognition,” <span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">Pattern Recognit.</span>, vol. 124, p. 108473, 2022.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
M. Ngxande, J. Tapamo, and M. Burke, “Bias remediation in driver drowsiness detection systems using generative adversarial networks,” <span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8, pp. 55592–55601, 2020.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
J. Wörle, B. Metz, and A. Prill, “How to induce drowsiness when testing driver drowsiness and attention warning (DDAW) systems,” <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 5, pp. 4758–4764, 2023.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, M. Holzbrecher, and T. Schnupp, “Detection and prediction of driver’s microsleep events,” in <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">Proc 14th Int Conf Road Safety on Four Continents</span>, vol. 11, Citeseer, 2007.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
R. Jabbar, K. Al-Khalifa, M. Kharbeche, W. K. M. Alhajyaseen, M. A. Jafari, and S. Jiang, “Real-time driver drowsiness detection for android application using deep neural networks techniques,” in <span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT 2018) / Affiliated Workshops, May 8-11, 2018, Porto, Portugal</span> (E. M. Shakshuki and A. Yasar, eds.), vol. 130 of <span id="bib.bib169.2.2" class="ltx_text ltx_font_italic">Procedia Computer Science</span>, pp. 400–407, Elsevier, 2018.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
R. Jabbar, K. Al-Khalifa, M. Kharbeche, W. K. M. Alhajyaseen, M. A. Jafari, and S. Jiang, “Real-time driver drowsiness detection for android application using deep neural networks techniques,” in <span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT 2018) / Affiliated Workshops, May 8-11, 2018, Porto, Portugal</span> (E. M. Shakshuki and A. Yasar, eds.), vol. 130 of <span id="bib.bib170.2.2" class="ltx_text ltx_font_italic">Procedia Computer Science</span>, pp. 400–407, Elsevier, 2018.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
W. Kong, W. Lin, F. Babiloni, S. Hu, and G. Borghini, “Investigating driver fatigue<em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic"> versus</em> alertness using the granger causality network,” <span id="bib.bib171.2.2" class="ltx_text ltx_font_italic">Sensors</span>, vol. 15, no. 8, pp. 19181–19198, 2015.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
F. Boutros, V. Struc, J. Fiérrez, and N. Damer, “Synthetic data for face recognition: Current state and future prospects,” <span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">Image Vis. Comput.</span>, vol. 135, p. 104688, 2023.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
F. Boutros, J. H. Grebe, A. Kuijper, and N. Damer, “Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion models,” in <span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</span>, pp. 19593–19604, IEEE, 2023.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
S. Debbarma and S. Bhadra, “Signal decomposition method with sensor-fusion for reducing motion artifacts in intra-oral EEG,” in <span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">2023 IEEE SENSORS, Vienna, Austria, October 29 - Nov. 1, 2023</span>, pp. 1–4, IEEE, 2023.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
M. S. Hossain, M. E. H. Chowdhury, M. B. I. Reaz, S. H. M. Ali, A. A. A. Bakar, S. Kiranyaz, A. Khandakar, M. Alhatou, R. Habib, and M. M. Hossain, “Motion artifacts correction from single-channel EEG and fnirs signals using novel wavelet packet decomposition in combination with canonical correlation analysis,” <span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 22, no. 9, p. 3169, 2022.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
P. Gajbhiye, R. K. Tripathy, A. Bhattacharyya, and R. B. Pachori, “Novel approaches for the removal of motion artifact from eeg recordings,” <span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">IEEE Sensors Journal</span>, vol. 19, no. 22, pp. 10600–10608, 2019.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
J.-M. Lee, D.-J. Kim, I.-Y. Kim, K.-S. Park, and S. I. Kim, “Detrended fluctuation analysis of eeg in sleep apnea using mit/bih polysomnography data,” <span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">Computers in biology and medicine</span>, vol. 32, no. 1, pp. 37–47, 2002.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Q. Ji, Z. Zhu, and P. Lan, “Real-time nonintrusive monitoring and prediction of driver fatigue,” <span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Veh. Technol.</span>, vol. 53, no. 4, pp. 1052–1068, 2004.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
H. U. R. Siddiqui, A. A. Saleem, R. Brown, B. Bademci, E. Lee, F. Rustam, and S. E. M. Dudley, “Non-invasive driver drowsiness detection system,” <span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 14, p. 4833, 2021.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
E. Zilberg, D. Burton, M. Xu, M. Karrar, and S. Lal, “Methodology and initial analysis results for development of non-invasive and hybrid driver drowsiness detection systems,” in <span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">Advances in Broadband Communication and Networks</span>, pp. 309–328, River Publishers, 2022.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
G. Schalk, “Decoding details of human functions using electrocorticography,” in <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">4th International Winter Conference on Brain-Computer Interface, BCI 2016, Gangwon Province, South Korea, February 22-24, 2016</span>, pp. 1–2, IEEE, 2016.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
J. Ma, Y. L. Murphey, and H. Zhao, “Real time drowsiness detection based on lateral distance using wavelet transform and neural network,” in <span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">IEEE Symposium Series on Computational Intelligence, SSCI 2015, Cape Town, South Africa, December 7-10, 2015</span>, pp. 411–418, IEEE, 2015.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
J. Zhao, C. Rong, C. Lin, and X. Dang, “Multivariate time series data imputation using attention-based mechanism,” <span id="bib.bib183.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, vol. 542, p. 126238, 2023.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
S. Ryu, M. Kim, and H. Kim, “Denoising autoencoder-based missing value imputation for smart meters,” <span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8, pp. 40656–40666, 2020.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
I. G. Daza, L. M. Bergasa, S. Bronte, J. J. Yebes, J. Almazán, and R. Arroyo, “Fusion of optimized indicators from advanced driver assistance systems (ADAS) for driver drowsiness detection,” <span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 14, no. 1, pp. 1106–1131, 2014.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
B. Cheng, W. Zhang, Y. Lin, R. Feng, and X. Zhang, “Driver drowsiness detection based on multisource information,” <span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">Human Factors and Ergonomics in Manufacturing &amp; Service Industries</span>, vol. 22, no. 5, pp. 450–467, 2012.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
J. Gwak, A. Hirao, and M. Shino, “An investigation of early detection of driver drowsiness using ensemble machine learning based on hybrid sensing,” <span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol. 10, no. 8, p. 2890, 2020.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
S. Samiee, S. Azadi, R. Kazemi, A. Nahvi, and A. Eichberger, “Data fusion to develop a driver drowsiness detection system with robustness to signal loss,” <span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 14, no. 9, pp. 17832–17847, 2014.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Z. Yang, R. O. Sinnott, J. Bailey, and Q. Ke, “A survey of automated data augmentation algorithms for deep learning-based image classification tasks,” <span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">Knowledge and Information Systems</span>, vol. 65, no. 7, pp. 2805–2861, 2023.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
B. P. Nayak, S. Kar, A. Routray, and A. K. Padhi, “A biomedical approach to retrieve information on driver’s fatigue by integrating eeg, ecg and blood biomarkers during simulated driving session,” in <span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)</span>, pp. 1–6, IEEE, 2012.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
K. T. Chui, K. F. Tsang, H. R. Chi, B. W. K. Ling, and C. K. Wu, “An accurate ecg-based transportation safety drowsiness detection scheme,” <span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, vol. 12, no. 4, pp. 1438–1452, 2016.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
G. M. Mohamed, S. S. Patel, and N. Naicker, “Data augmentation for deep learning algorithms that perform driver drowsiness detection,” <span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">International Journal of Advanced Computer Science and Applications</span>, vol. 14, pp. 233–248, 02 2023.

</span>
</li>
</ul>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span id="S10.5.1" class="ltx_text ltx_font_smallcaps">Biography Section</span>
</h2>

<figure id="S10.1" class="ltx_float biography">
<table id="S10.1.1" class="ltx_tabular">
<tr id="S10.1.1.1" class="ltx_tr">
<td id="S10.1.1.1.1" class="ltx_td"><img src="/html/2408.12990/assets/biying_gray.jpg" id="S10.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="92" height="125" alt="[Uncaptioned image]"></td>
<td id="S10.1.1.1.2" class="ltx_td">
<span id="S10.1.1.1.2.1" class="ltx_inline-block">
<span id="S10.1.1.1.2.1.1" class="ltx_p"><span id="S10.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Biying Fu</span> 
received the B.Sc. degree and the M.Sc. degree in electrical engineering and information technology from University of Karlsruhe, Germany, in 2011 and 2014 respectively. She finished her Ph.D. in 2020 in informatics at Technical University of Darmstadt, Germany, on the topic ”Sensor Applications for Human Activity Recognition in Smart Environments”. From 2014 till today, she is working with the Fraunhofer Institute for Computer Graphics (IGD), Germany. She is currently working in the department for Smart Living &amp; Biometric Technologies. Starting from 2022, she also works as a half-time Professor in Informatics at Hochschule RheinMain with the major ”Smart Environments”. Her research interest includes intelligent human machine interaction, machine learning, and deep learning for human activity recognition with sensor data.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="S10.2" class="ltx_float biography">
<table id="S10.2.1" class="ltx_tabular">
<tr id="S10.2.1.1" class="ltx_tr">
<td id="S10.2.1.1.1" class="ltx_td"><img src="/html/2408.12990/assets/fadi_boutros.jpeg" id="S10.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="120" alt="[Uncaptioned image]"></td>
<td id="S10.2.1.1.2" class="ltx_td">
<span id="S10.2.1.1.2.1" class="ltx_inline-block">
<span id="S10.2.1.1.2.1.1" class="ltx_p"><span id="S10.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Fadi Boutros</span>  is a scientific researcher at the Fraunhofer IGD and a principal investigator at the National Research Center for Applied Cybersecurity ATHENE, Germany. Fadi received his Ph.D. in computer science from TU Darmstadt (2022) and a master’s degree in ”Distributed Software Systems” from TU Darmstadt (2019). Also, he is participating in the Software Campus program, a management program of the German Federal Ministry of Education and Research (BMBF).
He authored and co-authored several conference and journal papers. His main research interests lie in the fields of biometrics, machine learning, synthetic data, and efficient deep learning. For his scientific work, he received several awards, including the CAST-Förderpreis 2019 award, the IJCB 2022 Qualcomm Audience Choice Award, and the 2022 EAB Biometrics Industry Award from the European Association for Biometrics (EAB) for his Ph.D. dissertation.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="S10.3" class="ltx_float biography">
<table id="S10.3.1" class="ltx_tabular">
<tr id="S10.3.1.1" class="ltx_tr">
<td id="S10.3.1.1.1" class="ltx_td"><img src="/html/2408.12990/assets/CT_Lin_gray.jpg" id="S10.3.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="S10.3.1.1.2" class="ltx_td">
<span id="S10.3.1.1.2.1" class="ltx_inline-block">
<span id="S10.3.1.1.2.1.1" class="ltx_p"><span id="S10.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Chin-Teng Lin</span>  (Fellow, IEEE) received the B.Sc. degree from National Chiao-Tung University (NCTU), Taiwan in 1986, and holds Master’s and PhD degrees in Electrical Engineering from Purdue University, USA, received in 1989 and 1992, respectively. He is currently a distinguished professor at School of Computer Science and Director of the Human Centric AI (HAI) Centre and Co-Director of the Australian Artificial Intelligence Institute (AAII) within the Faculty of Engineering and Information Technology at the University of Technology Sydney, Australia. He is also an Honorary Chair Professor of Electrical and Computer Engineering at NCTU. For his contributions to biologically inspired information systems, Prof Lin was awarded Fellowship with the IEEE in 2005, and with the International Fuzzy Systems Association (IFSA) in 2012. He received the IEEE Fuzzy Systems Pioneer Award in 2017. He has held notable positions as editor-in-chief of IEEE Transactions on Fuzzy Systems from 2011 to 2016; seats on Board of Governors for the IEEE Circuits and Systems (CAS) Society (2005-2008), IEEE Systems, Man, Cybernetics (SMC) Society (2003-2005), IEEE Computational Intelligence Society (2008-2010); Chair of the IEEE Taipei Section (2009-2010); Chair of IEEE CIS Awards Committee (2022, 2023); Distinguished Lecturer with the IEEE CAS Society (2003-2005) and the CIS Society (2015-2017); Chair of the IEEE CIS Distinguished Lecturer Program Committee (2018-2019); Deputy Editor-in-Chief of IEEE Transactions on Circuits and Systems-II (2006-2008); Program Chair of the IEEE International Conference on Systems, Man, and Cybernetics (2005); and General Chair of the 2011 IEEE International Conference on Fuzzy Systems. He is the co-author of Neural Fuzzy Systems (Prentice-Hall) and the author of Neural Fuzzy Control Systems with Structure and Parameter Learning (World Scientific). He has published more than 450 journal papers including about 210 IEEE journal papers in the areas of neural networks, fuzzy systems, brain-computer interface, multimedia information processing, cognitive neuro-engineering, and human-machine teaming, that have been cited more than 36,000 times. His current h-index is 94, and his i10-index is 436.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="S10.4" class="ltx_float biography">
<table id="S10.4.1" class="ltx_tabular">
<tr id="S10.4.1.1" class="ltx_tr">
<td id="S10.4.1.1.1" class="ltx_td"><img src="/html/2408.12990/assets/Naser_Damer_crop.jpg" id="S10.4.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="96" height="125" alt="[Uncaptioned image]"></td>
<td id="S10.4.1.1.2" class="ltx_td">
<span id="S10.4.1.1.2.1" class="ltx_inline-block">
<span id="S10.4.1.1.2.1.1" class="ltx_p"><span id="S10.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Naser Damer</span>  (Senior Member, IEEE) received the Ph.D. degree in computer science from TU Darmstadt in 2018. He is a Senior Researcher with Fraunhofer IGD. He is a Research Area Co-Coordinator and a Principal Investigator with the National Research Center for Applied Cybersecurity ATHENE, Germany. He lectures on Human and Identity-Centric Machine Learning with TU Darmstadt, Germany. His main research interests lie in the fields of biometrics and human-centric machine learning. He serves as an Associate Editor for Pattern Recognition (Elsevier), the Visual Computer (Springer), and the IEEE Transactions on Information Forensics and Security. He represents the German Institute for Standardization (DIN) in the ISO/IEC SC37 International Biometrics Standardization Committee. He is a member of the organizing teams of several conferences, workshops, and special sessions, including being the program co-chair of BIOSIG and a member of the IEEE Biometrics Council serving on its Technical Activities Committee.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.12989" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.12990" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.12990">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.12990" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.12991" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:47:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
