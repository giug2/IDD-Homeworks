<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems</title>
<!--Generated on Fri Sep 20 03:07:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="User Audit,  Personalized Recommendation Systems,  Algorithm Bias,  Intervention Tool,  Algorithm Transparency" lang="en" name="keywords"/>
<base href="/html/2409.13176v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S1" title="In Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S2" title="In Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Challenges of User Everyday Audits</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S2.SS1" title="In 2. Challenges of User Everyday Audits ‣ Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Novice and Low-Literacy Users</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S2.SS2" title="In 2. Challenges of User Everyday Audits ‣ Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Unstructured Audit Paths</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S2.SS3" title="In 2. Challenges of User Everyday Audits ‣ Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Noise</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S2.SS4" title="In 2. Challenges of User Everyday Audits ‣ Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Transitioning to Collective Audits</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S3" title="In Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Designing a User Audit Intervention Tool for Personalized Recommendation Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#S4" title="In Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qunfang Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Harvard University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Cambridge</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Massachusetts</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:qunfangwu@fas.harvard.edu">qunfangwu@fas.harvard.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lu Xian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Michigan</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xianl@umich.edu">xianl@umich.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id9.id1">As algorithms increasingly shape user experiences on personalized recommendation platforms, there is a growing need for tools that empower end users to audit these algorithms for potential bias and harms. This paper introduces a novel intervention tool, <span class="ltx_text ltx_font_italic" id="id9.id1.1">MapMyFeed</span>, designed to support everyday user audits. The tool addresses key challenges associated with user-driven algorithm audits, such as low algorithm literacy, unstructured audit paths, and the presence of noise. MapMyFeed assists users by offering guiding prompts, tracking audit paths via a browser extension, and visualizing audit results through a live dashboard. The tool will not only foster users’ algorithmic literacy and awareness but also enhance more transparent and fair recommendation systems.</p>
</div>
<div class="ltx_keywords">User Audit, Personalized Recommendation Systems, Algorithm Bias, Intervention Tool, Algorithm Transparency
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The 7th FAccTRec Workshop: Responsible Recommendation; October 14, 2024; </span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Algorithm audits examine the performance and effects of algorithmic systems on people especially when the internals of these systems remain opaque <cite class="ltx_cite ltx_citemacro_citep">(Metaxa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib26" title="">2021</a>)</cite>. They serve as tools for accountability that help mitigate undesirable consequences of algorithms <cite class="ltx_cite ltx_citemacro_citep">(Metaxa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib26" title="">2021</a>)</cite>.
The approach to conducting such audits is shaped by various factors including the auditors (who execute audits), stakeholders (who are affected by or benefit from audits), and evaluation metrics (what aspects are audited). Auditors can range from internal teams within companies to firms specializing in audits, as well as independent organizations and individuals <cite class="ltx_cite ltx_citemacro_citep">(Costanza-Chock et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib13" title="">2022</a>)</cite>.
Stakeholders range from end-users, who interact with technologies like recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib28" title="">2020</a>)</cite> and facial recognition <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib6" title="">2018</a>)</cite>, to regulators and algorithm vendors, each with distinct key interests such as personalized relevance of the recommendations, user privacy, policy-making, and risk management, respectively <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib4" title="">2021a</a>)</cite>. The choice of evaluation metrics, from discrimination/bias to efficiency or transparency <cite class="ltx_cite ltx_citemacro_citep">(Bandy, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib3" title="">2021</a>)</cite>, further influences audit methodologies <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib4" title="">2021a</a>; Costanza-Chock et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib13" title="">2022</a>)</cite>.
As such, developing effective algorithm audits requires an understanding of the interplay among auditors, the identification of stakeholders, and evaluation metrics <cite class="ltx_cite ltx_citemacro_citep">(Ayling and Chapman, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib2" title="">2022</a>; Sandvig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib29" title="">2014</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib4" title="">2021a</a>)</cite>. In this paper, we focus on a particular auditing approach, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">end-user audits</span>, in the context of social media recommendation algorithms, and propose an intervention tool for this approach. Building on prior literature, our proposed intervention positions end users as both auditors and key stakeholders, and develops user-centered metrics to assist their audits.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">End-user audits leverage users’ everyday interactions with algorithmic systems to surface harmful behaviors of the systems that are often hard to detect through formal audits conducted outside the everyday use context <cite class="ltx_cite ltx_citemacro_citep">(Seaver, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib30" title="">2017</a>; Friedman and Nissenbaum, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib17" title="">1996</a>)</cite>. Initially proposed by <cite class="ltx_cite ltx_citemacro_citet">Shen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>, the notion of user everyday audit is borrowed from Suchman’s  <cite class="ltx_cite ltx_citemacro_citep">(Suchman, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib32" title="">1987</a>)</cite> situational actions, where users adapt and decide their actions based on immediate circumstances rather than strictly following a pre-defined plan or script. Users often face unexpected or problematic algorithm recommendations through everyday interactions with algorithmic systems. In response, they adapt their actions dynamically, leveraging the context of the situation to better understand and effectively respond to what they have faced <cite class="ltx_cite ltx_citemacro_citep">(Suchman, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib32" title="">1987</a>; Cen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib11" title="">2023</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib21" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Different from formal, systemic audits that often consider regulators and society at large as key stakeholders <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib4" title="">2021a</a>; Metaxa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib26" title="">2021</a>)</cite>, everyday audits consider users as key stakeholders and empower them to understand and manage these algorithms.
Prior work has underscored the value of centering around user interactions and experiences with the platforms in understanding the interdependent relations between users, algorithms, and social media platforms <cite class="ltx_cite ltx_citemacro_citep">(e.g., Cotter, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib14" title="">2019</a>)</cite> and in empowering resistance against algorithmic biases and harms <cite class="ltx_cite ltx_citemacro_citep">(e.g., Velkova and Kaun, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib33" title="">2021</a>; Karizat et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib19" title="">2021</a>)</cite>. Scholars have also investigated users’ algorithm training strategies <cite class="ltx_cite ltx_citemacro_citep">(Cen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib9" title="">2024a</a>; Burrell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib7" title="">2019</a>; Haupt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib18" title="">2023</a>)</cite> and user audit models <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>; Lam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib20" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib23" title="">2023</a>)</cite> across different algorithmic systems. While this prior scholarship provides a foundation for conducting user audits, little work has discussed and designed interventions or educational tools to address the challenges of user everyday audits and assist users in performing these audits. Our contributions include designing a new tool for user everyday audits and examining intervention mechanisms (e.g., guiding prompts, audit visualization) to better support user audits. The proposed tool aims to improve users’ literacy and awareness of algorithmic impacts while promoting more transparent and fair recommendations. In this position paper, we first discuss the challenges of user everyday audits and then propose an intervention tool that addresses some, if not all, of these challenges for personalized recommendation systems and briefly delineates its functions. We conclude by discussing the future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Challenges of User Everyday Audits</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We categorize the challenges into four aspects: (1) lack of auditor expertise for novice and low-literacy users, (2) unstructured audit paths, (3) noise in audit performance, and (4) obstacles to transitioning to collective audits.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Novice and Low-Literacy Users</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">End-users vary in their algorithm literacy and expertise. While many user audit cases show that users have some algorithm expertise, others reveal that those with little or no algorithm knowledge need support from researchers or their technically savvy peers <cite class="ltx_cite ltx_citemacro_citep">(Lei, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib22" title="">2021</a>; Qadri, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib27" title="">2021</a>)</cite>. A user algorithm audit usually starts with problem initiation <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>. Users, especially those new to or with low algorithm literacy, may initiate audits based on vague perceptions and speculations. Users’ experiences and knowledge also affect their ability to identify algorithm biases. As <cite class="ltx_cite ltx_citemacro_citet">DeVos et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite> found, people’s experiences determine the types of bias they recognize; they often relied on second-hand knowledge and exhibited their own cognitive biases. Unlike crowd-sourced audits, where users are passive data contributors, everyday audits require users to lead their audits as “investigators” <cite class="ltx_cite ltx_citemacro_citep">(Lam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib20" title="">2022</a>)</cite>. Therefore, intervention tools should offer necessary knowledge and prompts for users at different levels of algorithm literacy to develop audit goals and strategies.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Unstructured Audit Paths</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">User algorithm audits benefit from the user’s everyday use of the system, making the audit process organic <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>. This organic nature is reflected in the entire audit process: a user may start an audit spontaneously, conduct the audit intermittently, and even carry out multiple audits simultaneously. Thus, user everyday audits follow a “non-linear” path and are unstructured <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>. This process differs from prior approaches in explainable AI or interactive recommendation systems, where users were guided by a fixed or semi-fixed procedure to understand the internal workings of a system and ways to achieve a desirable outcome.
Prior research has attempted to understand the process of users conducting algorithm audits <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">DeVos et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite> found that users followed a process of “search inspiration,” “sensemaking,” and “remediation” while identifying harmful algorithms. However, to guide fluid forms of user audits, the steps and process they theorized about need to be translated into adaptable and feasible guidance and supported by tools. An intervention tool can support unstructured audits by recording user actions during the audit process and organizing them into more structured formats, such as topics and timelines, for easier sensemaking.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Another issue with the unstructured nature of audits is users’ cognitive load. User audits often occur during routine system usage, where users may not consistently focus on auditing tasks. Additionally, some algorithmic issues require long-term observations and evaluations. Therefore, an intervention tool should reduce cognitive load by internally reminding users of their audit tasks and recording audit actions and outcomes over extended periods.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Noise</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">User audits are less controlled or systematic compared to internal or third-party audits, which can lead to potential noise and errors. Identifying the signal versus noise is a well-known challenge for user- and crowd-driven approaches <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite>. Additionally, various situational factors might influence the outcomes of user audits. These factors include the user’s environment, the context of algorithm use, and individual biases. Understanding these factors is crucial for identifying and mitigating noise or errors <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Casper et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib8" title="">2024</a>)</cite> discussed that black-box audits have inherent limitations that can lead to noise. Many black-box methods for explaining model decisions are unreliable because they often fail to accurately identify causal relationships between input features and outputs. For example, users might incorrectly attribute an algorithm’s decision to irrelevant features, misleading engineers about the true cause of the bias. To ensure the accuracy and integrity of the audit, the tool needs to manage noise effectively. We also acknowledge that algorithmic biases are often first identified through isolated incidents or personal anecdotes reported by users. Although these individual reports may not reflect systemic issues, evidence of harm from even a single user is valuable, regardless of its replicability or prevalence across the platform. These reports can reveal issues impacting minority groups or edge cases that might otherwise remain hidden. Therefore, the intervention tool should not minimize “noise” to enhance scientific robustness but utilize it to identify and address specific harms experienced by underrepresented or marginalized groups.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Transitioning to Collective Audits</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Individual user audits often transition into collective actions. As previously mentioned, algorithm audits extend beyond individual test cases to make broader declarations about the system as a whole <cite class="ltx_cite ltx_citemacro_citep">(Metaxa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib26" title="">2021</a>)</cite>. However, user audit is limited in developing a comprehensive understanding of the system <cite class="ltx_cite ltx_citemacro_citep">(Casper et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib8" title="">2024</a>)</cite>. In contrast, collective audits can more easily accumulate evidence and validate individual findings.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Supporting collective audits requires addressing issues throughout different audit stages.
Users first need to understand the audit task, a process known as collective sensemaking  <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>. However, <cite class="ltx_cite ltx_citemacro_citet">DeVos et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite> found that the lack of contextual information made it difficult for the group to differentiate between various explanations for observed behaviors based on user reports.
For example, explanations could vary among users from different demographics. Without this demographic context, the group found it difficult to understand these different explanations. They suggested that design solutions could allow for follow-up questions to those reporting their observations or provide reminders for auditors to consider missing information <cite class="ltx_cite ltx_citemacro_citep">(DeVos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib15" title="">2022</a>)</cite>.
Other challenges for collective audits include role assignment <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib23" title="">2023</a>)</cite>, data sharing <cite class="ltx_cite ltx_citemacro_citep">(Lam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib20" title="">2022</a>)</cite>, and task coordination <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>)</cite>.
Such nuanced collective audits necessitate robust support mechanisms.
Our intervention tool primarily focuses on individual user audits but is also designed to be adaptable for potential collective audits.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Designing a User Audit Intervention Tool for Personalized Recommendation Systems</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In response to these challenges, we propose a user audit intervention tool design for personalized recommendation systems. We consider a common scenario: users’ interactions with the system depend not only on the content recommended to them but on their attempts to shape future recommendations. For example, on personalized recommendation platforms, users often leverage their understanding of important engagement functions, including likes, comments, and dwell time, to manipulate the algorithm <cite class="ltx_cite ltx_citemacro_citep">(Cen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib10" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Our tool, <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">MapMyFeed</span>, assists everyday users in navigating the auditing process through three key features: (1) it offers guiding prompts that help users initiate and conduct audits, (2) it uses a browser extension to track the audit path, and (3) it employs a live dashboard to visualize and communicate the audit results.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The guiding prompts in our tool are crafted to assist novice and low-literacy users by equipping them with the thinking tools needed to navigate the auditing process. Drawing on explainable AI and algorithmic audit literature <cite class="ltx_cite ltx_citemacro_citep">(e.g., Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib24" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib25" title="">2021</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib31" title="">2021</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib5" title="">2021b</a>; Ehsan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib16" title="">2021</a>; Chromik and Butz, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib12" title="">2021</a>)</cite>, we have developed prompts that encourage users to engage critically with the system. The semi-structured guiding prompts offer initial context for users to start an audit with specific, self-defined goals, after which they are encouraged to proceed with organic interaction and exploration.
They prompt users to consider the relevance of recommendations (system performance), explore hypothetical changes in system outputs if certain features are altered (“what if” scenarios), and make preliminary hypotheses about how the system functions and the impact of its features (how). This semi-structured approach not only aids users in establishing audit objectives and metrics and understanding the steps of effective audits, but also allows them the flexibility to adhere to their preferred practices or explore new routines in their interactions with the recommender system.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The recording of audit paths meticulously logs users’ exposure to and interactions with the system during an audit, using a browser extension. This extension tracks user behaviors—for instance, on a social media platform—including viewing, clicking, liking, saving, sharing, and following.
Such historical data provides the context for users to understand their engagement with the system <cite class="ltx_cite ltx_citemacro_citep">(Zannettou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13176v1#bib.bib34" title="">2024</a>)</cite> and how their engagement shapes audit outcomes. Our semi-structured approach to audit goal setting, combined with the documentation of audit paths, helps address the issue of non-linear and unstructured audit paths in user everyday audits.
The collected user data will be anonymized before further analysis and presentation; user data will be used for research purposes only.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The live dashboard offers users reports summarizing their audits on the platform. These reports detail their interactions and audit outcomes, encourage them to reflect on their audit objectives, and enable them to review and compare their results with those of others in an aggregated format. This feature facilitates a deeper analysis and provides a broader context for understanding each user’s audit results. The reports filter out noise to ensure the accuracy of audit outcomes.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">The tool offers educational resources to guide them in understanding the mechanisms of algorithmic recommendations. This proactive involvement allows users to identify and report problematic or biased algorithmic behaviors, cultivating their awareness of fairness issues in recommendation systems.
By fostering algorithmic literacy, the tool not only educates users but also helps make the internal workings of the system and its impacts legible to users through their daily interactions. As an accountability mechanism, our tool supports the development of more equitable digital environments.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Future Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In the next phase of this research, we will focus on further developing and refining the proposed tool. We will conduct controlled lab experiments to evaluate how effectively the tool supports users in addressing key challenges, such as unstructured audit paths, noise management, and algorithmic literacy. Specifically, to assess the proposed intervention mechanisms, we will conduct user studies, including surveys, interviews, and think-aloud sessions, to capture users’ experiences with features like guiding prompts and audit outcome visualization. Through iterative design cycles, feedback gathered from these studies will guide ongoing improvements to the tool. In the long term, we will conduct longitudinal field studies to evaluate the tool’s performance in real-world environments, tracking users’ audit behaviors over time to assess how the tool improves user algorithmic literacy and its ability to surface algorithmic biases.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ayling and Chapman (2022)</span>
<span class="ltx_bibblock">
Jacqui Ayling and Adriane Chapman. 2022.

</span>
<span class="ltx_bibblock">Putting AI ethics to work: are the tools fit for purpose?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">AI and Ethics</em> 2, 3 (2022), 405–429.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bandy (2021)</span>
<span class="ltx_bibblock">
Jack Bandy. 2021.

</span>
<span class="ltx_bibblock">Problematic machine behavior: A systematic literature review of algorithm audits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the acm on human-computer interaction</em> 5, CSCW1 (2021), 1–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Shea Brown, Jovana Davidovic, and Ali Hasan. 2021a.

</span>
<span class="ltx_bibblock">The algorithm audit: Scoring the algorithms that score us.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Big Data &amp; Society</em> 8, 1 (2021), 2053951720983865.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Shea Brown, Jovana Davidovic, and Ali Hasan. 2021b.

</span>
<span class="ltx_bibblock">The algorithm audit: Scoring the algorithms that score us.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Big Data &amp; Society</em> (Jan. 2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/2053951720983865" title="">https://doi.org/10.1177/2053951720983865</a>
</span>
<span class="ltx_bibblock">Publisher: SAGE PublicationsSage UK: London, England.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)</span>
<span class="ltx_bibblock">
Joy Buolamwini and Timnit Gebru. 2018.

</span>
<span class="ltx_bibblock">Gender shades: Intersectional accuracy disparities in commercial gender classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Conference on fairness, accountability and transparency</em>. PMLR, 77–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burrell et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jenna Burrell, Zoe Kahn, Anne Jonas, and Daniel Griffin. 2019.

</span>
<span class="ltx_bibblock">When Users Control the Algorithms: Values Expressed in Practices on Twitter.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 3, CSCW (Nov. 2019), 1–20.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3359240" title="">https://doi.org/10.1145/3359240</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, David Krueger, and Dylan Hadfield-Menell. 2024.

</span>
<span class="ltx_bibblock">Black-Box Access is Insufficient for Rigorous AI Audits. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">The 2024 ACM Conference on Fairness, Accountability, and Transparency</em>. ACM, Rio de Janeiro Brazil, 2254–2272.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3630106.3659037" title="">https://doi.org/10.1145/3630106.3659037</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Sarah H. Cen, Andrew Ilyas, Jennifer Allen, Hannah Li, and Aleksander Madry. 2024a.

</span>
<span class="ltx_bibblock">Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.05596" title="">http://arxiv.org/abs/2405.05596</a>
</span>
<span class="ltx_bibblock">arXiv:2405.05596 [cs, stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Sarah H Cen, Andrew Ilyas, Jennifer Allen, Hannah Li, and Aleksander Madry. 2024b.

</span>
<span class="ltx_bibblock">Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">arXiv preprint arXiv:2405.05596</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cen et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sarah H. Cen, Andrew Ilyas, and Aleksander Madry. 2023.

</span>
<span class="ltx_bibblock">User Strategization and Trustworthy Algorithms.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.17666" title="">http://arxiv.org/abs/2312.17666</a>
</span>
<span class="ltx_bibblock">arXiv:2312.17666 [cs, stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chromik and Butz (2021)</span>
<span class="ltx_bibblock">
Michael Chromik and Andreas Butz. 2021.

</span>
<span class="ltx_bibblock">Human-XAI interaction: a review and design principles for explanation user interfaces. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Human-Computer Interaction–INTERACT 2021: 18th IFIP TC 13 International Conference, Bari, Italy, August 30–September 3, 2021, Proceedings, Part II 18</em>. Springer, 619–640.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costanza-Chock et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. 2022.

</span>
<span class="ltx_bibblock">Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. 1571–1583.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cotter (2019)</span>
<span class="ltx_bibblock">
Kelley Cotter. 2019.

</span>
<span class="ltx_bibblock">Playing the visibility game: How digital influencers and algorithms negotiate influence on Instagram.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">New media &amp; society</em> 21, 4 (2019), 895–913.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeVos et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Alicia DeVos, Aditi Dhabalia, Hong Shen, Kenneth Holstein, and Motahhare Eslami. 2022.

</span>
<span class="ltx_bibblock">Toward User-Driven Algorithm Auditing: Investigating users’ strategies for uncovering harmful algorithmic behavior. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">CHI Conference on Human Factors in Computing Systems</em>. ACM, New Orleans LA USA, 1–19.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3517441" title="">https://doi.org/10.1145/3491102.3517441</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsan et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Martina Mara, Marc Streit, Sandra Wachter, Andreas Riener, and Mark O Riedl. 2021.

</span>
<span class="ltx_bibblock">Operationalizing human-centered perspectives in explainable AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Extended abstracts of the 2021 CHI conference on human factors in computing systems</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman and Nissenbaum (1996)</span>
<span class="ltx_bibblock">
Batya Friedman and Helen Nissenbaum. 1996.

</span>
<span class="ltx_bibblock">Bias in computer systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ACM Transactions on information systems (TOIS)</em> 14, 3 (1996), 330–347.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haupt et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Andreas Haupt, Dylan Hadfield-Menell, and Chara Podimata. 2023.

</span>
<span class="ltx_bibblock">Recommending to Strategic Users.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.06559" title="">http://arxiv.org/abs/2302.06559</a>
</span>
<span class="ltx_bibblock">arXiv:2302.06559 [cs, econ].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karizat et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nadia Karizat, Dan Delmonaco, Motahhare Eslami, and Nazanin Andalibi. 2021.

</span>
<span class="ltx_bibblock">Algorithmic folk theories and identity: How TikTok users co-produce Knowledge of identity and engage in algorithmic resistance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW2 (2021), 1–44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michelle S. Lam, Mitchell L. Gordon, Danaë Metaxa, Jeffrey T. Hancock, James A. Landay, and Michael S. Bernstein. 2022.

</span>
<span class="ltx_bibblock">End-User Audits: A System Empowering Communities to Lead Large-Scale Investigations of Harmful Algorithmic Behavior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 6, CSCW2 (Nov. 2022), 1–34.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3555625" title="">https://doi.org/10.1145/3555625</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Angela Y. Lee, Hannah Mieczkowski, Nicole B. Ellison, and Jeffrey T. Hancock. 2022.

</span>
<span class="ltx_bibblock">The Algorithmic Crystal: Conceptualizing the Self through Algorithmic Personalization on TikTok.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 6, CSCW2 (Nov. 2022), 1–22.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3555601" title="">https://doi.org/10.1145/3555601</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei (2021)</span>
<span class="ltx_bibblock">
Ya-Wen Lei. 2021.

</span>
<span class="ltx_bibblock">Delivering solidarity: Platform architecture and collective contention in China’s platform economy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">American Sociological Review</em> 86, 2 (2021), 279–309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rena Li, Sara Kingsley, Chelsea Fan, Proteeti Sinha, Nora Wai, Jaimie Lee, Hong Shen, Motahhare Eslami, and Jason Hong. 2023.

</span>
<span class="ltx_bibblock">Participation and Division of Labor in User-Driven Algorithm Audits: How Do Everyday Users Work together to Surface Algorithmic Harms?. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. ACM, Hamburg Germany, 1–19.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3582074" title="">https://doi.org/10.1145/3544548.3582074</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Q Vera Liao, Daniel Gruen, and Sarah Miller. 2020.

</span>
<span class="ltx_bibblock">Questioning the AI: informing design practices for explainable AI user experiences. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2020 CHI conference on human factors in computing systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Q Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, and Daby Sow. 2021.

</span>
<span class="ltx_bibblock">Question-driven design process for explainable AI user experiences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:2104.03483</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Metaxa et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Danaë Metaxa, Joon Sung Park, Ronald E. Robertson, Karrie Karahalios, Christo Wilson, Jeff Hancock, and Christian Sandvig. 2021.

</span>
<span class="ltx_bibblock">Auditing algorithms: Understanding algorithmic systems from the outside in.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Foundations and Trends® in Human–Computer Interaction</em> 14, 4 (2021), 272–344.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nowpublishers.com/article/Details/HCI-083" title="">https://www.nowpublishers.com/article/Details/HCI-083</a>
</span>
<span class="ltx_bibblock">Publisher: Now Publishers, Inc..

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qadri (2021)</span>
<span class="ltx_bibblock">
Rida Qadri. 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Delivery Drivers Are Using Grey Market Apps to Make Their Jobs Suck Less</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio AF Almeida, and Wagner Meira Jr. 2020.

</span>
<span class="ltx_bibblock">Auditing radicalization pathways on YouTube. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 2020 conference on fairness, accountability, and transparency</em>. 131–141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandvig et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Christian Sandvig, Kevin Hamilton, Karrie Karahalios, and Cedric Langbort. 2014.

</span>
<span class="ltx_bibblock">Auditing algorithms: Research methods for detecting discrimination on internet platforms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Data and discrimination: converting critical concerns into productive inquiry</em> 22, 2014 (2014), 4349–4357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seaver (2017)</span>
<span class="ltx_bibblock">
Nick Seaver. 2017.

</span>
<span class="ltx_bibblock">Algorithms as culture: Some tactics for the ethnography of algorithmic systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Big data &amp; society</em> 4, 2 (2017), 2053951717738104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hong Shen, Alicia DeVos, Motahhare Eslami, and Kenneth Holstein. 2021.

</span>
<span class="ltx_bibblock">Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW2 (Oct. 2021), 433:1–433:29.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3479577" title="">https://doi.org/10.1145/3479577</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suchman (1987)</span>
<span class="ltx_bibblock">
Lucille Alice Suchman. 1987.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Plans and situated actions: The problem of human-machine communication</em>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Velkova and Kaun (2021)</span>
<span class="ltx_bibblock">
Julia Velkova and Anne Kaun. 2021.

</span>
<span class="ltx_bibblock">Algorithmic resistance: Media practices and the politics of repair.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Information, Communication &amp; Society</em> 24, 4 (2021), 523–540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zannettou et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Savvas Zannettou, Olivia Nemes-Nemeth, Oshrat Ayalon, Angelica Goetzen, Krishna P Gummadi, Elissa M Redmiles, and Franziska Roesner. 2024.

</span>
<span class="ltx_bibblock">Analyzing User Engagement with TikTok’s Short Format Video Recommendations using Data Donations. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 03:07:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
